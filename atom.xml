<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>A.P的文艺杂谈</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://airpoet.github.io/"/>
  <updated>2018-09-06T06:50:45.766Z</updated>
  <id>https://airpoet.github.io/</id>
  
  <author>
    <name>airpoet</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Git 使用</title>
    <link href="https://airpoet.github.io/2018/09/06/Tools/Git/Git-%E4%BD%BF%E7%94%A8/"/>
    <id>https://airpoet.github.io/2018/09/06/Tools/Git/Git-使用/</id>
    <published>2018-09-06T04:19:00.370Z</published>
    <updated>2018-09-06T06:50:45.766Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一-IDEA-中-git-的使用"><a href="#一-IDEA-中-git-的使用" class="headerlink" title="一. IDEA 中 git 的使用"></a>一. IDEA 中 git 的使用</h1><h4 id="1-电脑上安装-Git-工具"><a href="#1-电脑上安装-Git-工具" class="headerlink" title="1.电脑上安装 Git 工具"></a>1.电脑上安装 Git 工具</h4><p><a href="https://git-scm.com/downloads/" target="_blank" rel="noopener">https://git-scm.com/downloads/</a></p><h4 id="2-idea-中安装-getee-码云-插件"><a href="#2-idea-中安装-getee-码云-插件" class="headerlink" title="2.idea 中安装 getee(码云) 插件"></a>2.idea 中安装 getee(码云) 插件</h4><h4 id="3-git-的一些操作命令"><a href="#3-git-的一些操作命令" class="headerlink" title="3.git 的一些操作命令"></a>3.git 的一些操作命令</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检出分支, 后面是分支名</span></span><br><span class="line">git branch iss53  <span class="comment">#创建分支</span></span><br><span class="line">git checkout -b iss9527  <span class="comment">#创建并切换  -b &lt;branch&gt;   create and checkout a new branch</span></span><br><span class="line">git checkout iss9527    <span class="comment"># 切换分支</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分支合并</span></span><br><span class="line">➜  dmp git:(master) git merge iss9527</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除分支 (在 bug, 功能完成后, 并且合并完成后, 可以删除分支)</span></span><br><span class="line">➜  dmp git:(master) git branch -d iss9527</span><br><span class="line">Deleted branch iss9527 (was 240ba41).</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一-IDEA-中-git-的使用&quot;&gt;&lt;a href=&quot;#一-IDEA-中-git-的使用&quot; class=&quot;headerlink&quot; title=&quot;一. IDEA 中 git 的使用&quot;&gt;&lt;/a&gt;一. IDEA 中 git 的使用&lt;/h1&gt;&lt;h4 id=&quot;1-电脑上安装
      
    
    </summary>
    
      <category term="工具" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Git" scheme="https://airpoet.github.io/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>Maven相关</title>
    <link href="https://airpoet.github.io/2018/09/04/Maven/Maven%E7%9B%B8%E5%85%B3/"/>
    <id>https://airpoet.github.io/2018/09/04/Maven/Maven相关/</id>
    <published>2018-09-04T06:34:50.174Z</published>
    <updated>2018-09-04T06:38:55.129Z</updated>
    
    <content type="html"><![CDATA[<p><strong>1.如果在部署环境中,  已经存在某 jar 包,  因而项目在打包时, 不需要打此包时, 在 pom 最下面加上如下 scope 即可</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">ps: scope 的分类</span><br><span class="line"></span><br><span class="line">1.compile：默认值 他表示被依赖项目需要参与当前项目的编译，还有后续的测试，运行周期也参与其中，是一个比较强的依赖。打包的时候通常需要包含进去</span><br><span class="line"></span><br><span class="line">2.test：依赖项目仅仅参与测试相关的工作，包括测试代码的编译和执行，不会被打包，例如：junit</span><br><span class="line"></span><br><span class="line">3.runtime：表示被依赖项目无需参与项目的编译，不过后期的测试和运行周期需要其参与。与compile相比，跳过了编译而已。例如JDBC驱动，适用运行和测试阶段</span><br><span class="line"></span><br><span class="line">4.provided：打包的时候可以不用包进去，别的设施会提供。事实上该依赖理论上可以参与编译，测试，运行等周期。相当于compile，但是打包阶段做了exclude操作</span><br><span class="line"></span><br><span class="line">5.system：从参与度来说，和provided相同，不过被依赖项不会从maven仓库下载，而是从本地文件系统拿。需要添加systemPath的属性来定义路径</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;1.如果在部署环境中,  已经存在某 jar 包,  因而项目在打包时, 不需要打此包时, 在 pom 最下面加上如下 scope 即可&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight xml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td cl
      
    
    </summary>
    
      <category term="工具" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="Maven" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/Maven/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Maven" scheme="https://airpoet.github.io/tags/Maven/"/>
    
  </entry>
  
  <entry>
    <title>搭建HDP</title>
    <link href="https://airpoet.github.io/2018/09/01/%E7%8E%AF%E5%A2%83/%E6%90%AD%E5%BB%BAHDP/"/>
    <id>https://airpoet.github.io/2018/09/01/环境/搭建HDP/</id>
    <published>2018-09-01T05:59:10.757Z</published>
    <updated>2018-09-03T04:38:09.021Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-01-070744.png" alt="image-20180901150744204"></p><h3 id="安装时间同步服务"><a href="#安装时间同步服务" class="headerlink" title="安装时间同步服务"></a><strong>安装时间同步服务</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先安装阿里云 yum 源</span></span><br><span class="line">mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup</span><br><span class="line">curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo</span><br><span class="line">yum makecache</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装时间同步服务器</span></span><br><span class="line">yum install -y ntp</span><br><span class="line">service ntpd start <span class="comment">#开启服务</span></span><br><span class="line">chkconfig ntpd on  <span class="comment">#配置开机启动</span></span><br></pre></td></tr></table></figure><h3 id="关闭-THP"><a href="#关闭-THP" class="headerlink" title="关闭 THP"></a><strong>关闭 THP</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 关闭 selinux</span></span><br><span class="line"><span class="comment">## 查看是否开启</span></span><br><span class="line">sestatus</span><br><span class="line">vi /etc/sysconfig/selinux </span><br><span class="line">SELINUX=disabled</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果出现下述结果说明启动了 THP</span></span><br><span class="line">$ cat /sys/kernel/mm/transparent_hugepage/enabled </span><br><span class="line">[always] madvise never</span><br><span class="line"></span><br><span class="line">$ cat /sys/kernel/mm/transparent_hugepage/defrag </span><br><span class="line">[always] madvise never</span><br><span class="line"></span><br><span class="line"><span class="comment"># 永久关闭THP</span></span><br><span class="line">$ vi /etc/rc.d/rc.local </span><br><span class="line"><span class="comment">#添加</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">test</span> -f /sys/kernel/mm/transparent_hugepage/enabled; <span class="keyword">then</span></span><br><span class="line">            <span class="built_in">echo</span> never &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">test</span> -f /sys/kernel/mm/transparent_hugepage/defrag; <span class="keyword">then</span></span><br><span class="line">            <span class="built_in">echo</span> never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment">#重启</span></span><br><span class="line">reboot</span><br><span class="line"><span class="comment"># 再检查, 发现 never 上加了 []</span></span><br><span class="line">$ cat /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line">always madvise [never]</span><br><span class="line"></span><br><span class="line">$ cat /sys/kernel/mm/transparent_hugepage/defrag </span><br><span class="line">always madvise [never]</span><br></pre></td></tr></table></figure><h3 id="克隆机器"><a href="#克隆机器" class="headerlink" title="克隆机器"></a><strong>克隆机器</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 克隆前删一个文件</span></span><br><span class="line">$ rm /etc/udev/rules.d/70-persistent-net.rules</span><br></pre></td></tr></table></figure><h3 id="在-00-上安装-mysql"><a href="#在-00-上安装-mysql" class="headerlink" title="在 00 上安装 mysql"></a><strong>在 00 上安装 mysql</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">[root@nd-00 ~]<span class="comment"># yum list mysql-server  ## 如果有一项</span></span><br><span class="line">[root@nd-00 ~]<span class="comment"># yum install -y mysql-server    </span></span><br><span class="line">[root@nd-00 ~]<span class="comment"># service mysqld start   ## 启动 mysql, 根据提示设置密码</span></span><br><span class="line">[root@nd-00 ~]<span class="comment"># /usr/bin/mysqladmin -u root password 'root123'     ## 注意:一定不要把密码设置成纯数字,会有 bug</span></span><br><span class="line">[root@nd-00 ~]<span class="comment"># chkconfig mysqld on    ##设置 mysql 开机启动</span></span><br><span class="line"><span class="comment"># 修改Mysql配置文件</span></span><br><span class="line">[root@nd-00 ~]<span class="comment"># vi /etc/my.cnf </span></span><br><span class="line">    -------------------&gt;&gt;&gt;括号内是添加的</span><br><span class="line">    [mysqld]</span><br><span class="line">    datadir=/var/lib/mysql</span><br><span class="line">    socket=/var/lib/mysql/mysql.sock</span><br><span class="line">    user=mysql</span><br><span class="line">    <span class="comment"># Disabling symbolic-links is recommended to prevent assorted security risks</span></span><br><span class="line">    symbolic-links=0</span><br><span class="line">    &#123;------------------------------|</span><br><span class="line">    <span class="comment"># 设置数据库默认编码  </span></span><br><span class="line">    collation_server=utf8_general_ci</span><br><span class="line">    character_set_server=utf8</span><br><span class="line">    <span class="comment"># 设置默认存储引擎</span></span><br><span class="line">    default-storage-engine=INNODB</span><br><span class="line">    ------------------------------|&#125;</span><br><span class="line"></span><br><span class="line">    [mysqld_safe]</span><br><span class="line">    <span class="built_in">log</span>-error=/var/<span class="built_in">log</span>/mysqld.log</span><br><span class="line">    pid-file=/var/run/mysqld/mysqld.pid</span><br><span class="line"></span><br><span class="line">    &#123;------------------------------|</span><br><span class="line">    [client]</span><br><span class="line">    <span class="comment"># 设置客户端默认编码集</span></span><br><span class="line">    default-character-set=utf8</span><br><span class="line">    -------------------------------|&#125;</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 重启 mysql 服务</span></span><br><span class="line">[root@nd-00 ~]<span class="comment"># service mysqld restart</span></span><br><span class="line"><span class="comment"># 登录</span></span><br><span class="line">[root@nd-00 ~]<span class="comment"># mysql -uroot -proot123 </span></span><br><span class="line"><span class="comment"># 查看字符集</span></span><br><span class="line">mysql&gt; show variables like <span class="string">'character_set_%'</span>;</span><br><span class="line">+--------------------------+----------------------------+</span><br><span class="line">| Variable_name            | Value                      |</span><br><span class="line">+--------------------------+----------------------------+</span><br><span class="line">| character_set_client     | utf8                       |</span><br><span class="line">| character_set_connection | utf8                       |</span><br><span class="line">| character_set_database   | utf8                       |</span><br><span class="line">| character_set_filesystem | binary                     |</span><br><span class="line">| character_set_results    | utf8                       |</span><br><span class="line">| character_set_server     | utf8                       |</span><br><span class="line">| character_set_system     | utf8                       |</span><br><span class="line">| character_sets_dir       | /usr/share/mysql/charsets/ |</span><br><span class="line">+--------------------------+----------------------------+</span><br><span class="line">8 rows <span class="keyword">in</span> <span class="built_in">set</span> (0.00 sec)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在数据库创建相应的用户和 DB</span></span><br><span class="line"></span><br><span class="line"><span class="comment">######创建 ambari数据库及数据库的用户名和密码</span></span><br><span class="line">mysql&gt; create database ambari character <span class="built_in">set</span> utf8;     <span class="comment">#创建库</span></span><br><span class="line">mysql&gt; CREATE USER <span class="string">'ambari'</span>@<span class="string">'%'</span>IDENTIFIED BY <span class="string">'Ambari123'</span>;  <span class="comment">#创建用户</span></span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON ambari.* TO <span class="string">'ambari'</span>@<span class="string">'%'</span>;   <span class="comment">#授权</span></span><br><span class="line">mysql&gt; FLUSH PRIVILEGES;<span class="comment">#刷新权限</span></span><br><span class="line"></span><br><span class="line"><span class="comment">######创建 hive数据库及数据库的用户名和密码</span></span><br><span class="line">mysql&gt; create database hive character <span class="built_in">set</span> utf8;     <span class="comment">#创建库</span></span><br><span class="line">mysql&gt; CREATE USER <span class="string">'hive'</span>@<span class="string">'%'</span>IDENTIFIED BY <span class="string">'Hive123'</span>;  <span class="comment">#创建用户</span></span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON hive.* TO <span class="string">'hive'</span>@<span class="string">'%'</span>;   <span class="comment">#授权</span></span><br><span class="line">mysql&gt; FLUSH PRIVILEGES;<span class="comment">#刷新权限</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#######下载 mysql-connection-java</span></span><br><span class="line">[root@nd-00 ~]<span class="comment"># yum install -y mysql-connector-java</span></span><br><span class="line"><span class="comment"># 查看是否有mysql-connection-java</span></span><br><span class="line">[root@nd-00 ~]<span class="comment"># ls /usr/share/java</span></span><br></pre></td></tr></table></figure><h3 id="配置免密登录"><a href="#配置免密登录" class="headerlink" title="配置免密登录"></a>配置免密登录</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 所有机器都安装一个openssh 工具</span></span><br><span class="line">$ yum install -y openssh-clients</span><br><span class="line"></span><br><span class="line">[[[nd-00.hdp -&gt; 00.01.02.03.04]]]</span><br><span class="line">--------------</span><br><span class="line">[root@nd-00 ~]<span class="comment"># ssh-keygen -t rsa#然后一路回车</span></span><br><span class="line">[root@nd-00 ~]<span class="comment"># ssh-copy-id nd-00.hdp</span></span><br><span class="line">[root@nd-00 ~]<span class="comment"># ssh-copy-id nd-01.hdp</span></span><br><span class="line">[root@nd-00 ~]<span class="comment"># ssh-copy-id nd-02.hdp</span></span><br><span class="line">[root@nd-00 ~]<span class="comment"># ssh-copy-id nd-03.hdp</span></span><br><span class="line">[root@nd-00 ~]<span class="comment"># ssh-copy-id nd-04.hdp</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[[[nd-01.hdp -&gt; 01.02.03.04]]]</span><br><span class="line">---------------</span><br></pre></td></tr></table></figure><h3 id="在-yum-hdp-上安装-yum-相关工具"><a href="#在-yum-hdp-上安装-yum-相关工具" class="headerlink" title="在 yum.hdp 上安装 yum 相关工具"></a>在 yum.hdp 上安装 yum 相关工具</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum install yum-utils -y</span><br><span class="line">yum repolist</span><br><span class="line">yum install createrepo -y</span><br></pre></td></tr></table></figure><h3 id="2-5-安装-Apache-httpd"><a href="#2-5-安装-Apache-httpd" class="headerlink" title="2.5 安装 Apache httpd"></a>2.5 安装 Apache httpd</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 1&gt;安装 apache 的 httpd 服务</span></span><br><span class="line">[root@yum html]<span class="comment"># yum install httpd -y</span></span><br><span class="line"></span><br><span class="line">安装完httpd 后,会生成 /var/www/html 目录 (相当于 tomcat 的 webapp 目录).</span><br><span class="line">进入到 /var/www/html 目录下, 创建 ambari 和 hdp 目录, 用来存放安装文件.</span><br><span class="line"></span><br><span class="line">[root@yum ~]<span class="comment"># cd /var/www/html/</span></span><br><span class="line">[root@yum html]<span class="comment"># ls</span></span><br><span class="line">[root@yum html]<span class="comment"># mkdir /var/www/html/ambari</span></span><br><span class="line">[root@yum html]<span class="comment"># mkdir /var/www/html/hdp</span></span><br><span class="line">[root@yum html]<span class="comment"># mkdir /var/www/html/hdp/HDP-UTILS-1.1.0.21</span></span><br><span class="line">&lt;sub&gt;&lt;/sub&gt;&lt;sub&gt;&lt;/sub&gt;&lt;sub&gt;&lt;/sub&gt;&lt;sub&gt;&lt;/sub&gt;&lt;sub&gt;&lt;/sub&gt;&lt;sub&gt;&lt;/sub&gt;</span><br><span class="line"><span class="comment">#上传ambari 和 hdp.. </span></span><br><span class="line">&lt;sub&gt;&lt;/sub&gt;&lt;sub&gt;&lt;/sub&gt;&lt;sub&gt;&lt;/sub&gt;&lt;sub&gt;&lt;/sub&gt;&lt;sub&gt;&lt;/sub&gt;&lt;sub&gt;&lt;/sub&gt;</span><br><span class="line"><span class="comment"># 解压到对应目录</span></span><br><span class="line">[root@yum ~]<span class="comment"># tar -zxvf ambari-2.4.1.0-centos6.tar.gz -C /var/www/html/ambari/</span></span><br><span class="line">[root@yum ~]<span class="comment"># tar -zxvf HDP-2.5.0.0-centos6-rpm.tar.gz -C /var/www/html/hdp/ </span></span><br><span class="line">[root@yum ~]<span class="comment"># tar -zxvf HDP-UTILS-1.1.0.21-centos6.tar.gz -C /var/www/html/hdp/HDP-UTILS-1.1.0.21/</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### 2&gt;启动 httpd 服务: service httpd start</span></span><br><span class="line">httpd 默认的端口为80, 启动后再浏览器中输入: http://192.168.170.77</span><br><span class="line"></span><br><span class="line"><span class="comment">### 3&gt;设置 httpd 开机自启</span></span><br><span class="line">$&gt; chkconfig httpd on</span><br></pre></td></tr></table></figure><h3 id="2-6-配置本地-Repo"><a href="#2-6-配置本地-Repo" class="headerlink" title="2.6 配置本地 Repo"></a>2.6 配置本地 Repo</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">1&gt;. 配置 Ambari</span><br><span class="line">====================================================================================</span><br><span class="line">[root@yum hdp]<span class="comment"># yum install -y wget</span></span><br><span class="line">[root@yum hdp]<span class="comment"># wget -O /etc/yum.repos.d/ambari.repo http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.4.1.0/ambari.repo</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 把 repo 库修改为 yum.hdp 上的地址</span></span><br><span class="line">[root@yum yum.repos.d]<span class="comment"># vi ambari.repo.bak </span></span><br><span class="line">-----</span><br><span class="line"><span class="comment">#VERSION_NUMBER=2.4.1.0-22</span></span><br><span class="line">[Updates-ambari-2.4.1.0]</span><br><span class="line">name=ambari-2.4.1.0 - Updates</span><br><span class="line">baseurl=http://192.168.170.77/ambari/AMBARI-2.4.1.0/centos6/2.4.1.0-22/</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=http://192.168.170.77/ambari/AMBARI-2.4.1.0/centos6/2.4.1.0-22/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins</span><br><span class="line">enabled=1</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">2&gt;. 配置 HDP</span><br><span class="line">====================================================================================</span><br><span class="line">[root@yum yum.repos.d]<span class="comment"># vi HDP.repo </span></span><br><span class="line">------</span><br><span class="line"><span class="comment">#VERSION_NUMBER=2.5.0.0-1245</span></span><br><span class="line">[HDP-2.5.0.0]</span><br><span class="line">name=HDP VERSION - HDP-2.5.0.0</span><br><span class="line">baseurl=http://192.168.170.77/hdp/HDP/centos6/</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=http://192.168.170.77/hdp/HDP/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins</span><br><span class="line">enabled=1</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[HDP-UTILS-1.1.0.21]</span><br><span class="line">name=HDP-UTILS VERSION - HDP-UTILS-1.1.0.21</span><br><span class="line">baseurl=http://192.168.170.77/hdp/HDP-UTILS-1.1.0.21/</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=http://192.168.170.77/hdp/HDP-UTILS-1.1.0.21/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins</span><br><span class="line">enabled=1</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">3&gt;. 分发 Ambari.repo &amp; HDP.repo</span><br><span class="line">====================================================================================</span><br><span class="line">[root@yum yum.repos.d]<span class="comment"># scp ambari.repo HDP.repo nd-00.hdp:$PWD</span></span><br><span class="line">[root@nd-00 yum.repos.d]<span class="comment"># scp ambari.repo HDP.repo nd-01.hdp:$PWD</span></span><br><span class="line">[root@nd-00 yum.repos.d]<span class="comment"># scp ambari.repo HDP.repo nd-02.hdp:$PWD</span></span><br><span class="line">[root@nd-00 yum.repos.d]<span class="comment"># scp ambari.repo HDP.repo nd-03.hdp:$PWD</span></span><br><span class="line">[root@nd-00 yum.repos.d]<span class="comment"># scp ambari.repo HDP.repo nd-04.hdp:$PWD</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">4&gt;. 生成本地源</span><br><span class="line">====================================================================================</span><br><span class="line">使用 createrepo 命令, 生成本地源</span><br><span class="line">(createrepo 用以创建 yum 源(软件仓库), 即为存放于本地特定位置的众多 rpm 包建立索引,描述各个包所需依赖信息, 并形成元数据)</span><br><span class="line"></span><br><span class="line">[root@yum yum.repos.d]<span class="comment"># createrepo /var/www/html/hdp/HDP/centos6/</span></span><br><span class="line">[root@yum yum.repos.d]<span class="comment"># createrepo /var/www/html/hdp/HDP-UTILS-1.1.0.21/</span></span><br></pre></td></tr></table></figure><h3 id="3-安装-Ambari-Server"><a href="#3-安装-Ambari-Server" class="headerlink" title="3. 安装 Ambari-Server"></a>3. 安装 Ambari-Server</h3><h4 id="3-1-nd-00-hdp-节点安装"><a href="#3-1-nd-00-hdp-节点安装" class="headerlink" title="3.1. nd-00.hdp 节点安装"></a>3.1. nd-00.hdp 节点安装</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$&gt; yum install ambari-server</span><br><span class="line">$&gt; ambari-server setup <span class="comment"># 设置初始化参数</span></span><br><span class="line"></span><br><span class="line">​````````````````````````````````````````````````````````````</span><br></pre></td></tr></table></figure><p>[root@nd-00 yum.repos.d]# ambari-server setup<br>Using python  /usr/bin/python<br>Setup ambari-server<br>Checking SELinux…<br>SELinux status is ‘disabled’<br>Customize user account for ambari-server daemon [y/n] (n)? y<br>Enter user account for ambari-server daemon (root):root<br>Adjusting ambari-server permissions and ownership…<br>Checking firewall status…<br>Checking JDK…<br>[1] Oracle JDK 1.8 + Java Cryptography Extension (JCE) Policy Files 8<br>[2] Oracle JDK 1.7 + Java Cryptography Extension (JCE) Policy Files 7</p><h1 id="3-Custom-JDK"><a href="#3-Custom-JDK" class="headerlink" title="[3] Custom JDK"></a>[3] Custom JDK</h1><p>Enter choice (1): 3<br>WARNING: JDK must be installed on all hosts and JAVA_HOME must be valid on all hosts.<br>WARNING: JCE Policy files are required for configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Strength Jurisdiction Policy Files are valid on all hosts.<br>Path to JAVA_HOME: /usr/java/jdk1.8.0_181/<br>Validating JDK on Ambari Server…done.<br>Completing setup…<br>Configuring database…<br>Enter advanced database configuration [y/n] (n)? Y<sup>Hy</sup>H<sup>H</sup>H^H<br>input not recognized, please try again:<br>Enter advanced database configuration [y/n] (n)? y</p><h1 id="Configuring-database…"><a href="#Configuring-database…" class="headerlink" title="Configuring database…"></a>Configuring database…</h1><p>Choose one of the following options:<br>[1] - PostgreSQL (Embedded)<br>[2] - Oracle<br>[3] - MySQL / MariaDB<br>[4] - PostgreSQL<br>[5] - Microsoft SQL Server (Tech Preview)<br>[6] - SQL Anywhere</p><h1 id="7-BDB"><a href="#7-BDB" class="headerlink" title="[7] - BDB"></a>[7] - BDB</h1><p>Enter choice (1): 3<br>Hostname (localhost): nd-00<br>Port (3306):<br>Database name (ambari):<br>Username (ambari):<br>Enter Database Password (bigdata):<br>Re-enter password:<br>Configuring ambari database…<br>Copying JDBC drivers to server resources…<br>Configuring remote database connection properties…<br>WARNING: Before starting Ambari Server, you must run the following DDL against the database to create the schema: /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql<br>Proceed with configuring remote database connection properties [y/n] (y)? y<br>Extracting system views…<br>…..ambari-admin-2.4.1.0.22.jar<br>……<br>Adjusting ambari-server permissions and ownership…<br>Ambari Server ‘setup’ completed successfully.<br>​<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"># 记得要执行这个脚本: /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql</span><br><span class="line"></span><br><span class="line">## 首先允许远程登录mysql</span><br><span class="line">mysql&gt; mysql -uroot -proot123</span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON ambari.* TO &apos;ambari&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;Ambari123&apos;;</span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON ambari.* TO &apos;ambari&apos;@&apos;%&apos; IDENTIFIED BY &apos;Ambari123&apos;;</span><br><span class="line">mysql&gt; FLUSH PRIVILEGES;#刷新权限</span><br><span class="line"></span><br><span class="line">## 使用 ambari 用户登录 mysql</span><br><span class="line">[root@nd-00 yum.repos.d]# mysql -u ambari -hnd-00 -pAmbari123</span><br><span class="line">mysql&gt; use ambari</span><br><span class="line">mysql&gt; source /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql</span><br><span class="line">mysql&gt; exit</span><br><span class="line"></span><br><span class="line">## 重启 mysql</span><br><span class="line">[root@nd-00 yum.repos.d]# service mysqld restart</span><br><span class="line"></span><br><span class="line">​</span><br></pre></td></tr></table></figure></p><h3 id="启动ambari-server"><a href="#启动ambari-server" class="headerlink" title="启动ambari-server"></a>启动ambari-server</h3><p>$ ambari-server start<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### 3.2 安装 Agent</span><br><span class="line"></span><br><span class="line">**nd-00~04.hdp 所有节点安装 ambari-agent**</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">[all]# yum install -y ambari-agent     #安装</span><br><span class="line">[all]# service ambari-agent start     #启动</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 安装后可以直接访问: </span><br><span class="line">http://192.168.170.100:8080</span><br><span class="line">username:admin</span><br><span class="line">password:admin</span><br><span class="line"></span><br><span class="line"># nd0-4 开启 ntpd 服务</span><br><span class="line">service ntpd start</span><br></pre></td></tr></table></figure></p><h4 id="3-3-登录-ambari-server-之后的相关设置"><a href="#3-3-登录-ambari-server-之后的相关设置" class="headerlink" title="3.3 登录 ambari-server 之后的相关设置"></a>3.3 登录 ambari-server 之后的相关设置</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-02-055132.png" alt="image-20180902135132283"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-02-055329.png" alt="image-20180902135328906"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-02-060728.png" alt="image-20180902140728441"></p><h4 id="3-5-超级大-bug-ssl-问题"><a href="#3-5-超级大-bug-ssl-问题" class="headerlink" title="3.5 超级大 bug:  ssl 问题"></a>3.5 超级大 bug:  ssl 问题</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(<span class="string">'WARNING 2018-09-02 04:17:52,413 NetUtil.py:116 - Server at https://nd-00:8440 is not reachable, sleeping for 10 seconds...</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">ERROR 2018-09-02 04:17:53,793 NetUtil.py:88 - [Errno 8] _ssl.c:492: EOF occurred in violation of protocol</span></span><br><span class="line"><span class="string">ERROR 2018-09-02 04:17:53,794 NetUtil.py:89 - SSLError: Failed to connect. Please check openssl library versions. </span></span><br><span class="line"><span class="string">Refer to: https://bugzilla.redhat.com/show_bug.cgi?id=1022468 for more details.</span></span><br><span class="line"><span class="string">WARNING 2018-09-02 04:17:53,794 NetUtil.py:116 - Server at https://nd-00:8440 is not reachable, sleeping for 10 seconds...</span></span><br><span class="line"><span class="string">'</span>, None)</span><br></pre></td></tr></table></figure><p>参考: <a href="https://my.oschina.net/haha256/blog/1836261" target="_blank" rel="noopener">https://my.oschina.net/haha256/blog/1836261</a>   (感谢<sub></sub> <strong>浪费了一下午</strong>…..)</p><p><strong>解决办法: 将服务器中, jdk 的安全机制改了</strong> </p><p><code>vim /usr/java/jdk1.8.0_171-amd64/jre/lib/security/java.security</code></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-02-121243.png" alt="image-20180902201242973"></p><p>去掉3DES_EDE_CBC。重启ambari。</p><p>其它要注意的:  </p><ul><li>JAVA_HOME的 path 要写在最前面</li><li>所有 agent 开启 ntpd 服务</li><li>所有主机升级 ssl 为最新的:   yum -y update openssl</li><li>貌似开启了server&gt; <code>ambari-server start</code>, 就会自动开启各个节点的 agent</li></ul><p><strong>总结搜索问题的思路:</strong> </p><ul><li>可以直接搜索报出来的错</li><li>搜索关键词, 比如本错搜索如下2个关键词, google 第一页就可以找到解决办法!!!!<ul><li><code>amabri  openssl</code></li></ul></li></ul><p><strong>安装备注</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">nd-00.hdp</span><br><span class="line">nd-01.hdp</span><br><span class="line">nd-02.hdp</span><br><span class="line">nd-03.hdp</span><br><span class="line">nd-04.hdp</span><br><span class="line"></span><br><span class="line">http://yum/hdp/HDP/centos6/</span><br><span class="line">http://yum/hdp/HDP-UTILS-1.1.0.21/</span><br></pre></td></tr></table></figure><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-03-033533.png" alt="image-20180903113532918"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-03-033955.png" alt="image-20180903113954359"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-03-034010.png" alt="image-20180903114010177"></p><blockquote><p>解决这些问题</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@nd-00 ~]<span class="comment"># ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar</span></span><br></pre></td></tr></table></figure><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-03-035036.png" alt="image-20180903115035724"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-03-035850.png" alt="image-20180903115849862"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-03-040026.png" alt="image-20180903120025912"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-03-040230.png" alt="image-20180903120230116"></p><h3 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h3><p><strong>Admin Name</strong> : admin</p><p><strong>Cluster Name</strong> : mycluster</p><p><strong>Total Hosts</strong> : 5 (5 new)</p><p><strong>Repositories</strong>:</p><ul><li>redhat6 (HDP-2.5):<br><a href="http://yum/hdp/HDP/centos6/" target="_blank" rel="noopener">http://yum/hdp/HDP/centos6/</a></li><li>redhat6 (HDP-UTILS-1.1.0.21):<br><a href="http://yum/hdp/HDP-UTILS-1.1.0.21/" target="_blank" rel="noopener">http://yum/hdp/HDP-UTILS-1.1.0.21/</a></li></ul><p><strong>Services:</strong></p><ul><li><strong>HDFS</strong><ul><li>DataNode : 4 hosts</li><li>NameNode : nd-00.hdp</li><li>NFSGateway : 0 host</li><li>SNameNode : nd-01.hdp</li></ul></li><li><strong>YARN + MapReduce2</strong><ul><li>App Timeline Server : nd-01.hdp</li><li>NodeManager : 4 hosts</li><li>ResourceManager : nd-01.hdp</li></ul></li><li><strong>Tez</strong><ul><li>Clients : 3 hosts</li></ul></li><li><strong>Hive</strong><ul><li>Metastore : nd-01.hdp</li><li>HiveServer2 : nd-01.hdp</li><li>WebHCat Server : nd-01.hdp</li><li>Database : Existing MySQL / MariaDB Database</li></ul></li><li><strong>HBase</strong><ul><li>Master : nd-00.hdp</li><li>RegionServer : 4 hosts</li><li>Phoenix Query Server : 0 host</li></ul></li><li><strong>Pig</strong><ul><li>Clients : 3 hosts</li></ul></li><li><strong>ZooKeeper</strong><ul><li>Server : 3 hosts</li></ul></li><li><strong>Flume</strong><ul><li>Flume : 4 hosts</li></ul></li><li><strong>Ambari Infra</strong><ul><li>Infra Solr Instance : nd-00.hdp</li></ul></li><li><strong>Ambari Metrics</strong><ul><li>Metrics Collector : nd-02.hdp</li><li>Grafana : nd-00.hdp</li></ul></li><li><strong>Kafka</strong><ul><li>Broker : nd-00.hdp</li></ul></li><li><strong>SmartSense</strong><ul><li>Activity Analyzer : nd-00.hdp</li><li>Activity Explorer : nd-00.hdp</li><li>HST Server : nd-00.hdp</li></ul></li><li><strong>Spark</strong><ul><li>Livy Server : 0 host</li><li>History Server : nd-00.hdp</li><li>Thrift Server : 0 host</li></ul></li><li><strong>Slider</strong><ul><li>Clients : 3 hosts</li></ul></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-03-040706.png" alt="image-20180903120705901"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-01-070744.png&quot; alt=&quot;image-20180901150744204&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;安装时间同步服务&quot;&gt;&lt;a href=&quot;#安装
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>广告项目</title>
    <link href="https://airpoet.github.io/2018/08/30/Project/%E5%B9%BF%E5%91%8A/"/>
    <id>https://airpoet.github.io/2018/08/30/Project/广告/</id>
    <published>2018-08-30T03:10:47.159Z</published>
    <updated>2018-10-10T15:44:55.235Z</updated>
    
    <content type="html"><![CDATA[<h1 id="离线项目-DMP"><a href="#离线项目-DMP" class="headerlink" title="离线项目 DMP"></a>离线项目 DMP</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-30-074717.jpg" alt=""></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-30-031053.png" alt="image-20180830111052740"></p><p>专有名词: </p><ul><li>DSP(Demand Side Platform)</li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-30-033716.png" alt="image-20180830113716048"></p><p><strong>DSP 原理图</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-30-064757.jpg" alt=""></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-30-dsp%E5%8E%9F%E7%90%86%E5%9B%BE.png" alt=""></p><p><strong>整体流程图</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-15-%E7%B2%BE%E6%B0%94%E7%A5%9E.png" alt=""></p><p><strong>有相同字段值,  出现顺序不一样的用户使用图计算,  再进行合并</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-15-%E7%94%A8%E6%88%B7%E8%AF%86%E5%88%AB.png" alt=""></p><p><strong>DMP =&gt; 此部分的用户画像是大数据工程师做的</strong></p><p><strong>项目整体业务流程图</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-30-070149.png" alt="image-20180830150149616"></p><h3 id="项目技术点"><a href="#项目技术点" class="headerlink" title="项目技术点"></a>项目技术点</h3><ul><li>主要做离线这块, 这个项目实时的也知道</li><li>Flume 自定义 source</li></ul><p>字段 :</p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-30-075227.png" alt="image-20180830155227548"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-30-075645.png" alt="image-20180830155644911"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-30-075732.png" alt="image-20180830155732250"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-30-075858.png" alt="image-20180830155858280"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-30-075825.png" alt="image-20180830155825290"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-30-075936.png" alt="image-20180830155936444"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-30-080033.png" alt="image-20180830160032521"> </p><p><strong>计算逻辑</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-31-085937.png" alt="image-20180831165936780"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-31-090122.png" alt="image-20180831170122105"></p><p>问题: </p><ul><li>有 Spark 的 jvm调优经验吗?</li><li>有 shuffle 就一定慢吗</li></ul><p>Spark 任务涉及到的调优: </p><ul><li><p>序列化— Java 原生的序列化传输比较慢, 使用<code>KryoSerializer</code></p></li><li><p>垃圾回收时间( gc )长,  基本上都是在1s 内,   用了  gsm 的配置方式 : TODO, 效果不是很明显, 因为集群的资源还是比较丰富的</p></li><li><p>–num-executors </p><ul><li><p>本机总内存32G</p></li><li><p>默认一个 block 对应一个 executors</p></li><li><p>可以 repartition, 之后的分区数就是这个  executors 的数量, 一个分区对应一个 executors</p></li><li><p>一般分区数量为集群总核数的2-3倍</p></li><li><p>如果总虚拟内存 32G,  总核数 32核的话,    可以分区为 60,   4台节点的话, 每台的 <code>num-executors</code>为15个</p></li><li><p>但是还要看总的内存数量,  总内存数为  (executor-memory +1) * num-executors </p></li><li><p>因此 32 / 3 = 10, 所以设置 <code>num-executors</code>为10个,  32个核减去一个系统的占用,  一个 executor 得到2个核数</p></li><li><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">nohup spark-submit --class com.rox.dmp.tools.Bzip2Parquet  \</span><br><span class="line">    --master yarn \</span><br><span class="line">    --deploy-mode cluster \</span><br><span class="line">    --driver-memory 1g \</span><br><span class="line">    --executor-memory 2g \</span><br><span class="line">    --executor-cores 2 \</span><br><span class="line">    --num-executors 15 \</span><br><span class="line">    dmp-1.0-SNAPSHOT.jar \</span><br><span class="line">    /adlogs/biz2/mycus snappy 60 /adlogs/out/test56 &amp;  </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment">#############################</span></span><br><span class="line"></span><br><span class="line">nohup spark-submit --class com.rox.dmp.tools.Bzip2Parquet  \</span><br><span class="line">    --master yarn \</span><br><span class="line">    --deploy-mode cluster \</span><br><span class="line">    --driver-memory 1g \</span><br><span class="line">    --executor-memory 2g \</span><br><span class="line">    --executor-cores 2 \</span><br><span class="line">    --num-executors 8 \</span><br><span class="line">    /home/ap/jars/dmp-1.0-SNAPSHOT.jar \</span><br><span class="line">    /adlogs/biz2/mycus snappy 20 /adlogs/out/test57 &amp;  </span><br><span class="line">    </span><br><span class="line"><span class="comment"># 目前来看这样调整是比较快的    </span></span><br><span class="line">nohup spark-submit --class com.rox.dmp.tools.Bzip2Parquet  \</span><br><span class="line">    --master yarn \</span><br><span class="line">    --deploy-mode cluster \</span><br><span class="line">    --driver-memory 1g \</span><br><span class="line">    --executor-memory 2g \</span><br><span class="line">    --executor-cores 2 \</span><br><span class="line">    --num-executors 8 \</span><br><span class="line">    /home/ap/jars/dmp-1.0-SNAPSHOT.jar \</span><br><span class="line">    /adlogs/biz2/mycus snappy 16 /adlogs/out/test58 &amp;</span><br></pre></td></tr></table></figure></li></ul></li></ul><p><strong>开启 spark 的 history 服务器</strong>:  <code>start-history-server.sh</code></p><p>查看历史数据: </p><p><a href="http://cs1:18080/history/application_1536038023803_0008/1/jobs/" target="_blank" rel="noopener">http://cs1:18080/history/application_1536038023803_0008/1/jobs/</a></p><p>32核是虚的,  4台机器总共才8核</p><h4 id="增加任务的并行度总结"><a href="#增加任务的并行度总结" class="headerlink" title="增加任务的并行度总结"></a>增加任务的并行度总结</h4><ul><li>executor-memory:  <ul><li>大小和 num-executors 有关系, 他们的乘积不能大于集群 总的 内存 大小</li><li>(executor-memory + 1) * num-executors  &lt;= 集群<strong>总内存</strong>容量</li><li>注意: 做乘积的时候, executor-memory 得多加一个 G</li></ul></li><li>executor-cores:<ul><li>executor-cores * num-executors &lt;= 集群中的 <strong>总的核数</strong> 容量</li><li>一个 executor 如果只分配了一个核,  在这个 executor 中的线程数量同一时刻只能有一个 task, 并且是<strong>串行</strong></li><li>如果 executor 分配了 n 核, 在这个 executor 中的 task 是并行的, 并行的最大数量是 n</li></ul></li><li>num-executors<ul><li>申请的总的 executor数量,  <strong>executors</strong> 数量最好和 <strong>分区数量</strong> 成<strong>倍数</strong>关系</li></ul></li><li>partitionNumber <ul><li>Spark 官网建议我们分区的数量最好是机器核数的2-3倍</li></ul></li></ul><p><strong>其它的参考 spark 官网的调优指南</strong></p><p><a href="http://cwiki.apachecn.org/pages/viewpage.action?pageId=2887785#id-%E4%BC%98%E5%8C%96%E6%8C%87%E5%8D%97-%E6%95%B0%E6%8D%AE%E5%BA%8F%E5%88%97%E5%8C%96" target="_blank" rel="noopener">http://cwiki.apachecn.org/pages/viewpage.action?pageId=2887785#id-%E4%BC%98%E5%8C%96%E6%8C%87%E5%8D%97-%E6%95%B0%E6%8D%AE%E5%BA%8F%E5%88%97%E5%8C%96</a></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-04-075913.png" alt="image-20180904155913648"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-04-082628.png" alt="image-20180904162628395"></p><h4 id="使用-RDD-计算地域分布"><a href="#使用-RDD-计算地域分布" class="headerlink" title="使用 RDD 计算地域分布"></a><strong>使用 RDD 计算地域分布</strong></h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-05-065259.png" alt="image-20180905145259200"></p><h4 id="Graphx-共同好友示例"><a href="#Graphx-共同好友示例" class="headerlink" title="Graphx 共同好友示例"></a>Graphx 共同好友示例</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-09-083531.png" alt="image-20180909163531088"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-09-093227.png" alt="image-20180909173227079"></p><p>使用图计算, 合并相同的用户</p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-15-094824.png" alt="image-20180915174823257"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-09-140031.png" alt="image-20180909220030784"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-09-140211.png" alt="image-20180909220211357"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-09-140934.png" alt="image-20180909220934028"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-09-145049.png" alt="image-20180909225049363"> </p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-11-125644.png" alt="image-20180911205644312"></p><h1 id="实时项目"><a href="#实时项目" class="headerlink" title="实时项目"></a>实时项目</h1><h4 id="中国移动充值业务项目"><a href="#中国移动充值业务项目" class="headerlink" title="中国移动充值业务项目"></a>中国移动充值业务项目</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-11-153904.png" alt="image-20180911233904256"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-11-153951.png" alt="image-20180911233950976"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-11-172745.png" alt="image-20180912012744577"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-11-172913.png" alt="image-20180912012912852"></p><p> <img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-12-093648.png" alt="image-20180912173647719"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-14-081922.png" alt="image-20180914161921513"></p><p><strong>kafka偏移量的计算</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-14-024930.png" alt="image-20180914104930208"></p><p><strong>redis 的字段</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-14-024922.png" alt="image-20180914104921828"></p><p> <strong>介绍 DMP 项目</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-14-092325.png" alt="image-20180914172325118"></p><ul><li>DMP精准广告推送</li><li>两大块  <ul><li>报表<ul><li>维度, 指标, 意义..</li><li>技术结构<ul><li>spark-sql, spark-core</li></ul></li><li>技术亮点<ul><li>udf</li><li>parquet 文件</li><li>其它的优化–并行度,,….等</li><li>存到 mysql集群</li></ul></li><li>报表做出来后, 给j2ee组的人, 做查询和可视化</li></ul></li><li>用户画像<ul><li>标签体系, 知识库</li><li>原始日志拿过来之后, 与知识库进行匹配, 如果匹配上了, 给此人打上相应的标签</li><li>紧接着举个例子: 每个标签有自己唯一的编码, 类似于A00001, 这是一个大类, 大类下面又分为 00001, 这是一个子类,  后面会记录一个1,  这个数字是累加的,  数字越大, 就代表这个标签权重在这个人身上越大, 比如一个人比较爱看武侠, 权重是10,  一个人武侠是0, 言情是10</li><li>当时做一个商圈标签, 当时日志中只有经纬度坐标嘛,  当时就用了百度的 拟地理位置编码 服务,   结合每一次请求的经纬度, 会生成 sn签名, 然后再携带这个 sn 签名, 还有自己的 ak,sk,   然后通过 HttpClient 发送请求,  百度返回一个 json 字符串, 然后解析其中的 business 节点, 这个节点就是 纬度 和 经度 对应的 商圈信息;  把所有的日志中的经纬度拿出来,  把经纬度转成 geoHash 编码, 作为 key,  然后用经纬度请求百度接口拿到的 json字符串中的business 字段作为 value, 存到 redis,   这样会形成自己的商圈知识库,  然后基于此库, 打上相应的商圈标签 </li><li>如果某个经纬度查询不到商圈信息,  也就是说从百度的 api 中, 这个地点附近没有商圈,  就会用7位的 geoHash 的 key 做模糊查询,  原本8位经度是 19米, 7位是 76米, 7位直接在数据库中使用 like 进行模糊匹配就行了 keys xxxxxxx*</li><li>geoHash 编码:  简单来讲吗, 首先把纬度经度分别编码成2进制, 然后组合成新串, 再使用 0-9, b-z(去掉 a, i, l o),  进行 base32编码, 得到的一组字符串; 这个字符串有什么特征呢,  如果把2个经纬度坐标进行编码后, 得到的字符串前8位都一样的画,  那他们的距离就在19米之内,  如果是9位一样的话, 就是在2米之内, 7位一样的话, 就是在 73米之内</li><li>说难点的时候,  要说:  我觉得最有意思的是…</li><li>再比如: 问一个其它的: 比如男女标签, 首先是有这个字段的,  如果字段没有值的时候,  会根据使用的某类 app, 或者某些关键属性来打,  我们公司有这些知识库</li><li>还有一个标签衰减: 因为人的兴趣是随着时间变化而变化的嘛, 比如一段时间喜欢什么,  另一端时间又不喜欢了,  具体做法是: 把昨天的标签系数 * 一个权重值 + 今天的系数</li><li>标签数据存到 hbase 中,  rowkey 采用用户的 imei 号, 或者 mac 地址 , 只会把用户当天的标签存储到 hbase 中,  列族只有一个, 列是以日期命名的, 列以日期为 key, 一天一个列;  问: rowkey 为什么这么设计, 说便于查询把,  规定是要存到 hbase 中, 并且只存1个月, 我觉得存到 hdfs也可以;</li><li>把最终的用户标签存到 ES 中,  ES和 spark 有个整和的 jar 包</li></ul></li></ul></li></ul><p><strong>geoHash 的精确度表</strong></p><ul><li>9的话是2米左右</li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-14-120023.png" alt="image-20180914200023710"></p><p>Hive:  问题, 原理, 优化, 窗口函数(udf, udaf…区别)</p><p>hbase:  原理 (hmaster, hregionserve…),   优化(至少3点…), 问题…</p><p>spark:  源码</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;离线项目-DMP&quot;&gt;&lt;a href=&quot;#离线项目-DMP&quot; class=&quot;headerlink&quot; title=&quot;离线项目 DMP&quot;&gt;&lt;/a&gt;离线项目 DMP&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/stu
      
    
    </summary>
    
      <category term="Project" scheme="https://airpoet.github.io/categories/Project/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Project" scheme="https://airpoet.github.io/tags/Project/"/>
    
  </entry>
  
  <entry>
    <title>MySQL集群</title>
    <link href="https://airpoet.github.io/2018/08/28/MySQL/MySQL%E9%9B%86%E7%BE%A4/"/>
    <id>https://airpoet.github.io/2018/08/28/MySQL/MySQL集群/</id>
    <published>2018-08-28T13:22:47.912Z</published>
    <updated>2018-08-29T03:00:39.275Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-28-135858.png" alt="image-20180828215857532"></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#首先在node-4、node-5、node-6上安装MySQL</span><br><span class="line">#配置MySQL 5.7的yum源</span><br><span class="line"></span><br><span class="line">sudo tee -a /etc/yum.repos.d/mysql-community.repo &lt;&lt; EOF</span><br><span class="line">[mysql57-community]</span><br><span class="line">name=MySQL 5.7 Community Server</span><br><span class="line">baseurl=http://repo.mysql.com/yum/mysql-5.7-community/el/6/\$basearch/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#查看mysql源的信息</span><br><span class="line">yum repolist enabled | grep mysql</span><br><span class="line"></span><br><span class="line">#安装mysql的server</span><br><span class="line">sudo yum install -y mysql-community-server</span><br><span class="line"></span><br><span class="line">#启动mysql</span><br><span class="line">sudo service mysqld start</span><br><span class="line"></span><br><span class="line">#获取启动日志中的默认初始密码</span><br><span class="line">#sudo grep 'temporary password' /var/log/mysqld.log</span><br><span class="line"></span><br><span class="line">#获取密码并赋给一个变量</span><br><span class="line">PASSWORD=`sudo grep 'temporary password' /var/log/mysqld.log | awk '&#123;print $NF&#125;'` </span><br><span class="line"></span><br><span class="line">#使用root用户登录</span><br><span class="line">mysql -uroot -p$PASSWORD</span><br><span class="line"></span><br><span class="line">#修改root用户的密码</span><br><span class="line">ALTER USER 'root'@'localhost' IDENTIFIED BY 'XiaoNiu_123!';</span><br><span class="line"></span><br><span class="line">#修改mysql远程登录权限</span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'XiaoNiu_123!' WITH GRANT OPTION;</span><br><span class="line">FLUSH PRIVILEGES; </span><br><span class="line"></span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line">安装mycat</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">#然后在node-3安装JDK并配置环境变量</span><br><span class="line"></span><br><span class="line">#然后在node-3安装mycat</span><br><span class="line">#上传Mycat-server-1.6.5-release-20171008170112-linux.tar.gz安装包</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#修改conf目录下主要以下三个注意配置文件</span><br><span class="line">server.xml是Mycat服务器参数调整和用户授权的配置文件</span><br><span class="line">schema.xml是逻辑库定义和表以及分片定义的配置文件</span><br><span class="line">rule.xml是分片规则的配置文件</span><br><span class="line"></span><br><span class="line">#修改server.xml(修改了mycat的用户和逻辑的database)</span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">user</span> <span class="attr">name</span>=<span class="string">"xiaoniu"</span> <span class="attr">defaultAccount</span>=<span class="string">"true"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"password"</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"schemas"</span>&gt;</span>bigdata<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">user</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">user</span> <span class="attr">name</span>=<span class="string">"user"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"password"</span>&gt;</span>user<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"schemas"</span>&gt;</span>bigdata<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"readOnly"</span>&gt;</span>true<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">user</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#修改schema.xml（配置逻辑库下的逻辑表，已经数据存放的mysql节点）</span><br><span class="line">    <span class="tag">&lt;<span class="name">schema</span> <span class="attr">name</span>=<span class="string">"bigdata"</span> <span class="attr">checkSQLschema</span>=<span class="string">"false"</span> <span class="attr">sqlMaxLimit</span>=<span class="string">"100"</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- auto sharding by id (long) --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">table</span> <span class="attr">name</span>=<span class="string">"travelrecord"</span> <span class="attr">dataNode</span>=<span class="string">"dn1,dn2,dn3"</span> <span class="attr">rule</span>=<span class="string">"auto-sharding-long"</span> /&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- global table is auto cloned to all defined data nodes ,so can join</span></span><br><span class="line"><span class="comment">            with any table whose sharding node is in the same data node --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">table</span> <span class="attr">name</span>=<span class="string">"company"</span> <span class="attr">primaryKey</span>=<span class="string">"ID"</span> <span class="attr">type</span>=<span class="string">"global"</span> <span class="attr">dataNode</span>=<span class="string">"dn1,dn2,dn3"</span> /&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- random sharding using mod sharind rule --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">table</span> <span class="attr">name</span>=<span class="string">"hotnews"</span> <span class="attr">primaryKey</span>=<span class="string">"ID"</span> <span class="attr">autoIncrement</span>=<span class="string">"true"</span> <span class="attr">dataNode</span>=<span class="string">"dn1,dn2,dn3"</span></span></span><br><span class="line"><span class="tag">               <span class="attr">rule</span>=<span class="string">"mod-long"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">schema</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dataNode</span> <span class="attr">name</span>=<span class="string">"dn1"</span> <span class="attr">dataHost</span>=<span class="string">"cs4"</span> <span class="attr">database</span>=<span class="string">"db1"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dataNode</span> <span class="attr">name</span>=<span class="string">"dn2"</span> <span class="attr">dataHost</span>=<span class="string">"cs5"</span> <span class="attr">database</span>=<span class="string">"db2"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dataNode</span> <span class="attr">name</span>=<span class="string">"dn3"</span> <span class="attr">dataHost</span>=<span class="string">"cs6"</span> <span class="attr">database</span>=<span class="string">"db3"</span> /&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dataHost</span> <span class="attr">name</span>=<span class="string">"cs4"</span> <span class="attr">maxCon</span>=<span class="string">"1000"</span> <span class="attr">minCon</span>=<span class="string">"10"</span> <span class="attr">balance</span>=<span class="string">"0"</span></span></span><br><span class="line"><span class="tag">              <span class="attr">writeType</span>=<span class="string">"0"</span> <span class="attr">dbType</span>=<span class="string">"mysql"</span> <span class="attr">dbDriver</span>=<span class="string">"native"</span> <span class="attr">switchType</span>=<span class="string">"1"</span>  <span class="attr">slaveThreshold</span>=<span class="string">"100"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">heartbeat</span>&gt;</span>select user()<span class="tag">&lt;/<span class="name">heartbeat</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- can have multi write hosts --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">writeHost</span> <span class="attr">host</span>=<span class="string">"hostM1"</span> <span class="attr">url</span>=<span class="string">"192.168.170.134:3306"</span> <span class="attr">user</span>=<span class="string">"root"</span> <span class="attr">password</span>=<span class="string">"XiaoNiu_123!"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">writeHost</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dataHost</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dataHost</span> <span class="attr">name</span>=<span class="string">"cs5"</span> <span class="attr">maxCon</span>=<span class="string">"1000"</span> <span class="attr">minCon</span>=<span class="string">"10"</span> <span class="attr">balance</span>=<span class="string">"0"</span></span></span><br><span class="line"><span class="tag">              <span class="attr">writeType</span>=<span class="string">"0"</span> <span class="attr">dbType</span>=<span class="string">"mysql"</span> <span class="attr">dbDriver</span>=<span class="string">"native"</span> <span class="attr">switchType</span>=<span class="string">"1"</span>  <span class="attr">slaveThreshold</span>=<span class="string">"100"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">heartbeat</span>&gt;</span>select user()<span class="tag">&lt;/<span class="name">heartbeat</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- can have multi write hosts --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">writeHost</span> <span class="attr">host</span>=<span class="string">"hostM1"</span> <span class="attr">url</span>=<span class="string">"192.168.170.135:3306"</span> <span class="attr">user</span>=<span class="string">"root"</span> <span class="attr">password</span>=<span class="string">"XiaoNiu_123!"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">writeHost</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dataHost</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dataHost</span> <span class="attr">name</span>=<span class="string">"cs6"</span> <span class="attr">maxCon</span>=<span class="string">"1000"</span> <span class="attr">minCon</span>=<span class="string">"10"</span> <span class="attr">balance</span>=<span class="string">"0"</span></span></span><br><span class="line"><span class="tag">              <span class="attr">writeType</span>=<span class="string">"0"</span> <span class="attr">dbType</span>=<span class="string">"mysql"</span> <span class="attr">dbDriver</span>=<span class="string">"native"</span> <span class="attr">switchType</span>=<span class="string">"1"</span>  <span class="attr">slaveThreshold</span>=<span class="string">"100"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">heartbeat</span>&gt;</span>select user()<span class="tag">&lt;/<span class="name">heartbeat</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- can have multi write hosts --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">writeHost</span> <span class="attr">host</span>=<span class="string">"hostM1"</span> <span class="attr">url</span>=<span class="string">"192.168.170.136:3306"</span> <span class="attr">user</span>=<span class="string">"root"</span> <span class="attr">password</span>=<span class="string">"XiaoNiu_123!"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">writeHost</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dataHost</span>&gt;</span></span><br><span class="line"></span><br><span class="line">#在三台mysql上分别创建数据库db1、db2、db3</span><br><span class="line">#然后在每一个database中创建表，有三张（travelrecord、company、hotnews）注意主键的名称, 注意不要指定主键自增, 主键自增是 Mycat 维护的</span><br><span class="line"></span><br><span class="line">CREATE TABLE `company` (</span><br><span class="line">`id` BIGINT(20) NOT NULL,</span><br><span class="line">`name` VARCHAR(50),</span><br><span class="line">PRIMARY KEY(`id`)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">CREATE TABLE `hotnews` (</span><br><span class="line">`id` BIGINT(20) NOT NULL,</span><br><span class="line">`name` VARCHAR(50),</span><br><span class="line">PRIMARY KEY(`id`)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">CREATE TABLE `travelrecord` (</span><br><span class="line">`id` BIGINT(20) NOT NULL,</span><br><span class="line">`name` VARCHAR(50),</span><br><span class="line">PRIMARY KEY(`id`)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">#在node-4上启动mycat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># auto-sharding-long 默认一个节点存500万</span><br><span class="line"># </span><br><span class="line"></span><br><span class="line">mysql -h 192.168.10.104 -P 8066 -u root -p123456</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-28-135858.png&quot; alt=&quot;image-20180828215857532&quot;&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight xml&quot;&gt;
      
    
    </summary>
    
      <category term="MySQL" scheme="https://airpoet.github.io/categories/MySQL/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="MySQL" scheme="https://airpoet.github.io/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>单车项目</title>
    <link href="https://airpoet.github.io/2018/08/19/Project/%E5%8D%95%E8%BD%A6%E9%A1%B9%E7%9B%AE/"/>
    <id>https://airpoet.github.io/2018/08/19/Project/单车项目/</id>
    <published>2018-08-19T04:19:08.047Z</published>
    <updated>2018-09-15T10:02:32.508Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一-项目总体架构"><a href="#一-项目总体架构" class="headerlink" title="一. 项目总体架构"></a>一. 项目总体架构</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-20-014957.png" alt="image-20180820094956898"></p><h1 id="二-Nginx-部署-amp-负载均衡"><a href="#二-Nginx-部署-amp-负载均衡" class="headerlink" title="二. Nginx 部署 &amp; 负载均衡"></a>二. Nginx 部署 &amp; 负载均衡</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">使用 root 安装</span><br><span class="line"></span><br><span class="line">1.上传nginx安装包</span><br><span class="line">2.解压nginx</span><br><span class="line">    tar -zxvf nginx-1.12.2.tar.gz -C /usr/<span class="built_in">local</span>/src/</span><br><span class="line">3.进入到nginx的源码目录</span><br><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/src/nginx-1.12.2/</span><br><span class="line">4.预编译</span><br><span class="line">./configure</span><br><span class="line">5.安静gcc编译器</span><br><span class="line">yum -y install gcc pcre-devel openssl openssl-devel</span><br><span class="line">6.然后再执行</span><br><span class="line">./configure</span><br><span class="line">7.编译安装nginx</span><br><span class="line">make &amp;&amp; make install</span><br><span class="line">8.启动nginx</span><br><span class="line">sbin/nginx</span><br><span class="line">9.查看nginx进程</span><br><span class="line">ps -ef | grep nginx</span><br><span class="line">netstat -anpt | grep nginx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"><span class="comment">#将springboot程序部署在多台服务器上，然后启动springboot</span></span><br><span class="line">$&gt; java -jar /home/ap/jars/sbike-0.0.1-SNAPSHOT.war 1&gt;/home/ap/logs/sbike.log 2&gt;/home/ap/logs/sbike.err &amp;</span><br><span class="line">--------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"><span class="comment">#修改nginx的配置文件，让nginx实现负载均衡功能</span></span><br><span class="line">vi /usr/<span class="built_in">local</span>/nginx/conf/nginx.conf</span><br><span class="line"></span><br><span class="line"><span class="comment">#关于nginx 的详细配置见笔记</span></span><br><span class="line">https://app.yinxiang.com/shard/s37/nl/7399077/03809ee1-8c47-4383-98b1-a72867450681/</span><br><span class="line"></span><br><span class="line"><span class="comment">#负载均衡简单配置</span></span><br><span class="line">$&gt; vi /usr/<span class="built_in">local</span>/nginx/conf/nginx.conf</span><br><span class="line"></span><br><span class="line"><span class="comment">## 在 http 中插入如下内容</span></span><br><span class="line">http &#123;</span><br><span class="line">========================================================================</span><br><span class="line"><span class="comment">#响应数据的来源( tomcat 的服务器集群, weigth 是权重)</span></span><br><span class="line">    upstream tomcats &#123;</span><br><span class="line">        server cs1:8888 weight=1;</span><br><span class="line">        server cs2:8888 weight=1;</span><br><span class="line">        server cs3:8888 weight=1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">#负载均衡服务器配置</span></span><br><span class="line">    server &#123;</span><br><span class="line">        listen       80;</span><br><span class="line">        server_name  cs7;</span><br><span class="line"></span><br><span class="line">        location / &#123;</span><br><span class="line">            <span class="comment">#转发给tomcats组</span></span><br><span class="line">            proxy_pass http://tomcats;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">========================================================================</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">$&gt; nginx/sbin/nginx -h   <span class="comment">#查看帮助</span></span><br><span class="line"></span><br><span class="line">$&gt; nginx/sbin/nginx -h   <span class="comment">#测试配置是否语法正确</span></span><br><span class="line">$&gt; nginx/sbin/nginx -s reload  <span class="comment">#  -s signal     : send signal to a master process: stop, quit, reopen, reload</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#启动Nginx </span></span><br><span class="line">$&gt; sbin/nginx</span><br><span class="line"></span><br><span class="line">(注意: 此时打好的 war 包需要已经在 cs1-cs3上运行起来了,</span><br><span class="line">否则就无法往这3台 tomcat 上转发)</span><br><span class="line"></span><br><span class="line"><span class="comment">#访问: http://cs7/host</span></span><br><span class="line"><span class="comment">#会发现, 每一次访问都会分别转发给 cs1-cs3</span></span><br></pre></td></tr></table></figure><h1 id="三-Nginx-安装-kafka-插件"><a href="#三-Nginx-安装-kafka-插件" class="headerlink" title="三. Nginx 安装 kafka 插件"></a>三. Nginx 安装 kafka 插件</h1><blockquote><p>直接把日志给到 kafka<br><strong>注意: 从 Nginx 直推日志到 kafka 是一个亮点</strong></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line">安装nginx-kafka插件</span><br><span class="line"></span><br><span class="line">1.安装git</span><br><span class="line">yum install -y git</span><br><span class="line"></span><br><span class="line">2.切换到/usr/<span class="built_in">local</span>/src目录，然后将kafka的c客户端源码<span class="built_in">clone</span>到本地 (The Apache Kafka C/C++ library )</span><br><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/src</span><br><span class="line"><span class="comment"># 安装 kafka的 c/c++ 的客户端</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/edenhill/librdkafka</span><br><span class="line"></span><br><span class="line">3.进入到librdkafka，然后进行编译</span><br><span class="line"><span class="built_in">cd</span> librdkafka</span><br><span class="line">yum install -y gcc gcc-c++ pcre-devel zlib-devel</span><br><span class="line">./configure</span><br><span class="line">make &amp;&amp; make install</span><br><span class="line"></span><br><span class="line">4.安装nginx整合kafka的插件，进入到/usr/<span class="built_in">local</span>/src，<span class="built_in">clone</span> nginx整合kafka的源码</span><br><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/src</span><br><span class="line"><span class="comment">#安装 nginx 整合 kafka 的插件</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/brg-liuwei/ngx_kafka_module</span><br><span class="line"></span><br><span class="line">5.进入到nginx的源码包目录下（编译nginx，然后将将插件同时编译）</span><br><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/src/nginx-1.12.2</span><br><span class="line">./configure --add-module=/usr/<span class="built_in">local</span>/src/ngx_kafka_module/</span><br><span class="line">make</span><br><span class="line">make install</span><br><span class="line"></span><br><span class="line">6.修改nginx的配置文件，详情请查看当前目录的nginx.conf</span><br><span class="line"><span class="comment">#同样是在 http&#123;&#125; 中 修改的用 #### 包起来了</span></span><br><span class="line">http &#123;</span><br><span class="line">    include       mime.types;</span><br><span class="line">    default_type  application/octet-stream;</span><br><span class="line">    sendfile        on;</span><br><span class="line">    <span class="comment">#tcp_nopush     on;</span></span><br><span class="line">    <span class="comment">#keepalive_timeout  0;</span></span><br><span class="line">    keepalive_timeout  65;</span><br><span class="line">    <span class="comment">#gzip  on;</span></span><br><span class="line"><span class="comment">################################################</span></span><br><span class="line">    kafka;</span><br><span class="line">    kafka_broker_list cs1:9092 cs2:9092 cs3:9092;</span><br><span class="line"><span class="comment">################################################</span></span><br><span class="line">    server &#123;</span><br><span class="line">        listen       80;</span><br><span class="line">        <span class="comment">################################################</span></span><br><span class="line">        server_name  cs8;</span><br><span class="line">        <span class="comment">#charset koi8-r;</span></span><br><span class="line">        <span class="comment">#access_log  logs/host.access.log  main;</span></span><br><span class="line">    location = /kafka/track &#123;</span><br><span class="line">                kafka_topic track;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        location = /kafka/user &#123;</span><br><span class="line">                kafka_topic user;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">################################################</span></span><br><span class="line">        <span class="comment">#error_page  404              /404.html;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># redirect server error pages to the static page /50x.html</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        error_page   500 502 503 504  /50x.html;</span><br><span class="line">        location = /50x.html &#123;</span><br><span class="line">            root   html;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">7.启动zk和kafka集群(创建topic)</span><br><span class="line">zkServer.sh start</span><br><span class="line">kafka-server-start.sh -daemon /home/ap/apps/kafka/config/server.properties</span><br><span class="line"> &gt; 创建主题  track 和 user</span><br><span class="line">    kafka-topics.sh --create  --zookeeper cs1:2181 --replication-factor 3 --partitions 3 --topic track</span><br><span class="line">    kafka-topics.sh --create  --zookeeper cs1:2181 --replication-factor 3 --partitions 3 --topic user</span><br><span class="line"> &gt; 查看</span><br><span class="line"> kafka-topics.sh  --zookeeper cs1:2181 --describe  --topic track</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">8.启动nginx，报错，找不到kafka.so.1的文件</span><br><span class="line">error <span class="keyword">while</span> loading shared libraries: librdkafka.so.1: cannot open shared object file: No such file or directory</span><br><span class="line"></span><br><span class="line">9.加载so库</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"/usr/local/lib"</span> &gt;&gt; /etc/ld.so.conf</span><br><span class="line">ldconfig</span><br><span class="line"></span><br><span class="line">10.测试，向nginx中写入数据，然后观察kafka的消费者能不能消费到数据</span><br><span class="line"><span class="comment"># 因为在 nginx 的配置文件 nginx.conf 中, 已经指明了 kafka_broker_list 的所有主机, 所以才可以写到 kafka 中去</span></span><br><span class="line">curl localhost/kafka/topic -d <span class="string">"message send to kafka topic"</span></span><br><span class="line"><span class="comment">#例子: </span></span><br><span class="line">1&gt; 在 cs1~3上随便一台开一个消费者 </span><br><span class="line">$&gt; kafka-console-consumer.sh --bootstrap-server  cs1:9092,cs2:9092,cs3:9092 --topic track  --from-beginning</span><br><span class="line">2&gt; 在 nginx 服务骑上, 使用 <span class="comment"># curl localhost/kafka/topic -d "message send to kafka topic" 往配置的主题目录发送消息</span></span><br><span class="line">/usr/<span class="built_in">local</span>/nginx $&gt; curl localhost/kafka/track -d <span class="string">"哎哟我去, 你也太厉害辣...."</span></span><br></pre></td></tr></table></figure><blockquote><p>接下来, 测试微信小程序, 把 生命周期函数<code>onReady</code>后, 采集的用户<code>openid</code> + <code>经度</code> + <code>纬度</code> , post 请求 <code>nginx</code>的服务器 <a href="http://cs8/kafka/user,会直接转发到" target="_blank" rel="noopener">http://cs8/kafka/user,会直接转发到</a> <code>kafka</code>  对应的主题中</p></blockquote><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">0&gt; # 运行 sbike 程序</span><br><span class="line"></span><br><span class="line">1&gt; # wechat 小程序中</span><br><span class="line"> wx.request(&#123;</span><br><span class="line">          <span class="comment">//用POST方式请求es可以只指定index和type，不用指定id</span></span><br><span class="line">          <span class="comment">// url: "http://localhost:8888/log/ready",</span></span><br><span class="line">     </span><br><span class="line">          <span class="comment">// 注意: 这里直接请求 Nginx, 由 nginx 直接把此条user 日志, 推到 kafka 中</span></span><br><span class="line">          url: <span class="string">"http://cs8/kafka/user"</span>,</span><br><span class="line">          data: &#123;</span><br><span class="line">            time: <span class="keyword">new</span> <span class="built_in">Date</span>(),</span><br><span class="line">            openid: openid,</span><br><span class="line">            lat: lat,</span><br><span class="line">            log: log</span><br><span class="line">          &#125;,</span><br><span class="line">          method: <span class="string">"POST"</span></span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2&gt; # 在 cs1-cs3 随便开一个 kafkaconsumer 进行消费</span><br><span class="line">$&gt; kafka-<span class="built_in">console</span>-consumer.sh --bootstrap-server  cs1:<span class="number">9092</span>,<span class="attr">cs2</span>:<span class="number">9092</span>,<span class="attr">cs3</span>:<span class="number">9092</span> --topic top --<span class="keyword">from</span>-beginning</span><br><span class="line"></span><br><span class="line">3&gt; # 刷新 wechat 小程序, 会在 onReady后, 发送一条 post 请求到 nginx, 请求中为 user 信息</span><br><span class="line">在 kafka consumer 端, 会搜集到 user 信息</span><br></pre></td></tr></table></figure><hr><h1 id="四-Flume-高级"><a href="#四-Flume-高级" class="headerlink" title="四. Flume 高级"></a>四. Flume 高级</h1><h2 id="1-自定义-source"><a href="#1-自定义-source" class="headerlink" title="1.自定义 source"></a>1.自定义 source</h2><h4 id="方法总结"><a href="#方法总结" class="headerlink" title="方法总结"></a><strong>方法总结</strong></h4><ul><li>寻找相对应的源码实现类, 如果是 <code>exec source</code>, 就参考 <code>ExecSource</code> 类</li><li>继承对应类继承的类</li><li>至少实现 <code>configure, start, stop 三个方法</code></li><li>start 中创建一个 单线程的线程池 <code>Executors.newSingleThreadExecutor()</code></li><li>创建一个实现了 <code>runnable</code>接口的内部类<code>FileRunnable</code>, 实现主要逻辑<ul><li>构造方法中传入  监控文件路径, 存储 <code>offset</code> 文件路径, 字符集, 查看 <code>interval</code></li><li>检查是否有 offset 的文件, 没有就创建</li><li>用 <code>RandomAccessFile</code> 读取 offset,  <code>seek</code>到offset 读取</li><li>在 <strong>run</strong> 方法中 用 <code>RandomAccessFile</code>对象 <code>raf readline()</code>读取一行</li><li>如果读到的不为空, 读完后,  用<code>channelProcessor</code>推到 channel, 然后更新 offset, 为空, 则 sleep <code>interval</code></li></ul></li></ul><h4 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-20-143854.png" alt="image-20180820223854175"></p><h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><blockquote><p>a1.conf</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#bin/flume-ng agent -n a1 -f /home/hadoop/a1.conf -c conf -Dflume.root.logger=INFO,console</span></span><br><span class="line"><span class="comment">#定义agent名， source、channel、sink的名称</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1</span><br><span class="line"></span><br><span class="line"><span class="comment">#具体定义source</span></span><br><span class="line">a1.sources.r1.type = com.rox.flume.source.TailFileSource</span><br><span class="line">a1.sources.r1.filePath = /Users/shixuanji/Documents/Code/Datas/flumeTest/logs/access.txt</span><br><span class="line">a1.sources.r1.posiFile = /Users/shixuanji/Documents/Code/Datas/flumeTest/logs/posi.txt</span><br><span class="line">a1.sources.r1.interval = 2000</span><br><span class="line">a1.sources.r1.charset = UTF-8</span><br><span class="line"></span><br><span class="line"><span class="comment">#具体定义channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="comment">#具体定义sink (file_roll sink)</span></span><br><span class="line">a1.sinks.k1.type = file_roll</span><br><span class="line">a1.sinks.k1.sink.directory = /Users/shixuanji/Documents/Code/Datas/flumeTest/posiFile_1</span><br><span class="line"><span class="comment">#也可以直接在控制台输出</span></span><br><span class="line"><span class="comment">#a1.sinks.k1.type = logger</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#组装source、channel、sink</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><blockquote><p><strong>com.rox.flume.source.TailFileSource</strong></p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.flume.source;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.io.FileUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Context;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Event;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.EventDrivenSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.channel.ChannelProcessor;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.conf.Configurable;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.event.EventBuilder;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.source.AbstractSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.source.ExecSource;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.RandomAccessFile;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutorService;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.Executors;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.TimeUnit;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 参考源码类</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@class</span> ExecSource</span></span><br><span class="line"><span class="comment"> * flume source 的生命周期：构造器 -&gt; configure -&gt; start -&gt; processor.process</span></span><br><span class="line"><span class="comment"> * 1.读取配置文件：（配置文件的内容：读取哪个文件、编码集、偏移量写到哪个文件、多长时间检查一下文件是否有新内容）</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TailFileSource</span> <span class="keyword">extends</span> <span class="title">AbstractSource</span> <span class="keyword">implements</span> <span class="title">Configurable</span>, <span class="title">EventDrivenSource</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger logger = LoggerFactory.getLogger(TailFileSource.class);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String filePath;                <span class="comment">//监控文件路径</span></span><br><span class="line">    <span class="keyword">private</span> String charset;                 <span class="comment">//编码集</span></span><br><span class="line">    <span class="keyword">private</span> String posiFile;                <span class="comment">//存储偏移量文件路径</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> interval;                  <span class="comment">//多长时间检查一下文件是否有新内容, 后面用作: 每拉取一次, 就 sleep 多久</span></span><br><span class="line">    <span class="keyword">private</span> ExecutorService executor;       <span class="comment">//线程池对象</span></span><br><span class="line">    <span class="keyword">private</span> FileRunnable fileRunnable;      <span class="comment">//不断读取文件的多线程实现类</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        filePath = context.getString(<span class="string">"filePath"</span>);</span><br><span class="line">        charset = context.getString(<span class="string">"charset"</span>, <span class="string">"UTF-8"</span>);</span><br><span class="line">        posiFile = context.getString(<span class="string">"posiFile"</span>);</span><br><span class="line">        interval = context.getLong(<span class="string">"interval"</span>, <span class="number">1000L</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">//创建一个单线程的线程池</span></span><br><span class="line">        executor = Executors.newSingleThreadExecutor();</span><br><span class="line">        <span class="comment">//定义一个实现 Runnable 接口的类对象</span></span><br><span class="line">        fileRunnable = <span class="keyword">new</span> FileRunnable(filePath,charset,posiFile,interval,getChannelProcessor());</span><br><span class="line">        <span class="comment">//实现 Runnable 接口的类, 提交到线程池</span></span><br><span class="line">        executor.submit(fileRunnable);</span><br><span class="line">        <span class="comment">//调用父类的 start 方法</span></span><br><span class="line">        <span class="keyword">super</span>.start();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">stop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        fileRunnable.setFlag(<span class="keyword">false</span>);</span><br><span class="line">        executor.shutdown();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//如果没有停止成功, 每隔0.5秒询问一次, 直到当次 task 执行完成</span></span><br><span class="line">        <span class="keyword">while</span> (!executor.isTerminated()) &#123;</span><br><span class="line">            logger.debug(<span class="string">"Waiting for filer executor service to stop"</span>);</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                executor.awaitTermination(<span class="number">500</span>, TimeUnit.MILLISECONDS);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                logger.debug(<span class="string">"Interrupted while waiting for exec executor service "</span></span><br><span class="line">                        + <span class="string">"to stop. Just exiting."</span>);</span><br><span class="line">                Thread.currentThread().interrupt();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">super</span>.stop();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 定义一个内部类 FileRunnable 实现 Runnable 接口</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">FileRunnable</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">long</span> interval;</span><br><span class="line">        <span class="keyword">private</span> String charset;</span><br><span class="line">        <span class="keyword">private</span> ChannelProcessor channelProcessor;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">long</span> offset = <span class="number">0L</span>;</span><br><span class="line">        <span class="keyword">private</span> RandomAccessFile raf;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">boolean</span> flag = <span class="keyword">true</span>;</span><br><span class="line">        <span class="keyword">private</span> File positionFile;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 构造方法</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> filePath  监控的文件的路径</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> charset   字符编码</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> posiFile  存储偏移量文件的路径</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> interval  读取一次 source 的间隔时间</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> channelProcessor 通道处理器, 用于批量把事件写入通道</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">FileRunnable</span><span class="params">(String filePath, String charset, String posiFile, <span class="keyword">long</span> interval, ChannelProcessor channelProcessor)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">this</span>.interval = interval;</span><br><span class="line">            <span class="keyword">this</span>.charset = charset;         <span class="comment">// 这里的 charset 是 FileRunnable 在初始化的时候传进来的</span></span><br><span class="line">            <span class="keyword">this</span>.channelProcessor = channelProcessor;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//读取偏移量，如果有，就接着读，没有就从头读</span></span><br><span class="line">            positionFile = <span class="keyword">new</span> File(posiFile);</span><br><span class="line">            <span class="comment">//不存在存储偏移量的文件, 说明是第一次读, 此时创建一个</span></span><br><span class="line">            <span class="keyword">if</span> (!positionFile.exists()) &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    positionFile.createNewFile();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                    logger.error(<span class="string">"create position file error"</span>, e);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 存在偏移量文件, 此时读取偏移量</span></span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="comment">//使用 apache.common.io 包中的工具类 FileUtils 读取</span></span><br><span class="line">                String offsetString = FileUtils.readFileToString(positionFile);</span><br><span class="line">                <span class="comment">//如果不为空(记录过偏移量), 就转为 long</span></span><br><span class="line">                <span class="keyword">if</span> (offsetString != <span class="keyword">null</span> &amp;&amp; !<span class="string">""</span>.equals(offsetString.trim())) &#123;</span><br><span class="line">                    offset = Long.parseLong(offsetString);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">//创建一个 RandomAccessFile 对象, 用来读取指定偏移量</span></span><br><span class="line">                raf = <span class="keyword">new</span> RandomAccessFile(filePath, <span class="string">"r"</span>);</span><br><span class="line">                raf.seek(offset);</span><br><span class="line"></span><br><span class="line">            &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                logger.error(<span class="string">"read position file error"</span>, e);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * run 方法</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="comment">// 控制是否持续读取</span></span><br><span class="line">            <span class="keyword">while</span> (flag) &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    String line = raf.readLine();</span><br><span class="line">                    <span class="keyword">if</span> (line != <span class="keyword">null</span>) &#123;</span><br><span class="line">                        <span class="comment">/**</span></span><br><span class="line"><span class="comment">                         * 读取监控文件中的数据</span></span><br><span class="line"><span class="comment">                         * <span class="doctag">@charsetName</span>: 解码的类型</span></span><br><span class="line"><span class="comment">                         * <span class="doctag">@charset</span>: 解码后, 转为此种编码</span></span><br><span class="line"><span class="comment">                         */</span></span><br><span class="line">                        line = <span class="keyword">new</span> String(line.getBytes(<span class="string">"ISO-8859-1"</span>), charset);</span><br><span class="line"></span><br><span class="line">                        <span class="comment">//  ==&gt; 写入到 channel</span></span><br><span class="line">                        channelProcessor.processEvent(EventBuilder.withBody(line.getBytes()));</span><br><span class="line"></span><br><span class="line">                        <span class="comment">//获取最新的偏移量，然后更新偏移量</span></span><br><span class="line">                        offset = raf.getFilePointer();</span><br><span class="line"></span><br><span class="line">                        <span class="comment">//将偏移量写入到位置文件中, 覆盖</span></span><br><span class="line">                        FileUtils.writeStringToFile(positionFile, offset + <span class="string">""</span>, <span class="keyword">false</span>);</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        Thread.sleep(interval);</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                    logger.error(<span class="string">"read file thread error"</span>, e);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                    logger.error(<span class="string">"sleep interval Interrupted"</span>, e);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setFlag</span><span class="params">(<span class="keyword">boolean</span> b)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.flag = b;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-自定义-source-kafka-channel-kafka-cluster"><a href="#2-自定义-source-kafka-channel-kafka-cluster" class="headerlink" title="2.自定义 source + kafka channel  + kafka cluster"></a>2.自定义 source + kafka channel  + kafka cluster</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-20-145845.png" alt="image-20180820225844564"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自定义的 TailFileSource 为 source</span></span><br><span class="line"><span class="comment"># 使用 KafkaChannel 为 channel,  直接就干到了 kafka 中去了</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1&gt;在 kafka cluster 中 的一个节点开一个 consumer 消费者</span></span><br><span class="line">$ kafka-console-consumer.sh --bootstrap-server  cs1:9092,cs2:9092,cs3:9092 --topic usertest  --from-beginning</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2&gt; 配置 a0.conf</span></span><br><span class="line">===============================================================</span><br><span class="line"><span class="comment">#bin/flume-ng agent -c conf -f conf/a0.conf -n a0 -Dflume.root.logger=INFO,console</span></span><br><span class="line"><span class="comment">#定义agent名， source、channel、sink的名称</span></span><br><span class="line">a0.sources = r1</span><br><span class="line">a0.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="comment">#具体定义source</span></span><br><span class="line">a0.sources.r1.type = com.rox.flume.source.TailFileSource</span><br><span class="line">a0.sources.r1.filePath = /Users/shixuanji/Documents/Code/Datas/flumeTest/logs/access.txt</span><br><span class="line">a0.sources.r1.posiFile = /Users/shixuanji/Documents/Code/Datas/flumeTest/logs/posi.txt</span><br><span class="line">a0.sources.r1.interval = 2000</span><br><span class="line">a0.sources.r1.charset = UTF-8</span><br><span class="line"></span><br><span class="line"><span class="comment">#a0.sources.r1.interceptors = i1</span></span><br><span class="line"><span class="comment">#a0.sources.r1.interceptors.i1.type = cn.rox.flume.interceptor.JsonInterceptor$Builder</span></span><br><span class="line"><span class="comment">#a0.sources.r1.interceptors.i1.fields = id,name,fv,age</span></span><br><span class="line"><span class="comment">#a0.sources.r1.interceptors.i1.separator = ,</span></span><br><span class="line"></span><br><span class="line">a0.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel</span><br><span class="line">a0.channels.c1.kafka.bootstrap.servers = cs1:9092,cs2:9092,cs3:9092</span><br><span class="line">a0.channels.c1.kafka.topic = usertest</span><br><span class="line"><span class="comment">#Kafka Channel中, 设置写入为 flumeEvent 为 false, 默认是 true, 这里就不会写入事件(事件还包括body,header 等)</span></span><br><span class="line">a0.channels.c1.parseAsFlumeEvent = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">a0.sources.r1.channels = c1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">===============================================================</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3&gt;开启 flume 任务</span></span><br><span class="line">➜  apache-flume-1.8.0-bin bin/flume-ng agent -c conf -f conf/a0.conf -n a0 -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4&gt;往 文件中, 写数据 &amp; 查看 kafka consumer</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"xxxx"</span> &gt;&gt; /Users/shixuanji/Documents/Code/Datas/flumeTest/logs/access.txt</span><br></pre></td></tr></table></figure><h2 id="3-Kafka-cluster-为-source-kafka-Channel-HDFS-Sink"><a href="#3-Kafka-cluster-为-source-kafka-Channel-HDFS-Sink" class="headerlink" title="3.Kafka cluster 为 source +  kafka Channel + HDFS Sink"></a>3.Kafka cluster 为 source +  kafka Channel + HDFS Sink</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-20-151624.png" alt="image-20180820231623971"></p><h2 id="4-自定义拦截器"><a href="#4-自定义拦截器" class="headerlink" title="4.自定义拦截器"></a>4.自定义拦截器</h2><p>作用: 主要是为原本输入的 String 添加了 scheme,  与原本输入字段组成 json 串, 再输入到 kafka 中, 接上面的 4.2</p><p>在2 的 基础上, 增加了拦截器</p><blockquote><p>代码如下</p></blockquote><h4 id="a0-conf-文件"><a href="#a0-conf-文件" class="headerlink" title="a0.conf 文件"></a><code>a0.conf</code> 文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#bin/flume-ng agent -c conf -f conf/a0.conf -n a0 -Dflume.root.logger=INFO,console</span></span><br><span class="line"><span class="comment">#定义agent名， source、channel、sink的名称</span></span><br><span class="line">a0.sources = r1</span><br><span class="line">a0.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="comment">#具体定义source</span></span><br><span class="line">a0.sources.r1.type = com.rox.flume.source.TailFileSource</span><br><span class="line">a0.sources.r1.filePath = /Users/shixuanji/Documents/Code/Datas/flumeTest/logs/access.txt</span><br><span class="line">a0.sources.r1.posiFile = /Users/shixuanji/Documents/Code/Datas/flumeTest/logs/posi.txt</span><br><span class="line">a0.sources.r1.interval = 2000</span><br><span class="line">a0.sources.r1.charset = UTF-8</span><br><span class="line"></span><br><span class="line"><span class="comment">#拦截器</span></span><br><span class="line">a0.sources.r1.interceptors = i1</span><br><span class="line">a0.sources.r1.interceptors.i1.type = com.rox.flume.interceptor.JsonInterceptor<span class="variable">$Builder</span></span><br><span class="line">a0.sources.r1.interceptors.i1.fields = id,name,fv,age</span><br><span class="line">a0.sources.r1.interceptors.i1.separator = ,</span><br><span class="line"></span><br><span class="line">a0.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel</span><br><span class="line">a0.channels.c1.kafka.bootstrap.servers = cs1:9092,cs2:9092,cs3:9092</span><br><span class="line">a0.channels.c1.kafka.topic = usertest</span><br><span class="line">a0.channels.c1.parseAsFlumeEvent = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">a0.sources.r1.channels = c1</span><br></pre></td></tr></table></figure><h4 id="拦截器代码"><a href="#拦截器代码" class="headerlink" title="拦截器代码"></a>拦截器代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.flume.interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONObject;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Context;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Event;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.interceptor.Interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.LinkedHashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JsonInterceptor</span> <span class="keyword">implements</span> <span class="title">Interceptor</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String[] schema; <span class="comment">//id,name,fv,age</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String separator; <span class="comment">// ,</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 构造方法, 传入schema(表头), separator(分隔符)</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> schema</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> separator     内容的分隔符</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">JsonInterceptor</span><span class="params">(String schema, String separator)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.schema = schema.split(<span class="string">"[,]"</span>);   <span class="comment">// schema 的分隔符也为 ,</span></span><br><span class="line">        <span class="keyword">this</span>.separator = separator;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// no-op</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Event <span class="title">intercept</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">        Map&lt;String, String&gt; tuple = <span class="keyword">new</span> LinkedHashMap&lt;String, String&gt;();</span><br><span class="line">        <span class="comment">//将传入的Event中的body内容，加上schema，然后在放入到Event</span></span><br><span class="line">        String line = <span class="keyword">new</span> String(event.getBody());</span><br><span class="line">        String[] fields = line.split(separator);</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; schema.length; i++) &#123;</span><br><span class="line">            String key = schema[i];</span><br><span class="line">            String value = fields[i];</span><br><span class="line">            tuple.put(key, value);</span><br><span class="line">        &#125;</span><br><span class="line">        String json = JSONObject.toJSONString(tuple);</span><br><span class="line">        <span class="comment">//将转换好的json，再放入到Event中</span></span><br><span class="line">        event.setBody(json.getBytes());</span><br><span class="line">        <span class="keyword">return</span> event;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Event&gt; <span class="title">intercept</span><span class="params">(List&lt;Event&gt; events)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Event e : events) &#123;</span><br><span class="line">            intercept(e);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> events;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// no-op</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Interceptor.Builder的生命周期方法</span></span><br><span class="line"><span class="comment">     * 构造器 -&gt; configure -&gt; build</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Builder</span> <span class="keyword">implements</span> <span class="title">Interceptor</span>.<span class="title">Builder</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> String fields;</span><br><span class="line">        <span class="keyword">private</span> String separator;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Interceptor <span class="title">build</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="comment">//在build创建JsonInterceptor的实例</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> JsonInterceptor(fields, separator);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 配置文件中应该有哪些属性？</span></span><br><span class="line"><span class="comment">         * 1.数据的分割符</span></span><br><span class="line"><span class="comment">         * 2.字段名字（schema）</span></span><br><span class="line"><span class="comment">         * 3.schema字段的分隔符</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">            fields = context.getString(<span class="string">"fields"</span>);</span><br><span class="line">            separator = context.getString(<span class="string">"separator"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="5-TairDir"><a href="#5-TairDir" class="headerlink" title="5.TairDir"></a>5.TairDir</h2><p><strong>TailDir 可以监控多个文件,  使用正则表达式匹配</strong></p><p>这里定义2个 source,  channel,  sink.</p><p>其中一个是tairdir</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a2.conf</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#bin/flume-ng agent -n a2 -f /home/hadoop/a2.conf -c conf -Dflume.root.logger=INFO,console</span></span><br><span class="line"><span class="comment">#定义agent名， source、channel、sink的名称</span></span><br><span class="line">a2.sources = r1 r2</span><br><span class="line">a2.channels = c1 c2</span><br><span class="line">a2.sinks = k1 k2</span><br><span class="line"></span><br><span class="line"><span class="comment">#具体定义source</span></span><br><span class="line">a2.sources.r1.type = com.rox.flume.source.TailFileSource</span><br><span class="line">a2.sources.r1.filePath = /Users/shixuanji/Documents/Code/Datas/flumeTest/logs/access.txt</span><br><span class="line">a2.sources.r1.posiFile = /Users/shixuanji/Documents/Code/Datas/flumeTest/logs/posi.txt</span><br><span class="line">a2.sources.r1.interval = 1000</span><br><span class="line">a2.sources.r1.charset = UTF-8</span><br><span class="line"></span><br><span class="line">a2.sources.r2.type = TAILDIR</span><br><span class="line">a2.sources.r2.positionFile = /Users/shixuanji/Documents/Code/Datas/flumeTest/TailDir/position.json</span><br><span class="line">a2.sources.r2.filegroups = g1</span><br><span class="line">a2.sources.r2.filegroups.g1 = /Users/shixuanji/Documents/Code/Datas/flumeTest/TailDir/2018/.*.txt </span><br><span class="line">a2.sources.r2.fileHeader = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#具体定义channel</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">a2.channels.c2.type = memory</span><br><span class="line">a2.channels.c2.capacity = 1000</span><br><span class="line">a2.channels.c2.transactionCapacity = 100</span><br><span class="line"><span class="comment">#具体定义sink</span></span><br><span class="line">a2.sinks.k1.type = file_roll</span><br><span class="line">a2.sinks.k1.sink.directory = /Users/shixuanji/Documents/Code/Datas/flumeTest/TailDir/k1</span><br><span class="line"></span><br><span class="line">a2.sinks.k2.type = file_roll</span><br><span class="line">a2.sinks.k2.sink.directory = /Users/shixuanji/Documents/Code/Datas/flumeTest/TailDir/k2</span><br><span class="line"></span><br><span class="line"><span class="comment">#组装source、channel、sink</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line">a2.sources.r2.channels = c2</span><br><span class="line">a2.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure><p>但是 , 此时还是 <strong>不支持 递归监控</strong>文件,  可以用这个第三方插件</p><p><a href="https://github.com/qwurey/flume-source-taildir-recursive" target="_blank" rel="noopener">https://github.com/qwurey/flume-source-taildir-recursive</a></p><h1 id="五-腾讯云短信集成"><a href="#五-腾讯云短信集成" class="headerlink" title="五. 腾讯云短信集成"></a>五. 腾讯云短信集成</h1><p>加入 maven 依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.github.qcloudsms<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>qcloudsms<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h1 id="六-整体架构"><a href="#六-整体架构" class="headerlink" title="六. 整体架构"></a>六. 整体架构</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-26-053001.png" alt="image-20180826133001140"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-26-053013.png" alt="image-20180826133013229"></p><p>整体架构图</p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-09-15-%E5%85%B1%E4%BA%AB%E5%8D%95%E8%BD%A6%E6%9E%B6%E6%9E%84%E5%9B%BE.png" alt=""></p><h1 id="七-指标计算"><a href="#七-指标计算" class="headerlink" title="七. 指标计算"></a>七. 指标计算</h1><p>优化：<br>    1.将数据存储了类型转换成parquet格式（列式存储）<br>    2.在采集数据时转换（数据已经存储在kafka中了）<br>        2.1将数据同步到HDFS中，然后写sparksql，将json的数据转成parquet<br>        2.2编写一段sparkSteaming程序，实时的进行数据的转换</p><p>数据处理的流程？<br>    数据怎么来的？ 各种终端设备产生的日志（用户点触发了特定的事件，然后在对应的事件进程埋点，就是讲触发事件是产生的数据发送给日志采集服务器，是一个Nginx，可以将数据写入到kafka中，如果是离线，在将数据同步到HDFS中）<br>    有哪些关键字段？ 时间、地点（省、市、区）、金额、app类型（IOS、Android、微信小程序）<br>    怎么处理的？数据的预处理（清洗 -&gt; 转换（json -&gt; parquet)） sparksql（离线） sparkSteaming（实时）<br>    计算完保存到哪里？聚合后的数据保存在关系数据库中（集群）、明细数据保存到Hbase、存储到mongo、Elasticsearch中</p><p>今天收集的用户充值的数据可以计算出哪些指标<br>    实时指标：<br>        1.当天截止到当前时间的充值中金额<br>        2.各个省、市的充值金额<br>        3.各个终端类型的充值金额<br>        4.充值的成功率<br>        5.充值的渠道<br>        6.充值笔数<br>        7.省、市的充值笔数<br>    离线指标<br>        1.某一天的充值金额<br>        2.各个省、市的充值金额<br>        3.各个终端类型的充值金额<br>        4.充值的成功率<br>        5.充值的渠道<br>        6.充值笔数<br>        7.省、市的充值笔数</p><hr><p>收集用户的报修事件的日志<br>    数据怎么来的？各种终端设备产生的日志，发现单车不能使用，然后点击报修按钮、智能锁上报车辆状态<br>    有哪些关键字段？时间、地点（省、市、区）、用户id、车辆id、原因（部件）、app类型<br>    怎么处理的？数据的预处理（清洗 -&gt; 转换（json -&gt; parquet)） sparksql<br>    计算完保存到哪里？聚合后的数据保存在关系数据库中（集群）、明细数据保存到Hbase、存储到mongo、Elasticsearch中</p><p>能计算哪些指标<br>        1.报修次数<br>        2.报修的区域<br>        3.损坏的部件</p><hr><p>活动参与<br>    1.活动的点击次数<br>    2.活动的参与次数<br>    3.参会活动积分兑换<br>    4.哪些用户对哪些活动感兴趣</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一-项目总体架构&quot;&gt;&lt;a href=&quot;#一-项目总体架构&quot; class=&quot;headerlink&quot; title=&quot;一. 项目总体架构&quot;&gt;&lt;/a&gt;一. 项目总体架构&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/s
      
    
    </summary>
    
      <category term="Project" scheme="https://airpoet.github.io/categories/Project/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Project" scheme="https://airpoet.github.io/tags/Project/"/>
    
  </entry>
  
  <entry>
    <title>MongoDB + Mycat</title>
    <link href="https://airpoet.github.io/2018/08/16/MongoDB/MongoDB/"/>
    <id>https://airpoet.github.io/2018/08/16/MongoDB/MongoDB/</id>
    <published>2018-08-16T14:07:58.645Z</published>
    <updated>2018-09-01T12:45:44.526Z</updated>
    
    <content type="html"><![CDATA[<p>注意点: </p><ul><li>创建用户名, 密码前, 要先选择对应的库,  用户&amp;密码 是对应的库的</li></ul><h1 id="一-MongoDB-简介"><a href="#一-MongoDB-简介" class="headerlink" title="一. MongoDB 简介"></a>一. MongoDB 简介</h1><p><strong>mongdb一个NoSQL数据库，里面存储的是BSON（Binary Serialized Document Format)，支持集群，高可用、可扩展。</strong></p><table><thead><tr><th>MongoDB</th><th>MySQL</th></tr></thead><tbody><tr><td>database</td><td>database</td></tr><tr><td>collection</td><td>table</td></tr><tr><td>json</td><td>二维表</td></tr><tr><td>不支持SQL</td><td>SQL</td></tr><tr><td>_id</td><td>主键</td></tr></tbody></table><h1 id="二-安装-MongoDB"><a href="#二-安装-MongoDB" class="headerlink" title="二. 安装 MongoDB"></a>二. 安装 MongoDB</h1><h3 id="安装前准备"><a href="#安装前准备" class="headerlink" title="安装前准备"></a>安装前准备</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建一个普通用户ap</span></span><br><span class="line">useradd ap</span><br><span class="line"><span class="comment">#为hadoop用户添加密码：</span></span><br><span class="line"><span class="built_in">echo</span> 123456 | passwd --stdin ap</span><br><span class="line"><span class="comment">#将bigdata添加到sudoers</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"ap ALL = (root) NOPASSWD:ALL"</span> | tee /etc/sudoers.d/ap</span><br><span class="line">chmod 0440 /etc/sudoers.d/ap</span><br><span class="line"><span class="comment">#解决sudo: sorry, you must have a tty to run sudo问题，在/etc/sudoer注释掉 Default requiretty 一行</span></span><br><span class="line">sudo sed -i <span class="string">'s/Defaults    requiretty/Defaults:ap !requiretty/'</span> /etc/sudoers</span><br><span class="line"></span><br><span class="line"><span class="comment">#配置mongo的yum源</span></span><br><span class="line">sudo vi /etc/yum.repos.d/mongodb-org-3.4.repo</span><br><span class="line"></span><br><span class="line">[mongodb-org-3.4]</span><br><span class="line">name=MongoDB Repository</span><br><span class="line">baseurl=http://mirrors.aliyun.com/mongodb/yum/redhat/6Server/mongodb-org/3.4/x86_64/</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"></span><br><span class="line"><span class="comment">#关闭selinux</span></span><br><span class="line">vi /etc/sysconfig/selinux </span><br><span class="line">SELINUX=disabled</span><br><span class="line"></span><br><span class="line"><span class="comment">#重新启动</span></span><br><span class="line">reboot</span><br></pre></td></tr></table></figure><h3 id="mongo的安装和基本使用"><a href="#mongo的安装和基本使用" class="headerlink" title="mongo的安装和基本使用"></a>mongo的安装和基本使用</h3><p><a href="https://www.runoob.com/mongodb/mongodb-tutorial.html" target="_blank" rel="noopener">详细使用吗, 见菜鸟教程</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在机器上使用ap用户登录（本地安装rpm包d的话，rpm -ivh *.rpm）</span></span><br><span class="line">sudo yum install -y mongodb-org</span><br><span class="line"></span><br><span class="line"><span class="comment">#修改mongo的配置文件</span></span><br><span class="line">sudo vi /etc/mongod.conf </span><br><span class="line"></span><br><span class="line"><span class="comment">#注释掉bindIp或者修改成当前机器的某一个ip地址</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#启动mongo</span></span><br><span class="line">sudo service mongod start</span><br><span class="line"></span><br><span class="line"><span class="comment">#连接到mongo</span></span><br><span class="line"><span class="comment">## 如果注释掉了bindIp，那么连接时用</span></span><br><span class="line">mongo</span><br><span class="line"><span class="comment">## 指定了ip地址</span></span><br><span class="line">mongo --host 192.168.170.171 --port 27017</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用或创建database</span></span><br><span class="line">use ap</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建集合（表）</span></span><br><span class="line">db.createCollection(<span class="string">"bike"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#插入数据</span></span><br><span class="line">db.bike.insert(&#123;<span class="string">"_id"</span>: 100001, <span class="string">"status"</span>: 1, <span class="string">"desc"</span>: <span class="string">"test"</span>&#125;)</span><br><span class="line">db.bike.insert(&#123;<span class="string">"_id"</span>: 100002, <span class="string">"status"</span>: 1, <span class="string">"desc"</span>: <span class="string">"test"</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">#查找数据(所有)</span></span><br><span class="line">db.bike.find()</span><br><span class="line"></span><br><span class="line"><span class="comment">#退出</span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#关闭mongo服务</span></span><br><span class="line">sudu service mongod stop</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置服务开机启动</span></span><br><span class="line">sudo checkconfig mongod on</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置mongo服务开机不启动</span></span><br><span class="line">sudo chkconfig mongod off</span><br></pre></td></tr></table></figure><h3 id="mongo安全认证配置"><a href="#mongo安全认证配置" class="headerlink" title="mongo安全认证配置"></a>mongo安全认证配置</h3><blockquote><p>注意: </p><p>#如果修改了mongo存储是的目录那么一定要修改该目录的所属用户和组为mongod</p><p>$&gt; chown -R mongod:mongod  /mongo/</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#添加管理员用户</span></span><br><span class="line"><span class="comment">#使用admin这个database</span></span><br><span class="line">use admin</span><br><span class="line"></span><br><span class="line"><span class="comment">#在没有开启认证的情况下，创建一个超级用户</span></span><br><span class="line">db.createUser(</span><br><span class="line">     &#123;</span><br><span class="line">       user: <span class="string">"root"</span>,</span><br><span class="line">       <span class="built_in">pwd</span>: <span class="string">"123"</span>,</span><br><span class="line">       roles: [ &#123;role: <span class="string">"root"</span>, db: <span class="string">"admin"</span> &#125;]</span><br><span class="line">     &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#修改mongo的配置文件/etc/mongod.conf，配置mongo的安全认证, 前面空2格</span></span><br><span class="line">security:</span><br><span class="line">  authorization: enabled</span><br><span class="line"></span><br><span class="line"><span class="comment">#重启mongo服务</span></span><br><span class="line">service mongod restart</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#重新使用mongo shell连接</span></span><br><span class="line">mongo</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用admin database</span></span><br><span class="line">use admin</span><br><span class="line"></span><br><span class="line"><span class="comment">#授权登录</span></span><br><span class="line">db.auth(<span class="string">"admin"</span>, <span class="string">"admin123"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建一个bike数据库</span></span><br><span class="line">use bike</span><br><span class="line"></span><br><span class="line"><span class="comment">#添加一个普通用户，具备读写权限</span></span><br><span class="line">db.createUser(</span><br><span class="line">     &#123;</span><br><span class="line">       user: <span class="string">"ap"</span>,</span><br><span class="line">       <span class="built_in">pwd</span>: <span class="string">"123"</span>,</span><br><span class="line">       roles: [<span class="string">"readWrite"</span>]</span><br><span class="line">     &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用ap用户登录</span></span><br><span class="line">mongo</span><br><span class="line"><span class="comment">#因为用户都是针对数据库设置的, 所以要先选择数据库</span></span><br><span class="line">use bike</span><br><span class="line"><span class="comment"># 选择完数据库之后, 验证 用户名&amp;密码</span></span><br><span class="line">db.auth(<span class="string">"ap"</span>, <span class="string">"123"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#在database下创建collection</span></span><br><span class="line">db.createCollection(<span class="string">"users"</span>)</span><br><span class="line">db.createCollection(<span class="string">"bikes"</span>)</span><br><span class="line"></span><br><span class="line">show tables</span><br></pre></td></tr></table></figure><h1 id="三-MongoDB-集群搭建"><a href="#三-MongoDB-集群搭建" class="headerlink" title="三. MongoDB 集群搭建"></a>三. MongoDB 集群搭建</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###【在多台机器上执行下面的命令</span></span><br><span class="line"><span class="comment">#在所有创建一个ap普通用户：</span></span><br><span class="line">useradd ap</span><br><span class="line"><span class="comment">#为ap用户添加密码：</span></span><br><span class="line"><span class="built_in">echo</span> 123456 | passwd --stdin ap</span><br><span class="line"><span class="comment">#将ap添加到sudoers</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"ap ALL = (root) NOPASSWD:ALL"</span> | tee /etc/sudoers.d/ap</span><br><span class="line">chmod 0440 /etc/sudoers.d/ap</span><br><span class="line"><span class="comment">#解决sudo: sorry, you must have a tty to run sudo问题，在/etc/sudoer注释掉 Default requiretty 一行</span></span><br><span class="line">sudo sed -i <span class="string">'s/Defaults    requiretty/Defaults:ap !requiretty/'</span> /etc/sudoers</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建一个mongo目录</span></span><br><span class="line">mkdir /mongo</span><br><span class="line"><span class="comment">#给相应的目录添加权限</span></span><br><span class="line">chown -R ap:ap /mongo</span><br><span class="line"></span><br><span class="line"><span class="comment">#配置mongo的yum源</span></span><br><span class="line">cat &gt;&gt; /etc/yum.repos.d/mongodb-org.repo &lt;&lt; EOF</span><br><span class="line">[mongodb-org]</span><br><span class="line">name=MongoDB Repository</span><br><span class="line">baseurl=http://mirrors.aliyun.com/mongodb/yum/redhat/6Server/mongodb-org/3.4/x86_64/</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">#关闭selinux</span></span><br><span class="line">sed -i <span class="string">'s/SELINUX=enforcing/SELINUX=disabled/'</span> /etc/selinux/config</span><br><span class="line"></span><br><span class="line"><span class="comment">#重新启动</span></span><br><span class="line">reboot</span><br><span class="line"></span><br><span class="line">&lt;sub&gt;&lt;/sub&gt;&lt;sub&gt;&lt;/sub&gt;&lt;sub&gt;&lt;/sub&gt;&lt;sub&gt;&lt;/sub&gt;&lt;sub&gt;&lt;/sub&gt;~</span><br><span class="line">ps:  vi /etc/yum.conf  中 把 keepcache修改为1,  即可以缓存下载的 yum 文件,  在 /var/cache/yum/x86_64 中可以找到</span><br><span class="line">      然后 sudo rpm -ivh *.rpm 安装即可</span><br><span class="line">&lt;sub&gt;&lt;/sub&gt;&lt;sub&gt;&lt;/sub&gt;&lt;sub&gt;&lt;/sub&gt;&lt;sub&gt;&lt;/sub&gt;&lt;sub&gt;&lt;/sub&gt;~</span><br><span class="line">------------------------------------------------------------------------------------------------</span><br><span class="line">------------------------------------------------------------------------------------------------</span><br><span class="line"><span class="comment">#分别在多台机器上使用ap用户登录</span></span><br><span class="line">sudo yum install -y mongodb-org</span><br><span class="line"></span><br><span class="line"><span class="comment">#继续角色信息</span></span><br><span class="line">node-1        node-2          node-3</span><br><span class="line"></span><br><span class="line">mongos        mongos          mongos        路由服务器，寻址</span><br><span class="line">config        config          config        配置服务器，保存配置</span><br><span class="line">shard1(主)    shard2（主）     shard3（主）   分片：保存数据</span><br><span class="line">shard2        shard3          shard1   副本集：备份数据，可以配置读写分离（主负责写，从负责同步数据和读）</span><br><span class="line">shard3        shard1          shard2</span><br><span class="line">===============================================================================================================</span><br><span class="line">config</span><br><span class="line">===============================================================================================================</span><br><span class="line"><span class="comment">#分别在多台机器上创建mongo config server对应的目录</span></span><br><span class="line">mkdir -p /mongo/config/&#123;<span class="built_in">log</span>,data,run&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#分别在多台机器上修改config server的配置文件</span></span><br><span class="line">cat &gt;&gt; /mongo/config/mongod.conf &lt;&lt; EOF</span><br><span class="line">systemLog:</span><br><span class="line">  destination: file</span><br><span class="line">  logAppend: <span class="literal">true</span></span><br><span class="line">  path: /mongo/config/<span class="built_in">log</span>/mongod.log</span><br><span class="line">storage:</span><br><span class="line">  dbPath: /mongo/config/data</span><br><span class="line">  journal:</span><br><span class="line">    enabled: <span class="literal">true</span></span><br><span class="line">processManagement:</span><br><span class="line">  fork: <span class="literal">true</span></span><br><span class="line">  pidFilePath: /mongo/config/run/mongod.pid</span><br><span class="line">net:</span><br><span class="line">  port: 27100</span><br><span class="line">replication:</span><br><span class="line">  replSetName: config</span><br><span class="line">sharding:</span><br><span class="line">  clusterRole: configsvr</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># clusterRole: configsvr这个配置是固定的</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#【重要】启动所有的mongo config server服务</span></span><br><span class="line">mongod --config /mongo/config/mongod.conf</span><br><span class="line"></span><br><span class="line"><span class="comment">#登录任意一台配置服务器，初始化配置副本集</span></span><br><span class="line">mongo --port 27100</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建配置</span></span><br><span class="line">config = &#123;</span><br><span class="line">   _id : <span class="string">"config"</span>,</span><br><span class="line">    members : [</span><br><span class="line">        &#123;_id : 0, host : <span class="string">"192.168.170.131:27100"</span> &#125;,</span><br><span class="line">        &#123;_id : 1, host : <span class="string">"192.168.170.132:27100"</span> &#125;,</span><br><span class="line">        &#123;_id : 2, host : <span class="string">"192.168.170.133:27100"</span> &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化副本集配置</span></span><br><span class="line">rs.initiate(config)</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看分区状态</span></span><br><span class="line">rs.status()</span><br><span class="line"></span><br><span class="line"><span class="comment">#注意:其中，"_id" : "config"对应配置文件中配置的 replicaction.replSetName 一致，"members" 中的 "host" 为三个节点的ip和port</span></span><br><span class="line"></span><br><span class="line">===============================================================================================================</span><br><span class="line"><span class="comment">#配置第一个分片和副本集</span></span><br><span class="line">=====================================</span><br><span class="line"><span class="comment">#修改mongo shard1 server的配置文件</span></span><br><span class="line">mkdir -p /mongo/shard1/&#123;<span class="built_in">log</span>,data,run&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#分别在多台机器上修改shard1 server的配置文件</span></span><br><span class="line">cat &gt;&gt; /mongo/shard1/mongod.conf &lt;&lt; EOF</span><br><span class="line">systemLog:</span><br><span class="line">  destination: file</span><br><span class="line">  logAppend: <span class="literal">true</span></span><br><span class="line">  path: /mongo/shard1/<span class="built_in">log</span>/mongod.log</span><br><span class="line">storage:</span><br><span class="line">  dbPath: /mongo/shard1/data</span><br><span class="line">  journal:</span><br><span class="line">    enabled: <span class="literal">true</span></span><br><span class="line">processManagement:</span><br><span class="line">  fork: <span class="literal">true</span></span><br><span class="line">  pidFilePath: /mongo/shard1/run/mongod.pid</span><br><span class="line">net:</span><br><span class="line">  port: 27001</span><br><span class="line">replication:</span><br><span class="line">  replSetName: shard1</span><br><span class="line">sharding:</span><br><span class="line">  clusterRole: shardsvr</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">#启动所有的shard1 server</span></span><br><span class="line">mongod --config /mongo/shard1/mongod.conf</span><br><span class="line"></span><br><span class="line"><span class="comment">#登陆任意一台shard1服务器(希望哪一台机器是主，就登录到那一台机器上)，初始化副本集</span></span><br><span class="line">mongo --port 27001</span><br><span class="line"><span class="comment">#使用admin数据库</span></span><br><span class="line">use admin</span><br><span class="line"><span class="comment">#定义副本集配置</span></span><br><span class="line">config = &#123;</span><br><span class="line">   _id : <span class="string">"shard1"</span>,</span><br><span class="line">    members : [</span><br><span class="line">        &#123;_id : 0, host : <span class="string">"192.168.170.131:27001"</span> &#125;,</span><br><span class="line">        &#123;_id : 1, host : <span class="string">"192.168.170.132:27001"</span> &#125;,</span><br><span class="line">        &#123;_id : 2, host : <span class="string">"192.168.170.133:27001"</span> &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#初始化副本集配置</span></span><br><span class="line">rs.initiate(config);</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看分区状态</span></span><br><span class="line">rs.status()</span><br><span class="line"></span><br><span class="line">&lt;sub&gt;&lt;/sub&gt;&lt;sub&gt;&lt;/sub&gt;&lt;sub&gt;&lt;/sub&gt;&lt;sub&gt;&lt;/sub&gt;&lt;sub&gt;&lt;/sub&gt;~</span><br><span class="line">ps: 过程中出现的错误: 配置 member时候, 端口号写错了 ==&gt;  27001 写成了 27100, 结果导致如下的错误</span><br><span class="line">&#123;</span><br><span class="line">        <span class="string">"ok"</span> : 0,</span><br><span class="line">        <span class="string">"errmsg"</span> : <span class="string">"No host described in new configuration 1 for replica set shard1 maps to this node"</span>,</span><br><span class="line">        <span class="string">"code"</span> : 93,</span><br><span class="line">        <span class="string">"codeName"</span> : <span class="string">"InvalidReplicaSetConfig"</span></span><br><span class="line">&#125;</span><br><span class="line">&lt;sub&gt;&lt;/sub&gt;&lt;sub&gt;&lt;/sub&gt;&lt;sub&gt;&lt;/sub&gt;&lt;sub&gt;&lt;/sub&gt;&lt;sub&gt;&lt;/sub&gt;&lt;sub&gt;&lt;/sub&gt;</span><br><span class="line">===============================================================================================================</span><br><span class="line"><span class="comment">#配置第二个分片和副本集</span></span><br><span class="line">=====================================</span><br><span class="line"><span class="comment">#修改mongo shard2 server的配置文件</span></span><br><span class="line">mkdir -p /mongo/shard2/&#123;<span class="built_in">log</span>,data,run&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#分别在多台机器上修改shard2 server的配置文件</span></span><br><span class="line">cat &gt;&gt; /mongo/shard2/mongod.conf &lt;&lt; EOF</span><br><span class="line">systemLog:</span><br><span class="line">  destination: file</span><br><span class="line">  logAppend: <span class="literal">true</span></span><br><span class="line">  path: /mongo/shard2/<span class="built_in">log</span>/mongod.log</span><br><span class="line">storage:</span><br><span class="line">  dbPath: /mongo/shard2/data</span><br><span class="line">  journal:</span><br><span class="line">    enabled: <span class="literal">true</span></span><br><span class="line">processManagement:</span><br><span class="line">  fork: <span class="literal">true</span></span><br><span class="line">  pidFilePath: /mongo/shard2/run/mongod.pid</span><br><span class="line">net:</span><br><span class="line">  port: 27002</span><br><span class="line">replication:</span><br><span class="line">  replSetName: shard2</span><br><span class="line">sharding:</span><br><span class="line">  clusterRole: shardsvr</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">#启动所有的shard2 server</span></span><br><span class="line">mongod --config /mongo/shard2/mongod.conf</span><br><span class="line"></span><br><span class="line"><span class="comment">#登陆(node2)的shard2服务器，初始化副本集</span></span><br><span class="line">mongo --port 27002</span><br><span class="line"><span class="comment">#使用admin数据库</span></span><br><span class="line">use admin</span><br><span class="line"><span class="comment">#定义副本集配置</span></span><br><span class="line">config = &#123;</span><br><span class="line">   _id : <span class="string">"shard2"</span>,</span><br><span class="line">    members : [</span><br><span class="line">        &#123;_id : 0, host : <span class="string">"192.168.170.132:27002"</span> &#125;,</span><br><span class="line">        &#123;_id : 1, host : <span class="string">"192.168.170.133:27002"</span> &#125;,</span><br><span class="line">        &#123;_id : 2, host : <span class="string">"192.168.170.131:27002"</span> &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#初始化副本集配置</span></span><br><span class="line">rs.initiate(config)</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看分区状态</span></span><br><span class="line">rs.status()</span><br><span class="line"></span><br><span class="line">===============================================================================================================</span><br><span class="line"><span class="comment">#配置第三个分片和副本集</span></span><br><span class="line">=====================================</span><br><span class="line"><span class="comment">#修改mongo shard3 server的配置文件</span></span><br><span class="line">mkdir -p /mongo/shard3/&#123;<span class="built_in">log</span>,data,run&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#分别在多台机器上修改shard3 server的配置文件</span></span><br><span class="line">cat &gt;&gt; /mongo/shard3/mongod.conf &lt;&lt; EOF</span><br><span class="line">systemLog:</span><br><span class="line">  destination: file</span><br><span class="line">  logAppend: <span class="literal">true</span></span><br><span class="line">  path: /mongo/shard3/<span class="built_in">log</span>/mongod.log</span><br><span class="line">storage:</span><br><span class="line">  dbPath: /mongo/shard3/data</span><br><span class="line">  journal:</span><br><span class="line">    enabled: <span class="literal">true</span></span><br><span class="line">processManagement:</span><br><span class="line">  fork: <span class="literal">true</span></span><br><span class="line">  pidFilePath: /mongo/shard3/run/mongod.pid</span><br><span class="line">net:</span><br><span class="line">  port: 27003</span><br><span class="line">replication:</span><br><span class="line">  replSetName: shard3</span><br><span class="line">sharding:</span><br><span class="line">  clusterRole: shardsvr</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">#启动所有的shard3 server</span></span><br><span class="line">mongod --config /mongo/shard3/mongod.conf</span><br><span class="line"></span><br><span class="line"><span class="comment">#登陆node-3上的shard3服务器，初始化副本集</span></span><br><span class="line">mongo --port 27003</span><br><span class="line"><span class="comment">#使用admin数据库</span></span><br><span class="line">use admin</span><br><span class="line"><span class="comment">#定义副本集配置</span></span><br><span class="line">config = &#123;</span><br><span class="line">   _id : <span class="string">"shard3"</span>,</span><br><span class="line">    members : [</span><br><span class="line">        &#123;_id : 0, host : <span class="string">"192.168.170.133:27003"</span> &#125;,</span><br><span class="line">        &#123;_id : 1, host : <span class="string">"192.168.170.131:27003"</span> &#125;,</span><br><span class="line">        &#123;_id : 2, host : <span class="string">"192.168.170.132:27003"</span> &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#初始化副本集配置</span></span><br><span class="line">rs.initiate(config)</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看分区状态</span></span><br><span class="line">rs.status()</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------------------------------------------</span><br><span class="line">------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">------------------------------------------------------------------------------------------------</span><br><span class="line"><span class="comment">######注意：启动mongos是守候进程是因为/mongo/mongos/mongod.conf缺少了fork: true这个配置#######</span></span><br><span class="line">------------------------------------------------------------------------------------------------</span><br><span class="line">mkdir -p /mongo/mongos/&#123;<span class="built_in">log</span>,data,run&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#添加mongs的配置文件</span></span><br><span class="line">cat &gt;&gt; /mongo/mongos/mongod.conf &lt;&lt; EOF</span><br><span class="line">systemLog:</span><br><span class="line">  destination: file</span><br><span class="line">  logAppend: <span class="literal">true</span></span><br><span class="line">  path: /mongo/mongos/<span class="built_in">log</span>/mongod.log</span><br><span class="line">processManagement:</span><br><span class="line">  fork: <span class="literal">true</span></span><br><span class="line">  pidFilePath: /mongo/mongos/run/mongod.pid</span><br><span class="line">net:</span><br><span class="line">  port: 27200</span><br><span class="line">sharding:</span><br><span class="line">  configDB: config/192.168.170.131:27100,192.168.170.132:27100,192.168.170.133:27100</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">#注意，这里configDB后面的config要与配置服务器的_id保持一致</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#启动路由服务器</span></span><br><span class="line">mongos --config /mongo/mongos/mongod.conf</span><br><span class="line"></span><br><span class="line"><span class="comment">#登录其中的一台路由节点，手动启用分片</span></span><br><span class="line">mongo --port 27200</span><br><span class="line"></span><br><span class="line"><span class="comment">#添加分片到mongos</span></span><br><span class="line">sh.addShard(<span class="string">"shard1/192.168.170.131:27001,192.168.170.132:27001,192.168.170.133:27001"</span>)</span><br><span class="line">sh.addShard(<span class="string">"shard2/192.168.170.132:27002,192.168.170.133:27002,192.168.170.131:27002"</span>)</span><br><span class="line">sh.addShard(<span class="string">"shard3/192.168.170.133:27003,192.168.170.131:27003,192.168.170.132:27003"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置slave可读(在命令行中生效一次)，如果配置从接到可读，那么是连接客户端指定的</span></span><br><span class="line">rs.slaveOk()</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------------------------------------------</span><br><span class="line"><span class="comment">####没有分片是因为没有开启分片规则####################</span></span><br><span class="line">------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建mobike数据库</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#创建mobike数据库</span></span><br><span class="line">use admin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 admin数据库下 对 mobike 这个数据库开启分片功能</span></span><br><span class="line">db.runCommand(&#123;<span class="string">"enablesharding"</span>:<span class="string">"mobike"</span>&#125;) </span><br><span class="line"></span><br><span class="line"><span class="comment">#创建bikes集合</span></span><br><span class="line">db.createCollection(<span class="string">"bikes"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 在admin 数据库下,对bike数据库下的users集合按id的hash进行分片</span></span><br><span class="line">db.runCommand(&#123;<span class="string">"shardcollection"</span>:<span class="string">"mobike.bikes"</span>,<span class="string">"key"</span>:&#123;_id:<span class="string">'hashed'</span>&#125;&#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 插入数据</span></span><br><span class="line">db.bikes.insert(&#123;<span class="string">"bikeNo"</span> : <span class="string">"10001"</span>&#125;)</span><br><span class="line">.....</span><br><span class="line"></span><br><span class="line">================================================================================</span><br><span class="line"></span><br><span class="line"><span class="comment">#启动所有的config server</span></span><br><span class="line">mongod --config /mongo/config/mongod.conf</span><br><span class="line"><span class="comment">#启动所有的shard1</span></span><br><span class="line">mongod --config /mongo/shard1/mongod.conf</span><br><span class="line"><span class="comment">#启动所有的shard2</span></span><br><span class="line">mongod --config /mongo/shard2/mongod.conf</span><br><span class="line"><span class="comment">#启动所有的shard3</span></span><br><span class="line">mongod --config /mongo/shard3/mongod.conf</span><br><span class="line"><span class="comment">#启动所有的mongos</span></span><br><span class="line">mongos --config /mongo/mongos/mongod.conf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#关闭服务</span></span><br><span class="line">mongod --shutdown --dbpath /mongo/shard3/data</span><br><span class="line">mongod --shutdown --dbpath /mongo/shard2/data</span><br><span class="line">mongod --shutdown --dbpath /mongo/shard1/data</span><br><span class="line">mongod --shutdown --dbpath /mongo/config/data</span><br></pre></td></tr></table></figure><h1 id="四-Mycat-MySQL-集群-安装"><a href="#四-Mycat-MySQL-集群-安装" class="headerlink" title="四. Mycat (MySQL 集群) 安装"></a>四. Mycat (MySQL 集群) 安装</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">####################################</span></span><br><span class="line"><span class="comment"># 安装 MySQL</span></span><br><span class="line"><span class="comment">####################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#首先在node-4、node-5、node-6上安装MySQL</span></span><br><span class="line"><span class="comment">#配置MySQL 5.7的yum源</span></span><br><span class="line"></span><br><span class="line">sudo tee -a /etc/yum.repos.d/mysql-community.repo &lt;&lt; EOF</span><br><span class="line">[mysql57-community]</span><br><span class="line">name=MySQL 5.7 Community Server</span><br><span class="line">baseurl=http://repo.mysql.com/yum/mysql-5.7-community/el/6/\<span class="variable">$basearch</span>/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看mysql源的信息</span></span><br><span class="line">yum repolist enabled | grep mysql</span><br><span class="line"></span><br><span class="line"><span class="comment">#安装mysql的server</span></span><br><span class="line">sudo yum install -y mysql-community-server</span><br><span class="line"></span><br><span class="line"><span class="comment">#启动mysql</span></span><br><span class="line">sudo service mysqld start</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取启动日志中的默认初始密码</span></span><br><span class="line"><span class="comment">#sudo grep 'temporary password' /var/log/mysqld.log</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#获取密码并赋给一个变量</span></span><br><span class="line">PASSWORD=`sudo grep <span class="string">'temporary password'</span> /var/<span class="built_in">log</span>/mysqld.log | awk <span class="string">'&#123;print $NF&#125;'</span>` </span><br><span class="line"></span><br><span class="line"><span class="comment">#使用root用户登录</span></span><br><span class="line">mysql -uroot -p<span class="variable">$PASSWORD</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#修改root用户的密码</span></span><br><span class="line">ALTER USER <span class="string">'root'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'XiaoNiu_123!'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">#修改mysql远程登录权限</span></span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO <span class="string">'root'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'XiaoNiu_123!'</span> WITH GRANT OPTION;</span><br><span class="line">FLUSH PRIVILEGES; </span><br><span class="line"></span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line"><span class="comment">####################################</span></span><br><span class="line"><span class="comment"># 安装 Mycat</span></span><br><span class="line"><span class="comment">####################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#然后在node-3安装JDK并配置环境变量</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#然后在node-3安装mycat</span></span><br><span class="line"><span class="comment">#上传Mycat-server-1.6.5-release-20171008170112-linux.tar.gz安装包</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#修改conf目录下主要以下三个注意配置文件</span></span><br><span class="line">server.xml是Mycat服务器参数调整和用户授权的配置文件</span><br><span class="line">schema.xml是逻辑库定义和表以及分片定义的配置文件</span><br><span class="line">rule.xml是分片规则的配置文件</span><br><span class="line"></span><br><span class="line"><span class="comment">#修改server.xml(修改了mycat的用户和逻辑的database)</span></span><br><span class="line"></span><br><span class="line">&lt;user name=<span class="string">"xiaoniu"</span> defaultAccount=<span class="string">"true"</span>&gt;</span><br><span class="line">        &lt;property name=<span class="string">"password"</span>&gt;123456&lt;/property&gt;</span><br><span class="line">        &lt;property name=<span class="string">"schemas"</span>&gt;bigdata&lt;/property&gt;</span><br><span class="line">&lt;/user&gt;</span><br><span class="line"></span><br><span class="line">&lt;user name=<span class="string">"user"</span>&gt;</span><br><span class="line">        &lt;property name=<span class="string">"password"</span>&gt;user&lt;/property&gt;</span><br><span class="line">        &lt;property name=<span class="string">"schemas"</span>&gt;bigdata&lt;/property&gt;</span><br><span class="line">        &lt;property name=<span class="string">"readOnly"</span>&gt;<span class="literal">true</span>&lt;/property&gt;</span><br><span class="line">&lt;/user&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#修改schema.xml（配置逻辑库下的逻辑表，已经数据存放的mysql节点）</span></span><br><span class="line">    &lt;schema name=<span class="string">"bigdata"</span> checkSQLschema=<span class="string">"false"</span> sqlMaxLimit=<span class="string">"100"</span>&gt;</span><br><span class="line">        &lt;!-- auto sharding by id (long) --&gt;</span><br><span class="line">        &lt;table name=<span class="string">"travelrecord"</span> dataNode=<span class="string">"dn1,dn2,dn3"</span> rule=<span class="string">"auto-sharding-long"</span> /&gt;</span><br><span class="line"></span><br><span class="line">        &lt;!-- global table is auto cloned to all defined data nodes ,so can join</span><br><span class="line">            with any table whose sharding node is <span class="keyword">in</span> the same data node --&gt;</span><br><span class="line">        &lt;table name=<span class="string">"company"</span> primaryKey=<span class="string">"ID"</span> <span class="built_in">type</span>=<span class="string">"global"</span> dataNode=<span class="string">"dn1,dn2,dn3"</span> /&gt;</span><br><span class="line"></span><br><span class="line">        &lt;!-- random sharding using mod sharind rule --&gt;</span><br><span class="line">        &lt;table name=<span class="string">"hotnews"</span> primaryKey=<span class="string">"ID"</span> autoIncrement=<span class="string">"true"</span> dataNode=<span class="string">"dn1,dn2,dn3"</span></span><br><span class="line">               rule=<span class="string">"mod-long"</span> /&gt;</span><br><span class="line">    &lt;/schema&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dataNode name=<span class="string">"dn1"</span> dataHost=<span class="string">"cs4"</span> database=<span class="string">"db1"</span> /&gt;</span><br><span class="line">    &lt;dataNode name=<span class="string">"dn2"</span> dataHost=<span class="string">"cs5"</span> database=<span class="string">"db2"</span> /&gt;</span><br><span class="line">    &lt;dataNode name=<span class="string">"dn3"</span> dataHost=<span class="string">"cs6"</span> database=<span class="string">"db3"</span> /&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dataHost name=<span class="string">"cs4"</span> maxCon=<span class="string">"1000"</span> minCon=<span class="string">"10"</span> balance=<span class="string">"0"</span></span><br><span class="line">              writeType=<span class="string">"0"</span> dbType=<span class="string">"mysql"</span> dbDriver=<span class="string">"native"</span> switchType=<span class="string">"1"</span>  slaveThreshold=<span class="string">"100"</span>&gt;</span><br><span class="line">        &lt;heartbeat&gt;select user()&lt;/heartbeat&gt;</span><br><span class="line">        &lt;!-- can have multi write hosts --&gt;</span><br><span class="line">        &lt;writeHost host=<span class="string">"hostM1"</span> url=<span class="string">"192.168.170.134:3306"</span> user=<span class="string">"root"</span> password=<span class="string">"XiaoNiu_123!"</span>&gt;</span><br><span class="line">        &lt;/writeHost&gt;</span><br><span class="line">    &lt;/dataHost&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dataHost name=<span class="string">"cs5"</span> maxCon=<span class="string">"1000"</span> minCon=<span class="string">"10"</span> balance=<span class="string">"0"</span></span><br><span class="line">              writeType=<span class="string">"0"</span> dbType=<span class="string">"mysql"</span> dbDriver=<span class="string">"native"</span> switchType=<span class="string">"1"</span>  slaveThreshold=<span class="string">"100"</span>&gt;</span><br><span class="line">        &lt;heartbeat&gt;select user()&lt;/heartbeat&gt;</span><br><span class="line">        &lt;!-- can have multi write hosts --&gt;</span><br><span class="line">        &lt;writeHost host=<span class="string">"hostM1"</span> url=<span class="string">"192.168.170.135:3306"</span> user=<span class="string">"root"</span> password=<span class="string">"XiaoNiu_123!"</span>&gt;</span><br><span class="line">        &lt;/writeHost&gt;</span><br><span class="line">    &lt;/dataHost&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dataHost name=<span class="string">"cs6"</span> maxCon=<span class="string">"1000"</span> minCon=<span class="string">"10"</span> balance=<span class="string">"0"</span></span><br><span class="line">              writeType=<span class="string">"0"</span> dbType=<span class="string">"mysql"</span> dbDriver=<span class="string">"native"</span> switchType=<span class="string">"1"</span>  slaveThreshold=<span class="string">"100"</span>&gt;</span><br><span class="line">        &lt;heartbeat&gt;select user()&lt;/heartbeat&gt;</span><br><span class="line">        &lt;!-- can have multi write hosts --&gt;</span><br><span class="line">        &lt;writeHost host=<span class="string">"hostM1"</span> url=<span class="string">"192.168.170.136:3306"</span> user=<span class="string">"root"</span> password=<span class="string">"XiaoNiu_123!"</span>&gt;</span><br><span class="line">        &lt;/writeHost&gt;</span><br><span class="line">    &lt;/dataHost&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment">#在三台mysql上分别创建数据库db1、db2、db3</span></span><br><span class="line"><span class="comment">#然后在每一个database中创建表，有三张（travelrecord、company、hotnews）注意主键的名称, 注意不要指定主键自增, 主键自增是 Mycat 维护的</span></span><br><span class="line"></span><br><span class="line">CREATE TABLE `company` (</span><br><span class="line">`id` BIGINT(20) NOT NULL,</span><br><span class="line">`name` VARCHAR(50),</span><br><span class="line">PRIMARY KEY(`id`)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">CREATE TABLE `hotnews` (</span><br><span class="line">`id` BIGINT(20) NOT NULL,</span><br><span class="line">`name` VARCHAR(50),</span><br><span class="line">PRIMARY KEY(`id`)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">CREATE TABLE `travelrecord` (</span><br><span class="line">`id` BIGINT(20) NOT NULL,</span><br><span class="line">`name` VARCHAR(50),</span><br><span class="line">PRIMARY KEY(`id`)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">#在node-4上启动mycat</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># auto-sharding-long 默认一个节点存500万</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"></span><br><span class="line">mysql -h 192.168.10.104 -P 8066 -u root -p123456</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;注意点: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;创建用户名, 密码前, 要先选择对应的库,  用户&amp;amp;密码 是对应的库的&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;一-MongoDB-简介&quot;&gt;&lt;a href=&quot;#一-MongoDB-简介&quot; class=&quot;headerlink&quot; ti
      
    
    </summary>
    
      <category term="MongoDB" scheme="https://airpoet.github.io/categories/MongoDB/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="MongoDB" scheme="https://airpoet.github.io/tags/MongoDB/"/>
    
  </entry>
  
  <entry>
    <title>SpringBoot</title>
    <link href="https://airpoet.github.io/2018/08/15/Java/SpringBoot/SpringBoot/"/>
    <id>https://airpoet.github.io/2018/08/15/Java/SpringBoot/SpringBoot/</id>
    <published>2018-08-15T13:34:53.433Z</published>
    <updated>2018-08-17T02:43:43.551Z</updated>
    
    <content type="html"><![CDATA[<h1 id="创建方式"><a href="#创建方式" class="headerlink" title="创建方式"></a>创建方式</h1><h4 id="在网站中创建-http-start-spring-io-然后-down-下来"><a href="#在网站中创建-http-start-spring-io-然后-down-下来" class="headerlink" title="在网站中创建 http://start.spring.io/, 然后 down 下来"></a>在网站中创建 <a href="http://start.spring.io/" target="_blank" rel="noopener">http://start.spring.io/</a>, 然后 down 下来</h4><h4 id="直接使用-idea-创建"><a href="#直接使用-idea-创建" class="headerlink" title="直接使用 idea 创建"></a>直接使用 idea 创建</h4><h1 id="错误记录"><a href="#错误记录" class="headerlink" title="错误记录"></a>错误记录</h1><p>Consider defining a bean of type ‘com.rox.bike.mapper.BikeMapper’ in your configuration.</p><p><a href="https://www.cnblogs.com/waterystone/p/6214212.html" target="_blank" rel="noopener">解决方案参考</a></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-16-085754.png" alt="image-20180816165754112"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;创建方式&quot;&gt;&lt;a href=&quot;#创建方式&quot; class=&quot;headerlink&quot; title=&quot;创建方式&quot;&gt;&lt;/a&gt;创建方式&lt;/h1&gt;&lt;h4 id=&quot;在网站中创建-http-start-spring-io-然后-down-下来&quot;&gt;&lt;a href=&quot;#在网站中创建-
      
    
    </summary>
    
      <category term="Java" scheme="https://airpoet.github.io/categories/Java/"/>
    
      <category term="SprintBoot" scheme="https://airpoet.github.io/categories/Java/SprintBoot/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Java" scheme="https://airpoet.github.io/tags/Java/"/>
    
      <category term="SprintBoot" scheme="https://airpoet.github.io/tags/SprintBoot/"/>
    
  </entry>
  
  <entry>
    <title>ElasticSearch</title>
    <link href="https://airpoet.github.io/2018/08/12/ES/ElasticSearch/"/>
    <id>https://airpoet.github.io/2018/08/12/ES/ElasticSearch/</id>
    <published>2018-08-12T10:01:07.972Z</published>
    <updated>2018-08-25T15:29:22.479Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一-Lucene"><a href="#一-Lucene" class="headerlink" title="一. Lucene"></a>一. Lucene</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-14-lucene%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5.png" alt=""></p><p><strong>查询总结:</strong></p><ul><li>Term 查询(传入new Term()对象), 不分词, 严格匹配内容</li><li>分词查询</li><li>多字段查询</li><li>Boolean 查询</li><li>范围查询 (数字类型)</li><li>确切值查询 (数字类型)</li><li>查询所有</li></ul><p><strong>索引 和 Document 都存在磁盘</strong></p><p><a href="https://github.com/airpoet/bigdata/tree/master/ElasticSearchProj/LuceneDemo/src/main/java/com/rox/lucene" target="_blank" rel="noopener">代码见我的 github</a></p><h1 id="二-Nginx-整合KeepAlive"><a href="#二-Nginx-整合KeepAlive" class="headerlink" title="二. Nginx 整合KeepAlive"></a>二. Nginx 整合KeepAlive</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-13-073414.png" alt="image-20180813153414201"></p><h1 id="三-ECharts"><a href="#三-ECharts" class="headerlink" title="三. ECharts"></a>三. ECharts</h1><p>散点图, 堆叠区域图, 饼图, 热力图</p><ol><li><p>登录官网 <a href="http://echarts.baidu.com/" target="_blank" rel="noopener">http://echarts.baidu.com/</a></p></li><li><p>查看<code>实例-&gt;官方实例-&gt;随便看看</code></p></li><li><p>查看 <code>文档-&gt; 教程</code> <a href="http://echarts.baidu.com/tutorial.html#5%20%E5%88%86%E9%92%9F%E4%B8%8A%E6%89%8B%20ECharts" target="_blank" rel="noopener">http://echarts.baidu.com/tutorial.html#5%20%E5%88%86%E9%92%9F%E4%B8%8A%E6%89%8B%20ECharts</a></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">1. 引入 Echarts, 创建 html 文件 hello.html</span><br><span class="line">    <span class="meta">&lt;!DOCTYPE html&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"utf-8"</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 引入 ECharts 文件 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"echarts.min.js"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br><span class="line"></span><br><span class="line">2. 下载 echarts.min.js 或其他版本 http://echarts.baidu.com/download.html,  放在 hello.html 的同级目录下</span><br><span class="line">3. 创建容器</span><br><span class="line">    <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 为 ECharts 准备一个具备大小（宽高）的 DOM --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"main"</span> <span class="attr">style</span>=<span class="string">"width: 600px;height:400px;"</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"></span><br><span class="line">4. 通过 echarts.init 方法初始化一个 echarts 实例并通过 setOption 方法, 生成任何想要生成的内容</span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span>&gt;</span><span class="undefined"></span></span><br><span class="line"><span class="javascript">        <span class="comment">// 基于准备好的dom，初始化echarts实例</span></span></span><br><span class="line"><span class="javascript">        <span class="keyword">var</span> myChart = echarts.init(<span class="built_in">document</span>.getElementById(<span class="string">'main'</span>));</span></span><br><span class="line"><span class="undefined"></span></span><br><span class="line"><span class="javascript">        <span class="comment">// 指定图表的配置项和数据</span></span></span><br><span class="line"><span class="javascript">        <span class="keyword">var</span> option = &#123;</span></span><br><span class="line"><span class="undefined">           ......</span></span><br><span class="line"><span class="undefined">        &#125;;</span></span><br><span class="line"><span class="undefined"></span></span><br><span class="line"><span class="javascript">        <span class="comment">// 使用刚指定的配置项和数据显示图表。</span></span></span><br><span class="line"><span class="undefined">        myChart.setOption(option);</span></span><br><span class="line"><span class="undefined">    </span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol><h1 id="四-热力图"><a href="#四-热力图" class="headerlink" title="四. 热力图"></a>四. 热力图</h1><p><a href="http://lbsyun.baidu.com/jsdemo.htm#c1_15" target="_blank" rel="noopener">参考百度地图官方热力图案例</a></p><ul><li>可以直接使用百度开放平台的热力图, 申请 ak(秘钥) (accesskey)</li><li>把 ak 放到百度的 js 的后面的参数</li><li>加入一个热力图的 js</li><li>其它的参考官方案例慢慢调</li></ul><h1 id="五-ElasticSearch"><a href="#五-ElasticSearch" class="headerlink" title="五. ElasticSearch"></a>五. ElasticSearch</h1><p>版本: 5.4.3,  june27,2017</p><h4 id="CentOS7-命令"><a href="#CentOS7-命令" class="headerlink" title="CentOS7 命令"></a>CentOS7 命令</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看服务状态</span></span><br><span class="line">systemctl status NetworkManager / network</span><br><span class="line"><span class="comment">#停止服务</span></span><br><span class="line">systemctl stop NetworkManager</span><br><span class="line"><span class="comment">#启动服务</span></span><br><span class="line">systemctl start NetworkManager</span><br><span class="line"><span class="comment">#禁止服务开机启动</span></span><br><span class="line">systemctl <span class="built_in">disable</span> NetworkManager</span><br><span class="line"><span class="comment">#设置服务开机启动</span></span><br><span class="line">systemctl <span class="built_in">enable</span> NetworkManager</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看网络信息</span></span><br><span class="line">ip addr</span><br><span class="line"><span class="comment">#查看具体某个网卡信息</span></span><br><span class="line">ip addr ls eno16777736</span><br><span class="line"></span><br><span class="line"><span class="comment">#停止网卡</span></span><br><span class="line">ip link <span class="built_in">set</span> eno16777736 down</span><br><span class="line"><span class="comment">#启动网卡</span></span><br><span class="line">ip link <span class="built_in">set</span> eno16777736 up</span><br><span class="line"></span><br><span class="line"><span class="comment">#修改ip地址</span></span><br><span class="line">ip addr del 192.168.80.134/24 dev eno16777736</span><br><span class="line">ip addr add 192.168.80.136/24 dev eno16777736</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看路由信息</span></span><br><span class="line">ip route show</span><br><span class="line"><span class="comment">#添加路由</span></span><br><span class="line">ip route add default via 192.168.80.2 dev br0</span><br><span class="line"></span><br><span class="line"><span class="comment">#安装ifconfig命令</span></span><br><span class="line">yum install -y net-tools</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#修改主机名</span></span><br><span class="line">hostnamectl <span class="built_in">set</span>-hostname node-1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#防火墙配置:Centos升级到7之后，发现无法使用iptables控制Linux的端口，google之后发现Centos 7使用firewalld代替了原来的iptables。下面记录如何使用firewalld开放Linux端口： </span></span><br><span class="line"></span><br><span class="line"><span class="comment">#开启防火墙</span></span><br><span class="line">systemctl start firewalld</span><br><span class="line"></span><br><span class="line"><span class="comment">#开启端口</span></span><br><span class="line">firewall-cmd --zone=public --add-port=08/tcp --permanent</span><br><span class="line"> </span><br><span class="line">命令含义：</span><br><span class="line">--zone <span class="comment">#作用域</span></span><br><span class="line">--add-port=80/tcp  <span class="comment">#添加端口，格式为：端口/通讯协议</span></span><br><span class="line">--permanent   <span class="comment">#永久生效，没有此参数重启后失效</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#重新加载防火墙配置</span></span><br><span class="line">firewall-cmd --reload</span><br></pre></td></tr></table></figure><h4 id="安装单机版"><a href="#安装单机版" class="headerlink" title="安装单机版"></a>安装单机版</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">0. 使用 centos7</span><br><span class="line">1. 安装JDK（1.8）</span><br><span class="line">2. 上传解压Elasticsearch-5.4.3</span><br><span class="line">3. 创建一个普通用户，然后将对于的目录修改为普通用户的所属用户和所属组</span><br><span class="line">4. 修改配置文件config/elasticsearch.yml 的 network.host 为安装es 主机的 host</span><br><span class="line">network.host: 192.168.170.171<span class="comment">#修改</span></span><br><span class="line">transport.tcp.port: 9300    <span class="comment">#添加</span></span><br><span class="line">discovery.zen.ping.unicast.hosts: [<span class="string">"192.168.170.171:9300"</span>]<span class="comment">#修改</span></span><br><span class="line"></span><br><span class="line">5. 启动ES，发现报错</span><br><span class="line">bin/elasticsearch</span><br><span class="line"><span class="comment">#出现错误</span></span><br><span class="line">[1]: max file descriptors [4096] <span class="keyword">for</span> elasticsearch process is too low, increase to at least [65536]</span><br><span class="line">[2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]</span><br><span class="line"></span><br><span class="line"><span class="comment">#用户最大可创建文件数太小</span></span><br><span class="line">sudo vi /etc/security/limits.conf</span><br><span class="line">* soft nofile 65536</span><br><span class="line">* hard nofile 65536</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看可打开文件数量</span></span><br><span class="line"><span class="built_in">ulimit</span> -Hn</span><br><span class="line"></span><br><span class="line"><span class="comment">#最大虚拟内存太小</span></span><br><span class="line">sudo vi /etc/sysctl.conf </span><br><span class="line">vm.max_map_count=262144</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看虚拟内存的大小</span></span><br><span class="line">sudo sysctl -p</span><br><span class="line"></span><br><span class="line">6. 重启linux</span><br><span class="line">shutdown -r now</span><br><span class="line">关机</span><br><span class="line">    shutdown -h now</span><br><span class="line"></span><br><span class="line">7. 通过浏览器访问ES</span><br><span class="line">192.168.170.171:9200</span><br></pre></td></tr></table></figure><h4 id="安装集群"><a href="#安装集群" class="headerlink" title="安装集群"></a>安装集群</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line">1.安装jdk（jdk要求1.8.20以上）</span><br><span class="line"></span><br><span class="line">2.上传es安装包</span><br><span class="line"></span><br><span class="line">3.解压es</span><br><span class="line">tar -zxvf elasticsearch-5.4.3.tar.gz -C apps/</span><br><span class="line"></span><br><span class="line">4.修改配置</span><br><span class="line">vi aps/es/config/elasticsearch.yml</span><br><span class="line"></span><br><span class="line"><span class="comment">#集群名称，通过组播的方式通信，通过名称判断属于哪个集群</span></span><br><span class="line">cluster.name: my-es</span><br><span class="line"></span><br><span class="line"><span class="comment">#节点名称，要唯一</span></span><br><span class="line">node.name: node-1</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据存放位置</span></span><br><span class="line">path.data: /home/ap/data/es/data</span><br><span class="line"></span><br><span class="line"><span class="comment">#日志存放位置(可选)</span></span><br><span class="line">path.logs: /home/ap/data/es/logs</span><br><span class="line"></span><br><span class="line"><span class="comment">#es绑定的ip地址</span></span><br><span class="line">network.host: cs7_1</span><br><span class="line"></span><br><span class="line"><span class="comment">#es默认端口</span></span><br><span class="line">http.port: 9200</span><br><span class="line">transport.tcp.port: 9300</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化时可进行选举的节点</span></span><br><span class="line">discovery.zen.ping.unicast.hosts: [<span class="string">"cs7_1:9300"</span>,<span class="string">"cs7_2:9300"</span>,<span class="string">"cs7_3:9300"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#后台启动</span></span><br><span class="line">$~&gt; elasticsearch/bin/elasticsearch -d</span><br><span class="line"></span><br><span class="line">5.使用scp拷贝到其他节点</span><br><span class="line">scp elasticsearch.yml cs7_2:<span class="variable">$PWD</span></span><br><span class="line">scp elasticsearch.yml cs7_3:<span class="variable">$PWD</span></span><br><span class="line"></span><br><span class="line">6.在其他节点上修改es配置，需要修改的有node.name和network.host</span><br><span class="line"></span><br><span class="line">7.启动es（elasticsearch/bin/elasticsearch -h查看帮助文档） </span><br><span class="line">$~&gt; apps/es/bin/elasticsearch -d</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">8.用浏览器访问es所在机器的9200端口</span><br><span class="line">http://192.168.170.171:9200/ 或者 http://cs7_1:9200 </span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"name"</span> : <span class="string">"node-1"</span>,</span><br><span class="line">  <span class="string">"cluster_name"</span> : <span class="string">"my-es"</span>,</span><br><span class="line">  <span class="string">"cluster_uuid"</span> : <span class="string">"9jgURtc7TOejQnumbZt7VA"</span>,</span><br><span class="line">  <span class="string">"version"</span> : &#123;</span><br><span class="line">    <span class="string">"number"</span> : <span class="string">"5.4.3"</span>,</span><br><span class="line">    <span class="string">"build_hash"</span> : <span class="string">"eed30a8"</span>,</span><br><span class="line">    <span class="string">"build_date"</span> : <span class="string">"2017-06-22T00:34:03.743Z"</span>,</span><br><span class="line">    <span class="string">"build_snapshot"</span> : <span class="literal">false</span>,</span><br><span class="line">    <span class="string">"lucene_version"</span> : <span class="string">"6.5.1"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"tagline"</span> : <span class="string">"You Know, for Search"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 杀死 ES 进程, 关闭进程</span></span><br><span class="line"><span class="comment"># 后面是查出进程号, 相当于 kill xxx</span></span><br><span class="line"><span class="built_in">kill</span> `ps -ef | grep Elasticsearch | grep -v grep | awk <span class="string">'&#123;print $2&#125;'</span>`</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看集群状态</span></span><br><span class="line">curl -XGET <span class="string">'http://cs7_1:9200/_cluster/health?pretty'</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"cluster_name"</span> : <span class="string">"my-es"</span>,</span><br><span class="line">  <span class="string">"status"</span> : <span class="string">"green"</span>,</span><br><span class="line">  <span class="string">"timed_out"</span> : <span class="literal">false</span>,</span><br><span class="line">  <span class="string">"number_of_nodes"</span> : 3,</span><br><span class="line">  <span class="string">"number_of_data_nodes"</span> : 3,</span><br><span class="line">  <span class="string">"active_primary_shards"</span> : 0,</span><br><span class="line">  <span class="string">"active_shards"</span> : 0,</span><br><span class="line">  <span class="string">"relocating_shards"</span> : 0,</span><br><span class="line">  <span class="string">"initializing_shards"</span> : 0,</span><br><span class="line">  <span class="string">"unassigned_shards"</span> : 0,</span><br><span class="line">  <span class="string">"delayed_unassigned_shards"</span> : 0,</span><br><span class="line">  <span class="string">"number_of_pending_tasks"</span> : 0,</span><br><span class="line">  <span class="string">"number_of_in_flight_fetch"</span> : 0,</span><br><span class="line">  <span class="string">"task_max_waiting_in_queue_millis"</span> : 0,</span><br><span class="line">  <span class="string">"active_shards_percent_as_number"</span> : 100.0</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 浏览器中查看集群状态 - json</span></span><br><span class="line">http://cs7_1:9200/_cluster/health?pretty</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--------------------------------------------</span><br><span class="line"><span class="comment">###注意: 此错误已在上面安装单机的过程中解决掉</span></span><br><span class="line"><span class="comment">#出现错误 </span></span><br><span class="line">[1]: max file descriptors [4096] <span class="keyword">for</span> elasticsearch process is too low, increase to at least [65536]</span><br><span class="line">[2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]</span><br><span class="line"></span><br><span class="line"><span class="comment">#用户最大可创建文件数太小</span></span><br><span class="line">sudo vi /etc/security/limits.conf</span><br><span class="line">* soft nofile 65536</span><br><span class="line">* hard nofile 65536</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看可打开文件数量(重启后查询)</span></span><br><span class="line"><span class="built_in">ulimit</span> -Hn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#最大虚拟内存太小</span></span><br><span class="line">sudo vi /etc/sysctl.conf </span><br><span class="line">vm.max_map_count=262144</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看虚拟内存的大小</span></span><br><span class="line">sudo sysctl -p</span><br><span class="line">------------</span><br></pre></td></tr></table></figure><h4 id="ES-插件-ElasticSearch-Header-安装"><a href="#ES-插件-ElasticSearch-Header-安装" class="headerlink" title="ES 插件 ElasticSearch-Header 安装"></a>ES 插件 ElasticSearch-Header 安装</h4><p>9100端口</p><p>如果访问不了, 可以先查看端口, 通过端口查到进程, kill 掉, 再重启 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum install -y lsof</span><br><span class="line">lsof -i tcp:9100</span><br><span class="line"><span class="comment"># 找到 pid 后, kill 掉, 再重新启动</span></span><br></pre></td></tr></table></figure><p><a href="http://blog.csdn.net/napoay/article/details/53896348" target="_blank" rel="noopener">参考博客</a></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-15-013733.png" alt="image-20180815093732538"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">0&gt; yum 源首先改为国内的</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#更新</span></span><br><span class="line">sudo yum update -y</span><br><span class="line"></span><br><span class="line"><span class="comment">#安装扩展源</span></span><br><span class="line">sudo rpm -ivh http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm</span><br><span class="line">sudo rpm -ivh https://kojipkgs.fedoraproject.org//packages/http-parser/2.7.1/3.el7/x86_64/http-parser-2.7.1-3.el7.x86_64.rpm</span><br><span class="line"></span><br><span class="line"><span class="comment">#安装 js 的包管理工具 npm</span></span><br><span class="line">sudo yum install-y npm</span><br><span class="line"></span><br><span class="line"><span class="comment">#安装 git</span></span><br><span class="line">sudo yum install -y git</span><br><span class="line"></span><br><span class="line"><span class="comment">#安装 bzip2</span></span><br><span class="line">sudo yum install -y bzip2</span><br><span class="line"></span><br><span class="line"><span class="comment">#下载 elasticsearch-head</span></span><br><span class="line">git <span class="built_in">clone</span> git://github.com/mobz/elasticsearch-head.git</span><br><span class="line"></span><br><span class="line"><span class="comment">#将源码包下载后剪切到/bigdata目录，并改所属用户和组</span></span><br><span class="line">sudo chown -R ap:ap /home/ap/apps/elasticsearch-head</span><br><span class="line"></span><br><span class="line"><span class="comment">#进入到elasticsearch-head中</span></span><br><span class="line"><span class="built_in">cd</span> elasticsearch-head</span><br><span class="line"></span><br><span class="line"><span class="comment">#编译安装</span></span><br><span class="line">npm install</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">打开elasticsearch-head-master/Gruntfile.js，找到下面connect属性，新增hostname: <span class="string">'0.0.0.0'</span>,</span><br><span class="line">connect: &#123;</span><br><span class="line">                        server: &#123;</span><br><span class="line">                                options: &#123;</span><br><span class="line">                                        hostname: <span class="string">'0.0.0.0'</span>,</span><br><span class="line">                                        port: 9100,</span><br><span class="line">                                        base: <span class="string">'.'</span>,</span><br><span class="line">                                        keepalive: <span class="literal">true</span></span><br><span class="line">                                &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">编辑elasticsearch-5.4.3/config/elasticsearch.yml,加入以下内容：</span><br><span class="line">http.cors.enabled: <span class="literal">true</span></span><br><span class="line">http.cors.allow-origin: <span class="string">"*"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#运行服务</span></span><br><span class="line">npm run start</span><br></pre></td></tr></table></figure><h4 id="安装-IK-分词器"><a href="#安装-IK-分词器" class="headerlink" title="安装 IK 分词器"></a>安装 <code>IK</code> 分词器</h4><p><strong>github 地址 <a href="https://github.com/medcl/elasticsearch-analysis-ik/releases" target="_blank" rel="noopener">https://github.com/medcl/elasticsearch-analysis-ik/releases</a></strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"># 安装 &amp; 测试</span><br><span class="line">安装IK分词器</span><br><span class="line">下载对应版本的插件</span><br><span class="line">https://github.com/medcl/elasticsearch-analysis-ik/releases</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">首先下载es对应版本的ik分词器的zip包，上传到es服务器上，在es的安装目录下有一个plugins的目录，在这个目录下创建一个叫ik的目录</span><br><span class="line">然后将解压好的内容，拷贝到ik目录</span><br><span class="line">将ik目录拷贝到其他的es节点</span><br><span class="line">重新启动所有的es</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#创建索引名字叫news</span><br><span class="line">curl -XPUT http://192.168.170.171:9200/news</span><br><span class="line"></span><br><span class="line">#创建mapping（相当于数据中的schema信息，表名和字段名以及字段的类型）</span><br><span class="line">curl -XPOST http://192.168.170.171:9200/news/fulltext/_mapping -d'</span><br><span class="line">&#123;</span><br><span class="line">        <span class="attr">"properties"</span>: &#123;</span><br><span class="line">            <span class="attr">"content"</span>: &#123;</span><br><span class="line">                <span class="attr">"type"</span>: <span class="string">"text"</span>,</span><br><span class="line">                <span class="attr">"analyzer"</span>: <span class="string">"ik_max_word"</span>,</span><br><span class="line">                <span class="attr">"search_analyzer"</span>: <span class="string">"ik_max_word"</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    </span><br><span class="line">&#125;'</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">curl -XPOST http://192.168.170.171:9200/news/fulltext/1 -d'</span><br><span class="line">&#123;"content":"美国留给伊拉克的是个烂摊子吗"&#125;'</span><br><span class="line"></span><br><span class="line">curl -XPOST http://192.168.170.171:9200/news/fulltext/2 -d'</span><br><span class="line">&#123;"content":"公安部：各地校车将享最高路权"&#125;'</span><br><span class="line"></span><br><span class="line">curl -XPOST http://192.168.170.171:9200/news/fulltext/3 -d'</span><br><span class="line">&#123;"content":"中韩渔警冲突调查：韩警平均每天扣1艘中国渔船"&#125;'</span><br><span class="line"></span><br><span class="line">curl -XPOST http://192.168.170.171:9200/news/fulltext/4 -d'</span><br><span class="line">&#123;"content":"中国驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首"&#125;'</span><br><span class="line"></span><br><span class="line">curl -XPOST http://192.168.170.171:9200/news/fulltext/_search  -d'</span><br><span class="line">&#123;</span><br><span class="line">    <span class="attr">"query"</span> : &#123; <span class="attr">"match"</span> : &#123; <span class="attr">"content"</span> : <span class="string">"中国"</span> &#125;&#125;,</span><br><span class="line">    <span class="attr">"highlight"</span> : &#123;</span><br><span class="line">        <span class="attr">"pre_tags"</span> : [<span class="string">"&lt;font color='red'&gt;"</span>, <span class="string">"&lt;tag2&gt;"</span>],</span><br><span class="line">        <span class="attr">"post_tags"</span> : [<span class="string">"&lt;/font&gt;"</span>, <span class="string">"&lt;/tag2&gt;"</span>],</span><br><span class="line">        <span class="attr">"fields"</span> : &#123;</span><br><span class="line">            <span class="attr">"content"</span> : &#123;&#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;'</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">curl -XGET 'http://192.168.170.171:9200/_analyze?pretty&amp;analyzer=ik_max_word' -d '联想是全球最大的笔记本厂商'</span><br><span class="line"></span><br><span class="line"># 分的更准, 建议使用</span><br><span class="line">curl -XGET 'http://192.168.170.171:9200/_analyze?pretty&amp;analyzer=ik_smart' -d '联想是全球最大的笔记本厂商'</span><br><span class="line"></span><br><span class="line">curl -XPUT 'https://192.168.170.171:9200/iktest?pretty' -d '&#123;</span><br><span class="line">    "settings" : &#123;</span><br><span class="line">        "analysis" : &#123;</span><br><span class="line">            "analyzer" : &#123;</span><br><span class="line">                "ik" : &#123;</span><br><span class="line">                    "tokenizer" : "ik_max_word"</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    "mappings" : &#123;</span><br><span class="line">        "article" : &#123;</span><br><span class="line">            # dynamic 为 true, 可以指定多个字段</span><br><span class="line">            "dynamic" : true,      </span><br><span class="line">            "properties" : &#123;</span><br><span class="line">                "subject" : &#123;</span><br><span class="line">                    "type" : "string",</span><br><span class="line">                    "analyzer" : "ik_max_word"</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;'</span><br><span class="line"></span><br><span class="line">curl -XPUT 'https://192.168.170.171:9200/iktest?pretty' -d '&#123;</span><br><span class="line">    "settings" : &#123;</span><br><span class="line">        "analysis" : &#123;</span><br><span class="line">            "analyzer" : &#123;</span><br><span class="line">                "ik" : &#123;</span><br><span class="line">                    "tokenizer" : "ik_max_word"</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    "mappings" : &#123;</span><br><span class="line">        "article" : &#123;</span><br><span class="line">            "dynamic" : true,</span><br><span class="line">            "properties" : &#123;</span><br><span class="line">                "subject" : &#123;</span><br><span class="line">                    "type" : "string",</span><br><span class="line">                    "analyzer" : "ik_max_word"</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;'</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">curl -XGET 'http://192.168.170.171:9200/_analyze?pretty&amp;analyzer=ik_max_word' -d ‘中华人民共和国’</span><br></pre></td></tr></table></figure><h4 id="ES-的-JavaAPI-和-聚合查询"><a href="#ES-的-JavaAPI-和-聚合查询" class="headerlink" title="ES 的 JavaAPI 和 聚合查询"></a>ES 的 JavaAPI 和 聚合查询</h4><p><a href="https://github.com/airpoet/bigdata/tree/master/ElasticSearchProj/HelloES" target="_blank" rel="noopener">可以参考我的 github</a></p><h5 id="1-helloworld"><a href="#1-helloworld" class="headerlink" title="1. helloworld"></a>1. helloworld</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.es;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.get.GetResponse;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.transport.TransportClient;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.common.settings.Settings;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.common.transport.InetSocketTransportAddress;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.transport.client.PreBuiltTransportClient;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.net.InetAddress;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HelloWorldES</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//设置集群名称</span></span><br><span class="line">            Settings settings = Settings.builder()</span><br><span class="line">                    .put(<span class="string">"cluster.name"</span>, <span class="string">"my-es"</span>)</span><br><span class="line">                    .build();</span><br><span class="line">            <span class="comment">//创建client</span></span><br><span class="line">            TransportClient client = <span class="keyword">new</span> PreBuiltTransportClient(settings).addTransportAddresses(</span><br><span class="line">                    <span class="comment">//用java访问ES用的端口是9300</span></span><br><span class="line">                    <span class="keyword">new</span> InetSocketTransportAddress(InetAddress.getByName(<span class="string">"192.168.170.171"</span>), <span class="number">9300</span>),</span><br><span class="line">                    <span class="keyword">new</span> InetSocketTransportAddress(InetAddress.getByName(<span class="string">"192.168.170.172"</span>), <span class="number">9300</span>),</span><br><span class="line">                    <span class="keyword">new</span> InetSocketTransportAddress(InetAddress.getByName(<span class="string">"192.168.170.173"</span>), <span class="number">9300</span>)</span><br><span class="line">            );</span><br><span class="line">            <span class="comment">//搜索数据（.actionGet()方法是同步的，没有返回就等待）</span></span><br><span class="line">            GetResponse response = client.prepareGet(<span class="string">"news"</span>, <span class="string">"fulltext"</span>, <span class="string">"1"</span>).execute().actionGet();</span><br><span class="line">            <span class="comment">//输出结果</span></span><br><span class="line">            System.out.println(response);</span><br><span class="line">            <span class="comment">//关闭client</span></span><br><span class="line">            client.close();</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="2-ES-的-CRUD-增删改查"><a href="#2-ES-的-CRUD-增删改查" class="headerlink" title="2. ES 的 CRUD 增删改查"></a>2. ES 的 CRUD 增删改查</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.es;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.ActionListener;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.bulk.byscroll.BulkByScrollResponse;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.delete.DeleteResponse;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.get.GetResponse;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.get.MultiGetItemResponse;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.get.MultiGetResponse;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.index.IndexResponse;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.search.SearchRequestBuilder;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.search.SearchResponse;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.update.UpdateRequest;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.transport.TransportClient;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.common.settings.Settings;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.common.transport.InetSocketTransportAddress;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.index.query.QueryBuilder;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.index.query.QueryBuilders;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.index.reindex.DeleteByQueryAction;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.SearchHit;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.SearchHitField;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.SearchHits;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.aggregations.Aggregation;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.aggregations.AggregationBuilders;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.aggregations.Aggregations;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.aggregations.bucket.terms.DoubleTerms;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.aggregations.bucket.terms.StringTerms;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.aggregations.bucket.terms.Terms;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.aggregations.bucket.terms.TermsAggregationBuilder;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.aggregations.metrics.avg.AvgAggregationBuilder;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.aggregations.metrics.avg.InternalAvg;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.aggregations.metrics.max.InternalMax;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.aggregations.metrics.max.MaxAggregationBuilder;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.aggregations.metrics.sum.InternalSum;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.aggregations.metrics.sum.SumAggregationBuilder;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.transport.client.PreBuiltTransportClient;</span><br><span class="line"><span class="keyword">import</span> org.junit.Before;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.InetAddress;</span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> java.util.Set;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;</span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.elasticsearch.index.query.QueryBuilders.rangeQuery;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.4/index.html</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EsCRUD</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> TransportClient client = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//设置集群名称</span></span><br><span class="line">        Settings settings = Settings.builder()</span><br><span class="line">                .put(<span class="string">"cluster.name"</span>, <span class="string">"my-es"</span>)</span><br><span class="line">                <span class="comment">//自动感知的功能（可以通过当前指定的节点获取所有es节点的信息）</span></span><br><span class="line">                .put(<span class="string">"client.transport.sniff"</span>, <span class="keyword">true</span>)</span><br><span class="line">                .build();</span><br><span class="line">        <span class="comment">//创建client</span></span><br><span class="line">        client = <span class="keyword">new</span> PreBuiltTransportClient(settings).addTransportAddresses(</span><br><span class="line">                <span class="keyword">new</span> InetSocketTransportAddress(InetAddress.getByName(<span class="string">"192.168.170.170"</span>), <span class="number">9300</span>),</span><br><span class="line">                <span class="keyword">new</span> InetSocketTransportAddress(InetAddress.getByName(<span class="string">"192.168.170.171"</span>), <span class="number">9300</span>),</span><br><span class="line">                <span class="keyword">new</span> InetSocketTransportAddress(InetAddress.getByName(<span class="string">"192.168.170.172"</span>), <span class="number">9300</span>));</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 新增</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCreate</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        IndexResponse response = client.prepareIndex(<span class="string">"gamelog"</span>, <span class="string">"users"</span>, <span class="string">"1"</span>)</span><br><span class="line">                .setSource(</span><br><span class="line">                        jsonBuilder()</span><br><span class="line">                                .startObject()</span><br><span class="line">                                    .field(<span class="string">"username"</span>, <span class="string">"老李"</span>)</span><br><span class="line">                                    .field(<span class="string">"gender"</span>, <span class="string">"male"</span>)</span><br><span class="line">                                    .field(<span class="string">"birthday"</span>, <span class="keyword">new</span> Date())</span><br><span class="line">                                    .field(<span class="string">"fv"</span>, <span class="number">999</span>)</span><br><span class="line">                                    .field(<span class="string">"message"</span>, <span class="string">"老李还行"</span>)</span><br><span class="line">                                .endObject()</span><br><span class="line">                ).get();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//查找一条</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testGet</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        GetResponse response = client.prepareGet(<span class="string">"gamelog"</span>, <span class="string">"users"</span>, <span class="string">"1"</span>).get();</span><br><span class="line">        System.out.println(response.getSourceAsString());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//查找多条</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testMultiGet</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        MultiGetResponse multiGetItemResponses = client.prepareMultiGet()</span><br><span class="line">                .add(<span class="string">"gamelog"</span>, <span class="string">"users"</span>, <span class="string">"1"</span>)</span><br><span class="line">                .add(<span class="string">"gamelog"</span>, <span class="string">"users"</span>, <span class="string">"2"</span>, <span class="string">"3"</span>)</span><br><span class="line">                .add(<span class="string">"news"</span>, <span class="string">"fulltext"</span>, <span class="string">"1"</span>)</span><br><span class="line">                .get();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (MultiGetItemResponse itemResponse : multiGetItemResponses) &#123;</span><br><span class="line">            GetResponse response = itemResponse.getResponse();</span><br><span class="line">            <span class="keyword">if</span> (response.isExists()) &#123;</span><br><span class="line">                String json = response.getSourceAsString();</span><br><span class="line">                System.out.println(json);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 更新 (改)</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testUpdate</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        UpdateRequest updateRequest = <span class="keyword">new</span> UpdateRequest();</span><br><span class="line">        updateRequest.index(<span class="string">"gamelog"</span>);</span><br><span class="line">        updateRequest.type(<span class="string">"users"</span>);</span><br><span class="line">        updateRequest.id(<span class="string">"2"</span>);</span><br><span class="line">        updateRequest.doc(</span><br><span class="line">                jsonBuilder()</span><br><span class="line">                        .startObject()</span><br><span class="line">                        .field(<span class="string">"fv"</span>, <span class="number">999.9</span>)</span><br><span class="line">                        .endObject());</span><br><span class="line">        client.update(updateRequest).get();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 删除</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testDelete</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        DeleteResponse response = client.prepareDelete(<span class="string">"gamelog"</span>, <span class="string">"users"</span>, <span class="string">"1"</span>).get();</span><br><span class="line">        System.out.println(response);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 查询出结果 -&gt; 删除</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testDeleteByQuery</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        BulkByScrollResponse response =</span><br><span class="line">                DeleteByQueryAction.INSTANCE.newRequestBuilder(client)</span><br><span class="line">                        <span class="comment">//指定查询条件</span></span><br><span class="line">                        .filter(QueryBuilders.matchQuery(<span class="string">"username"</span>, <span class="string">"老段"</span>))</span><br><span class="line">                        <span class="comment">//指定索引名称</span></span><br><span class="line">                        .source(<span class="string">"gamelog"</span>)</span><br><span class="line">                        .get();</span><br><span class="line">        <span class="keyword">long</span> deleted = response.getDeleted();</span><br><span class="line">        System.out.println(deleted);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//异步删除</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testDeleteByQueryAsync</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        DeleteByQueryAction.INSTANCE.newRequestBuilder(client)</span><br><span class="line">                .filter(QueryBuilders.matchQuery(<span class="string">"gender"</span>, <span class="string">"male"</span>))</span><br><span class="line">                .source(<span class="string">"gamelog"</span>)</span><br><span class="line">                .execute(<span class="keyword">new</span> ActionListener&lt;BulkByScrollResponse&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onResponse</span><span class="params">(BulkByScrollResponse response)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">long</span> deleted = response.getDeleted();</span><br><span class="line">                        System.out.println(<span class="string">"数据删除了"</span>);</span><br><span class="line">                        System.out.println(deleted);</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(Exception e)</span> </span>&#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">"异步删除"</span>);</span><br><span class="line">            Thread.sleep(<span class="number">10000</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 范围查询</span></span><br><span class="line"><span class="comment">     * 注意: 这里如果前面设置的是 doule 类型, 搜索的范围也是 double 类型</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testRange</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        QueryBuilder qb = rangeQuery(<span class="string">"fv"</span>)</span><br><span class="line">                <span class="comment">// [88.99, 10000)</span></span><br><span class="line">                .from(<span class="number">88.99</span>)</span><br><span class="line">                .to(<span class="number">100000</span>)</span><br><span class="line">                .includeLower(<span class="keyword">true</span>)</span><br><span class="line">                .includeUpper(<span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">        SearchResponse response = client.prepareSearch(<span class="string">"gamelog"</span>).setQuery(qb).get();</span><br><span class="line"></span><br><span class="line">        System.out.println(response);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * NBA相关查询</span></span><br><span class="line"><span class="comment">     * curl -XPUT 'http://192.168.5.251:9200/player_info/player/1' -d '&#123; "name": "curry", "age": 29, "salary": 3500,"team": "war", "position": "pg"&#125;'</span></span><br><span class="line"><span class="comment">     * curl -XPUT 'http://192.168.5.251:9200/player_info/player/2' -d '&#123; "name": "thompson", "age": 26, "salary": 2000,"team": "war", "position": "pg"&#125;'</span></span><br><span class="line"><span class="comment">     * curl -XPUT 'http://192.168.5.251:9200/player_info/player/3' -d '&#123; "name": "irving", "age": 25, "salary": 2000,"team": "cav", "position": "pg"&#125;'</span></span><br><span class="line"><span class="comment">     * curl -XPUT 'http://192.168.5.251:9200/player_info/player/4' -d '&#123; "name": "green", "age": 26, "salary": 2000,"team": "war", "position": "pf"&#125;'</span></span><br><span class="line"><span class="comment">     * curl -XPUT 'http://192.168.5.251:9200/player_info/player/5' -d '&#123; "name": "james", "age": 33, "salary": 4000,"team": "cav", "position": "sf"&#125;'</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建 index 和 type</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testAddPlayer</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        IndexResponse response = client.prepareIndex(<span class="string">"player_info"</span>, <span class="string">"player"</span>, <span class="string">"1"</span>)</span><br><span class="line">                .setSource(</span><br><span class="line">                        jsonBuilder()</span><br><span class="line">                                .startObject()</span><br><span class="line">                                .field(<span class="string">"name"</span>, <span class="string">"James"</span>)</span><br><span class="line">                                .field(<span class="string">"age"</span>, <span class="number">33</span>)</span><br><span class="line">                                .field(<span class="string">"salary"</span>, <span class="number">3000</span>)</span><br><span class="line">                                .field(<span class="string">"team"</span>, <span class="string">"cav"</span>)</span><br><span class="line">                                .field(<span class="string">"position"</span>, <span class="string">"sf"</span>)</span><br><span class="line">                                .endObject()</span><br><span class="line">                ).get();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 复杂条件查询</span></span><br><span class="line"><span class="comment">     * https://elasticsearch.cn/article/102</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * select team, count(*) as player_count from player group by team;</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testAgg1</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定索引和type</span></span><br><span class="line">        SearchRequestBuilder builder = client.prepareSearch(<span class="string">"player_info"</span>).setTypes(<span class="string">"player"</span>);</span><br><span class="line">        <span class="comment">//按team分组然后聚合，但是并没有指定聚合函数</span></span><br><span class="line">        TermsAggregationBuilder teamAgg = AggregationBuilders.terms(<span class="string">"player_count"</span>).field(<span class="string">"team"</span>);</span><br><span class="line">        <span class="comment">//添加聚合器</span></span><br><span class="line">        builder.addAggregation(teamAgg);</span><br><span class="line">        <span class="comment">//触发</span></span><br><span class="line">        SearchResponse response = builder.execute().actionGet();</span><br><span class="line">        <span class="comment">//System.out.println(response);</span></span><br><span class="line">        <span class="comment">//将返回的结果放入到一个map中</span></span><br><span class="line">        Map&lt;String, Aggregation&gt; aggMap = response.getAggregations().getAsMap();</span><br><span class="line"><span class="comment">//        Set&lt;String&gt; keys = aggMap.keySet();</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//        for (String key: keys) &#123;</span></span><br><span class="line"><span class="comment">//            System.out.println(key);</span></span><br><span class="line"><span class="comment">//        &#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//        //取出聚合属性</span></span><br><span class="line">        StringTerms terms = (StringTerms) aggMap.get(<span class="string">"player_count"</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//</span></span><br><span class="line"><span class="comment">////        //依次迭代出分组聚合数据</span></span><br><span class="line"><span class="comment">//        for (Terms.Bucket bucket : terms.getBuckets()) &#123;</span></span><br><span class="line"><span class="comment">//            //分组的名字</span></span><br><span class="line"><span class="comment">//            String team = (String) bucket.getKey();</span></span><br><span class="line"><span class="comment">//            //count，分组后一个组有多少数据</span></span><br><span class="line"><span class="comment">//            long count = bucket.getDocCount();</span></span><br><span class="line"><span class="comment">//            System.out.println(team + " " + count);</span></span><br><span class="line"><span class="comment">//        &#125;</span></span><br><span class="line"></span><br><span class="line">        Iterator&lt;Terms.Bucket&gt; teamBucketIt = terms.getBuckets().iterator();</span><br><span class="line">        <span class="keyword">while</span> (teamBucketIt .hasNext()) &#123;</span><br><span class="line">            Terms.Bucket bucket = teamBucketIt.next();</span><br><span class="line">            String team = (String) bucket.getKey();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">long</span> count = bucket.getDocCount();</span><br><span class="line"></span><br><span class="line">            System.out.println(team + <span class="string">" "</span> + count);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * select team, position, count(*) as pos_count from player group by team, position;</span></span><br><span class="line"><span class="comment">     * 先按 team, team 中再按 position 聚合</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testAgg2</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        SearchRequestBuilder builder = client.prepareSearch(<span class="string">"player_info"</span>).setTypes(<span class="string">"player"</span>);</span><br><span class="line">        <span class="comment">//指定别名和分组的字段</span></span><br><span class="line">        TermsAggregationBuilder teamAgg = AggregationBuilders.terms(<span class="string">"team_name"</span>).field(<span class="string">"team"</span>);</span><br><span class="line">        TermsAggregationBuilder posAgg= AggregationBuilders.terms(<span class="string">"pos_count"</span>).field(<span class="string">"position"</span>);</span><br><span class="line">        <span class="comment">//添加两个聚合构建器</span></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 注意: 此处是在 teamAgg 上, 添加 subAggregation(posAgg)</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        builder.addAggregation(teamAgg.subAggregation(posAgg));</span><br><span class="line">        <span class="comment">//执行查询</span></span><br><span class="line">        SearchResponse response = builder.execute().actionGet();</span><br><span class="line">        <span class="comment">//将查询结果放入map中</span></span><br><span class="line">        Map&lt;String, Aggregation&gt; aggMap = response.getAggregations().getAsMap();</span><br><span class="line">        <span class="comment">//根据属性名到map中查找</span></span><br><span class="line">        StringTerms teams = (StringTerms) aggMap.get(<span class="string">"team_name"</span>);</span><br><span class="line">        <span class="comment">//循环查找结果</span></span><br><span class="line">        <span class="keyword">for</span> (Terms.Bucket teamBucket : teams.getBuckets()) &#123;</span><br><span class="line">            <span class="comment">//先按球队进行分组</span></span><br><span class="line">            String team = (String) teamBucket.getKey();</span><br><span class="line">            Map&lt;String, Aggregation&gt; subAggMap = teamBucket.getAggregations().getAsMap();</span><br><span class="line">            StringTerms positions = (StringTerms) subAggMap.get(<span class="string">"pos_count"</span>);</span><br><span class="line">            <span class="comment">//因为一个球队有很多位置，那么还要依次拿出位置信息</span></span><br><span class="line">            <span class="keyword">for</span> (Terms.Bucket posBucket : positions.getBuckets()) &#123;</span><br><span class="line">                <span class="comment">//拿到位置的名字</span></span><br><span class="line">                String pos = (String) posBucket.getKey();</span><br><span class="line">                <span class="comment">//拿出该位置的数量</span></span><br><span class="line">                <span class="keyword">long</span> docCount = posBucket.getDocCount();</span><br><span class="line">                <span class="comment">//打印球队，位置，人数</span></span><br><span class="line">                System.out.println(team + <span class="string">" "</span> + pos + <span class="string">" "</span> + docCount);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 结果:</span></span><br><span class="line"><span class="comment">         war pg 2</span></span><br><span class="line"><span class="comment">         war pf 1</span></span><br><span class="line"><span class="comment">         cav pg 1</span></span><br><span class="line"><span class="comment">         cav sf 1</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * select team, max(age) as max_age from player group by team;</span></span><br><span class="line"><span class="comment">     * 根据 team 名聚合后, 求聚合成员的 max age</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testAgg3</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定 索引&amp;类型  index &amp; type</span></span><br><span class="line">        SearchRequestBuilder builder = client.prepareSearch(<span class="string">"player_info"</span>).setTypes(<span class="string">"player"</span>);</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * term: 根据指定的名字创建一个新的聚合体</span></span><br><span class="line"><span class="comment">         * 指定按球队进行分组</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        TermsAggregationBuilder teamAgg = AggregationBuilders.terms(<span class="string">"team_name"</span>).field(<span class="string">"team"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定分组求最大值, 指定聚合函数(max)和需要聚合的字段(age)</span></span><br><span class="line">        MaxAggregationBuilder maxAgg = AggregationBuilders.max(<span class="string">"max_age"</span>).field(<span class="string">"age"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//分组后求最大值(添加聚合器--聚合规则)</span></span><br><span class="line">        builder.addAggregation(teamAgg.subAggregation(maxAgg));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//查询</span></span><br><span class="line">        SearchResponse response = builder.execute().actionGet();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 以 map 的形式获取聚合结果</span></span><br><span class="line">        Map&lt;String, Aggregation&gt; aggMap = response.getAggregations().getAsMap();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//根据team属性为 key，获取map中的内容-value</span></span><br><span class="line">        StringTerms teams = (StringTerms) aggMap.get(<span class="string">"team_name"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Terms.Bucket teamBucket : teams.getBuckets()) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//分组的属性名--有很多对, 把队名取出来</span></span><br><span class="line">            String team = (String) teamBucket.getKey();</span><br><span class="line"></span><br><span class="line">            <span class="comment">//在将聚合后取最大值的内容取出来放到map中</span></span><br><span class="line">            Map&lt;String, Aggregation&gt; subAggMap = teamBucket.getAggregations().getAsMap();</span><br><span class="line"></span><br><span class="line">            <span class="comment">//取分组后的最大值</span></span><br><span class="line">            InternalMax ages = (InternalMax)subAggMap.get(<span class="string">"max_age"</span>);</span><br><span class="line">            <span class="keyword">double</span> max = ages.getValue();</span><br><span class="line">            System.out.println(team + <span class="string">" "</span> + max);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * select team, avg(age) as avg_age, sum(salary) as total_salary from player group by team;</span></span><br><span class="line"><span class="comment">     * team分组 下求 avg &amp; sum</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testAgg4</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        SearchRequestBuilder builder = client.prepareSearch(<span class="string">"player_info"</span>).setTypes(<span class="string">"player"</span>);</span><br><span class="line">        <span class="comment">//指定分组字段</span></span><br><span class="line">        TermsAggregationBuilder termsAgg = AggregationBuilders.terms(<span class="string">"team_name"</span>).field(<span class="string">"team"</span>);</span><br><span class="line">        <span class="comment">//指定聚合函数是求平均数据</span></span><br><span class="line">        AvgAggregationBuilder avgAgg = AggregationBuilders.avg(<span class="string">"avg_age"</span>).field(<span class="string">"age"</span>);</span><br><span class="line">        <span class="comment">//指定另外一个聚合函数是求和</span></span><br><span class="line">        SumAggregationBuilder sumAgg = AggregationBuilders.sum(<span class="string">"total_salary"</span>).field(<span class="string">"salary"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//分组的聚合器关联了两个聚合函数(team 下关联了 avg &amp; sum)</span></span><br><span class="line">        builder.addAggregation(termsAgg.subAggregation(avgAgg).subAggregation(sumAgg));</span><br><span class="line"></span><br><span class="line">        SearchResponse response = builder.execute().actionGet();</span><br><span class="line"></span><br><span class="line">        Map&lt;String, Aggregation&gt; aggMap = response.getAggregations().getAsMap();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//按分组的名字取出数据</span></span><br><span class="line">        StringTerms teams = (StringTerms) aggMap.get(<span class="string">"team_name"</span>);</span><br><span class="line">        <span class="keyword">for</span> (Terms.Bucket teamBucket : teams.getBuckets()) &#123;</span><br><span class="line">            <span class="comment">//获取球队名字</span></span><br><span class="line">            String team = (String) teamBucket.getKey();</span><br><span class="line">            Map&lt;String, Aggregation&gt; subAggMap = teamBucket.getAggregations().getAsMap();</span><br><span class="line">            <span class="comment">//根据别名取出平均年龄</span></span><br><span class="line">            InternalAvg avgAge = (InternalAvg)subAggMap.get(<span class="string">"avg_age"</span>);</span><br><span class="line">            <span class="comment">//根据别名取出薪水总和</span></span><br><span class="line">            InternalSum totalSalary = (InternalSum)subAggMap.get(<span class="string">"total_salary"</span>);</span><br><span class="line">            <span class="keyword">double</span> avgAgeValue = avgAge.getValue();</span><br><span class="line">            <span class="keyword">double</span> totalSalaryValue = totalSalary.getValue();</span><br><span class="line">            System.out.println(team + <span class="string">" "</span> + avgAgeValue + <span class="string">" "</span> + totalSalaryValue);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * select team, sum(salary) as total_salary from player group by team order by total_salary desc;</span></span><br><span class="line"><span class="comment">     * 按 team 聚合, 按sum salary 降序排列</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testAgg5</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        SearchRequestBuilder builder = client.prepareSearch(<span class="string">"player_info"</span>).setTypes(<span class="string">"player"</span>);</span><br><span class="line">        <span class="comment">//按team进行分组，然后指定排序规则  asc 升序, desc 降序</span></span><br><span class="line">        <span class="comment">//排序规则在分组时就指定了</span></span><br><span class="line">        TermsAggregationBuilder termsAgg = AggregationBuilders.terms(<span class="string">"team_name"</span>).field(<span class="string">"team"</span>).order(Terms.Order.aggregation(<span class="string">"total_salary "</span>, <span class="keyword">false</span>));</span><br><span class="line">        SumAggregationBuilder sumAgg = AggregationBuilders.sum(<span class="string">"total_salary"</span>).field(<span class="string">"salary"</span>);</span><br><span class="line">        builder.addAggregation(termsAgg.subAggregation(sumAgg));</span><br><span class="line">        SearchResponse response = builder.execute().actionGet();</span><br><span class="line">        Map&lt;String, Aggregation&gt; aggMap = response.getAggregations().getAsMap();</span><br><span class="line">        StringTerms teams = (StringTerms) aggMap.get(<span class="string">"team_name"</span>);</span><br><span class="line">        <span class="keyword">for</span> (Terms.Bucket teamBucket : teams.getBuckets()) &#123;</span><br><span class="line">            String team = (String) teamBucket.getKey();</span><br><span class="line">            Map&lt;String, Aggregation&gt; subAggMap = teamBucket.getAggregations().getAsMap();</span><br><span class="line">            InternalSum totalSalary = (InternalSum)subAggMap.get(<span class="string">"total_salary"</span>);</span><br><span class="line">            <span class="keyword">double</span> totalSalaryValue = totalSalary.getValue();</span><br><span class="line">            System.out.println(team + <span class="string">" "</span> + totalSalaryValue);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="3-ES-的-Admin-API"><a href="#3-ES-的-Admin-API" class="headerlink" title="3. ES 的 Admin API"></a>3. ES 的 Admin API</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.es;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.AdminClient;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.IndicesAdminClient;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.transport.TransportClient;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.common.settings.Settings;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.common.transport.InetSocketTransportAddress;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.common.xcontent.XContentBuilder;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.common.xcontent.XContentFactory;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.transport.client.PreBuiltTransportClient;</span><br><span class="line"><span class="keyword">import</span> org.junit.Before;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.InetAddress;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AdminAPI</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> TransportClient client = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//在所有的测试方法之前执行</span></span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//设置集群名称</span></span><br><span class="line">        Settings settings = Settings.builder().put(<span class="string">"cluster.name"</span>, <span class="string">"my-es"</span>).build();</span><br><span class="line">        <span class="comment">//创建client</span></span><br><span class="line">        client = <span class="keyword">new</span> PreBuiltTransportClient(settings).addTransportAddresses(</span><br><span class="line">                <span class="keyword">new</span> InetSocketTransportAddress(InetAddress.getByName(<span class="string">"192.168.170.171"</span>), <span class="number">9300</span>),</span><br><span class="line">                <span class="keyword">new</span> InetSocketTransportAddress(InetAddress.getByName(<span class="string">"192.168.170.172"</span>), <span class="number">9300</span>),</span><br><span class="line">                <span class="keyword">new</span> InetSocketTransportAddress(InetAddress.getByName(<span class="string">"192.168.170.173"</span>), <span class="number">9300</span>));</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建索引，并配置一些参数</span></span><br><span class="line">    <span class="comment">//创建 再由索引创建 type</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">createIndexWithSettings</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">//获取Admin的API</span></span><br><span class="line">        AdminClient admin = client.admin();</span><br><span class="line">        <span class="comment">//使用Admin API对索引进行操作 拿到目录</span></span><br><span class="line">        IndicesAdminClient indices = admin.indices();</span><br><span class="line">        <span class="comment">//准备创建索引</span></span><br><span class="line">        indices.prepareCreate(<span class="string">"gamelog"</span>)</span><br><span class="line">                <span class="comment">//配置索引参数</span></span><br><span class="line">                .setSettings(</span><br><span class="line">                        <span class="comment">//参数配置器</span></span><br><span class="line">                        Settings.builder()<span class="comment">//指定索引分区的数量</span></span><br><span class="line">                                .put(<span class="string">"index.number_of_shards"</span>, <span class="number">4</span>)</span><br><span class="line">                                <span class="comment">//指定索引副本的数量（注意：不包括本身，如果设置数据存储副本为2，实际上数据存储了3份）</span></span><br><span class="line">                                .put(<span class="string">"index.number_of_replicas"</span>, <span class="number">2</span>)</span><br><span class="line">                )</span><br><span class="line">                <span class="comment">//真正执行</span></span><br><span class="line">                .get();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//跟索引添加mapping信息（给表添加schema信息）</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">putMapping</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">//创建索引</span></span><br><span class="line">        client.admin().indices().prepareCreate(<span class="string">"twitter"</span>)</span><br><span class="line">                <span class="comment">//创建一个type，并指定type中属性的名字和类型</span></span><br><span class="line">                .addMapping(<span class="string">"tweet"</span>,</span><br><span class="line">                        <span class="string">"&#123;\n"</span> +</span><br><span class="line">                                <span class="string">"    \"tweet\": &#123;\n"</span> +</span><br><span class="line">                                <span class="string">"      \"properties\": &#123;\n"</span> +</span><br><span class="line">                                <span class="string">"        \"message\": &#123;\n"</span> +</span><br><span class="line">                                <span class="string">"          \"type\": \"string\"\n"</span> +</span><br><span class="line">                                <span class="string">"        &#125;\n"</span> +</span><br><span class="line">                                <span class="string">"      &#125;\n"</span> +</span><br><span class="line">                                <span class="string">"    &#125;\n"</span> +</span><br><span class="line">                                <span class="string">"  &#125;"</span>)</span><br><span class="line">                .get();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 生产中用的!!</span></span><br><span class="line"><span class="comment">     * 你可以通过dynamic设置来控制这一行为，它能够接受以下的选项： ※</span></span><br><span class="line"><span class="comment">     * true：默认值。动态添加字段</span></span><br><span class="line"><span class="comment">     * false：忽略新字段</span></span><br><span class="line"><span class="comment">     * strict：如果碰到陌生字段，抛出异常</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testSettingsMappings</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">//1:settings</span></span><br><span class="line">        HashMap&lt;String, Object&gt; settings_map = <span class="keyword">new</span> HashMap&lt;String, Object&gt;(<span class="number">2</span>);</span><br><span class="line">        settings_map.put(<span class="string">"number_of_shards"</span>, <span class="number">3</span>);</span><br><span class="line">        settings_map.put(<span class="string">"number_of_replicas"</span>, <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2:mappings（映射、schema）</span></span><br><span class="line">        XContentBuilder builder = XContentFactory.jsonBuilder()</span><br><span class="line">                .startObject()</span><br><span class="line">                    .field(<span class="string">"dynamic"</span>, <span class="string">"true"</span>)</span><br><span class="line">                    <span class="comment">//设置type中的属性</span></span><br><span class="line">                    .startObject(<span class="string">"properties"</span>)</span><br><span class="line">                        <span class="comment">//num属性</span></span><br><span class="line">                        .startObject(<span class="string">"num"</span>)</span><br><span class="line">                            <span class="comment">//类型是integer</span></span><br><span class="line">                            .field(<span class="string">"type"</span>, <span class="string">"integer"</span>)</span><br><span class="line">                            <span class="comment">//不分词，但是建索引</span></span><br><span class="line">                            .field(<span class="string">"index"</span>, <span class="string">"not_analyzed"</span>)</span><br><span class="line">                            <span class="comment">//在文档中存储</span></span><br><span class="line">                            .field(<span class="string">"store"</span>, <span class="string">"yes"</span>)</span><br><span class="line">                        .endObject()</span><br><span class="line">                        <span class="comment">//name属性</span></span><br><span class="line">                        .startObject(<span class="string">"name"</span>)</span><br><span class="line">                            <span class="comment">//string类型</span></span><br><span class="line">                            .field(<span class="string">"type"</span>, <span class="string">"string"</span>)</span><br><span class="line">                            <span class="comment">//在文档中存储</span></span><br><span class="line">                            .field(<span class="string">"store"</span>, <span class="string">"yes"</span>)</span><br><span class="line">                            <span class="comment">//分词, 建立索引</span></span><br><span class="line">                            .field(<span class="string">"index"</span>, <span class="string">"analyzed"</span>)</span><br><span class="line">                            <span class="comment">//使用ik_smart进行分词</span></span><br><span class="line">                            .field(<span class="string">"analyzer"</span>, <span class="string">"ik_smart"</span>)</span><br><span class="line">                        .endObject()</span><br><span class="line">                    .endObject()</span><br><span class="line">                .endObject();</span><br><span class="line"></span><br><span class="line">        CreateIndexRequestBuilder prepareCreate = client.admin().indices().prepareCreate(<span class="string">"user_info"</span>);</span><br><span class="line">        <span class="comment">//管理索引（user_info）然后关联type（user）</span></span><br><span class="line">        prepareCreate.setSettings(settings_map).addMapping(<span class="string">"user"</span>, builder).get();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * XContentBuilder mapping = jsonBuilder()</span></span><br><span class="line"><span class="comment">     .startObject()</span></span><br><span class="line"><span class="comment">     .startObject("productIndex")</span></span><br><span class="line"><span class="comment">     .startObject("properties")</span></span><br><span class="line"><span class="comment">     .startObject("title").field("type", "string").field("store", "yes").endObject()</span></span><br><span class="line"><span class="comment">     .startObject("description").field("type", "string").field("index", "not_analyzed").endObject()</span></span><br><span class="line"><span class="comment">     .startObject("price").field("type", "double").endObject()</span></span><br><span class="line"><span class="comment">     .startObject("onSale").field("type", "boolean").endObject()</span></span><br><span class="line"><span class="comment">     .startObject("type").field("type", "integer").endObject()</span></span><br><span class="line"><span class="comment">     .startObject("createDate").field("type", "date").endObject()</span></span><br><span class="line"><span class="comment">     .endObject()</span></span><br><span class="line"><span class="comment">     .endObject()</span></span><br><span class="line"><span class="comment">     .endObject();</span></span><br><span class="line"><span class="comment">     PutMappingRequest mappingRequest = Requests.putMappingRequest("productIndex").type("productIndex").source(mapping);</span></span><br><span class="line"><span class="comment">     client.admin().indices().putMapping(mappingRequest).actionGet();</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * index这个属性，no代表不建索引</span></span><br><span class="line"><span class="comment">     * not_analyzed，建索引不分词</span></span><br><span class="line"><span class="comment">     * analyzed 即分词，又建立索引</span></span><br><span class="line"><span class="comment">     * expected [no], [not_analyzed] or [analyzed]</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">// 实战案例写法</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testSettingsPlayerMappings</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">//1:settings</span></span><br><span class="line">        HashMap&lt;String, Object&gt; settings_map = <span class="keyword">new</span> HashMap&lt;String, Object&gt;(<span class="number">2</span>);</span><br><span class="line">        settings_map.put(<span class="string">"number_of_shards"</span>, <span class="number">3</span>);</span><br><span class="line">        settings_map.put(<span class="string">"number_of_replicas"</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2:mappings</span></span><br><span class="line">        XContentBuilder builder = XContentFactory.jsonBuilder()</span><br><span class="line">                .startObject()<span class="comment">//</span></span><br><span class="line">                        .field(<span class="string">"dynamic"</span>, <span class="string">"true"</span>)</span><br><span class="line">                    .startObject(<span class="string">"properties"</span>)</span><br><span class="line">                        .startObject(<span class="string">"id"</span>)</span><br><span class="line">                            .field(<span class="string">"type"</span>, <span class="string">"integer"</span>)</span><br><span class="line">                            .field(<span class="string">"store"</span>, <span class="string">"yes"</span>)</span><br><span class="line">                        .endObject()</span><br><span class="line"></span><br><span class="line">                        .startObject(<span class="string">"name"</span>)</span><br><span class="line">                            .field(<span class="string">"type"</span>, <span class="string">"string"</span>)</span><br><span class="line">                            <span class="comment">// 只要不是 index, no,  就是建索引</span></span><br><span class="line">                            .field(<span class="string">"index"</span>, <span class="string">"not_analyzed"</span>)</span><br><span class="line">                        .endObject()</span><br><span class="line"></span><br><span class="line">                        .startObject(<span class="string">"age"</span>)</span><br><span class="line">                            .field(<span class="string">"type"</span>, <span class="string">"integer"</span>)</span><br><span class="line">                        .endObject()</span><br><span class="line"></span><br><span class="line">                        .startObject(<span class="string">"salary"</span>)</span><br><span class="line">                            .field(<span class="string">"type"</span>, <span class="string">"integer"</span>)</span><br><span class="line">                        .endObject()</span><br><span class="line"></span><br><span class="line">                        .startObject(<span class="string">"team"</span>)</span><br><span class="line">                            .field(<span class="string">"type"</span>, <span class="string">"string"</span>)</span><br><span class="line">                            .field(<span class="string">"index"</span>, <span class="string">"not_analyzed"</span>)</span><br><span class="line">                        .endObject()</span><br><span class="line"></span><br><span class="line">                        .startObject(<span class="string">"position"</span>)</span><br><span class="line">                            .field(<span class="string">"type"</span>, <span class="string">"string"</span>)</span><br><span class="line">                            .field(<span class="string">"index"</span>, <span class="string">"not_analyzed"</span>)</span><br><span class="line">                        .endObject()</span><br><span class="line"></span><br><span class="line">                        .startObject(<span class="string">"description"</span>)</span><br><span class="line">                            .field(<span class="string">"type"</span>, <span class="string">"string"</span>)</span><br><span class="line">                            .field(<span class="string">"store"</span>, <span class="string">"no"</span>)</span><br><span class="line">                            .field(<span class="string">"index"</span>, <span class="string">"analyzed"</span>)</span><br><span class="line">                            .field(<span class="string">"analyzer"</span>, <span class="string">"ik_smart"</span>)</span><br><span class="line">                        .endObject()</span><br><span class="line"></span><br><span class="line">                        .startObject(<span class="string">"addr"</span>)</span><br><span class="line">                            .field(<span class="string">"type"</span>, <span class="string">"string"</span>)</span><br><span class="line">                            .field(<span class="string">"store"</span>, <span class="string">"yes"</span>)</span><br><span class="line">                            .field(<span class="string">"index"</span>, <span class="string">"analyzed"</span>)</span><br><span class="line">                            .field(<span class="string">"analyzer"</span>, <span class="string">"ik_smart"</span>)</span><br><span class="line">                        .endObject()</span><br><span class="line">                    .endObject()</span><br><span class="line">                .endObject();</span><br><span class="line"></span><br><span class="line">        CreateIndexRequestBuilder prepareCreate = client.admin().indices().prepareCreate(<span class="string">"player_info"</span>);</span><br><span class="line">        prepareCreate.setSettings(settings_map).addMapping(<span class="string">"player"</span>, builder).get();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="ES-安装-SQL-插件-amp-图形化界面插件"><a href="#ES-安装-SQL-插件-amp-图形化界面插件" class="headerlink" title="ES 安装 SQL 插件 &amp; 图形化界面插件"></a>ES 安装 SQL 插件 &amp; 图形化界面插件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#es安装SQL插件</span></span><br><span class="line">./bin/elasticsearch-plugin install https://github.com/NLPchina/elasticsearch-sql/releases/download/5.4.3.0/elasticsearch-sql-5.4.3.0.zip</span><br><span class="line"></span><br><span class="line"><span class="comment">#然后将解压到plugins目录下的内容拷贝到其他es的节点的plugins目录, 重启 es, 就可以使用 sql 风格查询了, 类似于下面, 直接在浏览器输入:</span></span><br><span class="line">http://cs7_1:9200/_sql?sql=select * from player_info <span class="built_in">limit</span> 10</span><br><span class="line">http://cs7_1:9200/_sql?sql=select team, avg(salary) avg_sal from player_info group by team</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#下载SQL的Server 图形化界面插件</span></span><br><span class="line">wget https://github.com/NLPchina/elasticsearch-sql/releases/download/5.4.1.0/es-sql-site-standalone.zip</span><br><span class="line"></span><br><span class="line"><span class="comment">#用npm编译安装</span></span><br><span class="line">unzip es-sql-site-standalone.zip</span><br><span class="line"><span class="built_in">cd</span> site-server/</span><br><span class="line">npm install express --save</span><br><span class="line"></span><br><span class="line"><span class="comment">#修改SQL的Server的端口, 默认是8080</span></span><br><span class="line">vi site_configuration.json</span><br><span class="line">启动服务</span><br><span class="line">node node-server.js &amp;</span><br><span class="line"></span><br><span class="line">&gt;&gt; 注意: 图形化界面工具, 右上角的框中, 要填上连接 es 服务器的地址. </span><br><span class="line"> 此外, 还有 explain 功能, 可以把 sql 转化为restful 风格的 json, 如下图所示</span><br></pre></td></tr></table></figure><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-14-160912.png" alt="image-20180815000912709"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一-Lucene&quot;&gt;&lt;a href=&quot;#一-Lucene&quot; class=&quot;headerlink&quot; title=&quot;一. Lucene&quot;&gt;&lt;/a&gt;一. Lucene&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/s
      
    
    </summary>
    
      <category term="ELasticSearch" scheme="https://airpoet.github.io/categories/ELasticSearch/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="ELasticSearch" scheme="https://airpoet.github.io/tags/ELasticSearch/"/>
    
  </entry>
  
  <entry>
    <title>Hive各种分析函数案例</title>
    <link href="https://airpoet.github.io/2018/08/05/Hadoop/3-Hive/Hive%E5%90%84%E7%A7%8D%E5%88%86%E6%9E%90%E5%87%BD%E6%95%B0%E6%A1%88%E4%BE%8B/"/>
    <id>https://airpoet.github.io/2018/08/05/Hadoop/3-Hive/Hive各种分析函数案例/</id>
    <published>2018-08-05T09:17:04.637Z</published>
    <updated>2018-08-05T09:56:23.635Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/categories/Hadoop/Hive/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Spark常用命令</title>
    <link href="https://airpoet.github.io/2018/08/05/Spark/Spark%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    <id>https://airpoet.github.io/2018/08/05/Spark/Spark常用命令/</id>
    <published>2018-08-05T08:15:02.956Z</published>
    <updated>2018-08-05T14:16:51.392Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一-spark-submit-shell"><a href="#一-spark-submit-shell" class="headerlink" title="一. spark-submit/shell"></a>一. spark-submit/shell</h1><h2 id="1-standalone模式"><a href="#1-standalone模式" class="headerlink" title="1. standalone模式"></a>1. standalone模式</h2><p>注意: 以下都是 <code>standalone</code> 模式下的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用spark-submit提交一个任务： 比如求Pi</span></span><br><span class="line">~/apps/spark/bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://cs1:7077 \</span><br><span class="line">--executor-memory 512m \</span><br><span class="line">--total-executor-cores 1 \</span><br><span class="line">~/apps/spark/examples/jars/spark-examples_2.11-2.3.1.jar \</span><br><span class="line">100</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># [高可用] 方式下使用 spark-submit 提交一个任务： 比如求Pi</span></span><br><span class="line">~/apps/spark/bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://cs1:7077,cs6:7077 \</span><br><span class="line">--executor-memory 512m \</span><br><span class="line">--total-executor-cores 1 \</span><br><span class="line">~/apps/spark/examples/jars/spark-examples_2.11-2.3.1.jar \</span><br><span class="line">100</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入spark-shell连接spark集群：</span></span><br><span class="line">~/apps/spark/bin/spark-shell \</span><br><span class="line">--master spark://cs1:7077,cs6:7077 \</span><br><span class="line">--executor-memory 2G \</span><br><span class="line">--total-executor-cores 12</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入spark-shell连接 高可用spark集群：</span></span><br><span class="line">~/apps/spark/bin/spark-shell \</span><br><span class="line">--master spark://cs1:7077,cs6:7077 \</span><br><span class="line">--executor-memory 512m \</span><br><span class="line">--total-executor-cores 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 求wordcount直接打印输出，输入本地文件，所以应该最好在spark-shell连接local的时候使用</span></span><br><span class="line">sc.textFile(<span class="string">"file:///home/hadoop/words.txt"</span>).flatMap(_.split(<span class="string">" "</span>)).map((_,1)).reduceByKey(_+_).foreach(println)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 求wordcount直接打印输出，输入HDFS文件，所以应该最好在spark-shell连接spark集群的时候使用</span></span><br><span class="line">sc.textFile(<span class="string">"hdfs://mycluster/wc/input/words.txt"</span>).flatMap(_.split(<span class="string">" "</span>)).map((_,1)).reduceByKey(_+_).foreach(println)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 求wordcount直接打印输出，输入HDFS文件，输出HDFS文件</span></span><br><span class="line">sc.textFile(<span class="string">"hdfs://mycluster/wc/input/words.txt"</span>).flatMap(_.split(<span class="string">" "</span>)).map((_,1)).reduceByKey(_+_).sortBy(_._2).saveAsTextFile(<span class="string">"hdfs://mycluster/wc/output/"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 打印输出</span><br><span class="line">sc.textFile(<span class="string">"hdfs://mycluster/wc/input/words.txt"</span>).flatMap(_.split(<span class="string">" "</span>)).map((_,1)).reduceByKey(_+_).sortBy(_._2).foreach(x =&gt; println(x._1 +<span class="string">"-"</span>+ x._2))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 收集结果</span><br><span class="line">sc.textFile(<span class="string">"hdfs://mycluster/wc/input/words.txt"</span>).flatMap(_.split(<span class="string">" "</span>)).map((_,1)).reduceByKey(_+_).sortBy(_._2).collect</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 搭建高可用：修改spark-env.sh</span><br><span class="line"><span class="built_in">export</span> SPARK_DAEMON_JAVA_OPTS=<span class="string">"-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=cs1,hadoop03,cs6 -Dspark.deploy.zookeeper.dir=/spark231"</span></span><br><span class="line"></span><br><span class="line">// 修改spark-env.sh</span><br><span class="line"><span class="built_in">export</span> SPARK_HISTORY_OPTS=<span class="string">"-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=30 -Dspark.history.fs.logDirectory=hdfs://mycluster/sparklog231"</span></span><br><span class="line">// 修改spark-defaults.conf</span><br><span class="line">spark.eventLog.enabled               <span class="literal">true</span></span><br><span class="line">spark.eventLog.dir                   hdfs://mycluster/spark231log</span><br><span class="line">// 创建日志目录</span><br><span class="line">hadoop fs -mkdir -p hdfs://mycluster/sparklog231</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 启动和关闭spark集群</span><br><span class="line">~/apps/spark/sbin/stop-all.sh   </span><br><span class="line">~/apps/spark/sbin/start-all.sh  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 启动历史服务器</span><br><span class="line"><span class="variable">$SPARK_HOME</span>/sbin/start-history-server.sh</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 使用spark提交自己编写的wordcount</span><br><span class="line"><span class="variable">$SPARK_HOME</span>/bin/spark-submit \</span><br><span class="line">--class com.mazh.WordCountScala \</span><br><span class="line">--master spark://cs1:7077,cs6:7077 \</span><br><span class="line">--executor-memory 512m \</span><br><span class="line">--total-executor-cores 1 \</span><br><span class="line">/home/hadoop/sparkwc-1.0-SNAPSHOT.jar \</span><br><span class="line">hdfs://mycluster/wc/input \</span><br><span class="line">hdfs://mycluster/wc/output_11</span><br></pre></td></tr></table></figure><h2 id="2-Yarn-模式"><a href="#2-Yarn-模式" class="headerlink" title="2. Yarn 模式"></a>2. Yarn 模式</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">local</span> 本地单线程</span><br><span class="line"><span class="built_in">local</span>[K] 本地多线程（指定K个内核）</span><br><span class="line"><span class="built_in">local</span>[*] 本地多线程（指定所有可用内核）</span><br><span class="line">spark://HOST:PORT  连接到指定的 Spark standalone cluster master，需要指定端口。</span><br><span class="line">mesos://HOST:PORT  连接到指定的 Mesos 集群，需要指定端口。</span><br><span class="line">yarn-client客户端模式 连接到 YARN 集群。需要配置 HADOOP_CONF_DIR。</span><br><span class="line">yarn-cluster集群模式 连接到 YARN 集群。需要配置 HADOOP_CONF_DIR。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提交任务到本地运行：</span></span><br><span class="line"><span class="variable">$SPARK_HOME</span>/bin/spark-submit  \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master <span class="built_in">local</span>[4] \</span><br><span class="line">--driver-memory 512M \</span><br><span class="line">--executor-memory 512M \</span><br><span class="line">--total-executor-cores 1 \</span><br><span class="line"><span class="variable">$SPARK_HOME</span>/examples/jars/spark-examples_2.11-2.3.1.jar \</span><br><span class="line">10</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提交任务到Spark集群运行：</span></span><br><span class="line"><span class="variable">$SPARK_HOME</span>/bin/spark-submit  \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop02:7077,hadoop04:7077 \</span><br><span class="line">--driver-memory 512M \</span><br><span class="line">--executor-memory 512M \</span><br><span class="line">--total-executor-cores 1 \</span><br><span class="line"><span class="variable">$SPARK_HOME</span>/examples/jars/spark-examples_2.11-2.3.1.jar \</span><br><span class="line">100</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提交到YARN集群，使用yarn-client模式：</span></span><br><span class="line"><span class="variable">$SPARK_HOME</span>/bin/spark-submit  \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">--driver-memory 512M \</span><br><span class="line">--executor-memory 512M \</span><br><span class="line">--total-executor-cores 1 \</span><br><span class="line"><span class="variable">$SPARK_HOME</span>/examples/jars/spark-examples_2.11-2.3.1.jar \</span><br><span class="line">10</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提交到YARN集群，使用yarn-cluster模式：</span></span><br><span class="line"><span class="variable">$SPARK_HOME</span>/bin/spark-submit  \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--driver-memory 512M \</span><br><span class="line">--executor-memory 512M \</span><br><span class="line">--total-executor-cores 1 \</span><br><span class="line"><span class="variable">$SPARK_HOME</span>/examples/jars/spark-examples_2.11-2.3.1.jar \</span><br><span class="line">10</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提交spark任务到YARN集群时，要求配置：</span></span><br><span class="line">spark-env.sh中：</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=/home/hadoop/apps/hadoop-2.7.6/etc/hadoop/</span><br><span class="line"></span><br><span class="line">spark-defaults.conf中：</span><br><span class="line">spark.yarn.jars/home/hadoop/apps/hadoop-2.7.6/share/hadoop/yarn</span><br><span class="line">&gt; 或者直接指定在 hdfs 上的位置(先上传到 hdfs 上)</span><br><span class="line"></span><br><span class="line">如若不生效，则直接拷贝yarn-site.xml文件到<span class="variable">$SPARK_HOME</span>中</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 上述任务在启动的时候，有可能会出现异常， 修改hadoop集群的yarn-site.xml文件， 增加如下配置：</span></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">&lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</span><br><span class="line">&lt;description&gt;Whether virtual memory limits will be enforced <span class="keyword">for</span> containers&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;</span><br><span class="line">&lt;value&gt;4&lt;/value&gt;</span><br><span class="line">&lt;description&gt;Ratio between virtual memory to physical memory when setting memory limits <span class="keyword">for</span> containers&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h2 id="3-简而言之"><a href="#3-简而言之" class="headerlink" title="3.简而言之"></a>3.简而言之</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">// 使用spark-submit提交一个任务到普通的Spark Standalone集群： 比如求Pi</span><br><span class="line">~/apps/spark/bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop02:7077 \</span><br><span class="line">--executor-memory 512m \</span><br><span class="line">--total-executor-cores 1 \</span><br><span class="line">~/apps/spark/examples/jars/spark-examples_2.11-2.3.1.jar \</span><br><span class="line">100</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 高可用方式下使用spark-submit提交一个任务到高可用的Spark集群： 比如求Pi</span><br><span class="line">~/apps/spark/bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop02:7077,hadoop04:7077 \</span><br><span class="line">--executor-memory 512m \</span><br><span class="line">--total-executor-cores 1 \</span><br><span class="line">~/apps/spark/examples/jars/spark-examples_2.11-2.3.1.jar \</span><br><span class="line">100</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 高可用方式下使用spark-submit提交一个任务到高可用的YARN集群，使用client模式： 比如求Pi</span><br><span class="line">~/apps/spark/bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">--executor-memory 512m \</span><br><span class="line">--total-executor-cores 1 \</span><br><span class="line">~/apps/spark/examples/jars/spark-examples_2.11-2.3.1.jar \</span><br><span class="line">100</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 高可用方式下使用spark-submit提交一个任务到高可用的YARN集群，使用cluster模式： 比如求Pi</span><br><span class="line">~/apps/spark/bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--executor-memory 512m \</span><br><span class="line">--total-executor-cores 1 \</span><br><span class="line">~/apps/spark/examples/jars/spark-examples_2.11-2.3.1.jar \</span><br><span class="line">100</span><br></pre></td></tr></table></figure><h1 id="二-内存资源不足的配置"><a href="#二-内存资源不足的配置" class="headerlink" title="二. 内存资源不足的配置"></a>二. 内存资源不足的配置</h1><blockquote><p>修改<code>yarn-site.xml</code>, 增加2个配置</p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">        Whether virtual memory limits will be enforced for containers</span><br><span class="line">                是否会对容器执行虚拟内存限制</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">        Ratio between virtual memory to physical memory when setting memory limits for containers</span><br><span class="line">                设置容器的内存限制时虚拟内存与物理内存的比率</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h1 id="三-启动集群相关命令总结"><a href="#三-启动集群相关命令总结" class="headerlink" title="三. 启动集群相关命令总结"></a>三. 启动集群相关命令总结</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一：启动zookeeper</span></span><br><span class="line">zkServer.sh stop</span><br><span class="line">zkServer.sh start</span><br><span class="line">zkServer.sh status</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二：启动HDFS</span></span><br><span class="line">start-dfs.sh</span><br><span class="line">stop-dfs.sh</span><br><span class="line">hadoop-daemon.sh start namenode</span><br><span class="line">hadoop-daemon.sh start datanode</span><br><span class="line">hadoop-daemon.sh start zkfc</span><br><span class="line">hadoop-daemon.sh start journalnode</span><br><span class="line">hadoop-daemon.sh start secondarynamenode</span><br><span class="line">hdfs haadmin -transitionToActive --forcemanual nn1</span><br><span class="line">hdfs haadmin -getServiceState nn1</span><br><span class="line">hdfs haadmin -getServiceState nn2</span><br><span class="line">hdfs dfsadmin -safemode get</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三：启动YARN</span></span><br><span class="line">start-yarn.sh</span><br><span class="line">stop-yarn.sh</span><br><span class="line">yarn-daemon.sh start resourcemanager</span><br><span class="line">yarn-daemon.sh start nodemanager</span><br><span class="line">yarn rmadmin -getServiceState rm1</span><br><span class="line">yarn rmadmin -getServiceState rm2</span><br><span class="line">yarn rmadmin -transitionToActive --forcemanual rm1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第四： 启动MapReduce历史服务器</span></span><br><span class="line">mr-jobhistory-daemon.sh start historyserver</span><br><span class="line">mr-jobhistory-daemon.sh stop historyserver</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第五：启动hbase</span></span><br><span class="line">stop-hbase.sh</span><br><span class="line">start-hbase.sh</span><br><span class="line">hbase-daemon.sh start master</span><br><span class="line">hbase-daemon.sh start regionserver</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第六：启动Hive</span></span><br><span class="line">sh start-hive.sh</span><br><span class="line">nohup hiveserver2 1&gt;/home/hadoop/<span class="built_in">log</span>/hive_std.log 2&gt;/home/hadoop/<span class="built_in">log</span>/hive_err.log &amp;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第七：启动spark</span></span><br><span class="line"></span><br><span class="line">/home/hadoop/apps/spark-2.3.0-bin-hadoop2.7/sbin/start-all.sh</span><br><span class="line">/home/hadoop/apps/spark-2.3.0-bin-hadoop2.7/sbin/stop-all.sh</span><br><span class="line">/home/hadoop/apps/spark-2.3.0-bin-hadoop2.7/sbin/start-master.sh</span><br><span class="line">/home/hadoop/apps/spark-2.3.0-bin-hadoop2.7/sbin/stop-master.sh</span><br><span class="line">/home/hadoop/apps/spark-2.3.0-bin-hadoop2.7/sbin/start-slaves.sh</span><br><span class="line">/home/hadoop/apps/spark-2.3.0-bin-hadoop2.7/sbin/stop-slaves.sh</span><br><span class="line"></span><br><span class="line">在hadoop02机器上启动spark的historyserver</span><br><span class="line">/home/hadoop/apps/spark-2.3.0-bin-hadoop2.7/sbin/start-history-server.sh</span><br><span class="line"></span><br><span class="line">进入shell：</span><br><span class="line"><span class="variable">$SPARK_HOME</span>/bin/spark-shell</span><br><span class="line"><span class="variable">$SPARK_HOME</span>/bin/spark-shell \</span><br><span class="line">--master spark://hadoop02:7077,hadoop04:7077 \</span><br><span class="line">--executor-memory 2G \</span><br><span class="line">--total-executor-cores 2</span><br><span class="line"></span><br><span class="line">~/apps/spark-2.3.0-bin-hadoop2.7/bin/spark-shell \</span><br><span class="line">--master spark://hadoop02:7077 \</span><br><span class="line">--executor-memory 512m \</span><br><span class="line">--total-executor-cores 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spark-shell的wordCount程序：</span><br><span class="line">hadoop fs -mkdir -p /wc/input</span><br><span class="line">hadoop fs -put words.txt /wc/input</span><br><span class="line">sc.textFile(<span class="string">"hdfs://myha01/wc/input/words.txt"</span>).flatMap(_.split(<span class="string">" "</span>)).map((_,1)).reduceByKey(_+_).sortBy(_._2, <span class="literal">false</span>).collect</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第八：启动Storm</span></span><br><span class="line">nohup <span class="variable">$STORM_HOME</span>/bin/storm nimbus 1&gt;~/<span class="built_in">log</span>/storm-nibus.log 2&gt;&amp;1 &amp;  </span><br><span class="line">nohup <span class="variable">$STORM_HOME</span>/bin/storm ui 1&gt;~/<span class="built_in">log</span>/storm-ui.log 2&gt;&amp;1 &amp;  </span><br><span class="line">nohup <span class="variable">$STORM_HOME</span>/bin/storm supervisor 1&gt;~/<span class="built_in">log</span>/storm-supervisor.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><h4 id="Spark-启动脚本分析图"><a href="#Spark-启动脚本分析图" class="headerlink" title="Spark 启动脚本分析图"></a><strong>Spark 启动脚本分析图</strong></h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-05-spark%E8%84%9A%E6%9C%AC%E6%9E%84%E6%88%90%E5%88%86%E6%9E%90.png" alt=""></p><h4 id="SparkCore-调优思维导图"><a href="#SparkCore-调优思维导图" class="headerlink" title="SparkCore 调优思维导图"></a>SparkCore 调优思维导图</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-05-SparkCore%E8%B0%83%E4%BC%98.png" alt=""></p><h1 id="四-其它注意点"><a href="#四-其它注意点" class="headerlink" title="四. 其它注意点"></a>四. 其它注意点</h1><h3 id="1-关于sc-textFile-“-”-读取到文件的切片"><a href="#1-关于sc-textFile-“-”-读取到文件的切片" class="headerlink" title="1.关于sc.textFile(“.....”)读取到文件的切片"></a>1.关于<code>sc.textFile(“.....”)</code>读取到文件的切片</h3><ul><li><code>sc.textFile(&quot;xxx&quot;)</code>, 从 hdfs 上读取到的数据, 默认是2个分区</li><li>sc.textFile(“xxx”, 1) 这样会按照文件个数来切片, 如果后面不加上最小切片数量,  默认就是2个切片,  会把所有的文件的 size 加起来 除以2  得到一个 <code>goalsize</code> 目标切片大小,   来比较,  如果 &gt; goalsize 的1.1 倍的话,  就会被再切片</li><li>通常如果几个文件大小区别特别大(比如 3k, 3k,  300k) 的情况下 , 会被多切出一个或多个分区</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一-spark-submit-shell&quot;&gt;&lt;a href=&quot;#一-spark-submit-shell&quot; class=&quot;headerlink&quot; title=&quot;一. spark-submit/shell&quot;&gt;&lt;/a&gt;一. spark-submit/shell&lt;/h1
      
    
    </summary>
    
      <category term="Spark" scheme="https://airpoet.github.io/categories/Spark/"/>
    
      <category term="Shell" scheme="https://airpoet.github.io/categories/Spark/Shell/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Spark" scheme="https://airpoet.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark重点解析(四)=&gt; Spark Streaming</title>
    <link href="https://airpoet.github.io/2018/08/02/Spark/Spark%E9%87%8D%E7%82%B9%E8%A7%A3%E6%9E%90(%E5%9B%9B)=-Spark-Streaming/"/>
    <id>https://airpoet.github.io/2018/08/02/Spark/Spark重点解析(四)=-Spark-Streaming/</id>
    <published>2018-08-02T12:49:17.971Z</published>
    <updated>2018-09-13T15:52:00.834Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-StreamingContext-的创建过程解析"><a href="#0-StreamingContext-的创建过程解析" class="headerlink" title="0.  StreamingContext 的创建过程解析"></a>0.  StreamingContext 的创建过程解析</h1><h2 id="1-直接上代码"><a href="#1-直接上代码" class="headerlink" title="1.直接上代码"></a>1.直接上代码</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>&gt; 典型的创建方法</span><br><span class="line">首先创建一个 <span class="type">SparkConf</span> 对象</span><br><span class="line">传入 conf 和 <span class="type">Streaming</span> 的 batchTime, 创建 <span class="type">StreamingContext</span></span><br><span class="line">-------</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"KafkaDirectWordCount"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="number">2</span>&gt; 进入 <span class="type">StreamingContext</span> 中</span><br><span class="line">内部会创建一个 <span class="type">StreamingContext</span>, 用传进来的 conf 作为参数</span><br><span class="line">-------</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(conf: <span class="type">SparkConf</span>, batchDuration: <span class="type">Duration</span>) = &#123;</span><br><span class="line">        <span class="keyword">this</span>(<span class="type">StreamingContext</span>.createNewSparkContext(conf), <span class="literal">null</span>, batchDuration)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="number">3</span>&gt; 进入 createNewSparkContext, 再进入 <span class="type">SparkContext</span>中, 找到最长的 <span class="keyword">this</span>(..,..,..)方法</span><br><span class="line">这里其实也就是 <span class="type">SparkContext</span> 的初始化流程, 参考下图</span><br><span class="line">-------</span><br><span class="line">    <span class="keyword">private</span>[streaming] <span class="function"><span class="keyword">def</span> <span class="title">createNewSparkContext</span></span>(conf: <span class="type">SparkConf</span>): <span class="type">SparkContext</span> = &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    &#125;</span><br><span class="line">-------</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">       * Alternative constructor that allows setting common Spark properties directly</span></span><br><span class="line"><span class="comment">       *</span></span><br><span class="line"><span class="comment">       * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).</span></span><br><span class="line"><span class="comment">       * @param appName A name for your application, to display on the cluster web UI.</span></span><br><span class="line"><span class="comment">       * @param sparkHome Location where Spark is installed on cluster nodes.</span></span><br><span class="line"><span class="comment">       * @param jars Collection of JARs to send to the cluster. These can be paths on the local file</span></span><br><span class="line"><span class="comment">       *             system or HDFS, HTTP, HTTPS, or FTP URLs.</span></span><br><span class="line"><span class="comment">       * @param environment Environment variables to set on worker nodes.</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(</span><br><span class="line">        master: <span class="type">String</span>,</span><br><span class="line">        appName: <span class="type">String</span>,</span><br><span class="line">        sparkHome: <span class="type">String</span> = <span class="literal">null</span>,</span><br><span class="line">        jars: <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">Nil</span>,</span><br><span class="line">        environment: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="type">Map</span>()) = &#123;</span><br><span class="line">        <span class="keyword">this</span>(<span class="type">SparkContext</span>.updatedConf(<span class="keyword">new</span> <span class="type">SparkConf</span>(), master, appName, sparkHome, jars, environment))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">=--------------</span><br><span class="line">&gt; 暂存</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Worker to Master</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * @param id the worker id</span></span><br><span class="line"><span class="comment">   * @param host the worker host</span></span><br><span class="line"><span class="comment">   * @param port the worker post</span></span><br><span class="line"><span class="comment">   * @param worker the worker endpoint ref</span></span><br><span class="line"><span class="comment">   * @param cores the core number of worker</span></span><br><span class="line"><span class="comment">   * @param memory the memory size of worker</span></span><br><span class="line"><span class="comment">   * @param workerWebUiUrl the worker Web UI address</span></span><br><span class="line"><span class="comment">   * @param masterAddress the master address used by the worker to connect</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">RegisterWorker</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">      id: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">      host: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">      port: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">      worker: <span class="type">RpcEndpointRef</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">      cores: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">      memory: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">      workerWebUiUrl: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">      masterAddress: <span class="type">RpcAddress</span></span>)</span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">DeployMessage</span> </span>&#123;</span><br><span class="line">    <span class="type">Utils</span>.checkHost(host)</span><br><span class="line">    assert (port &gt; <span class="number">0</span>)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-26-spark%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%E5%9B%BE%E8%A7%A3.png" alt=""></p><h1 id="一-DStream-源码解析"><a href="#一-DStream-源码解析" class="headerlink" title="一. DStream 源码解析"></a>一. DStream 源码解析</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-02-DStream%E8%AF%B4%E6%98%8E.png" alt=""></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// RDDs generated, marked as private[streaming] so that testsuites can access it</span></span><br><span class="line"><span class="comment">// 内部有一个generatedRDDs方法, 是一个 HashMap[Time, RDD], key:time value:RDD</span></span><br><span class="line">  <span class="meta">@transient</span></span><br><span class="line">  <span class="keyword">private</span>[streaming] <span class="keyword">var</span> generatedRDDs = <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">Time</span>, <span class="type">RDD</span>[<span class="type">T</span>]]()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 根据指定的时间产生一个 RDD</span></span><br><span class="line">  <span class="comment">/** Method that generates an RDD for the given time */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(validTime: <span class="type">Time</span>): <span class="type">Option</span>[<span class="type">RDD</span>[<span class="type">T</span>]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 对 DStream 进行操作, 本质上是对 依赖的 父 RDD 进行操作</span></span><br><span class="line"><span class="number">1</span>&gt; </span><br><span class="line">  <span class="comment">/** Return a new DStream by applying a function to all elements of this DStream. */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](mapFunc: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">DStream</span>[<span class="type">U</span>] = ssc.withScope &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MappedDStream</span>(<span class="keyword">this</span>, context.sparkContext.clean(mapFunc))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="number">2</span>&gt; </span><br><span class="line"><span class="keyword">package</span> org.apache.spark.streaming.dstream</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MappedDStream</span>[<span class="type">T</span>: <span class="type">ClassTag</span>, <span class="type">U</span>: <span class="type">ClassTag</span>] (<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    parent: <span class="type">DStream</span>[<span class="type">T</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">    mapFunc: <span class="type">T</span> =&gt; <span class="type">U</span></span></span></span><br><span class="line"><span class="class"><span class="params">  </span>) <span class="keyword">extends</span> <span class="title">DStream</span>[<span class="type">U</span>](<span class="params">parent.ssc</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dependencies</span></span>: <span class="type">List</span>[<span class="type">DStream</span>[_]] = <span class="type">List</span>(parent)</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">slideDuration</span></span>: <span class="type">Duration</span> = parent.slideDuration</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(validTime: <span class="type">Time</span>): <span class="type">Option</span>[<span class="type">RDD</span>[<span class="type">U</span>]] = &#123;</span><br><span class="line">    parent.getOrCompute(validTime).map(_.map[<span class="type">U</span>](mapFunc))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="number">3</span>&gt;</span><br><span class="line">_.map[<span class="type">U</span>]: 最后的 map 方法,是调用 <span class="type">RDD</span> 的 map 方法, 传入自己定义的处理函数</span><br><span class="line">再掉 getOrCompute 方法 , 里面使用了 generatedRDDs</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>[streaming] <span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">getOrCompute</span></span>(time: <span class="type">Time</span>): <span class="type">Option</span>[<span class="type">RDD</span>[<span class="type">T</span>]] = &#123;</span><br><span class="line">    <span class="comment">// If RDD was already generated, then retrieve it from HashMap,</span></span><br><span class="line">    <span class="comment">// or else compute the RDD</span></span><br><span class="line">    generatedRDDs.get(time).orElse &#123;</span><br><span class="line"></span><br><span class="line"><span class="number">4</span>&gt;     </span><br><span class="line">发现父类创建的 <span class="type">RDD</span> 是一个 <span class="type">HashMap</span>[<span class="type">Time</span>, <span class="type">RDD</span>[<span class="type">T</span>]]</span><br><span class="line"><span class="meta">@transient</span></span><br><span class="line"><span class="keyword">private</span>[streaming] <span class="keyword">var</span> generatedRDDs = <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">Time</span>, <span class="type">RDD</span>[<span class="type">T</span>]]()</span><br></pre></td></tr></table></figure><p><strong>总结:</strong> </p><ul><li>Spark Streaming是一个基于Spark Core之上的实时计算框架，可以从很多数据源消费数据并对数据进行处理</li><li>在Spark Streaing中有一个最基本的抽象叫DStream（代理），本质上就是一系列连续的RDD，DStream其实就是对RDD的封装</li><li>DStream可以认为是一个RDD的工厂，该DStream里面生产都是相同业务逻辑的RDD，只不过是RDD里面要读取数据的不相同</li><li><strong>深入理解DStream:</strong> 他是sparkStreaming中的一个最基本的抽象，代表了一下列连续的数据流，本质上是一系列连续的RDD，你对DStream进行操作，就是对RDD进行操作</li><li>DStream每隔一段时间生成一个RDD，你对DStream进行操作，本质上是对里面的对应时间的RDD进行操作</li><li>DSteam和DStream之间存在依赖关系，在一个固定的时间点，对个存在依赖关系的DSrteam对应的RDD也存在依赖关系，</li><li>每个一个固定的时间，其实生产了一个小的DAG，周期性的将生成的小DAG提交到集群中运行</li></ul><h1 id="二-Spark-Streaming-amp-Kafka-连接的2种方式"><a href="#二-Spark-Streaming-amp-Kafka-连接的2种方式" class="headerlink" title="二. Spark Streaming &amp; Kafka 连接的2种方式"></a>二. Spark Streaming &amp; Kafka 连接的2种方式</h1><h2 id="Receiver-based"><a href="#Receiver-based" class="headerlink" title="Receiver-based"></a>Receiver-based</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-02-Receiver%E6%96%B9%E5%BC%8F.png" alt=""></p><h2 id="基于-Receiver-的wordcount-代码"><a href="#基于-Receiver-的wordcount-代码" class="headerlink" title="基于 Receiver 的wordcount 代码"></a>基于 Receiver 的wordcount 代码</h2><p><strong>主要是 直接使用 高级 api  <code>KafkaUtils.createStream</code></strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.spark.scala</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">ReceiverInputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka.<span class="type">KafkaUtils</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * SparkStreaming-Kafka</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KafkaWordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"KafkaWordCount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf,<span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> zkQuorum = <span class="string">"cs1:2181,cs2:2181,cs3:2181"</span></span><br><span class="line">    <span class="keyword">val</span> groupId = <span class="string">"g1"</span></span><br><span class="line">    <span class="keyword">val</span> topic = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>](<span class="string">"kafka-test"</span> -&gt; <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建 DStream, 需要 KafkaDStream</span></span><br><span class="line">    <span class="keyword">val</span> data: <span class="type">ReceiverInputDStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">KafkaUtils</span>.createStream(ssc, zkQuorum, groupId, topic)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//对数据进行处理</span></span><br><span class="line">    <span class="comment">//Kafak的ReceiverInputDStream[(String, String)]里面装的是一个元组（key是写入的key，value是实际写入的内容）</span></span><br><span class="line">    <span class="keyword">val</span> lines: <span class="type">DStream</span>[<span class="type">String</span>] = data.map(_._2)</span><br><span class="line"></span><br><span class="line">    println(<span class="string">"这里打印的是 元祖 中取出的第一个元素--key"</span> + data.map(_._1).toString)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//对DSteam进行操作，你操作这个抽象（代理，描述），就像操作一个本地的集合一样</span></span><br><span class="line">    <span class="comment">//切分压扁</span></span><br><span class="line">    <span class="keyword">val</span> words: <span class="type">DStream</span>[<span class="type">String</span>] = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 单词和1 组合</span></span><br><span class="line">    <span class="keyword">val</span> wordAndOne: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = words.map((_,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 聚合</span></span><br><span class="line">    <span class="keyword">val</span> reduced: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordAndOne.reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印结果</span></span><br><span class="line">    reduced.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 启动 sparkStreaming 程序</span></span><br><span class="line">    ssc.start()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 等待优雅的退出</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Direct-Approach-直连"><a href="#Direct-Approach-直连" class="headerlink" title="Direct Approach 直连"></a>Direct Approach 直连</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-02-%E7%9B%B4%E8%BF%9E%E6%96%B9%E5%BC%8F.png" alt=""></p><h2 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h2><ul><li>Receiver 接收固定时间间隔的数据(放在内存中的), 使用 Kafka 高级 API, 自动维护偏移量,  数据达到固定时间才进行处理, 效率低并且 容易丢失数据<ul><li>直接把偏移量记录到了 zk 当中, 但是效率低, 先拉数据, 拉到一定的时间再处理, 这样就有可能数据溢出, 为了保证数据不溢出, 就有一个 WALS 的机制(write ahead logs), 会把溢出的数据写入到 hdfs 中 ,  效率低</li></ul></li><li><p>Direct 直连方式, 相当于直接链接到 Kafka 的分区上, 使用 Kafka 底层的 API, 效率高, 需要自己维护偏移量</p><ul><li>直连方式, 就是一个迭代器, 变读边计算, 使用了 kafka 底层的 api, 直接连到了kafka 的分区文件上, 效率更高</li></ul></li><li><p>receiver 接受数据是在Executor端 cache </p><ul><li>如果使用的窗口函数的话，没必要进行cache, 默认就是cache， WAL ；</li><li>如果采用的不是窗口函数操作的话，你可以cache, 数据会放做一个副本放到另外一台节点上做容错</li></ul></li><li>direct直连方式, 接受数据是在Driver端</li></ul><h2 id="SparkStreaming-直连-kafka-代码"><a href="#SparkStreaming-直连-kafka-代码" class="headerlink" title="SparkStreaming 直连 kafka 代码"></a>SparkStreaming 直连 kafka 代码</h2><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/scala/KafkaDirectWordCount.scala" target="_blank" rel="noopener">另外一版本见, 主要是有个地方用的<code>transform</code>, 区别不大</a></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.spark.scala</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> kafka.common.<span class="type">TopicAndPartition</span></span><br><span class="line"><span class="keyword">import</span> kafka.message.<span class="type">MessageAndMetadata</span></span><br><span class="line"><span class="keyword">import</span> kafka.serializer.<span class="type">StringDecoder</span></span><br><span class="line"><span class="keyword">import</span> kafka.utils.&#123;<span class="type">ZKGroupTopicDirs</span>, <span class="type">ZkUtils</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.<span class="type">I0Itec</span>.zkclient.<span class="type">ZkClient</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">InputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka.&#123;<span class="type">HasOffsetRanges</span>, <span class="type">KafkaUtils</span>, <span class="type">OffsetRange</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Duration</span>, <span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * </span></span><br><span class="line"><span class="comment">  * 直连: 本质上就是创建一个 Kafka 的 inputDStream 的迭代器, kafka 产生一个数据, 这边就按照批次消费</span></span><br><span class="line"><span class="comment">  * 还可以提交到关系型数据库, 提交事务` </span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KafkaDirectWordCount2</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"KafkaDirectWordCount"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> brokerList = <span class="string">"cs2:9092,cs3:9092,cs4:9092"</span></span><br><span class="line">    <span class="keyword">val</span> zkQuorum = <span class="string">"cs1:2181,cs2:2181,cs3:2181,"</span></span><br><span class="line">    <span class="keyword">val</span> topic = <span class="string">"wordcount"</span></span><br><span class="line">    <span class="keyword">val</span> topics = <span class="type">Set</span>(topic)</span><br><span class="line">    <span class="keyword">val</span> group = <span class="string">"g001"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>(</span><br><span class="line">      <span class="string">"metadata.broker.list"</span> -&gt; brokerList,</span><br><span class="line">      <span class="string">"group.id"</span> -&gt; group,</span><br><span class="line">      <span class="comment">//从头开始读取数据</span></span><br><span class="line">      <span class="string">"auto.offset.reset"</span> -&gt; kafka.api.<span class="type">OffsetRequest</span>.<span class="type">SmallestTimeString</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> zkClient = <span class="keyword">new</span> <span class="type">ZkClient</span>(zkQuorum)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> topicDirs = <span class="keyword">new</span> <span class="type">ZKGroupTopicDirs</span>(group, topic)</span><br><span class="line">    <span class="keyword">val</span> zkTopicPath: <span class="type">String</span> = <span class="string">s"<span class="subst">$&#123;topicDirs.consumerOffsetDir&#125;</span>"</span>   <span class="comment">// ../topic =&gt; 拿到 zk 中存储的 consumerffset 的路径</span></span><br><span class="line">    <span class="keyword">val</span> childrenNum: <span class="type">Integer</span> = zkClient.countChildren(zkTopicPath)    <span class="comment">// 拿到路径 ../topic 下的子节点(分区)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> kafkaStream: <span class="type">InputDStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="literal">null</span>     <span class="comment">// 定义kafkaStream 流对象</span></span><br><span class="line">    <span class="keyword">var</span> fromOffsets: <span class="type">Map</span>[<span class="type">TopicAndPartition</span>, <span class="type">Long</span>] = <span class="type">Map</span>()      <span class="comment">// 以topic+partition 为 key, offset 为 value, 定义集合 Map 保证唯一性</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (childrenNum &gt; <span class="number">0</span>) &#123;        <span class="comment">// 存在分区信息</span></span><br><span class="line">      <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until childrenNum) &#123;</span><br><span class="line">        <span class="keyword">val</span> partitionOffset = zkClient.readData[<span class="type">String</span>](<span class="string">s"<span class="subst">$zkTopicPath</span>/<span class="subst">$&#123;i&#125;</span>"</span>)   <span class="comment">// 取出分区对应信息--&gt; offset</span></span><br><span class="line">        <span class="keyword">val</span> tp = <span class="type">TopicAndPartition</span>(topic, i)    <span class="comment">// 创建 tp 对象 (主题,分区)</span></span><br><span class="line">        fromOffsets += (tp -&gt; partitionOffset.toLong)</span><br><span class="line">      &#125;</span><br><span class="line">      println(<span class="string">"-------打印取出来的主题,分区 和 offset-------"</span>)</span><br><span class="line">      fromOffsets.foreach(println)</span><br><span class="line">      println(<span class="string">"------------------------------------------"</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> messageHandler = (mmd: <span class="type">MessageAndMetadata</span>[<span class="type">String</span>, <span class="type">String</span>]) =&gt; (mmd.key(), mmd.message())   <span class="comment">//定义 kafka 消息的 处理函数</span></span><br><span class="line"></span><br><span class="line">      kafkaStream = <span class="type">KafkaUtils</span>.createDirectStream</span><br><span class="line">        [<span class="type">String</span>, <span class="type">String</span>, <span class="type">StringDecoder</span>, <span class="type">StringDecoder</span>, (<span class="type">String</span>, <span class="type">String</span>)](ssc, kafkaParams, fromOffsets, messageHandler)   <span class="comment">// 拿到 kafkaDirectStream</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;    <span class="comment">// zk 中不存在分区信息, 就直接(从头)读取主题</span></span><br><span class="line">      kafkaStream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>, <span class="type">StringDecoder</span>, <span class="type">StringDecoder</span>](ssc, kafkaParams, topics)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 遍历kafkaStream</span></span><br><span class="line"><span class="comment">      * 数据收集到 kafka 中的时候, 对应的OffsetRange 啥的就已经确定了, 比如当前的 untilOffset 是多少</span></span><br><span class="line"><span class="comment">      * 这里只是拿到了这个流, 从中读取数据后, 顺便再把这个offset 存到zk 中去罢了</span></span><br><span class="line"><span class="comment">      这里可以先搞个 transform 把 offsetRanges 取出来</span></span><br><span class="line"><span class="comment">      </span></span><br><span class="line"><span class="comment">        var offsetRanges = Array[OffsetRange]()</span></span><br><span class="line"><span class="comment">        val transform: DStream[(String, String)] = kafkaStream.transform &#123; rdd =&gt;</span></span><br><span class="line"><span class="comment">        offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges</span></span><br><span class="line"><span class="comment">        rdd</span></span><br><span class="line"><span class="comment">      &#125;</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">  </span><br><span class="line">    kafkaStream.foreachRDD &#123; rdd =&gt;                                                             <span class="comment">// 迭代</span></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">        * OffsetRange(topic: 'wordcount', partition: 0, range: [20 -&gt; 20])</span></span><br><span class="line"><span class="comment">          OffsetRange(topic: 'wordcount', partition: 1, range: [19 -&gt; 19])</span></span><br><span class="line"><span class="comment">          OffsetRange(topic: 'wordcount', partition: 2, range: [19 -&gt; 19])</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">      <span class="keyword">var</span> offsetRanges: <span class="type">Array</span>[<span class="type">OffsetRange</span>] = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges     <span class="comment">// 强转为HasOffsetRanges, 取出 kafka 中的 offsetRanges(是一个由 OffsetRange 组成的数组)</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">// 这里打印的都是从 kafka 读到的实时流的 信息</span></span><br><span class="line">      println(<span class="string">"-------遍历offsetRanges数组----------------"</span>)</span><br><span class="line">      offsetRanges.foreach(x =&gt; println(x))</span><br><span class="line">      println(<span class="string">"------------------------------------------"</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> inputV  = rdd.map(_._2)                                                               <span class="comment">// 从 kafkaStream 中取出 真正的内容</span></span><br><span class="line"></span><br><span class="line">      inputV.foreachPartition(partition =&gt;                                                      <span class="comment">//对 RDD 进行操作, 触发 Action</span></span><br><span class="line">        <span class="comment">//++++++++++++++++++++++++++++++++++++//++++++++++++++++++++++++++++++++++++</span></span><br><span class="line">        partition.foreach(println)        <span class="comment">// 这里处理所有的业务逻辑代码, 这里仅仅只是做了打印</span></span><br><span class="line">        <span class="comment">//++++++++++++++++++++++++++++++++++++//++++++++++++++++++++++++++++++++++++</span></span><br><span class="line">      )</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 这里其实就相当于: 读取一个 (DStream) 的数据, 就往 zk 中更新一次 offset</span></span><br><span class="line">      <span class="keyword">for</span> (o &lt;- offsetRanges) &#123;                                                                  <span class="comment">// 从存储偏移量数组中取出值</span></span><br><span class="line">        <span class="keyword">val</span> zkPath = <span class="string">s"<span class="subst">$&#123;topicDirs.consumerOffsetDir&#125;</span>/<span class="subst">$&#123;o.partition&#125;</span>"</span>                            <span class="comment">// 从 zk 中取出当前消费的主题分区路径模板, 拼接上当前读到的分区, 更新上最新读到哪里</span></span><br><span class="line">        <span class="type">ZkUtils</span>.updatePersistentPath(zkClient, zkPath, o.untilOffset.toString)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 启动</span></span><br><span class="line">    ssc.start()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 优雅的等待结束</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="注意点"><a href="#注意点" class="headerlink" title="注意点:"></a>注意点:</h2><h4 id="RDD-的分区-amp-Kafka-的分区"><a href="#RDD-的分区-amp-Kafka-的分区" class="headerlink" title="RDD 的分区 &amp; Kafka 的分区"></a>RDD 的分区 &amp; Kafka 的分区</h4><ul><li>Kafka 的分区是物理的</li><li>Rdd 的分区是抽象的</li><li>Rdd 的分区 与 Kafka 的分区是 <strong>一一对应</strong>的</li></ul><h2 id="updateByKey-代码"><a href="#updateByKey-代码" class="headerlink" title="updateByKey 代码"></a>updateByKey 代码</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.spark.scala</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">HashPartitioner</span>, <span class="type">SparkConf</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">ReceiverInputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka.<span class="type">KafkaUtils</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 累加的 wordcount</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KafkaStatefulWordCount</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"KafkaWordCount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf,<span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果要使用updateStateByKey累加历史数据, 那么要把每次的结果保存起来</span></span><br><span class="line">    ssc.checkpoint(<span class="string">"file:///Users/shixuanji/Documents/Code/Datas/updatebykey"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> zkQuorum = <span class="string">"cs1:2181,cs2:2181,cs3:2181"</span></span><br><span class="line">    <span class="keyword">val</span> groupId = <span class="string">"g1"</span></span><br><span class="line">    <span class="keyword">val</span> topic = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>](<span class="string">"kafka-test"</span> -&gt; <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建 DStream, 需要 KafkaDStream</span></span><br><span class="line">    <span class="keyword">val</span> data: <span class="type">ReceiverInputDStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">KafkaUtils</span>.createStream(ssc, zkQuorum, groupId, topic)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//对数据进行处理</span></span><br><span class="line">    <span class="comment">//Kafak的ReceiverInputDStream[(String, String)]里面装的是一个元组（key是写入的key，value是实际写入的内容）</span></span><br><span class="line">    <span class="keyword">val</span> lines: <span class="type">DStream</span>[<span class="type">String</span>] = data.map(_._2)</span><br><span class="line"></span><br><span class="line">    println(<span class="string">"这里打印的是 元祖 中取出的第一个元素--key"</span> + data.map(_._1).toString)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//对DSteam进行操作，你操作这个抽象（代理，描述），就像操作一个本地的集合一样</span></span><br><span class="line">    <span class="comment">//切分压扁</span></span><br><span class="line">    <span class="keyword">val</span> words: <span class="type">DStream</span>[<span class="type">String</span>] = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 单词和1 组合</span></span><br><span class="line">    <span class="keyword">val</span> wordAndOne: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = words.map((_,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 聚合</span></span><br><span class="line">    <span class="keyword">val</span> reduced: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordAndOne.updateStateByKey(updateFunc,<span class="keyword">new</span> <span class="type">HashPartitioner</span>(ssc.sparkContext.defaultParallelism),<span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印结果(Action)</span></span><br><span class="line">    reduced.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 启动 sparkStreaming 程序</span></span><br><span class="line">    ssc.start()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 等待优雅的退出</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * updateFunc: (Iterator[(K, Seq[V], Option[S])]) =&gt; Iterator[(K, S)]</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="keyword">val</span> updateFunc = (iter:<span class="type">Iterator</span>[(<span class="type">String</span>, <span class="type">Seq</span>[<span class="type">Int</span>],<span class="type">Option</span>[<span class="type">Int</span>])]) =&gt; &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 第一个参数：聚合的key，就是单词</span></span><br><span class="line"><span class="comment">      * 第二个参数：当前批次产生批次该单词在每一个分区出现的次数</span></span><br><span class="line"><span class="comment">      * 第三个参数：初始值或累加的中间结果</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line"><span class="comment">//    iter.map(t =&gt; (t._1, t._2.sum + t._3.getOrElse(0)))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 这里可以使用更高级的模式匹配</span></span><br><span class="line">    iter.map&#123;<span class="keyword">case</span>(x, y, z) =&gt; (x, y.sum + z.getOrElse(<span class="number">0</span>))&#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="三-数据累加的方式"><a href="#三-数据累加的方式" class="headerlink" title="三. 数据累加的方式"></a>三. 数据累加的方式</h1><h2 id="1-使用-updateByKey"><a href="#1-使用-updateByKey" class="headerlink" title="1.. 使用 updateByKey"></a>1.. 使用 updateByKey</h2><p>见上面代码,  需要设置一个处理的函数,  需要设置 <code>checkPoint</code>,  不如直接在数据库中累加 和查询的方便</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;0-StreamingContext-的创建过程解析&quot;&gt;&lt;a href=&quot;#0-StreamingContext-的创建过程解析&quot; class=&quot;headerlink&quot; title=&quot;0.  StreamingContext 的创建过程解析&quot;&gt;&lt;/a&gt;0.  St
      
    
    </summary>
    
      <category term="Spark" scheme="https://airpoet.github.io/categories/Spark/"/>
    
      <category term="精讲" scheme="https://airpoet.github.io/categories/Spark/%E7%B2%BE%E8%AE%B2/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Spark" scheme="https://airpoet.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>i-Storm</title>
    <link href="https://airpoet.github.io/2018/08/01/Storm/i-Storm/"/>
    <id>https://airpoet.github.io/2018/08/01/Storm/i-Storm/</id>
    <published>2018-08-01T15:23:02.937Z</published>
    <updated>2018-08-01T17:34:31.655Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一-Storm-概述"><a href="#一-Storm-概述" class="headerlink" title="一. Storm 概述"></a>一. Storm 概述</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-01-164214.png" alt="image-20180802004158022"></p><p><a href="http://storm.apache.org/" target="_blank" rel="noopener">官网：</a></p><p><a href="http://storm.apache.org/downloads.html" target="_blank" rel="noopener">下载：</a></p><p><a href="http://storm.apache.org/releases/1.0.3/index.html" target="_blank" rel="noopener">文档：</a></p><p><a href="https://app.yinxiang.com/shard/s37/nl/7399077/bea5eec8-e207-466e-baf8-1e4b9af808c2/" target="_blank" rel="noopener">参考 PDF:</a></p><p>免费、开源、分布式、实时计算系统。 </p><p>吞吐量高。 </p><p>每秒每节点百万元组。 </p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-01-164403.png" alt="image-20180802004403219"></p><p><strong>Storm &amp; Hadoop 对比</strong></p><table><thead><tr><th style="text-align:center">storm</th><th style="text-align:center">hadoop</th></tr></thead><tbody><tr><td style="text-align:center">实时流处理</td><td style="text-align:center">批处理</td></tr><tr><td style="text-align:center">无状态</td><td style="text-align:center">有状态</td></tr><tr><td style="text-align:center">使用zk协同的主从架构</td><td style="text-align:center">无zk的主从架构。</td></tr><tr><td style="text-align:center">每秒处理数万消息</td><td style="text-align:center">HDFS MR数分钟、数小时</td></tr><tr><td style="text-align:center">不会主动停止</td><td style="text-align:center">终有完成的时候。</td></tr></tbody></table><h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><h4 id="Topology"><a href="#Topology" class="headerlink" title="Topology"></a>Topology</h4><ul><li>Spout + bolt连接在一起形成一个top拓扑，形成有向图，定点就是计算，边是数据流。 </li><li>Storm 的拓扑是对实时计算应用 逻辑的封装，它的作用与 MapReduce 的任务（Job）很相似，区别在于 MapReduce 的一个 Job 在得到结果之后总会结束，而拓扑会一直在集群中运行</li></ul><h4 id="Tuple"><a href="#Tuple" class="headerlink" title="Tuple"></a>Tuple</h4><ul><li>主要的数据结构，有序元素的列表。</li></ul><h4 id="Stream"><a href="#Stream" class="headerlink" title="Stream"></a>Stream</h4><ul><li><p>Streams 是 storm 最核心的抽象概念，<strong>一个 Stream 是分布式环境中并行创建和处理的一个没有边界的 tuple 序列，Streams 是由 Tuple(元组)组成的</strong>，Tuple 支持的类型有 Integer、Long、 Short、Byte、String、Double、Float、Boolean、Byte Arrays</p><p>当然，Tuple 也支持可序列化的对象。</p><p>数据流可以由一种能够表述数据流中元组的域（fields）的模式来定义。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 定义数据流的字段名称和顺序</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">declareOutputFields</span><span class="params">(OutputFieldsDeclarer outputFieldsDeclarer)</span> </span>&#123;</span><br><span class="line">    outputFieldsDeclarer.declare(<span class="keyword">new</span> Fields(<span class="string">"word"</span>));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 该方法就是接受到一行字符串，然后按照空格切割成多个单词</span></span><br><span class="line"><span class="comment">* 每个单词都通过 outputCollector 往外发送给下一个处理单元</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">(Tuple tuple)</span> </span>&#123;</span><br><span class="line">        String line = tuple.getStringByField(<span class="string">"line"</span>);</span><br><span class="line">        String[] words = line.trim().split(<span class="string">" "</span>);</span><br><span class="line">        <span class="keyword">for</span>(String word : words)&#123;</span><br><span class="line">        <span class="keyword">this</span>.outputCollector.emit(<span class="keyword">new</span> Values(word));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h4 id="Spouts"><a href="#Spouts" class="headerlink" title="Spouts"></a>Spouts</h4><ul><li>数据流源头。可以读取kafka队列消息。可以自定义。</li><li><strong>Spout 是主动模式</strong>，Spout 继承 BaseRichSpout 或者实现 IRichSpout 接口不断的调用 nextTuple() 函数，然后通过 emit 发送数据流。</li><li>有 <code>ack</code>和<code>fail</code>方法, 处理回调的 ack, fail 事件</li></ul><h4 id="Bolts"><a href="#Bolts" class="headerlink" title="Bolts"></a>Bolts</h4><ul><li>bolt 接收 Spout 或者上游的 Bolt 发来的 Tuple(数据流)，拓扑中所有的数据处理均是由 Bolt 完成的。通过数据过滤（filter）、函数处理（function）、聚合（aggregations）、联结（joins）、 数据库交互等功能，Bolt 几乎能够完成任何一种数据处理需求。一个 Bolt 可以实现简单的数 据流转换，而更复杂的数据流变换通常需要使用多个 Bolt 并通过多个步骤完成。</li><li><strong>Bolt 是被动模式</strong>，Bolt 继承 BaseBasicBolt 类或者实现 IRichBolt 接口等来实现，当 Bolt 接收 Spout 或者上游的 Bolt 发来的 Tuple(数据流)时调用 execute 方法，并对数据流进行处理完， OutputCollector 的 emit 发送数据流，execute 方法在 Bolt 中负责接收数据流和发送数据流。</li></ul><h4 id="task"><a href="#task" class="headerlink" title="task"></a>task</h4><ul><li>Bolt中每个Spout或者bolt都是一个task.</li></ul><h4 id="Stream-Grouping-※"><a href="#Stream-Grouping-※" class="headerlink" title="Stream Grouping ※"></a>Stream Grouping ※</h4><ul><li><p><strong>Storm 是通过 <code>Stream Grouping</code> 把 <code>spouts</code> 和 <code>Bolts</code> 串联起来组成了流数据处理结构。</strong></p></li><li><p>数据流分组定义了在 Bolt 的不 同任务（tasks）中划分数据流的方式。</p></li><li><p>Storm 对 Stream groupings 定义了 <strong>8 种数据流分组方式</strong>：</p></li><li><p>1、<strong>Shuffle grouping（随机分组）</strong>：对 Tuple(数据流)随机的分发到下一个 bolt 的 Tasks(任务)， 每个任务同等机会获取 Tuple，保证了集群的负载均衡。</p><p>2、<strong>Fields grouping（字段分组）</strong>：对指定的字段对数据流进行分组，相同的字段对应的数据 流都会交给同个 bolt 中 Task 来处理，不同的字段的数据流会分发到不同的 Task 来处理。</p><p>3、Partial Key grouping（部分字段分组）：按字段进行分组，这种跟 Fields grouping 很相似， 但这种方式会考虑下游 Bolt 数据处理的均衡性问题，会提供更好的资源利用。</p><p>4、All grouping（完全分组）：Tuple(数据流)会被同时的发送到下一个 bolt 中的所有 Task(任 务)。这种会导致网络传输量很大，慎重使用</p><p>5、Global grouping（全局分组）：Tuple(数据流)会被发送到下一个 Bolt 的 Id 最小的 Task(任 务)。</p><p>6、None grouping（无分组）：使用这种方式说明你不关心数据流如何分组。目前这种方式的 结果与随机分组完全等效，不过未来可能会考虑通过非分组方式来让 Bolt 和它所订阅的 Spout 或 Bolt 在同一个线程中执行。</p><p>7、Direct grouping（直接分组）：通过 OutputCollector emitDirect 方法指定下一个 bolt 的具体 Task 来处理。</p><p>8、Local or shuffle grouping（本地或随机分组）：如果目标 Bolt 有一个或更多的任务在相同 的 Worker 进程中，Tuple 就发送给这些 Task，否则 Tuple 会被随机分发（跟 Shuffle grouping 一样）。</p></li></ul><h2 id="Storm-架构"><a href="#Storm-架构" class="headerlink" title="Storm 架构"></a>Storm 架构</h2><ol><li><p><strong>Nimbus</strong>(灵气)<br>  master节点。<br>  核心组件，运行top。<br>  分析top并收集运行task。分发task给supervisor.<br>  监控top。<br>  无状态，依靠zk监控top的运行状况。</p></li><li><p><strong>Supervisor</strong>(监察)<br>  <strong>每个supervisor有n个worker进程</strong>，负责代理task给worker。<br>  worker在孵化执行线程最终运行task。<br>  storm使用内部消息系统在nimbus和supervisor之间进行通信。</p><p>  <strong>接受nimbus指令，管理worker进程完成task派发。</strong></p></li><li><p><strong>worker</strong><br>  执行特定的task，<strong>worker本身不执行任务，而是孵化executors，</strong><br>  <strong>让executors执行task。</strong></p></li></ol><h2 id="Storm-工作流程"><a href="#Storm-工作流程" class="headerlink" title="Storm 工作流程"></a>Storm 工作流程</h2><p><img src="" alt=""> </p><ol><li>nimbus等待提交的top</li><li>提交top后，nimbus收集task，</li><li>nimbus分发task给所有可用的supervisor</li><li>supervisor周期性发送心跳给nimbus表示自己还活着。</li><li>如果supervisor挂掉，不会发送心跳给nimubs，nimbus将task发送给其他的supervisor</li><li>nimubs挂掉，super会继续执行自己task。</li><li>task完成后，supervisor等待新的task</li><li>同时，挂掉的nimbus可以通过监控工具软件自动重启。</li></ol><h2 id="Storm-集群安装-amp-启动"><a href="#Storm-集群安装-amp-启动" class="headerlink" title="Storm 集群安装 &amp; 启动"></a>Storm 集群安装 &amp; 启动</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">1. jdk</span><br><span class="line">2. tar</span><br><span class="line">3. 环境变量</span><br><span class="line">4. 验证安装</span><br><span class="line">     $&gt; <span class="built_in">source</span> ~/.zshrc</span><br><span class="line">     $&gt; storm version</span><br><span class="line">5. 分发安装文件到其他节点。 </span><br><span class="line"> </span><br><span class="line">6. 配置[storm/conf/storm.yaml]</span><br><span class="line"></span><br><span class="line">     storm.local.dir: <span class="string">"/home/ap/storm"</span></span><br><span class="line">     storm.zookeeper.servers:</span><br><span class="line">         - <span class="string">"cs1"</span></span><br><span class="line">         - <span class="string">"cs2"</span></span><br><span class="line">         - <span class="string">"cs2"</span></span><br><span class="line">     <span class="comment">### nimbus.* configs are for the master</span></span><br><span class="line">     nimbus.seeds : [<span class="string">"cs1"</span>,<span class="string">"cs6"</span>]</span><br><span class="line"> </span><br><span class="line">7. 分发</span><br><span class="line"> </span><br><span class="line">8. 启动进程</span><br><span class="line">    1、首先在 cs1 和 cs6 机器上启动 nimbus：</span><br><span class="line">    nohup <span class="variable">$STORM_HOME</span>/bin/storm nimbus 1&gt;~/logs/storm-nibus.log 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line">    2、然后在 cs1 和 cs6 节点上启动 Storm UI:</span><br><span class="line">    nohup <span class="variable">$STORM_HOME</span>/bin/storm ui 1&gt;~/logs/storm-ui.log 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line">    3、然后在每一个 supervisor 节点(cs2,3,4)上启动 supervisor 进程：</span><br><span class="line">    nohup <span class="variable">$STORM_HOME</span>/bin/stor</span><br><span class="line">    m supervisor 1&gt;~/logs/storm-supervisor.log 2&gt;&amp;1 &amp;</span><br><span class="line">    </span><br><span class="line">9.通过webui查看</span><br><span class="line">    http://cs1:8080/</span><br><span class="line">    http://cs6:8080/</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">10. 启动 logviewer (可选)</span><br><span class="line">supervisor 节点:  </span><br><span class="line">$&gt; storm logviewer &amp;</span><br><span class="line">点击 cs1:8080 中的 host 名字, 就会进到对应的 logger 中</span><br></pre></td></tr></table></figure><h1 id="二-代码实例"><a href="#二-代码实例" class="headerlink" title="二. 代码实例"></a>二. 代码实例</h1><h2 id="1-编程实现CallLog日志统计"><a href="#1-编程实现CallLog日志统计" class="headerlink" title="1. 编程实现CallLog日志统计"></a>1. 编程实现CallLog日志统计</h2><ul><li><p><strong>xml 文件</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.storm<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>storm-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>代码</p><ul><li><a href="https://github.com/airpoet/bigdata/tree/master/Storm_Project/StormCalllogDemo/src/main/java/com/rox/storm/calllog" target="_blank" rel="noopener">见 github</a></li></ul></li><li><p>打成 jar 包在 centos 上运行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在centos上运行top</span></span><br><span class="line">$&gt; storm jar StormCalllogDemo-1.0-SNAPSHOT.jar com.rox.storm_wordcount.App</span><br></pre></td></tr></table></figure></li></ul><h2 id="2-使用storm流计算实现wordcount"><a href="#2-使用storm流计算实现wordcount" class="headerlink" title="2. 使用storm流计算实现wordcount"></a>2. 使用storm流计算实现wordcount</h2><p><a href="https://github.com/airpoet/bigdata/tree/master/Storm_Project/StormCalllogDemo/src/main/java/com/rox/storm/wordcount" target="_blank" rel="noopener">代码见我的 github</a></p><p><strong>注意点:</strong></p><ul><li>放到集群上运行后, 搜集结果不容易</li><li>加上了一个 <strong>Util</strong> 工具类. <ul><li><strong>3个阶段 向 <code>cs1</code> 通过 <code>socket</code> 经由 不同的 端口<code>port</code> 发送自身消息, 在<code>cs1</code>上, 用<code>nc</code> 开启监听</strong></li></ul></li></ul><h2 id="3-设置top的并发程度和任务"><a href="#3-设置top的并发程度和任务" class="headerlink" title="3. 设置top的并发程度和任务"></a>3. 设置top的并发程度和任务</h2><p><strong>parallelism = (task.count)spout  +  (task.count)bolt</strong></p><p>The total parallelism of the topology can be calculated with the total parallelism = number of spout tasks + number of bolt tasks formula. </p><p><strong>配置并发度.</strong></p><ol><li><p><strong>设置worker数据</strong><br>  conf.setNumWorkers(1);</p></li><li><p><strong>设置executors个数 </strong><br>  //设置Spout的并发hint (executor个数)<br>  builder.setSpout(“wcspout”, new WordCountSpout(),3);</p><p>  //设置bolt的并发暗示<br>  builder.setBolt(“split-bolt”, new SplitBolt(),4)</p></li><li><p><strong>设置task个数</strong><br>  builder.setSpout(“wcspout”, new WordCountSpout(),3).setNumTasks(3);<br>  //<br>  builder.setBolt(“split-bolt”, new SplitBolt(),4).shuffleGrouping(“wcspout”).setNumTasks(4);</p></li><li><p><strong>并发度 = 所有的task个数的总和。 </strong><br> 如果手动设置 task, task 为设置的数量,  不设置则为并发度的数量</p></li></ol><ul><li>task执行在 executor 上, executor是线程级别的, 一个executor默认是执行一个 task, 如果不手动设置的话, 设置用 setNumTasks(x), 注意这里设置的是当前总 task 的数量. </li><li>executor 执行在 worker 上, worker 是进程级别的, 一个 worker 可以有多个 executor </li><li>worker 由 supervisor 管理, 一个 supervisor 节点可以有多个 worker , 设置用 setNumWorkers(x) </li></ul><p><strong>一个 Storm 集群只有一个Nimbus 的 leader。</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-01-173110.png" alt="image-20180802013109948"></p><p>==&gt;  注意: 这里 cs4,37774 进程中的 一个executor 中启动了2个 task, 因为设置了并行 parallelism_hint =  3.setNumTasks(4), 那么肯定有一个 executor 中会执行2个 task</p><p>==&gt; 证实: <strong>设置了多少个 worker, 就有多少个进程</strong>, worker 数超过执行节点(supervisor)的数量时, 某一个sp 肯定会开启多个 worker, tasks 数量, 等价于对象的 个数</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一-Storm-概述&quot;&gt;&lt;a href=&quot;#一-Storm-概述&quot; class=&quot;headerlink&quot; title=&quot;一. Storm 概述&quot;&gt;&lt;/a&gt;一. Storm 概述&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.cloud
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="Storm" scheme="https://airpoet.github.io/categories/Hadoop/Storm/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="Storm" scheme="https://airpoet.github.io/tags/Storm/"/>
    
  </entry>
  
  <entry>
    <title>Spark重点解析(三) =&gt; Spark SQL</title>
    <link href="https://airpoet.github.io/2018/07/27/Spark/Spark%E9%87%8D%E7%82%B9%E8%A7%A3%E6%9E%90(%E4%B8%89)-=-Spark-SQL/"/>
    <id>https://airpoet.github.io/2018/07/27/Spark/Spark重点解析(三)-=-Spark-SQL/</id>
    <published>2018-07-27T06:32:27.283Z</published>
    <updated>2018-07-30T03:09:19.161Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一-SparkSQL-的前世今生"><a href="#一-SparkSQL-的前世今生" class="headerlink" title="一. SparkSQL 的前世今生"></a>一. SparkSQL 的前世今生</h1><ul><li>Hive =&gt; MapReduce =&gt; HDFS</li><li>Shark =&gt; 使用 Hive 的 SQL 解析引擎 =&gt; RDD =&gt; 通过Hive 的metadata表去操作 HDFS</li><li>SparkSQL =&gt; <strong>使用自己SQL 解析引擎</strong> =&gt; RDD =&gt; 通过Hive 的metadata表去操作 HDFS</li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-27-064024.jpg" alt=""></p><hr><h1 id="二-SparkSession"><a href="#二-SparkSession" class="headerlink" title="二. SparkSession"></a>二. SparkSession</h1><p>在 Spark 的早期版本(<strong>1.x 版本)</strong>中，SparkContext 是 Spark 的主要切入点，由于 RDD 是主要的 API，我 们通过 sparkContext 来创建和操作 RDD。对于每个其他的 API，我们需要使用不同的 context。 例如：</p><p>SparkContext      =&gt; 创建 RDD</p><p>StreamingContext =&gt; 创建 Streaming</p><p>SQLContext          =&gt; 创建 SQL</p><p>HiveContext          =&gt; 创建 Hive</p><p><strong>从 Spark 2.0开始, 引入 <code>SparkSession</code></strong></p><p>—- 为用户提供一个统一的切入点使用 Spark 各项功能</p><p>—- 允许用户通过它调用 DataFrame 和 Dataset 相关 API 来编写程序</p><p>—- 减少了用户需要了解的一些概念，可以很容易的与 Spark 进行交互</p><p>—- 与 Spark 交互之时不需要显示的创建 SparkConf、SparkContext 以及 SQlContext，这些对象已经封闭在 SparkSession 中</p><p>—- SparkSession 提供对 Hive 特征的内部支持：用 HiveQL 写 SQL 语句，访问 Hive UDFs，从 Hive 表中读取数据。</p><hr><h1 id="三-DataFrame"><a href="#三-DataFrame" class="headerlink" title="三. DataFrame"></a>三. DataFrame</h1><p><strong>注意</strong>: 这里的操作都是基于1.x 版本,  与2.x 的区别就是2.x 统一了操作入口为 SparkSession</p><p><a href="https://airpoet.github.io/2018/07/20/Spark/i-Spark-2/#more">2.x 的操作代码见我的另一篇 bolg</a></p><blockquote><p>从 json 文件读取为 dataframe, 使用spark 的 api 调用</p></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SQLContext</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataFrameOperation</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//create  datarame</span></span><br><span class="line">    <span class="comment">//首先创建程序入口</span></span><br><span class="line">    <span class="keyword">val</span> conf=<span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"DataFrameOperation"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc=<span class="keyword">new</span> <span class="type">SparkContext</span>(conf);</span><br><span class="line">    <span class="comment">//create sqlcontext</span></span><br><span class="line">    <span class="keyword">val</span> sqlContext=<span class="keyword">new</span> <span class="type">SQLContext</span>(sc);</span><br><span class="line">    <span class="keyword">val</span> df=sqlContext.read.json(<span class="string">"hdfs://mycluster/user/ap/sparkdatas/people"</span>)</span><br><span class="line">    df.show();  </span><br><span class="line">    </span><br><span class="line">    <span class="comment">//print schema</span></span><br><span class="line">    df.printSchema()</span><br><span class="line">    <span class="comment">//name age</span></span><br><span class="line">    df.select(<span class="string">"name"</span>).show();</span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    df.select(df(<span class="string">"name"</span>),df(<span class="string">"age"</span>)+<span class="number">1</span>).show()</span><br><span class="line">    <span class="comment">//where</span></span><br><span class="line">    df.filter(df(<span class="string">"age"</span>) &gt; <span class="number">21</span>).show()</span><br><span class="line">    <span class="comment">//groupby</span></span><br><span class="line">    df.groupBy(<span class="string">"age"</span>).count().show();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="四-RDD-转为-DataFrame"><a href="#四-RDD-转为-DataFrame" class="headerlink" title="四. RDD 转为 DataFrame"></a>四. RDD 转为 DataFrame</h1><h2 id="1-使用对象-RDD-反射-Reflection-的方式"><a href="#1-使用对象-RDD-反射-Reflection-的方式" class="headerlink" title="1.使用对象 RDD 反射 Reflection 的方式"></a>1.使用对象 RDD 反射 Reflection 的方式</h2><blockquote><p>Scala 方式</p></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">步骤: </span><br><span class="line"><span class="number">1</span>) 构造一个封装了指定对象的<span class="type">RDD</span></span><br><span class="line"><span class="number">2</span>) 引入sparksession的隐式转换</span><br><span class="line"><span class="number">3</span>) 调用 rdd.toDF()</span><br><span class="line">=====================================================================</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SQLContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">age:<span class="type">Int</span>,name:<span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">RDD2DataFrameReflection</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">     <span class="comment">//create  datarame</span></span><br><span class="line">    <span class="comment">//首先创建程序入口</span></span><br><span class="line">    <span class="keyword">val</span> conf=<span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"RDD2DataFrameReflection"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc=<span class="keyword">new</span> <span class="type">SparkContext</span>(conf);</span><br><span class="line">  </span><br><span class="line">    <span class="comment">//create sqlcontext</span></span><br><span class="line">    <span class="keyword">val</span> sqlContext=<span class="keyword">new</span> <span class="type">SQLContext</span>(sc);</span><br><span class="line">    <span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line">    <span class="keyword">val</span> personRDD= sc.textFile(<span class="string">"hdfs://mycluster/user/ap/sparkdatas/peopletxt/people.txt"</span>, <span class="number">2</span>)</span><br><span class="line">    .map &#123; line =&gt; line.split(<span class="string">","</span>) &#125;.map &#123; p =&gt; <span class="type">Person</span>(p(<span class="number">1</span>).trim().toInt,p(<span class="number">0</span>)) &#125;</span><br><span class="line">    <span class="keyword">val</span> personDF=personRDD.toDF()</span><br><span class="line">   <span class="comment">// 或者 personDF.createOrReplaceTempView("person") / createGlobalTempView / createTempView</span></span><br><span class="line">    personDF.registerTempTable(<span class="string">"person"</span>)</span><br><span class="line">    <span class="keyword">val</span> personDataframe=sqlContext.sql(<span class="string">"select name,age from person where age &gt; 13 and age &lt;= 19"</span>)</span><br><span class="line">    personDataframe.rdd.foreach &#123; row =&gt; println(row.getString(<span class="number">0</span>)+<span class="string">"  "</span>+row.getString(<span class="number">1</span>)) &#125;</span><br><span class="line">    personDataframe.rdd.saveAsTextFile(<span class="string">"hdfs://hadoop1:9000/reflectionresult/"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Java 方式</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.DataFrame;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SQLContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RDD2DataFrameReflection</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">conf.setAppName(<span class="string">"RDD2DataFrameReflection"</span>);</span><br><span class="line">JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">SQLContext sqlContext = <span class="keyword">new</span> SQLContext(sc);</span><br><span class="line">JavaRDD&lt;Person&gt; PersonRDD = sc.textFile(<span class="string">"hdfs://hadoop1:9000/examples/src/main/resources/people.txt"</span>)</span><br><span class="line">.map(<span class="keyword">new</span> Function&lt;String, Person&gt;() &#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> Person <span class="title">call</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">String[] strs = line.split(<span class="string">","</span>);</span><br><span class="line">String name=strs[<span class="number">0</span>];</span><br><span class="line"><span class="keyword">int</span> age=Integer.parseInt(strs[<span class="number">1</span>].trim());</span><br><span class="line">Person person=<span class="keyword">new</span> Person(age,name);</span><br><span class="line"><span class="keyword">return</span> person;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">DataFrame personDF = sqlContext.createDataFrame(PersonRDD, Person.class);</span><br><span class="line">personDF.registerTempTable(<span class="string">"person"</span>);</span><br><span class="line"></span><br><span class="line">DataFrame resultperson = sqlContext.sql(<span class="string">"select name,age from person where age &gt; 13 and age &lt;= 19"</span>);</span><br><span class="line">resultperson.javaRDD().foreach(<span class="keyword">new</span> VoidFunction&lt;Row&gt;() &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Row row)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">//把每一条数据都看成是一个row  row(0)=name  row(1)=age </span></span><br><span class="line">System.out.println(<span class="string">"name"</span>+row.getString(<span class="number">0</span>));</span><br><span class="line">System.out.println(<span class="string">"age"</span>+row.getInt(<span class="number">1</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">resultperson.javaRDD().saveAsTextFile(<span class="string">"hdfs://hadoop1:9000/reflectionresult"</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-使用构造StructType方式"><a href="#2-使用构造StructType方式" class="headerlink" title="2.  使用构造StructType方式"></a>2.  使用构造StructType方式</h2><blockquote><p>scala</p></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">要点: </span><br><span class="line"><span class="number">1</span>) 构造 <span class="type">StructType</span></span><br><span class="line"><span class="number">2</span>) 构造 rowRDD </span><br><span class="line"><span class="keyword">val</span> rowRDD = studentRDD.map(p =&gt; <span class="type">Row</span>(p(<span class="number">0</span>).toInt, p(<span class="number">1</span>).trim, p(<span class="number">2</span>).trim, p(<span class="number">3</span>).toInt, p(<span class="number">4</span>).trim))</span><br><span class="line"><span class="number">3</span>) 构造 <span class="type">DataFrame</span></span><br><span class="line">df = sparksession.createDataFrame(rowRDD, schema) </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SQLContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.<span class="type">StructType</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.<span class="type">StructField</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.<span class="type">StringType</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RDD2DataFrameProgrammatically</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> conf=<span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"RDD2DataFrameProgrammatically"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc=<span class="keyword">new</span> <span class="type">SparkContext</span>(conf);</span><br><span class="line">  </span><br><span class="line">    <span class="comment">//create sqlcontext</span></span><br><span class="line">    <span class="keyword">val</span> sqlContext=<span class="keyword">new</span> <span class="type">SQLContext</span>(sc);</span><br><span class="line">    <span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line">    <span class="keyword">val</span> personRDD= sc.textFile(<span class="string">"hdfs://hadoop1:9000/examples/src/main/resources/people.txt"</span>, <span class="number">1</span>)</span><br><span class="line">      </span><br><span class="line"><span class="comment">////////////////////////////////////////////////////////////////////////</span></span><br><span class="line">    <span class="comment">//create schema</span></span><br><span class="line">    <span class="keyword">val</span> schemaString=<span class="string">"nameage"</span>;</span><br><span class="line">    <span class="keyword">val</span> schema=<span class="type">StructType</span>(</span><br><span class="line">        schemaString.split(<span class="string">"\t"</span>).map &#123; fieldsName =&gt; <span class="type">StructField</span>(fieldsName,<span class="type">StringType</span>,<span class="literal">true</span>) &#125;</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">/**  参数</span></span><br><span class="line"><span class="comment">            case class StructField(</span></span><br><span class="line"><span class="comment">            name: String,</span></span><br><span class="line"><span class="comment">            dataType: DataType,</span></span><br><span class="line"><span class="comment">            nullable: Boolean = true,</span></span><br><span class="line"><span class="comment">            metadata: Metadata = Metadata.empty)</span></span><br><span class="line"><span class="comment">    构造方式:</span></span><br><span class="line"><span class="comment">    val schema = StructType(</span></span><br><span class="line"><span class="comment">              List(</span></span><br><span class="line"><span class="comment">                StructField("id", IntegerType, true),</span></span><br><span class="line"><span class="comment">                StructField("name", StringType, true),</span></span><br><span class="line"><span class="comment">                StructField("sex", StringType, true),</span></span><br><span class="line"><span class="comment">                StructField("age", IntegerType, true),</span></span><br><span class="line"><span class="comment">                StructField("department", StringType, true)</span></span><br><span class="line"><span class="comment">              )</span></span><br><span class="line"><span class="comment">)</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">     <span class="comment">//create rowrdd</span></span><br><span class="line">    <span class="keyword">val</span> rowRDD=personRDD.map &#123; line =&gt; line.split(<span class="string">","</span>) &#125;.map &#123; p =&gt; <span class="type">Row</span>(p(<span class="number">0</span>),p(<span class="number">1</span>)) &#125;</span><br><span class="line">    <span class="keyword">val</span> personDF=sqlContext.createDataFrame(rowRDD, schema)</span><br><span class="line">      </span><br><span class="line"><span class="comment">////////////////////////////////////////////////////////////////////////      </span></span><br><span class="line">    personDF.registerTempTable(<span class="string">"person"</span>);</span><br><span class="line">     <span class="keyword">val</span> personDataframe=sqlContext.sql(<span class="string">"select name,age from person where age &gt; 13 and age &lt;= 19"</span>)</span><br><span class="line">    </span><br><span class="line">     personDataframe.rdd.foreach &#123; row =&gt; println(row.getString(<span class="number">0</span>)+<span class="string">"=&gt;  "</span>+row.getString(<span class="number">1</span>)) &#125;</span><br><span class="line">    personDataframe.rdd.saveAsTextFile(<span class="string">"hdfs://hadoop1:9000/RDD2DataFrameProgrammatically/"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Java</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.DataFrame;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.RowFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SQLContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DataTypes;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructField;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructType;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RDD2DataFrameProgrammactically</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">conf.setAppName(<span class="string">"RDD2DataFrameProgrammactically"</span>);</span><br><span class="line">JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">SQLContext sqlContext = <span class="keyword">new</span> SQLContext(sc);</span><br><span class="line">JavaRDD&lt;String&gt; personRDD = sc.textFile(<span class="string">"hdfs://hadoop1:9000/examples/src/main/resources/people.txt"</span>);</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 这里的schemaString 是从数据库里面动态获取从来的</span></span><br><span class="line"><span class="comment"> * 在实际的开发中我们需要写另外的代码去获取</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">String schemaString=<span class="string">"nameage"</span>;</span><br><span class="line"><span class="comment">//create  schema</span></span><br><span class="line">ArrayList&lt;StructField&gt; list = <span class="keyword">new</span> ArrayList&lt;StructField&gt;();</span><br><span class="line"><span class="keyword">for</span>(String str:schemaString.split(<span class="string">"\t"</span>))&#123;</span><br><span class="line">list.add(DataTypes.createStructField(str, DataTypes.StringType, <span class="keyword">true</span>));</span><br><span class="line">&#125;</span><br><span class="line">StructType schema = DataTypes.createStructType(list);</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需要将RDD转换为一个JavaRDD《Row》</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">JavaRDD&lt;Row&gt; rowRDD = personRDD.map(<span class="keyword">new</span> Function&lt;String, Row&gt;() &#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> Row <span class="title">call</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">String[] fields = line.split(<span class="string">","</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> RowFactory.create(fields[<span class="number">0</span>],fields[<span class="number">1</span>]);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">DataFrame personDF = sqlContext.createDataFrame(rowRDD, schema);</span><br><span class="line">personDF.registerTempTable(<span class="string">"person"</span>);</span><br><span class="line"></span><br><span class="line">DataFrame resultperson = sqlContext.sql(<span class="string">"select name,age from person where age &gt; 13 and age &lt;= 19"</span>);</span><br><span class="line">resultperson.javaRDD().foreach(<span class="keyword">new</span> VoidFunction&lt;Row&gt;() &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Row row)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">//把每一条数据都看成是一个row  row(0)=name  row(1)=age </span></span><br><span class="line">System.out.println(<span class="string">"name"</span>+row.getString(<span class="number">0</span>));</span><br><span class="line">System.out.println(<span class="string">"age"</span>+row.getInt(<span class="number">1</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">resultperson.javaRDD().saveAsTextFile(<span class="string">"hdfs://hadoop1:9000/reflectionresult"</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3-DataSet-amp-DataFrame-amp-RDD"><a href="#3-DataSet-amp-DataFrame-amp-RDD" class="headerlink" title="3.DataSet &amp; DataFrame &amp; RDD"></a>3.DataSet &amp; DataFrame &amp; RDD</h2><p><strong>RDD 仅表示数据集，RDD 没有元数据，也就是说没有字段语义定义</strong></p><p><strong>DataFrame = RDD+Schema = SchemaRDD</strong></p><p><strong>Schema 是就是元数据，是语义描述信息。</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-27-084726.png" alt="image-20180727164726341"></p><p><strong>DataFrame 是一种特殊类型的 Dataset，DataSet[Row] = DataFrame</strong></p><p><strong>DataFrame 的数据类型统一是 Row,  缺点: </strong></p><ul><li>Row 不能直接操作 domain 对象</li><li>函数风格编程，没有面向对象风格的 API</li></ul><p>所以，Spark SQL 引入了 Dataset，扩展了 DataFrame API，提供了编译时类型检查，面向对象风格的 API。</p><ul><li>Dataset 可以和 DataFrame、RDD 相互转换。DataFrame=Dataset[Row]</li><li>可见 DataFrame 是一种特殊的 Dataset。</li></ul><p><strong>既然 Spark SQL 提供了 SQL 访问方式，那为什么还需要 DataFrame 和 Dataset 的 API 呢？</strong></p><ul><li>SQL 的表达能力却是有限的</li><li>DataFrame 和 Dataset 可以采用更加通用的语言（Scala 或 Python）来表达用户的 查询请求。</li><li>DataFrame / Dataset 的面向对象语法, 可以更快捕捉错误，因为 SQL 是运行时捕获异常，而 Dataset 是 编译时检查错误。</li></ul><p><strong>DataFrame &amp; Dataset 的部分 api</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-27-085823.png" alt="image-20180727165823013"></p><h4 id="总的来讲"><a href="#总的来讲" class="headerlink" title="总的来讲"></a>总的来讲</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataFream</span>=<span class="type">DataSet</span>[row]  <span class="comment">//弱类型</span></span><br><span class="line"><span class="type">DataFream</span>=<span class="type">Untyped</span> <span class="type">Dataset</span></span><br><span class="line"><span class="type">DataSet</span>=<span class="type">Untyped</span> <span class="type">Dataset</span> +typed <span class="type">Dataset</span></span><br></pre></td></tr></table></figure><h4 id="举个🌰"><a href="#举个🌰" class="headerlink" title="举个🌰"></a>举个🌰</h4><blockquote><p><strong>SparkCore: RDD</strong></p></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">defined</span> <span class="title">class</span> <span class="title">Person</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">val</span> <span class="title">rdd</span> </span>= sc.textFile(<span class="string">"/user/ap/sparkdatas/peopletxt/people.txt"</span>).map(line =&gt; line.split(<span class="string">","</span>)).map(x =&gt; <span class="type">Person</span>(x(<span class="number">0</span>),x(<span class="number">1</span>).trim.toLong))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Person</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">14</span>] at map at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"><span class="comment">// 这里得到的是 RDD[Person] 类型</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect</span><br><span class="line">res11: <span class="type">Array</span>[<span class="type">Person</span>] = <span class="type">Array</span>(<span class="type">Person</span>(<span class="type">Michael</span>,<span class="number">29</span>), <span class="type">Person</span>(<span class="type">Andy</span>,<span class="number">30</span>), <span class="type">Person</span>(<span class="type">Justin</span>,<span class="number">19</span>))</span><br></pre></td></tr></table></figure><blockquote><p><strong>SparkSQL: DataFrame</strong> </p></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">"/user/ap/sparkdatas/people"</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line"><span class="comment">// Row  =&gt;相当于 数据库的表里面的一行数据  DataFrame=DataSet[Row]</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>SparkSQL: DataSet</strong> </p></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// as[Person] 把 Json 读出来的 DataFrame 直接反射到 Person类上, 转为 DataSet  !! 还有这种骚操作??</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> personDS = spark.read.json(<span class="string">"/user/ap/sparkdatas/people"</span>).as[<span class="type">Person</span>]</span><br><span class="line"><span class="comment">// </span></span><br><span class="line">personDS: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Person</span>] = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 再把 dataSet 转为 dataFrame</span></span><br><span class="line"><span class="keyword">val</span> personDS2 = personDS.toDF </span><br><span class="line">res13: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br></pre></td></tr></table></figure><h4 id="来张图"><a href="#来张图" class="headerlink" title="来张图!"></a>来张图!</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-27-145640.png" alt="image-20180727225640182"></p><hr><h1 id="五-数据源之load-和-Save"><a href="#五-数据源之load-和-Save" class="headerlink" title="五. 数据源之load 和 Save"></a>五. 数据源之load 和 Save</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SQLContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hdfs.server.namenode.<span class="type">SafeMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SaveMode</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DatasourceLoadAndSave</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf=<span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"RDD2DataFrameReflection"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc=<span class="keyword">new</span> <span class="type">SparkContext</span>(conf);</span><br><span class="line">    <span class="keyword">val</span> sqlContext=<span class="keyword">new</span> <span class="type">SQLContext</span>(sc);</span><br><span class="line">  <span class="comment">//  sqlContext.read.json("person.json");</span></span><br><span class="line">    <span class="comment">//sparksql默认支持的是parquet文件格式</span></span><br><span class="line">  <span class="keyword">val</span> df=  sqlContext.read.load(<span class="string">"examples/src/main/resources/users.parquet"</span>);</span><br><span class="line">  <span class="comment">//  sqlContext.read.format("json").load("person.json")</span></span><br><span class="line">  <span class="comment">//  sqlContext.read.format("parquet").load("users.parquet")</span></span><br><span class="line">  <span class="comment">//df保存的时候，如果不指定保存的文件格式，默认就是parquet</span></span><br><span class="line">    df.select(<span class="string">"name"</span>, <span class="string">"age"</span>).write.save(<span class="string">"namesAndAges.parquet"</span>);</span><br><span class="line">  <span class="comment">//  df.select("name", "age").write.json("user.json")</span></span><br><span class="line">  <span class="comment">//   df.select("name", "age").write.format("json").save("user.json")</span></span><br><span class="line">    df.select(<span class="string">"name"</span>, <span class="string">"age"</span>).write.format(<span class="string">"parquet"</span>).save(<span class="string">"user.parquet"</span>)</span><br><span class="line">    </span><br><span class="line">    df.select(<span class="string">"name"</span>, <span class="string">"age"</span>).write.mode(<span class="type">SaveMode</span>.<span class="type">ErrorIfExists</span>).format(<span class="string">"json"</span>).save(<span class="string">"hdfs://hadoop1:9000/user.json"</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 保存结果文件的时候的策略</span></span><br><span class="line"><span class="comment">     * if data/table already exists,</span></span><br><span class="line"><span class="comment">       Append,    // contents of the DataFrame are expected to be appended to existing data</span></span><br><span class="line"><span class="comment">       Overwrite,   // existing data is expected to be overwritten</span></span><br><span class="line"><span class="comment">       ErrorIfExists,   // an exception is expected to be thrown</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="六-数据源之parquet-files-合并"><a href="#六-数据源之parquet-files-合并" class="headerlink" title="六. 数据源之parquet  files 合并"></a>六. 数据源之parquet  files 合并</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df1 = sc.makeRDD(<span class="number">1</span> to <span class="number">5</span>).map(i =&gt; (i, i * <span class="number">2</span>)).toDF(<span class="string">"single"</span>, <span class="string">"double"</span>)</span><br><span class="line">df1.write.parquet(<span class="string">"/user/ap/test_table/key=1"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df2 = sc.makeRDD(<span class="number">6</span> to <span class="number">10</span>).map(i =&gt; (i, i * <span class="number">3</span>)).toDF(<span class="string">"single"</span>, <span class="string">"triple"</span>)</span><br><span class="line">df2.write.parquet(<span class="string">"/user/ap/test_table/key=2"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//合并两个dataframe, 我们期望的就是合并出来应该是三个属性分别是single double triple</span></span><br><span class="line"><span class="comment">//实际上最后还会有一个分区就是keys</span></span><br><span class="line"><span class="keyword">val</span> df3 = sqlContext.read.option(<span class="string">"mergeSchema"</span>, <span class="string">"true"</span>).parquet(<span class="string">"/user/ap/test_table"</span>)</span><br><span class="line"></span><br><span class="line">df3.printSchema()</span><br><span class="line">df3.show()</span><br></pre></td></tr></table></figure><h1 id="七-数据源之-MySQL"><a href="#七-数据源之-MySQL" class="headerlink" title="七. 数据源之 MySQL"></a>七. 数据源之 MySQL</h1><p><a href="https://airpoet.github.io/2018/07/20/Spark/i-Spark-2/#more">在 idea 中访问 mysql 的, 看这里</a></p><p><strong>注意: 启动 Spark Shell，必须指定 mysql 连接驱动 jar 包</strong></p><blockquote><p>启动本机的单进程 Shell</p></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark-shell \</span><br><span class="line">--jars $<span class="type">SPARK_HOME</span>/jars/mysql-connector-java<span class="number">-5.1</span><span class="number">.40</span>-bin.jar \</span><br><span class="line">--driver-<span class="class"><span class="keyword">class</span><span class="title">-path</span> <span class="title">$SPARK_HOME/jars/mysql-connector-java-5</span>.1.40<span class="title">-bin</span>.<span class="title">jar</span></span></span><br></pre></td></tr></table></figure><blockquote><p>连接 Spark 集群的 shell</p></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">spark-shell \</span><br><span class="line">--driver-<span class="class"><span class="keyword">class</span><span class="title">-path</span> <span class="title">$SPARK_HOME/jars/mysql-connector-java-5</span>.1.40<span class="title">-bin</span>.<span class="title">jar</span> <span class="title">\</span></span></span><br><span class="line"><span class="class"><span class="title">--master</span> <span class="title">yarn</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">查询</span></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">val</span> <span class="title">jdbcDF</span> </span>= spark.read.format(<span class="string">"jdbc"</span>).option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://cs2:3306/test"</span>).option(<span class="string">"dbtable"</span>, <span class="string">"student"</span>).option(<span class="string">"user"</span>, <span class="string">"root"</span>).option(<span class="string">"password"</span>, <span class="string">"123"</span>).option(<span class="string">"driver"</span>,<span class="string">"com.mysql.jdbc.Driver"</span>).load();</span><br><span class="line"></span><br><span class="line">jdbcDF: org.apache.spark.sql.<span class="type">DataFrame</span> = [id: int, name: string ... <span class="number">2</span> more fields]</span><br><span class="line"></span><br><span class="line">scala&gt; jdbcDF.show</span><br><span class="line">+---+------------+--------+-----+</span><br><span class="line">| id|        name|  course|score|</span><br><span class="line">+---+------------+--------+-----+</span><br><span class="line">|  <span class="number">1</span>|     huangbo|    math|   <span class="number">81</span>|</span><br><span class="line">|  <span class="number">2</span>|     huangbo| englist|   <span class="number">87</span>|</span><br><span class="line">|  <span class="number">3</span>|     huangbo|computer|   <span class="number">67</span>|</span><br><span class="line">|  <span class="number">4</span>|     xuzheng|    math|   <span class="number">89</span>|</span><br><span class="line">|  <span class="number">5</span>|     xuzheng| english|   <span class="number">92</span>|</span><br><span class="line">|  <span class="number">6</span>|     xuzheng|computer|   <span class="number">83</span>|</span><br><span class="line">|  <span class="number">7</span>|wangbaoqiang|    math|   <span class="number">78</span>|</span><br><span class="line">|  <span class="number">8</span>|wangbaoqiang| english|   <span class="number">88</span>|</span><br><span class="line">|  <span class="number">9</span>|wangbaoqiang|computer|   <span class="number">90</span>|</span><br><span class="line">| <span class="number">10</span>|    dengchao|    math|   <span class="number">88</span>|</span><br><span class="line">+---+------------+--------+-----+</span><br></pre></td></tr></table></figure><blockquote><p>通过 Spark-submit 的方式</p></blockquote><p>编写代码</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//通过读取文件创建 RDD</span></span><br><span class="line"><span class="keyword">val</span> studentRDD = sc.textFile(args(<span class="number">0</span>)).map(_.split(<span class="string">","</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">//通过 StructType 直接指定每个字段的 schema</span></span><br><span class="line"><span class="keyword">val</span> schema = <span class="type">StructType</span>(</span><br><span class="line">  <span class="type">List</span>(</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"id"</span>, <span class="type">IntegerType</span>, <span class="literal">true</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"name"</span>, <span class="type">StringType</span>, <span class="literal">true</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"sex"</span>, <span class="type">StringType</span>, <span class="literal">true</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"age"</span>, <span class="type">IntegerType</span>, <span class="literal">true</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"department"</span>, <span class="type">StringType</span>, <span class="literal">true</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 RDD 映射到 rowRDD</span></span><br><span class="line"><span class="keyword">val</span> rowRDD = studentRDD.map(p =&gt; <span class="type">Row</span>(p(<span class="number">0</span>).toInt, p(<span class="number">1</span>).trim, p(<span class="number">2</span>).trim, p(<span class="number">3</span>).toInt,</span><br><span class="line">  p(<span class="number">4</span>).trim))</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 schema 信息应用到 rowRDD 上</span></span><br><span class="line"><span class="keyword">val</span> studentDataFrame = sqlContext.createDataFrame(rowRDD, schema)</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建 Properties 存储数据库相关属性</span></span><br><span class="line"><span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">prop.put(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">prop.put(<span class="string">"password"</span>, <span class="string">"root"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//将数据追加到数据库</span></span><br><span class="line">studentDataFrame.write.mode(<span class="string">"append"</span>).jdbc(<span class="string">"jdbc:mysql://hadoop02:3306/spider"</span>,</span><br><span class="line">  <span class="string">"student"</span>, prop)</span><br><span class="line"></span><br><span class="line"><span class="comment">//停止 SparkContext</span></span><br><span class="line">sc.stop()</span><br></pre></td></tr></table></figure><p>准备数据：student.txt 存储在 HDFS 上的/student 目录中</p><p>给项目打成 jar 包，上传到客户端</p><p>提交任务给 Spark 集群：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$<span class="type">SPARK_HOME</span>/bin/spark-submit \</span><br><span class="line">--<span class="class"><span class="keyword">class</span> <span class="title">com</span>.<span class="title">mazh</span>.<span class="title">spark</span>.<span class="title">sql</span>.<span class="title">SparkSQL_JDBC</span> <span class="title">\</span></span></span><br><span class="line"><span class="class"><span class="title">--master</span> <span class="title">yarn</span> <span class="title">\</span></span></span><br><span class="line"><span class="class"><span class="title">--driver-class-path</span> <span class="title">$SPARK_HOME/jars/mysql-connector-java-5</span>.1.40<span class="title">-bin</span>.<span class="title">jar</span> <span class="title">\</span></span></span><br><span class="line"><span class="class"><span class="title">/home/hadoop/Spark_WordCount-1</span>.0<span class="title">-SNAPSHOT</span>.<span class="title">jar</span> <span class="title">\</span></span></span><br><span class="line"><span class="class"><span class="title">hdfs</span></span>:<span class="comment">//mycluster/user/ap/sparkdatas/student.txt</span></span><br></pre></td></tr></table></figure><h1 id="八-数据源之json"><a href="#八-数据源之json" class="headerlink" title="八. 数据源之json"></a>八. 数据源之json</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 方式 1</span></span><br><span class="line"><span class="keyword">val</span> df1 = sparkSession.read.json(<span class="string">"file://.."</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 方式 2</span></span><br><span class="line"><span class="keyword">val</span> df2 = sparkSession.read.format(<span class="string">"json"</span>).load(<span class="string">"hdfs://mycluster/..."</span>)</span><br></pre></td></tr></table></figure><h1 id="九-数据源之-Hive"><a href="#九-数据源之-Hive" class="headerlink" title="九. 数据源之 Hive"></a>九. 数据源之 Hive</h1><p><a href="https://airpoet.github.io/2018/07/20/Spark/i-Spark-2/">其它可以参考我的另一篇文章</a></p><p>与之前的自己构造数据, 或者读<code>json</code>数据等,得到 <code>DataFrame</code> , 然后把此 <code>df.createTempView(“tmpView”)</code></p><p>然后再对此 <code>tmpView</code> 使用 sql 语句查询不一样</p><p><strong>直接用 <code>sparkSession.sql(“...”)</code> 操作的对象默认就是 hive</strong> </p><p><strong>==&gt; 访问 hive 的元数据库(mysql),  把 hive 底层的执行引擎 <code>MapReduce</code> 换成了<code>SparkCore</code></strong></p><h1 id="十-数据源之-HBase"><a href="#十-数据源之-HBase" class="headerlink" title="十. 数据源之 HBase"></a>十. 数据源之 HBase</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一-SparkSQL-的前世今生&quot;&gt;&lt;a href=&quot;#一-SparkSQL-的前世今生&quot; class=&quot;headerlink&quot; title=&quot;一. SparkSQL 的前世今生&quot;&gt;&lt;/a&gt;一. SparkSQL 的前世今生&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;Hive =
      
    
    </summary>
    
      <category term="Spark" scheme="https://airpoet.github.io/categories/Spark/"/>
    
      <category term="精讲" scheme="https://airpoet.github.io/categories/Spark/%E7%B2%BE%E8%AE%B2/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Spark" scheme="https://airpoet.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark重点解析(二) =&gt; Spark调优</title>
    <link href="https://airpoet.github.io/2018/07/27/Spark/Spark%E9%87%8D%E7%82%B9%E8%A7%A3%E6%9E%90(%E4%BA%8C)%20=%3E%20Spark%E8%B0%83%E4%BC%98/"/>
    <id>https://airpoet.github.io/2018/07/27/Spark/Spark重点解析(二) =&gt; Spark调优/</id>
    <published>2018-07-26T17:47:32.984Z</published>
    <updated>2018-08-02T00:56:23.786Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Spark整套调优方案主要分为开发调优、资源调优、数据倾斜调优、shuffle调优几个部分。</p><p><strong>开发调优和资源调优</strong>是所有Spark作业都需要注意和遵循的一些基本原则，<strong>是高性能Spark作业的基础</strong>；</p><p><strong>数据倾斜调优</strong>，主要讲解了一套完整的用来<strong>解决Spark作业数据倾斜的解决方案</strong>；</p><p><strong>shuffle调优</strong>，主要讲解了如何<strong>对Spark作业的shuffle运行过程以及细节进行调优</strong>。</p><p><strong>本文主要参考美团点评技术博客之</strong></p><ul><li><a href="https://tech.meituan.com/spark_tuning_basic.html" target="_blank" rel="noopener">Spark性能优化指南——基础篇</a></li><li><a href="https://tech.meituan.com/spark_tuning_pro.html" target="_blank" rel="noopener">Spark性能优化指南——高级篇</a></li></ul><hr><h1 id="一-开发调优"><a href="#一-开发调优" class="headerlink" title="一. 开发调优"></a>一. 开发调优</h1><h2 id="调优概述"><a href="#调优概述" class="headerlink" title="调优概述"></a>调优概述</h2><p>Spark性能优化的第一步，就是要在开发Spark作业的过程中注意和应用一些性能优化的基本原则。开发调优，就是要让大家了解以下一些Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化等。在开发过程中，时时刻刻都应该注意以上原则，并将这些原则根据具体的业务以及实际的应用场景，灵活地运用到自己的Spark作业中。</p><h2 id="原则一：避免创建重复的RDD"><a href="#原则一：避免创建重复的RDD" class="headerlink" title="原则一：避免创建重复的RDD"></a>原则一：避免创建重复的RDD</h2><p>通常来说，我们在开发一个Spark作业时，首先是基于某个数据源（比如Hive表或HDFS文件）创建一个初始的RDD；接着对这个RDD执行某个算子操作，然后得到下一个RDD；以此类推，循环往复，直到计算出最终我们需要的结果。在这个过程中，多个RDD会通过不同的算子操作（比如map、reduce等）串起来，这个“RDD串”，就是RDD lineage，也就是“RDD的血缘关系链”。</p><p>我们在开发过程中要注意：对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。</p><p>一些Spark初学者在刚开始开发Spark作业时，或者是有经验的工程师在开发RDD lineage极其冗长的Spark作业时，可能会忘了自己之前对于某一份数据已经创建过一个RDD了，从而导致对于同一份数据，创建了多个RDD。这就意味着，我们的Spark作业会进行多次重复计算来创建多个代表相同数据的RDD，进而增加了作业的性能开销。</p><h3 id="一个简单的例子"><a href="#一个简单的例子" class="headerlink" title="一个简单的例子"></a>一个简单的例子</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。也就是说，需要对一份数据执行两次算子操作。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。</span></span><br><span class="line"><span class="comment">// 这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，然后分别对每个RDD都执行了一个算子操作。</span></span><br><span class="line"><span class="comment">// 这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd2.reduce(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。</span></span><br><span class="line"><span class="comment">// 这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，然后对这一个RDD执行了多次算子操作。</span></span><br><span class="line"><span class="comment">// 但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。</span></span><br><span class="line"><span class="comment">// 要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，才能保证一个RDD被多次使用时只被计算一次。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure><h2 id="原则二：尽可能复用同一个RDD"><a href="#原则二：尽可能复用同一个RDD" class="headerlink" title="原则二：尽可能复用同一个RDD"></a>原则二：尽可能复用同一个RDD</h2><p>除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数。</p><h3 id="一个简单的例子-1"><a href="#一个简单的例子-1" class="headerlink" title="一个简单的例子"></a>一个简单的例子</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 错误的做法。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 有一个&lt;Long, String&gt;格式的RDD，即rdd1。</span></span><br><span class="line"><span class="comment">// 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; rdd1 = ...</span><br><span class="line"><span class="type">JavaRDD</span>&lt;<span class="type">String</span>&gt; rdd2 = rdd1.map(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 分别对rdd1和rdd2执行了不同的算子操作。</span></span><br><span class="line">rdd1.reduceByKey(...)</span><br><span class="line">rdd2.map(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的做法。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。</span></span><br><span class="line"><span class="comment">// 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 其实在这种情况下完全可以复用同一个RDD。</span></span><br><span class="line"><span class="comment">// 我们可以使用rdd1，既做reduceByKey操作，也做map操作。</span></span><br><span class="line"><span class="comment">// 在进行第二个map操作时，只使用每个数据的tuple._2，也就是rdd1中的value值，即可。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; rdd1 = ...</span><br><span class="line">rdd1.reduceByKey(...)</span><br><span class="line">rdd1.map(tuple._2...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。</span></span><br><span class="line"><span class="comment">// 但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，rdd1实际上还是会被计算两次。</span></span><br><span class="line"><span class="comment">// 因此还需要配合“原则三：对多次使用的RDD进行持久化”进行使用，才能保证一个RDD被多次使用时只被计算一次。</span></span><br></pre></td></tr></table></figure><h2 id="原则三：对多次使用的RDD进行持久化"><a href="#原则三：对多次使用的RDD进行持久化" class="headerlink" title="原则三：对多次使用的RDD进行持久化"></a>原则三：对多次使用的RDD进行持久化</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-29-140950.png" alt="image-20180729220949680"></p><p>当你在Spark代码中多次对一个RDD做了算子操作后，恭喜，你已经实现Spark作业第一步的优化了，也就是尽可能复用RDD。此时就该在这个基础之上，进行第二步优化了，也就是要保证对一个RDD执行多次算子操作时，这个RDD本身仅仅被计算一次。</p><p>Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。</p><p>因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。</p><h3 id="对多次使用的RDD进行持久化的代码示例"><a href="#对多次使用的RDD进行持久化的代码示例" class="headerlink" title="对多次使用的RDD进行持久化的代码示例"></a>对多次使用的RDD进行持久化的代码示例</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的做法。</span></span><br><span class="line"><span class="comment">// cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。</span></span><br><span class="line"><span class="comment">// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。</span></span><br><span class="line"><span class="comment">// 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>).cache()</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。</span></span><br><span class="line"><span class="comment">// 比如说，StorageLevel.MEMORY_AND_DISK_SER表示，内存充足时优先持久化到内存中，内存不充足时持久化到磁盘文件中。</span></span><br><span class="line"><span class="comment">// 而且其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个partition都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。</span></span><br><span class="line"><span class="comment">// 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，从而发生频繁GC。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>).persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure><p>对于persist()方法而言，我们可以根据不同的业务场景选择不同的持久化级别。</p><h3 id="Spark的持久化级别"><a href="#Spark的持久化级别" class="headerlink" title="Spark的持久化级别"></a>Spark的持久化级别</h3><table><thead><tr><th>持久化级别</th><th>含义解释</th></tr></thead><tbody><tr><td>MEMORY_ONLY</td><td>使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。</td></tr><tr><td>MEMORY_AND_DISK</td><td>使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。</td></tr><tr><td>MEMORY_ONLY_SER</td><td>基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td></tr><tr><td>MEMORY_AND_DISK_SER</td><td>基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td></tr><tr><td>DISK_ONLY</td><td>使用未序列化的Java对象格式，将数据全部写入磁盘文件中。</td></tr><tr><td>MEMORY_ONLY_2, MEMORY_AND_DISK_2, 等等.</td><td>对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。</td></tr></tbody></table><h3 id="如何选择一种最合适的持久化策略"><a href="#如何选择一种最合适的持久化策略" class="headerlink" title="如何选择一种最合适的持久化策略"></a>如何选择一种最合适的持久化策略</h3><ul><li>默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。</li><li>如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。</li><li>如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。</li><li>通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。</li></ul><h2 id="原则四：尽量避免使用shuffle类算子"><a href="#原则四：尽量避免使用shuffle类算子" class="headerlink" title="原则四：尽量避免使用shuffle类算子"></a>原则四：尽量避免使用shuffle类算子</h2><p>如果有可能的话，要尽量避免使用shuffle类算子。因为<strong>Spark作业运行过程中，最消耗性能的地方就是shuffle过程</strong>。shuffle过程，简单来说，就是<strong>将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作</strong>。比如reduceByKey、join等算子，都会触发shuffle操作。</p><p>shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。<strong>磁盘IO和网络数据传输也是shuffle性能较差的主要原因。</strong></p><p>因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。</p><h3 id="Broadcast与map进行join代码示例-※"><a href="#Broadcast与map进行join代码示例-※" class="headerlink" title="Broadcast与map进行join代码示例 ※"></a>Broadcast与map进行join代码示例 ※</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 传统的join操作会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.join(rdd2)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Broadcast+map的join操作，不会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 使用Broadcast将一个数据量较小的RDD作为广播变量。</span></span><br><span class="line"><span class="keyword">val</span> rdd2Data = rdd2.collect()</span><br><span class="line"><span class="keyword">val</span> rdd2DataBroadcast = sc.broadcast(rdd2Data)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。</span></span><br><span class="line"><span class="comment">// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。</span></span><br><span class="line"><span class="comment">// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.map(rdd2DataBroadcast...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。</span></span><br><span class="line"><span class="comment">// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。</span></span><br></pre></td></tr></table></figure><h2 id="原则五：使用map-side预聚合的shuffle操作"><a href="#原则五：使用map-side预聚合的shuffle操作" class="headerlink" title="原则五：使用map-side预聚合的shuffle操作"></a>原则五：使用map-side预聚合的shuffle操作</h2><p>如果因为业务需要，一定要使用<strong>shuffle操作</strong>，无法用map类的算子来替代，那么<strong>尽量使用可以map-side预聚合的算子</strong>。</p><p>所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，<strong>在可能的情况下，建议使用<code>reduceByKey</code>或者<code>aggregateByKey</code>算子来替代掉<code>groupByKey</code>算子</strong>。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而<strong><code>groupByKey</code>算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差</strong>。</p><p>比如如下两幅图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。<img src="https://tech.meituan.com/img/spark-tuning/group-by-key-wordcount.png" alt="groupByKey实现wordcount原理"></p><p><img src="https://tech.meituan.com/img/spark-tuning/reduce-by-key-wordcount.png" alt="reduceByKey实现wordcount原理"></p><h2 id="原则六：使用高性能的算子"><a href="#原则六：使用高性能的算子" class="headerlink" title="原则六：使用高性能的算子"></a>原则六：使用高性能的算子</h2><p>除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。</p><h3 id="1-使用reduceByKey-aggregateByKey替代groupByKey"><a href="#1-使用reduceByKey-aggregateByKey替代groupByKey" class="headerlink" title="1. 使用reduceByKey/aggregateByKey替代groupByKey"></a>1. 使用reduceByKey/aggregateByKey替代groupByKey</h3><p>详情见“原则五：使用map-side预聚合的shuffle操作”。</p><h3 id="2-使用mapPartitions替代普通map"><a href="#2-使用mapPartitions替代普通map" class="headerlink" title="2. 使用mapPartitions替代普通map"></a>2. 使用mapPartitions替代普通map</h3><p>mapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！</p><h3 id="3-使用foreachPartitions替代foreach"><a href="#3-使用foreachPartitions替代foreach" class="headerlink" title="3. 使用foreachPartitions替代foreach"></a>3. 使用foreachPartitions替代foreach</h3><p>原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。</p><h3 id="4-使用filter之后进行coalesce操作"><a href="#4-使用filter之后进行coalesce操作" class="headerlink" title="4. 使用filter之后进行coalesce操作"></a>4. 使用filter之后进行coalesce操作</h3><p>通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。</p><h3 id="5-使用repartitionAndSortWithinPartitions替代repartition与sort类操作"><a href="#5-使用repartitionAndSortWithinPartitions替代repartition与sort类操作" class="headerlink" title="5. 使用repartitionAndSortWithinPartitions替代repartition与sort类操作"></a>5. 使用repartitionAndSortWithinPartitions替代repartition与sort类操作</h3><p>repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。</p><h2 id="原则七：广播大变量"><a href="#原则七：广播大变量" class="headerlink" title="原则七：广播大变量"></a>原则七：广播大变量</h2><p>有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。</p><p>在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。</p><p>因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。</p><h3 id="广播大变量的代码示例"><a href="#广播大变量的代码示例" class="headerlink" title="广播大变量的代码示例"></a>广播大变量的代码示例</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 以下代码在算子函数中，使用了外部的变量。</span></span><br><span class="line"><span class="comment">// 此时没有做任何特殊操作，每个task都会有一份list1的副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line">rdd1.map(list1...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 以下代码将list1封装成了Broadcast类型的广播变量。</span></span><br><span class="line"><span class="comment">// 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。</span></span><br><span class="line"><span class="comment">// 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。</span></span><br><span class="line"><span class="comment">// 每个Executor内存中，就只会驻留一份广播变量副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line"><span class="keyword">val</span> list1Broadcast = sc.broadcast(list1)</span><br><span class="line">rdd1.map(list1Broadcast...)</span><br></pre></td></tr></table></figure><h2 id="原则八：使用Kryo优化序列化性能"><a href="#原则八：使用Kryo优化序列化性能" class="headerlink" title="原则八：使用Kryo优化序列化性能"></a>原则八：使用Kryo优化序列化性能</h2><p><strong>在Spark中，主要有三个地方涉及到了序列化：</strong></p><ul><li>在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。</li><li>将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。</li><li>使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。</li></ul><p>对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。</p><p><strong>以下是使用<code>Kryo</code>的代码示例</strong>，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等, 避免会使用全限定名, 这样就会有额外的开销）：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建SparkConf对象。</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(...).setAppName(...)</span><br><span class="line"><span class="comment">// 设置序列化器为KryoSerializer。</span></span><br><span class="line">conf.set(<span class="string">"spark.serializer"</span>, <span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</span><br><span class="line"><span class="comment">// 注册要序列化的自定义类型。</span></span><br><span class="line">conf.registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">MyClass1</span>], classOf[<span class="type">MyClass2</span>]))</span><br></pre></td></tr></table></figure><h2 id="原则九：优化数据结构"><a href="#原则九：优化数据结构" class="headerlink" title="原则九：优化数据结构"></a>原则九：优化数据结构</h2><p>Java中，有三种类型比较耗费内存：</p><ul><li><strong>对象</strong>，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。</li><li><strong>字符串</strong>，每个字符串内部都有一个字符数组以及长度等额外信息。</li><li><strong>集合类型</strong>，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。</li></ul><p>因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用<strong>字符串替代对象</strong>，使用<strong>原始类型（比如Int、Long）替代字符串</strong>，使用<strong>数组替代集合</strong>类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。</p><p>但是在笔者的编码实践中发现，要做到该原则其实并不容易。因为我们同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于我们的编码难度以及代码可维护性，也是一个极大的挑战。因此笔者建议，<strong>在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性。</strong></p><p>主要就是优化你的算子函数，内部使用到的局部数据，或者是算子函数外部的数据。都可以进行数据结构的优化。优化之后，都会减少其对内存的消耗和占用。 </p><p><strong>通常企业级应用中的做法是: </strong></p><ul><li>对于<strong>HashMap、List</strong>这种数据，统一<strong>用String拼接成特殊格式的字符串</strong>，比如Map&lt;Integer, Person&gt; persons = new HashMap&lt;Integer, Person&gt;()。可以优化为，特殊的字符串格式：id:name,address|id:name,address…。 </li><li><strong>避免使用多层嵌套的对象结构</strong>。比如说，public class Teacher { private List<student> students = new ArrayList\&lt;Student>() }。就是非常不好的例子。因为Teacher类的内部又嵌套了大量的小Student对象。对于上述例子，也完全可以使用特殊的字符串来进行数据的存储。比如，<strong>用json字符串</strong>来存储数据，就是一个很好的选择。 <code>{&quot;teacherId&quot;: 1, &quot;teacherName&quot;: &quot;leo&quot;, students:[{&quot;studentId&quot;: 1, &quot;studentName&quot;: &quot;tom&quot;},{&quot;studentId&quot;:2, &quot;studentName&quot;:&quot;marry&quot;}]}</code></student></li><li><strong>尽量使用int替代String</strong> ,在spark应用中，<strong>id就不要用常用的uuid了</strong>，因为无法转成int，就<strong>用自增的int类型的id即可</strong>。（sdfsdfdf-234242342-sdfsfsfdfd） </li></ul><h2 id="原则十-GroupByKey-和-ReduceByKey的使用"><a href="#原则十-GroupByKey-和-ReduceByKey的使用" class="headerlink" title="原则十: GroupByKey 和 ReduceByKey的使用"></a>原则十: GroupByKey 和 ReduceByKey的使用</h2><blockquote><p>GroupByKey</p></blockquote><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-30-groupByKey%E5%8E%9F%E7%90%86.png" alt=""></p><blockquote><p>ReduceByKey</p></blockquote><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-30-reduceByKey%E5%8E%9F%E7%90%86.png" alt=""></p><hr><h1 id="二-资源调优"><a href="#二-资源调优" class="headerlink" title="二. 资源调优"></a>二. 资源调优</h1><h2 id="调优概述-1"><a href="#调优概述-1" class="headerlink" title="调优概述"></a>调优概述</h2><p>在开发完Spark作业之后，就该为作业配置合适的资源了。<strong>Spark的资源参数，基本都可以在spark-submit命令中作为参数设置。</strong>很多Spark初学者，通常不知道该设置哪些必要的参数，以及如何设置这些参数，最后就只能胡乱设置，甚至压根儿不设置。资源参数设置的不合理，可能会导致没有充分利用集群资源，作业运行会极其缓慢；或者设置的资源过大，队列没有足够的资源来提供，进而导致各种异常。总之，无论是哪种情况，都会导致Spark作业的运行效率低下，甚至根本无法运行。因此我们<strong>必须对Spark作业的资源使用原理有一个清晰的认识</strong>，<strong>并知道在Spark作业运行过程中，有哪些资源参数是可以设置的，以及如何设置合适的参数值。</strong></p><h2 id="Spark作业基本运行原理"><a href="#Spark作业基本运行原理" class="headerlink" title="Spark作业基本运行原理"></a>Spark作业基本运行原理</h2><p><img src="https://tech.meituan.com/img/spark-tuning/spark-base-mech.png" alt="Spark基本运行原理"></p><p>详细原理见上图。我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而<strong>Driver进程要做的第一件事情，就是向集群管理器</strong>（可以是Spark Standalone集群，也可以是其他的资源管理集群，美团•大众点评使用的是YARN作为资源管理集群）<strong>申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。</strong></p><p>在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。<strong>一个stage的所有task都执行完毕之后，<u>会在各个节点本地的磁盘文件中写入计算中间结果</u>，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。</strong>如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。</p><p><strong>Spark是根据shuffle类算子来进行stage的划分</strong>。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。</p><p>当我们在代码中执行了<strong>cache/persist等持久化操作</strong>时，根据我们选择的持久化级别的不同，<strong>每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中</strong>。</p><p>因此<strong>Executor的内存主要分为三块</strong>：第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD持久化时使用，默认占Executor总内存的60%。(这里<strong>指的是静态内存分配</strong>, 详情见下文) </p><p>task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。<strong>如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。</strong></p><p>以上就是Spark作业的基本运行原理的说明，大家可以结合上图来理解。理解作业基本原理，是我们进行资源参数调优的基本前提。</p><h2 id="数据本地化"><a href="#数据本地化" class="headerlink" title="数据本地化"></a>数据本地化</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-29-171030.jpg" alt=""></p><p>移动代码到其他节点，会比移动数据到代码所在的节点上去，速度要快得多，因为代码比较小。Spark也正是基于这个数据本地化的原则来构建task调度算法的。 </p><p>数据本地化，指的是，数据离计算它的代码有多近。<strong>基于数据距离代码的距离，有几种数据本地化级别：</strong> </p><ol><li><code>PROCESS_LOCAL</code>：数据和计算它的代码在同一个JVM进程中。</li><li><code>NODE_LOCAL</code>：数据和计算它的代码在一个节点上，但是不在一个进程中，比如在不同的executor进程中，或者是数据在HDFS文件的block中。</li><li><code>NO_PREF</code>：数据从哪里过来，性能都是一样的。</li><li><code>RACK_LOCAL</code>：数据和计算它的代码在一个机架上。</li><li><code>ANY</code>：数据可能在任意地方，比如其他网络环境内，或者其他机架上。</li></ol><p>Spark倾向于使用最好的本地化级别来调度task，但是这是不可能的。如果没有任何未处理的数据在空闲的executor上，那么Spark就会放低本地化级别。这时有两个选择：第一，等待，直到executor上的cpu释放出来，那么就分配task过去；第二，立即在任意一个executor上启动一个task。 </p><p>Spark默认会等待一会儿，来期望task要处理的数据所在的节点上的executor空闲出一个cpu，从而将task分配过去。只要超过了时间，那么Spark就会将task分配到其他任意一个空闲的executor上。 </p><p>可以设置参数，<code>spark.locality</code>系列参数，来调节Spark等待task可以进行数据本地化的时间。<code>spark.locality.wait</code>（3000毫秒）、spark.locality.wait.node、spark.locality.wait.process、spark.locality.wait.rack。 </p><h2 id="内存空间分配"><a href="#内存空间分配" class="headerlink" title="内存空间分配"></a>内存空间分配</h2><h3 id="1-静态内存管理"><a href="#1-静态内存管理" class="headerlink" title="1.静态内存管理"></a>1.静态内存管理</h3><p>在 Spark 最初采用的静态内存管理机制下，存储内存、执行内存和其他内存的大小在 Spark 应用程序运行期间均为固定的，但用户可以应用程序启动前进行配置</p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-28-083345.png" alt="image-20180728163345199"></p><p><strong>可用的存储内存</strong><code>StorageMemory =systemMaxMemory * spark.storage.memoryFraction * spark.storage.safetyFraction = 43%</code></p><p><strong>可用的执行内存</strong> = <code>systemMaxMemory * spark.shuffle.memoryFraction * spark.shuffle.safetyFraction = 16%</code></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-28-083901.png" alt="image-20180728163901027"></p><p>由于新的内存管理机制的出现，这种方式 使用，出于兼容旧版本的应用程序的目的，Spark 仍然保留了它的实现。</p><h3 id="2-统一内存管理"><a href="#2-统一内存管理" class="headerlink" title="2.统一内存管理"></a>2.统一内存管理</h3><p><strong>Spark-1.6 之后引入的统一内存管理机制，与静态内存管理的区别在于存储内存和执行内存 共享同一块空间，可以动态占用对方的空闲区域</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-28-084013.png" alt="image-20180728164012590"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-28-084035.png" alt="image-20180728164035249"></p><p><strong>其中最重要的优化在于动态占用机制</strong></p><ol><li>设定基本的存储内存和执行内存区域（spark.memory.storageFraction参数), 该设定确定了双方各自拥有的空间的范围</li><li>双方的空间都不足时，则存储到硬盘；若己方空间不足而对方空余时，可借用对方的空 间;（存储空间不足是指不足以放下一个完整的 Block）</li><li><strong><code>执行内存</code>的空间被<code>存储内存</code>占用后，可让<code>存储内存</code>将占用的部分转存到硬盘，然后<code>归还</code>借用的空间</strong></li><li><strong><code>存储内存</code>的空间被<code>执行内存</code>占用后，<code>无法让执行内存</code>“归还”，因为需要考虑 Shuffle 过程中的很多 因素，实现起来较为复杂</strong></li></ol><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-28-090452.png" alt="image-20180728170452408"></p><h3 id="垃圾回收-JVM-优化"><a href="#垃圾回收-JVM-优化" class="headerlink" title="垃圾回收, JVM 优化"></a>垃圾回收, JVM 优化</h3><blockquote><p>GC 对性能的影响</p></blockquote><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-29-GC%E5%AF%B9Spark%E6%80%A7%E8%83%BD%E5%BD%B1%E5%93%8D%E7%9A%84%E5%8E%9F%E7%90%86.png" alt=""></p><p>对于垃圾回收的性能问题，首先要做的就是，使用更高效的数据结构，比如array和string；其次就是在持久化rdd时，使用序列化的持久化级别，而且用Kryo序列化类库，这样，每个partition就只是一个对象——一个字节数组。 </p><p>我们可以<strong>对垃圾回收进行监测</strong>，包括多久进行一次垃圾回收，以及每次垃圾回收耗费的时间。只要在spark-submit脚本中，增加一个配置即可，–conf “spark.executor.extraJavaOptions=-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps”。 这里虽然会打印出Java虚拟机的垃圾回收的相关信息，但是是输出到了worker上的日志中，而不是driver的日志中。 </p><p>也完全可以通过<strong>SparkUI</strong>（4040端口）来观察每个stage的垃圾回收的情况。 </p><blockquote><p><strong>调节executor内存比例</strong></p></blockquote><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-29-%E8%B0%83%E8%8A%82executor%E5%86%85%E5%AD%98%E6%AF%94%E4%BE%8B.png" alt=""></p><p>默认情况下，Spark使用每个executor 60%的内存空间来缓存RDD，那么在task执行期间创建的对象，只有40%的内存空间来存放。 </p><p>在上述情况下，如果发现垃圾回收频繁发生。那么就需要对那个比例进行调优，使用new SparkConf().set(“spark.storage.memoryFraction”, “0.5”)即可，可以将RDD缓存占用空间的比例降低，从而给更多的空间让task创建的对象进行使用。 </p><p>因此，<strong>对于RDD持久化，完全可以使用Kryo序列化，加上降低其executor内存占比的方式，来减少其内存消耗</strong>。给task提供更多的内存，从而避免task的执行频繁触发垃圾回收。 </p><h3 id="高级垃圾回收调优"><a href="#高级垃圾回收调优" class="headerlink" title="高级垃圾回收调优"></a>高级垃圾回收调优</h3><blockquote><p>JVM minor gc与full gc原理</p></blockquote><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-29-JVM%20minor%20gc%E4%B8%8Efull%20gc%E5%8E%9F%E7%90%86.png" alt=""></p><p>Java堆空间被划分成了两块空间，一个是年轻代，一个是老年代。年轻代放的是短时间存活的对象，老年代放的是长时间存活的对象。年轻代又被划分了三块空间，Eden、Survivor1、Survivor2。</p><p>首先，Eden区域和Survivor1区域用于存放对象，Survivor2区域备用。创建的对象，首先放入Eden区域和Survivor1区域，如果Eden区域满了，那么就会触发一次Minor GC，进行年轻代的垃圾回收。Eden和Survivor1区域中存活的对象，会被移动到Survivor2区域中。然后Survivor1和Survivor2的角色调换，Survivor1变成了备用</p><p>如果一个对象，在年轻代中，撑过了多次垃圾回收，都没有被回收掉，那么会被认为是长时间存活的，此时就会被移入老年代。此外，如果在将Eden和Survivor1中的存活对象，尝试放入Survivor2中时，发现Survivor2放满了，那么会直接放入老年代。此时就出现了，短时间存活的对象，进入老年代的问题。</p><p>如果老年代的空间满了，那么就会触发Full GC，进行老年代的垃圾回收操作。</p><p>Spark中，垃圾回收调优的目标就是，只有真正长时间存活的对象，才能进入老年代，短时间存活的对象，只能呆在年轻代。不能因为某个Survivor区域空间不够，在Minor GC时，就进入了老年代。从而造成短时间存活的对象，长期呆在老年代中占据了空间，而且Full GC时要回收大量的短时间存活的对象，导致Full GC速度缓慢。</p><p><strong>如果发现，在task执行期间，大量full gc发生了，那么说明，年轻代的Eden区域，给的空间不够大。此时可以执行一些操作来优化垃圾回收行为：</strong></p><p>1、包括降低spark.storage.memoryFraction的比例，给年轻代更多的空间，来存放短时间存活的对象；</p><p>2、给Eden区域分配更大的空间，使用-Xmn即可，通常建议给Eden区域，预计大小的4/3；</p><p>3、如果使用的是HDFS文件，那么很好估计Eden区域大小，如果每个executor有4个task，然后每个hdfs压缩块解压缩后大小是3倍，此外每个hdfs块的大小是64M，那么Eden区域的预计大小就是：4 <em> 3 </em> 64MB，然后呢，再通过-Xmn参数，将Eden区域大小设置为4 <em> 3 </em> 64 * 4/3。</p><p>其实啊，根据经验来看，<strong>对于垃圾回收的调优，尽量就是说，调节executor内存的比例就可以了。</strong>因为jvm的调优是非常复杂和敏感的。除非是，真的到了万不得已的地方，然后呢，自己本身又对jvm相关的技术很了解，那么此时进行eden区域的调节，调优，是可以的。</p><p><strong>一些高级的参数</strong></p><p>-XX:SurvivorRatio=4：如果值为4，那么就是两个Survivor跟Eden的比例是2:4，也就是说每个Survivor占据的年轻代的比例是1/6，所以，你其实也可以尝试调大Survivor区域的大小。</p><p>-XX:NewRatio=4：调节新生代和老年代的比例</p><h2 id="资源参数调优"><a href="#资源参数调优" class="headerlink" title="资源参数调优"></a>资源参数调优</h2><p>了解完了Spark作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓的Spark资源参数调优，其实主要就是对Spark运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升Spark作业的执行性能。以下参数就是Spark中主要的资源参数，每个参数都对应着作业运行原理中的某个部分，我们同时也给出了一个调优的参考值。</p><h3 id="num-executors"><a href="#num-executors" class="headerlink" title="num-executors"></a>num-executors</h3><ul><li>参数说明：<u>该参数用于设置Spark作业总共要用多少个Executor进程来执行。</u>Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。</li><li><strong>参数调优建议</strong>：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。</li></ul><h3 id="executor-memory"><a href="#executor-memory" class="headerlink" title="executor-memory"></a>executor-memory</h3><ul><li>参数说明：<u>该参数用于设置每个Executor进程的内存。</u>Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。</li><li><strong>参数调优建议</strong>：每个Executor进程的内存设置4G<sub>8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/3</sub>1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。</li></ul><h3 id="executor-cores"><a href="#executor-cores" class="headerlink" title="executor-cores"></a>executor-cores</h3><ul><li>参数说明：<u>该参数用于设置每个Executor进程的CPU core数量。</u>这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。</li><li><strong>参数调优建议</strong>：Executor的CPU core数量设置为2<sub>4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，<strong>如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3</strong></sub>1/2左右比较合适，也是避免影响其他同学的作业运行。</li></ul><h3 id="driver-memory"><a href="#driver-memory" class="headerlink" title="driver-memory"></a>driver-memory</h3><ul><li>参数说明：<u>该参数用于设置Driver进程的内存。</u></li><li><strong>参数调优建议</strong>：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。</li></ul><h3 id="spark-default-parallelism-※-task-并行度"><a href="#spark-default-parallelism-※-task-并行度" class="headerlink" title="spark.default.parallelism ※  task 并行度"></a>spark.default.parallelism ※  task 并行度</h3><ul><li><strong>参数说明</strong>：<strong><u>该参数用于设置<code>每个stage的默认task数量</code>。</u>**</strong>这个参数极为重要<strong>，如果不设置可能会直接影响你的Spark作业性能。 </strong>提高并行度.**</li><li><strong>参数调优建议</strong>：Spark作业的默认task数量为500<sub>1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），<em>如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃</em>。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此<strong>Spark官网建议的设置原则是，设置该参数为<code>num-executors * executor-cores</code>的<code>2&lt;/sub&gt;3</code>倍较为合适</strong>，比如<strong>所有Executor的总CPU core</strong>数量为300个，那么设置1000个task是可以的，此时<u>可以充分地利用Spark集群的资源</u>。</sub></li></ul><h3 id="spark-storage-memoryFraction"><a href="#spark-storage-memoryFraction" class="headerlink" title="spark.storage.memoryFraction"></a>spark.<u>storage</u>.memoryFraction</h3><ul><li>参数说明：<u>该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6</u>。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。</li><li><strong>参数调优建议</strong>：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li></ul><h3 id="spark-shuffle-memoryFraction"><a href="#spark-shuffle-memoryFraction" class="headerlink" title="spark.shuffle.memoryFraction"></a>spark.<u>shuffle</u>.memoryFraction</h3><ul><li>参数说明：<u>该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。</u>也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。</li><li><strong>参数调优建议</strong>：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li></ul><p><strong>资源参数的调优，没有一个固定的值，需要同学们根据自己的实际情况（包括Spark作业中的shuffle操作数量、RDD持久化操作数量以及spark web ui中显示的作业gc情况），同时参考本篇文章中给出的原理以及调优建议，合理地设置上述参数。</strong></p><h2 id="资源参数参考示例"><a href="#资源参数参考示例" class="headerlink" title="资源参数参考示例"></a>资源参数参考示例</h2><p>以下是一份spark-submit命令的示例，大家可以参考一下，并根据自己的实际情况进行调节：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --master yarn-cluster \</span><br><span class="line">  --num-executors 100 \</span><br><span class="line">  --executor-memory 6G \</span><br><span class="line">  --executor-cores 4 \</span><br><span class="line">  --driver-memory 1G \</span><br><span class="line">  --conf spark.default.parallelism=1000 \</span><br><span class="line">  --conf spark.storage.memoryFraction=0.5 \</span><br><span class="line">  --conf spark.shuffle.memoryFraction=0.3 \</span><br></pre></td></tr></table></figure><hr><h1 id="三-数据倾斜调优"><a href="#三-数据倾斜调优" class="headerlink" title="三. 数据倾斜调优"></a>三. 数据倾斜调优</h1><h2 id="调优概述-2"><a href="#调优概述-2" class="headerlink" title="调优概述"></a>调优概述</h2><p>有的时候，我们可能会遇到大数据计算中一个最棘手的问题——数据倾斜，此时Spark作业的性能会比期望差很多。数据倾斜调优，就是使用各种技术方案解决不同类型的数据倾斜问题，以保证Spark作业的性能。</p><h2 id="数据倾斜发生时的现象"><a href="#数据倾斜发生时的现象" class="headerlink" title="数据倾斜发生时的现象"></a>数据倾斜发生时的现象</h2><ul><li>绝大多数task执行得都非常快，但个别task执行极慢。比如，总共有1000个task，997个task都在1分钟之内执行完了，但是剩余两三个task却要一两个小时。这种情况很常见。</li><li>原本能够正常执行的Spark作业，某天突然报出OOM（内存溢出）异常，观察异常栈，是我们写的业务代码造成的。这种情况比较少见。</li></ul><h2 id="数据倾斜发生的原理"><a href="#数据倾斜发生的原理" class="headerlink" title="数据倾斜发生的原理"></a>数据倾斜发生的原理</h2><p>数据倾斜的原理很简单：在进行shuffle的时候，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理，比如按照key进行聚合或join等操作。此时如果某个key对应的数据量特别大的话，就会发生数据倾斜。比如大部分key对应10条数据，但是个别key却对应了100万条数据，那么大部分task可能就只会分配到10条数据，然后1秒钟就运行完了；但是个别task可能分配到了100万数据，要运行一两个小时。因此，整个Spark作业的运行进度是由运行时间最长的那个task决定的。</p><p>因此出现数据倾斜的时候，Spark作业看起来会运行得非常缓慢，甚至可能因为某个task处理的数据量过大导致内存溢出。</p><p>下图就是一个很清晰的例子：hello这个key，在三个节点上对应了总共7条数据，这些数据都会被拉取到同一个task中进行处理；而world和you这两个key分别才对应1条数据，所以另外两个task只要分别处理1条数据即可。此时第一个task的运行时间可能是另外两个task的7倍，而<strong>整个stage的运行速度也由运行最慢的那个task所决定</strong>。</p><p><img src="https://tech.meituan.com/img/spark-tuning/skwed-mech.png" alt="数据倾斜原理"></p><h2 id="如何定位导致数据倾斜的代码"><a href="#如何定位导致数据倾斜的代码" class="headerlink" title="如何定位导致数据倾斜的代码"></a>如何定位导致数据倾斜的代码</h2><p>数据倾斜只会发生在shuffle过程中。这里给大家罗列一些常用的并且可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。</p><h3 id="某个task执行特别慢的情况"><a href="#某个task执行特别慢的情况" class="headerlink" title="某个task执行特别慢的情况"></a>某个task执行特别慢的情况</h3><p>首先要看的，就是数据倾斜发生在第几个stage中。</p><p>如果是用yarn-client模式提交，那么本地是直接可以看到log的，可以在log中找到当前运行到了第几个stage；如果是用yarn-cluster模式提交，则可以通过Spark Web UI来查看当前运行到了第几个stage。此外，无论是使用yarn-client模式还是yarn-cluster模式，我们都可以在Spark Web UI上深入看一下当前这个stage各个task分配的数据量，从而进一步确定是不是task分配的数据不均匀导致了数据倾斜。</p><p>比如下图中，倒数第三列显示了每个task的运行时间。明显可以看到，有的task运行特别快，只需要几秒钟就可以运行完；而有的task运行特别慢，需要几分钟才能运行完，此时单从运行时间上看就已经能够确定发生数据倾斜了。此外，倒数第一列显示了每个task处理的数据量，明显可以看到，运行时间特别短的task只需要处理几百KB的数据即可，而运行时间特别长的task需要处理几千KB的数据，处理的数据量差了10倍。此时更加能够确定是发生了数据倾斜。</p><p><img src="https://tech.meituan.com/img/spark-tuning/shuffle-skwed-web-ui-demo.png" alt="img"></p><p>知道数据倾斜发生在哪一个stage之后，接着我们就需要根据stage划分原理，推算出来发生倾斜的那个stage对应代码中的哪一部分，这部分代码中肯定会有一个shuffle类算子。精准推算stage与代码的对应关系，需要对Spark的源码有深入的理解，这里我们可以介绍一个相对简单实用的推算方法：只要看到Spark代码中出现了一个shuffle类算子或者是Spark SQL的SQL语句中出现了会导致shuffle的语句（比如group by语句），那么就可以判定，以那个地方为界限划分出了前后两个stage。</p><p>这里我们就以Spark最基础的入门程序——单词计数来举例，如何用最简单的方法大致推算出一个stage对应的代码。如下示例，在整个代码中，只有一个reduceByKey是会发生shuffle的算子，因此就可以认为，以这个算子为界限，会划分出前后两个stage。</p><ul><li><strong>stage0</strong>，主要是执行从textFile到map操作，以及执行shuffle write操作。<strong>shuffle write</strong>操作，我们可以简单理解为对pairs RDD中的数据进行分区操作，每个task处理的数据中，相同的key会写入同一个磁盘文件内。</li><li><strong>stage1</strong>，主要是执行从reduceByKey到collect操作，stage1的各个task一开始运行，就会首先执行<strong>shuffle read</strong>操作。执行shuffle read操作的task，会从stage0的各个task所在节点拉取属于自己处理的那些key，然后对同一个key进行全局性的聚合或join等操作，在这里就是对key的value值进行累加。stage1在执行完reduceByKey算子之后，就计算出了最终的wordCounts RDD，然后会执行collect算子，将所有数据拉取到Driver上，供我们遍历和打印输出。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> lines = sc.textFile(<span class="string">"hdfs://..."</span>)</span><br><span class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"><span class="keyword">val</span> pairs = words.map((_, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> wordCounts = pairs.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">wordCounts.collect().foreach(println(_))</span><br></pre></td></tr></table></figure><p>通过对单词计数程序的分析，希望能够让大家了解最基本的stage划分的原理，以及stage划分后shuffle操作是如何在两个stage的边界处执行的。然后我们就知道如何快速定位出发生数据倾斜的stage对应代码的哪一个部分了。比如我们在Spark Web UI或者本地log中发现，stage1的某几个task执行得特别慢，判定stage1出现了数据倾斜，那么就可以回到代码中定位出stage1主要包括了reduceByKey这个shuffle类算子，此时基本就可以确定是由reduceByKey算子导致的数据倾斜问题。比如某个单词出现了100万次，其他单词才出现10次，那么stage1的某个task就要处理100万数据，整个stage的速度就会被这个task拖慢。</p><h3 id="某个task莫名其妙内存溢出的情况"><a href="#某个task莫名其妙内存溢出的情况" class="headerlink" title="某个task莫名其妙内存溢出的情况"></a>某个task莫名其妙内存溢出的情况</h3><p>这种情况下去定位出问题的代码就比较容易了。我们建议直接看yarn-client模式下本地log的异常栈，或者是通过YARN查看yarn-cluster模式下的log中的异常栈。一般来说，通过异常栈信息就可以定位到你的代码中哪一行发生了内存溢出。然后在那行代码附近找找，一般也会有shuffle类算子，此时很可能就是这个算子导致了数据倾斜。</p><p>但是大家要注意的是，不能单纯靠偶然的内存溢出就判定发生了数据倾斜。因为自己编写的代码的bug，以及偶然出现的数据异常，也可能会导致内存溢出。因此还是要按照上面所讲的方法，通过Spark Web UI查看报错的那个stage的各个task的运行时间以及分配的数据量，才能确定是否是由于数据倾斜才导致了这次内存溢出。</p><h2 id="查看导致数据倾斜的key的数据分布情况"><a href="#查看导致数据倾斜的key的数据分布情况" class="headerlink" title="查看导致数据倾斜的key的数据分布情况"></a>查看导致数据倾斜的key的数据分布情况</h2><p>知道了数据倾斜发生在哪里之后，通常需要分析一下那个执行了shuffle操作并且导致了数据倾斜的RDD/Hive表，查看一下其中key的分布情况。这主要是为之后选择哪一种技术方案提供依据。针对不同的key分布与不同的shuffle算子组合起来的各种情况，可能需要选择不同的技术方案来解决。</p><p>此时根据你执行操作的情况不同，可以有很多种<strong>查看key分布的方式</strong>：</p><ol><li>如果<strong>是Spark SQL中的group by、join语句导致的数据倾斜</strong>，那么就<strong>查询一下SQL中使用的表的key分布情况</strong>。</li><li>如果是对<strong>Spark RDD执行shuffle算子导致的数据倾斜</strong>，那么<strong>可以在Spark作业中加入查看key分布的代码</strong>，比如RDD.countByKey()。然后对统计出来的各个key出现的次数，collect/take到客户端打印一下，就可以看到key的分布情况。</li></ol><p>举例来说，对于上面所说的单词计数程序，如果确定了是stage1的reduceByKey算子导致了数据倾斜，那么就应该看看进行reduceByKey操作的RDD中的key分布情况，在这个例子中指的就是pairs RDD。如下示例，我们可以先<strong>对pairs采样10%的样本数据，然后使用countByKey算子统计出每个key出现的次数，最后在客户端遍历和打印样本数据中各个key的出现次数。</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sampledPairs = pairs.sample(<span class="literal">false</span>, <span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">val</span> sampledWordCounts = sampledPairs.countByKey()</span><br><span class="line">sampledWordCounts.foreach(println(_))</span><br></pre></td></tr></table></figure><h2 id="数据倾斜的解决方案"><a href="#数据倾斜的解决方案" class="headerlink" title="数据倾斜的解决方案"></a>数据倾斜的解决方案</h2><h3 id="解决方案一：使用Hive-ETL预处理数据"><a href="#解决方案一：使用Hive-ETL预处理数据" class="headerlink" title="解决方案一：使用Hive ETL预处理数据"></a>解决方案一：使用Hive ETL预处理数据</h3><p><strong>方案适用场景：</strong>导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀（比如某个key对应了100万数据，其他key才对应了10条数据），而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。</p><p><strong>方案实现思路：</strong>此时可以评估一下，是否可以通过Hive来进行数据预处理（即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行join），然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。</p><p><strong>方案实现原理：</strong>这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。我们只是把数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已。</p><p><strong>方案优点：</strong>实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。</p><p><strong>方案缺点：</strong>治标不治本，Hive ETL中还是会发生数据倾斜。</p><p><strong>方案实践经验：</strong>在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。</p><p><strong>项目实践经验：</strong>在美团·点评的交互式用户行为分析系统中使用了这种方案，该系统主要是允许用户通过Java Web系统提交数据分析统计任务，后端通过Java提交Spark作业进行数据分析统计。要求Spark作业速度必须要快，尽量在10分钟以内，否则速度太慢，用户体验会很差。所以我们将有些Spark作业的shuffle操作提前到了Hive ETL中，从而让Spark直接使用预处理的Hive中间表，尽可能地减少Spark的shuffle操作，大幅度提升了性能，将部分作业的性能提升了6倍以上。</p><h3 id="解决方案二：过滤少数导致倾斜的key"><a href="#解决方案二：过滤少数导致倾斜的key" class="headerlink" title="解决方案二：过滤少数导致倾斜的key"></a>解决方案二：过滤少数导致倾斜的key</h3><p><strong>方案适用场景：</strong>如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。</p><p><strong>方案实现思路：</strong>如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。</p><p><strong>方案实现原理：</strong>将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。</p><p><strong>方案优点：</strong>实现简单，而且效果也很好，可以完全规避掉数据倾斜。</p><p><strong>方案缺点：</strong>适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。</p><p><strong>方案实践经验：</strong>在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。</p><h3 id="解决方案三：提高shuffle操作的并行度"><a href="#解决方案三：提高shuffle操作的并行度" class="headerlink" title="解决方案三：提高shuffle操作的并行度"></a>解决方案三：提高shuffle操作的并行度</h3><p><strong>方案适用场景：</strong>如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。</p><p><strong>方案实现思路：</strong>在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。</p><p><strong>方案实现原理：</strong>增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。具体原理如下图所示。</p><p><strong>方案优点：</strong>实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。</p><p><strong>方案缺点：</strong>只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。</p><p><strong>方案实践经验：</strong>该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用嘴简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。</p><p> <img src="https://tech.meituan.com/img/spark-tuning/shuffle-skwed-add-partition.png" alt="img"></p><h3 id="解决方案四：两阶段聚合（局部聚合-全局聚合）"><a href="#解决方案四：两阶段聚合（局部聚合-全局聚合）" class="headerlink" title="解决方案四：两阶段聚合（局部聚合+全局聚合）"></a>解决方案四：两阶段聚合（局部聚合+全局聚合）</h3><p><strong>方案适用场景：</strong>对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。</p><p><strong>方案实现思路：</strong>这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。</p><p><strong>方案实现原理：</strong>将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。</p><p><strong>方案优点：</strong>对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。</p><p><strong>方案缺点：</strong>仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。</p><p><img src="https://tech.meituan.com/img/spark-tuning/shuffle-skwed-two-phase-aggr.png" alt="img"></p><blockquote><p>Java 版</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 第一步，给RDD中的每个key都打上一个随机前缀。</span></span><br><span class="line">JavaPairRDD&lt;String, Long&gt; randomPrefixRdd = rdd.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> PairFunction&lt;Tuple2&lt;Long,Long&gt;, String, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Long&gt; <span class="title">call</span><span class="params">(Tuple2&lt;Long, Long&gt; tuple)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                Random random = <span class="keyword">new</span> Random();</span><br><span class="line">                <span class="keyword">int</span> prefix = random.nextInt(<span class="number">10</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, Long&gt;(prefix + <span class="string">"_"</span> + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第二步，对打上随机前缀的key进行局部聚合。</span></span><br><span class="line">JavaPairRDD&lt;String, Long&gt; localAggrRdd = randomPrefixRdd.reduceByKey(</span><br><span class="line">        <span class="keyword">new</span> Function2&lt;Long, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Long <span class="title">call</span><span class="params">(Long v1, Long v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第三步，去除RDD中每个key的随机前缀。</span></span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; removedRandomPrefixRdd = localAggrRdd.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> PairFunction&lt;Tuple2&lt;String,Long&gt;, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title">call</span><span class="params">(Tuple2&lt;String, Long&gt; tuple)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">long</span> originalKey = Long.valueOf(tuple._1.split(<span class="string">"_"</span>)[<span class="number">1</span>]);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;Long, Long&gt;(originalKey, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第四步，对去除了随机前缀的RDD进行全局聚合。</span></span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; globalAggrRdd = removedRandomPrefixRdd.reduceByKey(</span><br><span class="line">        <span class="keyword">new</span> Function2&lt;Long, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Long <span class="title">call</span><span class="params">(Long v1, Long v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure><blockquote><p> scala 版</p></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sc.textFile(<span class="string">"hdfs://mycluster/user/ap/a.txt"</span>,<span class="number">4</span>).flatMap(_.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>)).map(t =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> w1 = t._1; </span><br><span class="line">  <span class="keyword">import</span> scala.util.<span class="type">Random</span>;</span><br><span class="line">  <span class="keyword">val</span> n = <span class="type">Random</span>.nextInt(<span class="number">100</span>); </span><br><span class="line">  (w1 + <span class="string">"_"</span> + n, t._2);</span><br><span class="line">&#125;).reduceByKey(_ + _,<span class="number">4</span>).map(t =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> w2 = t._1; </span><br><span class="line">  <span class="keyword">val</span> count = t._2; </span><br><span class="line">  <span class="keyword">val</span> w3 = w2.split(<span class="string">"_"</span>)(<span class="number">0</span>); </span><br><span class="line">  (w3, count);</span><br><span class="line">&#125;).reduceByKey(_ + _,<span class="number">4</span>).saveAsTextFile(<span class="string">"/user/ap/scala/DataSkew"</span>)</span><br></pre></td></tr></table></figure><h3 id="解决方案五：将reduce-join转为map-join"><a href="#解决方案五：将reduce-join转为map-join" class="headerlink" title="解决方案五：将reduce join转为map join"></a>解决方案五：将reduce join转为map join</h3><p><strong>方案适用场景：</strong>在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。</p><p><strong>方案实现思路：</strong>不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。</p><p><strong>方案实现原理：</strong>普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。</p><p><strong>方案优点：</strong>对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。</p><p><strong>方案缺点：</strong>适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。</p><h3 id="解决方案五：将reduce-join转为map-join-1"><a href="#解决方案五：将reduce-join转为map-join-1" class="headerlink" title="解决方案五：将reduce join转为map join"></a>解决方案五：将reduce join转为map join</h3><p><strong>方案适用场景：</strong>在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。</p><p><strong>方案实现思路：</strong>不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。</p><p><strong>方案实现原理：</strong>普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。</p><p><strong>方案优点：</strong>对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。</p><p><strong>方案缺点：</strong>适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。</p><p> <img src="https://tech.meituan.com/img/spark-tuning/shuffle-skwed-map-join.png" alt="img"></p><blockquote><p><strong>通过广播变量, 避免掉 shuffle</strong></p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先将数据量比较小的RDD的数据，collect到Driver中来。</span></span><br><span class="line">List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1.collect()</span><br><span class="line"><span class="comment">// 然后使用Spark的广播功能，将小RDD的数据转换成广播变量，这样每个Executor就只有一份RDD的数据。</span></span><br><span class="line"><span class="comment">// 可以尽可能节省内存空间，并且减少网络传输性能开销。</span></span><br><span class="line"><span class="keyword">final</span> Broadcast&lt;List&lt;Tuple2&lt;Long, Row&gt;&gt;&gt; rdd1DataBroadcast = sc.broadcast(rdd1Data);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 对另外一个RDD执行map类操作，而不再是join类操作。</span></span><br><span class="line">JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRdd = rdd2.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> PairFunction&lt;Tuple2&lt;Long,String&gt;, String, Tuple2&lt;String, Row&gt;&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, String&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="comment">// 在算子函数中，通过广播变量，获取到本地Executor中的rdd1数据。</span></span><br><span class="line">                List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1DataBroadcast.value();</span><br><span class="line">                <span class="comment">// 可以将rdd1的数据转换为一个Map，便于后面进行join操作。</span></span><br><span class="line">                Map&lt;Long, Row&gt; rdd1DataMap = <span class="keyword">new</span> HashMap&lt;Long, Row&gt;();</span><br><span class="line">                <span class="keyword">for</span>(Tuple2&lt;Long, Row&gt; data : rdd1Data) &#123;</span><br><span class="line">                    rdd1DataMap.put(data._1, data._2);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 获取当前RDD数据的key以及value。</span></span><br><span class="line">                String key = tuple._1;</span><br><span class="line">                String value = tuple._2;</span><br><span class="line">                <span class="comment">// 从rdd1数据Map中，根据key获取到可以join到的数据。</span></span><br><span class="line">                Row rdd1Value = rdd1DataMap.get(key);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, String&gt;(key, <span class="keyword">new</span> Tuple2&lt;String, Row&gt;(value, rdd1Value));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这里得提示一下。</span></span><br><span class="line">-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=</span><br><span class="line"><span class="comment">// 上面的做法，仅仅适用于rdd1中的key没有重复，全部是唯一的场景。</span></span><br><span class="line"><span class="comment">// 如果rdd1中有多个相同的key，那么就得用flatMap类的操作，在进行join的时候不能用map，而是得遍历rdd1所有数据进行join。</span></span><br><span class="line"><span class="comment">// rdd2中每条数据都可能会返回多条join后的数据。</span></span><br><span class="line">-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=</span><br></pre></td></tr></table></figure><h3 id="解决方案六：采样倾斜key并分拆join操作"><a href="#解决方案六：采样倾斜key并分拆join操作" class="headerlink" title="解决方案六：采样倾斜key并分拆join操作"></a>解决方案六：采样倾斜key并分拆join操作</h3><p><strong>方案适用场景：</strong>两个RDD/Hive表进行join的时候，如果数据量都比较大，无法采用“解决方案五”，那么此时可以看一下两个RDD/Hive表中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD/Hive表中的少数几个key的数据量过大，而另一个RDD/Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。</p><p><strong>方案实现思路：</strong></p><ul><li>对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。</li><li>然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。</li><li>接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。</li><li>再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。</li><li>而另外两个普通的RDD就照常join即可。</li><li>最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。</li></ul><p><strong>方案实现原理：</strong>对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。具体原理见下图。</p><p><strong>方案优点：</strong>对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。</p><p><strong>方案缺点：</strong>如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。</p><p> <img src="https://tech.meituan.com/img/spark-tuning/shuffle-skwed-sample-expand.png" alt="img"></p><blockquote><p><strong>这Java 代码可真恶心, 有空改成 scala</strong></p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先从包含了少数几个导致数据倾斜key的rdd1中，采样10%的样本数据。</span></span><br><span class="line">JavaPairRDD&lt;Long, String&gt; sampledRDD = rdd1.sample(<span class="keyword">false</span>, <span class="number">0.1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 对样本数据RDD统计出每个key的出现次数，并按出现次数降序排序。</span></span><br><span class="line"><span class="comment">// 对降序排序后的数据，取出top 1或者top 100的数据，也就是key最多的前n个数据。</span></span><br><span class="line"><span class="comment">// 具体取出多少个数据量最多的key，由大家自己决定，我们这里就取1个作为示范。</span></span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; mappedSampledRDD = sampledRDD.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> PairFunction&lt;Tuple2&lt;Long,String&gt;, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;Long, Long&gt;(tuple._1, <span class="number">1L</span>);</span><br><span class="line">            &#125;     </span><br><span class="line">        &#125;);</span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; countedSampledRDD = mappedSampledRDD.reduceByKey(</span><br><span class="line">        <span class="keyword">new</span> Function2&lt;Long, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Long <span class="title">call</span><span class="params">(Long v1, Long v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; reversedSampledRDD = countedSampledRDD.mapToPair( </span><br><span class="line">        <span class="keyword">new</span> PairFunction&lt;Tuple2&lt;Long,Long&gt;, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title">call</span><span class="params">(Tuple2&lt;Long, Long&gt; tuple)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;Long, Long&gt;(tuple._2, tuple._1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 倾斜量最大的 key</span></span><br><span class="line"><span class="keyword">final</span> Long skewedUserid = reversedSampledRDD.sortByKey(<span class="keyword">false</span>).take(<span class="number">1</span>).get(<span class="number">0</span>)._2;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从rdd1中分拆出导致数据倾斜的key，形成独立的RDD。</span></span><br><span class="line">JavaPairRDD&lt;Long, String&gt; skewedRDD = rdd1.filter(</span><br><span class="line">        <span class="keyword">new</span> Function&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Boolean <span class="title">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"><span class="comment">// 从rdd1中分拆出不导致数据倾斜的普通key，形成独立的RDD。</span></span><br><span class="line">JavaPairRDD&lt;Long, String&gt; commonRDD = rdd1.filter(</span><br><span class="line">        <span class="keyword">new</span> Function&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Boolean <span class="title">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> !tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125; </span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// rdd2，就是那个所有key的分布相对较为均匀的rdd。</span></span><br><span class="line"><span class="comment">// 这里将rdd2中，前面获取到的key对应的数据，过滤出来，分拆成单独的rdd，并对rdd中的数据使用flatMap算子都扩容100倍。</span></span><br><span class="line"><span class="comment">// 对扩容的每条数据，都打上0～100的前缀。</span></span><br><span class="line">JavaPairRDD&lt;String, Row&gt; skewedRdd2 = rdd2.filter(</span><br><span class="line">         <span class="keyword">new</span> Function&lt;Tuple2&lt;Long,Row&gt;, Boolean&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Boolean <span class="title">call</span><span class="params">(Tuple2&lt;Long, Row&gt; tuple)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).flatMapToPair(<span class="keyword">new</span> PairFlatMapFunction&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Iterable&lt;Tuple2&lt;String, Row&gt;&gt; call(</span><br><span class="line">                    Tuple2&lt;Long, Row&gt; tuple) <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                Random random = <span class="keyword">new</span> Random();</span><br><span class="line">                List&lt;Tuple2&lt;String, Row&gt;&gt; list = <span class="keyword">new</span> ArrayList&lt;Tuple2&lt;String, Row&gt;&gt;();</span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">                    list.add(<span class="keyword">new</span> Tuple2&lt;String, Row&gt;(i + <span class="string">"_"</span> + tuple._1, tuple._2));</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> list;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将rdd1中分拆出来的导致倾斜的key的独立rdd，每条数据都打上100以内的随机前缀。</span></span><br><span class="line"><span class="comment">// 然后将这个rdd1中分拆出来的独立rdd，与上面rdd2中分拆出来的独立rdd，进行join。</span></span><br><span class="line">JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD1 = skewedRDD.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> PairFunction&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, String&gt; <span class="title">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                Random random = <span class="keyword">new</span> Random();</span><br><span class="line">                <span class="keyword">int</span> prefix = random.nextInt(<span class="number">100</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, String&gt;(prefix + <span class="string">"_"</span> + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        .join(skewedRdd2)</span><br><span class="line">        .mapToPair(<span class="keyword">new</span> PairFunction&lt;Tuple2&lt;String,Tuple2&lt;String,Row&gt;&gt;, Long, Tuple2&lt;String, Row&gt;&gt;() &#123;</span><br><span class="line">                        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="keyword">public</span> Tuple2&lt;Long, Tuple2&lt;String, Row&gt;&gt; call(</span><br><span class="line">                            Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; tuple)</span><br><span class="line">                            <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                            <span class="keyword">long</span> key = Long.valueOf(tuple._1.split(<span class="string">"_"</span>)[<span class="number">1</span>]);</span><br><span class="line">                            <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;Long, Tuple2&lt;String, Row&gt;&gt;(key, tuple._2);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将rdd1中分拆出来的包含普通key的独立rdd，直接与rdd2进行join。</span></span><br><span class="line">JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD2 = commonRDD.join(rdd2);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将倾斜key join后的结果与普通key join后的结果，uinon起来。</span></span><br><span class="line"><span class="comment">// 就是最终的join结果。</span></span><br><span class="line">JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD = joinedRDD1.union(joinedRDD2);</span><br></pre></td></tr></table></figure><h3 id="解决方案七：使用随机前缀和扩容RDD进行join"><a href="#解决方案七：使用随机前缀和扩容RDD进行join" class="headerlink" title="解决方案七：使用随机前缀和扩容RDD进行join"></a>解决方案七：使用随机前缀和扩容RDD进行join</h3><p><strong>方案适用场景：</strong>如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用最后一种方案来解决问题了。</p><p><strong>方案实现思路：</strong></p><ul><li>该方案的实现思路基本和“解决方案六”类似，首先查看RDD/Hive表中的数据分布情况，找到那个造成数据倾斜的RDD/Hive表，比如有多个key都对应了超过1万条数据。</li><li>然后将该RDD的每条数据都打上一个n以内的随机前缀。</li><li>同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。</li><li>最后将两个处理后的RDD进行join即可。</li></ul><p><strong>方案实现原理：</strong>将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到多个task中去处理，而不是让一个task处理大量的相同key。该方案与“解决方案六”的不同之处就在于，上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。</p><p><strong>方案优点：</strong>对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。</p><p><strong>方案缺点：</strong>该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。</p><p><strong>方案实践经验：</strong>曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先将其中一个key分布相对较为均匀的RDD膨胀100倍。</span></span><br><span class="line">JavaPairRDD&lt;String, Row&gt; expandedRDD = rdd1.flatMapToPair(</span><br><span class="line">        <span class="keyword">new</span> PairFlatMapFunction&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Iterable&lt;Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, Row&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                List&lt;Tuple2&lt;String, Row&gt;&gt; list = <span class="keyword">new</span> ArrayList&lt;Tuple2&lt;String, Row&gt;&gt;();</span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">                    list.add(<span class="keyword">new</span> Tuple2&lt;String, Row&gt;(<span class="number">0</span> + <span class="string">"_"</span> + tuple._1, tuple._2));</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> list;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 其次，将另一个有数据倾斜key的RDD，每条数据都打上100以内的随机前缀。</span></span><br><span class="line">JavaPairRDD&lt;String, String&gt; mappedRDD = rdd2.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> PairFunction&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, String&gt; <span class="title">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                Random random = <span class="keyword">new</span> Random();</span><br><span class="line">                <span class="keyword">int</span> prefix = random.nextInt(<span class="number">100</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, String&gt;(prefix + <span class="string">"_"</span> + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将两个处理后的RDD进行join即可。</span></span><br><span class="line">JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRDD = mappedRDD.join(expandedRDD);</span><br></pre></td></tr></table></figure><h3 id="解决方案八：多种方案组合使用"><a href="#解决方案八：多种方案组合使用" class="headerlink" title="解决方案八：多种方案组合使用"></a>解决方案八：多种方案组合使用</h3><p>在实践中发现，很多情况下，如果只是处理较为简单的数据倾斜场景，那么使用上述方案中的某一种基本就可以解决。但是如果要处理一个较为复杂的数据倾斜场景，那么可能需要将多种方案组合起来使用。比如说，我们针对出现了多个数据倾斜环节的Spark作业，可以先运用解决方案一和二，预处理一部分数据，并过滤一部分数据来缓解；其次可以对某些shuffle操作提升并行度，优化其性能；最后还可以针对不同的聚合或join操作，选择一种方案来优化其性能。大家需要对这些方案的思路和原理都透彻理解之后，在实践中根据各种不同的情况，灵活运用多种方案，来解决自己的数据倾斜问题。</p><hr><h1 id="四-shuffle-调优"><a href="#四-shuffle-调优" class="headerlink" title="四. shuffle 调优"></a>四. shuffle 调优</h1><h2 id="调优概述-3"><a href="#调优概述-3" class="headerlink" title="调优概述"></a>调优概述</h2><p>大多数Spark作业的性能主要就是消耗在了shuffle环节，因为该环节包含了大量的磁盘IO、序列化、网络数据传输等操作。因此，如果要让作业的性能更上一层楼，就有必要对shuffle过程进行调优。但是也必须提醒大家的是，影响一个Spark作业性能的因素，主要还是代码开发、资源参数以及数据倾斜，shuffle调优只能在整个Spark的性能调优中占到一小部分而已。因此大家务必把握住调优的基本原则，千万不要舍本逐末。下面我们就给大家详细讲解shuffle的原理，以及相关参数的说明，同时给出各个参数的调优建议。</p><h2 id="ShuffleManager发展概述"><a href="#ShuffleManager发展概述" class="headerlink" title="ShuffleManager发展概述"></a>ShuffleManager发展概述</h2><p> 在Spark的源码中，<strong>负责shuffle过程的执行、计算和处理的组件主要就是<code>ShuffleManager</code></strong>，也即shuffle管理器。而随着Spark的版本的发展，ShuffleManager也在不断迭代，变得越来越先进。</p><p>在<strong>Spark 1.2以前，默认的shuffle计算引擎是<code>HashShuffleManager</code>。</strong>该ShuffleManager而HashShuffleManager有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。</p><p>因此在<strong>Spark 1.2以后的版本中，默认的ShuffleManager改成了<code>SortShuffleManager</code>。</strong><u>SortShuffleManager</u>相较于HashShuffleManager来说，有了一定的改进。主要就在于，<u>每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并（merge）成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的shuffle read task拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。</u></p><p>下面我们详细分析一下HashShuffleManager和SortShuffleManager的原理。</p><h2 id="HashShuffleManager运行原理"><a href="#HashShuffleManager运行原理" class="headerlink" title="HashShuffleManager运行原理"></a>HashShuffleManager运行原理</h2><h3 id="未经优化的HashShuffleManager"><a href="#未经优化的HashShuffleManager" class="headerlink" title="未经优化的HashShuffleManager"></a>未经优化的HashShuffleManager</h3><p>下图说明了未经优化的HashShuffleManager的原理。这里我们先明确一个假设前提：每个Executor只有1个CPU core，也就是说，无论这个Executor上分配多少个task线程，同一时间都只能执行一个task线程。</p><p>我们先从<code>shuffle write</code>开始说起。shuffle write阶段，主要就是在一个stage结束计算之后，为了下一个stage可以执行shuffle类的算子（比如reduceByKey），而将每个task处理的数据按key进行“分类”。所谓“分类”，就是对相同的key执行hash算法，从而将相同key都写入同一个磁盘文件中，而每一个磁盘文件都只属于下游stage的一个task。在将数据写入磁盘之前，会先将数据写入内存缓冲中，当内存缓冲填满之后，才会溢写到磁盘文件中去。</p><p>那么<strong>每个执行shuffle write的task</strong>，要为下一个stage创建多少个磁盘文件呢？很简单，<strong>下一个stage的task有多少个，当前stage的每个task就要创建多少份磁盘文件</strong>。 如果单纯按照 key 来 hash 的话, 有多少不同的 key, ,每个 task 就会创建多少份磁盘文件.</p><p>比如下一个stage总共有100个task，那么当前stage的每个task都要创建100份磁盘文件。如果当前stage有50个task，总共有10个Executor，每个Executor执行5个Task，那么每个Executor上总共就要创建500个磁盘文件，所有Executor上会创建5000个磁盘文件。由此可见，未经优化的shuffle write操作所产生的磁盘文件的数量是极其惊人的。</p><p>接着我们来说说<code>shuffle read</code>。shuffle read，通常就是一个stage刚开始时要做的事情。此时该stage的每一个task就需要将上一个stage的计算结果中的所有相同key，从各个节点上通过网络都拉取到自己所在的节点上，然后进行key的聚合或连接等操作。由于shuffle write的过程中，task给下游stage的每个task都创建了一个磁盘文件，因此shuffle read的过程中，每个task只要从上游stage的所有task所在节点上，拉取属于自己的那一个磁盘文件即可。</p><p>shuffle read的拉取过程是一边拉取一边进行聚合的。每个shuffle read task都会有一个自己的buffer缓冲，每次都只能拉取与buffer缓冲相同大小的数据，然后通过内存中的一个Map进行聚合等操作。聚合完一批数据后，再拉取下一批数据，并放到buffer缓冲中进行聚合操作。以此类推，直到最后将所有数据到拉取完，并得到最终的结果。</p><p> <img src="https://tech.meituan.com/img/spark-tuning/hash-shuffle-common.png" alt="img"></p><h3 id="优化后的HashShuffleManager"><a href="#优化后的HashShuffleManager" class="headerlink" title="优化后的HashShuffleManager"></a>优化后的HashShuffleManager</h3><p>下图说明了优化后的<code>HashShuffleManager</code>的原理。这里说的<strong>优化，是指我们可以设置一个参数</strong>，<strong><code>spark.shuffle.consolidateFiles</code>。</strong>该参数默认值为false，将其<strong>设置为true即可开启优化机制</strong>。通常来说，如果我们使用HashShuffleManager，那么都建议开启这个选项。</p><p>开启consolidate机制之后，在shuffle write过程中，task就不是为下游stage的每个task创建一个磁盘文件了。此时会出现shuffleFileGroup的概念，<strong>每个<code>shuffleFileGroup</code>会对应一批磁盘文件，磁盘文件的数量与下游stage的task数量是相同的。</strong>一个Executor上有多少个CPU core，就可以并行执行多少个task。而第一批并行执行的每个task都会创建一个shuffleFileGroup，并将数据写入对应的磁盘文件内。</p><p><strong>当Executor的CPU core执行完一批task，接着执行下一批task时，下一批task就会复用之前已有的shuffleFileGroup，包括其中的磁盘文件。</strong>也就是说, <strong>同一个Executor的所有 task 中, 总共有多少不同的 key, 就会创建多少个磁盘文件, 这些磁盘文件都在同一个shuffleFileGroup中.</strong> 而不会出现, 相同的 Executor 的不同 task 中, 如果存在相同的 key, 还会各自创建自己的磁盘文件的情况. </p><p>也就是说，此时task会将数据写入已有的磁盘文件中，而不会写入新的磁盘文件中。因此，consolidate机制允许不同的task复用同一批磁盘文件，这样就可以有效将多个task的磁盘文件进行一定程度上的合并，从而大幅度减少磁盘文件的数量，进而提升shuffle write的性能。</p><p>假设第二个stage有100个task，第一个stage有50个task，总共还是有10个Executor，每个Executor执行5个task。那么原本使用未经优化的HashShuffleManager时，每个Executor会产生500个磁盘文件，所有Executor会产生5000个磁盘文件的。但是此时经过优化之后，每个Executor创建的磁盘文件的数量的计算公式为：CPU core的数量 * 下一个stage的task数量。也就是说，每个Executor此时只会创建100个磁盘文件，所有Executor只会创建1000个磁盘文件。</p><p> <img src="https://tech.meituan.com/img/spark-tuning/hash-shuffle-consolidate.png" alt="img"></p><h2 id="SortShuffleManager运行原理"><a href="#SortShuffleManager运行原理" class="headerlink" title="SortShuffleManager运行原理"></a>SortShuffleManager运行原理</h2><p><code>SortShuffleManager</code>的运行机制主要分成两种，一种是普通运行机制，另一种是bypass运行机制。<strong>当shuffle read task的数量小于等于spark.shuffle.sort.bypassMergeThreshold参数的值时（默认为200），就会启用bypass机制。</strong></p><h3 id="普通运行机制"><a href="#普通运行机制" class="headerlink" title="普通运行机制"></a>普通运行机制</h3><p>下图说明了普通的<code>SortShuffleManager</code>的原理。在该模式下，<strong>数据会先写入一个内存数据结构中，此时根据不同的shuffle算子，可能选用不同的数据结构</strong>。如果是<code>reduceByKey</code>这种聚合类的shuffle算子，那么会选用<code>Map数据结构</code>，一边通过Map进行聚合，一边写入内存；如果是<code>join</code>这种普通的shuffle算子，那么会<code>选用Array数据结构</code>，<strong>直接写入内存</strong>。接着，<u>每写一条数据进入内存数据结构之后，就会判断一下，是否达到了某个临界阈值</u>。如果<strong>达到临界阈值</strong>的话，那么就会尝试<strong>将内存数据结构中的数据溢写到磁盘，然后清空内存数据结构</strong>。</p><p><strong>在溢写到磁盘文件之前，会先对内存数据结构中已有的数据, 根据key, 把 key 对应的 value 进行排序</strong>。排序过后，会分批将数据写入磁盘文件。默认的batch数量是10000条(10000个 key, 一个 key 一条)，也就是说，排序好的数据，会以每批1万条数据的形式分批写入磁盘文件。写入磁盘文件是通过Java的BufferedOutputStream实现的。BufferedOutputStream是Java的缓冲输出流，首先会将数据缓冲在内存中，当内存缓冲满溢之后再一次写入磁盘文件中，这样可以减少磁盘IO次数，提升性能。</p><p>一个task将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写操作，也就会产生多个临时文件。最后会将之前所有的临时磁盘文件都进行合并，这就是merge过程，此时会将之前所有临时磁盘文件中的数据读取出来，然后依次写入最终的磁盘文件之中。此外，<strong>由于一个task就只对应一个磁盘文件，也就意味着该task为下游stage的task准备的数据都在这一个文件中</strong>，因此<strong>还会单独写一份索引文件，其中标识了下游各个task的数据在文件中的start offset与end offset。</strong></p><p>SortShuffleManager由于有一个磁盘文件merge的过程，因此大大减少了文件数量。比如第一个stage有50个task，总共有10个Executor，每个Executor执行5个task，而第二个stage有100个task。由于<strong>每个task最终只有一个磁盘文件</strong>，因此此时每个Executor上只有5个磁盘文件，所有Executor只有50个磁盘文件。</p><p><strong>注意:</strong> </p><ul><li>排序是指 key 的 value 排序</li><li>batch 写入磁盘是指, 默认 10000 个 (k,v)写一次.</li><li>是否会 hash? :TODO</li></ul><p><img src="https://tech.meituan.com/img/spark-tuning/sort-shuffle-common.png" alt="img"></p><h3 id="bypass运行机制"><a href="#bypass运行机制" class="headerlink" title="bypass运行机制"></a>bypass运行机制</h3><p>下图说明了bypass SortShuffleManager的原理。bypass运行机制的触发条件如下：</p><ul><li>shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值。</li><li><strong>其实bypass 就是一个高级的参数, 设置后, 如果小于此阈值, key 对应的 value 就不会进行排序</strong></li><li>不是聚合类的shuffle算子（比如reduceByKey）。</li></ul><p>此时task会为每个下游task都创建一个临时磁盘文件，并将数据按key进行hash然后根据key的hash值，将key写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件的。最后，同样会将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。</p><p>该过程的磁盘写机制其实跟未经优化的HashShuffleManager是一模一样的，因为都要创建数量惊人的磁盘文件，只是在最后会做一个磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的HashShuffleManager来说，shuffle read的性能会更好。</p><p>而该机制与普通SortShuffleManager运行机制的不同在于：第一，磁盘写机制不同；第二，不会进行排序。也就是说，启用该机制的最大好处在于，shuffle write过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销。</p><p><img src="https://tech.meituan.com/img/spark-tuning/sort-shuffle-bypass.png" alt="img"></p><h2 id="shuffle相关参数调优"><a href="#shuffle相关参数调优" class="headerlink" title="shuffle相关参数调优"></a>shuffle相关参数调优</h2><p>以下是Shffule过程中的一些主要参数，这里详细讲解了各个参数的功能、默认值以及基于实践经验给出的调优建议。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">Sparkconf</span>()</span><br><span class="line">conf.set(<span class="string">"spark.shuffle.manager"</span>,<span class="string">"hash"</span>)</span><br><span class="line">conf.set(<span class="string">"spark.shuffle.consolidateFiles"</span>,<span class="literal">true</span>)</span><br><span class="line">....</span><br><span class="line"><span class="comment">// 类似于这样设置</span></span><br></pre></td></tr></table></figure><h3 id="spark-shuffle-file-buffer"><a href="#spark-shuffle-file-buffer" class="headerlink" title="spark.shuffle.file.buffer"></a>spark.shuffle.file.buffer</h3><ul><li><strong>默认值：32k</strong></li><li>参数说明：该参数<strong>用于设置<code>shuffle write task</code>的<code>BufferedOutputStream</code>的<code>buffer</code>缓冲大小</strong>。将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘。</li><li>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少<code>shuffle write</code>过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。<strong>在实践中发现，合理调节该参数，性能会有1%~5%的提升。</strong></li></ul><h3 id="spark-reducer-maxSizeInFlight"><a href="#spark-reducer-maxSizeInFlight" class="headerlink" title="spark.reducer.maxSizeInFlight"></a>spark.reducer.maxSizeInFlight</h3><ul><li><strong>默认值：48m</strong></li><li>参数说明：该参数用于设置<strong>shuffle read task的buffer缓冲大小</strong>，而这个buffer缓冲决定了每次能够拉取多少数据。</li><li>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</li></ul><h3 id="spark-shuffle-io-maxRetries"><a href="#spark-shuffle-io-maxRetries" class="headerlink" title="spark.shuffle.io.maxRetries"></a>spark.shuffle.io.maxRetries</h3><ul><li><strong>默认值：3</strong></li><li>参数说明：shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了<strong>可以重试的最大次数</strong>。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。</li><li>调优建议：对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，<strong>对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。</strong></li></ul><h3 id="spark-shuffle-io-retryWait"><a href="#spark-shuffle-io-retryWait" class="headerlink" title="spark.shuffle.io.retryWait"></a>spark.shuffle.io.retryWait</h3><ul><li><strong>默认值：5s</strong></li><li>参数说明：具体解释同上，该参数代表了<strong>每次重试拉取数据的等待间隔，默认是5s。</strong></li><li>调优建议：<strong>建议加大间隔时长（比如60s），以增加shuffle操作的稳定性。</strong></li></ul><h3 id="spark-shuffle-memoryFraction-1"><a href="#spark-shuffle-memoryFraction-1" class="headerlink" title="spark.shuffle.memoryFraction"></a>spark.shuffle.memoryFraction</h3><ul><li><strong>默认值：0.2</strong></li><li>参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。</li><li>调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很少使用持久化操作，建议调高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，<strong>合理调节该参数可以将性能提升10%左右。</strong></li></ul><h3 id="spark-shuffle-manager"><a href="#spark-shuffle-manager" class="headerlink" title="spark.shuffle.manager"></a>spark.shuffle.manager</h3><ul><li><strong>默认值：sort</strong></li><li>参数说明：该参数<strong>用于设置ShuffleManager的类型</strong>。<strong><code>Spark 1.5</code>以后，有三个可选项：<code>hash</code>、<code>sort</code>和<code>tungsten-sort</code>。</strong><code>HashShuffleManager</code>是<code>Spark 1.2</code>以前的默认选项，但是<code>Spark 1.2</code><strong>以及之后</strong>的版本<strong>默认都是<code>SortShuffleManager</code>了</strong>。<code>tungsten-sort</code>与sort类似，但是使用了tungsten计划中的<strong>堆外内存管理机制</strong>，内存使用效率更高。</li><li>调优建议：由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中<strong>需要该排序机制</strong>的话，则<strong>使用默认的SortShuffleManager就可以</strong>；而如果你的业务逻辑<strong>不需要对数据进行排序</strong>，那么建议参考后面的几个参数调优，<strong>通过bypass机制或优化的HashShuffleManager来避免排序操作</strong>，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug。</li></ul><h3 id="spark-shuffle-sort-bypassMergeThreshold"><a href="#spark-shuffle-sort-bypassMergeThreshold" class="headerlink" title="spark.shuffle.sort.bypassMergeThreshold"></a>spark.shuffle.sort.bypassMergeThreshold</h3><ul><li><strong>默认值：200</strong></li><li>参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。</li><li>调优建议：当你<strong>使用<code>SortShuffleManager</code>时，如果的确<code>不需要</code>排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量</strong>。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。</li></ul><h3 id="spark-shuffle-consolidateFiles"><a href="#spark-shuffle-consolidateFiles" class="headerlink" title="spark.shuffle.consolidateFiles"></a>spark.shuffle.consolidateFiles</h3><ul><li><strong>默认值：false</strong></li><li>参数说明：<strong>如果使用HashShuffleManager，该参数有效。</strong>如果设置为<strong>true</strong>，那么就会<strong>开启consolidate机制</strong>，会<strong>大幅度合并shuffle write的输出文件</strong>，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。</li><li>调优建议：如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还<strong>可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。</strong>在实践中尝试过，发现<strong>其性能比开启了bypass机制的SortShuffleManager要高出10%~30%。</strong></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;Spark整套调优方案主要分为开发调优、资源调优、数据倾斜调优、shuffle调优几个部分。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;开发调优和资源调
      
    
    </summary>
    
      <category term="Spark" scheme="https://airpoet.github.io/categories/Spark/"/>
    
      <category term="精讲" scheme="https://airpoet.github.io/categories/Spark/%E7%B2%BE%E8%AE%B2/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Spark" scheme="https://airpoet.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark重点解析(一) =&gt; SparkCore</title>
    <link href="https://airpoet.github.io/2018/07/25/Spark/Spark%E9%87%8D%E7%82%B9%E8%A7%A3%E6%9E%90(%E4%B8%80)%20=%3E%20SparkCore/"/>
    <id>https://airpoet.github.io/2018/07/25/Spark/Spark重点解析(一) =&gt; SparkCore/</id>
    <published>2018-07-25T06:58:04.056Z</published>
    <updated>2018-08-05T14:39:26.491Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一-Spark-与-MapReduce-区别"><a href="#一-Spark-与-MapReduce-区别" class="headerlink" title="一. Spark 与 MapReduce 区别"></a>一. Spark 与 MapReduce 区别</h1><p><strong>Apache Spark™</strong> is a fast and general engine for large-scale data processing.</p><p><strong>与mapreduce比较 </strong>：</p><ul><li><p>Spark大多数执行过程是基于内存的迭代</p></li><li><p>MapReduce 的 优点, SparkCore 都有</p></li><li><p>Hive 能做的操作, SparkSQL 都能做, 可以写 SQL 语句转换为 SparkCore 代码</p></li><li><p>Spark Streaming 提供近实时流</p></li><li><p>超过80个类似于 map, reduce 这样的操作</p></li><li><p>可以在<a href="http://www.alluxio.org/" target="_blank" rel="noopener">Tachyon</a>（基于内存的分布式的文件系统 (HDFS 是基于磁盘)) 上运行Spark, 会更快</p></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-25-070107.png" alt="image-20180725150106695"></p><hr><h1 id="二-什么是RDD"><a href="#二-什么是RDD" class="headerlink" title="二. 什么是RDD"></a>二. 什么是RDD</h1><p>1、RDD是Spark提供的核心<strong>抽象</strong>，全称为Resillient Distributed Dataset，即<strong>弹性</strong>分布式数据集。</p><p>2、RDD在抽象上来说是一种元素集合，包含了数据。它是<strong>被分区的</strong>，分为多个分区，每个分区分布在集群中的不同节点上，从而让RDD中的数据可以被并行操作。（分布式数据集）</p><ul><li>如果读取的文件本来就存在于3个分区, 这些操作会并行操作, 如何并行操作?  :TODO</li><li>如果存在于3个分区, 手动规定了2个分区, 那么是如何工作的 ? :TODO</li></ul><p>3、RDD通常<strong>通过Hadoop上的文件</strong>，即HDFS文件或者Hive表，来进行<strong>创建</strong>；有时也可以<strong>通过应用程序中的集合来创建</strong>。</p><ul><li><strong>从文件系统读取</strong>: local 或 HDFS <code>sc.textFile(&quot;/Users/shixuanji/Documents/a.txt&quot;,2);</code></li><li>Hive 表: : TODO</li><li>并行化的方式创建(多用于测试): <code>val rdd = sc.makeRDD(1 to 10)</code>  或者 <code>val rdd = sc.parallelize(arr);</code></li></ul><p>4、RDD最重要的特性就是，提供了<strong>容错性</strong>，可以自动从节点失败中恢复过来。即如果某个节点上的RDD partition，因为节点故障，导致数据丢了，那么<strong>RDD会自动通过自己的数据来源重新计算该partition</strong>。这一切对使用者是透明的。</p><p>5、RDD的数据<strong>默认</strong>情况下存<strong>放在内存</strong>中的，但是在<strong>内存资源不足时</strong>，Spark<strong>会自动将RDD数据写入磁盘</strong>。（弹性 == 灵活）</p><blockquote><p><strong>下图中画橙色框的都是 RDD </strong></p></blockquote><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-25-070716.png" alt="image-20180725150715950"></p><h4 id="源码中的注释说明"><a href="#源码中的注释说明" class="headerlink" title="源码中的注释说明"></a><strong>源码中的注释说明</strong></h4><p>1、A list of partitions：一组分片（Partition），即数据集的基本组成单位</p><p>2、A function for computing each split：一个计算每个分区的函数</p><p>3、A list of dependencies on other RDDs：RDD 之间的依赖关系</p><ul><li>NarrowDependency   完全依赖, 窄依赖</li><li>ShuffleDependency   部分依赖, ‘’宽依赖’’</li></ul><p>4、Optionally, a Partitioner for <strong>key-value RDDs</strong> (e.g. to say that the RDD is hash-partitioned)：</p><ul><li>Partitioner: 自定义分区使用</li></ul><p>5、Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)：一个列表，存储存取每个 Partition 的优先位置（preferred location）。</p><h4 id="Spark-中其它重要概念"><a href="#Spark-中其它重要概念" class="headerlink" title="Spark 中其它重要概念"></a>Spark 中其它重要概念</h4><p><strong>Cluster Manager</strong>：Spark 的集群管理器，主要负责资源的分配与管理。集群管理器分配的资 源属于一级分配，它将各个 Worker 上的内存、CPU 等资源分配给应用程序，但是并不负责 对 Executor 的资源分配。目前，Standalone、YARN、Mesos、K8S，EC2 等都可以作为 Spark 的集群管理器。</p><p><strong>Master</strong>：Spark 集群的主节点。</p><p><strong>Worker</strong>：Spark 集群的工作节点。对 Spark 应用程序来说，由集群管理器分配得到资源的 Worker 节点主要负责以下工作：创建 Executor，将资源和任务进一步分配给 Executor，同步 资源信息给 Cluster Manager。</p><p><strong>Executor</strong>：执行计算任务的一些进程。主要负责任务的执行以及与 Worker、Driver Application 的信息同步。</p><p><strong>Driver Appication：</strong>客户端驱动程序，也可以理解为客户端应用程序，用于将任务程序转换 为 RDD 和 DAG，并与 Cluster Manager 进行通信与调度。</p><p><strong>关系</strong>:</p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-26-093416.png" alt="image-20180726173416513"></p><hr><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-26-093433.png" alt="image-20180726173433286"></p><p>1、用户使用 SparkContext 提供的 API（常用的有 textFile、sequenceFile、runJob、stop 等） 编写 Driver Application 程序。此外 SQLContext、HiveContext 及 StreamingContext 对 SparkContext 进行封装，并提供了 SQL、Hive 及流式计算相关的 API。</p><p>2、使用 SparkContext 提交的用户应用程序，首先会使用 BlockManager 和 BroadcastManager 将任务的 Hadoop 配置进行广播。然后由 DAGScheduler 将任务转换为 RDD 并组织成 DAG， DAG 还将被划分为不同的 Stage。<strong>最后由 TaskScheduler 借助 ActorSystem 将任务提交给集群 管理器（Cluster Manager）。</strong></p><p>3、集群管理器（ClusterManager）给任务分配资源，即将具体任务分配到 Worker 上，Worker 创建 Executor 来处理任务的运行。Standalone、YARN、Mesos、EC2 等都可以作为 Spark 的集 群管理器。</p><p><strong>注意:</strong>  如果是 –deploy-mode <strong>client</strong> 模式, <strong>client 就是 Driver</strong>,   –deploy-mode <strong>cluster</strong> 模式, <strong>Driver 是由集群分配的一台 worker节点</strong></p><h1 id="三-Spark-的架构-standalone"><a href="#三-Spark-的架构-standalone" class="headerlink" title="三. Spark 的架构(standalone)"></a>三. Spark 的架构(standalone)</h1><p>涉及到的名词: <strong>Driver</strong>, <strong>Master</strong>, <strong>Worker</strong>, <strong>Executor</strong> , <strong>Task</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-25-075654.png" alt="image-20180725155654170"></p><hr><h1 id="四-Spark-任务提交"><a href="#四-Spark-任务提交" class="headerlink" title="四. Spark 任务提交"></a>四. Spark 任务提交</h1><p>参考官网 <a href="http://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/submitting-applications.html</a></p><p><strong>Client</strong>模式</p><p>不指定<code>deploy-mode</code> ,默认就是<strong>client</strong>模式，也就是<strong>哪一台服务器提交</strong>spark代码，那么<strong>哪一台就是driver服务器</strong>。</p><p><strong>Cluster</strong>模式</p><p>需要指定<code>deploy-mode</code>，driver服务器并不是提交代码的那一台服务器，而是在提交代码的时候，在<strong>worker</strong>主机上，<strong>随机挑选一台作为driver服务器</strong>，那么<u>如果提交10个应用，那么就有可能10台driver服务器</u>。</p><p><strong>–master spark://xxxxxx</strong></p><p>需要启动 spark 集群</p><p><strong>–master yarn</strong> </p><p>不需要启动 spark 集群, 提交的程序由 yarn 管理</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run application locally on 8 cores</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master <span class="built_in">local</span>[8] \</span><br><span class="line">  /path/to/examples.jar \</span><br><span class="line">  100</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run on a Spark standalone cluster in client deploy mode</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master spark://207.184.161.138:7077 \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --total-executor-cores 100 \</span><br><span class="line">  /path/to/examples.jar \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run on a Spark standalone cluster in cluster deploy mode with supervise</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master spark://207.184.161.138:7077 \</span><br><span class="line">  --deploy-mode cluster \</span><br><span class="line">  --supervise \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --total-executor-cores 100 \</span><br><span class="line">  /path/to/examples.jar \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run on a YARN cluster (公司常用)</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=XXX</span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master yarn \</span><br><span class="line">  --deploy-mode cluster \  <span class="comment"># can be client for client mode</span></span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --num-executors 50 \</span><br><span class="line">  /path/to/examples.jar \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run a Python application on a Spark standalone cluster</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --master spark://207.184.161.138:7077 \</span><br><span class="line">  examples/src/main/python/pi.py \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run on a Mesos cluster in cluster deploy mode with supervise</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master mesos://207.184.161.138:7077 \</span><br><span class="line">  --deploy-mode cluster \</span><br><span class="line">  --supervise \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --total-executor-cores 100 \</span><br><span class="line">  http://path/to/examples.jar \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run on a Kubernetes cluster in cluster deploy mode</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master k8s://xx.yy.zz.ww:443 \</span><br><span class="line">  --deploy-mode cluster \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --num-executors 50 \</span><br><span class="line">  http://path/to/examples.jar \</span><br><span class="line">  1000</span><br></pre></td></tr></table></figure><hr><h1 id="五-Transformation和action原理"><a href="#五-Transformation和action原理" class="headerlink" title="五. Transformation和action原理"></a>五. Transformation和action原理</h1><p><strong>Spark支持两种RDD操作：<code>transformation</code>和<code>action</code></strong>。</p><ul><li><code>transformation</code>操作会<strong>针对已有的RDD创建一个新的RDD</strong>；</li><li>而<code>action</code>则主要是对RDD进行最后的操作，比如遍历、reduce、保存到文件等，并可以<strong>返回结果给Driver程序</strong>。</li></ul><p>例如，map就是一种transformation操作，它用于将已有RDD的每个元素传入一个自定义的函数，并获取一个新的元素，然后将所有的新元素组成一个新的RDD。而reduce就是一种action操作，它用于对RDD中的所有元素进行聚合操作，并获取一个最终的结果，然后返回给Driver程序。</p><p><strong>transformation的特点就是lazy特性</strong>。lazy特性指的是，如果一个spark应用中只定义了transformation操作，那么即使你执行该应用，这些操作也不会执行。也就是说，transformation是不会触发spark程序的执行的，它们只是记录了对RDD所做的操作，但是不会自发的执行。只有当transformation之后，接着执行了一个action操作，那么所有的transformation才会执行。<strong>Spark通过这种lazy特性，来进行底层的spark应用执行的优化，<u>避免产生过多中间结果</u>。</strong></p><p><strong>action操作执行，会触发一个<code>spark job</code>的运行，从而触发这个action之前所有的transformation的执行。</strong>这是action的特性。</p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-25-084407.png" alt="image-20180725164407028"></p><hr><h1 id="六-Transformation-和-Action-算子"><a href="#六-Transformation-和-Action-算子" class="headerlink" title="六. Transformation 和 Action 算子"></a>六. Transformation 和 Action 算子</h1><h2 id="1-Transformation-算子"><a href="#1-Transformation-算子" class="headerlink" title="1.Transformation 算子"></a>1.Transformation 算子</h2><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/java/TransfromationOperation.java" target="_blank" rel="noopener">Java 版代码, 见这里 &amp; 上层目录</a> </p><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/scala/TransformationOperations.scala" target="_blank" rel="noopener">Scala 版代码, 见这里 &amp; 上层目录</a></p><p><strong>或 ../</strong></p><h4 id="map"><a href="#map" class="headerlink" title="map:"></a>map:</h4><p>对调用map的RDD数据集中的每个element都使用func，然后返回一个新的RDD,这个返回的数据集是分布式的数据集</p><h4 id="filter"><a href="#filter" class="headerlink" title="filter:"></a>filter:</h4><p>对调用filter的RDD数据集中的每个元素都使用func，然后返回一个包含使func为true的元素构成的RDD</p><h4 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap:"></a>flatMap:</h4><p>和map差不多，但是flatMap生成的是多个结果</p><h4 id="groupByKey-reduceByKey-sortByKey"><a href="#groupByKey-reduceByKey-sortByKey" class="headerlink" title="groupByKey, reduceByKey, sortByKey :"></a>groupByKey, reduceByKey, sortByKey :</h4><p>凡是这种.. ByKey 的, 必须传入一个对偶元祖,  Java中是 JavaPairRdd</p><h4 id="cogroup-与-join-与-union-区别"><a href="#cogroup-与-join-与-union-区别" class="headerlink" title="cogroup 与 join 与 union 区别:"></a>cogroup 与 join 与 union 区别:</h4><p><a href="http://lxw1234.com/archives/2015/07/384.htm" target="_blank" rel="noopener">详情可参考</a></p><ul><li><strong>cogroup</strong><ul><li>相当于SQL中的<strong>全外关联full outer join</strong>，返回左右RDD中的记录，<strong>关联不上的为空。</strong></li><li>return:   JavaPairRDD[K, (JIterable[V], JIterable[W])</li><li>RDD的value是一个Pair的实例,这个实例包含两个Iterable的值, <code>V</code>表示的是RDD1中相同KEY的值, <code>W</code>表示的是RDD2中相同key的值. </li></ul></li><li><strong>join</strong><ul><li>相当于SQL中的<strong>内关联join</strong>，<strong>只返回两个RDD根据K可以关联上的结果</strong>，join只能用于两个RDD之间的关联，如果要多个RDD关联，多关联几次即可。</li></ul></li><li><strong>leftOuterJoin / rigthOuterJoin</strong><ul><li><strong>left</strong>: 左边的一定显示, 右边的 Join 上才显示</li><li><strong>right</strong>:  右边的一定显示, 左边的 join 上才显示</li></ul></li><li><strong>union</strong><ul><li>求rdd并集，但是不去重</li></ul></li></ul><h4 id="Intersection，Distinct，Cartesian"><a href="#Intersection，Distinct，Cartesian" class="headerlink" title="Intersection，Distinct，Cartesian"></a>Intersection，Distinct，Cartesian</h4><ul><li><strong>intersection</strong> <ul><li>intersection <strong>求交集</strong>,提取两个rdd中都含有的元素。</li><li>Returns a new RDD that contains the intersection of elements in the source dataset and the argument.</li></ul></li><li><strong>Distinct</strong> (独特的,有区别的)<ul><li>去重 </li><li>Return a new RDD containing the distinct elements in this RDD.</li></ul></li><li><strong>Cartesian</strong> (笛卡尔积)<ul><li>笛卡尔积, 全连接, 前后集合个数为a,b,  a x b  种组合</li></ul></li></ul><h4 id="mapPartition，reparation，coalesce"><a href="#mapPartition，reparation，coalesce" class="headerlink" title="mapPartition，reparation，coalesce"></a>mapPartition，reparation，coalesce</h4><ul><li><p><strong>mapPartition</strong></p><ul><li>该函数和map函数类似，只不过映射函数的参数<strong>由RDD中的每一个元素</strong>变成了<strong><code>RDD</code>中每一个分区的迭代器</strong>。如果在映射的过程中需要频繁创建额外的对象，使用mapPartitions要比map高效的多。 </li><li>比如，将RDD中的所有数据通过JDBC连接写入数据库，如果使用map函数，可能要为每一个元素都创建一个<code>connection</code>，这样开销很大，如果使用<code>mapPartitions</code>，那么只需要针对每一个分区建立一个<code>connection</code>。 </li></ul></li><li><p><strong>coalesce</strong> </p><ul><li><strong>coalesce: 只能用于减少分区的数量</strong>，而且<strong>可以选择不发生shuffle</strong> 其实说白了他就是<strong>合并分区</strong></li><li><strong>repartition:可以增加分区，也可以减少分区</strong>，<strong>必须会发生shuffle</strong>，相当于就是进行<strong>重新分区</strong></li></ul></li><li><p><strong>reparation</strong></p><ul><li>reparition是<code>coalesce shuffle</code>为<code>true</code>的简易实现 </li></ul></li></ul><h4 id="sample-和-aggregateByKey"><a href="#sample-和-aggregateByKey" class="headerlink" title="sample   和  aggregateByKey"></a>sample   和  aggregateByKey</h4><ul><li><p><strong>sample</strong></p><ul><li><p>对RDD中的集合内元素进行采样，第一个参数withReplacement是true表示有放回取样，false表示无放回。第二个参数表示比例 </p></li><li><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *- @param withReplacement can elements be sampled multiple times (replaced when sampled out)</span></span><br><span class="line"><span class="comment">    @param fraction expected size of the sample as a fraction of this RDD's size</span></span><br><span class="line"><span class="comment">       seed  最好不要动</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span></span>(</span><br><span class="line">      withReplacement: <span class="type">Boolean</span>,</span><br><span class="line">      fraction: <span class="type">Double</span>,</span><br><span class="line">      seed: <span class="type">Long</span> = <span class="type">Utils</span>.random.nextLong): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p><strong>aggregateByKey</strong></p><ul><li><code>aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])</code>            //按照key进行聚合 </li><li>其实<code>reduceBykey</code> 就是<code>aggregateByKey</code>的简化版。  </li><li><code>aggregateByKey</code>多提供了一个函数 seqOp 类似于Mapreduce的combine操作（就在map端执行reduce的操作）</li></ul></li></ul><h4 id="mapPartitionsWithIndex-和-repartitionAndSortWithinPartitions"><a href="#mapPartitionsWithIndex-和-repartitionAndSortWithinPartitions" class="headerlink" title="mapPartitionsWithIndex 和 repartitionAndSortWithinPartitions"></a>mapPartitionsWithIndex 和 repartitionAndSortWithinPartitions</h4><ul><li><strong>mapPartitionsWithIndex</strong> <ul><li>说白了就是可以打印出当前所在分区数</li></ul></li><li><strong>repartitionAndSortWithinPartitions</strong><ul><li><strong>该方法依据partitioner对RDD进行分区，并且在每个结果分区中按key进行排序；通过对比sortByKey发现，这种方式比先分区，然后在每个分区中进行排序效率高，这是因为它可以将排序融入到shuffle阶段。</strong> </li></ul></li></ul><h2 id="2-Action-算子"><a href="#2-Action-算子" class="headerlink" title="2. Action 算子"></a>2. Action 算子</h2><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/java/ActionOperations.java" target="_blank" rel="noopener"><strong>Java 代码见这里</strong></a></p><h4 id="reduce"><a href="#reduce" class="headerlink" title="reduce();"></a>reduce();</h4><ul><li>def reduce(f: JFunction2[T, T, T]): T = rdd.reduce(f)</li></ul><h4 id="collect"><a href="#collect" class="headerlink" title="collect();"></a>collect();</h4><ul><li>Return an array that contains all of the elements in this RDD.</li><li>this method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver’s memory.</li></ul><h4 id="take-n"><a href="#take-n" class="headerlink" title="take(n);"></a>take(n);</h4><ul><li>Take the first num elements of the RDD. This currently scans the partitions <em>one by one</em>, so it will be slow if a lot of partitions are required. In that case, use collect() to get the  whole RDD instead.</li><li>this method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver’s memory.</li></ul><h4 id="count"><a href="#count" class="headerlink" title="count();"></a>count();</h4><ul><li>Return the number of elements in the RDD.</li></ul><h4 id="takeOrdered"><a href="#takeOrdered" class="headerlink" title="takeOrdered();"></a>takeOrdered();</h4><ul><li>Returns the <strong>first k (smallest)</strong> elements from this RDD using the  <strong>natural ordering</strong> for T while maintain the order.</li><li>top(n): 自然排序后,最大的前 n</li></ul><h4 id="saveAsTextFile"><a href="#saveAsTextFile" class="headerlink" title="saveAsTextFile();"></a>saveAsTextFile();</h4><ul><li>Save this RDD as a text file, using string representations of elements.</li></ul><h4 id="countByKey"><a href="#countByKey" class="headerlink" title="countByKey();"></a>countByKey();</h4><ul><li>return  map&lt;String, Integer&gt;, key 为 key, value 为 key 的数量</li></ul><h4 id="takeSample"><a href="#takeSample" class="headerlink" title="takeSample();"></a>takeSample();</h4><ul><li>withReplacement：元素可以多次(重复)抽样(在抽样时替换) 如果为 false, 在抽样数 &gt; 样本数时, 只能返回样本数的样本</li><li>num：返回的样本的大小</li><li>seed：随机数生成器的种子</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeSample</span></span>(</span><br><span class="line">    withReplacement: <span class="type">Boolean</span>,</span><br><span class="line">    num: <span class="type">Int</span>,</span><br><span class="line">    seed: <span class="type">Long</span> = <span class="type">Utils</span>.random.nextLong): <span class="type">Array</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure><hr><h1 id="七-RDD-的持久化"><a href="#七-RDD-的持久化" class="headerlink" title="七. RDD 的持久化"></a>七. RDD 的持久化</h1><p>将数据通过操作持久化（或缓存）在内存中是Spark的重要能力之一。当你缓存了一个RDD，每个节点都缓存了RDD的所有分区。这样就可以在内存中进行计算。这样可以使以后在RDD上的动作更快（通常可以提高10倍）。</p><p>你可以对希望缓存的RDD通过使用persist或cache方法进行标记。它通过动作操作第一次在RDD上进行计算后，它就会被缓存在节点上的内存中。Spark的缓存具有容错性，如果RDD的某一分区丢失，它会自动使用最初创建RDD时的转换操作进行重新计算。</p><p>另外，RDD可以被持久化成不同的级别。比如，可以允许你存储在磁盘，内存，甚至是序列化的<strong>Java</strong>对象（节省空间），备份在不同的节点上，或者存储在基于内存的文件系统Tachyon上。通过向persist()方法传递StorageLevel对象来设置。cache方法是使用默认级别<code>StorageLevel.MEMORY_ONLY</code>的方法。</p><h4 id="选持久化方案建议："><a href="#选持久化方案建议：" class="headerlink" title="选持久化方案建议："></a><strong>选持久化方案建议：</strong></h4><ol><li>优先选择MEMORY_ONLY，如果可以用内存缓存所有的数据，那么也就意味着我的计算是纯内存的计算，速度当然快。 </li><li>如果MEMORY_ONLY 缓存不了所有的数据，MEMORY_ONLY_SER 把数据实现序列化然后进行存储。这样也是纯内存操作，速度也快，只不过需要耗费一点cpu资源需要反序列化。 </li><li>也可以选用带_2这种方式, 此方式会存2份, 一份存在本地, 另一份会存到另外的节点。恢复速度的时候可以使用备份。</li><li>能不能使用DISK的，就不使用DISK，有时候从磁盘读，还不如从新计算一次。 </li></ol><h4 id="关于tachyon"><a href="#关于tachyon" class="headerlink" title="关于tachyon "></a>关于<strong><a href="http://www.alluxio.org/" target="_blank" rel="noopener">tachyon</a> </strong></h4><p>Spark2.0开始就不把tachyon(现在成为alluxio)集成在自身内部了, 依然可以直接用</p><p><strong>基于内存的分布式文件系统</strong> </p><p><strong>出现原因</strong>:</p><ul><li>spark运行以 JVM为基础，所以spark的任务会把数据存入JVM的堆中，随着计算的迭代，JVM堆中存放的数据量迅速增大，对于spark而言，spark的计算引擎和存储引擎处在同一个JVM中，所以会有重复的GC方面的开销。这样就增大了系统的延时。</li><li>当JVM崩溃时，缓存在JVM堆中的数据也会消失，这个时候spark不得不根据RDD的血缘关系重新计算数据。</li><li>如果spark需要其他的框架的共享数据，比如就是hadoop的Mapreduce，这个时候就必须通过第三方来共享，比如借助HDFS，那么这样的话，就需要额外的开销，借助的是HDFS，那么就需要磁盘IO的开销。</li><li>因为我们基于内存的分布式计算框架有以上的问题，那么就促使了内存分布式文件系统的诞生，比如tachyon。</li></ul><p><strong>Tachyon可以解决spark的什么问题呢？</strong></p><p>如果我们把数据存放到tachyon上面：</p><ul><li><strong>减少Spark GC的开销。</strong> </li><li><strong>当spark 的JVM崩溃的时候，存放在tachyon上的数据不受影响。</strong> </li><li><strong>spark如果要想跟被的计算工具共享数据，只要通过tachyon的Client就可以做到了。并且延迟远低于HDFS等系统。</strong> </li></ul><hr><h1 id="八-广播变量-和-累加器"><a href="#八-广播变量-和-累加器" class="headerlink" title="八. 广播变量 和  累加器"></a>八. 广播变量 和  累加器</h1><h2 id="1-广播变量"><a href="#1-广播变量" class="headerlink" title="1. 广播变量"></a>1. 广播变量</h2><p><strong>案例</strong>&gt; <strong>广播 ip 规则, 匹配 ip 十进制地址, 存入 mysql, 完整过程图示</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-08-04-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F%202.png" alt=""></p><p>每个 executor 拥有一份， 这个 executor 启动的 task 会共享这个变量</p><p>使用了广播变量之后, executor 中所有的 task 都会共享此变量, 否则每个 task 都会发一份</p><p>在 Driver 端可以修改广播变量的值，在 Executor 端无法修改广播变量的值。</p><p><strong>使用:</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义</span></span><br><span class="line"><span class="keyword">val</span> a = <span class="number">3</span></span><br><span class="line"><span class="keyword">val</span> broadcast = sc.broadcast(a)</span><br><span class="line"><span class="comment">// 获取</span></span><br><span class="line"><span class="keyword">val</span> c = broadcast.value</span><br></pre></td></tr></table></figure><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-26-090621.png" alt="image-20180726170621192"></p><h2 id="2-累加器"><a href="#2-累加器" class="headerlink" title="2. 累加器"></a>2. 累加器</h2><p><strong>使用场景</strong>: 异常监控，调试，记录符合某特性的数据的数目等</p><p><strong>如果一个变量不被声明为一个累加器</strong>，那么它将在 被改变时不会在 driver 端进行全局汇总，即在<strong>分布式运行时每个 task 运行的只是原始变量的一个副本</strong>，并不能改变原始变量的值</p><p>但是当这个变量被声明为<strong>累加器</strong>后，<strong>该变量就会有分布式计数的功能。</strong></p><p>累加器<strong>在 Driver 端定义赋初始值</strong>，累加器只能在 <strong>Driver 端读取最后的值</strong>，在 <strong>Excutor 端更 新</strong>。</p><p><strong>使用:</strong> </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义</span></span><br><span class="line"><span class="keyword">val</span> a = sc.longAccumulator(<span class="number">0</span>)</span><br><span class="line"><span class="comment">// 获取</span></span><br><span class="line"><span class="keyword">val</span> b = a.value</span><br></pre></td></tr></table></figure><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-26-090907.png" alt="image-20180726170906622"></p><hr><h1 id="九-Spark-on-Yarn-模式"><a href="#九-Spark-on-Yarn-模式" class="headerlink" title="九. Spark on Yarn 模式"></a>九. Spark on Yarn 模式</h1><p><strong>配置</strong>: 只需要在<code>conf/spark-env.sh</code> 中配置<code>export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</code> 就可以了</p><p><strong>使用YARN模式的时候，不需要启动master和worker了。</strong> </p><p><strong>只需要启动HDFS和YARN即可。</strong> </p><p><strong>与 <code>Standalone</code>主要区别是</strong>: <code>spark-submit</code>后面的参数中<code>--master</code>后面的不是 spark://….,   而是 <code>yarn</code>,  这样:<code>--master yarn</code></p><h4 id="Spark-on-Yarn-模式注意点"><a href="#Spark-on-Yarn-模式注意点" class="headerlink" title="Spark on Yarn 模式注意点:"></a>Spark on Yarn 模式注意点:</h4><p><strong>注意：如果你配置<code>spark-on-yarn的client</code>模式，其实会报错。</strong></p><p><strong>内部有一个内存检测机制</strong></p><p>修改所有<code>yarn</code>节点的<code>yarn-site.xml</code>，在该文件中添加如下配置</p> <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="–deploy-mode-client"><a href="#–deploy-mode-client" class="headerlink" title="–deploy-mode client"></a>–deploy-mode client</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-26-165721.png" alt="image-20180727005721062"></p><h4 id="–deploy-mode-cluster"><a href="#–deploy-mode-cluster" class="headerlink" title="–deploy-mode cluster"></a>–deploy-mode cluster</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-26-165858.png" alt="image-20180727005857420"></p><h1 id="十-宽窄依赖"><a href="#十-宽窄依赖" class="headerlink" title="十. 宽窄依赖"></a>十. 宽窄依赖</h1><p><strong>窄依赖是指父RDD的每个分区都只被子RDD一个分区使用。</strong>(独生, NarrowDependency)</p><p><strong>宽依赖就是指父RDD的分区被多个子RDD的分区所依赖。</strong> (超生, ShuffleDependency)</p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-26-170442.png" alt="image-20180727010441959"></p><h1 id="十一-Stage-划分"><a href="#十一-Stage-划分" class="headerlink" title="十一. Stage 划分"></a>十一. Stage 划分</h1><p>开发完一个应用以后，把应用提交到集群，那么这个应用就叫做Application</p><p>这个应用里面我们开发了好多代码，这些代码里面凡是遇到一个action操作，就会产生一个job任务。</p><p>也就意味着，一个Application有一个或者一个以上的job任务。</p><p>然后这些job任务划分为不同stage去执行，stage里面就是运行不同的task任务。</p><p>遇到一个 shuffle 算子, 就会从中间分开, 划分为2个 stage</p><p>Task计算的就是分区上面的数据。</p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-26-172557.png" alt="image-20180727012556575"></p><h1 id="十二-Spark-任务调度"><a href="#十二-Spark-任务调度" class="headerlink" title="十二. Spark 任务调度"></a><strong>十二. Spark 任务调度</strong></h1><p><strong>Shuffle 机制见下一篇</strong></p><h2 id="1-简版"><a href="#1-简版" class="headerlink" title="1.简版"></a>1.简版</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-26-173025.png" alt="image-20180727013025763"></p><h2 id="2-完整版"><a href="#2-完整版" class="headerlink" title="2.完整版"></a>2.完整版</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-26-spark%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%E5%9B%BE%E8%A7%A3.png" alt=""></p><p><strong>注意:</strong> </p><ul><li>AppClient / clientActor：在 Standalone 模式下的实现是 <code>StandaloneAppClient</code> 类</li><li>dirverActor:  :TODO</li><li><strong>第7点, <code>ApplicationDescription</code>参数中封装的是一些系统信息, 和 用户设置的 cpu 的核数, 执行的线程数之类的数据</strong></li><li><strong>worker 端是在全图的 第 19步(action 算子触发后, 会提交 job )才知道, 去哪里计算数据.</strong> </li></ul><h1 id="十三-TopN-案例"><a href="#十三-TopN-案例" class="headerlink" title="十三. TopN 案例"></a>十三. TopN 案例</h1><p><strong>对一个文件里面的单词进行单词计数，然后取前3个出现次数最多的三个单词。</strong> </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TopN</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">     <span class="keyword">val</span> conf=<span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"TopN"</span>)</span><br><span class="line">     <span class="keyword">val</span> sc=<span class="keyword">new</span> <span class="type">SparkContext</span>(conf);</span><br><span class="line">     <span class="keyword">val</span> file=sc.textFile(<span class="string">"file://..."</span>)</span><br><span class="line">     <span class="keyword">val</span> topN=file.flatMap &#123; line =&gt; line.split(<span class="string">"\t"</span>) &#125;</span><br><span class="line">     .map &#123; word =&gt; (word,<span class="number">1</span>) &#125;</span><br><span class="line">     .reduceByKey(_+_)  <span class="comment">//key word  value:count</span></span><br><span class="line">     .map(tuple =&gt;(tuple._2,tuple._1))<span class="comment">// swap k,v</span></span><br><span class="line">     .sortByKey(<span class="literal">false</span>)   <span class="comment">// sort the count</span></span><br><span class="line">     .take(<span class="number">3</span>)  <span class="comment">// take top 3</span></span><br><span class="line">     <span class="keyword">for</span>( i &lt;- topN)&#123;</span><br><span class="line">       println(i._2 + <span class="string">"  出现次数："</span>+i._1);</span><br><span class="line">     &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="十四-网站访问日志分析"><a href="#十四-网站访问日志分析" class="headerlink" title="十四. 网站访问日志分析"></a>十四. 网站访问日志分析</h1><h3 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h3><p><strong>需求一 ：</strong></p><p>The average, min, and max content size of responses returned from the server.</p><p><strong>需求二： </strong></p><p>A count of response code’s returned.</p><p><strong>需求三： </strong></p><p>All IPAddresses that have accessed this server more than N times.</p><p><strong>需求四：</strong></p><p>The top endpoints requested by count.  TopN 找出被访问次数最多的地址的前三个</p><h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">log.txt</span><br><span class="line">------------</span><br><span class="line">10.0.0.153<span class="comment">#-#-#[12/Mar/2004:12:23:18-0800]#"GET /cgi-bin/mailgraph.cgi/mailgraph_3_err.png HTTP/1.1"#200#5554</span></span><br><span class="line">10.0.0.153<span class="comment">#-#-#[12/Mar/2004:12:23:40-0800]#"GET /dccstats/index.html HTTP/1.1"#304#2000</span></span><br><span class="line">10.0.0.153<span class="comment">#-#-#[12/Mar/2004:12:23:41-0800]#"GET /dccstats/stats-spam.1day.png HTTP/1.1"#200#2964</span></span><br></pre></td></tr></table></figure><h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ApacheAccesslog</span></span></span><br><span class="line"><span class="class"><span class="title">-----------------------</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">ApacheAccesslog</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    ipAddress:<span class="type">String</span>, // ip地址</span></span></span><br><span class="line"><span class="class"><span class="params">    clientIndentd:<span class="type">String</span>, //标识符</span></span></span><br><span class="line"><span class="class"><span class="params">    userId:<span class="type">String</span> ,//用户<span class="type">ID</span></span></span></span><br><span class="line"><span class="class"><span class="params">    dateTime:<span class="type">String</span> ,//时间</span></span></span><br><span class="line"><span class="class"><span class="params">    method:<span class="type">String</span> ,//请求方式</span></span></span><br><span class="line"><span class="class"><span class="params">    endPoint:<span class="type">String</span> ,//目标地址</span></span></span><br><span class="line"><span class="class"><span class="params">    protocol:<span class="type">String</span> ,//协议</span></span></span><br><span class="line"><span class="class"><span class="params">    responseCode:<span class="type">Int</span> ,//网页请求响应类型</span></span></span><br><span class="line"><span class="class"><span class="params">    contenSize:<span class="type">Long</span>   //内容长度</span></span></span><br><span class="line"><span class="class"><span class="params">    </span></span></span><br><span class="line"><span class="class"><span class="params">    </span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">ApacheAccesslog</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">parseLog</span></span>(log:<span class="type">String</span>):<span class="type">ApacheAccesslog</span>=&#123;</span><br><span class="line">  <span class="keyword">val</span> logArray= log.split(<span class="string">"#"</span>);</span><br><span class="line">  <span class="keyword">val</span> url=logArray(<span class="number">4</span>).split(<span class="string">" "</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="type">ApacheAccesslog</span>(logArray(<span class="number">0</span>),logArray(<span class="number">1</span>),logArray(<span class="number">2</span>),logArray(<span class="number">3</span>),url(<span class="number">0</span>),url(<span class="number">1</span>),url(<span class="number">2</span>),logArray(<span class="number">5</span>).toInt,logArray(<span class="number">6</span>).toLong);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogAnalyer</span></span></span><br><span class="line"><span class="class"><span class="title">--------------------</span></span></span><br><span class="line"><span class="class"><span class="title">import</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">SparkConf</span></span></span><br><span class="line"><span class="class"><span class="title">import</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">SparkContext</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">LogAnalyer</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> conf=<span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"LogAnalyer"</span>)</span><br><span class="line">     <span class="keyword">val</span> sc=<span class="keyword">new</span> <span class="type">SparkContext</span>(conf);</span><br><span class="line">     <span class="keyword">val</span> logsRDD=sc.textFile(<span class="string">"file://...."</span>)</span><br><span class="line">          .map &#123; line =&gt; <span class="type">ApacheAccesslog</span>.parseLog(line) &#125;</span><br><span class="line">          .cache()</span><br><span class="line">     <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 需求一 </span></span><br><span class="line"><span class="comment">      * The average, min, and max content size of responses returned from the server.</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">     <span class="keyword">val</span> contextSize=logsRDD.map &#123; log =&gt; log.contenSize &#125;</span><br><span class="line">        <span class="comment">// get max</span></span><br><span class="line">      <span class="keyword">val</span> maxSize=contextSize.max()</span><br><span class="line">       <span class="comment">//get min</span></span><br><span class="line">      <span class="keyword">val</span> minSize=contextSize.min()</span><br><span class="line">      <span class="comment">// total / count</span></span><br><span class="line">      <span class="comment">//get average</span></span><br><span class="line">      <span class="keyword">val</span> averageSize=contextSize.reduce(_+_)/contextSize.count()</span><br><span class="line">      println(<span class="string">"=============================需求一-=============================="</span>);</span><br><span class="line">      println(<span class="string">"最大值："</span>+maxSize  + <span class="string">"  最小值："</span>+minSize + <span class="string">"   平均值："</span>+averageSize);</span><br><span class="line">     <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 需求二</span></span><br><span class="line"><span class="comment">      * A count of response code's returned.</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">        println(<span class="string">"=============================需求二-=============================="</span>);</span><br><span class="line">     logsRDD.map &#123; log =&gt; (log.responseCode,<span class="number">1</span>) &#125;</span><br><span class="line">     .reduceByKey(_+_)</span><br><span class="line">     .foreach(result =&gt; println(<span class="string">" 响应状态："</span>+result._1 + <span class="string">"  出现的次数："</span>+result._2))</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 需求三</span></span><br><span class="line"><span class="comment">      * All IPAddresses that have accessed this server more than N times.</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">      println(<span class="string">"=============================需求三-=============================="</span>);</span><br><span class="line">     <span class="keyword">val</span> result= logsRDD.map &#123; log =&gt;( log.ipAddress,<span class="number">1</span>) &#125;</span><br><span class="line">     .reduceByKey(_+_)</span><br><span class="line">     .filter(result =&gt; result._2 &gt; <span class="number">1</span>)   <span class="comment">// &gt; 10000 </span></span><br><span class="line">     .take(<span class="number">2</span>)  <span class="comment">//  &gt;  10</span></span><br><span class="line">     <span class="keyword">for</span>( tuple &lt;- result)&#123;</span><br><span class="line">       println(<span class="string">"ip : "</span>+tuple._1 + <span class="string">"  出现的次数："</span>+tuple._2);</span><br><span class="line">     &#125;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 需求四</span></span><br><span class="line"><span class="comment">      * The top endpoints requested by count.  TopN</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">      println(<span class="string">"=============================需求四-=============================="</span>);</span><br><span class="line">     <span class="keyword">val</span> topN=logsRDD.map &#123; log =&gt; (log.endPoint,<span class="number">1</span>) &#125;</span><br><span class="line">     .reduceByKey(_+_)</span><br><span class="line">     .map(result =&gt; (result._2,result._1))</span><br><span class="line">     .sortByKey(<span class="literal">false</span>)</span><br><span class="line">     .take(<span class="number">2</span>)</span><br><span class="line">     <span class="keyword">for</span>(tuple &lt;- topN)&#123;</span><br><span class="line">        println(<span class="string">"目标地址 : "</span>+tuple._2 + <span class="string">"  出现的次数："</span>+tuple._1);</span><br><span class="line">     &#125;</span><br><span class="line">     logsRDD.unpersist(<span class="literal">true</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="十五-线程池"><a href="#十五-线程池" class="headerlink" title="十五. 线程池"></a>十五. 线程池</h1><blockquote><p>3种线程池的构造方式见代码</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.concurrent.Executor;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutorService;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.Executors;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by zx on 2017/10/10.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ThreadPoolDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建一个单线程的线程池(池中只有一个线程)</span></span><br><span class="line">        <span class="comment">//ExecutorService pool = Executors.newSingleThreadExecutor();</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//固定大小的线程池(参数传几个就是几个)</span></span><br><span class="line">        <span class="comment">//ExecutorService pool = Executors.newFixedThreadPool(5);</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//可缓冲的线程词(可以有多个线程)</span></span><br><span class="line">        ExecutorService pool = Executors.newCachedThreadPool();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">20</span>; i ++) &#123;</span><br><span class="line"></span><br><span class="line">            pool.execute(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                    <span class="comment">//打印当前线程的名字</span></span><br><span class="line">                    System.out.println(Thread.currentThread().getName());</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        Thread.sleep(<span class="number">5000</span>);</span><br><span class="line">                    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                    System.out.println(Thread.currentThread().getName() + <span class="string">" is over"</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"all Task is submitted"</span>);</span><br><span class="line">        <span class="comment">//pool.shutdownNow();</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一-Spark-与-MapReduce-区别&quot;&gt;&lt;a href=&quot;#一-Spark-与-MapReduce-区别&quot; class=&quot;headerlink&quot; title=&quot;一. Spark 与 MapReduce 区别&quot;&gt;&lt;/a&gt;一. Spark 与 MapReduc
      
    
    </summary>
    
      <category term="Spark" scheme="https://airpoet.github.io/categories/Spark/"/>
    
      <category term="精讲" scheme="https://airpoet.github.io/categories/Spark/%E7%B2%BE%E8%AE%B2/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Spark" scheme="https://airpoet.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>i-Spark-5</title>
    <link href="https://airpoet.github.io/2018/07/25/Spark/i-Spark-5/"/>
    <id>https://airpoet.github.io/2018/07/25/Spark/i-Spark-5/</id>
    <published>2018-07-25T03:36:06.050Z</published>
    <updated>2018-08-05T08:58:04.461Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一-Spark-的闭包处理"><a href="#一-Spark-的闭包处理" class="headerlink" title="一. Spark 的闭包处理"></a>一. Spark 的闭包处理</h1><p><strong>RDD</strong>, <code>resilient distributed dataset</code>,弹性(容错)分布式数据集。</p><p>分区列表, function,dep Option(分区类, Pair[Key,Value]),首选位置。</p><p><strong>运行job时，spark将rdd打碎变换成task</strong>,<strong>每个task由一个executor执行</strong>。</p><p>执行之前，<strong>spark会进行task的闭包(closure)计算。</strong></p><p>闭包是指针对executor可见的变量和方法,以备在rdd的foreach中进行计算。</p><p><strong>闭包就是串行化后并发送给每个executor.</strong></p><p><strong>local模式下</strong>，所有spark程序运行在同一JVM中，共享对象，<strong>counter是可以累加的</strong>。</p><p>原因是所有executor指向的是同一个引用。</p><p><strong>cluster模式下，不可以，counter是闭包处理的。</strong></p><p>每个节点对driver上的counter是不可见的。</p><p>只能看到自己内部串行化的counter副本。</p><h1 id="二-Spark的应用的-部署模式"><a href="#二-Spark的应用的-部署模式" class="headerlink" title="二. Spark的应用的 部署模式"></a>二. Spark的应用的 部署模式</h1><h2 id="1-部署模式概述"><a href="#1-部署模式概述" class="headerlink" title="1. 部署模式概述"></a>1. 部署模式概述</h2><p><code>spark-submit --class xxx xx.jar --deploy-mode (client | cluster)</code></p><p>  <strong><code>--deploy-mode</code>指定部署driver程序在client主机上还是在worker节点上。</strong> </p><p><strong>[client]</strong></p><ul><li>driver运行在client主机上。</li><li>client可以不在cluster(集群)中。 </li></ul><p><strong>[cluster]</strong></p><ul><li><code>driver</code>程序提交给<code>spark cluster</code>的某个<code>worker</code>节点来执行。</li><li>worker是cluster中的一员。</li><li>导出的jar需要放置到所有worker节点都可见的位置(如<code>hdfs</code>)才可以。</li></ul><p><strong>不论哪种方式，rdd的运算都在worker执行</strong></p><h2 id="2-deploy-mode部署模式验证"><a href="#2-deploy-mode部署模式验证" class="headerlink" title="2. deploy mode部署模式验证"></a>2. deploy mode部署模式验证</h2><h4 id="部署模式划分"><a href="#部署模式划分" class="headerlink" title="部署模式划分"></a>部署模式划分</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 客户端 client 模式</span></span><br><span class="line">    $&gt;spark-submit --class com.rox.spark.scala.DeployModeTest --master spark://cs1:7077 --deploy-mode client SparkDemo-1-deploymode.jar</span><br><span class="line">    结论: driver就是自己, 如果在cs1上, driver 就是cs1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 上传jar到hdfs。</span></span><br><span class="line"><span class="comment"># 集群 cluster 模式</span></span><br><span class="line">    $&gt;spark-submit --class com.it18zhang.spark.scala.DeployModeTest --master spark://cs1:7077 --deploy-mode cluster hdfs://cs1:8020/user/centos/SparkDemo-1-deploymode.jar</span><br><span class="line">    结论: driver是由 spark 挑选的,如果是 cs1上提交的,因为 cs1也不是 worker, 所以 driver和 worker 都跟cs1无关, 注意, cluster 模式,只要一提交完就结束了, 因为接下来跟他自己没关系了</span><br></pre></td></tr></table></figure><h4 id="小技巧-导出-maven-中所有的依赖库到文件中"><a href="#小技巧-导出-maven-中所有的依赖库到文件中" class="headerlink" title="小技巧: 导出 maven 中所有的依赖库到文件中"></a>小技巧: 导出 maven 中所有的依赖库到文件中</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># maven 命令,下载指定job 的所有依赖的 jar 包到 当前的 ./lib 文件夹</span></span><br><span class="line">mvn -DoutputDirectory=./lib -DgroupId=com.rox -DartifactId=SparkDemo1 -Dversion=1.0-SNAPSHOT dependency:copy-dependencies</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把当前目录下的所有文件名写入到后面的文件中, 文件之间默认已空格隔开</span></span><br><span class="line">ls | xargs &gt; a.txt</span><br></pre></td></tr></table></figure><h4 id="配置spark-on-yarn执行模式"><a href="#配置spark-on-yarn执行模式" class="headerlink" title="配置spark on yarn执行模式"></a>配置spark on yarn执行模式</h4><ul><li>使用 spark on yarn 不需要启动 spark!!  job直接执行在 yarn 上</li><li>yarn 模式使用 yarn模式来作为 master, 而 yarn 已经配置了 HA 了</li></ul><p><strong>配置 jars 文件的目录</strong></p><ol><li><strong>将spark的jars文件放到hdfs上.</strong><br>  $&gt;hdfs dfs -mkdir -p /user/ap/spark/jars<br>  [ap@cs1]~/apps/spark%&gt;  hdfs dfs -put jars/ /user/ap/spark</li><li><strong>配置spark属性文件</strong><br> [/spark/conf/spark-default.conf]<br> spark.yarn.jars hdfs://mycluster/user/ap/spark/jars/*</li></ol><blockquote><p> 配置完这些之后: 提交任务的时候就会出现这个, 意思是, 找到了 jar 包, 就不复制了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; INFO Client: Source and destination file systems are the same. Not copying hdfs://mycluster/user/ap/spark/jars/JavaEWAH-0.3.2.jar</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><h4 id="遇到问题-内存不够"><a href="#遇到问题-内存不够" class="headerlink" title="遇到问题: 内存不够"></a>遇到问题: 内存不够</h4><blockquote><p>解决: 配置 <code>yarn-site.xml</code></p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">        Whether virtual memory limits will be enforced for containers</span><br><span class="line">                是否会对容器执行虚拟内存限制</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">        Ratio between virtua4l memory to physical memory when setting memory limits for containers</span><br><span class="line">                设置容器的内存限制时虚拟内存与物理内存的比率</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="配置机架感知"><a href="#配置机架感知" class="headerlink" title="配置机架感知"></a>配置机架感知</h4><blockquote><p>核心是自己实现一个类, 然后再 配置文件中声明 机架感知的实现类是哪个</p><p>具体使用自行查资料</p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># [core-site.xml]</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>topology.node.switch.mapping.impl<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.rox.hdfs.rackaware.MyRackAware<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.hdfs.rackaware.MyRackAware</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.net.DNSToSwitchMapping;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.FileWriter;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *机架感知类</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyRackAware</span> <span class="keyword">implements</span> <span class="title">DNSToSwitchMapping</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;String&gt; <span class="title">resolve</span><span class="params">(List&lt;String&gt; names)</span> </span>&#123;</span><br><span class="line">        List&lt;String&gt; list =  <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            FileWriter fw = <span class="keyword">new</span> FileWriter(<span class="string">"/home/centos/rackaware.txt"</span>,<span class="keyword">true</span>);</span><br><span class="line">            <span class="keyword">for</span>(String str : names)&#123;</span><br><span class="line">                <span class="comment">//输出原来的信息,ip地址(主机名)</span></span><br><span class="line">                fw.write(str + <span class="string">"\r\n"</span>);</span><br><span class="line">                <span class="comment">//</span></span><br><span class="line">                <span class="keyword">if</span>(str.startsWith(<span class="string">"192"</span>))&#123;</span><br><span class="line">                    <span class="comment">//192.168.231.202</span></span><br><span class="line">                    String ip = str.substring(str.lastIndexOf(<span class="string">"."</span>) + <span class="number">1</span>);</span><br><span class="line">                    <span class="keyword">if</span>(Integer.parseInt(ip) &lt;= <span class="number">203</span>) &#123;</span><br><span class="line">                        list.add(<span class="string">"/rack1/"</span> + ip);</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">else</span>&#123;</span><br><span class="line">                        list.add(<span class="string">"/rack2/"</span> + ip);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(str.startsWith(<span class="string">"s"</span>))&#123;</span><br><span class="line">                    String ip = str.substring(<span class="number">1</span>);</span><br><span class="line">                    <span class="keyword">if</span> (Integer.parseInt(ip) &lt;= <span class="number">203</span>) &#123;</span><br><span class="line">                        list.add(<span class="string">"/rack1/"</span> + ip);</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        list.add(<span class="string">"/rack2/"</span> + ip);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            fw.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> list;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reloadCachedMappings</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reloadCachedMappings</span><span class="params">(List&lt;String&gt; names)</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="代码解释"><a href="#代码解释" class="headerlink" title="代码解释"></a><strong>代码解释</strong></h4><ul><li>通过Socket把打印的数据, 发送到指定机器<code>cs5</code>上</li><li>在<code>cs5</code>上, 开启 <code>nc</code>端口, 监听消息</li><li><code>standalone</code> 和 <code>yarn</code>模式分别打成 <code>jar包</code>,  在spark 环境中执行</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.spark.scala</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.net.&#123;<span class="type">InetAddress</span>, <span class="type">Socket</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DeployModeTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * write写出去</span></span><br><span class="line"><span class="comment">    * 从运行 此程序的节点, 写到 cs5 上</span></span><br><span class="line"><span class="comment">    * @param str</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">printInfo</span></span>(str:<span class="type">String</span>):<span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ip = <span class="type">InetAddress</span>.getLocalHost.getHostAddress</span><br><span class="line">    <span class="keyword">val</span> sock = <span class="keyword">new</span> <span class="type">Socket</span>(<span class="string">"cs5"</span>,<span class="number">8888</span>)</span><br><span class="line">    <span class="keyword">val</span> out = sock.getOutputStream</span><br><span class="line">    out.write((ip + <span class="string">""</span> + str + <span class="string">"\r\n"</span>).getBytes())</span><br><span class="line">    out.flush()</span><br><span class="line">    sock.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">"DeployModeTest"</span>)</span><br><span class="line">    <span class="comment">// standalone 模式</span></span><br><span class="line">    <span class="comment">//conf.setMaster("spark://cs1:7077")</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Yarn模式</span></span><br><span class="line">    conf.setMaster(<span class="string">"yarn"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// local 模式</span></span><br><span class="line">    <span class="comment">// conf.setMaster("local[4]")</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    printInfo(<span class="string">"hello guys---我就是Driver"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * Distribute a local Scala collection to form an RDD.</span></span><br><span class="line"><span class="comment">      * numSlices: the partition number of the new RDD.</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>,  <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印第一次map</span></span><br><span class="line">    <span class="keyword">val</span> rdd2 = rdd1.map(e =&gt; &#123;</span><br><span class="line">      printInfo(<span class="string">"直接定义3个分区, map1: "</span>+e)</span><br><span class="line">      e * <span class="number">2</span></span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 重新分区为 2</span></span><br><span class="line">    <span class="keyword">val</span> rdd3 = rdd2.repartition(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="keyword">val</span> rdd4 = rdd3.map(e =&gt; &#123;</span><br><span class="line">      printInfo(<span class="string">"重分区为2后, map2: "</span> + e)</span><br><span class="line">      e</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> res = rdd4.reduce((a,b)=&gt;&#123;</span><br><span class="line">      printInfo(<span class="string">"求和, reduce: "</span> + a + <span class="string">","</span> + b)</span><br><span class="line">      a + b</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    println(res)</span><br><span class="line">    printInfo(<span class="string">"最后发给driver: "</span> + res)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="三-Spark-集群的运行方式"><a href="#三-Spark-集群的运行方式" class="headerlink" title="三. Spark 集群的运行方式"></a>三. Spark 集群的运行方式</h1><p><strong>主要是<code>cluster manager</code>的区别。</strong> </p><p><strong>[local]</strong> </p><ul><li>…</li></ul><p><strong>[standalone]</strong> </p><ul><li>使用SparkMaster进程作为管理节点, 需要开启Spark 集群</li></ul><p><strong>[mesos]</strong> </p><ul><li>使用mesos的master作为管理节点。 </li></ul><p><strong>[yarn]</strong> </p><ul><li><p>使用hadoop的ResourceManager作为master节点。</p></li><li><p>不用开启 spark 集群。因为是依托<code>yarn 集群</code>来执行<code>spark</code> application,  高可用也是依托于 yarn 的高可用, 程序在执行时, 会拷贝 一些spark 依赖环境到 hdfs 上, 事后会删除</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一-Spark-的闭包处理&quot;&gt;&lt;a href=&quot;#一-Spark-的闭包处理&quot; class=&quot;headerlink&quot; title=&quot;一. Spark 的闭包处理&quot;&gt;&lt;/a&gt;一. Spark 的闭包处理&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;RDD&lt;/strong&gt;, &lt;c
      
    
    </summary>
    
      <category term="Spark" scheme="https://airpoet.github.io/categories/Spark/"/>
    
      <category term="部署模式" scheme="https://airpoet.github.io/categories/Spark/%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Spark" scheme="https://airpoet.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Python &amp; Hadoop | Spark 生态</title>
    <link href="https://airpoet.github.io/2018/07/23/Spark/Python%20&amp;%20Hadoop%7CSpark%E7%94%9F%E6%80%81/"/>
    <id>https://airpoet.github.io/2018/07/23/Spark/Python &amp; Hadoop|Spark生态/</id>
    <published>2018-07-23T06:27:10.923Z</published>
    <updated>2018-07-23T08:06:55.514Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一-Python-访问-MySQL"><a href="#一-Python-访问-MySQL" class="headerlink" title="一. Python 访问 MySQL"></a>一. Python 访问 MySQL</h1><h2 id="1-安装-pymysql-模块"><a href="#1-安装-pymysql-模块" class="headerlink" title="1.安装 pymysql 模块"></a>1.安装 pymysql 模块</h2><p>1) <code>idea</code>中,  <code>import pymysql</code>, 没有安装的话, <code>option + return</code> 安装</p><h2 id="2-访问-mysql-测试"><a href="#2-访问-mysql-测试" class="headerlink" title="2. 访问 mysql 测试"></a>2. 访问 mysql 测试</h2><p>看看能否打印 <code>mysql</code> 的版本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python3</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8-*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="comment"># 开启链接</span></span><br><span class="line">    conn = pymysql.connect(host=<span class="string">'localhost'</span>,user=<span class="string">'root'</span>,passwd=<span class="string">'123'</span>,</span><br><span class="line">                           db=<span class="string">'python'</span>,port=<span class="number">3306</span>,charset=<span class="string">'utf8'</span>)</span><br><span class="line">    <span class="comment"># 打开游标</span></span><br><span class="line">    cur = conn.cursor()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 执行 sql</span></span><br><span class="line">    cur.execute(<span class="string">'select version()'</span>)</span><br><span class="line"></span><br><span class="line">    version = cur.fetchone()</span><br><span class="line"></span><br><span class="line">    print(version)</span><br><span class="line">    cur.close()</span><br><span class="line">    conn.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">except</span> Exception:</span><br><span class="line">    print(<span class="string">"发生异常"</span>)</span><br></pre></td></tr></table></figure><h2 id="3-查询-mysql"><a href="#3-查询-mysql" class="headerlink" title="3. 查询 mysql"></a>3. 查询 mysql</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python3</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8-*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="comment"># 开启链接</span></span><br><span class="line">    conn = pymysql.connect(host=<span class="string">'localhost'</span>,user=<span class="string">'root'</span>,passwd=<span class="string">'123'</span>,</span><br><span class="line">                           db=<span class="string">'python'</span>,port=<span class="number">3306</span>,charset=<span class="string">'utf8'</span>)</span><br><span class="line">    <span class="comment"># 打开游标</span></span><br><span class="line">    cur = conn.cursor()</span><br><span class="line"></span><br><span class="line">    sql = <span class="string">"select id, name,age from t1 where name like 'tom8%'"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 执行 sql</span></span><br><span class="line">    cur.execute(sql)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 取结果</span></span><br><span class="line">    all = cur.fetchall()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> rec <span class="keyword">in</span> all:</span><br><span class="line">        print(rec)</span><br><span class="line">        <span class="comment"># print(str(rec[0]))</span></span><br><span class="line"></span><br><span class="line">    conn.commit()</span><br><span class="line">    cur.close()</span><br><span class="line">    conn.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">except</span> Exception:</span><br><span class="line">    print(<span class="string">"发生异常"</span>)</span><br></pre></td></tr></table></figure><h2 id="4-大批量插入-mysql"><a href="#4-大批量插入-mysql" class="headerlink" title="4. 大批量插入 mysql"></a>4. 大批量插入 mysql</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python3</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8-*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="comment"># 开启链接</span></span><br><span class="line">    conn = pymysql.connect(host=<span class="string">'localhost'</span>,user=<span class="string">'root'</span>,passwd=<span class="string">'123'</span>,</span><br><span class="line">                           db=<span class="string">'python'</span>,port=<span class="number">3306</span>,charset=<span class="string">'utf8'</span>)</span><br><span class="line">    <span class="comment"># 打开游标</span></span><br><span class="line">    cur = conn.cursor()</span><br><span class="line"></span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; <span class="number">10000</span>:</span><br><span class="line">        sql = <span class="string">"insert into t1(name,age) VALUES ('%s','%d')"</span> % (<span class="string">'tom'</span> + str(i), i % <span class="number">100</span>)</span><br><span class="line">        print(sql)</span><br><span class="line">        cur.execute(sql)</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    conn.commit()</span><br><span class="line">    cur.close()</span><br><span class="line">    conn.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">except</span> Exception:</span><br><span class="line">    print(<span class="string">"发生异常"</span>)</span><br></pre></td></tr></table></figure><h2 id="5-执行事务"><a href="#5-执行事务" class="headerlink" title="5. 执行事务"></a>5. 执行事务</h2><p>关闭 autocommit</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python3</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8-*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="comment"># 开启连接</span></span><br><span class="line">    conn = pymysql.connect(host=<span class="string">'localhost'</span>,user=<span class="string">'root'</span>,passwd=<span class="string">'123'</span>,db=<span class="string">'python'</span>,port=<span class="number">3306</span>,charset=<span class="string">'utf8'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 关闭自动提交</span></span><br><span class="line">    conn.autocommit(<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#开启事务</span></span><br><span class="line">    conn.begin()</span><br><span class="line">    <span class="comment"># 打开游标</span></span><br><span class="line">    cur = conn.cursor()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 删除</span></span><br><span class="line">    sql = <span class="string">"delete from t1 WHERE id &gt; 20000"</span></span><br><span class="line">    <span class="comment"># 改</span></span><br><span class="line">    sql = <span class="string">"update t1 set age = age -1 where age &gt;=50 "</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 聚合</span></span><br><span class="line">    sql = <span class="string">"select count(*) from t1 where age &lt; 20"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 执行 sql</span></span><br><span class="line">    cur.execute(sql)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 有结果的, 执行完后需要fetch结果</span></span><br><span class="line">    res = cur.fetchone()</span><br><span class="line">    print(res[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提交连接</span></span><br><span class="line">    conn.commit()</span><br><span class="line">    <span class="comment"># 关闭游标</span></span><br><span class="line">    cur.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">except</span> Exception:</span><br><span class="line">    print(<span class="string">"发生异常"</span>)</span><br><span class="line">    conn.rollback()</span><br><span class="line"></span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    conn.close()</span><br></pre></td></tr></table></figure><h1 id="二-Spark-环境使用-Python-操作-HBase"><a href="#二-Spark-环境使用-Python-操作-HBase" class="headerlink" title="二. Spark 环境使用 Python 操作 HBase"></a>二. Spark 环境使用 Python 操作 HBase</h1><h2 id="1-环境准备"><a href="#1-环境准备" class="headerlink" title="1. 环境准备"></a>1. 环境准备</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">0.启动hbase集群</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line">如果时钟不同步。</span><br><span class="line">$&gt;su root</span><br><span class="line">$&gt;xcall.sh <span class="string">"ntpdate asia.pool.ntp.org"</span></span><br><span class="line">当然, 也可以设置脚本自动同步, 详细见我的 hadoopHA 环境搭建</span><br><span class="line"></span><br><span class="line">1.在 HBase 目录下,启动hbase的thriftserver，满足和第三方应用通信。</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line">$&gt;hbase-daemon.sh start thrift2</span><br><span class="line"></span><br><span class="line">2.查看WebUI</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line">/<span class="comment"># webui端口 , 9090 rpc端口</span></span><br><span class="line">http://cs1:9095/        </span><br><span class="line"></span><br><span class="line">3.安装mac下thrift的编译器</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line">brew install thrift</span><br><span class="line"></span><br><span class="line">4.下载并安装thrift的python模块.</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line">import thrift  ==&gt;没有的话, option + <span class="built_in">return</span> 安装</span><br><span class="line"></span><br><span class="line">5.找到hbase.thrift文件进行编译，产生python文件。</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line">使用以下命令进行编译</span><br><span class="line">cmd&gt; thrift  -o ./out -gen py hbase.thrift</span><br><span class="line">生成后的路径在这里: /Users/shixuanji/Documents/资源/Jar包/HBase/out/gen-py</span><br><span class="line"></span><br><span class="line">6.创建idea的下的新模块</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">7.创建python文件Demo1.py</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">8.复制生成python文件到idea下。</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line">mythrift/hbase/..</span><br><span class="line"></span><br><span class="line">9.控制台环境测试</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line">    移除spark/conf/core-site.xml | hdfs-site.xml | hive-site.xml文件</span><br><span class="line">    [scala] ~/apps/spark/bin</span><br><span class="line">    &lt;spark-shell&gt;</span><br><span class="line">    val rdd = sc.makeRDD(1 to 10)</span><br><span class="line">    rdd.map(e=&gt;(e,1))</span><br><span class="line"></span><br><span class="line">    [python] ~/apps/spark/bin</span><br><span class="line">    &lt;pyspark&gt;</span><br><span class="line">    arr = [1,2,3,4]</span><br><span class="line">    rdd = sc.parallelize(arr);</span><br><span class="line">    rdd.map(lambda e : (e,1))</span><br></pre></td></tr></table></figure><h2 id="2-具体代码"><a href="#2-具体代码" class="headerlink" title="2.具体代码"></a>2.具体代码</h2><h4 id="2-1-对-hbase-的增删改查"><a href="#2-1-对-hbase-的增删改查" class="headerlink" title="2.1 对 hbase 的增删改查"></a>2.1 对 hbase 的增删改查</h4><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/HBasePythonDemo/BasicPyHbase.py" target="_blank" rel="noopener">https://github.com/airpoet/bigdata/blob/master/Spark_Project/HBasePythonDemo/BasicPyHbase.py</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- encoding=utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment">#导入thrift的python模块</span></span><br><span class="line"><span class="keyword">from</span> thrift <span class="keyword">import</span> Thrift</span><br><span class="line"><span class="keyword">from</span> thrift.transport <span class="keyword">import</span> TSocket</span><br><span class="line"><span class="keyword">from</span> thrift.transport <span class="keyword">import</span> TTransport</span><br><span class="line"><span class="keyword">from</span> thrift.protocol <span class="keyword">import</span> TBinaryProtocol</span><br><span class="line"></span><br><span class="line"><span class="comment">#导入自已编译生成的hbase python模块</span></span><br><span class="line"><span class="keyword">from</span> mythrift.hbase <span class="keyword">import</span> THBaseService</span><br><span class="line"><span class="keyword">from</span> mythrift.hbase.ttypes <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> mythrift.hbase.ttypes <span class="keyword">import</span> TResult</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#创建Socket连接，到s201:9090</span></span><br><span class="line">transport = TSocket.TSocket(<span class="string">'cs1'</span>, <span class="number">9090</span>)</span><br><span class="line">transport = TTransport.TBufferedTransport(transport)</span><br><span class="line">protocol = TBinaryProtocol.TBinaryProtocol(transport)</span><br><span class="line">client = THBaseService.Client(protocol)</span><br><span class="line"></span><br><span class="line"><span class="comment">#打开传输端口!!!</span></span><br><span class="line">transport.open()</span><br><span class="line"></span><br><span class="line"><span class="comment"># put操作</span></span><br><span class="line">table = <span class="string">b'ns1:t1'</span></span><br><span class="line">row = <span class="string">b'row1'</span></span><br><span class="line">v1 = TColumnValue(<span class="string">b'f1'</span>, <span class="string">b'id'</span>, <span class="string">b'101'</span>)</span><br><span class="line">v2 = TColumnValue(<span class="string">b'f1'</span>, <span class="string">b'name'</span>, <span class="string">b'tomas'</span>)</span><br><span class="line">v3 = TColumnValue(<span class="string">b'f1'</span>, <span class="string">b'age'</span>, <span class="string">b'12'</span>)</span><br><span class="line">vals = [v1, v2, v3]</span><br><span class="line">put = TPut(row, vals)</span><br><span class="line">client.put(table, put)</span><br><span class="line">print(<span class="string">"okkkk!!"</span>)</span><br><span class="line">transport.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#get</span></span><br><span class="line">table = <span class="string">b'ns1:t1'</span></span><br><span class="line">rowkey=<span class="string">b"row1"</span></span><br><span class="line">col_id = TColumn(<span class="string">b"f1"</span>,<span class="string">b"id"</span>)</span><br><span class="line">col_name = TColumn(<span class="string">b"f1"</span>,<span class="string">b"name"</span>)</span><br><span class="line">col_age = TColumn(<span class="string">b"f1"</span>,<span class="string">b"age"</span>)</span><br><span class="line"></span><br><span class="line">cols = [col_id,col_name,col_age]</span><br><span class="line">get = TGet(rowkey,cols)</span><br><span class="line">res = client.get(table,get)</span><br><span class="line">print(bytes.decode(res.columnValues[<span class="number">0</span>].qualifier))</span><br><span class="line">print(bytes.decode(res.columnValues[<span class="number">0</span>].family))</span><br><span class="line">print(res.columnValues[<span class="number">0</span>].timestamp)</span><br><span class="line">print(bytes.decode(res.columnValues[<span class="number">0</span>].value))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#delete</span></span><br><span class="line">table = <span class="string">b'ns1:t1'</span></span><br><span class="line">rowkey = <span class="string">b"row1"</span></span><br><span class="line">col_id = TColumn(<span class="string">b"f1"</span>, <span class="string">b"id"</span>)</span><br><span class="line">col_name = TColumn(<span class="string">b"f1"</span>, <span class="string">b"name"</span>)</span><br><span class="line">col_age = TColumn(<span class="string">b"f1"</span>, <span class="string">b"age"</span>)</span><br><span class="line">cols = [col_id, col_name]</span><br><span class="line"></span><br><span class="line"><span class="comment">#构造删除对象</span></span><br><span class="line">delete = TDelete(rowkey,cols)</span><br><span class="line">res = client.deleteSingle(table, delete)</span><br><span class="line">transport.close()</span><br><span class="line">print(<span class="string">"ok"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Scan</span></span><br><span class="line">table = <span class="string">b'ns1:t12'</span></span><br><span class="line">startRow = <span class="string">b'1530357094900-43dwMLjxI5-0'</span></span><br><span class="line">stopRow = <span class="string">b'1530357183537-43dwMLjxI5-6'</span></span><br><span class="line">payload = TColumn(<span class="string">b"f1"</span>, <span class="string">b"payload"</span>)</span><br><span class="line"></span><br><span class="line">cols = [payload]</span><br><span class="line"></span><br><span class="line">scan = TScan(startRow=startRow,stopRow=stopRow,columns=cols)</span><br><span class="line"><span class="comment"># 这里如果不传 stopRow 就是扫描到结尾</span></span><br><span class="line">scan = TScan(startRow=startRow, columns=cols)</span><br><span class="line">r = client.getScannerResults(table,scan,<span class="number">100</span>);</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> r:</span><br><span class="line">    print(<span class="string">"============"</span>)</span><br><span class="line">    print(bytes.decode(x.columnValues[<span class="number">0</span>].qualifier))</span><br><span class="line">    print(bytes.decode(x.columnValues[<span class="number">0</span>].family))</span><br><span class="line">    print(x.columnValues[<span class="number">0</span>].timestamp)</span><br><span class="line">    print(bytes.decode(x.columnValues[<span class="number">0</span>].value))</span><br></pre></td></tr></table></figure><h4 id="2-2-将爬虫爬取的网页存入-hbase"><a href="#2-2-将爬虫爬取的网页存入-hbase" class="headerlink" title="2.2 将爬虫爬取的网页存入 hbase"></a>2.2 将爬虫爬取的网页存入 hbase</h4><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/HBasePythonDemo/Crawler2HBase.py" target="_blank" rel="noopener">https://github.com/airpoet/bigdata/blob/master/Spark_Project/HBasePythonDemo/Crawler2HBase.py</a></p><blockquote><p>   CrawerPageDao.py  </p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python3</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8-*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入thrift的python模块</span></span><br><span class="line"><span class="keyword">from</span> thrift <span class="keyword">import</span> Thrift</span><br><span class="line"><span class="keyword">from</span> thrift.transport <span class="keyword">import</span> TSocket</span><br><span class="line"><span class="keyword">from</span> thrift.transport <span class="keyword">import</span> TTransport</span><br><span class="line"><span class="keyword">from</span> thrift.protocol <span class="keyword">import</span> TBinaryProtocol</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入自已编译生成的hbase python模块</span></span><br><span class="line"><span class="keyword">from</span> mythrift.hbase <span class="keyword">import</span> THBaseService</span><br><span class="line"><span class="keyword">from</span> mythrift.hbase.ttypes <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> mythrift.hbase.ttypes <span class="keyword">import</span> TResult</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> base64</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">创建Socket连接，到s201:9090</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">transport = TSocket.TSocket(<span class="string">'cs1'</span>, <span class="number">9090</span>)</span><br><span class="line">transport = TTransport.TBufferedTransport(transport)</span><br><span class="line">protocol = TBinaryProtocol.TBinaryProtocol(transport)</span><br><span class="line">client = THBaseService.Client(protocol)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">定义函数，保存网页</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">savePage</span><span class="params">(url,page)</span>:</span></span><br><span class="line">    <span class="comment">#开启连接</span></span><br><span class="line">    transport.open()</span><br><span class="line">    <span class="comment">#对url进行base64编码，形成bytes,作为rowkey</span></span><br><span class="line">    urlBase64Bytes = base64.encodebytes(url.encode(<span class="string">"utf-8"</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># put操作</span></span><br><span class="line">    table = <span class="string">b'ns1:pages'</span></span><br><span class="line">    rowkey = urlBase64Bytes</span><br><span class="line">    v1 = TColumnValue(<span class="string">b'f1'</span>, <span class="string">b'page'</span>, page)</span><br><span class="line">    vals = [v1]</span><br><span class="line">    put = TPut(rowkey, vals)</span><br><span class="line">    client.put(table, put)</span><br><span class="line">    transport.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">判断网页是否存在</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exists</span><span class="params">(url)</span>:</span></span><br><span class="line">    transport.open()</span><br><span class="line">    <span class="comment"># 对url进行base64编码，形成bytes,作为rowkey</span></span><br><span class="line">    urlBase64Bytes = base64.encodebytes(url.encode(<span class="string">"utf-8"</span>))</span><br><span class="line">    print(urlBase64Bytes)</span><br><span class="line"></span><br><span class="line">    table = <span class="string">b'ns1:pages'</span></span><br><span class="line">    rowkey = urlBase64Bytes</span><br><span class="line">    col_page = TColumn(<span class="string">b"f1"</span>,<span class="string">b"page"</span>)</span><br><span class="line"></span><br><span class="line">    cols = [col_page]</span><br><span class="line">    get = TGet(rowkey,cols)</span><br><span class="line">    res = client.get(table, get)</span><br><span class="line">    transport.close()</span><br><span class="line">    <span class="keyword">return</span> res.row <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span></span><br></pre></td></tr></table></figure><blockquote><p>  Crawler2HBase.py  </p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python3</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8-*-</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 首先创建 hbase 表: pages</span></span><br><span class="line"><span class="comment"># $hbase&gt; create 'ns1:pages','f1'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> CrawerPageDao</span><br><span class="line"></span><br><span class="line"><span class="comment">#下载网页方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="comment">#判断当前的网页是否已经下载</span></span><br><span class="line">    resp = urllib.request.urlopen(url)</span><br><span class="line">    pageBytes = resp.read()</span><br><span class="line">    resp.close</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> CrawerPageDao.exists(url):</span><br><span class="line">        CrawerPageDao.savePage(url, pageBytes);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment">#解析网页的内容</span></span><br><span class="line">        pageStr = pageBytes.decode(<span class="string">"utf-8"</span>);</span><br><span class="line">        <span class="comment">#解析href地址</span></span><br><span class="line">        pattern = <span class="string">u'&lt;a[\u0000-\uffff&amp;&amp;&lt;sup&gt;[href]]*href="([\u0000-\uffff&amp;&amp;&lt;/sup&gt;"]*?)"'</span></span><br><span class="line">        res = re.finditer(pattern, pageStr)</span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> res:</span><br><span class="line">            addr = r.group(<span class="number">1</span>);</span><br><span class="line">            print(addr)</span><br><span class="line">            <span class="keyword">if</span> addr.startswith(<span class="string">"//"</span>):</span><br><span class="line">                addr = addr.replace(<span class="string">"//"</span>,<span class="string">"http://"</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">#判断网页中是否包含自己的地址</span></span><br><span class="line">            <span class="keyword">if</span> addr.startswith(<span class="string">"http://"</span>) <span class="keyword">and</span> url != addr <span class="keyword">and</span> (<span class="keyword">not</span> CrawerPageDao.exists(addr)):</span><br><span class="line">                download(addr) ;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        print(e)</span><br><span class="line">        print(pageBytes.decode(<span class="string">"gbk"</span>, errors=<span class="string">'ignore'</span>));</span><br><span class="line">        <span class="keyword">return</span> ;</span><br><span class="line"></span><br><span class="line">download(<span class="string">"http://jd.com"</span>);</span><br></pre></td></tr></table></figure><hr><h1 id="三-使用python实现spark的数据分析"><a href="#三-使用python实现spark的数据分析" class="headerlink" title="三. 使用python实现spark的数据分析"></a>三. 使用python实现spark的数据分析</h1><p><strong>参考这本书, data 等都有下载地址</strong> </p><blockquote><p>Apache Spark 2 for Beginners</p></blockquote><h2 id="1-环境准备-1"><a href="#1-环境准备-1" class="headerlink" title="1.环境准备"></a>1.环境准备</h2><p><strong>首先当前python环境必须安装了这些组件, 由于我的mac上已经装了, 这里就不再装了</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1.numpy</span><br><span class="line">    cmd&gt;pip install -i https://pypi.tuna.tsinghua.edu.cn/simple numpy</span><br><span class="line"></span><br><span class="line">2.scipy</span><br><span class="line">    pip install -i https://pypi.tuna.tsinghua.edu.cn/simple scipy</span><br><span class="line"></span><br><span class="line">3.matplotpy</span><br><span class="line">    pip install -i https://pypi.tuna.tsinghua.edu.cn/simple matplotlib</span><br><span class="line">    python -m pip install -U pip setuptools</span><br><span class="line">    python -m pip install matplotlib</span><br></pre></td></tr></table></figure><h2 id="2-在-mac-环境的-Spark-下"><a href="#2-在-mac-环境的-Spark-下" class="headerlink" title="2.在 mac 环境的 Spark 下"></a>2.在 mac 环境的 Spark 下</h2><p>也可以在 Linux 下的图形界面中通过 terminal 操作.</p><p><strong>目录建议不要有中文,  否则会有一些警告甚至错误</strong></p><p><strong>我的目录在这里:</strong> <code>/Users/shixuanji/Documents/资源/Jar包/Spark/spark-2.1.3-bin-hadoop2.7/bin/pyspark</code></p><p>进入我的 <code>iTerm2</code>, 进入<code>pyspark</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">#导入sql</span></span><br><span class="line">    <span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line">    <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">    <span class="keyword">import</span> pylab <span class="keyword">as</span> P</span><br><span class="line">    plt.rcdefaults()        <span class="comment"># Restore the rc params from Matplotlib's internal defaults</span></span><br><span class="line">    dataDir =<span class="string">"file:///home/centos/ml-data/ml-1m/users.dat"</span></span><br><span class="line">    lines = sc.textFile(dataDir)</span><br><span class="line">    splitLines = lines.map(<span class="keyword">lambda</span> l: l.split(<span class="string">"::"</span>))</span><br><span class="line">    usersRDD = splitLines.map(<span class="keyword">lambda</span> p: Row(id=p[<span class="number">0</span>],gender=p[<span class="number">1</span>],age=int(p[<span class="number">2</span>]), occupation=p[<span class="number">3</span>], zipcode=p[<span class="number">4</span>]))</span><br><span class="line">    usersDF = spark.createDataFrame(usersRDD)</span><br><span class="line">    usersDF.createOrReplaceTempView(<span class="string">"users"</span>)</span><br><span class="line">    usersDF.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#生成直方图</span></span><br><span class="line">    ageDF = spark.sql(<span class="string">"SELECT age FROM users"</span>)</span><br><span class="line">    ageList = ageDF.rdd.map(<span class="keyword">lambda</span> p: p.age).collect()</span><br><span class="line">    ageDF.describe().show()</span><br><span class="line"></span><br><span class="line">    plt.hist(ageList)</span><br><span class="line">    plt.title(<span class="string">"Age distribution of the users\n"</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"Age"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"Number of users"</span>)</span><br><span class="line">    plt.show(block=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#密度图</span></span><br><span class="line">    <span class="keyword">from</span> scipy.stats <span class="keyword">import</span> gaussian_kde</span><br><span class="line">    density = gaussian_kde(ageList)</span><br><span class="line">    xAxisValues = np.linspace(<span class="number">0</span>,<span class="number">100</span>,<span class="number">1000</span>)</span><br><span class="line">    density.covariance_factor = <span class="keyword">lambda</span> : <span class="number">.5</span></span><br><span class="line">    density._compute_covariance()</span><br><span class="line">    plt.title(<span class="string">"Age density plot of the users\n"</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"Age"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"Density"</span>)</span><br><span class="line">    plt.plot(xAxisValues, density(xAxisValues))</span><br><span class="line">    plt.show(block=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成嵌套子图</span></span><br><span class="line">    plt.subplot(<span class="number">121</span>)</span><br><span class="line">    plt.hist(ageList)</span><br><span class="line">    plt.title(<span class="string">"Age distribution of the users\n"</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"Age"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"Number of users"</span>)</span><br><span class="line">    plt.subplot(<span class="number">122</span>)</span><br><span class="line">    plt.title(<span class="string">"Summary of distribution\n"</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"Age"</span>)</span><br><span class="line">    plt.boxplot(ageList, vert=<span class="keyword">False</span>)</span><br><span class="line">    plt.show(block=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#柱状图</span></span><br><span class="line">    occ10 = spark.sql(<span class="string">"SELECT occupation, count(occupation) as usercount FROM users GROUP BY occupation ORDER BY usercount DESC LIMIT 10"</span>)</span><br><span class="line">    occ10.show()</span><br><span class="line"></span><br><span class="line">    occTuple = occ10.rdd.map(<span class="keyword">lambda</span> p:(p.occupation,p.usercount)).collect()</span><br><span class="line">    occList, countList = zip(*occTuple)</span><br><span class="line">    occList</span><br><span class="line"></span><br><span class="line">    y_pos = np.arange(len(occList))</span><br><span class="line">    plt.barh(y_pos, countList, align=<span class="string">'center'</span>, alpha=<span class="number">0.4</span>)</span><br><span class="line">    plt.yticks(y_pos, occList)</span><br><span class="line">    plt.xlabel(<span class="string">'Number of users'</span>)</span><br><span class="line">    plt.title(<span class="string">'Top 10 user types\n'</span>)</span><br><span class="line">    plt.gcf().subplots_adjust(left=<span class="number">0.15</span>)</span><br><span class="line">    plt.show(block=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#堆栈条形图</span></span><br><span class="line">    occGender = spark.sql(<span class="string">"SELECT occupation, gender FROM users"</span>)</span><br><span class="line">    occGender.show()</span><br><span class="line"></span><br><span class="line">    occCrossTab = occGender.stat.crosstab(<span class="string">"occupation"</span>,<span class="string">"gender"</span>)</span><br><span class="line">    occupationsCrossTuple = occCrossTab.rdd.map(<span class="keyword">lambda</span> p:(p.occupation_gender,p.M, p.F)).collect()</span><br><span class="line">    occList, mList, fList = zip(*occupationsCrossTuple)</span><br><span class="line">    N = len(occList)</span><br><span class="line">    ind = np.arange(N)</span><br><span class="line">    width = <span class="number">0.75</span></span><br><span class="line">    p1 = plt.bar(ind, mList, width, color=<span class="string">'r'</span>)</span><br><span class="line">    p2 = plt.bar(ind, fList, width, color=<span class="string">'y'</span>, bottom=mList)</span><br><span class="line">    plt.ylabel(<span class="string">'Count'</span>)</span><br><span class="line">    plt.title(<span class="string">'Gender distribution by occupation\n'</span>)</span><br><span class="line">    plt.xticks(ind + width/<span class="number">2.</span>, occList, rotation=<span class="number">90</span>)</span><br><span class="line">    plt.legend((p1[<span class="number">0</span>], p2[<span class="number">0</span>]), (<span class="string">'Male'</span>, <span class="string">'Female'</span>))</span><br><span class="line">    plt.gcf().subplots_adjust(bottom=<span class="number">0.25</span>)</span><br><span class="line">    plt.show(block=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#饼图</span></span><br><span class="line">    occupationsBottom10 = spark.sql(<span class="string">"SELECT occupation,count(occupation) as usercount FROM users GROUP BY occupation ORDER BY usercount LIMIT 10"</span>)</span><br><span class="line">    occupationsBottom10Tuple = occupationsBottom10.rdd.map(<span class="keyword">lambda</span> p:(p.occupation,p.usercount)).collect()</span><br><span class="line">    occupationsBottom10List, countBottom10List =zip(*occupationsBottom10Tuple)</span><br><span class="line">    explode = (<span class="number">0</span>, <span class="number">0.3</span>, <span class="number">0.2</span>, <span class="number">0.15</span>,<span class="number">0.1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0.1</span>)</span><br><span class="line">    plt.pie(countBottom10List, explode=explode,labels=occupationsBottom10List, autopct=<span class="string">'%1.1f%%'</span>, shadow=<span class="keyword">True</span>,startangle=<span class="number">90</span>)</span><br><span class="line">    plt.title(<span class="string">'Bottom 10 user types\n'</span>)</span><br><span class="line">    plt.show(block=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一-Python-访问-MySQL&quot;&gt;&lt;a href=&quot;#一-Python-访问-MySQL&quot; class=&quot;headerlink&quot; title=&quot;一. Python 访问 MySQL&quot;&gt;&lt;/a&gt;一. Python 访问 MySQL&lt;/h1&gt;&lt;h2 id=&quot;1-安
      
    
    </summary>
    
      <category term="Spark" scheme="https://airpoet.github.io/categories/Spark/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Spark" scheme="https://airpoet.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>i-Spark-4</title>
    <link href="https://airpoet.github.io/2018/07/20/Spark/i-Spark-4/"/>
    <id>https://airpoet.github.io/2018/07/20/Spark/i-Spark-4/</id>
    <published>2018-07-20T14:11:10.340Z</published>
    <updated>2018-07-23T06:20:44.679Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一-机器学习分类"><a href="#一-机器学习分类" class="headerlink" title="一. 机器学习分类"></a>一. 机器学习分类</h1><h2 id="1-监督学习"><a href="#1-监督学习" class="headerlink" title="1.监督学习"></a>1.监督学习</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-20-141755.png" alt="image-20180720221754476"></p><ul><li>有训练数据集。规范数据。合规数据。产生推断函数.然后对新数据应用函数。 </li><li>director actor edit         Label </li></ul><h2 id="2-非监督学习"><a href="#2-非监督学习" class="headerlink" title="2.非监督学习"></a>2.非监督学习</h2><ul><li>没有训练数据。 </li><li>分组。 </li></ul><h2 id="3-推荐"><a href="#3-推荐" class="headerlink" title="3.推荐"></a>3.推荐</h2><ul><li>协同过滤. </li><li>猜测你喜欢. </li><li>电商。 </li></ul><hr><h1 id="二-Spark机器学习库"><a href="#二-Spark机器学习库" class="headerlink" title="二. Spark机器学习库"></a>二. Spark机器学习库</h1><p><strong>[Estimator]</strong></p><ul><li><p>运行在包含了feature和label(结果)的dataFrame之上，对数据进行训练创建model。</p></li><li><p>该模型用于以后的预测。</p></li></ul><p><strong>[Transformer]</strong></p><ul><li>将包含feature的Dataframe变换成了包含了预测的dataframe.</li><li>由Estimator创建的model就是Transformer。</li></ul><p><strong>[Parameter]</strong></p><ul><li>Estimator和Transformer使用的数据，通常和机器学习的算法相关。</li><li>Spark API给出了一致性API针对算法。</li></ul><p><strong>[Pipeline]</strong></p><ul><li>将Estimators和Transformers组合在一起，形成机器学习工作流.</li></ul><h4 id="机器学习应用步骤"><a href="#机器学习应用步骤" class="headerlink" title="机器学习应用步骤"></a>机器学习应用步骤</h4><ol><li>读取数据文件形成训练数据框 </li><li>创建LinearRegression并设置参数 </li><li>对训练数据进行模型拟合，完成评估管线. </li><li>创建包含测试数据的DataFrame，典型包含feature和label，可以通过比较预测标签和测试标签确认model是ok， </li><li>使用模型，对测试数据进行变换(应用模型),抽取feature ，label，predication. </li></ol><h1 id="三-代码实例"><a href="#三-代码实例" class="headerlink" title="三. 代码实例"></a>三. 代码实例</h1><h2 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1. 线性回归"></a>1. 线性回归</h2><blockquote><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/scala/SparkMLDemo1.scala" target="_blank" rel="noopener">测试案例</a></p></blockquote><h2 id="2-逻辑回归"><a href="#2-逻辑回归" class="headerlink" title="2. 逻辑回归"></a>2. 逻辑回归</h2><blockquote><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/scala/LogicRegressWineClassifyDemo.scala" target="_blank" rel="noopener">酒质量预测</a></p></blockquote><blockquote><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/scala/SpamFilterDemo1.scala" target="_blank" rel="noopener">垃圾邮件过滤</a></p></blockquote><h2 id="3-ALS-最小二乘法"><a href="#3-ALS-最小二乘法" class="headerlink" title="3. ALS (最小二乘法)"></a>3. ALS (最小二乘法)</h2><blockquote><p>商品推荐: <a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/scala/RecommDemo.scala" target="_blank" rel="noopener">添加向指定用户推荐n款商品; 将指定的商品推荐给n个用户; 向所有用户推荐n种商品</a></p></blockquote><blockquote><p>电影推荐: <a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/scala/MovieRecommDemo.scala" target="_blank" rel="noopener">ALS算法电影推荐</a> </p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一-机器学习分类&quot;&gt;&lt;a href=&quot;#一-机器学习分类&quot; class=&quot;headerlink&quot; title=&quot;一. 机器学习分类&quot;&gt;&lt;/a&gt;一. 机器学习分类&lt;/h1&gt;&lt;h2 id=&quot;1-监督学习&quot;&gt;&lt;a href=&quot;#1-监督学习&quot; class=&quot;header
      
    
    </summary>
    
      <category term="Spark" scheme="https://airpoet.github.io/categories/Spark/"/>
    
      <category term="MLLib" scheme="https://airpoet.github.io/categories/Spark/MLLib/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MLLib" scheme="https://airpoet.github.io/tags/MLLib/"/>
    
      <category term="机器学习" scheme="https://airpoet.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>i-Spark-3</title>
    <link href="https://airpoet.github.io/2018/07/20/Spark/i-Spark-3/"/>
    <id>https://airpoet.github.io/2018/07/20/Spark/i-Spark-3/</id>
    <published>2018-07-20T12:06:12.974Z</published>
    <updated>2018-08-02T03:08:03.045Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一-Spark-Streaming-简介"><a href="#一-Spark-Streaming-简介" class="headerlink" title="一. Spark Streaming 简介"></a>一. Spark Streaming 简介</h1><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h2><ul><li>是 <code>spark core</code> 的扩展，针对实时数据流处理,具有可扩展、高吞吐量、容错.</li><li>数据可以是来自于<code>kafka, flume, tcpsocket</code>,使用高级函数(map reduce filter ,join , window)</li><li>处理的数据可以推送到 <code>database</code>, <code>hdfs</code>, 针对数据流处理可以应用到机器学习和图计算中。</li></ul><p><strong>实时计算相关技术</strong><br>    Strom / JStrom                Spark Streming               Flink<br>       实时性高                       有延迟                         实时性高<br>       吞吐量较低                     吞吐量高                        吞吐量高<br>       只能实时计算                离线+实时                      离线+实时<br>       算子比较少                      算子丰富                       算子丰富<br>           没有                 机器学习                        没有<br>           没有                 图计算                          没有<br>           使用比较少             非常火                          一般</p><h2 id="2-DStream-Discretized-Stream-离散流"><a href="#2-DStream-Discretized-Stream-离散流" class="headerlink" title="2. DStream (Discretized Stream ) 离散流"></a>2. DStream (Discretized Stream ) 离散流</h2><h4 id="概念"><a href="#概念" class="headerlink" title="概念:"></a><strong>概念:</strong></h4><ul><li>离散流,表示的是连续的数据流。连续的RDD序列。准实时计算。</li><li>通过kafka、flume等输入数据流产生，也可以通过对其他DStream进行高阶变换产生。</li><li>在内部，DStream表现为RDD序列。</li></ul><h4 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项:"></a><strong>注意事项:</strong></h4><ul><li>启动上下文之后，不能启动新的离散流</li><li>上下文停止后不能restart</li><li>同一 JVM只有一个 active 的 StreamingContext</li><li>停止StreamingContext会一同stop掉SparkContext，如若只停止StreamingContext. <code>ssc.stop(false|true);</code> :TODO</li><li>SparkContext可以创建多个StreamingContext, 创建新的之前停掉旧的。</li></ul><h4 id="DStream和Receiver"><a href="#DStream和Receiver" class="headerlink" title="DStream和Receiver:"></a>DStream和Receiver:</h4><ul><li><strong>介绍</strong><ul><li>Receiver是接受者，从source接受数据，存储在内存中供spark处理。</li></ul></li><li><strong>源</strong><ul><li>基本源:fileSystem | socket,内置API支持。</li><li>高级源:kafka | flume | …，需要引入pom.xml依赖.</li></ul></li><li><strong>注意</strong><ul><li>使用local模式时，不能使用一个线程.使用的local[n]，n需要大于receiver的个数。 </li></ul></li></ul><hr><h1 id="二-体验Spark-Streaming"><a href="#二-体验Spark-Streaming" class="headerlink" title="二. 体验Spark Streaming"></a>二. 体验Spark Streaming</h1><h2 id="1-引入-pom-依赖"><a href="#1-引入-pom-依赖" class="headerlink" title="1.引入 pom 依赖"></a>1.引入 pom 依赖</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="2-Scala-操作代码-in-IDEA"><a href="#2-Scala-操作代码-in-IDEA" class="headerlink" title="2. Scala 操作代码 in IDEA"></a>2. Scala 操作代码 in IDEA</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//local[n] n &gt; 1</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建Spark流上下文,批次时长是1s</span></span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建socket文本流</span></span><br><span class="line"><span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//压扁</span></span><br><span class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">//变换成对偶</span></span><br><span class="line"><span class="keyword">val</span> pairs = words.map((_,<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 化简</span></span><br><span class="line"><span class="keyword">val</span> count = pairs.reduceByKey(_+_) ;</span><br><span class="line">count.print()</span><br><span class="line"></span><br><span class="line"><span class="comment">//启动</span></span><br><span class="line">ssc.start()</span><br><span class="line"></span><br><span class="line"><span class="comment">//等待结束</span></span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure><h2 id="3-通过-nc-与-Spark-交互"><a href="#3-通过-nc-与-Spark-交互" class="headerlink" title="3. 通过 nc 与 Spark 交互"></a>3. 通过 nc 与 Spark 交互</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1&gt; 启动 nc</span><br><span class="line">nc -lk 9999</span><br><span class="line"></span><br><span class="line">2&gt; 运行 Spark Streaming 代码, 开启监听 localhost:9999 </span><br><span class="line"></span><br><span class="line">3&gt; 在 nc 命令行输入单词 </span><br><span class="line">$&gt; hello world </span><br><span class="line">$&gt; ....</span><br><span class="line"></span><br><span class="line">$&gt; 观察 spark 控制台打印</span><br><span class="line"></span><br><span class="line">PS: 把 log4j 文件放到项目的 resources 下, 设置log4j 的打印级别为 WARN, 否则很难观察清楚</span><br></pre></td></tr></table></figure><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/resources/log4j.properties" target="_blank" rel="noopener">log4j 文件请参考</a></p><h2 id="4-导出-Spark-Streaming-为-jar-包-放到-Linux-下运行"><a href="#4-导出-Spark-Streaming-为-jar-包-放到-Linux-下运行" class="headerlink" title="4.导出 Spark Streaming 为 jar 包, 放到 Linux 下运行"></a>4.导出 Spark Streaming 为 jar 包, 放到 Linux 下运行</h2><p><strong>注意: 直接spark-submit   或者 spark-submit –help , 会弹出帮助</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$&gt; spark-submit --name wcstreaming </span><br><span class="line">                --<span class="class"><span class="keyword">class</span> <span class="title">com</span>.<span class="title">rox</span>.<span class="title">spark</span>.<span class="title">scala</span>.<span class="title">SparkStreamingDemo</span></span></span><br><span class="line"><span class="class">                <span class="title">--master</span> <span class="title">spark</span></span>:<span class="comment">//cs1:7077</span></span><br><span class="line">                <span class="type">SparkDemo1</span><span class="number">-1.0</span>-<span class="type">SNAPSHOT</span>.jar</span><br></pre></td></tr></table></figure><hr><h1 id="三-Kafka-与-Spark-Streaming-整合"><a href="#三-Kafka-与-Spark-Streaming-整合" class="headerlink" title="三. Kafka 与 Spark Streaming 整合"></a>三. Kafka 与 Spark Streaming 整合</h1><h2 id="1-回忆Kafka"><a href="#1-回忆Kafka" class="headerlink" title="1. 回忆Kafka"></a>1. <strong>回忆Kafka</strong></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1&gt; 启动 kafka,cs2~cs4  前提先启动 zk</span><br><span class="line">$&gt; kafka-server-start.sh /home/ap/apps/kafka/config/server.properties</span><br><span class="line"></span><br><span class="line">2&gt; 查看主题: </span><br><span class="line">$&gt; kafka-topics.sh --zookeeper cs1:2181 --list</span><br><span class="line"></span><br><span class="line">3&gt; 开启消费者</span><br><span class="line">[ap@cs4]~%&gt;  kafka-console-consumer.sh --zookeeper cs3:2181 --topic kafka-test</span><br><span class="line"></span><br><span class="line">4&gt; 开启生产者</span><br><span class="line">[ap@cs2]~%&gt;  kafka-console-producer.sh --broker-list cs4:9092 --topic kafka-test </span><br><span class="line"></span><br><span class="line">&gt;&gt; 在生产者发送消息, 查看消费者</span><br></pre></td></tr></table></figure><h2 id="2-Driver-高可用"><a href="#2-Driver-高可用" class="headerlink" title="2.Driver 高可用"></a>2.Driver 高可用</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># supervise</span></span><br><span class="line"><span class="comment"># Run on a Spark standalone cluster in cluster deploy mode with supervise</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master spark://207.184.161.138:7077 \</span><br><span class="line">  --deploy-mode cluster \</span><br><span class="line">  --supervise \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --total-executor-cores 100 \</span><br><span class="line">  /path/to/examples.jar \</span><br><span class="line">  1000</span><br></pre></td></tr></table></figure><h2 id="3-使用-Java编写-Kafka-SparkStreaming-代码"><a href="#3-使用-Java编写-Kafka-SparkStreaming-代码" class="headerlink" title="3. 使用 Java编写 Kafka-SparkStreaming 代码"></a>3. 使用 Java编写 Kafka-SparkStreaming 代码</h2><p><a href="https://github.com/airpoet/bigdata/blob/068fbf3e23f6514fadcef10cef5b6ce8d87318d8/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/java/KafkaSparkstreamingDemo.java" target="_blank" rel="noopener">具体见 github</a></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.spark.java;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.TaskContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Seconds;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.TopicPartition;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaSparkstreamingDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">        conf.setAppName(<span class="string">"KafkaSparkstreamingDemo"</span>);</span><br><span class="line">        conf.setMaster(<span class="string">"local[4]"</span>);</span><br><span class="line">        <span class="comment">//创建Spark流应用上下文</span></span><br><span class="line">        JavaStreamingContext streamingContext = <span class="keyword">new</span> JavaStreamingContext(conf, Seconds.apply(<span class="number">2</span>));</span><br><span class="line"></span><br><span class="line">        Map&lt;String, Object&gt; kafkaParams = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        kafkaParams.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"cs2:9092,cs3:9092"</span>);</span><br><span class="line">        kafkaParams.put(<span class="string">"key.deserializer"</span>, StringDeserializer.class);</span><br><span class="line">        kafkaParams.put(<span class="string">"value.deserializer"</span>, StringDeserializer.class);</span><br><span class="line">        kafkaParams.put(<span class="string">"group.id"</span>, <span class="string">"g6"</span>);</span><br><span class="line">        kafkaParams.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>);</span><br><span class="line">        kafkaParams.put(<span class="string">"enable.auto.commit"</span>, <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">        Collection&lt;String&gt; topics = Arrays.asList(<span class="string">"kafka-test"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 取出 kafka stream</span></span><br><span class="line">        <span class="keyword">final</span> JavaInputDStream&lt;ConsumerRecord&lt;String, String&gt;&gt; stream =</span><br><span class="line">                KafkaUtils.createDirectStream(</span><br><span class="line">                        streamingContext,</span><br><span class="line">                        LocationStrategies.PreferConsistent(),</span><br><span class="line">                        ConsumerStrategies.&lt;String, String&gt;Subscribe(topics, kafkaParams)</span><br><span class="line">                );</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 压扁</span></span><br><span class="line">        JavaDStream&lt;String&gt; wordDS = stream.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;ConsumerRecord&lt;String, String&gt;, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Iterator&lt;String&gt; <span class="title">call</span><span class="params">(ConsumerRecord&lt;String, String&gt; r)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String value = r.value();</span><br><span class="line">                List&lt;String&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">                String[] arr = value.split(<span class="string">" "</span>);</span><br><span class="line">                <span class="keyword">for</span> (String s : arr) &#123;</span><br><span class="line">                    list.add(s);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> list.iterator();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 映射成元祖 (拼1)</span></span><br><span class="line">        JavaPairDStream&lt;String,Integer&gt; pairDS = wordDS.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(s,<span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">              </span><br><span class="line">        <span class="comment">// 聚合</span></span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; countDS = pairDS.reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer v1, Integer v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 打印计算结果</span></span><br><span class="line">        countDS.print();</span><br><span class="line"></span><br><span class="line">        streamingContext.start();</span><br><span class="line"></span><br><span class="line">        streamingContext.awaitTermination();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="四-跨单位时间-单位距离-Window-跨批次-updateStateByKey"><a href="#四-跨单位时间-单位距离-Window-跨批次-updateStateByKey" class="headerlink" title="四.  跨单位时间,单位距离 (Window) ,  跨批次(updateStateByKey)"></a>四.  跨单位时间,单位距离 (Window) ,  跨批次(updateStateByKey)</h1><h2 id="1-Window"><a href="#1-Window" class="headerlink" title="1. Window"></a>1. Window</h2><h4 id="关键参数"><a href="#关键参数" class="headerlink" title="关键参数:"></a><strong>关键参数</strong>:</h4><p><strong>batch interval</strong> </p><ul><li>批次的间隔. </li></ul><p><strong>windows length</strong></p><ul><li>窗口长度,跨批次。是批次的整数倍。 </li></ul><p><strong>slide interval</strong> </p><ul><li>滑动间隔,窗口计算的间隔时间，有时批次interval的整倍数。 </li></ul><p><br></p><h4 id="关键代码示例"><a href="#关键代码示例" class="headerlink" title="关键代码示例:"></a>关键代码示例:</h4><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/java/WCSparkStreamWindowApp.java" target="_blank" rel="noopener">具体查看 github</a></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//聚合</span></span><br><span class="line"><span class="comment">/** 统计同一 window 下的 key 的聚合 (用的比较多...)</span></span><br><span class="line"><span class="comment">     def reduceByKeyAndWindow(reduceFunc: Function2[V, V, V],</span></span><br><span class="line"><span class="comment">             windowDuration: Duration,</span></span><br><span class="line"><span class="comment">             slideDuration: Duration): JavaPairDStream[K, V]</span></span><br><span class="line"><span class="comment">    &gt;&gt;&gt; Return a new DStream by applying `reduceByKey` over a sliding window</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="type">JavaPairDStream</span>&lt;<span class="type">String</span>,<span class="type">Integer</span>&gt; countDS = pairDS.reduceByKeyAndWindow(<span class="keyword">new</span> <span class="type">Function2</span>&lt;<span class="type">Integer</span>, <span class="type">Integer</span>, <span class="type">Integer</span>&gt;() &#123;</span><br><span class="line">    public <span class="type">Integer</span> call(<span class="type">Integer</span> v1, <span class="type">Integer</span> v2) &#123;</span><br><span class="line">        <span class="keyword">return</span> v1 + v2;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;,<span class="type">Seconds</span>.apply(<span class="number">6</span>),<span class="type">Seconds</span>.apply(<span class="number">4</span>));</span><br></pre></td></tr></table></figure><h2 id="2-updateStateByKey"><a href="#2-updateStateByKey" class="headerlink" title="2. updateStateByKey"></a>2. updateStateByKey</h2><p><strong><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/java/WordCountSparkStreamingJava.java" target="_blank" rel="noopener">跨批次统计, 会一直累加, 具体查看 github</a></strong></p><h4 id="关键代码示例-1"><a href="#关键代码示例-1" class="headerlink" title="关键代码示例"></a>关键代码示例</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 可用于跨批次统计 updateStateByKeya</span></span><br><span class="line">JavaPairDStream&lt;String,Integer&gt; jps = pairDS.updateStateByKey(<span class="keyword">new</span> Function2&lt;List&lt;Integer&gt;, Optional&lt;Integer&gt;, Optional&lt;Integer&gt;&gt;() &#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Optional&lt;Integer&gt; <span class="title">call</span><span class="params">(List&lt;Integer&gt; v1, Optional&lt;Integer&gt; v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Integer newCount = v2.isPresent() ? v2.get() : <span class="number">0</span>  ;</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"old value : "</span> + newCount);</span><br><span class="line">        <span class="keyword">for</span>(Integer i : v1)&#123;</span><br><span class="line">            System.out.println(<span class="string">"new value : "</span> + i);</span><br><span class="line">            newCount = newCount +  i;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> Optional.of(newCount);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><hr><h1 id="五-spark-streaming中的容错实现-※"><a href="#五-spark-streaming中的容错实现-※" class="headerlink" title="五. spark streaming中的容错实现 ※"></a>五. spark streaming中的容错实现 ※</h1><h2 id="1-生产环境中spark-streaming的job的注意事项"><a href="#1-生产环境中spark-streaming的job的注意事项" class="headerlink" title="1.生产环境中spark streaming的job的注意事项"></a>1.生产环境中spark streaming的job的注意事项</h2><p><strong>避免单点故障。</strong></p><p><strong>Driver</strong>               </p><ul><li>驱动,运行用户编写的程序代码的主机。 </li></ul><p><strong>Executors</strong>        </p><ul><li>执行的spark driver提交的job,内部含有附加组件比如receiver</li><li>receiver接受数据并以block方式保存在memory中，同时，将数据块复制到其他executor中，以备容错。</li><li>每个批次末端会形成新的DStream，交给 下游处理。</li><li>如果receiver故障，其他执行器中的receiver会启动进行数据的接收。 </li></ul><p><strong>checkpoint</strong></p><ul><li>检查点</li><li>用于容错处理, 设置后, 会把数据存储到检查点目录,   当出问题后,  从检查点恢复.</li></ul><h2 id="2-通过checkpoint-实现-Spark-Streaming-的容错"><a href="#2-通过checkpoint-实现-Spark-Streaming-的容错" class="headerlink" title="2. 通过checkpoint 实现 Spark Streaming 的容错"></a>2. 通过<strong>checkpoint</strong> 实现 Spark Streaming 的容错</h2><p>如果<strong>executor故障</strong>，所有未被处理的数据都会丢失，解决办法可以通过wal (hbase,hdfs/WALs)方式将数据预先写入到hdfs或者s3.</p><p>如果<strong>Driver故障</strong>，driver程序就会停止，所有executor都是丢失连接，停止计算过程。</p><p><strong>解决办法需要配置和编程。</strong></p><ul><li>配置Driver程序自动重启，使用特定的clustermanager实现。 :TODO   how?</li><li><strong>重启时，从宕机的地方进行重启，通过检查点机制可以实现该功能。</strong><ul><li>检查点目录可以是本地，可以是hdfs, <strong>生产中一般都是 hdfs</strong></li><li><strong>不再使用new方式</strong>创建<code>SparkStreamContext</code>对象，而是通过工厂方式<code>JavaStreamingContext.getOrCreate()</code>方法创建</li><li>上下文对象,首先会检查检查点目录，看是否有job运行，没有就new新的。</li></ul></li><li>编写容错测试代码,计算过程编写到 <code>Function0</code> 的<code>call</code>方法中。 </li></ul><p><strong>知识点</strong>: Function0, 1, 2, 3, 4</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">为何要使用 Function0?</span><br><span class="line">    因为 JavaStreamingContext.getOrCreate(path,function0) 的第二个参数就是 Function0</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A zero-argument function that returns an R.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Function0</span>&lt;<span class="title">R</span>&gt; <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="function">R <span class="title">call</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/java/SparkStreamingCheckpointDemo.java" target="_blank" rel="noopener">Java 代码, 具体见 github</a></p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">package</span> com.rox.spark.java;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function0;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Duration;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkStreamingCheckpointDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Create a factory object that can create and setup a new JavaStreamingContext</span></span><br><span class="line"><span class="comment">         * 可以使用 Function0</span></span><br><span class="line"><span class="comment">         * A zero-argument function that returns an R.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        Function0&lt;JavaStreamingContext&gt; contextFactory = <span class="keyword">new</span> Function0&lt;JavaStreamingContext&gt;()&#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="comment">// 首次创建 context 时, 调用此方法</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> JavaStreamingContext <span class="title">call</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">                conf.setMaster(<span class="string">"local[4]"</span>);</span><br><span class="line">                conf.setAppName(<span class="string">"SparkStreamingCheckpointDemo"</span>);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 创建流上下文对象</span></span><br><span class="line">                JavaStreamingContext jsc = <span class="keyword">new</span> JavaStreamingContext(conf, <span class="keyword">new</span> Duration(<span class="number">2</span> * <span class="number">1000</span>));</span><br><span class="line"></span><br><span class="line">                <span class="comment">// Create an input stream from network source hostname:port.</span></span><br><span class="line">                JavaDStream&lt;String&gt; lines = jsc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">10086</span>);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// =============== 变换代码 ===============</span></span><br><span class="line">                <span class="comment">// 设置一个窗口时长为1天, 滚动间隔为 2s</span></span><br><span class="line">                JavaDStream&lt;Long&gt; longJavaDStream = lines.countByWindow(<span class="keyword">new</span> Duration(<span class="number">24</span> * <span class="number">60</span> * <span class="number">60</span> * <span class="number">1000</span>), <span class="keyword">new</span> Duration(<span class="number">2</span> * <span class="number">1000</span>));</span><br><span class="line"></span><br><span class="line">                longJavaDStream.print();</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 设置检查点 目录</span></span><br><span class="line">                jsc.checkpoint(<span class="string">"file:///Users/shixuanji/Documents/Code/temp/check"</span>);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 返回流上下文对象</span></span><br><span class="line">                <span class="keyword">return</span> jsc;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         *   def getOrCreate(</span></span><br><span class="line"><span class="comment">         checkpointPath: String,</span></span><br><span class="line"><span class="comment">         creatingFunc: JFunction0[JavaStreamingContext]</span></span><br><span class="line"><span class="comment">         ): JavaStreamingContext</span></span><br><span class="line"><span class="comment">         注意: 第2个参数是一个 Function0对象</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="comment">// 失败后, 重新创建时, 会经过检查点</span></span><br><span class="line">        JavaStreamingContext context = JavaStreamingContext.getOrCreate(<span class="string">"file:///Users/shixuanji/Documents/Code/temp/check"</span>, contextFactory);</span><br><span class="line"></span><br><span class="line">        context.start();</span><br><span class="line">        context.awaitTermination();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一-Spark-Streaming-简介&quot;&gt;&lt;a href=&quot;#一-Spark-Streaming-简介&quot; class=&quot;headerlink&quot; title=&quot;一. Spark Streaming 简介&quot;&gt;&lt;/a&gt;一. Spark Streaming 简介&lt;/h1
      
    
    </summary>
    
      <category term="Spark" scheme="https://airpoet.github.io/categories/Spark/"/>
    
      <category term="SparkStreaming" scheme="https://airpoet.github.io/categories/Spark/SparkStreaming/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Spark" scheme="https://airpoet.github.io/tags/Spark/"/>
    
      <category term="SparkStreaming" scheme="https://airpoet.github.io/tags/SparkStreaming/"/>
    
  </entry>
  
</feed>
