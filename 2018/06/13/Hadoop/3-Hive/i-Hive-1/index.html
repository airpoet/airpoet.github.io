<!DOCTYPE html>




<html class="theme-next pisces" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">


  <link rel="manifest" href="/images/manifest.json">


  <meta name="msapplication-config" content="/images/browserconfig.xml" />



  <meta name="keywords" content="原创,技术,Hadoop,Hive," />





  <link rel="alternate" href="/atom.xml" title="A.P的文艺杂谈" type="application/atom+xml" />






<meta name="description" content="1.  Hive 初探1.1 Hive 的数据存储 Hive的数据存储基于Hadoop HDFS Hive没有专门的数据存储格式 存储结构主要包括：数据库、文件、表、视图、索引 Hive默认可以直接加载文本文件（TextFile），还支持SequenceFile、RCFile  创建表时，指定Hive数据的列分隔符与行分隔符，Hive即可解析数据  1.2 Hive的系统架构  用户接口，包括 C">
<meta name="keywords" content="原创,技术,Hadoop,Hive">
<meta property="og:type" content="article">
<meta property="og:title" content="i-Hive-1">
<meta property="og:url" content="https://airpoet.github.io/2018/06/13/Hadoop/3-Hive/i-Hive-1/index.html">
<meta property="og:site_name" content="A.P的文艺杂谈">
<meta property="og:description" content="1.  Hive 初探1.1 Hive 的数据存储 Hive的数据存储基于Hadoop HDFS Hive没有专门的数据存储格式 存储结构主要包括：数据库、文件、表、视图、索引 Hive默认可以直接加载文本文件（TextFile），还支持SequenceFile、RCFile  创建表时，指定Hive数据的列分隔符与行分隔符，Hive即可解析数据  1.2 Hive的系统架构  用户接口，包括 C">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-13-152054.jpg">
<meta property="og:image" content="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-13-140311.jpg">
<meta property="og:image" content="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-043653.png">
<meta property="og:image" content="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-043558.png">
<meta property="og:image" content="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-13-163316.jpg">
<meta property="og:image" content="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-17-032955.gif">
<meta property="og:image" content="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-14-113618.png">
<meta property="og:image" content="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-14-113550.png">
<meta property="og:image" content="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-15-070816.jpg">
<meta property="og:image" content="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-070624.png">
<meta property="og:image" content="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-071819.png">
<meta property="og:image" content="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-072342.png">
<meta property="og:image" content="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-072352.png">
<meta property="og:image" content="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-075926.png">
<meta property="og:image" content="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-080134.png">
<meta property="og:image" content="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-080605.png">
<meta property="og:image" content="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-080942.png">
<meta property="og:image" content="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-081706.jpg?ff=fh3">
<meta property="og:image" content="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-082423.png">
<meta property="og:image" content="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-084428.png">
<meta property="og:image" content="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-084655.png">
<meta property="og:image" content="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-16-071623.jpg">
<meta property="og:image" content="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-17-071714.png">
<meta property="og:image" content="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-17-082606.png">
<meta property="og:updated_time" content="2018-06-22T13:29:40.732Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="i-Hive-1">
<meta name="twitter:description" content="1.  Hive 初探1.1 Hive 的数据存储 Hive的数据存储基于Hadoop HDFS Hive没有专门的数据存储格式 存储结构主要包括：数据库、文件、表、视图、索引 Hive默认可以直接加载文本文件（TextFile），还支持SequenceFile、RCFile  创建表时，指定Hive数据的列分隔符与行分隔符，Hive即可解析数据  1.2 Hive的系统架构  用户接口，包括 C">
<meta name="twitter:image" content="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-13-152054.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'airpoet'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://airpoet.github.io/2018/06/13/Hadoop/3-Hive/i-Hive-1/"/>





  <title>i-Hive-1 | A.P的文艺杂谈</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">A.P的文艺杂谈</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://airpoet.github.io/2018/06/13/Hadoop/3-Hive/i-Hive-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="airpoet">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-14-045043.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A.P的文艺杂谈">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">i-Hive-1</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-13T22:04:40+08:00">
                2018-06-13
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-06-22T21:29:40+08:00">
                2018-06-22
              </time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index">
                    <span itemprop="name">Hadoop</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/Hive/" itemprop="url" rel="index">
                    <span itemprop="name">Hive</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/06/13/Hadoop/3-Hive/i-Hive-1/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/06/13/Hadoop/3-Hive/i-Hive-1/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv">本文总阅读量
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>次
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="1-Hive-初探"><a href="#1-Hive-初探" class="headerlink" title="1.  Hive 初探"></a>1.  Hive 初探</h1><h3 id="1-1-Hive-的数据存储"><a href="#1-1-Hive-的数据存储" class="headerlink" title="1.1 Hive 的数据存储"></a>1.1 Hive 的数据存储</h3><ul>
<li>Hive的数据存储基于Hadoop HDFS</li>
<li>Hive没有专门的数据存储格式</li>
<li>存储结构主要包括：数据库、文件、表、视图、索引</li>
<li>Hive默认可以直接加载文本文件（TextFile），还支持SequenceFile、RCFile </li>
<li>创建表时，指定Hive数据的列分隔符与行分隔符，Hive即可解析数据</li>
</ul>
<h3 id="1-2-Hive的系统架构"><a href="#1-2-Hive的系统架构" class="headerlink" title="1.2 Hive的系统架构"></a>1.2 Hive的系统架构</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-13-152054.jpg" alt=""></p>
<ul>
<li>用户接口，包括 CLI，JDBC/ODBC，WebUI</li>
<li>元数据存储，通常是存储在关系数据库如 mysql, derby 中</li>
<li>解释器、编译器、优化器、执行器</li>
<li>Hadoop：用 HDFS 进行存储，利用 MapReduce 进行计算</li>
</ul>
<h3 id="1-3-Hive的系统架构"><a href="#1-3-Hive的系统架构" class="headerlink" title="1.3 Hive的系统架构"></a>1.3 Hive的系统架构</h3><ul>
<li>用户接口主要有三个：CLI，JDBC/ODBC和 WebUI<ul>
<li>CLI，即Shell命令行</li>
<li>JDBC/ODBC 是 Hive 的Java，与使用传统数据库JDBC的方式类似</li>
<li>WebGUI是通过浏览器访问 Hive</li>
</ul>
</li>
<li>Hive 将元数据存储在数据库中(metastore)，目前只支持 mysql、derby。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等</li>
<li>解释器、编译器、优化器完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划（plan）的生成。生成的查询计划存储在 HDFS 中，并在随后由 MapReduce 调用执行</li>
<li>Hive 的数据存储在 HDFS 中，大部分的查询由 MapReduce 完成（包含 <em> 的查询，比如 select </em> from table 不会生成 MapRedcue 任务</li>
</ul>
<h3 id="1-4-Hive的metastore"><a href="#1-4-Hive的metastore" class="headerlink" title="1.4 Hive的metastore"></a>1.4 Hive的metastore</h3><ul>
<li>metastore是hive元数据的集中存放地。</li>
<li>metastore默认使用内嵌的derby数据库作为存储引擎</li>
<li>Derby引擎的缺点：一次只能打开一个会话</li>
<li>使用Mysql作为外置存储引擎，多用户同时访问 </li>
</ul>
<h3 id="1-5-Hive-和-Hadoop-的调用关系"><a href="#1-5-Hive-和-Hadoop-的调用关系" class="headerlink" title="1.5 Hive 和 Hadoop 的调用关系"></a>1.5 Hive 和 Hadoop 的调用关系</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-13-140311.jpg" alt=""></p>
<blockquote>
<p>1、提交sql  交给驱动<br>2、驱动编译    解析相关的字段表信息<br>3、去metastore查询相关的信息    返回字段表信息<br>4、编译返回信息 发给驱动<br>5、驱动发送一个执行计划    交给执行引擎<br>6.1、DDLs <strong>对数据库表的操作的, 直接和metastore交互</strong>,<code>create table t1(name string);</code></p>
<p>6.2、完成job返回数据信息、找<strong>namenode</strong>查数据<br>6.3、<strong>namenode</strong>交互<code>select count(1) from t1;</code><br>7、返回结果信息集</p>
</blockquote>
<h3 id="1-6-Hive-参数配置使用"><a href="#1-6-Hive-参数配置使用" class="headerlink" title="1.6 Hive 参数配置使用"></a>1.6 Hive 参数配置使用</h3><table>
<thead>
<tr>
<th>命名空间</th>
<th>使用权限</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>hivevar</td>
<td>可读写</td>
<td>$   hive -d name=zhangsan;</td>
</tr>
<tr>
<td>hiveconf</td>
<td>可读写</td>
<td>\$   hive –hiveconf hive.cli.print.current.db=true;   $   hive –hiveconf hive.cli.print.header=true;</td>
</tr>
<tr>
<td>system</td>
<td>可读写</td>
<td>java定义的配置属性，如system:user.name</td>
</tr>
<tr>
<td>env</td>
<td>只读</td>
<td>shell环境变量，如env:USER</td>
</tr>
</tbody>
</table>
<ul>
<li><p>hivevar </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用场景: 起别名</span></span><br><span class="line">hive -d name=zhangsan  <span class="comment">#传参</span></span><br><span class="line">&gt; create table t2(name string,<span class="variable">$&#123;name&#125;</span> string); <span class="comment">#取参数</span></span><br><span class="line">&gt; desc t2;</span><br><span class="line">---</span><br><span class="line">name                	string</span><br><span class="line">zhangsan            	string</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>hiveconf :</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  显示当前数据库名称</span></span><br><span class="line">[ap@cs2]~% hive --hiveconf hive.cli.print.current.db=<span class="literal">true</span>;</span><br><span class="line">hive (default)&gt; create database mydb;</span><br><span class="line">hive (default)&gt; use mydb;</span><br><span class="line">hive (mydb)&gt; </span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示表头(字段名)</span></span><br><span class="line">hive --hiveconf hive.cli.print.header=<span class="literal">true</span>;</span><br><span class="line">select * from t2;</span><br><span class="line">t2.name	t2.zhangsan</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="1-7-Hive-的脚本执行"><a href="#1-7-Hive-的脚本执行" class="headerlink" title="1.7 Hive 的脚本执行"></a>1.7 Hive 的脚本执行</h3><ul>
<li><strong>Hive -e “xx ”</strong><ul>
<li>e 就是 edit, 在终端打印输出</li>
</ul>
</li>
<li><strong>Hive -e “show tables” &gt;&gt; a.txt</strong><ul>
<li>可以把执行结果重定向到文件中</li>
</ul>
</li>
<li><strong>Hive -S -e “show tables” &gt;&gt; a.txt</strong><ul>
<li>-S : silence 安静的执行</li>
</ul>
</li>
<li><strong>hive -f file</strong> <ul>
<li>hive -f hql ,  hql 是文件, 执行文件</li>
<li>执行完了之后,  就离开 hive 命令行</li>
</ul>
</li>
<li><strong>hive -i /home/ap/hive-init.sql</strong> <ul>
<li>执行完了,还在控制台, 可以继续操作</li>
</ul>
</li>
<li><strong>hive&gt;source file</strong><ul>
<li>source + 文件名  : 直接执行当前目录文件</li>
<li>source /home/ap/xx.sql;</li>
</ul>
</li>
</ul>
<h3 id="1-8-hive与依赖环境的交互"><a href="#1-8-hive与依赖环境的交互" class="headerlink" title="1.8 hive与依赖环境的交互"></a>1.8 hive与依赖环境的交互</h3><ul>
<li><strong>与linux交互命令 ！</strong><ul>
<li><code>!ls</code></li>
<li><code>!pwd</code></li>
</ul>
</li>
<li><strong>与hdfs交互命令</strong><ul>
<li><code>dfs -ls /</code></li>
<li><code>dfs -mkdir /hive</code></li>
<li><code>hive (default)&gt; dfs -rm -r /user/hive/warehouse/t5;</code></li>
</ul>
</li>
<li><strong>beeline 与 linux &amp; hdfs 交互</strong><ul>
<li>!help 查看帮助</li>
</ul>
</li>
</ul>
<p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-043653.png" alt="image-20180619123652339"></p>
<p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-043558.png" alt="image-20180619123557944"></p>
<h3 id="1-9-Hive-的-JDBC-模式"><a href="#1-9-Hive-的-JDBC-模式" class="headerlink" title="1.9 Hive 的 JDBC 模式"></a>1.9 Hive 的 JDBC 模式</h3><ul>
<li><p>JAVA API交互执行方式 </p>
</li>
<li><p>hive 远程服务 (端口号1000  0) 启动方式</p>
<ul>
<li><code>hive --service hiveserver2</code></li>
<li><code>org.apache.hive.jdbc.HiveDriver</code></li>
</ul>
</li>
<li><p>在java代码中调用hive的JDBC建立连接</p>
</li>
<li><p><strong>用 beeline 连接</strong></p>
<ul>
<li><p><strong>方式1: 直接登录</strong></p>
<ul>
<li>注意: 这里的<strong>cs2是指的数据库所在的服务器</strong>, 如果mysql 安装在 cs2上, 那么不管在哪台机器上登录beeline , 都这样输入就行了</li>
</ul>
<p><code>beeline -u jdbc:hive2://cs2:10000 -n ap</code></p>
</li>
<li><p><strong>方式2: 输入用户名密码登录</strong></p>
<p><code>!connect jdbc:hive2://cs2:10000</code></p>
</li>
</ul>
</li>
<li><p><strong>beeline注意点:</strong> </p>
<ul>
<li>使用 beeline 连接时, 貌似无法与 Linux 目录交互</li>
<li>当前目录在<code>/home/ap/apps/apache-hive-2.3.2-bin/bin/</code>下</li>
<li><strong>要传文件的话, 要使用全路径</strong></li>
</ul>
</li>
</ul>
<h3 id="1-10-SET命令使用"><a href="#1-10-SET命令使用" class="headerlink" title="1.10 SET命令使用"></a>1.10 SET命令使用</h3><ul>
<li>Hive 控制台set 命令<ul>
<li>set;    set -v;  显示所有的环境变量</li>
<li><code>set hive.cli.print.current.db=true;</code></li>
<li><code>set hive.cli.print.header=true;</code> </li>
<li><code>set hive.metastore.warehouse.dir=/hive;</code></li>
</ul>
</li>
<li><strong>hive参数初始化配置set命令:</strong><ul>
<li><strong>~/.hiverc</strong><ul>
<li>创建此文件, 在此文件中配置初始化命令</li>
</ul>
</li>
<li>补充：<br><strong>hive历史操作命令集</strong><br><strong>~/.hivehistory</strong></li>
</ul>
</li>
</ul>
<p><br></p>
<h1 id="2-Hive数据类型"><a href="#2-Hive数据类型" class="headerlink" title="2.  Hive数据类型"></a>2.  Hive数据类型</h1><h3 id="2-1-基本数据类型"><a href="#2-1-基本数据类型" class="headerlink" title="2.1 基本数据类型"></a>2.1 基本数据类型</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-13-163316.jpg" alt=""></p>
<h3 id="2-2-复合数据类型"><a href="#2-2-复合数据类型" class="headerlink" title="2.2 复合数据类型"></a>2.2 复合数据类型</h3><blockquote>
<p>创建学生表</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> student(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">INT</span>,</span><br><span class="line">    <span class="keyword">name</span> <span class="keyword">STRING</span>,</span><br><span class="line">    favors <span class="built_in">ARRAY</span>\&lt;<span class="keyword">STRING</span>&gt;,</span><br><span class="line">    scores <span class="keyword">MAP</span>&lt;<span class="keyword">STRING</span>, <span class="built_in">FLOAT</span>&gt;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>默认分隔符</th>
<th>描述</th>
<th>语句</th>
</tr>
</thead>
<tbody>
<tr>
<td>\n</td>
<td>分隔行</td>
<td>LINES   TERMINATED BY ‘\t’</td>
</tr>
<tr>
<td>^A</td>
<td>分隔字段(列)，显示编码使用\001</td>
<td>FIELDS   TERMINATED BY ‘\001’</td>
</tr>
<tr>
<td>^B</td>
<td>分隔复合类型中的元素，显示编码使用\002</td>
<td>COLLECTION   ITEMS TERMINATED BY ‘\002’</td>
</tr>
<tr>
<td>^C</td>
<td>分隔map元素的key和value，显示编码使用\003</td>
<td>MAP   KEYS TERMINATED BY ‘\003’</td>
</tr>
</tbody>
</table>
<h4 id="2-2-1-Struct-使用"><a href="#2-2-1-Struct-使用" class="headerlink" title="2.2.1.  Struct 使用"></a>2.2.1.  Struct 使用</h4><p><strong>Structs内部的数据可以通过DOT（.）来存取</strong>，例如，表中一列c的类型为<code>STRUCT{a INT; b INT}</code>，我们可以通过c.a来访问域a</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># 数据</span><br><span class="line">1001,zhangsan:24</span><br><span class="line">1002,lisi:28</span><br><span class="line">1003,wangwu:25</span><br><span class="line"></span><br><span class="line"># 1.创建表</span><br><span class="line">hive&gt; create table student_test(id INT, info struct&lt;name:STRING, age:INT&gt;)ROW FORMAT DELIMITED FIELDS TERMINATED BY ','COLLECTION ITEMS TERMINATED BY ':';</span><br><span class="line"></span><br><span class="line"># 2.加载表</span><br><span class="line">hive&gt; load data local inpath "student_test" into table student_test;</span><br><span class="line"></span><br><span class="line"># 3.顺便设置 显示表头,和当前数据库</span><br><span class="line">hive&gt; set hive.cli.print.header=true;</span><br><span class="line">hive&gt; set hive.cli.print.current.db=true;</span><br><span class="line"></span><br><span class="line"># 4. 展示所有的</span><br><span class="line">hive (default)&gt; select * from student_test;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">student_test.id	student_test.info</span><br><span class="line">1001	&#123;"name":"zhangsan","age":24&#125;</span><br><span class="line">1002	&#123;"name":"lisi","age":28&#125;</span><br><span class="line">1003	&#123;"name":"wangwu","age":25&#125;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line"></span><br><span class="line"># Struct -结构体-使用 . </span><br><span class="line">hive (default)&gt; select id,info.name,info.age from student_test;</span><br><span class="line">id	name	age</span><br><span class="line">1001	zhangsan	24</span><br><span class="line">1002	lisi	28</span><br><span class="line">1003	wangwu	25</span><br></pre></td></tr></table></figure>
<h4 id="2-2-2-Array-使用"><a href="#2-2-2-Array-使用" class="headerlink" title="2.2.2. Array 使用"></a>2.2.2. Array 使用</h4><p><strong>Array中的数据为相同类型</strong>，例如，假如array A中元素<code>[&#39;a&#39;,&#39;b&#39;,&#39;c’]</code>，则A[1]的值为’b’</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># 原始数据</span><br><span class="line">zhangsan,24:25:27:37</span><br><span class="line">lisi,28:39:23:43</span><br><span class="line">wangwu,25:23:02:54</span><br><span class="line"></span><br><span class="line"># 创建表</span><br><span class="line">hive (default)&gt; create table class_test(name string,student_id_list array&lt;int&gt;) row format delimited fields terminated by ',' collection items terminated by ':';</span><br><span class="line"></span><br><span class="line"># 加载表</span><br><span class="line">hive (default)&gt; load data local inpath "class_test" into table class_test;</span><br><span class="line"></span><br><span class="line"># 查看表</span><br><span class="line">hive (default)&gt; select * from class_test;</span><br><span class="line">OK</span><br><span class="line">class_test.name	class_test.student_id_list</span><br><span class="line">zhangsan	[24,25,27,37]</span><br><span class="line">lisi	[28,39,23,43]</span><br><span class="line">wangwu	[25,23,2,54]</span><br><span class="line"></span><br><span class="line"># 查看数据中某个元素</span><br><span class="line">hive (default)&gt; select name, student_id_list[0] from class_test where name='zhangsan';</span><br><span class="line">OK</span><br><span class="line">name	_c1</span><br><span class="line">zhangsan	24</span><br></pre></td></tr></table></figure>
<h4 id="2-2-3-Map-使用"><a href="#2-2-3-Map-使用" class="headerlink" title="2.2.3. Map 使用"></a>2.2.3. Map 使用</h4><p>访问指定域可以通过[“指定域名称”]进行，例如，一个Map M包含了一个group-&gt;gid的kv对，<strong>gid的值可以通过M[‘group’]来获取</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"># 原始数据</span><br><span class="line">1001	job:80,team:60,person:70</span><br><span class="line">1002	job:60,team:80,person:80</span><br><span class="line">1003	job:90,team:70,person:100</span><br><span class="line"></span><br><span class="line"># 创建表</span><br><span class="line">hive (default)&gt; create table employee(id string,perf map&lt;string,int&gt;) row format delimited fields terminated by '\t' collection items terminated by ',' map keys terminated by ':';</span><br><span class="line"></span><br><span class="line"># 导入</span><br><span class="line">hive (default)&gt; load data local inpath "employee_data" into table employee;</span><br><span class="line"></span><br><span class="line"># 查看</span><br><span class="line">hive (default)&gt; select * from employee;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">employee.id	employee.perf</span><br><span class="line">1001	&#123;"job":80,"team":60,"person":70&#125;</span><br><span class="line">1002	&#123;"job":60,"team":80,"person":80&#125;</span><br><span class="line">1003	&#123;"job":90,"team":70,"person":100&#125;</span><br><span class="line">Time taken: 0.228 seconds, Fetched: 3 row(s)</span><br><span class="line"></span><br><span class="line"># 查看单个</span><br><span class="line">hive (default)&gt; select id,perf['job'],perf['team'],perf['person'] from employee;</span><br><span class="line">OK</span><br><span class="line">id	_c1	_c2	_c3</span><br><span class="line">1001	80	60	70</span><br><span class="line">1002	60	80	80</span><br><span class="line">1003	90	70	100</span><br><span class="line"></span><br><span class="line"># 显示别名</span><br><span class="line">hive (default)&gt; select id,perf['job'] as job,perf['team'] as team,perf['person'] as person from employee;</span><br><span class="line">OK</span><br><span class="line">id	job	team	person</span><br><span class="line">1001	80	60	70</span><br><span class="line">1002	60	80	80</span><br><span class="line">1003	90	70	100</span><br></pre></td></tr></table></figure>
<p><br></p>
<h1 id="3-DDL-DML"><a href="#3-DDL-DML" class="headerlink" title="3. DDL , DML"></a>3. DDL , DML</h1><h2 id="3-1-DDL"><a href="#3-1-DDL" class="headerlink" title="3.1 DDL"></a>3.1 DDL</h2><h3 id="3-1-1-数据库定义"><a href="#3-1-1-数据库定义" class="headerlink" title="3.1.1  数据库定义"></a>3.1.1  数据库定义</h3><ul>
<li><p>默认数据库”default”</p>
</li>
<li><p>使用某个数据库 <code>use &lt;数据库名&gt;</code></p>
</li>
<li><p>创建一个新库</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span>  </span><br><span class="line">[<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] mydb  </span><br><span class="line">[LOCATION] <span class="string">'/.......'</span>  </span><br><span class="line">[<span class="keyword">COMMENT</span>] <span class="string">'....’;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">hive&gt;SHOW DATABASES;</span></span><br><span class="line"><span class="string">hive&gt;DESCRIBE DATABASE [extended] mydb;</span></span><br><span class="line"><span class="string">hive&gt;DROP DATABASE [IF EXISTS] mydb [CASCADE];</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>创建</p>
<ul>
<li><code>create database db1;</code></li>
</ul>
</li>
<li><p>删除</p>
<ul>
<li><code>drop database if exists db1;</code></li>
<li><strong>级联删除</strong><ul>
<li><code>drop database if exists db1 cascade;</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="3-1-2-表定义-修改"><a href="#3-1-2-表定义-修改" class="headerlink" title="3.1.2  表定义/修改"></a>3.1.2  表定义/修改</h3><ul>
<li><p><strong>创建表</strong></p>
<ul>
<li><p>hive&gt;CREATE TABLE IF NOT EXISTS t1(…) </p>
<p>[COMMENT ‘….’] </p>
<p>[LOCATION ‘…’] </p>
<ul>
<li><code>hive (default)&gt; create table t4(name string,age int) row format delimited fields terminated by &quot;\t”;</code></li>
</ul>
</li>
<li><p>hive&gt; SHOW TABLES in mydb;</p>
<ul>
<li><code>show tables in mydb ‘’class*“</code> : 查看以 mydb 库中, 以 class 开头的表</li>
</ul>
</li>
<li><p><strong>hive&gt;CREATE TABLE t2 LIKE t1;</strong>   复制表</p>
<ul>
<li><strong>只会复制表结构</strong></li>
<li><code>hive (default)&gt; create table t2 like t1;</code></li>
<li><code>hive (mydb)&gt; create table t3 like default.employee;</code><ul>
<li>复制其它库的表</li>
</ul>
</li>
</ul>
</li>
<li><p>hive&gt;DESCRIBE t2;</p>
<ul>
<li>desc t2;  # 效果一样的</li>
<li><code>desc extended t1;</code>    # 查看更详细的表信息</li>
<li><strong><code>hive (default)&gt; desc formatted t1;</code>  # 格式化查看表的详细信息</strong></li>
</ul>
</li>
<li><p>drop  table xxx;</p>
<ul>
<li>删除表</li>
</ul>
</li>
<li><p><strong>查看建表语句</strong></p>
<ul>
<li><strong>show create table t_table;</strong></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>修改表</strong></p>
<ul>
<li><strong>重命名表</strong><ul>
<li><code>ALTER TABLE table_name RENAME TO new_table_name</code></li>
</ul>
</li>
<li><strong>增加/删除 分区</strong><ul>
<li><code>alter table student_p add partition(part=&#39;a&#39;) partition(part=&#39;b&#39;);</code><ul>
<li>两个 partition中没有’,’</li>
</ul>
</li>
<li>alter table student drop partition(stat_data=‘ffff’), partition(part=‘a’),partiton(part=‘b’);<ul>
<li>两个 partition中有’,’</li>
</ul>
</li>
</ul>
</li>
<li><strong>增加/更新 列</strong><ul>
<li><code>ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)</code><ul>
<li><em>注：ADD<strong>是代表新增一字段，字段位置在所有列后面(partition</strong>列前)<strong>，REPLACE</strong>则是表示替换表中所有字段。</em> </li>
<li><code>alter table student add  columns (name1 string);</code></li>
</ul>
</li>
<li><code>ALTER TABLE table_name CHANGE c_name new_c_name new_c_type [FIRST | AFTER c_name]</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="3-1-3-列定义"><a href="#3-1-3-列定义" class="headerlink" title="3.1.3 列定义"></a>3.1.3 列定义</h3><ul>
<li><p>修改列的名称、类型、位置、注释</p>
<ul>
<li><p><code>ALTER TABLE t3 CHANGE COLUMN old_name new_name String COMMENT &#39;...&#39; AFTER column2;</code></p>
</li>
<li><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 修改列名</span><br><span class="line">hive (default)&gt; alter table t1 change column name username string comment 'new name';</span><br><span class="line"># 查看表</span><br><span class="line">hive (default)&gt; desc t1;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">col_name	data_type	<span class="keyword">comment</span></span><br><span class="line">username            	<span class="keyword">string</span>              	<span class="keyword">new</span> <span class="keyword">name</span></span><br><span class="line">age                 	<span class="built_in">int</span></span><br><span class="line"><span class="comment">---</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>增加列</p>
<ul>
<li><p><code>hive&gt; ALTER TABLE t3 ADD COLUMNS(gender int);</code></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 查看表结构</span><br><span class="line">hive (default)&gt; desc t3;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">col_name	data_type	<span class="keyword">comment</span></span><br><span class="line"><span class="keyword">name</span>                	<span class="keyword">string</span></span><br><span class="line"><span class="comment">---</span></span><br><span class="line"># 添加列</span><br><span class="line">hive (<span class="keyword">default</span>)&gt; <span class="keyword">alter</span> <span class="keyword">table</span> t3 <span class="keyword">add</span> <span class="keyword">columns</span>(gender <span class="built_in">int</span>);</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">hive (default)&gt; desc t3;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">col_name	data_type	<span class="keyword">comment</span></span><br><span class="line"><span class="keyword">name</span>                	<span class="keyword">string</span></span><br><span class="line">gender              	<span class="built_in">int</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>删除列  replace </p>
<ul>
<li><strong>非常不建议使用</strong>, 会造成数据错乱, 一般采取重新创建一张表的方式.</li>
</ul>
</li>
</ul>
<h3 id="3-1-4-显示命令"><a href="#3-1-4-显示命令" class="headerlink" title="3.1.4 显示命令"></a>3.1.4 显示命令</h3><blockquote>
<p>show tables</p>
<p>show databases</p>
<p>show partitions</p>
<p>show functions</p>
<p>desc extended t_name;</p>
<p>desc formatted table_name;</p>
</blockquote>
<h2 id="3-2-DML"><a href="#3-2-DML" class="headerlink" title="3.2 DML"></a>3.2 DML</h2><h3 id="3-2-1-Load"><a href="#3-2-1-Load" class="headerlink" title="3.2.1 Load"></a>3.2.1 Load</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-17-032955.gif" alt="*">  <strong>语法结构</strong></p>
<p><code>LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]</code></p>
<p><strong>说明：</strong></p>
<ol>
<li>Load 操作只是单纯的复制/移动操作，将数据文件移动到 Hive 表对应的位置。</li>
<li>filepath：<ul>
<li>相对路径，例如：<code>project/data1</code></li>
<li>绝对路径，例如：<code>/user/hive/project/data1</code></li>
<li>包含模式的完整 URI，例如：<ul>
<li><code>hdfs://namenode:9000/user/hive/project/data1</code></li>
</ul>
</li>
</ul>
</li>
<li><p>LOCAL关键字</p>
<ul>
<li>如果指定了 LOCAL， load 命令会去查找本地文件系统中的 filepath。</li>
<li>如果没有指定 LOCAL 关键字，则根据inpath中的uri查找文件</li>
</ul>
</li>
<li><p>OVERWRITE 关键字</p>
<ul>
<li>如果使用了 OVERWRITE 关键字，则目标表（或者分区）中的内容会被删除，然后再将 filepath 指向的文件/目录中的内容添加到表/分区中。 </li>
<li>如果目标表（分区）已经有一个文件，并且文件名和 filepath 中的文件名冲突，那么现有的文件会被新文件所替代。 </li>
</ul>
</li>
</ol>
<h3 id="3-2-2-Insert"><a href="#3-2-2-Insert" class="headerlink" title="3.2.2 Insert"></a>3.2.2 Insert</h3><h4 id="语法结构"><a href="#语法结构" class="headerlink" title="语法结构"></a><strong>语法结构</strong></h4><ul>
<li><p>普通插入</p>
<ul>
<li>INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 …)] select_statement1 FROM from_statement </li>
</ul>
</li>
<li><p>Multiple inserts:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">FROM from_statement </span><br><span class="line">[<span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename1 [<span class="keyword">PARTITION</span> (partcol1=val1, partcol2=val2 ...)] select_statement1 ]</span><br><span class="line">[<span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename2 [<span class="keyword">PARTITION</span> ...] select_statement2] ...</span><br><span class="line"></span><br><span class="line"># 多重插入举例</span><br><span class="line"><span class="keyword">from</span> student</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> student_p <span class="keyword">partition</span>(part=<span class="string">'a'</span>)</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">where</span> <span class="keyword">id</span>&lt;<span class="number">95011</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> student_p <span class="keyword">partition</span>(part=<span class="string">'b'</span>)</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">where</span> <span class="keyword">id</span>&gt;<span class="number">95011</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Dynamic partition inserts:</p>
<ul>
<li>不指定分区字段, 按照 from 表的分区字段插入</li>
<li><code>INSERT OVERWRITE TABLE tablename PARTITION (partcol1, partcol2 ...) select_statement FROM from_statement</code></li>
</ul>
</li>
</ul>
<h4 id="导出表数据"><a href="#导出表数据" class="headerlink" title="导出表数据"></a>导出表数据</h4><p><strong>语法结构</strong></p>
<ul>
<li><p><code>INSERT OVERWRITE [LOCAL] DIRECTORY directory1 SELECT ... FROM ...</code></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 导出到本地</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/home/ap/test/stucent1'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student1;</span><br><span class="line"><span class="comment">--------------------</span></span><br><span class="line">例子:</span><br><span class="line">'查询学生信息，按性别分区，在分区内按年龄有序'</span><br><span class="line">0: jdbc:hive2://cs2:10000&gt; set mapred.reduce.tasks=2;</span><br><span class="line">No rows affected (0.015 seconds)</span><br><span class="line">0: jdbc:hive2://cs2:10000&gt; insert overwrite local directory '/home/ap/ihiveout'</span><br><span class="line">. . . . . . . . . . . . .&gt; select * from student distribute by Sex sort by Sage;</span><br><span class="line"><span class="comment">--------------------</span></span><br><span class="line"></span><br><span class="line"># 导出到 HDFS (仅仅是少了一个 local)</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">directory</span> <span class="string">'/test/stucent1'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student1;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>multiple inserts:</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">FROM from_statement</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE [<span class="keyword">LOCAL</span>] <span class="keyword">DIRECTORY</span> directory1 select_statement1</span><br><span class="line">[<span class="keyword">INSERT</span> OVERWRITE [<span class="keyword">LOCAL</span>] <span class="keyword">DIRECTORY</span> directory2 select_statement2] ...</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="4-Hive的数据模型"><a href="#4-Hive的数据模型" class="headerlink" title="4. Hive的数据模型"></a>4. Hive的数据模型</h1><h3 id="4-1-管理表-又称为内部表-受控表"><a href="#4-1-管理表-又称为内部表-受控表" class="headerlink" title="4.1 管理表 - 又称为内部表, 受控表"></a>4.1 管理表 - 又称为内部表, 受控表</h3><h4 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a><strong>基本操作</strong></h4><ul>
<li>创建数据文件inner_table.dat</li>
<li>创建表<ul>
<li><code>hive&gt;create table inner_table (key string);</code></li>
</ul>
</li>
<li><strong>加载数据</strong><ul>
<li><strong>加载本地数据</strong><ul>
<li><code>hive&gt;load data local inpath &#39;/root/inner_table.dat&#39; into table inner_table;</code></li>
</ul>
</li>
<li><strong>加载HDFS 上数据</strong><ul>
<li><code>hive&gt;load data inpath ‘xxx’ into table xxx;</code></li>
</ul>
</li>
<li><strong>区别</strong><ul>
<li>加载 hdfs 上的数据没有 <strong>local</strong></li>
<li><strong>加载本地数据是 copy 一份, 加载 hdfs 上的数据是直接移动数据到加载的表目录下– mv</strong></li>
</ul>
</li>
</ul>
</li>
<li>查看数据<ul>
<li><code>select * from inner_table</code></li>
<li><code>select count(*) from inner_table</code></li>
</ul>
</li>
<li><strong>删除表 <code>drop table inner_table</code></strong></li>
<li><strong>清空表 <code>truncate table table_name;</code></strong> </li>
<li><strong>注意: </strong><ul>
<li><strong>如果创建表的时候, 只指定了目录, 没有指定表名, 删除表的时候, 会把该目录下的所有表全部删掉</strong></li>
<li><code>hive (mydb)&gt; create table t2(id int)location &#39;/home/t2&#39;;</code></li>
</ul>
</li>
</ul>
<h4 id="内部表解释"><a href="#内部表解释" class="headerlink" title="内部表解释"></a><strong>内部表解释</strong></h4><ul>
<li>管理表，也称作<strong>内部表</strong>,受控表<ul>
<li>所有的 Table 数据（不包括 External Table）<strong>都保存在warehouse这个目录中。</strong></li>
<li><strong>删除表时，元数据与数据都会被删除</strong></li>
<li>创建过程和数据加载过程（这两个过程可以在同一个语句中完成），<strong>在加载数据的过程中，实际数据会被移动到数据仓库目录中</strong>；之后<strong>对数据对访问</strong>将会<strong>直接在数据仓库目录中</strong>完成。删除表时，表中的数据和元数据将会被同时删除</li>
</ul>
</li>
</ul>
<h4 id="内部表转为外部表-外部表转为内部表"><a href="#内部表转为外部表-外部表转为内部表" class="headerlink" title="内部表转为外部表,  外部表转为内部表"></a><strong>内部表转为外部表,  外部表转为内部表</strong></h4><ul>
<li><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">hive (mydb)&gt; create table t1(id int);</span><br><span class="line"></span><br><span class="line"># manage_table 转换为 外部表 external_table </span><br><span class="line">## 注意: 修改为外部表时, 后面2个都要大写</span><br><span class="line">hive (mydb)&gt; alter table t1 set tblproperties('EXTERNAL'='TRUE');</span><br><span class="line">## 修改为内部表</span><br><span class="line">hive (mydb)&gt; alter table t1 set tblproperties('EXTERNAL'='FALSE');</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 查看t1详细信息</span><br><span class="line">desc formatted t1;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">Location:           	hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1</span><br><span class="line">Table Type:         	EXTERNAL_TABLE</span><br><span class="line"><span class="comment">-- </span></span><br><span class="line"></span><br><span class="line"># 删除t1</span><br><span class="line">hive (mydb)&gt; drop table t1;</span><br><span class="line"># 此时再查看, 已经没了</span><br><span class="line">hive (mydb)&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">tab_name</span><br><span class="line"></span><br><span class="line"># 但是查看hdfs 路径会发现还在, 因为此表现在已经是外部表, 删除不会删除数据</span><br><span class="line">dfs -ls /user/hive/warehouse/mydb.db/t1</span><br><span class="line"></span><br><span class="line"># 如果此时再创建一个新表 t1, 表结构一样, 则数据会自动加载</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="4-2-※-外部表"><a href="#4-2-※-外部表" class="headerlink" title="4.2 ※ 外部表"></a>4.2 ※ 外部表</h3><h4 id="4-2-1基本操作"><a href="#4-2-1基本操作" class="headerlink" title="4.2.1基本操作"></a><strong>4.2.1基本操作</strong></h4><ul>
<li>创建数据文件external_table.dat</li>
<li>创建表<ul>
<li><code>hive&gt;create external table external_table1 (key string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39; location &#39;/home/external’;</code></li>
</ul>
</li>
<li>在HDFS创建目录/home/external<ul>
<li><code>#hadoop fs -put /home/external_table.dat /home/external</code></li>
<li>在工作中, 一般都这样使用, 把数据上传到 hdfs 中</li>
</ul>
</li>
<li>加载数据<ul>
<li><code>LOAD DATA  &#39;/home/external_table1.dat&#39; INTO TABLE external_table1;</code></li>
</ul>
</li>
<li>查看数据<ul>
<li><code>select * from external_table</code></li>
<li><code>select count(*) from external_table</code></li>
</ul>
</li>
<li>删除表 <ul>
<li><code>drop table external_table</code></li>
</ul>
</li>
</ul>
<h4 id="4-2-2-外部表解释"><a href="#4-2-2-外部表解释" class="headerlink" title="4.2.2 外部表解释"></a><strong>4.2.2 外部表解释</strong></h4><ul>
<li>包含External 的表叫外部表<ul>
<li>删除外部表只删除metastore的元数据，不删除hdfs中的表数据</li>
<li>外部表 只有一个过程，加载数据和创建表同时完成，并不会移动到数据仓库目录中，只是与外部数据建立一个链接。当删除一个 外部表 时，仅删除该链接</li>
<li>指向已经在 HDFS 中存在的数据，可以创建 Partition</li>
<li>它和 内部表 在元数据的组织上是相同的，而实际数据的存储则有较大的差异</li>
</ul>
</li>
</ul>
<h4 id="4-2-3-外部表语法"><a href="#4-2-3-外部表语法" class="headerlink" title="4.2.3 外部表语法"></a>4.2.3 外部表语法</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> page_view</span><br><span class="line">( viewTime <span class="built_in">INT</span>, </span><br><span class="line">  userid <span class="built_in">BIGINT</span>,</span><br><span class="line">  page_url <span class="keyword">STRING</span>, 	</span><br><span class="line"> referrer_url <span class="keyword">STRING</span>, 							</span><br><span class="line">  ip <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'IP Address of the User'</span>,</span><br><span class="line">  country <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'country of origination‘</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">    COMMENT '</span>This <span class="keyword">is</span> the staging page <span class="keyword">view</span> <span class="keyword">table</span><span class="string">'</span></span><br><span class="line"><span class="string">    ROW FORMAT DELIMITED FIELDS TERMINATED BY '</span>\t<span class="string">' 	LINES 	TERMINATED BY '</span>\n<span class="string">'</span></span><br><span class="line"><span class="string">    STORED AS TEXTFILE</span></span><br><span class="line"><span class="string">    LOCATION '</span>hdfs://centos:<span class="number">9000</span>/<span class="keyword">user</span>/<span class="keyword">data</span>/staging/page_view<span class="string">';</span></span><br></pre></td></tr></table></figure>
<h4 id="4-2-4外部表注意点"><a href="#4-2-4外部表注意点" class="headerlink" title="4.2.4外部表注意点:"></a>4.2.4外部表注意点:</h4><ul>
<li><p>先创建外部表/内部表, 表名为<code>t3</code>, 再往<code>t3</code>传对应字段的数据, 就可以直接 select 数据了</p>
</li>
<li><p><u>删除外部表之后, 原本数据不会删除</u>, 此时<strong>在相同的父路径</strong>创建与被删除表<strong>字段相同&amp;名称相同</strong>的<strong>内部/外部表</strong>, 数据<strong>也会直接加载</strong></p>
</li>
<li><p><strong>再看一个操作</strong></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 在 mydb.db 库下创建一个外部表 t5, 指定路径为 '/external/t5'</span><br><span class="line"># 此时在 mydb.db 库的路径下是不存在 t5表的, 而是存在 '/external/t5' 中</span><br><span class="line"># 但是使用 'show tables' 是存在 t5 的</span><br><span class="line">hive (mydb)&gt; create external table t5(id int) location '/external/t5';</span><br><span class="line"># 往此目录传数据, 注意: 此时传过去, intdata 数据存在 t5目录下</span><br><span class="line">[ap@cs2]~% hadoop fs -put intdata /external/t5</span><br><span class="line">[ap@cs2]~% hadoop fs -put intdata /external/t5/i2</span><br><span class="line"># 继续传数据, 查询的时候, 就是传的全部数据'相当于所有的数据都在 t5表中'</span><br><span class="line">[ap@cs2]~% hadoop fs -put intdata /external/t5</span><br><span class="line"># 注意: 如果传到 t5 目录下, 没有指定数据文件名的话, 会默认采用数据的名称文件.</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="4-3-※-分区表"><a href="#4-3-※-分区表" class="headerlink" title="4.3  ※ 分区表"></a>4.3  ※ 分区表</h3><h4 id="4-3-1-基本概念和操作"><a href="#4-3-1-基本概念和操作" class="headerlink" title="4.3.1 基本概念和操作"></a>4.3.1 基本概念和操作</h4><ul>
<li>分区可以理解为分类，通过分类把不同类型的数据放到不同的目录下。</li>
<li>分类的标准就是分区字段，可以一个，也可以多个。</li>
<li>分区表的意义在于优化查询。查询时尽量利用分区字段。如果不使用分区字段，就会全部扫描。</li>
</ul>
<blockquote>
<p><strong>创建分区表, 指定分区字段</strong></p>
<p><code>hive&gt;CREATE TABLE t3(...) PARTITIONED BY (province string);</code></p>
<ul>
<li>创建表的时候, 指定分区字段 key<code>province</code></li>
</ul>
<p><strong>为分区字段添加一个值</strong></p>
<p><code>hive&gt;ALTER TABLE t3 ADD [IF NOT EXISTS] PARTITION(...) LOCATION &#39;...’;</code></p>
<ul>
<li><code>alter table t3 add if not exists partition(province=&#39;hubei&#39;) partition(province=&#39;shanghai&#39;);</code></li>
<li><code>alter table t3 add if not exists partition(province=&#39;jiangsu&#39;);</code></li>
<li>可为此分区字段添加多个值,  为 province 添加 hubei, hunan….</li>
</ul>
<p><strong>查看表的分区字段&amp;值</strong></p>
<p><code>hive&gt;SHOW PARTITIONS t3 [partition (province=&#39;beijing&#39;)];</code></p>
<p><strong>删除分区</strong></p>
<p><code>hive&gt;ALTER TABLE t3 DROP PARTITION(province=‘beijing’.);</code></p>
<ul>
<li>这里是删除北京的分区 (如果是内部表, 会连数据一起删除)</li>
</ul>
<p><strong>设置表不能被删除/查询</strong>  ——– 这里报语法错误, :TODO</p>
<ul>
<li>防止分区被删除:<code>alter table student_p partition (part=&#39;aa&#39;) enable no_drop;</code></li>
<li>防止分区被查询:<code>alter table student_p partition (part=&#39;aa&#39;) enable offline;</code></li>
<li>enable 和 disable 是反向操作</li>
</ul>
<p><strong>其它一些相关命令</strong></p>
<p>SHOW TABLES; # 查看所有的表</p>
<p>SHOW TABLES ‘<em>TMP</em>‘; #支持模糊查询</p>
<p><code>SHOW PARTITIONS TMP_TABLE;</code> #查看表有哪些分区</p>
<p>DESC TMP_TABLE; #查看表结构</p>
</blockquote>
<h4 id="4-3-2-创建分区表完整语法"><a href="#4-3-2-创建分区表完整语法" class="headerlink" title="4.3.2 创建分区表完整语法"></a>4.3.2 创建分区表完整语法</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> tmp_table #表名</span><br><span class="line">(</span><br><span class="line">title   <span class="keyword">string</span>, # 字段名称 字段类型</span><br><span class="line">minimum_bid     <span class="keyword">double</span>,</span><br><span class="line">quantity        <span class="built_in">bigint</span>,</span><br><span class="line">have_invoice    <span class="built_in">bigint</span></span><br><span class="line">)<span class="keyword">COMMENT</span> <span class="string">'注释：XXX'</span> #表注释</span><br><span class="line"> PARTITIONED <span class="keyword">BY</span>(pt <span class="keyword">STRING</span>) #分区表字段（如果你文件非常之大的话，采用分区表可以快过滤出按分区字段划分的数据）</span><br><span class="line"> <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> </span><br><span class="line">   <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\001'</span>   # 字段是用什么分割开的</span><br><span class="line">STORED AS SEQUENCEFILE; #用哪种方式存储数据，SEQUENCEFILE是hadoop自带的文件压缩格式</span><br></pre></td></tr></table></figure>
<h4 id="4-3-3-分区表注意点-错误点"><a href="#4-3-3-分区表注意点-错误点" class="headerlink" title="4.3.3 分区表注意点(错误点)"></a>4.3.3 分区表注意点(错误点)</h4><ul>
<li><p><strong>1) 分区表在 load 数据的时候, 得指定分区, 否则会报错</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 错误1: 分区表在 load 数据的时候, 得指定分区</span><br><span class="line">hive (mydb)&gt; load data local inpath '~/ihivedata/intdata' into table t6;</span><br><span class="line">FAILED: SemanticException [Error 10062]: Need to specify partition columns because the destination table is partitioned</span><br><span class="line"></span><br><span class="line"># 错误2: 导入本地数据的时候, 'path'是从当前所在路径开始的</span><br><span class="line">hive (mydb)&gt; load data local inpath '~/ihivedata/intdata' into table t6 partition(class='job1');</span><br><span class="line">FAILED: SemanticException Line 1:23 Invalid path ''&lt;sub&gt;/ihivedata/intdata'': No files matching path file:/home/ap/&lt;/sub&gt;/ihivedata/intdata</span><br><span class="line"></span><br><span class="line"># 这里就正确了</span><br><span class="line">hive (mydb)&gt; load data local inpath 'ihivedata/intdata' into table t6 partition(class='job1');</span><br><span class="line">Loading data to table mydb.t6 partition (class=job1)</span><br><span class="line">OK</span><br></pre></td></tr></table></figure>
<ul>
<li><p><strong>本质原因:</strong> </p>
<ul>
<li><strong>分区表的分区, 就是在 hdfs 上, 原表的文件夹下面创建了一个子文件夹, 文件夹名就是分区名.</strong></li>
</ul>
<p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-14-113618.png" alt="image-20180614193618027"></p>
<ul>
<li><strong>从本地 load 数据:</strong> <code>hive (mydb)&gt; load data local inpath &#39;ihivedata/intdata&#39; into table t6 partition(class=&#39;job1&#39;);</code></li>
<li><strong>load 数据指定分区之后, 会直接 load 到数据文件夹里面</strong></li>
</ul>
<p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-14-113550.png" alt="image-20180614193549709"></p>
</li>
<li><p><strong>2) 没有添加分区时, 直接往不存在的分区导入数据, 分区会自动创建</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 直接往不存在的分区load数据, 分区会自动创建</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'ihivedata/intdata'</span> <span class="keyword">into</span> <span class="keyword">table</span> t6 <span class="keyword">partition</span>(<span class="keyword">class</span>=<span class="string">'job110'</span>);</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>3) 手动在表中创建分区(文件夹), 并直接向此文件夹中导入数据</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"># 直接创建目录</span><br><span class="line">hive (mydb)&gt; dfs -mkdir /user/hive/warehouse/mydb.db/t6/class=job120;</span><br><span class="line"></span><br><span class="line"># 直接从 hadoop 端传数据</span><br><span class="line">hadoop fs -put ihivedata/intdata /user/hive/warehouse/mydb.db/t6/class=job120</span><br><span class="line"></span><br><span class="line"># 此时再 show partitions t6; 会发现并没有此分区</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">partition</span><br><span class="line">class=job1</span><br><span class="line">class=job110</span><br><span class="line">class=job2</span><br><span class="line">class=job3</span><br><span class="line">class=job4</span><br><span class="line"></span><br><span class="line"># 此时就需要手动'激活'此分区, 加入了就有了</span><br><span class="line">hive (mydb)&gt; alter table t6 add partition(class='job120');</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">hive (mydb)&gt; show partitions t6;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">partition</span><br><span class="line">class=job1</span><br><span class="line">class=job110</span><br><span class="line">class=job120</span><br><span class="line">class=job2</span><br><span class="line">class=job3</span><br><span class="line">class=job4</span><br><span class="line"></span><br><span class="line"># 查看分区信息</span><br><span class="line">hive (mydb)&gt; select * from t6 where class='job110';</span><br><span class="line">OK</span><br><span class="line">t6.id	t6.class</span><br><span class="line">1	job110</span><br><span class="line">2	job110</span><br><span class="line">3	job110</span><br><span class="line">4	job110</span><br><span class="line">5	job110</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="4-3-4-复合分区"><a href="#4-3-4-复合分区" class="headerlink" title="4.3.4 复合分区"></a>4.3.4 复合分区</h4><h5 id="基本操作-1"><a href="#基本操作-1" class="headerlink" title="基本操作"></a>基本操作</h5><ul>
<li>创建数据文件<code>partition_table.dat</code></li>
<li>创建表<ul>
<li><code>create table t7(name string,age int)partitioned by(class string,city string)row format delimited fields terminated by &#39;\t&#39; stored as TEXTFILE;</code></li>
</ul>
</li>
<li><strong>在 Hive 下加载数据到分区</strong><ul>
<li><code>load data local inpath &#39;ihivedata/partidata&#39; into table t7 partition(class=&#39;job1&#39;,city=&#39;beijing&#39;);</code></li>
<li><code>load data local inpath &#39;ihivedata/partidata&#39; into table t7 partition(class=&#39;job1&#39;,city=&#39;shanghai&#39;);</code></li>
<li><code>load data local inpath &#39;ihivedata/partidata&#39; into table t7 partition(class=&#39;job2&#39;,city=&#39;ss&#39;);</code></li>
<li><strong>注意: 多级分区其实就是多级目录</strong><ul>
<li>越靠近左边, 目录层级越高; </li>
<li>越靠近右边, 目录层级越低; </li>
</ul>
</li>
<li><strong>load 数据到多级分区, load层级必须和整个层级数量相同</strong><ul>
<li>也就是说, 如果<strong>分区有2层</strong>, <strong>传数据</strong>的时候, 也<strong>必须传2层分区</strong>, 并且<strong>层级顺序必须一致</strong></li>
</ul>
</li>
</ul>
</li>
<li><strong>从Linux 本地直接导数据到分区</strong><ul>
<li><strong>可以直接在 hadoop UI 页面, 查看路径, 然后直接传到此路径中</strong></li>
<li><code>hadoop fs -put ihivedata/partidata /user/hive/warehouse/mydb.db/t7/class=job1/city=beijing/p2</code></li>
</ul>
</li>
<li>查看数据<ul>
<li><code>select * from partition_table</code></li>
<li><code>select count(*) from partition_table</code></li>
</ul>
</li>
<li>删除表<ul>
<li><code>drop table partition_table</code></li>
</ul>
</li>
<li>工作中 用的最多的是 <strong>外部表 + 分区表</strong> </li>
</ul>
<h3 id="4-4-桶表-主要用于抽样查询"><a href="#4-4-桶表-主要用于抽样查询" class="headerlink" title="4.4 桶表 - 主要用于抽样查询"></a>4.4 桶表 - 主要用于抽样查询</h3><h4 id="桶表的基本操作"><a href="#桶表的基本操作" class="headerlink" title="桶表的基本操作"></a>桶表的基本操作</h4><ul>
<li><strong>创建桶表完整过程</strong></li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#创建分桶表</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> stu_buck;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu_buck(Sno <span class="built_in">int</span>,Sname <span class="keyword">string</span>,Sex <span class="keyword">string</span>,Sage <span class="built_in">int</span>,Sdept <span class="keyword">string</span>)</span><br><span class="line">clustered <span class="keyword">by</span>(Sno) </span><br><span class="line">sorted <span class="keyword">by</span>(Sno <span class="keyword">DESC</span>)</span><br><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#设置变量,设置分桶为true, 设置reduce数量是分桶的数量个数</span><br><span class="line"><span class="keyword">set</span> hive.enforce.bucketing = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">4</span>;</span><br><span class="line"></span><br><span class="line">#开始往创建的分通表插入数据(插入数据需要是已分桶, 且排序的)</span><br><span class="line">#可以使用distribute by(sno) sort by(sno asc)   或是排序和分桶的字段相同的时候使用Cluster by(字段)</span><br><span class="line">#注意使用cluster by  就等同于分桶+排序(sort)</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_buck</span><br><span class="line"><span class="keyword">select</span> Sno,Sname,Sex,Sage,Sdept <span class="keyword">from</span> student <span class="keyword">distribute</span> <span class="keyword">by</span>(Sno) <span class="keyword">sort</span> <span class="keyword">by</span>(Sno <span class="keyword">asc</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> stu_buck</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student <span class="keyword">distribute</span> <span class="keyword">by</span>(Sno) <span class="keyword">sort</span> <span class="keyword">by</span>(Sno <span class="keyword">asc</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> stu_buck</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student cluster <span class="keyword">by</span>(Sno);</span><br><span class="line"><span class="comment">-----</span></span><br><span class="line">以上3者效果一样的</span><br></pre></td></tr></table></figure>
<ul>
<li><p><strong>保存select查询结果的几种方式：</strong></p>
<ul>
<li><p>将查询结果保存到一张新的hive表中</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_tmp</span><br><span class="line"><span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t_p;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<ol start="2">
<li><p>将查询结果保存到一张已经存在的hive表中</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span>  <span class="keyword">table</span> t_tmp</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t_p;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol start="3">
<li><p>将查询结果保存到指定的文件目录（可以是本地，也可以是hdfs）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/home/hadoop/test'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t_p;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">directory</span> <span class="string">'/aaa/test'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t_p;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ul>
<li><p><strong>数据加载到桶表时，会对字段取hash值，然后与桶的数量取模。把数据放到对应的文件中。</strong></p>
<ul>
<li>所以<strong>顺序是打乱的</strong>, 不是原始 t1的数据顺序</li>
</ul>
</li>
<li><p><strong>查看数据</strong></p>
<ul>
<li><p>可以直接select * 查看全部</p>
</li>
<li><p>也可以直接单独查看每个桶的数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hive (mydb)&gt; dfs -cat /user/hive/warehouse/mydb.db/bucket_table/000000_0;</span><br><span class="line">4</span><br><span class="line">hive (mydb)&gt; dfs -cat /user/hive/warehouse/mydb.db/bucket_table/000001_0;</span><br><span class="line">5</span><br><span class="line">1</span><br><span class="line">hive (mydb)&gt; dfs -cat /user/hive/warehouse/mydb.db/bucket_table/000002_0;</span><br><span class="line">2</span><br><span class="line">hive (mydb)&gt; dfs -cat /user/hive/warehouse/mydb.db/bucket_table/000003_0;</span><br><span class="line">3</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><strong>修改桶的个数</strong></p>
<ul>
<li><code>alter table bucket_table clustered by (id) sorted by(id) into 10 buckets;</code><ul>
<li>但是这样修改之后, 生成的是原来的 copy, 并且里面的数据也很奇怪, 不知道是按照什么来执行的? <strong>:TODO</strong></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>注意：</strong></p>
<ul>
<li><strong>物理上，每个桶就是表(或分区）目录里的一个文件</strong></li>
<li>一个作业产生的<strong>桶(输出文件)和reduce任务个数相同</strong></li>
</ul>
</li>
<li><p><strong>桶表工作中容易遇到的错误</strong></p>
<ul>
<li><p><strong>向桶表中插入其它表查出的数据的时候,  必须指定字段名</strong>, 否则会报字段不匹配.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">FAILED: SemanticException [Error 10044]: Line 1:12 Cannot <span class="keyword">insert</span> <span class="keyword">into</span> target <span class="keyword">table</span> because <span class="keyword">column</span> <span class="built_in">number</span>/types <span class="keyword">are</span> different <span class="string">'bucket_table'</span>: <span class="keyword">Table</span> insclause<span class="number">-0</span> has <span class="number">1</span> <span class="keyword">columns</span>, but <span class="keyword">query</span> has <span class="number">2</span> columns.</span><br><span class="line"></span><br><span class="line"># 应该是这样</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> bucket_table <span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> t6;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h4 id="※-桶表的抽样查询"><a href="#※-桶表的抽样查询" class="headerlink" title="※ 桶表的抽样查询"></a>※ 桶表的抽样查询</h4><ul>
<li>桶表的抽样查询<ul>
<li>select * from bucket_table tablesample<strong>(bucket 1 out of 4 on id);</strong></li>
</ul>
</li>
<li>tablesample是抽样语句<ul>
<li>语法解析：<strong>TABLESAMPLE(BUCKET x OUT OF y)</strong></li>
<li><strong>y必须是table总bucket数的<u>倍数</u>或者<u>因子</u>。</strong></li>
<li>hive根据y的大小，决定抽样的比例。</li>
<li><strong>例如</strong>，table总共分了64份，当y=32时，抽取(64/32=)2个bucket的数据，当y=128时，抽取(64/128=)1/2个bucket的数据。x表示从哪个bucket开始抽取。</li>
<li><strong>例如</strong>，table总bucket数为32，tablesample(bucket 3 out of 16)，表示总共抽取（32/16=）2个bucket的数据，分别为第3个bucket和第（3+16=）19个bucket的数据。</li>
</ul>
</li>
</ul>
<h1 id="5-Hive-视图的操作"><a href="#5-Hive-视图的操作" class="headerlink" title="5. Hive 视图的操作"></a>5. Hive 视图的操作</h1><ul>
<li>使用视图可以<strong>降低查询的复杂度</strong></li>
<li><strong>视图的创建</strong><ul>
<li><strong>create view</strong> v1 <strong>AS</strong> <u>select  t1.name from t1</u>;</li>
</ul>
</li>
<li><strong>视图的删除</strong><ul>
<li><strong>drop view</strong> if exists v1;</li>
</ul>
</li>
</ul>
<h1 id="6-Hive-索引的操作"><a href="#6-Hive-索引的操作" class="headerlink" title="6. Hive 索引的操作"></a>6. Hive 索引的操作</h1><ul>
<li><p><strong>创建索引</strong></p>
<ul>
<li><code>create index t1_index on table t1(id) as &#39;org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler&#39; with deferred rebuild in table t1_index_table;</code></li>
<li><code>t1_index</code>: <strong>索引名称</strong></li>
<li>as: 指定索引器</li>
<li><code>t1_index_table</code>: <strong>要创建的索引表</strong></li>
</ul>
</li>
<li><p><strong>显示索引</strong></p>
<ul>
<li><code>show formatted index on t1;</code></li>
</ul>
</li>
<li><p><strong>重建索引</strong></p>
<ul>
<li><p><strong>alter index</strong> t1_index <strong>on</strong> t1 <strong>rebuild</strong>;</p>
</li>
<li><p>重建完索引之后, 查看 t1_index_table 这张表, 就存了t1表文件具体的位置, 最后一列<code>t1_index_table._offsets</code>是 <strong>索引的偏移量, 类似于指针,  偏移量是索引的精髓</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (mydb)&gt; select * from t1_index_table;</span><br><span class="line">OK</span><br><span class="line">t1_index_table.id	t1_index_table._bucketname	t1_index_table._offsets</span><br><span class="line">1	hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata	[0]</span><br><span class="line">2	hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata	[2]</span><br><span class="line">3	hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata	[4]</span><br><span class="line">4	hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata	[6]</span><br><span class="line">5	hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata	[8]</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><strong>分区字段本质上其实就是索引</strong></p>
</li>
</ul>
<h1 id="7-装载数据"><a href="#7-装载数据" class="headerlink" title="7. 装载数据"></a>7. 装载数据</h1><h3 id="7-1-普通装载数据"><a href="#7-1-普通装载数据" class="headerlink" title="7.1 普通装载数据:"></a><strong>7.1 普通装载数据:</strong></h3><ul>
<li><strong>从本地</strong>  put</li>
<li><p><strong>从 hive</strong> cp</p>
</li>
<li><p><strong>从文件中装载数据</strong></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;LOAD DATA [LOCAL] INPATH '...' [OVERWRITE] INTO TABLE t2 [PARTITION (province='beijing')];</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>通过查询表装载数据</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 方式1</span><br><span class="line">hive&gt;INSERT OVERWRITE TABLE t2 PARTITION (province='beijing') SELECT * FROM xxx WHERE xxx;</span><br><span class="line"></span><br><span class="line"># 方式2</span><br><span class="line">hive&gt;FROM t4 </span><br><span class="line"> <span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> t3 <span class="keyword">PARTITION</span> (...) <span class="keyword">SELECT</span> ...WHERE... </span><br><span class="line"> <span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> t3 <span class="keyword">PARTITION</span> (...) <span class="keyword">SELECT</span> ...WHERE... </span><br><span class="line"> <span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> t3 <span class="keyword">PARTITION</span> (...) <span class="keyword">SELECT</span> ...WHERE...;</span><br><span class="line"> </span><br><span class="line"> # 方式3</span><br><span class="line"> 直接插入数据, 也会转化为文件的形式, 存在表的目录下</span><br><span class="line">- <span class="keyword">insert</span> <span class="keyword">into</span> table_name  <span class="keyword">values</span>(xxx); </span><br><span class="line"></span><br><span class="line"># 方式4</span><br><span class="line"> 直接传文件</span><br><span class="line"> - <span class="keyword">load</span> <span class="keyword">data</span> (<span class="keyword">local</span>) inpath ‘ xxx’ <span class="keyword">into</span> <span class="keyword">table</span> t_1;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="7-2动态装载数据"><a href="#7-2动态装载数据" class="headerlink" title="7.2动态装载数据"></a>7.2动态装载数据</h3><ul>
<li><p>不开启动态装载时</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;INSERT OVERWRITE TABLE t3 PARTITION(province='bj', city='bj') </span><br><span class="line"><span class="keyword">SELECT</span> t.province, t.city <span class="keyword">FROM</span> temp t <span class="keyword">WHERE</span> t.province=<span class="string">'bj'</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>开启动态分区支持</strong></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;set hive.exec.dynamic.partition=true;</span><br><span class="line">hive&gt;set hive.exec.dynamic.partition.mode=nostrict;</span><br><span class="line">hive&gt;set hive.exec.max.dynamic.partitions.pernode=1000;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>把 t6 表的所有的字段 (包括分区字段) 加载进 t9 对应的分区</strong></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (mydb)&gt; insert overwrite table t9 partition(class) select id,class from t6;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>单语句建表并同时装载数据</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;CREATE TABLE t4 AS SELECT ....</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="8-导出数据"><a href="#8-导出数据" class="headerlink" title="8. 导出数据"></a>8. 导出数据</h1><ul>
<li><strong>在hdfs之间复制文件(夹)</strong><ul>
<li><code>hadoop fs -cp source destination</code></li>
<li><code>hive&gt; dfs -cp source destination</code></li>
<li>案例: <code>hive&gt;  dfs -get  /user/hive/warehouse/mydb.db/t9  /root/t9;</code><ul>
<li>从 hdfs 复制到本地</li>
</ul>
</li>
</ul>
</li>
<li>使用DIRECTORY<ul>
<li><code>hive&gt;INSERT OVERWRITE 【LOCAL】 DIRECTORY &#39;...&#39; SELECT ...FROM...WHERE ...;</code></li>
<li>案例:通过查询导出到 t9, 走的 MapReduce<ul>
<li>导到到 hdfs:  <code>insert overwrite directory &quot;/home/t9&quot; select * from t9;</code></li>
<li>导出到本地: <code>insert overwrite local directory &quot;/home/ap/t9&quot; select * from t9;</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="9-读模式-amp-写模式"><a href="#9-读模式-amp-写模式" class="headerlink" title="9. 读模式&amp;写模式"></a>9. 读模式&amp;写模式</h1><ul>
<li>RDBMS是写模式</li>
<li>Hive是读模式</li>
</ul>
<h1 id="10-完整建表语句语法"><a href="#10-完整建表语句语法" class="headerlink" title="10. 完整建表语句语法"></a>10. 完整建表语句语法</h1><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db_name.]table_name</span><br><span class="line">  [(col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)]</span><br><span class="line">    [<span class="keyword">COMMENT</span> table_comment]</span><br><span class="line">    [PARTITIONED <span class="keyword">BY</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)]</span><br><span class="line">    [CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) </span><br><span class="line">    [SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span>|<span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS]</span><br><span class="line">    [SKEWED <span class="keyword">BY</span> (col_name, col_name, ...) <span class="keyword">ON</span> ([(col_value, col_value, ...), ...|col_value, col_value, ...]) </span><br><span class="line">    [<span class="keyword">STORED</span> <span class="keyword">AS</span> DIRECTORIES]   ]</span><br><span class="line">  [ [<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format] </span><br><span class="line">    [<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format] | <span class="keyword">STORED</span> <span class="keyword">BY</span> <span class="string">'storage.handler.class.name'</span> [<span class="keyword">WITH</span> SERDEPROPERTIES (...)]   ]</span><br><span class="line">  [LOCATION hdfs_path]</span><br><span class="line">  [TBLPROPERTIES (property_name=property_value, ...)]     </span><br><span class="line">  [<span class="keyword">AS</span> select_statement]  (Note: <span class="keyword">not</span> supported <span class="keyword">when</span> creating <span class="keyword">external</span> tables.)</span><br></pre></td></tr></table></figure>
<h1 id="11-文件格式"><a href="#11-文件格式" class="headerlink" title="11. 文件格式"></a>11. 文件格式</h1><ul>
<li>TextFile</li>
<li>SequenceFile</li>
<li>RCFile</li>
<li>ORC</li>
</ul>
<h3 id="11-1-默认存储格式就是-TextFile"><a href="#11-1-默认存储格式就是-TextFile" class="headerlink" title="11.1 默认存储格式就是 TextFile"></a>11.1 默认存储格式就是 TextFile</h3><ul>
<li>存储空间消耗比较大，</li>
<li>并且压缩的text 无法分割和合并</li>
<li>查询的效率最低,可以直接存储，</li>
<li>加载数据的速度最高</li>
</ul>
<h3 id="11-2-使用SequenceFile存储"><a href="#11-2-使用SequenceFile存储" class="headerlink" title="11.2 使用SequenceFile存储"></a>11.2 使用SequenceFile存储</h3><ul>
<li>存储空间消耗大</li>
<li>压缩的文件可以分割和合并 </li>
<li>查询效率高</li>
<li>需要通过text文件转化来加载</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test2(<span class="keyword">str</span> <span class="keyword">STRING</span>) <span class="keyword">STORED</span> <span class="keyword">AS</span> SEQUENCEFILE;  </span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> hive.exec.compress.output=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> mapred.output.compress=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> mapred.output.compression.codec=com.hadoop.compression.lzo.LzoCodec;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> io.seqfile.compression.type=<span class="keyword">BLOCK</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> io.compression.codecs=com.hadoop.compression.lzo.LzoCodec;</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> test2 <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> test1;</span><br></pre></td></tr></table></figure>
<p><strong>注意点: SequenceFile 类型的表, 不能直接导入数据文件,  只能通过从他表查询</strong></p>
<ul>
<li><p><code>insert overwrite table t2 select * from t1;</code></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 查看此 'SequenceFile' 表</span><br><span class="line">hive (db2)&gt; dfs -cat /user/hive/warehouse/db2.db/t2/000000_0 ;</span><br><span class="line">SEQ"org.apache.hadoop.io.BytesWritableorg.apache.hadoop.io.Text*org.apache.hadoop.io.compress.DefaultCodec���/*&lt;bb�m�?x�c453x�c464x�c475x�c486x�c497hive (db2)&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="11-3-使用RCFile存储"><a href="#11-3-使用RCFile存储" class="headerlink" title="11.3 使用RCFile存储"></a>11.3 使用RCFile存储</h3><p><strong>RCFILE是一种行列存储相结合的存储方式。首先，其将数据按行分块，保证同一个record在一个块上，避免读一个记录需要读取多个block。其次，块数据列式存储，有利于数据压缩和快速的列存取。</strong></p>
<ul>
<li>rcfile 存储空间最小</li>
<li>查询的效率最高</li>
<li>需要通过text文件转化来加载</li>
<li>加载的速度最低</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test3(<span class="keyword">str</span> <span class="keyword">STRING</span>)  <span class="keyword">STORED</span> <span class="keyword">AS</span> RCFILE;  </span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> hive.exec.compress.output=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> mapred.output.compress=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> mapred.output.compression.codec=com.hadoop.compression.lzo.LzoCodec;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> io.compression.codecs=com.hadoop.compression.lzo.LzoCodec;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> test3 <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> test1;</span><br></pre></td></tr></table></figure>
<p><strong>注意点:  RCFile 也只能从其它表导入数据</strong></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (db2)&gt; dfs -cat /user/hive/warehouse/db2.db/t3/000000_0;</span><br><span class="line">RCF*org.apache.hadoop.io.compress.DefaultCodechive.io.rcfile.column.number1</span><br></pre></td></tr></table></figure>
<h3 id="11-4-使用ORC存储-最好的一种格式"><a href="#11-4-使用ORC存储-最好的一种格式" class="headerlink" title="11.4 使用ORC存储(最好的一种格式)"></a>11.4 使用ORC存储(最好的一种格式)</h3><p><strong>是一种针对 RCFile 优化的格式</strong></p>
<p>主要特点: <strong>压缩, 索引, 单文件输出</strong></p>
<p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-15-070816.jpg" alt=""></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t1_orc(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span> <span class="keyword">stored</span> <span class="keyword">as</span> orc </span><br><span class="line"></span><br><span class="line">tblproperties(<span class="string">"orc.compress"</span>=<span class="string">"ZLIB"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> ... [<span class="keyword">PARTITION</span> partition_spec] <span class="keyword">SET</span> FILEFORMAT ORC;</span><br><span class="line"></span><br><span class="line"># 也可以改为其它的, 修改的语法就是这样</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> t1 <span class="keyword">set</span> fileformat textfile;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SET</span> hive.default.fileformat=Orc;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> t1_orc <span class="keyword">select</span> * <span class="keyword">from</span> t1;</span><br></pre></td></tr></table></figure>
<p><strong>注意点:  </strong></p>
<ul>
<li><strong>ORC 也只能从其它表导入数据</strong></li>
<li><strong>占用空间大, 一个 block 有256M, 之前2种都是128M</strong></li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive (db2)&gt; dfs -cat /user/hive/warehouse/db2.db/t4/000000_0;</span><br><span class="line">ORC</span><br><span class="line">P+</span><br><span class="line">P	�6�b�``���ь@�H�</span><br><span class="line">                       1q01-</span><br><span class="line">P</span><br><span class="line">PK</span><br><span class="line"># ("</span><br><span class="line">       id0P:</span><br><span class="line">P@�;��"</span><br><span class="line">       (0��ORChive</span><br></pre></td></tr></table></figure>
<h1 id="12-序列化-amp-反序列化-Hive-SerDe"><a href="#12-序列化-amp-反序列化-Hive-SerDe" class="headerlink" title="12. 序列化 &amp; 反序列化 (Hive SerDe)"></a>12. 序列化 &amp; 反序列化 (Hive SerDe)</h1><h3 id="12-1-SerDe"><a href="#12-1-SerDe" class="headerlink" title="12.1 SerDe"></a>12.1 SerDe</h3><ul>
<li><strong>What is a SerDe?</strong><ul>
<li>SerDe 是 “Serializer and Deserializer.”的缩写</li>
<li>Hive 使用 SerDe和FileFormat进行行内容的读写.</li>
</ul>
</li>
<li><strong>Hive序列化流程</strong><ul>
<li><strong>从 HDFS 上读入文件 (反序列化)</strong><ul>
<li><code>HDFS文件 --&gt; InputFileFormat --&gt; &lt;key, value&gt; --&gt; Deserializer --&gt; 行对象</code></li>
</ul>
</li>
<li><strong>写出到 HDFS (序列化)</strong><ul>
<li><code>行对象 --&gt; Serializer --&gt; &lt;key, value&gt; --&gt; OutputFileFormat --&gt; HDFS文件</code></li>
</ul>
</li>
<li>注意: <strong>数据全部存在在value中，key内容无意义</strong></li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>Hive 使用如下FileFormat 类读写 HDFS files:</strong></p>
<ul>
<li>TextInputFormat/HiveIgnoreKeyTextOutputFormat: 读写普通HDFS文本文件.</li>
<li>SequenceFileInputFormat/SequenceFileOutputFormat: 读写SequenceFile格式的HDFS文件</li>
<li>….</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>Hive 使用如下SerDe 类(反)序列化数据:</strong></p>
<ul>
<li>MetadataTypedColumnsetSerDe: 读写csv、tsv文件和默认格式文件</li>
<li>ThriftSerDe: 读写Thrift 序列化后的对象.</li>
<li>DynamicSerDe: 读写Thrift序列化后的对象, 不过不需要解读schema中的ddl.</li>
</ul>
</li>
</ul>
<h3 id="12-2-使用CSV-Serde"><a href="#12-2-使用CSV-Serde" class="headerlink" title="12.2 使用CSV Serde"></a>12.2 使用CSV Serde</h3><p>CSV格式的文件也称为逗号分隔值（Comma-Separated Values，CSV，有时也称为字符分隔值，因为分隔字符也可以不是逗号。在本文中的CSV格式的数据就不是简单的逗号分割的），其文件以纯文本形式存储表格数据（数字和文本）。CSV文件由任意数目的记录组成，记录间以某种换行符分隔；每条记录由字段组成，字段间的分隔符是其它字符或字符串，最常见的是逗号或制表符。通常，所有记录都有完全相同的字段序列。<br>默认的分隔符是</p>
<blockquote>
<p>DEFAULT_ESCAPE_CHARACTER \<br>DEFAULT_QUOTE_CHARACTER  “     —如果没有，则不需要指定<br>DEFAULT_SEPARATOR  , </p>
</blockquote>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> csv_table(a <span class="keyword">string</span>, b <span class="keyword">string</span>) <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> SERDE <span class="string">'org.apache.hadoop.hive.serde2.OpenCSVSerde'</span> <span class="keyword">WITH</span> SERDEPROPERTIES (<span class="string">"separatorChar"</span>=<span class="string">"\t"</span>, <span class="string">"quoteChar"</span>=<span class="string">"'"</span>, <span class="string">"escapeChar"</span>=<span class="string">"\\"</span>)  <span class="keyword">STORED</span> <span class="keyword">AS</span> TEXTFILE;</span><br><span class="line"></span><br><span class="line"># separatorChar：分隔符</span><br><span class="line"># quoteChar：引号符</span><br><span class="line"># escapeChar：转义符</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt;&gt;  :TODO 创建表没成功, 用到时再说</span><br></pre></td></tr></table></figure>
<h1 id="13-Lateral-View-语法"><a href="#13-Lateral-View-语法" class="headerlink" title="13. Lateral View 语法"></a>13. Lateral View 语法</h1><p>lateral view用于和split, explode等UDTF一起使用，它能够<strong>将一行数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合</strong>。lateral view首先为原始表的每行调用UDTF，UTDF会把一行拆分成一或者多行，lateral view再把结果组合，产生一个<strong>支持别名表的虚拟表</strong>。</p>
<p><strong>lateral: 侧面, 横切面</strong></p>
<p><strong>Lateral View: 切面表</strong></p>
<ul>
<li><p>创建表</p>
<ul>
<li><strong>create table t8(name string,nums array\&lt;int>)row format delimited fields terminated by “\t” COLLECTION ITEMS TERMINATED BY ‘:’;</strong></li>
</ul>
</li>
<li><p>数据切割</p>
<ul>
<li><strong>SELECT name,new_num FROM t8 LATERAL VIEW explode(nums) num AS new_num;</strong></li>
<li><code>select name,id from class_test lateral view explode(student_id_list) list as id;</code></li>
<li><strong>注意: as 前面的 <code>list</code> 貌似是可以随表起名的</strong></li>
</ul>
</li>
<li><p>效果演示</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from class_test;</span><br><span class="line">OK</span><br><span class="line">class_test.name	class_test.student_id_list</span><br><span class="line">zhangsan	[24,25,27,37]</span><br><span class="line">lisi	[28,39,23,43]</span><br><span class="line">wangwu	[25,23,2,54]</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">hive (default)&gt; select name,id from class_test lateral view explode(student_id_list) list as id;</span><br><span class="line">OK</span><br><span class="line">name	id</span><br><span class="line">zhangsan	24</span><br><span class="line">zhangsan	25</span><br><span class="line">zhangsan	27</span><br><span class="line">zhangsan	37</span><br><span class="line">lisi	28</span><br><span class="line">lisi	39</span><br><span class="line">lisi	23</span><br><span class="line">lisi	43</span><br><span class="line">wangwu	25</span><br><span class="line">wangwu	23</span><br><span class="line">wangwu	2</span><br><span class="line">wangwu	54</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="14-Hive的高级函数"><a href="#14-Hive的高级函数" class="headerlink" title="14. Hive的高级函数"></a>14. Hive的高级函数</h1><h3 id="14-1-简介"><a href="#14-1-简介" class="headerlink" title="14.1 简介"></a>14.1 简介</h3><ul>
<li>简单查询<ul>
<li>select … from…where…</li>
</ul>
</li>
<li><strong>使用各种函数</strong><ul>
<li><strong>hive&gt;show functions;</strong><ul>
<li>展示所有函数</li>
</ul>
</li>
<li><strong>hive&gt;describe function  xxx;</strong><ul>
<li>详细描述函数用法</li>
</ul>
</li>
</ul>
</li>
<li>LIMIT语句</li>
<li>列别名</li>
<li>嵌套select语句</li>
</ul>
<h3 id="14-2-高级函数分类"><a href="#14-2-高级函数分类" class="headerlink" title="14.2 高级函数分类"></a>14.2 高级函数分类</h3><ul>
<li><p>标准函数</p>
<ul>
<li>reverse()</li>
<li>upper()</li>
</ul>
</li>
<li><p>聚合函数</p>
<ul>
<li><p>avg()</p>
</li>
<li><p>sum()</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">avg</span>(<span class="keyword">id</span>) <span class="keyword">from</span> t3;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">sum</span>(<span class="keyword">id</span>) <span class="keyword">from</span> t3;</span><br><span class="line"></span><br><span class="line"># 最简答的嵌套</span><br><span class="line">hive (mydb)&gt; select t.id from (select * from t1 where id &gt; 3)t;</span><br><span class="line"></span><br><span class="line"># 最简单的 group by</span><br><span class="line"></span><br><span class="line"># if else 的效果</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, </span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> </span><br><span class="line"><span class="keyword">when</span> <span class="keyword">id</span> &lt;= <span class="number">2</span> </span><br><span class="line">	<span class="keyword">then</span> <span class="string">'low'</span></span><br><span class="line"><span class="keyword">when</span> <span class="keyword">id</span>&gt;=<span class="number">3</span> <span class="keyword">and</span> <span class="keyword">id</span> &lt;<span class="number">4</span> </span><br><span class="line">	<span class="keyword">then</span> <span class="string">'middle'</span></span><br><span class="line"><span class="keyword">when</span> <span class="keyword">id</span>&gt;=<span class="number">4</span> <span class="keyword">and</span> <span class="keyword">id</span> &lt;<span class="number">5</span> </span><br><span class="line">	<span class="keyword">then</span> <span class="string">'high'</span></span><br><span class="line"><span class="keyword">else</span> </span><br><span class="line">	<span class="string">'very high'</span></span><br><span class="line"><span class="keyword">end</span> </span><br><span class="line"></span><br><span class="line"><span class="keyword">as</span> </span><br><span class="line">id_highly </span><br><span class="line"><span class="keyword">from</span> t1;</span><br><span class="line"></span><br><span class="line"><span class="comment">----执行结果----</span></span><br><span class="line">id	id_highly</span><br><span class="line">1	low</span><br><span class="line">2	low</span><br><span class="line">3	middle</span><br><span class="line">4	high</span><br><span class="line">5	very high</span><br><span class="line"></span><br><span class="line"><span class="comment">--------------------</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">sid</span>,</span><br><span class="line"><span class="keyword">case</span> course</span><br><span class="line"><span class="keyword">when</span> <span class="string">'yuwen'</span> <span class="keyword">then</span> score</span><br><span class="line"><span class="keyword">else</span> <span class="string">'0'</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">(别名) yuwen</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> course</span><br><span class="line"><span class="keyword">when</span> <span class="string">'shuxue'</span> <span class="keyword">then</span> score</span><br><span class="line"><span class="keyword">else</span> <span class="string">'0'</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">(别名) shuxue</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># <span class="keyword">cast</span> 转换函数, 大概是这么用</span><br><span class="line">hive (mydb)&gt; <span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> t1 <span class="keyword">where</span> <span class="keyword">cast</span>(<span class="keyword">id</span> <span class="keyword">AS</span> <span class="built_in">FLOAT</span>) &lt;<span class="number">3.0</span>;</span><br><span class="line">OK</span><br><span class="line">id</span><br><span class="line">1</span><br><span class="line">2</span><br></pre></td></tr></table></figure>
<pre><code>- ![image-20180619150824031](http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-070824.png)
</code></pre></li>
</ul>
</li>
</ul>
<p><strong>首先当前不存在的列补0, 然后按照学号分组求和</strong></p>
<p>​    </p>
<p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-070624.png" alt="image-20180619150623836"></p>
<ul>
<li>然后按照 sid 分组求和/求最大值, 就可以了</li>
<li>同一列不同的放在不同的列上, 常用的方法</li>
</ul>
<p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-071819.png" alt="image-20180619151819409"></p>
<p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-072342.png" alt="image-20180619152341523"></p>
<p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-072352.png" alt="image-20180619152351866"></p>
<hr>
<p>面试题4</p>
<p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-075926.png" alt="image-20180619155925871"></p>
<p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-080134.png" alt="image-20180619160134008"></p>
<p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-080605.png" alt="image-20180619160604750"></p>
<p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-080942.png" alt="image-20180619160941777"></p>
<p>array_contains()</p>
<p>desc function array_contains()</p>
<ul>
<li><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-081706.jpg?ff=fh3" alt=""></li>
</ul>
<ul>
<li><p>自定义函数</p>
<ul>
<li>UDF</li>
</ul>
</li>
</ul>
<hr>
<p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-082423.png" alt="image-20180619162423281"></p>
<p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-084428.png" alt="image-20180619164428063"></p>
<p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-084655.png" alt="image-20180619164654514"></p>
<h1 id="15-Hive-性能调优"><a href="#15-Hive-性能调优" class="headerlink" title="15. Hive 性能调优"></a>15. Hive 性能调优</h1><h3 id="15-1-什么时候可以避免执行MapReduce？"><a href="#15-1-什么时候可以避免执行MapReduce？" class="headerlink" title="15.1 什么时候可以避免执行MapReduce？"></a>15.1 什么时候可以避免执行MapReduce？</h3><ul>
<li><p><code>select *  or  select field1,field2</code></p>
</li>
<li><p><code>limite 10</code></p>
</li>
<li><p>where语句中只有分区字段</p>
</li>
<li><p>使用本地<code>set hive.exec.mode.local.auto=true;</code></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t3;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,age <span class="keyword">from</span> t3;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,age <span class="keyword">from</span> t3 <span class="keyword">limit</span> <span class="number">2</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,age <span class="keyword">from</span> t3 <span class="keyword">where</span> age=<span class="number">25</span>;</span><br><span class="line"># 当 where 是本地字段(列中的字段), 是不走 MR的</span><br></pre></td></tr></table></figure>
</li>
<li><p>group by语句：</p>
<ul>
<li>通常和聚合函数一起使用，按照一个或者多个列对结果进行分组，然后对每组执行聚合操作</li>
</ul>
</li>
<li>having语句：<ul>
<li>限制结果的输出</li>
</ul>
</li>
<li>hive将查询转化为MapReduce执行，hive的优化可以转化为mapreduce的优化！</li>
</ul>
<h3 id="15-2-hive是如何将查询转化为MapReduce的？-EXPLAIN的使用"><a href="#15-2-hive是如何将查询转化为MapReduce的？-EXPLAIN的使用" class="headerlink" title="15.2 hive是如何将查询转化为MapReduce的？-EXPLAIN的使用"></a><strong>15.2 hive是如何将查询转化为MapReduce的？-EXPLAIN的使用</strong></h3><ul>
<li>hive对sql的查询计划信息解析</li>
<li><p>EXPLAIN SELECT COUNT(1) FROM T1;</p>
</li>
<li><p><strong>EXPLAIN EXTENDED</strong> </p>
<ul>
<li><p><strong>显示详细扩展查询计划信息</strong></p>
</li>
<li><p><strong><code>EXPLAIN EXTENDED SELECT COUNT(1) FROM T1;</code></strong></p>
</li>
<li><p>为啥我的 explain extended 只有固定的几行?</p>
<ul>
<li><strong>因为这个 count 没有调动 MR, 用 sum 就会启用 MR, 会出现长长的 log</strong></li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">---不会启用 MR</span></span><br><span class="line">hive (mydb)&gt; explain EXTENDED select count(1) from t9;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line"><span class="keyword">Explain</span></span><br><span class="line">STAGE DEPENDENCIES:</span><br><span class="line">  Stage<span class="number">-0</span> <span class="keyword">is</span> a root stage</span><br><span class="line">  ...</span><br><span class="line"><span class="comment">-----------------------------</span></span><br><span class="line"><span class="comment">---这里会启用 MR</span></span><br><span class="line">hive (mydb)&gt; <span class="keyword">explain</span> <span class="keyword">EXTENDED</span> <span class="keyword">select</span> <span class="keyword">sum</span>(<span class="keyword">id</span>) <span class="keyword">from</span> t9;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line"><span class="keyword">Explain</span></span><br><span class="line">STAGE DEPENDENCIES:</span><br><span class="line">  Stage<span class="number">-1</span> <span class="keyword">is</span> a root stage</span><br><span class="line">  Stage<span class="number">-0</span> depends <span class="keyword">on</span> stages: Stage<span class="number">-1</span></span><br><span class="line"></span><br><span class="line">STAGE PLANS:</span><br><span class="line">  Stage: Stage<span class="number">-1</span></span><br><span class="line">  ....</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h3 id="15-3-性能调优"><a href="#15-3-性能调优" class="headerlink" title="15.3 性能调优"></a>15.3 性能调优</h3><h4 id="15-3-1-本地mr"><a href="#15-3-1-本地mr" class="headerlink" title="15.3.1 本地mr"></a>15.3.1 本地mr</h4><ul>
<li><p><strong>本地模式设置方式：</strong></p>
<ul>
<li><code>set mapred.job.tracker=local;</code><ul>
<li>mapper 的本地模式, <strong>只有在开发中才会使用</strong></li>
</ul>
</li>
<li><strong><code>set hive.exec.mode.local.auto=true;</code></strong><ul>
<li><strong>Hive 的执行模式</strong></li>
<li><strong>可以用在生产中</strong>, 因为是自动模式, 可根据参数变化</li>
<li><strong>设置这里才会转成 local  hadoop</strong></li>
<li>按照这里设定的规则<code>hive.exec.mode.local.auto.input.files.max</code></li>
</ul>
</li>
<li>测试 select 1 from wlan limit 5;</li>
</ul>
</li>
<li><p><strong>下面两个参数是local mr中常用的控制参数:</strong></p>
<ul>
<li><p><code>hive.exec.mode.local.auto.inputbytes.max</code>默认134217728</p>
<ul>
<li><strong>设置local mr的最大输入数据量</strong>,当输入数据量小于这个值的时候会采用local  mr的方式</li>
</ul>
</li>
<li><p><strong><code>hive.exec.mode.local.auto.input.files.max</code>默认是4</strong></p>
<ul>
<li><p>设置local mr的最大输入文件个数,当输入文件个数小于这个值的时候会采用local mr的方式</p>
</li>
<li><p>大于此数时, 就不会转化为 local hadoop</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Cannot run job locally: Number of Input Files (= 6) is larger than hive.exec.mode.local.auto.input.files.max(= 4)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>可以这样修改local mr的最大输入文件个数值, 主要在调试阶段使用</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (mydb)&gt; set hive.exec.mode.local.auto.input.files.max=8;</span><br><span class="line"></span><br><span class="line"># 这样设置了之后, 只要文件数&lt;=8, 就会在本地运行</span><br><span class="line">Job running in-process (local Hadoop)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="15-3-2-开启并行计算"><a href="#15-3-2-开启并行计算" class="headerlink" title="15.3.2 开启并行计算"></a>15.3.2 开启并行计算</h4><ul>
<li>开启并行计算,增加集群的利用率<ul>
<li><code>set hive.exec.parallel=true;</code></li>
</ul>
</li>
</ul>
<h4 id="15-3-3-设置严格模式"><a href="#15-3-3-设置严格模式" class="headerlink" title="15.3.3 设置严格模式"></a>15.3.3 设置严格模式</h4><ul>
<li><strong>设置严格模式</strong><ul>
<li><code>set hive.mapred.mode=strict;</code></li>
</ul>
</li>
<li>设置非严格模式<ul>
<li><code>set hive.mapred.mode=nonstrict;</code></li>
</ul>
</li>
<li><strong>strict可以禁止三种类型的查询：</strong><ul>
<li>强制分区表的where条件过滤</li>
<li>Order by语句必须使用limit<ul>
<li><code>hive (mydb)&gt; select id from t9 where class=&#39;job110&#39; order by id limit 3;</code></li>
</ul>
</li>
<li>限制笛卡尔积查询</li>
</ul>
</li>
</ul>
<h4 id="15-3-4-调整mapper和reducer的数量"><a href="#15-3-4-调整mapper和reducer的数量" class="headerlink" title="15.3.4 调整mapper和reducer的数量"></a>15.3.4 调整mapper和reducer的数量</h4><ul>
<li>调整mapper和reducer的数量<ul>
<li>太多map导致启动产生过多开销</li>
<li><code>marpred.min.split.size</code></li>
<li>按照输入数据量大小确定reducer数目,<ul>
<li>`set mapred.reduce.tasks=  默认3</li>
<li>dfs -count  /分区目录/* </li>
<li>hive.exec.reducers.max设置阻止资源过度消耗</li>
</ul>
</li>
</ul>
</li>
<li>JVM重用<ul>
<li>小文件多或task多的业务场景</li>
<li>set mapred.job.reuse.jvm.num.task=10</li>
<li>会一直占用task槽</li>
</ul>
</li>
</ul>
<h4 id="15-3-5-排序方面的优化"><a href="#15-3-5-排序方面的优化" class="headerlink" title="15.3.5 排序方面的优化"></a>15.3.5 排序方面的优化</h4><ul>
<li><p><strong>order by</strong> 语句：是<strong>全局</strong>排序, 用的比较多</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 加个 desc 就是倒序排序</span><br><span class="line">hive (mydb)&gt; select id from bucket_table order by id desc limit 10;</span><br><span class="line">Automatically selecting local only mode for query</span><br></pre></td></tr></table></figure>
<p> <strong>sort by</strong> 语句：      是<strong>单reduce</strong>排序</p>
<ul>
<li>一般与 <code>distribute by</code>结合使用</li>
</ul>
</li>
<li><p><strong>distribute by</strong>语句：类似于分桶，<strong>根据指定的字段</strong>将<strong>数据分到不同的 reducer</strong>，且分发算法是 <strong>hash 散列</strong></p>
<ul>
<li><p><strong>与 sort by 结合用的比较多</strong>, <strong>在每个分区内有序</strong></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 注意此处 distribute 的作用</span><br><span class="line">hive (mydb)&gt; select id from bucket_table distribute by id sort by id desc limit 10;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><strong>cluster by</strong>语句：</p>
<ul>
<li><code>select * from t9 cluster by id;</code></li>
<li>可以确保类似的数据的分发到同一个reduce task中，并且保证数据有序, 防止所有的数据分发到同一个reduce上，导致整体的job时间延长</li>
</ul>
</li>
<li><p><strong>cluster by语句的等价语句：</strong></p>
<ul>
<li><em>如果分桶和sort字段是同一个时，此时，cluster by = distribute by + sort by</em> </li>
<li><code>distribute by Word sort by Word ASC</code></li>
</ul>
</li>
</ul>
<h4 id="15-3-6-Map-side聚合"><a href="#15-3-6-Map-side聚合" class="headerlink" title="15.3.6 Map-side聚合"></a>15.3.6 Map-side聚合</h4><p><strong>可以直接在 <code>.hiverc</code>中配置</strong></p>
<ul>
<li><code>set hive.map.aggr=true;</code></li>
<li>这个设置可以将顶层的聚合操作放在Map阶段执行，从而减轻清洗阶段数据传输和Reduce阶段的执行时间，提升总体性能。:TODO 不太懂</li>
<li>缺点：该设置会消耗更多的内存。</li>
<li>执行select count(1) from wlan;</li>
</ul>
<h4 id="15-3-6-Join-优化"><a href="#15-3-6-Join-优化" class="headerlink" title="15.3.6 Join 优化"></a>15.3.6 Join 优化</h4><ul>
<li>优先过滤后再进行 Join 操作，最大限度的减少参与 join 的数据量</li>
<li>小表 join 大表，最好启动 mapjoin<ul>
<li>在使用写有 Join 操作的查询语句时有一条原则:应该将条目少的表/子查询放在 Join 操作 符的左边。 </li>
</ul>
</li>
<li>Join on 的条件相同的话，最好放入同一个 job，并且 join 表的排列顺序从小到大</li>
</ul>
<blockquote>
<p>在编写 Join 查询语句时，如果确定是由于 join 出现的数据倾斜，那么请做如下设置:</p>
<p>set hive.skewjoin.key=100000; // 这个是 join 的键对应的记录条数超过这个值则会进行</p>
<p>分拆，值根据具体数据量设置</p>
<p>set hive.optimize.skewjoin=true; // 如果是 join 过程出现倾斜应该设置为 true</p>
</blockquote>
<h1 id="16-表连接-只支持等值连接"><a href="#16-表连接-只支持等值连接" class="headerlink" title="16. 表连接  (只支持等值连接)"></a>16. 表连接  (只支持等值连接)</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-16-071623.jpg" alt=""></p>
<h3 id="16-1-简介"><a href="#16-1-简介" class="headerlink" title="16.1 简介"></a>16.1 简介</h3><ul>
<li><p>INNER JOIN</p>
<ul>
<li>两张表中都有，且两表符合连接条件</li>
<li>select t1.name,t1.age,t9.age from t9 join t1 on t1.name=t9.name;</li>
</ul>
</li>
<li><p><strong>LEFT OUTER JOIN</strong></p>
<ul>
<li>左表中符合where条件出现，右表可以为空</li>
<li><strong>从左表返回所有的行(字段), 右表没有匹配where 条件的话返回null</strong></li>
</ul>
</li>
<li><p>RIGHT OUTER JOIN</p>
<ul>
<li>右表中符合where条件出现，左表可以为空</li>
</ul>
</li>
<li><p>FULL OUTER JOIN</p>
<ul>
<li>返回所有表符合where条件的所有记录，没有NULL替代</li>
</ul>
</li>
<li><p><strong>LEFT SEMI-JOIN</strong></p>
<ul>
<li><p>左表中符合右表on条件出现，右表不出现</p>
</li>
<li><p>Hive 当前<strong>没有实现 IN/EXISTS 子查询，可以用 LEFT SEMI JOIN 重写子查询语句</strong>。LEFT SEMI JOIN 的限制是， JOIN 子句中右边的表只能在 ON 子句中设置过滤条件，在 WHERE 子句、SELECT 子句或其他地方过滤都不行。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> a.key, a.value <span class="keyword">FROM</span> a</span><br><span class="line"><span class="keyword">WHERE</span> a.key <span class="keyword">in</span> (<span class="keyword">SELECT</span> b.key <span class="keyword">FROM</span> B);</span><br><span class="line"></span><br><span class="line"># 可以被重写为：</span><br><span class="line"><span class="keyword">SELECT</span> a.key, a.val <span class="keyword">FROM</span> a <span class="keyword">LEFT</span> SEMI <span class="keyword">JOIN</span> b <span class="keyword">on</span> (a.key = b.key)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>笛卡尔积</p>
<ul>
<li>是m x n的结果</li>
</ul>
</li>
<li><p><strong>map-side JOIN</strong></p>
<ul>
<li>只有一张小表，在mapper的时候将小表完全放在内存中</li>
<li>select /<em>+ mapjoin(t9) </em>/t1.name,t1.age from t9 JOIN t1on t1.name=t9.name;</li>
</ul>
</li>
</ul>
<h3 id="16-2-代码测试"><a href="#16-2-代码测试" class="headerlink" title="16.2  代码测试"></a>16.2  代码测试</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line">关于hive中的各种join</span><br><span class="line"></span><br><span class="line">准备数据</span><br><span class="line">1,a</span><br><span class="line">2,b</span><br><span class="line">3,c</span><br><span class="line">4,d</span><br><span class="line">7,y</span><br><span class="line">8,u</span><br><span class="line"></span><br><span class="line">2,bb</span><br><span class="line">3,cc</span><br><span class="line">7,yy</span><br><span class="line">9,pp</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">建表：</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> a(<span class="keyword">id</span> <span class="built_in">int</span>,<span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> b(<span class="keyword">id</span> <span class="built_in">int</span>,<span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br><span class="line"></span><br><span class="line">导入数据：</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/a.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> a;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/b.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> b;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">实验：</span><br><span class="line">** inner join</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> a <span class="keyword">inner</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.id=b.id;</span><br><span class="line">+<span class="comment">-------+---------+-------+---------+--+</span></span><br><span class="line">| a.id  | a.name  | b.id  | b.name  |</span><br><span class="line">+<span class="comment">-------+---------+-------+---------+--+</span></span><br><span class="line">| 2     | b       | 2     | bb      |</span><br><span class="line">| 3     | c       | 3     | cc      |</span><br><span class="line">| 7     | y       | 7     | yy      |</span><br><span class="line">+<span class="comment">-------+---------+-------+---------+--+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**left join</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> a <span class="keyword">left</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.id=b.id;</span><br><span class="line">+<span class="comment">-------+---------+-------+---------+--+</span></span><br><span class="line">| a.id  | a.name  | b.id  | b.name  |</span><br><span class="line">+<span class="comment">-------+---------+-------+---------+--+</span></span><br><span class="line">| 1     | a       | NULL  | NULL    |</span><br><span class="line">| 2     | b       | 2     | bb      |</span><br><span class="line">| 3     | c       | 3     | cc      |</span><br><span class="line">| 4     | d       | NULL  | NULL    |</span><br><span class="line">| 7     | y       | 7     | yy      |</span><br><span class="line">| 8     | u       | NULL  | NULL    |</span><br><span class="line">+<span class="comment">-------+---------+-------+---------+--+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**right join</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> a <span class="keyword">right</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.id=b.id;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> a <span class="keyword">full</span> <span class="keyword">outer</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.id=b.id;</span><br><span class="line">+<span class="comment">-------+---------+-------+---------+--+</span></span><br><span class="line">| a.id  | a.name  | b.id  | b.name  |</span><br><span class="line">+<span class="comment">-------+---------+-------+---------+--+</span></span><br><span class="line">| 1     | a       | NULL  | NULL    |</span><br><span class="line">| 2     | b       | 2     | bb      |</span><br><span class="line">| 3     | c       | 3     | cc      |</span><br><span class="line">| 4     | d       | NULL  | NULL    |</span><br><span class="line">| 7     | y       | 7     | yy      |</span><br><span class="line">| 8     | u       | NULL  | NULL    |</span><br><span class="line">| NULL  | NULL    | 9     | pp      |</span><br><span class="line">+<span class="comment">-------+---------+-------+---------+--+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> a <span class="keyword">left</span> semi <span class="keyword">join</span> b <span class="keyword">on</span> a.id = b.id;</span><br><span class="line">+<span class="comment">-------+---------+--+</span></span><br><span class="line">| a.id  | a.name  |</span><br><span class="line">+<span class="comment">-------+---------+--+</span></span><br><span class="line">| 2     | b       |</span><br><span class="line">| 3     | c       |</span><br><span class="line">| 7     | y       |</span><br><span class="line">+<span class="comment">-------+---------+--+</span></span><br></pre></td></tr></table></figure>
<p><br></p>
<h1 id="17-Hive自定义函数-amp-Transform"><a href="#17-Hive自定义函数-amp-Transform" class="headerlink" title="17.  Hive自定义函数 &amp; Transform"></a>17.  Hive自定义函数 &amp; Transform</h1><h3 id="17-1-自定义函数类别"><a href="#17-1-自定义函数类别" class="headerlink" title="17.1 自定义函数类别"></a>17.1 自定义函数类别</h3><ul>
<li><strong>UDF</strong>  <ul>
<li>作用于单个数据行，产生一个数据行作为输出。（数学函数，字符串函数）</li>
</ul>
</li>
<li><strong>UDAF</strong><ul>
<li>(用户定义聚集函数)：接收多个输入数据行，并产生一个输出数据行。（count，max）</li>
</ul>
</li>
</ul>
<h3 id="17-2-UDF-开发实例"><a href="#17-2-UDF-开发实例" class="headerlink" title="17.2 UDF 开发实例"></a>17.2 UDF 开发实例</h3><h4 id="17-2-1-简单入门"><a href="#17-2-1-简单入门" class="headerlink" title="17.2.1 简单入门"></a>17.2.1 简单入门</h4><ul>
<li><p>先开发一个java类，继承UDF，并重载evaluate方法 </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.hive.udf;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ToLowerCase</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// 必须是 public</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">evaluate</span><span class="params">(String field)</span> </span>&#123;</span><br><span class="line">		String res = field.toLowerCase();</span><br><span class="line">		<span class="keyword">return</span> res;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>打成jar包上传到服务器 </p>
</li>
<li><p>将jar包添加到hive的classpath </p>
<ul>
<li><code>hive&gt;add JAR /home/ap/udf.jar;</code></li>
</ul>
</li>
<li><p>创建临时函数与开发好的java class关联 </p>
<ul>
<li><code>Hive&gt;create temporary function tolowercase as &#39;com.rox.hive.udf.ToLowerCase&#39;;</code></li>
</ul>
</li>
<li><p>即可在hql中使用自定义的函数strip  </p>
<ul>
<li><code>select tolowercase(name) from t_1..</code></li>
</ul>
</li>
</ul>
<h4 id="17-2-2-稍稍复杂"><a href="#17-2-2-稍稍复杂" class="headerlink" title="17.2.2 稍稍复杂"></a>17.2.2 稍稍复杂</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"># 需求: 通过一些手机号判断手机区域</span><br><span class="line">1364535532,10</span><br><span class="line">1374535532,42</span><br><span class="line">1384535532,34</span><br><span class="line">1364535532,45</span><br><span class="line">1384535532,22</span><br><span class="line">136-beijing</span><br><span class="line">137-shanghai</span><br><span class="line">138-guangzhou</span><br><span class="line"><span class="comment">-----</span></span><br><span class="line"></span><br><span class="line">## 1.编写 UDF</span><br><span class="line">	public static HashMap&lt;String, String&gt; provinceMap = new HashMap&lt;String, String&gt;();</span><br><span class="line">	static &#123;</span><br><span class="line">		provinceMap.put("136", "beijing");</span><br><span class="line">		provinceMap.put("136", "shanghai");</span><br><span class="line">		provinceMap.put("136", "guangzhou");</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	public String evaluate(String phoneNum) &#123;</span><br><span class="line">		return provinceMap.get(phoneNum.substring(0, 3)) == null ? "huoxing" : provinceMap.get(phoneNum.substring(0, 3));</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">## 2.打包上传, 添加到 classpath, 创建临时函数</span><br><span class="line"></span><br><span class="line">## 3.创建表,加载数据</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> flow_t(pnum <span class="keyword">string</span>,flow <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/ap/ihivedata/flow.tmp'</span> <span class="keyword">into</span> <span class="keyword">table</span> flow_t;</span><br><span class="line"></span><br><span class="line">## 4.使用</span><br><span class="line">0: jdbc:hive2://cs2:10000&gt; select pnum,tolow(pnum),flow from flow_t;</span><br><span class="line">+<span class="comment">-------------+------------+-------+</span></span><br><span class="line">|    pnum     |    _c1     | flow  |</span><br><span class="line">+<span class="comment">-------------+------------+-------+</span></span><br><span class="line">| 1364535532  | guangzhou  | 10    |</span><br><span class="line">| 1374535532  | huoxing    | 42    |</span><br><span class="line">| 1384535532  | huoxing    | 34    |</span><br><span class="line">| 1364535532  | guangzhou  | 45    |</span><br><span class="line">| 1384535532  | huoxing    | 22    |</span><br><span class="line">+<span class="comment">-------------+------------+-------+</span></span><br></pre></td></tr></table></figure>
<h4 id="17-2-3-有些复杂"><a href="#17-2-3-有些复杂" class="headerlink" title="17.2.3  有些复杂"></a>17.2.3  有些复杂</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"># 解析 json 数据表 rating.json</span><br><span class="line">## 1. 写 udf</span><br><span class="line"></span><br><span class="line"><span class="comment">// com.rox.json.MovieRateBean </span></span><br><span class="line"><span class="keyword">package</span> com.rox.json;</span><br><span class="line"><span class="keyword">import</span> lombok.Getter;</span><br><span class="line"><span class="keyword">import</span> lombok.Setter;</span><br><span class="line"><span class="meta">@Getter</span></span><br><span class="line"><span class="meta">@Setter</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MovieRateBean</span> </span>&#123;</span><br><span class="line">	<span class="keyword">private</span> String movie;</span><br><span class="line">	<span class="keyword">private</span> String rate;</span><br><span class="line">	<span class="keyword">private</span> String timeStamp;</span><br><span class="line">	<span class="keyword">private</span> String uid;</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> movie + <span class="string">"\t"</span> + rate + <span class="string">"\t"</span></span><br><span class="line">				+ timeStamp + <span class="string">"\t"</span> + uid;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// com.rox.json.JsonParser </span></span><br><span class="line"><span class="keyword">package</span> com.rox.json;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"><span class="keyword">import</span> com.fasterxml.jackson.databind.ObjectMapper;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JsonParser</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">evaluate</span><span class="params">(String jsonLine)</span> </span>&#123;</span><br><span class="line">		ObjectMapper objectMapper = <span class="keyword">new</span> ObjectMapper();</span><br><span class="line">		<span class="keyword">try</span> &#123;</span><br><span class="line">			MovieRateBean bean = objectMapper.readValue(jsonLine, MovieRateBean.class);</span><br><span class="line">			<span class="keyword">return</span> bean.toString();</span><br><span class="line">		&#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">			e.printStackTrace();</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">return</span> <span class="string">""</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line">====================================================</span><br><span class="line">## 2.打包上传, 添加到 classpath, 创建临时函数, 检查是否成功</span><br><span class="line">show functions;</span><br><span class="line"></span><br><span class="line">## 3.创建表,加载数据</span><br><span class="line"><span class="function">create table <span class="title">t_json</span><span class="params">(line string)</span> row format delimited</span>;</span><br><span class="line"></span><br><span class="line">load data local inpath <span class="string">'/home/ap/ihivedata/flow.log'</span> into table t_json;</span><br><span class="line"></span><br><span class="line">## 4.检查数据</span><br><span class="line">select * from t_json limit <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line">## 5.调用函数</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="comment">//cs2:10000&gt; select jsonparser(line)parsedline  from t_json limit 10;</span></span><br><span class="line">+---------------------+</span><br><span class="line">|     parsedline      |</span><br><span class="line">+---------------------+</span><br><span class="line">| <span class="number">1193</span>	<span class="number">5</span>	<span class="number">978300760</span>	<span class="number">1</span>  |</span><br><span class="line">| <span class="number">661</span>	<span class="number">3</span>	<span class="number">978302109</span>	<span class="number">1</span>   |</span><br><span class="line">| <span class="number">914</span>	<span class="number">3</span>	<span class="number">978301968</span>	<span class="number">1</span>   |</span><br><span class="line">| <span class="number">3408</span>	<span class="number">4</span>	<span class="number">978300275</span>	<span class="number">1</span>  |</span><br><span class="line">| <span class="number">2355</span>	<span class="number">5</span>	<span class="number">978824291</span>	<span class="number">1</span>  |</span><br><span class="line">| <span class="number">1197</span>	<span class="number">3</span>	<span class="number">978302268</span>	<span class="number">1</span>  |</span><br><span class="line">| <span class="number">1287</span>	<span class="number">5</span>	<span class="number">978302039</span>	<span class="number">1</span>  |</span><br><span class="line">| <span class="number">2804</span>	<span class="number">5</span>	<span class="number">978300719</span>	<span class="number">1</span>  |</span><br><span class="line">| <span class="number">594</span>	<span class="number">4</span>	<span class="number">978302268</span>	<span class="number">1</span>   |</span><br><span class="line">| <span class="number">919</span>	<span class="number">4</span>	<span class="number">978301368</span>	<span class="number">1</span>   |</span><br><span class="line">+---------------------+</span><br><span class="line"><span class="comment">// 但是这样只是把每一行解析出来了,</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">## 6.删除原来的表    </span><br><span class="line">drop table <span class="keyword">if</span> exists t_rating;</span><br><span class="line"></span><br><span class="line">## 7.重新创建一个表</span><br><span class="line"><span class="function">create table <span class="title">t_rating</span><span class="params">(movieid string,rate <span class="keyword">int</span>,timestring string,uid string)</span></span></span><br><span class="line"><span class="function">row format delimited fields terminated by '\t'</span>;</span><br><span class="line"></span><br><span class="line">## 8.根据查出来的每一行, 按照 '\t'分割, 然后再插入到表中</span><br><span class="line">create table  t_rating as</span><br><span class="line">select </span><br><span class="line">split(jsonparser(line),<span class="string">'\t'</span>)[<span class="number">0</span>]as movieid, split(jsonparser(line),<span class="string">'\t'</span>)[<span class="number">1</span>] as rate, split(jsonparser(line),<span class="string">'\t'</span>)[<span class="number">2</span>] as timestring, split(jsonparser(line),<span class="string">'\t'</span>)[<span class="number">3</span>] as uid   </span><br><span class="line">from t_json limit <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 但是执行结果会报错,不知道为啥,难道是 java 代码的问题? :TODO</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">-------</span><br><span class="line">## 9.内置json函数</span><br><span class="line"><span class="function">select <span class="title">get_json_object</span><span class="params">(line,<span class="string">'$.movie'</span>)</span> as moive,<span class="title">get_json_object</span><span class="params">(line,<span class="string">'$.rate'</span>)</span> as rate  from rat_json limit 10</span>;</span><br></pre></td></tr></table></figure>
<h4 id="17-3-Transform实现"><a href="#17-3-Transform实现" class="headerlink" title="17.3 Transform实现"></a>17.3 Transform实现</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"># 1、先加载rating.json文件到hive的一个原始表 t_json</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> rat_json(line <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/ap/rating.json'</span> <span class="keyword">into</span> <span class="keyword">table</span> t_json;</span><br><span class="line"></span><br><span class="line">2、需要解析json数据成四个字段，插入一张新的表 t_rating</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> t_rating;</span><br><span class="line"># 创建表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_rating(movieid <span class="keyword">string</span>,rate <span class="built_in">int</span>,timestring <span class="keyword">string</span>,uid <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"># 插入, 也可以直接创建  create table xx as + select...</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> t_rating</span><br><span class="line"><span class="keyword">select</span> get_json_object(line,<span class="string">'$.movie'</span>) <span class="keyword">as</span> moiveid, get_json_object(line,<span class="string">'$.rate'</span>) <span class="keyword">as</span> rate, get_json_object(line,<span class="string">'$.timeStamp'</span>) <span class="keyword">as</span> timestring,</span><br><span class="line">get_json_object(line,<span class="string">'$.uid'</span>) <span class="keyword">as</span> uid </span><br><span class="line"><span class="keyword">from</span> t_json;</span><br><span class="line"></span><br><span class="line">3. 写一个 python 脚本</span><br><span class="line">vi weekday_mapper.py</span><br><span class="line"></span><br><span class="line">#!/bin/python</span><br><span class="line">import sys</span><br><span class="line">import datetime</span><br><span class="line"></span><br><span class="line">for line in sys.stdin:</span><br><span class="line">  line = line.strip()</span><br><span class="line">  movieid, rating, timestring,userid = line.split('\t')</span><br><span class="line">  weekday = datetime.datetime.fromtimestamp(float(timestring)).isoweekday()</span><br><span class="line">  print '\t'.join([movieid, rating, str(weekday),userid])</span><br><span class="line">  </span><br><span class="line">4. 保存文件, 然后将文件加入 hive 的 classpath</span><br><span class="line">hive&gt;add FILE /home/hadoop/weekday_mapper.py;</span><br><span class="line"></span><br><span class="line">5. 此时可以直接创建新表</span><br><span class="line">hive&gt;create TABLE u_data_new as</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  TRANSFORM (movieid, rate, timestring,uid)</span><br><span class="line">  <span class="keyword">USING</span> <span class="string">'python weekday_mapper.py'</span></span><br><span class="line">  <span class="keyword">AS</span> (movieid, rate, <span class="keyword">weekday</span>,uid)</span><br><span class="line"><span class="keyword">FROM</span> t_rating;</span><br><span class="line"></span><br><span class="line">6. 查询结果</span><br><span class="line">## distinct看看有多少个不重复的数字</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">distinct</span>(<span class="keyword">weekday</span>) <span class="keyword">from</span> u_data_new <span class="keyword">limit</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure>
<h1 id="18-案例"><a href="#18-案例" class="headerlink" title="18. 案例"></a>18. 案例</h1><h3 id="18-1-广告推送-用户画像的介绍"><a href="#18-1-广告推送-用户画像的介绍" class="headerlink" title="18.1 广告推送-用户画像的介绍"></a>18.1 广告推送-用户画像的介绍</h3><p><strong>一个广告推送平台的项目结构示意图</strong></p>
<p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-17-071714.png" alt="image-20180617151714445"></p>
<h3 id="18-2-累计报表实现套路-面试"><a href="#18-2-累计报表实现套路-面试" class="headerlink" title="18.2 累计报表实现套路(面试)"></a>18.2 累计报表实现套路(面试)</h3><p><strong>题:  求每个月的累计访问次数</strong></p>
<p><strong>此处的访问次数可以换成工资,等等..</strong></p>
<p><strong>有如下访客访问次数统计表 t_access_times</strong></p>
<table>
<thead>
<tr>
<th>访客</th>
<th>月份</th>
<th>访问次数</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>2015-01</td>
<td>5</td>
</tr>
<tr>
<td>A</td>
<td>2015-01</td>
<td>15</td>
</tr>
<tr>
<td>B</td>
<td>2015-01</td>
<td>5</td>
</tr>
<tr>
<td>A</td>
<td>2015-01</td>
<td>8</td>
</tr>
<tr>
<td>B</td>
<td>2015-01</td>
<td>25</td>
</tr>
<tr>
<td>A</td>
<td>2015-01</td>
<td>5</td>
</tr>
<tr>
<td>A</td>
<td>2015-02</td>
<td>4</td>
</tr>
<tr>
<td>A</td>
<td>2015-02</td>
<td>6</td>
</tr>
<tr>
<td>B</td>
<td>2015-02</td>
<td>10</td>
</tr>
<tr>
<td>B</td>
<td>2015-02</td>
<td>5</td>
</tr>
<tr>
<td>……</td>
<td>……</td>
<td>……</td>
</tr>
</tbody>
</table>
<p><strong>需要输出报表：t_access_times_accumulate</strong></p>
<table>
<thead>
<tr>
<th>访客</th>
<th>月份</th>
<th>月访问总计</th>
<th>累计访问总计</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>2015-01</td>
<td>33</td>
<td>33</td>
</tr>
<tr>
<td>A</td>
<td>2015-02</td>
<td>10</td>
<td>43</td>
</tr>
<tr>
<td>…….</td>
<td>…….</td>
<td>…….</td>
<td>…….</td>
</tr>
<tr>
<td>B</td>
<td>2015-01</td>
<td>30</td>
<td>30</td>
</tr>
<tr>
<td>B</td>
<td>2015-02</td>
<td>15</td>
<td>45</td>
</tr>
<tr>
<td>…….</td>
<td>…….</td>
<td>…….</td>
<td>…….</td>
</tr>
</tbody>
</table>
<h5 id="解题代码"><a href="#解题代码" class="headerlink" title="解题代码"></a>解题代码</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 准备数据</span><br><span class="line">vi t_access_times</span><br><span class="line"></span><br><span class="line">A,2015-01,5</span><br><span class="line">A,2015-01,15</span><br><span class="line">B,2015-01,5</span><br><span class="line">A,2015-01,8</span><br><span class="line">B,2015-01,254</span><br><span class="line">A,2015-01,5</span><br><span class="line">A,2015-02,4</span><br><span class="line">A,2015-02,6</span><br><span class="line">B,2015-02,10</span><br><span class="line">B,2015-02,5</span><br><span class="line"><span class="comment">----</span></span><br><span class="line"></span><br><span class="line"># 建表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_access_times(username <span class="keyword">string</span>,<span class="keyword">month</span> <span class="keyword">string</span>,salary <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br><span class="line"></span><br><span class="line"># 加载数据</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/ap/t_access_times'</span> <span class="keyword">into</span> <span class="keyword">table</span> t_access_times;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">1、第一步，先求个用户的月总金额</span><br><span class="line"><span class="keyword">select</span> username,<span class="keyword">month</span>,<span class="keyword">sum</span>(salary) <span class="keyword">as</span> salary <span class="keyword">from</span> t_access_times <span class="keyword">group</span> <span class="keyword">by</span> username,<span class="keyword">month</span>;</span><br><span class="line"></span><br><span class="line">+<span class="comment">-----------+----------+---------+--+</span></span><br><span class="line">| username  |  month   | salary  |</span><br><span class="line">+<span class="comment">-----------+----------+---------+--+</span></span><br><span class="line">| A         | 2015-01  | 33      |</span><br><span class="line">| A         | 2015-02  | 10      |</span><br><span class="line">| B         | 2015-01  | 30      |</span><br><span class="line">| B         | 2015-02  | 15      |</span><br><span class="line">+<span class="comment">-----------+----------+---------+--+</span></span><br><span class="line"></span><br><span class="line">2、第二步，将月总金额表 自己连接 自己连接</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> aa <span class="keyword">as</span></span><br><span class="line">(<span class="keyword">select</span> username,<span class="keyword">month</span>,<span class="keyword">sum</span>(salary) <span class="keyword">as</span> salary <span class="keyword">from</span> t_access_times <span class="keyword">group</span> <span class="keyword">by</span> username,<span class="keyword">month</span>) A </span><br><span class="line"><span class="keyword">inner</span> <span class="keyword">join</span> </span><br><span class="line">(<span class="keyword">select</span> username,<span class="keyword">month</span>,<span class="keyword">sum</span>(salary) <span class="keyword">as</span> salary <span class="keyword">from</span> t_access_times <span class="keyword">group</span> <span class="keyword">by</span> username,<span class="keyword">month</span>) B</span><br><span class="line">A.username=B.username</span><br><span class="line"></span><br><span class="line">+<span class="comment">-------------+----------+-----------+-------------+----------+-----------+--+</span></span><br><span class="line">| a.username  | a.month  | a.salary  | b.username  | b.month  | b.salary  |</span><br><span class="line">+<span class="comment">-------------+----------+-----------+-------------+----------+-----------+--+</span></span><br><span class="line">| A           | <span class="number">2015</span><span class="number">-01</span>  | <span class="number">33</span>        | A           | <span class="number">2015</span><span class="number">-01</span>  | <span class="number">33</span>        |</span><br><span class="line">| A           | <span class="number">2015</span><span class="number">-01</span>  | <span class="number">33</span>        | A           | <span class="number">2015</span><span class="number">-02</span>  | <span class="number">10</span>        |</span><br><span class="line">| A           | <span class="number">2015</span><span class="number">-02</span>  | <span class="number">10</span>        | A           | <span class="number">2015</span><span class="number">-01</span>  | <span class="number">33</span>        |</span><br><span class="line">| A           | <span class="number">2015</span><span class="number">-02</span>  | <span class="number">10</span>        | A           | <span class="number">2015</span><span class="number">-02</span>  | <span class="number">10</span>        |</span><br><span class="line">| B           | <span class="number">2015</span><span class="number">-01</span>  | <span class="number">30</span>        | B           | <span class="number">2015</span><span class="number">-01</span>  | <span class="number">30</span>        |</span><br><span class="line">| B           | <span class="number">2015</span><span class="number">-01</span>  | <span class="number">30</span>        | B           | <span class="number">2015</span><span class="number">-02</span>  | <span class="number">15</span>        |</span><br><span class="line">| B           | <span class="number">2015</span><span class="number">-02</span>  | <span class="number">15</span>        | B           | <span class="number">2015</span><span class="number">-01</span>  | <span class="number">30</span>        |</span><br><span class="line">| B           | <span class="number">2015</span><span class="number">-02</span>  | <span class="number">15</span>        | B           | <span class="number">2015</span><span class="number">-02</span>  | <span class="number">15</span>        |</span><br><span class="line">+<span class="comment">-------------+----------+-----------+-------------+----------+-----------+--+</span></span><br><span class="line"></span><br><span class="line"><span class="number">3</span>、第三步，从上一步的结果中</span><br><span class="line">进行分组查询，分组的字段是a.username a.month</span><br><span class="line">求月累计值：  将b.month &lt;= a.month的所有b.salary求和即可</span><br><span class="line"><span class="keyword">select</span> A.username,A.month,<span class="keyword">max</span>(A.salary) <span class="keyword">as</span> salary,<span class="keyword">sum</span>(B.salary) <span class="keyword">as</span> accumulate</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">(<span class="keyword">select</span> username,<span class="keyword">month</span>,<span class="keyword">sum</span>(salary) <span class="keyword">as</span> salary <span class="keyword">from</span> t_access_times <span class="keyword">group</span> <span class="keyword">by</span> username,<span class="keyword">month</span>) A </span><br><span class="line"><span class="keyword">inner</span> <span class="keyword">join</span> </span><br><span class="line">(<span class="keyword">select</span> username,<span class="keyword">month</span>,<span class="keyword">sum</span>(salary) <span class="keyword">as</span> salary <span class="keyword">from</span> t_access_times <span class="keyword">group</span> <span class="keyword">by</span> username,<span class="keyword">month</span>) B</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">A.username=B.username</span><br><span class="line"><span class="keyword">where</span> B.month &lt;= A.month</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> A.username,A.month</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> A.username,A.month;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">+<span class="comment">-------------+----------+---------+-------------+</span></span><br><span class="line">| a.username  | a.month  | salary  | accumulate  |</span><br><span class="line">+<span class="comment">-------------+----------+---------+-------------+</span></span><br><span class="line">| A           | 2015-01  | 33      | 33          |</span><br><span class="line">| A           | 2015-02  | 10      | 43          |</span><br><span class="line">| B           | 2015-01  | 259     | 259         |</span><br><span class="line">| B           | 2015-02  | 15      | 274         |</span><br><span class="line">+<span class="comment">-------------+----------+---------+-------------+</span></span><br></pre></td></tr></table></figure>
<h3 id="18-3-待做项目"><a href="#18-3-待做项目" class="headerlink" title="18.3 待做项目"></a>18.3 待做项目</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-17-082606.png" alt="image-20180617162606289"></p>
<h1 id="19-注意点"><a href="#19-注意点" class="headerlink" title="19.注意点"></a>19.注意点</h1><ol>
<li><strong>使用聚合函数, 后面一定要分组(group by xxx),  group by 会自动去重</strong><ul>
<li>如果 sql 语句中有 group by, 那么 select 后面必须有 group by 的字段, 或聚合函数</li>
</ul>
</li>
<li><strong>使用order by 排序某个字段时, 必须在 select中出现此字段, 否则会找不到 :TODO</strong><ul>
<li>待验证</li>
</ul>
</li>
</ol>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>如果帮到你, 可以给我赞助杯咖啡☕️</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/uploads/images/wechat-reward.jpg" alt="airpoet 微信支付"/>
        <p>微信支付</p>
      </div>
    

    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/原创/" rel="tag"># 原创</a>
          
            <a href="/tags/技术/" rel="tag"># 技术</a>
          
            <a href="/tags/Hadoop/" rel="tag"># Hadoop</a>
          
            <a href="/tags/Hive/" rel="tag"># Hive</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/06/12/Hadoop/2-MapReduce/MapReduce-简单总结/" rel="next" title="MapReduce-简单总结">
                <i class="fa fa-chevron-left"></i> MapReduce-简单总结
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/06/14/Hadoop/3-Hive/Hive学习-2/" rel="prev" title="Hive学习—2">
                Hive学习—2 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-14-045043.jpg"
                alt="airpoet" />
            
              <p class="site-author-name" itemprop="name">airpoet</p>
              <p class="site-description motion-element" itemprop="description">没有边界就没有自由.</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">112</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">41</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">40</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://www.jianshu.com/u/3b9b97fe7e04" target="_blank" title="我的简书">
                      
                        <i class="fa fa-fw fa-globe"></i>我的简书</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:airpoet@qq.com" target="_blank" title="Mile2Me">
                      
                        <i class="fa fa-fw fa-envelope"></i>Mile2Me</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友情链接
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.jianshu.com/u/8ee264c6557d" title="女神凉姨" target="_blank">女神凉姨</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-Hive-初探"><span class="nav-text">1.  Hive 初探</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-Hive-的数据存储"><span class="nav-text">1.1 Hive 的数据存储</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-Hive的系统架构"><span class="nav-text">1.2 Hive的系统架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-Hive的系统架构"><span class="nav-text">1.3 Hive的系统架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-Hive的metastore"><span class="nav-text">1.4 Hive的metastore</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-Hive-和-Hadoop-的调用关系"><span class="nav-text">1.5 Hive 和 Hadoop 的调用关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-6-Hive-参数配置使用"><span class="nav-text">1.6 Hive 参数配置使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-7-Hive-的脚本执行"><span class="nav-text">1.7 Hive 的脚本执行</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-8-hive与依赖环境的交互"><span class="nav-text">1.8 hive与依赖环境的交互</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-9-Hive-的-JDBC-模式"><span class="nav-text">1.9 Hive 的 JDBC 模式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-10-SET命令使用"><span class="nav-text">1.10 SET命令使用</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-Hive数据类型"><span class="nav-text">2.  Hive数据类型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-基本数据类型"><span class="nav-text">2.1 基本数据类型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-复合数据类型"><span class="nav-text">2.2 复合数据类型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-1-Struct-使用"><span class="nav-text">2.2.1.  Struct 使用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-2-Array-使用"><span class="nav-text">2.2.2. Array 使用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-3-Map-使用"><span class="nav-text">2.2.3. Map 使用</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-DDL-DML"><span class="nav-text">3. DDL , DML</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-DDL"><span class="nav-text">3.1 DDL</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-1-数据库定义"><span class="nav-text">3.1.1  数据库定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-2-表定义-修改"><span class="nav-text">3.1.2  表定义/修改</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-3-列定义"><span class="nav-text">3.1.3 列定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-4-显示命令"><span class="nav-text">3.1.4 显示命令</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-DML"><span class="nav-text">3.2 DML</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-1-Load"><span class="nav-text">3.2.1 Load</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2-Insert"><span class="nav-text">3.2.2 Insert</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#语法结构"><span class="nav-text">语法结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#导出表数据"><span class="nav-text">导出表数据</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-Hive的数据模型"><span class="nav-text">4. Hive的数据模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-管理表-又称为内部表-受控表"><span class="nav-text">4.1 管理表 - 又称为内部表, 受控表</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基本操作"><span class="nav-text">基本操作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#内部表解释"><span class="nav-text">内部表解释</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#内部表转为外部表-外部表转为内部表"><span class="nav-text">内部表转为外部表,  外部表转为内部表</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-※-外部表"><span class="nav-text">4.2 ※ 外部表</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-1基本操作"><span class="nav-text">4.2.1基本操作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-2-外部表解释"><span class="nav-text">4.2.2 外部表解释</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-3-外部表语法"><span class="nav-text">4.2.3 外部表语法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-4外部表注意点"><span class="nav-text">4.2.4外部表注意点:</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-※-分区表"><span class="nav-text">4.3  ※ 分区表</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-1-基本概念和操作"><span class="nav-text">4.3.1 基本概念和操作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-2-创建分区表完整语法"><span class="nav-text">4.3.2 创建分区表完整语法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-3-分区表注意点-错误点"><span class="nav-text">4.3.3 分区表注意点(错误点)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-4-复合分区"><span class="nav-text">4.3.4 复合分区</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#基本操作-1"><span class="nav-text">基本操作</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-桶表-主要用于抽样查询"><span class="nav-text">4.4 桶表 - 主要用于抽样查询</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#桶表的基本操作"><span class="nav-text">桶表的基本操作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#※-桶表的抽样查询"><span class="nav-text">※ 桶表的抽样查询</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-Hive-视图的操作"><span class="nav-text">5. Hive 视图的操作</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6-Hive-索引的操作"><span class="nav-text">6. Hive 索引的操作</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#7-装载数据"><span class="nav-text">7. 装载数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-普通装载数据"><span class="nav-text">7.1 普通装载数据:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2动态装载数据"><span class="nav-text">7.2动态装载数据</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#8-导出数据"><span class="nav-text">8. 导出数据</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#9-读模式-amp-写模式"><span class="nav-text">9. 读模式&amp;写模式</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#10-完整建表语句语法"><span class="nav-text">10. 完整建表语句语法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#11-文件格式"><span class="nav-text">11. 文件格式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#11-1-默认存储格式就是-TextFile"><span class="nav-text">11.1 默认存储格式就是 TextFile</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-2-使用SequenceFile存储"><span class="nav-text">11.2 使用SequenceFile存储</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-3-使用RCFile存储"><span class="nav-text">11.3 使用RCFile存储</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-4-使用ORC存储-最好的一种格式"><span class="nav-text">11.4 使用ORC存储(最好的一种格式)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#12-序列化-amp-反序列化-Hive-SerDe"><span class="nav-text">12. 序列化 &amp; 反序列化 (Hive SerDe)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#12-1-SerDe"><span class="nav-text">12.1 SerDe</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-2-使用CSV-Serde"><span class="nav-text">12.2 使用CSV Serde</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#13-Lateral-View-语法"><span class="nav-text">13. Lateral View 语法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#14-Hive的高级函数"><span class="nav-text">14. Hive的高级函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#14-1-简介"><span class="nav-text">14.1 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-2-高级函数分类"><span class="nav-text">14.2 高级函数分类</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#15-Hive-性能调优"><span class="nav-text">15. Hive 性能调优</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#15-1-什么时候可以避免执行MapReduce？"><span class="nav-text">15.1 什么时候可以避免执行MapReduce？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-2-hive是如何将查询转化为MapReduce的？-EXPLAIN的使用"><span class="nav-text">15.2 hive是如何将查询转化为MapReduce的？-EXPLAIN的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-3-性能调优"><span class="nav-text">15.3 性能调优</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#15-3-1-本地mr"><span class="nav-text">15.3.1 本地mr</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#15-3-2-开启并行计算"><span class="nav-text">15.3.2 开启并行计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#15-3-3-设置严格模式"><span class="nav-text">15.3.3 设置严格模式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#15-3-4-调整mapper和reducer的数量"><span class="nav-text">15.3.4 调整mapper和reducer的数量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#15-3-5-排序方面的优化"><span class="nav-text">15.3.5 排序方面的优化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#15-3-6-Map-side聚合"><span class="nav-text">15.3.6 Map-side聚合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#15-3-6-Join-优化"><span class="nav-text">15.3.6 Join 优化</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#16-表连接-只支持等值连接"><span class="nav-text">16. 表连接  (只支持等值连接)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#16-1-简介"><span class="nav-text">16.1 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#16-2-代码测试"><span class="nav-text">16.2  代码测试</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#17-Hive自定义函数-amp-Transform"><span class="nav-text">17.  Hive自定义函数 &amp; Transform</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#17-1-自定义函数类别"><span class="nav-text">17.1 自定义函数类别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#17-2-UDF-开发实例"><span class="nav-text">17.2 UDF 开发实例</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#17-2-1-简单入门"><span class="nav-text">17.2.1 简单入门</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#17-2-2-稍稍复杂"><span class="nav-text">17.2.2 稍稍复杂</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#17-2-3-有些复杂"><span class="nav-text">17.2.3  有些复杂</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#17-3-Transform实现"><span class="nav-text">17.3 Transform实现</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#18-案例"><span class="nav-text">18. 案例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#18-1-广告推送-用户画像的介绍"><span class="nav-text">18.1 广告推送-用户画像的介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#18-2-累计报表实现套路-面试"><span class="nav-text">18.2 累计报表实现套路(面试)</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#解题代码"><span class="nav-text">解题代码</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#18-3-待做项目"><span class="nav-text">18.3 待做项目</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#19-注意点"><span class="nav-text">19.注意点</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">airpoet</span>

  
</div>









        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      本站访客数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人次
    </span>
  

  
    <span class="site-pv">
      本站总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    
      <div id="needsharebutton-float">
        <span class="btn">
          <i class="fa fa-share-alt" aria-hidden="true"></i>
        </span>
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://A.P.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'https://airpoet.github.io/2018/06/13/Hadoop/3-Hive/i-Hive-1/';
          this.page.identifier = '2018/06/13/Hadoop/3-Hive/i-Hive-1/';
          this.page.title = 'i-Hive-1';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://A.P.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  
  
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css">

  
  
  <script src="/lib/needsharebutton/needsharebutton.js"></script>

  <script>
    
    
      flOptions = {};
      
          flOptions.iconStyle = "box";
      
          flOptions.boxForm = "horizontal";
      
          flOptions.position = "middleRight";
      
          flOptions.networks = "Weibo,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-float', flOptions);
    
  </script>

  

  

  
  <script type="text/javascript" src="/js/src/js.cookie.js?v=5.1.4"></script>
  <script type="text/javascript" src="/js/src/scroll-cookie.js?v=5.1.4"></script>


  

</body>
</html>
