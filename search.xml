<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Redis]]></title>
    <url>%2F2018%2F07%2F03%2FHadoop%2F8_Redis%2FRedis%2F</url>
    <content type="text"><![CDATA[1.安装1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192931. 下载2. tar3. cd redis-3.xxxx/deps4. make geohash-int hiredis jemalloc linenoise lua-----------------5. cd .. 退回到 redis 根目录$redis-3.xxxx&gt; make test 等待几分钟, 等待编译完成会出现 `编译完成没有错误`的英文... 然后直接 make 6. 接下来就是执行 make install 安装了启动等脚本了 默认会自动安装到 /usr/local/binmake 后面跟上 PREFIX=path 可以自己指定安装路径这里安装到 ~/apps/ 下的 redis 目录中$&gt; sudo make PREFIX=/home/ap/apps/redis install7. # 复制配置文件到redis安装目录$redis-3.xxxx&gt; sudo cp redis.conf /home/ap/apps/redis&gt; vi redis.conf-------------------------# 修改 redis-server 后台启动(也可以不配, 用 nohup的方式后台启动)daemonize yes# 修改 protected-mode 为 noprotected-mode no# 注释掉 bind 本机端口# bind 127.0.0.1 ::1# 配置环境变量export REDIS_HOME=/home/ap/apps/redisexport PATH=$PATH:$REDIS_HOME/bin# 把环境变量 .zshrc 和 存有 sh 命令的 redis 安装目录发送到其它机器# source 要使用机器的 .zshrc配置8.  启动试试看~# 如果配置了后台启动, 并且加载了配置文件的话就不会出现以下图形了 $&gt; redis-server ~/apps/redis/redis.conf 也可以直接 $&gt; redis-server 这样就不会加载配置文件, 会出现下面的.. _._ _.-``__ ''-._ _.-`` `. `_. ''-._ Redis 4.0.10 (00000000/0) 64 bit .-`` .-```. ```\/ _.,_ ''-._ ( ' , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|'` _.-'| Port: 6379 | `-._ `._ / _.-' | PID: 1034 `-._ `-._ `-./ _.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | http://redis.io `-._ `-._`-.__.-'_.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | `-._ `-._`-.__.-'_.-' _.-' `-._ `-.__.-' _.-' `-._ _.-' `-.__.-'出现一个这个吊的图像, 就说明启动成功了.....# 也可以这样后台启动, 带 lognohup &lt;sub&gt;/apps/redis/bin/redis-server &lt;/sub&gt;/apps/redis/redis.conf 1&gt;&lt;sub&gt;/logs/redis_log/redis_std.log 2&gt;&lt;/sub&gt;/logs/redis_log/redis_error.log &amp;7、启动客户端，执行命令：[hadoop@hadoop02 ~]$ redis-cli#  查看是否有进程ps -ef | grep redis --------[ap@cs1]~% ps -ef | grep redisap 36260 35955 0 14:11 pts/3 00:00:00 redis-server *:6379ap 36287 36267 0 14:11 pts/5 00:00:00 grep redis&gt;&gt; 最开始的 36260 就是 pid9.  连接# redis/bin/redis-cli [-h localhost -p 6379 ]# 如果是本机启动客户端$&gt; redis-cli# 如果是从其他节点上链接 redis，那么可以这么做：$cs2&gt; redis-cli -h cs1 -p 6379cs1:6379&gt; pingPONG10. 停止服务cli端: shutdown save|nosave 2.Redis 注意点REmote DIctionary Server(Redis)Redis 常被称作是一款 key-value 内 存存储系统或者内存数据库，同时由于它支持丰富的数据结构，又被称为一种数据结构服务 器（Data Structure Server）。 因为值（value）可以是字符串(String)，哈希(Map)，列表(list)， 集合(sets)和有序集合(sorted sets)等类型。 与其他 key-value 缓存产品比较独有特点1、Redis 支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载 进行使用。 2、Redis 不仅仅支持简单的 key-value 类型的数据，同时还提供 list，set，zset，hash 等数据 结构的存储。 3、Redis 支持数据的备份，即 master-slave 模式的数据备份。 Redis优势1、性能极高：Redis 能读的速度是 110000 次/s，写的速度是 81000 次/s 。 2、丰富的数据类型：Redis 支持二进制案例的 String, List, Hash, Set 及 Sorted Set 数据类型操 作。 3、原子操作：Redis 的所有操作都是原子性的，同时 Redis 还支持对几个操作全并后的原子 性执行。 4、丰富的特性：Redis 还支持 Publish/Subscribe，通知 key 过期，支持高可用集群等等特性。 5、数据持久化机制 持久化机制有两种：1、RDB 方式：定期将内存数据 dump 到磁盘 2、AOF(append only file)持久化机制：用记日志的方式记录每一条数据更新操作，一旦 出现灾难事件，可以通过日志重放来恢复整个数据库 Redis适用场景1、TopN 需求：取最新的 n 个数据，如读取作家博客最新的 50 篇文章，通过 List 实现按时 间排序的数据的高效获取 2、排行榜应用：以特定条件为排序标准，将其设成 sorted set 的 score，进而实现高效获取 3、需要精准设定过期时间的应用：把 sorted set 的 score 值设置成过期时间的时间戳，那么 就可以简单地通过过期时间排序，定时清除过期数据了 4、计数器应用：Redis 的命令都是原子性的，可以轻松地利用 INCR，DECR 命令来构建计数 器系统。 5、去除大量数据中的重复数据：将数据放入 set 中，就能实现对重复数据的排除 6、构建队列系统：使用 list 可以构建队列系统，使用 sorted set 甚至可以构建有优先级的队 列系统。 7、实时系统，反垃圾系统：通过上面说到的 set 功能，你可以知道一个终端用户是否进行 了某个操作，可以找到其操作的集合并进行分析统计对比等。 8、Publish/SubScribe 构建实时消息系统 9、缓存（会话，商品列表，评论列表，经常查询的数据等） 3.Redis基本操作具体见文档 也可以查询网站 3.1 字符串3.1.1 String shellString 类型是二进制安全的。意思是 Redis 的 String 可以包含任何数据。比如 jpg 图片或者序 列化的对象 。 String 类型是 Redis 最基本的数据类型，一个键最大能存储 512MB。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243如果给定了 NX 选项，那么命令仅在键 key 不存在的情况下，才进行设置操作；如果键 key 已经存 在，那么 SET ... NX 命令不做动作（不会覆盖旧值）。===&gt; not exist如果给定了 XX 选项，那么命令仅在键 key 已经存在的情况下，才进行设置操作；如果键 key 不存 在，那么 SET ... XX 命令不做动作（一定会覆盖旧值）。# 简单存取----------------------------------------------------------------------复杂度为 O(1) 。redis&gt; SET msg "hello world" OKredis&gt; GET msghello world# 删除del key1 key2 ... Keyn--------------------------------------------------------------------作用: 删除1个或多个键返回值: 不存在的key忽略掉,返回真正删除的key的数量# 给key 设置新值rename key newkey------------------------------------------------------------------------作用: 给key赋一个新的key名注:如果newkey已存在,则newkey的原值被覆盖# multi 存取----------------------------------------------------------------------# 中间的分隔符是自己指定的127.0.0.1:6379&gt; mset ss::name "airpoet" ss::age "a8" ss::like "ycy"OK127.0.0.1:6379&gt; mget ss::name ss::age ss::like1) "airpoet"2) "a8"3) "ycy"127.0.0.1:6379&gt; mset ss.age "18" ss.teacher "noone"OK127.0.0.1:6379&gt; mget ss.age ss::age1) "18"2) "a8"# msetnx----------------------------------------------------------------------只有在所有给定键都不存在的情况下， MSETNX 会为所有给定键设置值，效果和同时执行多个 SETNX 一样。如果给定的键至少有一个是存在的 ，那么 MSETNX 将不执行任何设置操作。redis&gt; MSETNX nx-1 "hello" nx-2 "world" nx-3 "good luck" 1redis&gt; SET ex-key "bad key here"OKredis&gt; MSETNX nx-4 "apple" nx-5 "banana" ex-key "cherry" nx-6 "durian" 0# 设置新值并返回旧值GETSET key new-value------------------------------------------------------------------------------------# 先设置redis&gt; SET getset-str "i'm old value"OK# 设置新的值, 返回旧值redis&gt; GETSET getset-str "i'm new value" i'm old value# 重新 get 一下, 发现已是新值redis&gt; GET getset-str i'm new value#追加内容到字符串末尾APPEND key value------------------------------------------------------------------------------------redis&gt; SET myPhone "nokia" OKredis&gt; APPEND myPhone "-1110" (integer) 10redis&gt; GET myPhone "nokia-1110"#返回值的长度------------------------------------------------------------------------------------redis&gt; SET msg "hello" OK redis&gt; STRLEN msg (integer) 5 redis&gt; APPEND msg " world" (integer) 11 redis&gt; STRLEN msg (integer) 11==========================================================================================# 索引-------------------------------------------------------------------------------------$ 如果字符串长度为 n从左到右&gt;&gt; 0 ~ n-1从右到做&gt;&gt; -1 ~ -n # 范围设置SETRANGE key index value-------------------------------------------------------------------------------------从索引 index 开始，用 value 覆写(overwrite)给定键 key 所储存的字符串值。只接受正数索引。redis&gt; SET msg "hello"OKredis&gt; SETRANGE msg 1 "appy" (integer) 5redis&gt; GET msg "happy"# 范围取值GETRANGE key start end-------------------------------------------------------------------------------------返回键 key 储存的字符串值中，位于 start 和 end 两个索引之间的内容(闭区间，start 和 end 会被包括 在内)。和 SETRANGE 只接受正数索引不同， GETRANGE 的索引可以是正数或者 负数。redis&gt; SET msg "hello world"OKredis&gt; GETRANGE msg 0 4 "hello"# 从后往前取范围, 依然是从左往右(从前往后)读的redis&gt; GETRANGE msg -5 -1 "world"==========================================================================================# 数字操作-------------------------------------------------------------------------------------只要储存在字符串键里面的值可以被解释为 64 位整数，或者 IEEE-754 标准的 64 位浮点数，那么用户就可以对这个字符串键执行针对数字值的命令。 # 增加或者减少数字的值 INCRBY, DECRBY可实现案例: 计数器(counter), id 生成器 # 键 num 不存在，命令先将 num 的值初始化为 0 ,然后再执行加 100 操作 redis&gt; INCRBY num 100 (integer) 100 redis&gt; DECRBY num 50 (integer) 50 # 浮点数的自增和自减INCRBYFLOAT key increment减的话 用负数-------------------------------------------------------------------------------------redis&gt; SET num 10OKredis&gt; INCRBYFLOAT num 3.14 "13.14"redis&gt; INCRBYFLOAT num -2.04 "11.1"即使字符串键储存的是数字值，它也可以执行 APPEND、STRLEN、SETRANGE 和 GETRANGE当用户针对一个数字值执行这些命令的时候，Redis 会先将数字值转换为字符串，然后再执行命令。 ==========================================================================================# 二进制的操作索引是从右往左递增 具体查看应用案例: 实现在线人数统计应用案例: 使用 Redis 缓存热门图片(二进制)https://app.yinxiang.com/shard/s37/nl/7399077/92dcb5fb-c005-45dc-bcc5-d00863c08f84/==========================================================================================# 储存中文时的注意事项--------------------------------------------一个英文字符只需要使用 单个字节来储存，而一个中文字符却需要使用多个字 节来储存。STRLEN、SETRANGE 和 GETRANGE 都是为英文设置的，它们只会在字符为单个字节的情况下正常 工作，而一旦我们储存的是类似中文这样的多字节字符，那么这三个命令就不再适用了。# 在 redis-cli 中使用中文时，必须打开 --raw 选项，才能正常显示中文$ redis-cli --raw$ redis&gt; SET msg "世界你好" OK$ redis&gt; GET msg 世界你好# strlen 只能获取字节$redis&gt; STRLEN msg 12==========================================================================================# 移动库move key db(注意: 一个redis进程,打开了不止一个数据库, 默认打开16个数据库,从0到15编号,如果想打开更多数据库,可以从配置文件修改)---------------------------------------------------------------------------redis 127.0.0.1:6379[1]&gt; select 2OKredis 127.0.0.1:6379[2]&gt; keys *(empty list or set)redis 127.0.0.1:6379[2]&gt; select 0OKredis 127.0.0.1:6379&gt; keys *1) "name"2) "cc"3) "a"4) "b"# 移动key 到其它的库redis 127.0.0.1:6379&gt; move cc 2(integer) 1redis 127.0.0.1:6379&gt; select 2OKredis 127.0.0.1:6379[2]&gt; keys *1) "cc"redis 127.0.0.1:6379[2]&gt; get cc"3"# 插入和读取一条 String 类型的数据127.0.0.1:6379&gt; set name huangboOK127.0.0.1:6379&gt; get name"huangbo"127.0.0.1:6379&gt;------------------------#对 string 类型数据进行增减（前提是这条数据的 value 可以看成数字）127.0.0.1:6379&gt; set number 10OK127.0.0.1:6379&gt; incr number(integer) 11127.0.0.1:6379&gt; decr number(integer) 10127.0.0.1:6379&gt; incrby number 10(integer) 20127.0.0.1:6379&gt; decrby number 5(integer) 15------------------------# 一次性插入或者获取多条数据127.0.0.1:6379&gt; mset name huangbo age 18 facevalue 99OK127.0.0.1:6379&gt; mget name age facevalue1) "huangbo"2) "18"3) "99"------------------------# 在插入一条 string 类型数据的同时为它指定一个存活期限解释: 表示 yu 的 jiyi 只有 7s，7s 之后会自动清除这个 key-value127.0.0.1:6379&gt; setex yu 7 jiyiOK127.0.0.1:6379&gt; get yu"jiyi"127.0.0.1:6379&gt; get yu(nil) 3.1.2 String 使用案例见 github 3.2 list 3.2.1 List常用操作示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# 从头部（左边left）插入数据127.0.0.1:6379&gt; lpush names huangbo xuzheng wangbaoqiang(integer) 3127.0.0.1:6379&gt; lrange names 0 -11) "wangbaoqiang"2) "xuzheng"3) "huangbo"# 从尾部（右边right）插入数据127.0.0.1:6379&gt; rpush names ycy cy y(integer) 6# 读取 list 中指定范围的 values ==&gt; 好像不支持负数索引 ??==&gt; 索引是闭区间127.0.0.1:6379&gt; lrange names 0 -11) "wangbaoqiang"2) "xuzheng"3) "huangbo"4) "ycy"5) "cy"6) "y"127.0.0.1:6379&gt; LRANGE names 0 01) "wangbaoqiang"# 从头部(前面)弹出一个元素127.0.0.1:6379&gt; lpop names"wangbaoqiang"127.0.0.1:6379&gt; LRANGE names 0 -11) "xuzheng"2) "huangbo"3) "ycy"4) "cy"5) "y"# 从尾部(最后right)弹出一个元素127.0.0.1:6379&gt; rpop names"y"127.0.0.1:6379&gt; LRANGE names 0 -11) "xuzheng"2) "huangbo"3) "ycy"4) "cy"# 从前面 list 的尾部(right)弹出(pop)一个元素 压入(push)到 后面 list的 头部(left) -----------------------------------127.0.0.1:6379&gt; LRANGE names 0 -11) "xuzheng"2) "huangbo"3) "ycy"4) "cy"127.0.0.1:6379&gt; LRANGE nickname 0 -11) "YangChaoyue"2) "WuXuanyi"3) "MengMeiqi"127.0.0.1:6379&gt;  RPOPLPUSH names nickname "cy"127.0.0.1:6379&gt; LRANGE nickname 0 -11) "cy"2) "YangChaoyue"3) "WuXuanyi"4) "MengMeiqi"127.0.0.1:6379&gt; LRANGE names 0 -11) "xuzheng"2) "huangbo"3) "ycy"# 求 list 的长度 3.2.2 list应用案例代码见 github 3.3 Set 集合3.3.1 Set 的 shell 使用Redis 的 Set 是 string 类型的无序集合。 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是 O(1)。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# 插入 set 数据 &amp; 查询 set 数据127.0.0.1:6379&gt; sadd bigdata hadoop spark hive(integer) 3127.0.0.1:6379&gt; scard bigdata(integer) 3127.0.0.1:6379&gt; smembers bigdata1) "hadoop"2) "spark"3) "hive"# 判断一个成员是否属于某条指定的 set 数据127.0.0.1:6379&gt; sismember bigdata hbase(integer) 0127.0.0.1:6379&gt; sismember bigdata hive(integer) 1# 求两个 set 数据的差集$ setdiff [set1] [set2]&gt; 删除 set1 中 set1,set2的交集, 返回删除操作后的 set1(对 set1 的真实存储不影响)127.0.0.1:6379&gt; SMEMBERS bigdata1) "hadoop"2) "spark"3) "hive"127.0.0.1:6379&gt; SMEMBERS bigdata21) "hbase"2) "spark"3) "storm"127.0.0.1:6379&gt; sdiff bigdata bigdata21) "hadoop"2) "hive"# 将上面2者的差集, 存入一个新的 set127.0.0.1:6379&gt; sdiffstore bigdatadiff bigdata bigdata2(integer) 2127.0.0.1:6379&gt; SMEMBERS bigdatadiff1) "hadoop"2) "hive"# 求交集 &amp; 存入新的集合127.0.0.1:6379&gt; SMEMBERS bigdata1) "hadoop"2) "spark"3) "hive"127.0.0.1:6379&gt; SMEMBERS bigdata21) "hbase"2) "spark"3) "storm"127.0.0.1:6379&gt; SINTER bigdata bigdata21) "spark"127.0.0.1:6379&gt; SINTERSTORE interdb bigdata bigdata2(integer) 1127.0.0.1:6379&gt; SMEMBERS interdb1) "spark"# 求并集 sunion127.0.0.1:6379&gt; SMEMBERS bigdata1) "hadoop"2) "spark"3) "hive"127.0.0.1:6379&gt; SMEMBERS bigdata21) "hbase"2) "spark"3) "storm"# 返回并集127.0.0.1:6379&gt; sunion bigdata bigdata21) "hadoop"2) "spark"3) "hive"4) "storm"5) "hbase"# 存储并集127.0.0.1:6379&gt; SUNIONSTORE uniondb bigdata bigdata2(integer) 5127.0.0.1:6379&gt; SMEMBERS uniondb1) "hadoop"2) "spark"3) "hive"4) "storm"5) "hbase" 3.3.2 set 使用案例3.4 ZSet–有序集合Zset 和 Set 一样也是 String 类型元素的集合，且不允许重复的成员。 不同的是每个元素都会关联一个 double 类型的分数。Redis 正是通过分数来为集合中的成员 进行从小到大的排序。 Zset 的成员是唯一的，但分数(score)却可以重复。 3.4.1 zset shell操作1234567891011121314151617181920212223242526272829303132333435363738394041424344# 往 redis 库中插入一条 sortedset 数据# 注意: sco在前面(float), 可重复, 'value'不可重复$ zadd [zset名] sco 'value'127.0.0.1:6379&gt; zadd yanzhi 70 huangbo 90 xuzheng 80 wangbaoqiang(integer) 3# 升序排列127.0.0.1:6379&gt; zrange yanzhi 0 41) "huangbo"2) "wangbaoqiang"3) "xuzheng"# 降序排列127.0.0.1:6379&gt; zrevrange yanzhi 0 41) "xuzheng"2) "wangbaoqiang"3) "huangbo"# 查询某个成员的名次127.0.0.1:6379&gt; zrevrange yanzhi 0 41) "xuzheng"2) "wangbaoqiang"3) "huangbo"127.0.0.1:6379&gt; zrank yanzhi huangbo(integer) 0127.0.0.1:6379&gt; zrank yanzhi wangbaoqiang(integer) 1127.0.0.1:6379&gt; zrank yanzhi xuzheng(integer) 2#修改成员的分数zincrby yanzhi 50 huangbo"120"127.0.0.1:6379&gt; ZRANGE yanzhi 0 -11) "wangbaoqiang"2) "xuzheng"3) "huangbo"$ zrevrank 返回 huangbo 的 yanzhi 排名 $ 'value' huangbo 不能重复, 对应的分数可以重复127.0.0.1:6379&gt; zrevrank yanzhi huangbo(integer) 0 3.4.2 案例Lol 盒子英雄数据排行榜： 1、在 redis 中需要一个榜单所对应的 sortedset 数据 2、玩家每选择一个英雄打一场游戏，就对 sortedset 数据的相应的英雄分数+1 3、Lol 盒子上查看榜单时，就调用 zrange 来看榜单中的排序结果 代码见 github 3.5 散列 – Hash– 哈希表一个散列由多个域值对(field-value pair)组成，散列的键和值都可以是文字、整数、浮点数或者二 进制数据。 同一个散列里面的各个域必 须是独一无二的，但不同域的 值可以是重复的。 尽量使用散列键而不是字符串键来储存键值对数据，因为散列键管理方便、能够避免键名冲突、并且还能够节约内存。 Redis Hash 是一个键值对集合。 Redis Hash 类型可以看成具有 String Key 和 String Value 的 map 容器 Redis Hash 是一个 String 类型的 field 和 value 的映射表，Hash 特别适合用于存储对象。 3.5.1 shell 操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119# 散列(hash)类似于 HashMap一个散列由多个域 值对(field-value pair)组成，散列的域和值都可以是文字、整数、浮点数或者二 进制数据同一个散列里面的每个域必 须是独一无二、各不相同 的，而域的值则没有这一要求，换句话说，不同域的值 可以是重的。#关联域值对HSET key field valueHGET key field:TODO 真正的 kv 是 f v, 那设置这个 key 干嘛的? ? 👌===&gt; key 是散列名称, 比如 散列 message 的 field-value pair 是"sender" "peter" -------------------------------------------------------------------------------------redis&gt; HSET message "id" 10086 (integer) 1redis&gt; HSET message "sender" "peter" (integer) 1redis&gt; HSET message "receiver" "jack" (integer) 1==========================================================================================#一次设置或获取散列中的多个域值对HMSET key field value [field value ...]HMGET key field [field ...]redis&gt; HMSET message "id" 10086 "sender" "peter" "receiver" "jack"OKredis&gt; HMGET message "id" "sender" "receiver"1) "10086"2) "peter"3) "jack"#获取散列包含的所有域、值、或者域值对HKEYS key =&gt; 返回散列键 key 包含的所有域。HVALS key =&gt; 返回散列键 key 中，所有域的值。HGETALL key =&gt; 返回散列键 key 包含的所有域值对。redis&gt; HKEYS message 1) "id"2) "sender"3) "receiver"4) "date"5) "content"redis&gt; HVALS message 1) "10086"2) "peter"3) "jack"4) "2014-8-3 3:25 p.m." 5) "Good morning, jack!"redis&gt; HGETALL message1) "id" #域2) "10086" #值3) "sender" #域4) "peter" #值5) "receiver"6) "jack"7) "date"8) "2014-8-3 3:25 p.m." 9) "content"10) "Good morning, jack!"================================================================# hset , hget , hgetall127.0.0.1:6379&gt; hset shouji iphone 8(integer) 1127.0.0.1:6379&gt; hset shouji xiaomi 7(integer) 1127.0.0.1:6379&gt; hset shouji huawei p20(integer) 1127.0.0.1:6379&gt; hgetall shouji1) "iphone"2) "8"3) "xiaomi"4) "7"5) "huawei"6) "p20"127.0.0.1:6379&gt; hget shouji huawei"p20"# 取出 hash 数据中所有 fields / values127.0.0.1:6379&gt; hkeys shouji1) "iphone"2) "xiaomi"3) "huawei"127.0.0.1:6379&gt; hvals shouji1) "8"2) "7"3) "p20"# 为 hash 数据中指定的一个 field 的值进行增减127.0.0.1:6379&gt; hincrby shouji xiaomi 10(integer) 17127.0.0.1:6379&gt; hget shouji xiaomi"17"# 从 hash 数据中删除一个字段 field 及其值127.0.0.1:6379&gt; HGETALL shouji1) "iphone"2) "8"3) "xiaomi"4) "17"5) "huawei"6) "p20"127.0.0.1:6379&gt; hdel shouji iphone(integer) 1127.0.0.1:6379&gt; HGETALL shouji1) "xiaomi"2) "17"3) "huawei"4) "p20" 3.5.2 案例: 实现购物车 需求点： 加入购物车 查询购物车 修改购物车 清空购物车 代码见 github 3.6 键值相关命令 shell简单实用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101# 查询所有 cart 开头的 key127.0.0.1:6379&gt; keys cart*1) "cart:huangbo"2) "cart:xuzheng"3) "cart:wangbaoqiang"# 查询所有 key127.0.0.1:6379&gt; keys * 1) "bigdata:hadoop" 2) "uniondb" 3) "myset" 4) "cart:huangbo" 5) "bigdata:spark" ..... # 判断一个 key 是否存在 exists 127.0.0.1:6379&gt; EXISTS bigdata(integer) 1# del 删除一个或多个 key127.0.0.1:6379&gt; del facevalue(integer) 1# type 查看类型, 便于针对类型操作127.0.0.1:6379&gt; type bigdataset127.0.0.1:6379&gt; SMEMBERS bigdata1) "hadoop"2) "spark"3) "hive"# expire [key] second 设置一个 key 的过期时间, 单位是秒127.0.0.1:6379&gt; type nameslist127.0.0.1:6379&gt; lrange names 0 -11) "xuzheng"2) "huangbo"3) "ycy"$ 127.0.0.1:6379&gt; expire names 5(integer) 1127.0.0.1:6379&gt; lrange names 0 -11) "xuzheng"2) "huangbo"3) "ycy"$ 127.0.0.1:6379&gt; lrange names 0 -1(empty list or set)# expireat [key] timestamp 设置在时间戳 timestamp 过期# ttl [key] 查询 key 的有效时长, 不存在或没有超时设置, 返回 -1127.0.0.1:6379&gt; ttl bigdata(integer) -1# move [key] database 将当前库中的 key 移动到其它数据库中# 默认是有16个 databases 0-15127.0.0.1:6379&gt; move bigdata 2(integer) 1127.0.0.1:6379&gt; type bigdatanone127.0.0.1:6379&gt; SMEMBERS bigdata(empty list or set)127.0.0.1:6379&gt; SELECT 2OK127.0.0.1:6379[2]&gt; keys *1) "bigdata"127.0.0.1:6379[2]&gt; SMEMBERS bigdata1) "hadoop"2) "spark"3) "hive"# 移除给定key 的过期时间127.0.0.1:6379[2]&gt; keys *1) "bigdata"127.0.0.1:6379[2]&gt; expire bigdata 100(integer) 1127.0.0.1:6379[2]&gt; ttl bigdata(integer) 96127.0.0.1:6379[2]&gt; persist bigdata(integer) 1127.0.0.1:6379[2]&gt; ttl bigdata(integer) -1# 随机获取 key空间中的一个127.0.0.1:6379&gt; randomkey"student_tang"# 重命名 key127.0.0.1:6379&gt; zrange yanzhi 0 -11) "wangbaoqiang"2) "xuzheng"3) "huangbo"127.0.0.1:6379&gt; rename yanzhi shuiMaOK127.0.0.1:6379&gt; zrange shuiMa 0 -11) "wangbaoqiang"2) "xuzheng"3) "huangbo"# renamenx [key] newkey 如果 newkey 存在, 则失败返回0# type [key] 查询 key 的类型 3.7 服务器相关命令 简单测试12345678910111213141516171819202122232425262728293031# 选择数据库 (redis 默认数据库编号 0-15)127.0.0.1:6379&gt; select 3OK# quit退出# echo msg 发现不能有空格127.0.0.1:6379[3]&gt; echo 要吐了"\xe8\xa6\x81\xe5\x90\x90\xe4\xba\x86"127.0.0.1:6379[3]&gt; echo iFellSick"iFellSick"# dbsize 返回当期那数据库中的 key 的条数127.0.0.1:6379[3]&gt; select 0OK127.0.0.1:6379&gt; DBSIZE(integer) 23# flushdb 删除当前选择数据库中的所有key127.0.0.1:6379[2]&gt; dbsize(integer) 1127.0.0.1:6379[2]&gt; keys *1) "bigdata"127.0.0.1:6379[2]&gt; flushdbOK127.0.0.1:6379[2]&gt; keys *(empty list or set)# flushall 删除所有数据库中的所有的 key 5. Redis高可用在 Redis 中，实现高可用的技术主要包括持久化、复制、哨兵和集群 第一：持久化 持久化是最简单的高可用方法(有时甚至不被归为高可用的手段)，主要作用是数据备份， 即将数据存储在硬盘，保证数据不会因进程退出而丢失。 第二：主从复制 复制是高可用 Redis 的基础，哨兵和集群都是在复制基础上实现高可用的。复制主要实现 了数据的多机备份，以及对于读操作的负载均衡和简单的故障恢复。缺陷：故障恢复无 法自动化；写操作无法负载均衡；存储能力受到单机的限制。 第三：哨兵 哨兵在复制的基础上，哨兵实现了自动化的故障恢复。缺陷：写操作无法负载均衡；存 储能力受到单机的限制。 第四：集群 通过集群，Redis 解决了写操作无法负载均衡，以及存储能力受到单机限制的问题，实现 了较为完善的高可用方案。 5.1 Redis 持久化Redis 持久化分为 RDB 持久化和 AOF 持久化： RDB：将当前数据保存到硬盘 AOF：将每次执行的写命令保存到硬盘（类似于 MySQL 的 binlog） 由于 AOF 持久化的实时性更好，即当进程意外退出时丢失的数据更少，因此 AOF 是目前主 流的持久化方式，不过 RDB 持久化仍然有其用武之地。 5.1.1 RDB 手动方式 save save 命令会阻塞 Redis 服务器进程，直到 RDB 文件创建完毕为止 bgsave bgsave 命令会创建一个子进程，由子进程来负责创建 RDB 文件，父进程(即 Redis 主 进程)则继续处理请求。 自动触发 在配置文件redis.conf中配置 save m n save 900 1 save 300 10 save 60 10000 #900 秒内如果超过 1 个 key 被修改，则发起快照保存 #300 秒内容如超过 10 个 key 被修改，则发起快照保存 #60 秒内容如超过 10000 个 key 被修改，则发起快照保存 5.1.2 AOF 方式AOF 比快照方式有更好的持久化性，是由于在使用 AOF 持久化方式时，Redis 会将每一个收 到的写命令都通过 write 函数追加到文件中(默认是 appendonly.aof)。当 Redis 重启时会通过 重新执行文件中保存的写命令来在内存中重建整个数据库的内容。 我们可以通过配置文件告诉 redis 我们想要 通过 fsync 函数强制 os 写入到磁盘的时机。 三种方式如下（默认是：每秒 fsync 一次）： 5.2 Redis 主从复制Redis 主从复制配置和使用都非常简单。通过主从复制可以允许多个 slave server 拥有和 master server 相同的数据库副本。下面是关于 redis 主从复制的一些特点： 1、master 可以有多个 slave 2、除了多个 slave 连到相同的 master 外，slave 也可以连接其他 slave 形成图状结构 3、主从复制不会阻塞 master。也就是说当一个或多个 slave 与 master 进行初次同步数据时， master 可以继续处理 client 发来的请求。相反 slave 在初次同步数据时则会阻塞不能处理 client 的请求。 4、主从复制可以用来提高系统的可伸缩性,我们可以用多个 slave 专门用于 client 的读请求， 比如 sort 操作可以使用 slave 来处理。也可以用来做简单的数据冗余 5、可以在 master 禁用数据持久化，只需要注释掉 master 配置文件中的所有 save 配置，然 后只在 slave 上配置数据持久化。 主从复制的过程当设置好 slave 服务器后，slave 会建立和 master 的连接，然后发送 sync 命令。 无论是第一 次同步建立的连接还是连接断开后的重新连接，master 都会启动(fork)一个后台进程，将数 据库快照保存到文件中（fork 一个进程入内在也被复制了，即内存会是原来的两倍），同时 master 主进程会开始收集新的写命令并缓存起来。 后台进程完成写文件后，master 就发送 文件给 slave，slave 将文件保存到磁盘上，然后加载到内存恢复数据库快照到 slave 上。 接着 master 就会把缓存的命令转发给 slave。 而且后续 master 收到的写命令都会通过开始建立 的连接发送给 slave。 从 master 到 slave 的同步数据的命令和从 client 发送的命令使用相同的 协议格式。 当 master 和 slave 的连接断开时 slave 可以自动重新建立连接。 如果 master 同时 收到多个 slave 发来的同步连接命令，只会使用启动一个进程来写数据库镜像，然后发送给 所有 slave。 配置 slave 服务器只需要在配置文件中加入如下配置： slaveof cs1 6379 #指定 master 的 ip 和端口 注意：主节点不用加！！！ 6.Redis.conf配置文件参数说明redis.conf 配置项说明如下： 1、Redis 默认不是以守护进程的方式运行，可以通过该配置项修改，使用 yes 启用守护进 程 daemonize no 2、当 Redis 以守护进程方式运行时，Redis 默认会把 pid 写入/var/run/redis.pid 文件，可以 通过 pidfile 指定 pidfile /var/run/redis.pid 3、指定 Redis 监听端口，默认端口为 6379，作者在自己的一篇博文中解释了为什么选用 6379 作为默认端口，因为 6379 在手机按键上 MERZ 对应的号码，而 MERZ 取自意大利歌 女 Alessia Merz 的名字 port 6379 4、绑定的主机地址 bind 127.0.0.1 5、当客户端闲置多长时间后关闭连接，如果指定为 0，表示关闭该功能 timeout 300 6、指定日志记录级别，Redis 总共支持四个级别：debug、verbose、notice、warning，默 认为 verbose loglevel verbose 7、日志记录方式，默认为标准输出，如果配置 Redis 为守护进程方式运行，而这里又配 置为日志记录方式为标准输出，则日志将会发送给/dev/null logfile stdout 8、设置数据库的数量，默认数据库为 0，可以使用 SELECT 命令在连接上指定数据 库 id databases 16 9、指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配 合 save \&lt;seconds> \&lt;changes> Redis 默认配置文件中提供了三个条件： save 900 1 save 300 10 save 60 10000 分别表示 900 秒（15 分钟）内有 1 个更改，300 秒（5 分钟）内有 10 个更改以及 60 秒 内有 10000 个更改。 10、指定存储至本地数据库时是否压缩数据，默认为 yes，Redis 采用 LZF 压缩，如果为了 节省 CPU 时间，可以关闭该选项，但会导致数据库文件变的巨大 rdbcompression yes 11、指定本地数据库文件名，默认值为 dump.rdb dbfilename dump.rdb 12、指定本地数据库存放目录 dir ./ 13、设置当本机为 slave 服务时，设置 master 服务的 IP 地址及端口，在 Redis 启动时，它 会自动从 master 进行数据同步 slaveof \&lt;masterip> \&lt;masterport> 14、当 master 服务设置了密码保护时，slav 服务连接 master 的密码 masterauth \&lt;master-password> 15、设置 Redis 连接密码，如果配置了连接密码，客户端在连接 Redis 时需要通过 AUTH \&lt;password>命令提供密码，默认关闭 requirepass foobared 16、设置同一时间最大客户端连接数，默认无限制，Redis 可以同时打开的客户端连接数 为 Redis 进程可以打开的最大文件描述符数，如果设置 maxclients 0，表示不作限制。当 客户端连接数到达限制时，Redis 会关闭新的连接并向客户端返回 max number of clients reached 错误信息 maxclients 128 17、指定 Redis 最大内存限制，Redis 在启动时会把数据加载到内存中，达到最大内存后， Redis 会先尝试清除已到期或即将到期的 Key，当此方法处理 后，仍然到达最大内存设置， 将无法再进行写入操作，但仍然可以进行读取操作。Redis 新的 vm 机制，会把 Key 存放 内存，Value 会存放在 swap 区 maxmemory \&lt;bytes> 18、指定是否在每次更新操作后进行日志记录，Redis 在默认情况下是异步的把数据写入 磁盘，如果不开启，可能会在断电时导致一段时间内的数据丢失。因为 redis 本身同步数 据文件是按上面 save 条件来同步的，所以有的数据会在一段时间内只存在于内存中。默 认为 no appendonly no 19、指定更新日志文件名，默认为 appendonly.aof appendfilename appendonly.aof 20、指定更新日志条件，共有 3 个可选值： no：表示等操作系统进行数据缓存同步到磁盘（快） always：表示每次更新操作后手动调用 fsync()将数据写到磁盘（慢，安全） everysec：表示每秒同步一次（折衷，默认值） appendfsync everysec 21、指定是否启用虚拟内存机制，默认值为 no，简单的介绍一下，VM 机制将数据分页 存放，由 Redis 将访问量较少的页即冷数据 swap 到磁盘上，访问多的页面由磁盘自动换 出到内存中（在后面的文章我会仔细分析 Redis 的 VM 机制） vm-enabled no 22、虚拟内存文件路径，默认值为/tmp/redis.swap，不可多个 Redis 实例共享 vm-swap-file /tmp/redis.swap 23、将所有大于 vm-max-memory 的数据存入虚拟内存,无论 vm-max-memory 设置多小,所 有索引数据都是内存存储的(Redis 的索引数据 就是 keys),也就是说,当 vm-max-memory 设 置为 0 的时候,其实是所有 value 都存在于磁盘。默认值为 0 vm-max-memory 0 24、Redis swap 文件分成了很多的 page，一个对象可以保存在多个 page 上面，但一个 page 上不能被多个对象共享，vm-page-size 是要根据存储的 数据大小来设定的，作者建议如 果存储很多小对象，page 大小最好设置为 32 或者 64bytes；如果存储很大大对象，则可 以使用更大的 page，如果不 确定，就使用默认值 vm-page-size 32 25、设置 swap 文件中的 page 数量，由于页表（一种表示页面空闲或使用的 bitmap）是 在放在内存中的，，在磁盘上每 8 个 pages 将消耗 1byte 的内存。 vm-pages 134217728 26、设置访问 swap 文件的线程数,最好不要超过机器的核数,如果设置为 0,那么所有对 swap 文件的操作都是串行的，可能会造成比较长时间的延迟。默认值为 4 vm-max-threads 4 27、设置在向客户端应答时，是否把较小的包合并为一个包发送，默认为开启 glueoutputbuf yes 28、指定在超过一定的数量或者最大的元素超过某一临界值时，采用一种特殊的哈希算 法 hash-max-zipmap-entries 64 hash-max-zipmap-value 512 29、指定是否激活重置哈希，默认为开启（后面在介绍 Redis 的哈希算法时具体介绍） activerehashing yes 30、指定包含其它的配置文件，可以在同一主机上多个 Redis 实例之间使用同一份配置文 件，而同时各个实例又拥有自己的特定配置文件 include /path/to/local.conf 6.其它需要了解的添加注释的技巧: 最好的注释方式: 少而精 完成一段小逻辑的代码就加个注释 容易费解的地方加注释 拓展: 缓存淘汰算法 :TODONRU(Not recently used) FIFO(First-in, first-out) Second-chance LRU(Least recently Used) redis 的内存模型 Hadoop3.0 新特性机制 纠删码 机制 7.Redis 错误12345678910111213141516171819202122# 错误1(error) MISCONF Redis is configured to save RDB snapshots, but it is currently not able to persist on disk. Commands that may modify the data set are disabled, because this instance is configured to report errors during writes if RDB snapshotting fails (stop-writes-on-bgsave-error option). Please check the Redis logs for details about the RDB error.127.0.0.1:6379&gt; config set stop-writes-on-bgsave-error noOK127.0.0.1:6379&gt; pingPONG# 错误2cc: ../deps/hiredis/libhiredis.a: No such file or directorycc: ../deps/lua/src/liblua.a: No such file or directorycc: ../deps/geohash-int/geohash.o: No such file or directorycc: ../deps/geohash-int/geohash_helper.o: No such file or directorymake[1]: * [redis-server] Error 1make[1]: Leaving directory `/usr/local/src/redis-3.2.9/src'make: * [all] Error 2----------------解决办法进入源码包目录下的deps目录中执行make geohash-int hiredis jemalloc linenoise lua然后再进行../make编译就可以了再 $&gt; sudo make PREFIX=/usr/local/redis install 8.参考资料文档PDF]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka]]></title>
    <url>%2F2018%2F07%2F01%2FHadoop%2F7-Flume%26Kafka%2FKafka%2F</url>
    <content type="text"><![CDATA[1.概览分布式流处理平台。 分布式, 副本, 容错的分布式存储流. 在系统之间构建实时数据流管道。 以topic分类对记录进行存储 每个记录包含key-value+timestamp 每秒钟百万消息吞吐量。 Java 的 JMS 有2种模式: 发布订阅模式( 拿到的是 copy) &amp; 队列模式(queue): kafka 通过 group 把这两种模式完美的结合起来 如果想要实现queue 模式, 就把消费者放在同一组, 此组中只能有一个成员来消费此消息 想要实现发布订阅模式, 就把每个消费者各自一组, 每个人都可以拿到一个消息副本 关键词: producer //消息生产者 consumer //消息消费者 consumer group //消费者组 kafka server //broker,kafka服务器 topic //主题,副本数,分区. zookeeper //hadoop namenoade + RM HA | hbase | kafka 优化方案: 把磁盘挂载在某个目录, 然后让这个目录, 只是存储某个分区的数据 kafka 的要点一条消息是存在一个分区的 副本–可靠性 分区–高效性 =&gt; 并发访问 分区(partition)是 producer 通过负载均衡选择的 一个分区存着多条消息, 消息是有序的 一个分区内的消息只能被一个组内的一个成员消费 topic 是在 kafka server 上 topic 包含 副本数, 分区 producer &amp; consumer 都是针对 topic 来操作的 kafka 的 consumer 是拉模式, 从 kafka去pull 消息, 实时计算框架有多大能力, 就 pull 多少 所以 kafka 一般都用在实时/准实时 计算中 顺序读写 追加数据, 是追加到最后 读取是从开头读取 可用 offset 跳转到指定的数据段 数据可重复消费 Consumers 一条数据只能被同组内一个成员消费 不同组的可以消费同一个数据 SSD读写速度: 1200 ~ 3500 MB/s 注意点: Kafka 的分区数, 可以动态调整 Kafka 的每个分区都有对应的冗余数量, 默认是1 每个分区如果有多个副本, 这些副本中, 会有一个是 active 的状态, 负责读写, 其它的都是 standby 的状态 对于HDFS和kafka的 block partition 对于不同的文件或者topic来说，都可以有不同的副本数！(每个 topic 可以单独设置)设置的副本数不能超过broker的数量！！！ Kafka 的大致工作模式：1、启动 ZooKeeper 的 server 2、启动 Kafka 的 server 3、Producer 生产数据，然后通过 ZooKeeper 找到 Broker，再将数据 push 到 Broker 保存 4、Consumer 通过 ZooKeeper 找到 Broker，然后再主动 pull 数据 2.安装 kafka12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273740.选择cs1 ~ cs6, 6台主机都安装kafka---------------------------------------------------------------------1.准备zk略---------------------------------------------------------------------2.jdk略---------------------------------------------------------------------3.tar文件---------------------------------------------------------------------4.环境变量---------------------------------------------------------------------略5.配置kafka--------------------------------------------------------------------- [kafka/config/server.properties] ... # 这里每台不一样 broker.id=1 ... listeners=PLAINTEXT://:9092 ... # 这里每台不一样 advertised.listeners=PLAINTEXT://cs1:9092 host.name=cs1 ... log.dirs=/home/centos/kafka/logs ... zookeeper.connect=s201:2181,s202:2181,s203:21816.分发整个文件 &amp; 符号链接, .zshrc配置文件, 同时修改每个server.properties 文件的broker.id参数 ---------------------------------------------------------------------7.启动kafka服务器--------------------------------------------------------------------- a)先启动zk b)启动kafka b1)前台启动 [cs2 ~ cs4] [ap@cs1]~% kafka-server-start.sh /home/ap/apps/kafka/config/server.properties b2)后台启动 # 这里日志直接打印在控制台 $&gt; kafka-server-start.sh -daemon /home/ap/apps/kafka/config/server.properties [或者] # 这样能把日志输出到文件中 nohup kafka-server-start.sh /home/ap/apps/kafka/config/server.properties 1&gt;&lt;sub&gt;/kafka/logs/kafka_std.log 2&gt;&lt;/sub&gt;/kafka/logs/kafka_err.log &amp; c)验证kafka服务器是否启动 $&gt;netstat -anop | grep 90928.创建主题 --------------------------------------------------------------------- [ap@cs2]~% kafka-topics.sh --create --zookeeper cs1:2181 --replication-factor 3 --partitions 3 --topic test9.查看主题列表---------------------------------------------------------------------[ap@cs2]~% kafka-topics.sh --list --zookeeper cs1:218110.启动控制台生产者 (前提是 操作本机 &amp; cs2 上已经启动kafka服务)--------------------------------------------------------------------- 注意 : producer 不直接连接 zk, 而是连接 broker, 也就是 kafka server. 其它的都是连 zk 如果producer 连的某一台挂掉的话, 可以在 --broker-list 后面, 加上多台服务器,用逗号隔开 [ap@cs2]~% kafka-console-producer.sh --broker-list cs2:9092 --topic test11.启动控制台消费者 (前提是 操作本机 &amp; cs2 上已经启动卡夫卡服务)--------------------------------------------------------------------- # 使用bootstrap-server参数 替代zookeeper Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper]. [ap@cs3]~% kafka-console-consumer.sh --bootstrap-server cs2:9092 --topic test --from-beginning12.在生产者控制台输入hello world--------------------------------------------------------------------- 3.kafka集群在zk的配置123456789101112131415161718192021222324/controller ===&gt; &#123;"version":1,"brokerid":202,"timestamp":"1490926369148"/controller_epoch ===&gt; 1/brokers/brokers/ids/brokers/ids/202 ===&gt; &#123;"jmx_port":-1,"timestamp":"1490926370304","endpoints":["PLAINTEXT://s202:9092"],"host":"s202","version":3,"port":9092&#125;/brokers/ids/203/brokers/ids/204 /brokers/topics/test/partitions/0/state ===&gt;&#123;"controller_epoch":1,"leader":203,"version":1,"leader_epoch":0,"isr":[203,204,202]&#125;/brokers/topics/test/partitions/1/state ===&gt;.../brokers/topics/test/partitions/2/state ===&gt;.../brokers/seqid ===&gt; null/admin/admin/delete_topics/test ===&gt;标记删除的主题, 但是实际删除后, 这里没出现/isr_change_notification/consumers/xxxx//config 4.kafka 主题&amp;副本操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110---------------------------------------------------------------------PS: 查看帮助套路* 如果看 topic 相关帮助 * bin/kafka-topic.sh 脚本直接回车, 查看帮助---------------------------------------------------------------------查看主题列表---------------------------------------------------------------------[ap@cs2]~% kafka-topics.sqqh --list --zookeeper cs1:2181创建主题--------------------------------------------------------- repliation_factor 2 partitions 5 $&gt;kafka-topics.sh --zookeeper cs2:2181 --replication-factor 2 --partitions 3 --create --topic test2 2 x 3 = 6 //个文件夹 2份副本, 每份副本有3个分区 总分区数为 6 每个分区都有一个 leader, follower 就是其它的副本 删除主题------------------------------------------------------------[ap@cs2]~% kafka-topics.sh --zookeeper cs1:2181,cs2:2181,cs3:2181 --delete --topic kafka_test1查看主题描述--------------------------------------------------------[ap@cs2]~% kafka-topics.sh --zookeeper cs1:2181,cs2:2181,cs3:2181 --describe --topic kafka_test1增加/删除分区数--------------------------------------------------------kafka-topics.sh \--alter \--zookeeper cs1:2181,cs2:2181,cs3:2181 \--topic kafka_test1 \--partitions 20--------------kafka-topics.sh \--alter \--zookeeper hadoop02:2181,hadoop03:2181,hadoop04:2181 \ --topic kafka_test \--replication-factor 2查看某 topic 某个分区的偏移量最大值和最小值 :TODO 啥意思??--------------------------------------------------------kafka-run-class.sh kafka.tools.GetOffsetShell --topic kafka_test1 --time -1 --broker-list cs1:9092,cs2:9092,cs3:9092 --partitions 1 重新布局分区和副本，手动再平衡 :TODO 还未测试 ------------------------------------------------------------alter 似乎是有问题的, create好像没问题此句是手动把分区放到 203 &amp; 204上了 $&gt;kafka-topics.sh --alter --zookeeper s202:2181 --topic test2 --replica-assignment 203:204,203:204,203:204,203:204,203:204 测试 producer 发消息后, 存在本地的logs 目录的文件--------------------------------------------------------------1. 开启一个producer, 发消息2. 查看本地 cd ~/kafka/logs3. 查看文件大小, 找出文件(一次性查看) 1. [ap@cs1]~% xcall.sh "ls -lh kafka/logs/test2-*" 副本--------------------------------------------------------------broker存放消息以消息达到顺序存放。生产和消费都是副本感知的。支持到n-1故障。每个分区都有leader，follow.leader挂掉时，消息分区写入到本地log或者，向生产者发送消息确认回执之前，生产者向新的leader发送消息。新leader的选举是通过isr进行，第一个注册的follower成为leader。## kafka支持副本模式, 就是 producer 写到 broker的 topic中的 partition 的副本机制---------------------[同步复制] 1.producer联系zk识别leader 2.向leader发送消息 3.leadr收到消息写入到本地log 4.follower从leader pull消息 5.follower向本地写入log 6.follower向leader发送ack消息 7.leader收到所有follower的ack消息 8.leader向producer回传ack [异步副本]-------------------------------------------------------------- 和同步复制的区别在于 leader写入本地log之后， 直接向client回传ack消息，不需要等待所有follower复制完成。 # kafka 副本模式 &amp; leader 机制 | 官方文档----------------------------------------------------ps : 推选leader过程就是follower在zk中注册过程，第一个注册就是leaderThe process of choosing the new lead replica is that all followers' In-sync Replicas (ISRs) register themselves with ZooKeeper. The very first registered replica becomes the new lead replica, and the rest of the registered replicas become the followers.Kafka supports the following replication modes:• Synchronous replication: In synchronous replication, a producer first identifies the lead replica from ZooKeeper and publishes the message. As soon as the message is published, it is written to the log of the lead replica and all the followers of the lead start pulling the message, and by using a single channel, the order of messages is ensured. Each follower replica sends an acknowledgement to the lead replica once the message is written to its respective logs. Once replications are complete and all expected acknowledgements are received, the lead replica sends an acknowledgement to the producer.On the consumer side, all the pulling of messages is done from the lead replica.• Asynchronous replication: The only difference in this mode is that as soon as a lead replica writes the message to its local log, it sends the acknowledgement to the message client and does not wait for the acknowledgements from follower replicas. But as a down side, this mode does not ensure the message delivery in case of broker failure. 5.Kafka java API5.0 pom 文件1234567891011121314151617181920212223242526272829&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.rox&lt;/groupId&gt; &lt;artifactId&gt;Kafka_Demo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka_2.11&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 5.1实现消息生产者，发送&amp;接受 消息12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package com.rox.kafkademo.test;import kafka.consumer.Consumer;import kafka.consumer.ConsumerConfig;import kafka.consumer.ConsumerIterator;import kafka.consumer.KafkaStream;import kafka.producer.KeyedMessage$;import org.junit.Test;import kafka.javaapi.producer.Producer;import kafka.producer.KeyedMessage;import kafka.producer.ProducerConfig;import java.util.HashMap;import java.util.List;import java.util.Map;import java.util.Properties;public class TestProducer &#123;  // 发送消息 producer @Test public void testSend() &#123; Properties props = new Properties(); // broker列表 props.put("metadata.broker.list","cs2:9092"); // 串行化 props.put("serializer.class", "kafka.serializer.StringEncoder"); props.put("request.required.acks", "1"); //创建生产者配置对象 ProducerConfig config = new ProducerConfig(props); //创建生产者 Producer&lt;String, String&gt; producer = new Producer&lt;String, String&gt;(config); KeyedMessage&lt;String, String&gt; msg = new KeyedMessage&lt;String, String&gt;("test2", "100", "hello word fuck you kafka"); // 发送 producer.send(msg); System.out.println("发送完成"); &#125; // 接收消息 consumer @Test public void testConsumer() &#123; Properties props = new Properties(); props.put("zookeeper.connect", "cs2:2181"); props.put("group.id", "g2"); props.put("zookeeper.session.timeout.ms", "500"); props.put("auto.commit.interval.ms", "1000"); props.put("auto.offset.reset", "smallest"); // 创建消费者配置对象 ConsumerConfig config = new ConsumerConfig(props); Map&lt;String, Integer&gt; map = new HashMap&lt;String, Integer&gt;(); map.put("test2", new Integer(1)); Map&lt;String, List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt;&gt; msgs = Consumer.createJavaConsumerConnector(new ConsumerConfig(props)).createMessageStreams(map); List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt; msgList = msgs.get("test2"); for(KafkaStream&lt;byte[],byte[]&gt; stream : msgList)&#123; ConsumerIterator&lt;byte[],byte[]&gt; it = stream.iterator(); while(it.hasNext())&#123; byte[] message = it.next().message(); System.out.println(new String(message)); &#125; &#125; &#125;&#125; 5.2 新的 API6.Flume 集成 kafak6.1KafkaSinkkafka 是 consumer (消费者) flume 的消息, 经由 kafkaSink 导出到 kafka –最常用 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273[flume 为生产者]1.1) flume 参数配置 ---------------------------------------------------- a1.sources = r1 a1.sinks = k1 a1.channels = c1 a1.sources.r1.type=netcat a1.sources.r1.bind=localhost a1.sources.r1.port=8888 a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink a1.sinks.k1.kafka.topic = test3 a1.sinks.k1.kafka.bootstrap.servers = cs2:9092 a1.sinks.k1.kafka.flumeBatchSize = 20 a1.sinks.k1.kafka.producer.acks = 1 a1.channels.c1.type=memory a1.sources.r1.channels = c1 a1.sinks.k1.channel = c11.2) 开启 kafka 消费者---------------------------------------------------- kafka-console-consumer.sh --bootstrap-server cs2:9092 --topic test3 --from-beginning 注意 : Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper]. 使用 [bootstrap-server]代替[zookeeper]1.3) flume 执行 kafka_sink.conf ---------------------------------------------------- flume-ng agent -f apps/flume/conf/confs/kafka_sink.conf -n a11.4) 验证端口---------------------------------------------------- netstat -anop | grep 8888 [OR] lsof -i tcp:88881.5) nc 连接---------------------------------------------------- nc localhost 8888 发消息....1.6) 观察 kafka 消费者接受消息---------------------------------------------------- 发现特别快! ========================================================简单案例2--------------a1.sources = r1a1.sinks = k1a1.channels = c1a1.sources.r1.type=exec#-F 最后10行,如果从头开始收集 -c +0 -F:持续收集后续数据,否则进程停止。a1.sources.r1.command=tail -F -c +0 /home/ap/calllog/calllog.loga1.channels.c1.type=memorya1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSinka1.sinks.k1.kafka.topic = callloga1.sinks.k1.kafka.bootstrap.servers = cs2:9092 cs3:9092 cs4:9092# How many messages to process in one batch. Larger batches improve throughput while adding latency.a1.sinks.k1.kafka.flumeBatchSize = 20# How many replicas must acknowledge a message before its considered successfully written. Accepted values are 0 (Never wait for acknowledgement), 1 (wait for leader only), -1 (wait for all replicas) Set this to -1 to avoid data loss in some cases of leader failure.a1.sinks.k1.kafka.producer.acks = 1a1.sources.r1.channels = c1a1.sinks.k1.channel = c1 6.2 KafkaSource :TODOflume source 从 kafka 抓数据, flume source 是消费者, kafka 开启 producer. 123456789101112131415161718192021222324252627[消费者]1) 配置 a1.sources = r1 a1.sinks = k1 a1.channels = c1 a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource a1.sources.r1.batchSize = 5000 a1.sources.r1.batchDurationMillis = 2000 a1.sources.r1.kafka.bootstrap.servers = cs2:9092 a1.sources.r1.kafka.topics = test3 a1.sources.r1.kafka.consumer.group.id = g4 a1.sinks.k1.type = logger a1.channels.c1.type=memory a1.sources.r1.channels = c1 a1.sinks.k1.channel = c12) 启动 flume flume-ng agent -f /home/ap/apps/flume/conf/confs/kafka_source.conf -n a1 -Dflume.root.logger=INFO,console3) 启动 kafka producer kafka-console-producer.sh --broker-list cs2:9092 --topic test3 4) kafka 发消息: 收不到!!!! :TODO 6.3 KafkaChannel :TODO12345678910111213141516171819202122:TODO 也出错 ?? 越界 生产者 + 消费者 a1.sources = r1 a1.sinks = k1 a1.channels = c1 a1.sources.r1.type = avro a1.sources.r1.bind = localhost a1.sources.r1.port = 8888 a1.sinks.k1.type = logger a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel a1.channels.c1.kafka.bootstrap.servers = cs2:9092 a1.channels.c1.kafka.topic = test3 a1.channels.c1.kafka.consumer.group.id = g6 a1.channels.c1.kafka.parseAsFlumeEvent = false a1.channels.c1.zookeeperConnect= cs2:2181 a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume]]></title>
    <url>%2F2018%2F06%2F30%2FHadoop%2F7-Flume%26Kafka%2FFlume%2F</url>
    <content type="text"><![CDATA[1.Flume 概念提供 收集、移动、聚合大量日志数据的服务。 基于流数据的架构，用于在线日志分析。 基于事件。 在生产和消费者之间启动协调作用。 提供了事务保证，确保消息一定被分发。 多种 Source, Channel, Sink配置方式。 与 Sqoop 区别 Sqoop 用来采集关系型数据库数据 Flume 用 来采集流动型数据。 关键词解释 Client Client 是一个将原始 log 包装成 events 并且发送他们到一个或多个 agent 的实体, 交给 source 处理 Event Event 由可选的 header 和载有数据的一个 byte array 构成。 Source 接受数据，类型有多种。 Channel 临时存放地，对Source中来的数据进行缓冲，直到sink消费掉。 Sink 从channel提取数据存放到中央化存储(hadoop / hbase)。 Iterator 作用于 Source，按照预设的顺序在必要地方装饰和过滤 events Channel Selector 允许 Source 基于预设的标准，从所有 channel 中，选择一个或者多个 channel A simple Flume agent with one source, channel, and sink Source ==&gt;Channel Channel Flume 的数据流由事件(Event)贯穿始终。事件是 Flume 的基本数据单位，它携带日志数据(字 节数组形式)并且携带有头信息，这些 Event 由 Agent 外部的 Source 生成，当 Source 捕获事 件后会进行特定的格式化，然后 Source 会把事件推入(单个或多个)Channel 中。你可以把 Channel 看作是一个缓冲区，它将保存事件直到 Sink 处理完该事件。Sink 负责持久化日志或 者把事件推向另一个 Source。 Flume 以 agent 为最小的独立运行单位。一个 agent 就是一个 JVM。单 agent 由 Source、Sink 和 Channel 三大组件构成，如下图: 参考文档 2.安装 Flume1.下载2.tar3.环境变量注意: 配置的话, 使用脚本就方便些, 但是如果配置文件在 flume 文件夹中, 路径就比较长 ​ 不配置的话, 要进入到flume 路径下使用, 但是此时配置文件的路径就比较短了 PS: 如果.sh文件没有执行权限, 即x权限, 要用相对路径来启动, 即如果在 bin 目录, 要用./flume-ng, 在 bin 外,要用 bin/flume执行, 有x权限的话, 就可以直接在 bin 下, 使用 flume-ng, 如果配置了环境变量, 可以在任何路径下使用 flume-ng了. Open flume-env.sh file and set the JAVA_Home to the folder where Java was installed in your system. 1export JAVA_HOME=/usr/local/jdk1.8.0_73 In the .zshrc file, set FLUME_HOME 12345#flumeexport FLUME_HOME=/home/ap/apps/flumeexport PATH=$PATH:$FLUME_HOME/bin---source .zshrc 4.验证` $&gt; flume-ng version` //next generation.下一代. 3.配置flume准备工作3.0如果 yum 没有设置网络源的, 设置一下 阿里 | 网易123456789101112# 先备份原来的 CentOS-Base.repo 为 CentOS-Base.repo.bakmv CentOS-Base.repo CentOS-Base.repo.bak# 下载阿里基本源 $&gt;sudo wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo# 下载阿里epel源$&gt;sudo wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo# 生成缓存文件$&gt;sudo yum clean all$&gt;sudo yum makecache 3.1 所有机器安装 nc 或者 telnet123$&gt;yum search nc$&gt; xcall.sh sudo yum install -y nc.x86_64$&gt; xcall.sh sudo yum install -y telnet 3.2 使用 nc123456789101112cs1_1&gt; nc -lk 8888 : 开启8888端口监听-------再开一个cs1 sessioncs1_2&gt; netstat -anop | grep 8888 : 查看是有有此端口或者 &gt; lsof -i tcp:8888 : 可以直接看到 pidtcp 0 0 0.0.0.0:8888 0.0.0.0:* LISTEN 17251/nc off (0.00/0/0)---------有的话就可以测试连接了 (客户端连接)cs1_2&gt; nc localhost 8888 ==================telnet 使用方式类似, 可查看帮助 4. flume配置官方文档 总体概述 实时处理架构flume: 监控日志文件 每次产生一条新的数据, 会被实时的收集到下一跳 Kafka: 消息系统 对消息进行简单处理, 当然, 具有简单缓冲作用 Storm / SparkStreaming: 流式的分布式计算引擎 Redis: 缓存组件 内存数据库 Hbase / HDFS /ES …: 最后持久化到这些地方.. 4.1 Flume Source注意: 启动 flume 配置的时候, 后面加上 -Dflume.root.logger=INFO,console 还有一种更详细的 : -Dflume.root.logger=DEBUG,console 会打印更详细的日志 4.1.1 netcat通过消息通信的方式, 进行采集 1234567891011121314151617181920212223242526272829303132331.创建配置文件 [/soft/flume/conf/helloworld.conf] #声明三种组件 a1.sources = r1 a1.channels = c1 a1.sinks = k1 #定义source信息 a1.sources.r1.type=netcat a1.sources.r1.bind=localhost a1.sources.r1.port=8888 #定义sink信息 a1.sinks.k1.type=logger #定义channel信息 a1.channels.c1.type=memory #绑定在一起 ps: 注意: 一个source(源) 可以输出到多个 channels(通道) 一个sink(沉漕)只能从一个 channel(通道)中获取数据 a1.sources.r1.channels=c1 a1.sinks.k1.channel=c1 2.运行 a)启动flume agent $&gt; bin/flume-ng agent -f conf/helloworld.conf -n a1 -Dflume.root.logger=INFO,console b)启动nc的客户端 $&gt;nc localhost 8888 $nc&gt;hello world c)在flume的终端输出hello world. 4.1.2 exec实时日志收集,实时收集日志。 测试: 先启动监控, 再创建文件, 能否监控到 结论: 可以监控到 123456789101112131415161718192021222324252627281. 配置 # 声明3种组件 a1.sources=r1 a1.channels=c1 a1.sinks=k1 # 定义 source 信息 a1.sources.r1.type=exec a1.sources.r1.command=tail -F /home/ap/test.txt #收集最后的10行 a1.sources.r1.command=tail -F -c +0 /home/ap/test.txt #从第0行开始收集 # 定义 sink 信息 a1.sinks.k1.type=logger # 定义 channel 信息 a1.channels.c1.type=memory # 绑定在一起 a1.sources.r1.channels=c1 a1.sinks.k1.channel=c12&gt; 新建文件 test.txt3&gt; 启动 [ap@cs1]~% flume-ng agent -f apps/flume/conf/confs/exec.conf -n a1 -Dflume.root.logger=INFO,console4&gt; 修改 test.txt 文件$&gt; echo ddd &gt; test.txt 观察 agent 控制台变化 4.1.3 批量收集(spool)监控一个文件夹，静态文件。 文件最好是直接从外部移入, 文件夹内最好不要做文件编辑。 收集完之后，会重命名文件成新文件。.compeleted. 之后就不会再次处理这个文件了. 只会监控新增加的文件, 不会监控删除的本地文件. 123456789101112131415161718192021222324252627a) 配置 # 声明3种组件 a1.sources=r1 a1.channels=c1 a1.sinks=k1 # 定义 source 信息 a1.sources.r1.type=spooldir a1.sources.r1.spoolDir=/home/ap/spool a1.sources.r1.fileHeader=true # 定义 sink 信息 a1.sinks.k1.type=logger # 定义 channel 信息 a1.channels.c1.type=memory # 绑定在一起 a1.sources.r1.channels=c1 a1.sinks.k1.channel=c1b)创建目录 $&gt;mkdir ~/spoolc)启动flume可以直接启动 $&gt;flume-ng agent -f apps/flume/conf/confs/spool.conf -n a1 -Dflume.root.logger=INFO,console 4.1.4 序列sources (seq)一个简单的序列生成器, 它连续生成一个计数器, 它从0开始, 增量为 1, 并在 totalEvents 停止。 主要用于测试。 1234567891011121314151617181920212223241) 配置# 声明3种组件a1.sources=r1a1.channels=c1a1.sinks=k1# 定义 source 信息a1.sources.r1.type=seqa1.sources.r1.totalEvents=1000# 定义 sink 信息a1.sinks.k1.type=logger# 定义 channel 信息a1.channels.c1.type=memory# 绑定在一起a1.sources.r1.channels=c1a1.sinks.k1.channel=c12) 运行$&gt;bin/flume-ng agent -f conf/confs/helloworld.seq.conf -n a1 -Dflume.root.logger=INFO,console 4.1.5 压力 source (用于压力测试)123456a1.sources = stresssource-1a1.channels = memoryChannel-1a1.sources.stresssource-1.type = org.apache.flume.source.StressSourcea1.sources.stresssource-1.size = 10240a1.sources.stresssource-1.maxTotalEvents = 1000000a1.sources.stresssource-1.channels = memoryChannel-1 4.1.6 Multiplexing Channel Selector :TODOflume 多路复用 官网 4.2. Flume Sink沉漕, source 经过 channel, 最后下沉到 sink, 再由 sink 输出 注意, flume 后面加上 &amp;是指后台运行 官网 4.2.1输出 (sink) 到 HDFS123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384851) 配置===================================================================a1.sources = r1a1.channels = c1a1.sinks = k1a1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 8888a1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H/%M/%Sa1.sinks.k1.hdfs.filePrefix = events-# round目录 是否会产生新目录,每十分钟产生一个新目录,一般控制的目录方面。#2017-12-12 --&gt;#2017-12-12 --&gt;%H%M%S比如 写上 1 / day, 就是一天产生的数据文件都在 一个日期的目录下.---a1.sinks.k1.hdfs.round = true a1.sinks.k1.hdfs.roundValue = 1a1.sinks.k1.hdfs.roundUnit = day#使用本地时间戳为时间序列头 a1.sinks.k1.hdfs.useLocalTimeStamp=true#是否产生新文件。 -------------只要3个条件中某一个满足, 就会滚动一个文件# roll滚动, 此事件内的日志会写入到一个文件# 目前的猜测: 10个字节就会触发, 如果10秒内 0 &lt; input &lt; 10字节, 也会滚动 =&gt; 是的# 等待滚动当前的文件 10秒,Number of seconds to wait before rolling current file (0 = never roll based on time interval)a1.sinks.k1.hdfs.rollInterval=10 # 这是指10个字节就触发滚动 File size to trigger roll, in bytes (0: never roll based on file size) a1.sinks.k1.hdfs.rollSize=10 # Number of events written to file before it rolled (0 = never roll based on number of events) 一行就是一个事件 a1.sinks.k1.hdfs.rollCount=3 a1.channels.c1.type=memorya1.sources.r1.channels = c1a1.sinks.k1.channel = c1-----------------# sink其它可选配置参数(详情可查官网)agent1.sinks.sink1.type = hdfs#a1.sinks.k1.channel = c1agent1.sinks.sink1.hdfs.path =hdfs://myha01/weblog/flume-event/%y-%m-%d/%H-%M agent1.sinks.sink1.hdfs.filePrefix = tomcat_agent1.sinks.sink1.hdfs.maxOpenFiles = 5000agent1.sinks.sink1.hdfs.batchSize= 100agent1.sinks.sink1.hdfs.fileType = DataStreamagent1.sinks.sink1.hdfs.writeFormat =Textagent1.sinks.sink1.hdfs.rollSize = 102400agent1.sinks.sink1.hdfs.rollCount = 1000000agent1.sinks.sink1.hdfs.rollInterval = 60agent1.sinks.sink1.hdfs.round = trueagent1.sinks.sink1.hdfs.roundValue = 10agent1.sinks.sink1.hdfs.roundUnit = minuteagent1.sinks.sink1.hdfs.useLocalTimeStamp = true&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;如果 HDFS 集群是高可用集群，那么必须要放入 core-site.xml 和 hdfs-site.xml 文件到 $FLUME_HOME/conf/confs 目录中, 就是跟配置文件同级目录.&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;2) 开启agent===================================================================[ap@cs1]~/apps/flume/conf/confs% flume-ng agent -f hdfs.conf -n a13) nc 连接===================================================================nc local 8888连接上了之后, 发出去的消息就会写入 hdfs 4) 写入时, 完整的 log 是这样的===================================================================18/06/30 18:39:18 INFO hdfs.HDFSSequenceFile: writeFormat = Writable, UseRawLocalFileSystem = false18/06/30 18:39:18 INFO hdfs.BucketWriter: Creating /user/ap/flume/events/18-06-30/18/39/00/events-.1530355158463.tmp18/06/30 18:39:28 INFO hdfs.BucketWriter: Closing /user/ap/flume/events/18-06-30/18/39/00/events-.1530355158463.tmp18/06/30 18:39:28 INFO hdfs.BucketWriter: Renaming /user/ap/flume/events/18-06-30/18/39/00/events-.1530355158463.tmp to /user/ap/flume/events/18-06-30/18/39/00/events-.153035515846318/06/30 18:39:28 INFO hdfs.HDFSEventSink: Writer callback called.5) 查看 ===================================================================注意: 序列文件用 text 查看 4.2.2 输出到 Hive写入太慢, 因为要转为 MR, 所以一般不会用 4.2.3 输出 (sink) 到 HBase123456789101112131415161718192021222324253.1) 配置文件===================================================================a1.sources = r1a1.channels = c1a1.sinks = k1a1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 8888a1.sinks.k1.type = hbasea1.sinks.k1.table = ns1:t12a1.sinks.k1.columnFamily = f1a1.sinks.k1.serializer = org.apache.flume.sink.hbase.RegexHbaseEventSerializera1.channels.c1.type=memorya1.sources.r1.channels = c1a1.sinks.k1.channel = c13.2) 开启 nc 输入===================================================================3.3) 在 hbase shell 中 scan 表 'ns1:t12'=================================================================== 4.2.4 输出到 kafka:TODO 4.3. Flume Channel4.3.1.Memory Channel以上用的都是Memory Channel, 不再显示 4.3.2 FileChannel1234567891011121314151617181920212223242526271) 配置===================================================================a1.sources = r1a1.sinks= k1a1.channels = c1a1.sources.r1.type=netcata1.sources.r1.bind=localhosta1.sources.r1.port=8888a1.sinks.k1.type=loggera1.channels.c1.type = filea1.channels.c1.checkpointDir = /home/centos/flume/fc_checka1.channels.c1.dataDirs = /home/centos/flume/fc_dataa1.sources.r1.channels=c1a1.sinks.k1.channel=c12) 创建的文件大概是这样===================================================================$~/flumedata&gt; ls -r fc*fc_data:log-1.meta log-1 in_use.lockfc_check:queueset in_use.lock inflighttakes inflightputs checkpoint.meta checkpoint 4.3.3 可溢出文件通道This channel is currently experimental and not recommended for use in production. 123456789101112[spillable_channel.conf]a1.channels = c1a1.channels.c1.type = SPILLABLEMEMORY#0表示禁用内存通道，等价于文件通道a1.channels.c1.memoryCapacity = 0#0,禁用文件通道，等价内存通道。a1.channels.c1.overflowCapacity = 2000a1.channels.c1.byteCapacity = 800000a1.channels.c1.checkpointDir = /user/centos/flume/fc_checka1.channels.c1.dataDirs = /user/centos/flume/fc_data 4.4 使用AvroSource和AvroSink实现跃点agent处理 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869 [avro_hop.conf]############ a1 ############# 声明3种组件a1.sources=r1a1.channels=c1a1.sinks=k1# 定义 source 信息a1.sources.r1.type=netcata1.sources.r1.bind=localhosta1.sources.r1.port=8888# 定义 sink 信息a1.sinks.k1.type=avroa1.sinks.k1.hostname=localhosta1.sinks.k1.port=9999# 定义 channel 信息a1.channels.c1.type=memory# 绑定在一起a1.sources.r1.channels=c1a1.sinks.k1.channel=c1############ a2 ############# 声明3种组件a2.sources=r2a2.channels=c2a2.sinks=k2# 定义 source 信息# source 是 avro, 绑定端口9999a2.sources.r2.type=avroa2.sources.r2.bind=localhosta2.sources.r2.port=9999# 定义 sink 信息# sink 打印到控制台a2.sinks.k2.type=logger# 定义 channel 信息a2.channels.c2.type=memory# 绑定在一起a2.sources.r2.channels=c2a2.sinks.k2.channel=c22.启动a2===================================================================$&gt;flume-ng agent -f /soft/flume/conf/avro_hop.conf -n a2 -Dflume.root.logger=INFO,console3.验证a2===================================================================$&gt;netstat -anop | grep 99994.启动a1===================================================================$&gt;flume-ng agent -f /soft/flume/conf/avro_hop.conf -n a15.验证a1===================================================================$&gt;netstat -anop | grep 88886.nc 连接a1 &amp; 发消息===================================================================$&gt;nc localhost 8888 4.5 Flume 的高可用配置 是通过配置优先级来实现主从, 优先级是组内的优先级, 需要先设置为一组 优先级高的 down 掉了, 再启动起来, 依然是主 ==&gt; 已验证 1234567891011==============================--设置优先级代配置如下----------#set sink group 设置为一组a1.sinkgroups.g1.sinks = k1 k2#set failover 设置容灾相关的参数a1.sinkgroups.g1.processor.type = failover# 设置 组内成员的优先级, 决定 主备a1.sinkgroups.g1.processor.priority.k1 = 10 a1.sinkgroups.g1.processor.priority.k2 = 1 a1.sinkgroups.g1.processor.maxpenalty = 10000 以下配置方案与上图不同的, 用的是1个 channel, 2个`sink` 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011. 配置 agent ==&gt; cs1, cs2================================================================================================[ha_agent.conf]------------------#agent name: a1 a1.channels = c1 a1.sources = r1a1.sinks = k1 k2#set gruopa1.sinkgroups = g1#set channela1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100# set sourcesa1.sources.r1.channels = c1a1.sources.r1.type = execa1.sources.r1.command = tail -F /home/ap/flumedata/testha.log# 这里有疑问 : 这里设置拦截器的意义?? 随便设置的吗?? :TODOa1.sources.r1.interceptors = i1 i2 a1.sources.r1.interceptors.i1.type = static a1.sources.r1.interceptors.i1.key = Type a1.sources.r1.interceptors.i1.value = LOGIN a1.sources.r1.interceptors.i2.type = timestamp# set sink1 -&gt; 通过 avro 沉到 cs3的 source 上a1.sinks.k1.channel = c1 a1.sinks.k1.type = avro a1.sinks.k1.hostname = cs3 a1.sinks.k1.port = 52020# set sink2 -&gt; 通过 avro 沉到 cs4的 source 上a1.sinks.k2.channel = c1 a1.sinks.k2.type = avro a1.sinks.k2.hostname = cs4 a1.sinks.k2.port = 52020#set sink group 设置为一组a1.sinkgroups.g1.sinks = k1 k2#set failover 设置容灾相关的参数a1.sinkgroups.g1.processor.type = failover# 设置 组内成员的优先级, 决定 主备a1.sinkgroups.g1.processor.priority.k1 = 10 a1.sinkgroups.g1.processor.priority.k2 = 1 a1.sinkgroups.g1.processor.maxpenalty = 100002. 配置 collector ==&gt; cs3,cs4================================================================================================[ha_collector.conf]#set agent name a1.sources = r1a1.channels = c1 a1.sinks = k1#set channela1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100# other node,nna to nnsa1.sources.r1.type = avro# :TODO 这些 key, value 设置了之后, 用在什么地方???## 当前主机是什么，就修改成什么主机名 a1.sources.r1.bind = cs3 a1.sources.r1.port = 52020 a1.sources.r1.interceptors = i1 a1.sources.r1.interceptors.i1.type = static a1.sources.r1.interceptors.i1.key = Collector## 当前主机是什么，就修改成什么主机名 a1.sources.r1.interceptors.i1.value = cs3 a1.sources.r1.channels = c1#set sink to hdfsa1.sinks.k1.type=hdfsa1.sinks.k1.hdfs.path=/user/ap/flume/event_ha/loghdfs a1.sinks.k1.hdfs.fileType=DataStream a1.sinks.k1.hdfs.writeFormat=TEXT a1.sinks.k1.hdfs.rollInterval=10a1.sinks.k1.channel=c1 a1.sinks.k1.hdfs.filePrefix=%Y-%m-%d3. 启动================================================================================================# 先启动 cs3,cs4 ==&gt; collector $&gt; [ap@cs4]~/apps/flume/conf/confs% flume-ng agent -f ha_collector.conf -n a1 -Dflume.root.logger=DEBUG,console# 再启动 cs1, cs2 ==&gt; agent, 连接 collector[ap@cs1]~/apps/flume/conf/confs% flume-ng agent -f ha_agent.conf -n a1 -Dflume.root.logger=DEBUG,console 4.6 实用业务场景知识点: 拦截器 interceptors 4.6.1 需求: A、B 两台日志服务机器实时生产日志主要类型为 access.log、nginx.log、web.log 现在要求: 把 A、B 机器中的 access.log、nginx.log、web.log 采集汇总到 C 机器上然后统一收集到 hdfs 中。 但是在 hdfs 中要求的目录为: /source/logs/access/20160101/** /source/logs/nginx/20160101/** /source/logs/web/20160101/** 4.6.2 需求分析需求分析图示 数据处理流程分析 在每一台节点上都要 搜集 3个 source 源 搜集完成后发往同一台节点 4.6.3 需求实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261.在cs1,cs2上配置flume, 通过 avro, sink到cs3=============================================================[exec_source_avro_sink.conf]------------------------------# 指定各个核心组件 a1.sources = r1 r2 r3 a1.sinks = k1 a1.channels = c1# 准备数据源# static 拦截器的功能就是往采集到的数据的 header 中插入自己定义的 key-value 对 # sources-&gt;r1a1.sources.r1.type = execa1.sources.r1.command = tail -F -c +0 /home/ap/flumedata/access.log a1.sources.r1.interceptors = i1a1.sources.r1.interceptors.i1.type = statica1.sources.r1.interceptors.i1.key = typea1.sources.r1.interceptors.i1.value = access# sources-&gt;r2a1.sources.r2.type = execa1.sources.r2.command = tail -F -c +0 /home/ap/flumedata/nginx.log a1.sources.r2.interceptors = i2a1.sources.r2.interceptors.i2.type = static a1.sources.r2.interceptors.i2.key = type a1.sources.r2.interceptors.i2.value = nginx# sources-&gt;r3a1.sources.r3.type = execa1.sources.r3.command = tail -F -c +0 /home/ap/flumedata/web.log a1.sources.r3.interceptors = i3a1.sources.r3.interceptors.i3.type = static a1.sources.r3.interceptors.i3.key = type a1.sources.r3.interceptors.i3.value = web# Describe the sinka1.sinks.k1.type = avro a1.sinks.k1.hostname = cs3 a1.sinks.k1.port = 41414# Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 20000 a1.channels.c1.transactionCapacity = 10000# Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sources.r2.channels = c1 a1.sources.r3.channels = c1 a1.sinks.k1.channel = c12. 在 cs3上配置 flume, source 为 avro, 接受来自 cs1,cs2下沉的数据======================================================================[avro_source_hdfs_sink.conf]-------------------------------#定义 agent 名， source、channel、sink 的名称 a1.sources = r1a1.sinks = k1a1.channels = c1#定义 source a1.sources.r1.type = avro a1.sources.r1.bind = localhosta1.sources.r1.port =41414#添加时间拦截器#Note:For all of the time related escape sequences, a header with the key “timestamp” must #exist among the headers of the event (unless hdfs.useLocalTimeStamp is set to true). One way #to add this automatically is to use the TimestampInterceptor. a1.sources.r1.interceptors = i1 a1.sources.r1.interceptors.i1.type=org.apache.flume.interceptor.TimestampInterceptor$Builder#定义 channelsa1.channels.c1.type = memory #The maximum number of events stored in the channela1.channels.c1.capacity = 20000 #The maximum number of events the channel will take from a source or give to a sink per transactiona1.channels.c1.transactionCapacity = 10000#定义 sinka1.sinks.k1.type = hdfs a1.sinks.k1.hdfs.path = /user/ap/flume/event/%&#123;type&#125;/%Y%m%d a1.sinks.k1.hdfs.filePrefix = eventsa1.sinks.k1.hdfs.fileType = DataStreama1.sinks.k1.hdfs.writeFormat = Text#时间类型a1.sinks.k1.hdfs.useLocalTimeStamp = true #生成的文件不按条数生成, 否则就是按照设定的 event产生的, 1行就是一个 event #Number of events written to file before it rolled (0 = never roll based on number of events)a1.sinks.k1.hdfs.rollCount = 0#生成的文件按时间生成,单位是秒#Number of seconds to wait before rolling current file (0 = never roll based on time interval)a1.sinks.k1.hdfs.rollInterval = 30#生成的文件按大小生成, 单位是 bytes#1 MB = 1,024 KB = 1,048,576 Bytes #File size to trigger roll, in bytes (0: never roll based on file size)a1.sinks.k1.hdfs.rollSize = 10485760 #这里是10MB#批量写入 hdfs 的个数a1.sinks.k1.hdfs.batchSize = 20#flume 操作 hdfs 的线程数(包括新建，写入等) a1.sinks.k1.hdfs.threadsPoolSize=10#操作 hdfs 超时时间a1.sinks.k1.hdfs.callTimeout=30000#组装 source、channel、sink a1.sources.r1.channels = c1 a1.sinks.k1.channel = c13.启动服务============================================================================# 启动cs3 =&gt; 服务器 c[ap@cs3]~/apps/flume/conf/confs% flume-ng agent -f avro_source_hdfs_sink.conf -n a1 -Dflume.root.logger=DEBUG,console# 启动cs1, cs2 =&gt; 服务器 a, b# 注意: 千万别启错了, 否则很难找到原因..[ap@cs2]~/apps/flume/conf/confs% flume-ng agent -f exec_source_avro_sink.conf -n a1 -Dflume.root.logger=DEBUG,console==============遇到问题: 这种通过avro多点 sink 到一个 source 的时候, 实现不创建文件夹/文件的, 好像确实写入不进去...===&gt; md, 原因应该是启动错配置了 5.Flume的Maven依赖123456789101112131415161718192021222324252627282930313233&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.rox&lt;/groupId&gt; &lt;artifactId&gt;Flume_test&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt; &lt;version&gt;1.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flume.flume-ng-sinks&lt;/groupId&gt; &lt;artifactId&gt;flume-hdfs-sink&lt;/artifactId&gt; &lt;version&gt;1.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flume.flume-ng-sinks&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-hbase-sink&lt;/artifactId&gt; &lt;version&gt;1.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flume.flume-ng-channels&lt;/groupId&gt; &lt;artifactId&gt;flume-file-channel&lt;/artifactId&gt; &lt;version&gt;1.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt;]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Azkaban]]></title>
    <url>%2F2018%2F06%2F29%2FHadoop%2F6-Sqoop%26Azkaban%2FAzkaban%2F</url>
    <content type="text"><![CDATA[1.出现契机应用场景举例: 1、通过 Hadoop 先将原始数据同步到 HDFS 上; 2、借助 MapReduce 计算框架对原始数据进行清洗转换，生成的数据以分区表的形式存储到 多张 Hive 表中; 3、需要对 Hive 中多个表的数据进行 Join 处理，得到一个明细数据 Hive 大表; 4、将明细数据进行各种统计分析，得到结果报表信息; 5、需要将统计分析得到的结果数据同步到业务系统中，供业务调用使用。 2.常见工作流调度系统 &amp; 对比在 Hadoop 领域，常见的工作流调度器有 Oozie，Azkaban，Cascading，Hamake 等 2.1.各种调度工具对比 2.2.Azkaban 与 Oozie 对比ooize 相比 azkaban 是一个重量级的任务调度系统，功能全面，但配置使用也更复杂。如果可以不 在意某些功能的缺失，轻量级调度器 azkaban 是很不错的候选对象。  工作流定义 Azkaban 使用 Properties 文件定义工作流 Oozie 使用 XML 文件定义工作流  定时执行 Azkaban 的定时执行任务是基于时间的 Oozie 的定时执行任务基于时间和输入数据 2.3.Azkaban 组件关系关系数据库(目前仅支持 MySQL) web 管理服务器-AzkabanWebServer 执行服务器-AzkabanExecutorServer Azkaban 使用 MySQL 来存储它的状态信息，Azkaban Executor Server 和 Azkaban Web Server 均使用到了 MySQL 数据库。 它有如下功能特点:  Web 用户界面  方便上传工作流  方便设置任务之间的关系  调度工作流  认证/授权(权限的工作)  能够杀死并重新启动工作流  模块化和可插拔的插件机制  项目工作区  工作流和任务的日志记录和审计 3.Azkaban 安装部署3.1准备工作Azkaban Web 服务器:azkaban-web-server-2.5.0.tar.gz Azkaban Excutor 执行服务器 :azkaban-executor-server-2.5.0.tar.gz Azkaban 初始化脚本文件:azkaban-sql-script-2.5.0.tar.gz 3.2安装说明将安装文件上传到集群,最好上传到安装hive、sqoop 的机器上,方便命令的执行。并最好同 一存放在 apps 目录下,用于存放源安装文件.新建 azkaban 目录,用于存放 azkaban 运行程序 3.3解压123tar -zxvf azkaban-web-server-2.5.0.tar.gz -C /home/apps/ap/azkaban/tar -zxvf azkaban-executor-server-2.5.0.tar.gz -C /home/apps/ap/azkaban/tar -zxvf azkaban-sql-script-2.5.0.tar.gz -C /home/apps/ap/azkaban/ 3.4进入 mysql 导入任务 job 表12345mysql&gt; create database azkaban; Query OK, 1 row affected (0.01 sec)mysql&gt; use azkaban; Database changedmysql&gt; source /home/hadoop/apps/azkaban/azkaban-script-2.5.0/create-all-sql-2.5.0.sql; 3.5创建 SSL 配置最好是在 azkaban 目录下执行: 123456789101112131415161718192021222324252627282930# 执行命令: keytool -keystore keystore -alias jetty -genkey -keyalg RSA# 运行此命令后,会提示输入当前生成 keystore 的密码及相应信息,输入密码请劳记,信息如下:Enter keystore password:Re-enter new password:What is your first and last name? [Unknown]:What is the name of your organizational unit? [Unknown]:What is the name of your organization? [Unknown]:What is the name of your City or Locality? [Unknown]:What is the name of your State or Province? [Unknown]:What is the two-letter country code for this unit? [Unknown]: CNIs CN=Unknown, OU=Unknown, O=Unknown, L=Unknown, ST=Unknown, C=CN correct? [no]: yEnter key password for &lt;jetty&gt;(RETURN if same as keystore password):此时, 会发现目录下有一个 Keystore 文件------------------------------------------完成上述工作后,将在当前目录生成 keystore 证书文件,将 keystore 拷贝到 azkaban web 服务 器根目录中.如:cp keystore azkaban-web-2.5.0 3.6 修改配置文件注意: 配置文件后面一定不要有空格 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182831) 生成时区配置文件 Asia/Shanghai，用交互式命令 tzselect 即可, 如果有的话就不用再生成了 tzselect 2) 拷贝该时区文件，覆盖系统本地时区配置 sudo cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime3)  azkaban web 服务器配置 3.1)  配置 web 的 azkaban.properties  cd ~/apps/azkaban/azkaban-web-2.5.0/conf/ vi azkaban.properties 以下是需要修改的内容 #默认根 web 目录web.resource.dir=/home/ap/apps/azkaban/azkaban-web-2.5.0/web#默认时区,已改为亚洲/上海 默认为美国default.timezone.id=Asia/Shanghai#用户配置,具体配置参加下文user.manager.xml.file=/home/ap/apps/azkaban/azkaban-web-2.5.0/conf/azkaban-users.xml#global配置文件所在位置executor.global.properties=/home/ap/apps/azkaban/azkaban-executor-2.5.0/conf/global.properties#数据库相关配置mysql.host=cs2mysql.database=azkabanmysql.user=rootmysql.password=123#SSL 文件名jetty.keystore=/home/ap/apps/azkaban/azkaban-web-2.5.0/keystore#SSL 文件密码jetty.password=123456#Jetty 主密码 与 keystore 文件相同jetty.keypassword=123456#SSL 文件名jetty.truststore=/home/ap/apps/azkaban/azkaban-web-2.5.0/keystore# SSL 文件密码jetty.trustpassword=1234563.2) 配置 web 的 azkaban-users.xml # 增加 管理员用户vi azkaban-users.xml &lt;azkaban-users&gt; &lt;user username="azkaban" password="azkaban" roles="admin" groups="azkaban"/&gt; &lt;user username="metrics" password="metrics" roles="metrics"/&gt; # 此行是加的, 指用户名密码都为 admin &lt;user username="admin" password="admin" roles="admin,metrics"/&gt; &lt;role name="admin" permissions="ADMIN"/&gt; &lt;role name="metrics" permissions="METRICS"/&gt; &lt;/azkaban-users&gt; 4)  配置执行服务器 executor  cd apps/azkaban/azkaban-executor-2.5.0/conf/ vi azkaban.properties-----以下是需要修改的内容------#时区default.timezone.id=Asia/Shanghai # Azkaban JobTypes 插件配置，插件所在位置 azkaban.jobtype.plugin.dir=/home/hadoop/apps/azkaban/azkaban-executor-2.5.0/plugins/jobtypes#Loader for projectsexecutor.global.properties=/home/ap/apps/azkaban/azkaban-executor-2.5.0/conf/global.properties#mysql 需要修改的地方mysql.host=cs2mysql.database=azkabanmysql.user=rootmysql.password=1235) 配置环境变量 vi .zshrc # azkabanexport AZKABAN_WEB_HOME=/home/ap/apps/azkaban/azkaban-web-2.5.0export AZKABAN_EXE_HOME=/home/ap/apps/azkaban/azkaban-executor-2.5.0export PATH=$PATH:$AZKABAN_WEB_HOME/bin:$AZKABAN_EXE_HOME/bin 3.7 启动12345678910111213141516171819201)  启动 Azkaban Web Server: # ps: .out 是日志, 可以先创建 logs 目录, 日志地址随便# 前台启动azkaban-web-start.sh# 后台启动nohup azkaban-web-start.sh 1&gt;/home/ap/logs/azwebstd.out 2&gt;/home/ap/logs/azweberr.out &amp;2)  启动 Azkaban Executor: #前台启动azkaban-executor-start.sh# 后台启动nohup azkaban-executor-start.sh 1&gt;/home/ap/logs/azexstd.out 2&gt;/home/ap/logs/azexerr.out &amp;3)  登录 webUI https://cs2:8443/u: adminp: admin# 注意: 配置后面不能有空格!!! 4.Azkaban 实战演示见这里]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Azkaban</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>Azkaban</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sqoop]]></title>
    <url>%2F2018%2F06%2F28%2FHadoop%2F6-Sqoop%26Azkaban%2FSqoop%2F</url>
    <content type="text"><![CDATA[1. 作用 Sqoop 是 Apache 旗下一款“Hadoop 和关系数据库服务器之间传送数据”的工具。 导入数据：MySQL，Oracle 导入数据到 Hadoop 的 HDFS、HIVE、HBASE 等数据存储系统 导出数据：从 Hadoop 的文件系统中导出数据到关系数据库 MySQL 等 Sqoop 的本质还是一个命令行工具，和 HDFS，Hive 相比，并没有什么高深的理论。 2.工作机制将导入导出命令翻译成 MapReduce 程序来实现 在翻译出的 MapReduce 中主要是对 InputFormat 和 OutputFormat 进行定制 3.安装注意: 目录下要有hive, 因为要拿到 hive 的 home, 执行 hive 操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253541、 准备安装包 sqoop-1.4.6.bin_hadoop-2.0.4-alpha.tar.gz2、 解压安装包到安装目录tar -zxvf sqoop-1.4.6.bin_hadoop-2.0.4-alpha.tar.gz -C apps/ cd appsmv sqoop-1.4.6.bin_hadoop-2.0.4-alpha/ sqoop-1.4.63、 进入到 conf 文件夹，找到 sqoop-env-template.sh，修改其名称为 sqoop-env.sh cd confmv sqoop-env-template.sh sqoop-env.sh4、 修改 sqoop-env.sh-----export HADOOP_COMMON_HOME=/home/ap/apps/hadoop export HADOOP_MAPRED_HOME=/home/ap/apps/hadoop export HBASE_HOME=/home/ap/apps/hbaseexport HIVE_HOME=/home/ap/apps/hiveexport ZOOCFGDIR=/home/ap/apps/zookeeper/conf-----5、 加入 mysql 驱动包到 sqoop-1.4.6/lib 目录下cp mysql-connector-java-5.1.40-bin.jar ~/apps/sqoop-1.4.6/lib/6、 配置系统环境变量 vi ~/.bashrc然后输入:export SQOOP_HOME=/home/hadoop/apps/sqoop-1.4.6 export PATH=$PATH:$SQOOP_HOME/bin然后保存退出source ~/.bashrc7、 验证安装是否成功sqoop-version 或者 sqoopversionps : 吹出现警告, 不用管---[ap@cs2]~% sqoop versionWarning: /home/ap/apps/sqoop/../hcatalog does not exist! HCatalog jobs will fail.Please set $HCAT_HOME to the root of your HCatalog installation.Warning: /home/ap/apps/sqoop/../accumulo does not exist! Accumulo imports will fail.Please set $ACCUMULO_HOME to the root of your Accumulo installation.Warning: /home/ap/apps/sqoop/../zookeeper does not exist! Accumulo imports will fail.Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.18/06/28 20:43:52 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6Sqoop 1.4.6git commit id c0c5a81723759fa575844a0a1eae8f510fa32c25Compiled by root on Mon Apr 27 14:38:36 CST 20158\  可以配置记住密码 但是好像不起作用???查看任务详细或者执行任务的时候不输入密码 免密[sqoop-site.xml]添加: &lt;property&gt; &lt;name&gt;sqoop.metastore.client.record.password&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, allow saved passwords in the metastore.&lt;/description&gt; &lt;/property&gt; 4. 基本使用121) sqoop help : 查看帮助2) sqoop help import : 进一步层级查看 5. Sqoop 数据导入常用指令12345678910--connect 指定数据库链接url--username 指定数据库的用户名--password 指定数据库的密码--table 指定要导出数据的mysql数据库表-m 指定MapTask的个数--target-dir 指定导出数据在HDFS上的存储目录--fields-terminated-by 指定每条记录中字段之间的分隔符--where 指定查询SQL的where条件--query 指定查询SQL--columns 指定查询列 5.1 list mysql 的数据库 &amp; 表, 复制mysql 中表结构相同表 –&gt; hive12345678910111213141516171819202122列出MySQL数据有哪些数据库：sqoop list-databases \--connect jdbc:mysql://cs2:3306/ \--username root \--password 123列出MySQL中的某个数据库有哪些数据表：sqoop list-tables \--connect jdbc:mysql://cs2:3306/mysql \--username root \--password 123创建一张跟mysql中的help_keyword表一样的hive表hk：(没有数据貌似)sqoop create-hive-table \--connect jdbc:mysql://cs2:3306/mysql \--username root \--password 123 \--table help_keyword \--hive-table hk 5.2 导入 mysql中表 到 HDFS123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081导入MySQL表中数据到HDFS中：// 普通导入：导入mysql库中的help_keyword的数据到HDFS上的默认路径：/user/ap/help_keywordsqoop import \--connect jdbc:mysql://cs2:3306/mysql \--username root \--password 123 \--table help_keyword \-m 1最后的1,是指使用1个 mapTask 导入到hdfs后, 可以使用 hdfs dfs -text /.... 查看数据----// 导入： 指定分隔符 &amp; 导入路径 &amp; mapTask 个数sqoop import \--connect jdbc:mysql://cs2:3306/mysql \--username root \--password 123 \--table help_keyword \--target-dir /user/ap/my_help_keyword1 \--fields-terminated-by '\t' \-m 3// 导入数据：带where条件sqoop import \--connect jdbc:mysql://cs2:3306/mysql \--username root \--password 123 \--where "name='STRING' " \--table help_keyword \--target-dir /user/ap/my_help_keyword2 \-m 1// 查询指定列-------------发现一个结论, 如果启动3个 mapper, 但是最后只有一个结论, 只会生成一个结果文件sqoop import \--connect jdbc:mysql://cs2:3306/mysql \--username root \--password 123 \--columns "name" \--where "name='STRING' " \--table help_keyword \--target-dir /user/ap/my_help_keyword3 \-m 3// 导入：指定自定义查询SQL--------------疑点: split-by 的作用, 和 -m 数量的关系sqoop import \--connect jdbc:mysql://cs2:3306/mysql \--username root \--password 123 \--target-dir /user/ap/my_help_keyword4 \--query 'select help_keyword_id,name from help_keyword WHERE $CONDITIONS and name = "STRING"' \--split-by help_keyword_id \--fields-terminated-by '\t' \-m 1----------------------------sqoop import \--connect jdbc:mysql://cs2:3306/mysql \--username root \--password 123 \--target-dir /user/hadoop/my_help_keyword5 \--query "select help_keyword_id,name from help_keyword WHERE \$CONDITIONS" \--split-by help_keyword_id \--fields-terminated-by '\t' \-m 1# 在以上需要按照自定义SQL语句导出数据到HDFS的情况下：1、引号问题，要么外层使用单引号，内层使用双引号，$CONDITIONS的$符号不用转义， 要么外层使用双引号，那么内层使用单引号，然后$CONDITIONS的$符号需要转义2、自定义的SQL语句中必须带有WHERE \$CONDITIONS 5.3 导入MySQL数据库中的表数据到Hive中：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051导入MySQL数据库中的表数据到Hive中：// 普通导入：数据存储在默认的default hive库中，表名就是对应的mysql的表名：// 通过对比发现, sqoop 是默认导入到 hdfs 的, 导入到hdfs时, 不用加额外的参数sqoop import \--connect jdbc:mysql://cs2:3306/mysql \--username root \--password 123 \--table help_keyword \--hive-import \-m 1hadoop fs -cat /user/myhive/warehouse/help_keyword/part-m-00000 // 查看数据当然也可以在 hive 中查看数据// 指定行分隔符和列分隔符，指定hive-import，指定覆盖导入，指定自动创建hive表，指定表名，指定删除中间结果数据目录 Database does not exist: mydb_test ???注意: 要先创建 database mydb_test问题: 为什么导出后, 变成了4个块sqoop import \--connect jdbc:mysql://cs2:3306/mysql \--username root \--password 123 \--table help_keyword \--fields-terminated-by "\t" \--lines-terminated-by "\n" \--hive-import \--hive-overwrite \--create-hive-table \--delete-target-dir \--hive-database mydb_test \--hive-table new_help_keyword另外一种写法：sqoop import \--connect jdbc:mysql://cs2:3306/mysql \--username root \--password 123 \--table help_keyword \--fields-terminated-by "\t" \--lines-terminated-by "\n" \--hive-import \--hive-overwrite \--create-hive-table \--hive-table mydb_test.new_help_keyword1 \--delete-target-dir 5.4 增量导入到 HDFS—-应该也是可以导入到 Hive 12345678910111213141516171819202122232425262728第三部分：增量导入Incremental import arguments: --check-column &lt;column&gt; Source column to check for incremental change 原始的列作为增长改变的 --incremental &lt;import-type&gt; Define an incremental import of type 'append' or 'lastmodified' --last-value &lt;value&gt; Last imported value in the incremental check column比较使用于自增长主键!! // 增量导入sqoop import \--connect jdbc:mysql://cs2:3306/mysql \--username root \--password 123 \--table help_keyword \--target-dir /user/ap/my_help_keyword_import1 \--incremental append \--check-column help_keyword_id \--last-value 500 \-m 3 5.5 第四部分： 导入数据到HBase1234567891011121314第四部分： 导入数据到HBase导入MySQL数据库中的表数据到HBase中：sqoop import \--connect jdbc:mysql://cs2:3306/mysql \--username root \--password 123 \--table help_keyword \--hbase-table new_help_keyword \--column-family person \--hbase-row-key help_keyword_id 5.6 导出12345678910111213141516171819202122232425262728293031323334353637第五部分：导出：注意：导出的RDBMS的表必须自己预先创建，不会自动创建------mysql-----先在 mysql 中创建库create database sqoopdb default character set utf8 COLLATE utf8_general_ci; use sqoopdb;CREATE TABLE sqoopstudent ( id INT NOT NULL PRIMARY KEY, name VARCHAR(20), sex VARCHAR(20), age INT,department VARCHAR(20));// 导出HDFS数据到MySQL：sqoop export \--connect jdbc:mysql://cs2:3306/sqoopdb \--username root \--password 123 \--table sqoopstudent \--export-dir /sqoopdata \--fields-terminated-by ','// 导出hive数据到MySQL：sqoop export \--connect jdbc:mysql://cs2:3306/sqoopdb \--username root \--password 123 \--table uv_info \--export-dir /user/hive/warehouse/uv/dt=2011-08-03 \--input-fields-terminated-by '\t' 5.6 导出HBase数据到MySQL很遗憾，现在还没有直接的命令将 HBase 的数据导出到 MySQL 一般采用如下 3 种方法: 1、将 HBase 数据，扁平化成 HDFS 文件，然后再由 sqoop 导入 2、将 HBase 数据导入 Hive 表中，然后再导入 MySQL 3、直接使用 HBase 的 Java API 读取表数据，直接向 MySQL 导入，不需要使用 sqoop 5.7 sqoop 支持 &amp; 不支持hdfs to mysql 可以直接使用。！！hive to mysql 就是hdfs to mysqlhbase to mysql 很不幸，不支持。！ 怎么实现？ 1、扁平化到HDFS 2、hbase和hive做整合 3、自己编写程序去实现从HBASE当中读取数据，然后写入到mYsql 6.Sqoop Job 作业查看帮助: sqoop help job 6.0 mysql 创建数据库 spider 和 表 lagou12# 这样可以直接导入 sql 数据source /home/ap/temp/lagou.sql 6.1 创建作业 Job (–create)123456789# 注意: mysql 中此时已经存在 spider 库, 库中有 lagou 表# ps: 导入表sqoop job --create my_sqoop_job \-- import \--connect jdbc:mysql://cs2:3306/spider \--username root \--password 123 \--table lagou 6.2 查看作业 Job (–list)1234sqoop job --list----------------------Available jobs: my_sqoop_job 6.3 查看作业详细信息 (–show)1sqoop job --show my_sqoop_job 6.4 执行作业1sqoop job --exec my_sqoop_job 6.5 删除作业1sqoop job --delete my_sqoop_job 7.Sqoop 导入导出原理7.1 导入原理 1、第一步，Sqoop 会通过 JDBC 来获取所需要的数据库元数据，例如，导入表的列名，数据 类型等。 2、第二步，这些数据库的数据类型(varchar, number 等)会被映射成 Java 的数据类型(String, int 等)，根据这些信息，Sqoop 会生成一个与表名同名的类用来完成序列化工作，保存表中的 每一行记录。 3、第三步，Sqoop 启动 MapReducer 作业 4、第四步，启动的作业在 input 的过程中，会通过 JDBC 读取数据表中的内容，这时，会使 用 Sqoop 生成的类进行反序列化操作 5、第五步，最后将这些记录写到 HDFS 中，在写入到 HDFS 的过程中，同样会使用 Sqoop 生 成的类进行反序列化 7.2 导出原理 1、 第一步，sqoop 依然会通过 JDBC 访问关系型数据库，得到需要导出数据的元数据信息 2、 第二步，根据获取到的元数据的信息，sqoop 生成一个 Java 类，用来进行数据的传输载 体。该类必须实现序列化和反序列化 3、 第三步，启动 mapreduce 作业 4、 第四步，sqoop 利用生成的这个 java 类，并行的从 hdfs 中读取数据 5、 第五步，每个 map 作业都会根据读取到的导出表的元数据信息和读取到的数据，生成一 批 insert 语句，然后多个 map 作业会并行的向数据库 mysql 中插入数据 所以，数据是从 hdfs 中并行的进行读取，也是并行的进入写入，那并行的读取是依赖 hdfs 的性能，而并行的写入到 mysql 中，那就要依赖于 mysql 的写入性能嘞。 官网: 参考PDF:]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Sqoop</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>Sqoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[i-HBase-1]]></title>
    <url>%2F2018%2F06%2F26%2FHadoop%2F5-HBase%2Fi-HBase-1%2F</url>
    <content type="text"><![CDATA[Hbase概念 hbase &amp; hdfs 关系图 hadoop数据库，分布式可伸缩大型数据存储。 用户对随机、实时读写数据。 十亿行 x 百万列。 版本化、非关系型数据库。 Feature Linear and modular scalability. //线性模块化扩展方式。 Strictly consistent reads and writes. //严格一致性读写 Automatic and configurable sharding of tables//自动可配置表切割 Automatic failover support between RegionServers.//区域服务器之间自动容在 Convenient base classes for backing Hadoop MapReduce jobs with Apache HBase tables. Easy to use Java API for client access.//java API Block cache and Bloom Filters for real-time queries//块缓存和布隆过滤器用于实时查询 Query predicate push down via server side Filters//通过服务器端过滤器实现查询预测 Thrift gateway and a REST-ful Web service that supports XML, Protobuf, and binary data encoding options Extensible jruby-based (JIRB) shell Support for exporting metrics via the Hadoop metrics subsystem to files or Ganglia; or via JMX //可视化 面向列数据库。 HBase要点 1) 它介于 NoSQL 和 RDBMS 之间，仅能通过主键(rowkey)和主键的 range 来检索数据 2) HBase 查询数据功能很简单，不支持 join 等复杂操作 3) 不支持复杂的事务，只支持行级事务(可通过 hive 支持来实现多表 join 等复杂操作)。 4) HBase 中支持的数据类型:byte 5) 主要用来存储结构化和半结构化的松散数据。 HBase 表特点 大:一个表可以有上十亿行，上百万列 面向列:面向列(族)的存储和权限控制，列(簇)独立检索。 稀疏:对于为空(null)的列，并不占用存储空间，因此，表可以设计的非常稀疏。 无模式:每行都有一个可排序的主键和任意多的列，列可以根据需要动态的增加，同一 张表中不同的行可以有截然不同的列 HBase 存储机制 面向列存储，table是按row排序。 底层是 跳表 &amp; 布隆过滤器 HBase 定位机制(三级坐标) 行rowkey 列族 &amp; 列column family &amp; column 时间戳timestamp版本 表 行 列 列族的关系 官网的关系图如下 其它版本 Table is a collection of rows.表是行的集合 Row is a collection of column families行是列族的集合 Column family is a collection of columns.列族是列的集合 Column is a collection of key value pairs.列是键值对的集合(列其实就是 key) RowKey, Column Family, TimeStamp, Cell 概念解释 RowKey 是用来检索记录的主键 rowkey 行键可以是任意字符串(最大长度是 64KB，实际应用中长度一般为 10-100bytes)，最好是 16。 每一个物理文件中都会存一个 rk:TODO ?? 在 HBase 内部，rowkey 保存为字节数组。HBase 会对表中的数据按照 rowkey 排序 (字典顺序) 设计 key 时，要充分排序存储这 个特性，将经常一起读取的行存储放到一起。(位置相关性) 注意：字典序对 int 排序的结果是1,10,100,11,12,13,14,15,16,17,18,19,2,20,21,…9,91,92,93,94,95,96,97,98,99。要保持整形的自然序，行键必须用 0 作左填充。 Column Family 列族是表的 Schema 的一部分(而列不是)，必须在使用表之前定义好，而且定义好了之后就不能更改。 HBase 表中的每个列，都归属与某个列簇。 列簇越多，在取一行数据时所要参与 IO、搜寻的文件就越多，所以，如果没有必要，不要 设置太多的列簇（最好就一个列簇） TimeStamp HBase 中通过 rowkey 和 columns 确定的为一个存储单元称为 cell。 每个 cell 都保存着同一份 数据的多个版本。 版本通过时间戳来索引。 每个 cell 中，不同版本的数据按照时间倒序排序，即最新的数据排在最前面。 hbase 提供了两种数据版 本回收方式: 保存数据的最后 n 个版本 保存最近一段时间内的版本(设置数据的生命周期 TTL)。 Cell 单元格 由{rowkey, column( = + ), version} 唯一确定的单元。 Cell 中的数据是没有类型的，全部是字节码形式存贮。 HBase 的HA搭建 选择安装的主机cs1~cs4, 安装jdk, 解压hbase 包, 配置环境变量 验证安装是否成功$&gt;hbase version 配置 hbase 完全分布式[hbase/conf/hbase-env.sh]export JAVA_HOME=/usr/local/jdk1.8.0_73export HBASE_MANAGES_ZK=false [hbse-site.xml] hbase.cluster.distributed true hbase.rootdir hdfs://mycluster/hbase hbase.zookeeper.quorum cs1:2181,cs2:2181,cs3:2181 hbase.zookeeper.property.dataDir /home/ap/zookeeper 配置 regonservers[hbase/conf/regionservers]cs2cs3cs4cs5 修改 backup-masters（自行创建），指定备用的主HMaster vi backup-masterscs6 最重要一步: 傻逼版: 要把 hadoop 的 hdfs-site.xml 和 core-site.xml 放到 hbase/conf 下, 这样hbase 才能通过命名空间mycluster 找到当前可用 namenode, 再通过 namenode 分配指定 hbase 在 HDFS上的存储路径cp /apps/hadoop/etc/hadoop/core-site.xml /apps/hbase/conf/cp /apps/hadoop/etc/hadoop/hdfs-site.xml /apps/hbase/conf/ 高级版: 在hbase-env.sh文件添加hadoop配置文件目录到HBASE_CLASSPATH环境变量并分发.[/soft/hbase/conf/hbase-env.sh]export HBASE_CLASSPATH=$HBASE_CLASSPATH:/soft/hadoop/etc/hadoop 在hbase/conf/目录下创建到hadoop的hdfs-site.xml符号连接。$&gt;ln -s /soft/hadoop/etc/hadoop/hdfs-site.xml /soft/hbase/conf/hdfs-site.xml 启动 hbase 集群$&gt;start-hbase.sh 登录 hbase 的 webUI http://cs1:16010/ 注意, 页面上显示的版本, 是~/apps/hbase/lib中自己存储的 jar 包的版本 启动/停止 hbase 进程 start-hbase.sh 等价于 hbase-daemon.sh start master hbase-daemons.sh start regionserver 注意: 在哪个节点启动 start-hbase.sh , 就在哪里启动 HMaster 进程, 如果 stop-hbase.sh停止 hbase HBase shell 操作登录shell终端. $hbase&gt; hbase shell help 相关 $hbase&gt; help查看所有帮助 $hbase&gt; table_help关于表操作的另外一种方式的帮助文档 $hbase&gt; help “dml”获取一组命令的提示 $hbase&gt;help ‘list_namespace’查看特定的命令帮助 desc 查看描述信息 desc ‘t1’ : 下面这些在建表的时候都可以指定{NAME =&gt; ‘f1’, BLOOMFILTER =&gt; ‘ROW’, VERSIONS =&gt; ‘1’, IN_MEMORY =&gt; ‘false’, KEEP_DELETED_CELLS =&gt; ‘FALSE’, DATA_BLOCK_ENCODING =&gt; ‘NONE’, TTL =&gt; ‘FOREVER’, COMPRESSION =&gt; ‘NONE’, MIN_VERSIONS =&gt; ‘0’, BLOCKCACHE =&gt; ‘true’, BLOCKSIZE =&gt; ‘65536’, REPLICATION_SCOPE =&gt; ‘0’}1 row(s) in 0.1780 seconds list 相关 $hbase&gt; list列出所有非系统表 $hbase&gt;list_namespace列出名字空间(数据库) $hbase&gt;list_namespace_tables ‘defalut’列出名字空间的所以表 create 相关 $hbase&gt;create_namespace ‘ns1’创建名字空间 $hbase&gt;create ‘ns1:t1’,’f1’创建表,指定空间下,列族注意: 如果 t1已经创建了, 就不能通过这个方式来添加 列族 create ‘t1’,’f1’ create ‘user_info’,{NAME=&gt;’base_info’,VERSIONS=&gt;3 },{NAME=&gt;’extra_info’,VERSIONS=&gt;1 }同时创建多个 列族 exist 相关 exists ‘t1’查看表是否存在 put 相关 注意: 如果往 同表 &amp; 同rowkey &amp; 同样的列 中插入 value, 会默认展示最近的一个时间戳的信息, 如果 get 的回收指定 VERSIONS数量, 则会展示此数量的版本信息 向 user 表中插入信息，row key 为 rk0001，列簇 info 中添加 name 列标示符，值为 zhangsan put ‘user’, ‘rk0001’, ‘info:name’, ‘zhangsan’, 1482077777777手动指定时间戳时间戳是可以自己指定的，如若不指定，则会自动获取系统的当前时间的时间戳 向 user 表中插入信息，row key 为 rk0001，列簇 info 中添加 gender 列标示符，值为 female put ‘user’, ‘rk0001’, ‘info:gender’, ‘female’ 向 user 表中插入信息，row key 为 rk0001，列簇 info 中添加 age 列标示符，值为 20 put ‘user’, ‘rk0001’, ‘info:age’, 20 向 user 表中插入信息，row key 为 rk0001，列簇 data 中添加 pic 列标示符，值为 picture put ‘user’, ‘rk0001’, ‘data:pic’, ‘picture’ get 相关 获取 user 表中 row key 为 rk0001 的所有信息 注意: ‘表名’, ‘ rowkey’ get ‘user’, ‘rk0001’ 获取 user 表中 row key 为 rk0001，info 列簇的所有信息 注意: ‘表名’, ‘ rowkey’, ‘列族名’ get ‘user’, ‘rk0001’, ‘info’ 获取 user 表中 row key 为 rk0001，info 列簇的 name、age 列标示符的信息 注意: ‘表名’, ‘rowkey’, ‘列族名’:’列名’, ‘列族名’:’列名’ get ‘user’, ‘rk0001’, ‘info:name’, ‘info:age’ 获取 user 表中 row key 为 rk0001，info、data 列簇的信息 注意: ‘表名’, ‘rowkey’, ‘列族名’, ‘列族名’ get ‘user’, ‘rk0001’, ‘info’, ‘data’ get ‘user’, ‘rk0001’, {COLUMN =&gt; [‘info’, ‘data’]} get ‘user’, ‘rk0001’, {COLUMN =&gt; [‘info:name’, ‘data:pic’]} 获取 user 表中 row key 为 rk0001，列簇为 info，版本号最新 5 个的信息 注意: ‘表名’, ‘rowkey’, {COLUMN =&gt;’列族名’, 存储版本数=&gt; 数量} get ‘user’, ‘rk0001’, {COLUMN =&gt; ‘info’, VERSIONS =&gt; 2} get ‘user’, ‘rk0001’, {COLUMN =&gt; ‘info:name’, VERSIONS =&gt; 5} get ‘user’, ‘rk0001’, {COLUMN =&gt; ‘info:name’, VERSIONS =&gt; 5, TIMERANGE =&gt; [1392368783980, 1392380169184]} 获取 user 表中 row key 为 rk0001，列标示符中含有 a 的信息 get ‘people’, ‘rk0001’, {FILTER =&gt; “(QualifierFilter(=,’substring:a’))”} 获取 国籍 为中国的用户信息 注意: ‘表名’, ‘rowkey’, ‘列族名’:’列名’, ‘value’ put ‘user’, ‘rk0002’, ‘info:name’, ‘fanbingbing’ put ‘user’, ‘rk0002’, ‘info:gender’, ‘female’ put ‘user’, ‘rk0002’, ‘info:nationality’, ‘中国’ get ‘user’, ‘rk0002’, {FILTER =&gt; “ValueFilter(=, ‘binary:中国’)”} 获取2个不同列中的指定字段 注意: ‘表名’, ‘rowkey’, {COLUMN =&gt; [‘列族名’:’列名’, ‘列族名’:’列名’]} get ‘user’, ‘rk0002’, {COLUMN =&gt; [‘info:name’, ‘data:pic’]} scan 相关 扫描元数据 scan ‘hbase : meta’ t9,,1529936935368.f1a900954c069f0319195f16043c8e1a column=info:regioninfo, timestamp=1529936935933, value={ENCODED =&gt; f1a900954c069f0319195f16043c8e1a, NAME =&gt; ‘t9,,1529936935368.f1a900954c069f031919 . 5f16043c8e1a.’, STARTKEY =&gt; ‘’, ENDKEY =&gt; ‘’} t9,,1529936935368.f1a900954c069f0319195f16043c8e1a column=info:seqnumDuringOpen, timestamp=1529936935933, value=\x00\x00\x00\x00\x00\x00\x00\x02 . t9,,1529936935368.f1a900954c069f0319195f16043c8e1a column=info:server, timestamp=1529936935933, value=cs5:16020 . t9,,1529936935368.f1a900954c069f0319195f16043c8e1a column=info:serverstartcode, timestamp=1529936935933, value=1529923622913 $hbase&gt;split ‘ns1:t1’//切割表 $hbase&gt;split ‘’ //切割区域, 见下面 切割之前 切割之后 UI界面 table regions 继续进行 region 切割 scan ‘hbase:meta’ 找到切割点 STARTKEY =&gt; ‘row000550’ 的 那行的 NAME =&gt; 后面的值 从这个值开始切割到指定值split ‘t9,row000551,1529938745618.90ed2fb6b7744ea5f0a7ff613d05aada.’,’row000888’ 扫描表中所有的信息 $hbase&gt;scan ‘ns1:t1’ 查询 user 表中列簇为 info 的信息 hbase&gt; scan ‘hbase:meta’扫描权标 $hbase&gt; scan ‘hbase:meta’, {COLUMNS =&gt; ‘info’}注意: COLUMNS要大写 $hbase&gt; scan ‘ns1:t1’, {COLUMNS =&gt; [‘c1’, ‘c2’], LIMIT =&gt; 10, STARTROW =&gt; ‘xyz’} scan ‘user’, {COLUMNS =&gt; ‘info’, RAW =&gt; true, VERSIONS =&gt; 5} scan ‘persion’, {COLUMNS =&gt; ‘info’, RAW =&gt; true, VERSIONS =&gt; 3}Scan 时可以设置是否开启 Raw 模式，开启 Raw 模式会返回包括已添加删除标记但是未 实际删除的数据。 查询 user 表中列簇为 info 和 data 的信息 scan ‘user’, {COLUMNS =&gt; [‘info’, ‘data’]} scan ‘user’, {COLUMNS =&gt; [‘info:name’, ‘data:pic’]} 查询 user 表中列簇为 info、列标示符为 name 的信息 scan ‘user’, {COLUMNS =&gt; ‘info:name’} 查询 user 表中列簇为 info、列标示符为 name 的信息,并且版本最新的 5 个 scan ‘user’, {COLUMNS =&gt; ‘info:name’, VERSIONS =&gt; 5} 查询 user 表中列簇为 info 和 data 且列标示符中含有 a 字符的信息 scan ‘user’, {COLUMNS =&gt; [‘info’, ‘data’], FILTER =&gt; “(QualifierFilter(=,’substring:a’))”} 查询 user 表中列簇为 info，rk 范围是[rk0001, rk0003)的数据 scan ‘people’, {COLUMNS =&gt; ‘info’, STARTROW =&gt; ‘rk0001’, ENDROW =&gt; ‘rk0003’} 查询 user 表中 row key 以 rk 字符开头的 scan ‘user’,{FILTER=&gt;”PrefixFilter(‘rk’)”} 查询 user 表中指定范围的数据 scan ‘user’, {TIMERANGE =&gt; [1392368783980, 1392380169184]} delete/truncate 相关 (删除,清空数据) 注意: 删除列族, 在 alter 中 删除 user 表 row key 为 rk0001，列标示符为 info:name 的字段数据 delete ‘people’, ‘rk0001’, ‘info:name’ 删除 user 表 row key 为 rk0001，列标示符为 info:name，timestamp 为 1392383705316 的数据 delete ‘user’, ‘rk0001’, ‘info:name’, 1392383705316 清空 user 表中的数据 truncate ‘people’目前版本会先 disable , 再 truncate, 最后再 enable alter 相关 注意,在修改列族 时 hbase&gt; alter ‘t1’, NAME =&gt; ‘f1’, VERSIONS =&gt; 5 VERSIONS 是指能存多少个版本, 如果不指定的话, 退出 hbase 的时候, 默认只会留最新的一个版本 如果存在, 就是修改 不存在, 就是增加 添加两个列簇 f1 和 f2 简写: alter ‘t1’, ‘f2’ alter ‘people’, NAME =&gt; ‘f1’ alter ‘user’, NAME =&gt; ‘f2’ 停用/启用 表 enable ‘t9’启用 is_enabled ‘t9’是否可用 disable ‘t9’停用 is_disabled ‘t9’是否不可用 删除一个列簇: 注意: 当表中只有一个列族时, 无法将其删除 disable ‘user’(新版本不用) alter ‘user’, NAME =&gt; ‘f1’, METHOD =&gt; ‘delete’ 或 alter ‘user’, ‘delete’ =&gt; ‘f1’ enable ‘user’ 添加列簇 f1 同时删除列簇 f2 disable ‘user’(新版本不用) alter ‘user’, {NAME =&gt; ‘f1’}, {NAME =&gt; ‘f2’, METHOD =&gt; ‘delete’} enable ‘user’ 将 user 表的 f1 列簇版本号改为 5 disable ‘user’(新版本不用) alter ‘people’, NAME =&gt; ‘info’, VERSIONS =&gt; 5people 为 rowkey enable ‘user’ drop 相关 disable ‘user’ drop ‘user’ count 相关 $hbase&gt;count ‘ns1:t1’统计函数, 1000行统计一次 flush 相关 把文件刷到磁盘的过程 hbase&gt; flush ‘TABLENAME’ hbase&gt; flush ‘REGIONNAME’ hbase&gt; flush ‘ENCODED_REGIONNAME’ ValueFilter 过滤器 get ‘person’, ‘rk0001’, {FILTER =&gt; “ValueFilter(=, ‘binary:中国’)”} get ‘person’, ‘rk0001’, {FILTER =&gt; “(QualifierFilter(=,’substring:a’))”} exit 退出$hbase&gt; exit 遇到问题总结 插入前, 必须创建好 namespace相当于数据库的概念 table表命 column family列族名 不需要提前创建 行名 列名 value 如果表已经创建, 要增加列族的话, 只能用 alter, 不能继续用 create 通过Java API访问Hbase 创建hbase模块 添加依赖&lt;?xml version=”1.0” encoding=”UTF-8”?&gt; 4.0.0 com.rox HbaseDemo 1.0-SNAPSHOT org.apache.hbase hbase-client 1.2.3 复制hbase集群的hbase-site.xml文件到模块的src/main/resources目录下。 -—————————————————————————————— Hbase API 类和数据模型之间的对应关系: \0. 总的对应关系 HBaseAdmin 编程实现 代码见 https://github.com/airpoet/bigdata/blob/master/HBaseDemo/src/test/java/com/rox/text/TestCRUD.javapublic class TestCRUD {@Testpublic void put() throws IOException {// 创建 conf 对象Configuration conf = HBaseConfiguration.create();/ 通过连接工厂创建连接对 /// 通过连接工厂创建连接对象Connection conn = ConnectionFactory.createConnection(conf);// 通过连接查询 TableName 对象TableName tname = TableName.valueOf(“ns1:t1”);// 获得 table对象Table table = conn.getTable(tname);// 通过bytes 工具类转化字符串为字节数组byte[] bytes = Bytes.toBytes(“row3”);// 创建 put 对象, 传入行号Put put = new Put(bytes);// 创建 列族, 列, value 的 byte 数据byte[] f1 = Bytes.toBytes(“f1”);byte[] id = Bytes.toBytes(“id”);byte[] value = Bytes.toBytes(188);put.addColumn(f1, id, value);// table put 数据table.put(put);//============================// getbyte[] rowid = Bytes.toBytes(“row3”);Get get = new Get(rowid);// 得到 resResult res = table.get(get); // 从 res 中取出 valuebyte[] idvalue = res.getValue(Bytes.toBytes(“f1”),Bytes.toBytes(“id”));System.out.println(Bytes.toInt(idvalue));}} HBase 写入过程 &amp; 存储路径 WALwrite ahead log,写前日志。 HDFS 上存储路径详解 http://cs1:50070/explorer.html#/hbase/data/ns1/t1/97eada11d196f1e134c41e859d338e07/f1/fsfsfgwgfsgfrgfdg data存储数据的目录 ns1namespace 名称 t1表名 97eada11d196f1e134c41e859d338e07region 的编号 f1列族 fsfsfgwgfsgfrgfdgHFile: 列族存储的文件, 每一个 hfile 文件对应的是一个列族 meta 数据路径 http://cs1:50070/explorer.html#/hbase/data/hbase/meta/1588230740/info HBase 和 Hive 的比较 相同点 HBase 和 Hive 都是架构在 Hadoop 之上，用 HDFS 做底层的数据存储，用 MapReduce 做 数据计算 不同点 解决的问题不同 Hive 是建立在 Hadoop 之上为了降低 MapReduce 编程复杂度的 ETL 工具。 HBase 是为了弥补 Hadoop 对实时操作的缺陷 表的架构不同 Hive 表是纯逻辑表，因为 Hive 的本身并不能做数据存储和计算，而是完全依赖 Hadoop HBase 是物理表，提供了一张超大的内存 Hash 表来存储索引，方便查询 定位 &amp; 访问机制不同 Hive 是数据仓库工具，需要全表扫描，就用 Hive，因为 Hive 是文件存储 HBase 是数据库，需要索引访问，则用 HBase，因为 HBase 是面向列的 NoSQL 数据库 存储模式不同 Hive 表中存入数据(文件)时不做校验，属于读模式存储系统 HBase 表插入数据时，会和 RDBMS 一样做 Schema 校验，所以属于写模式存储系统 是否实时处理(处理效率)不同 Hive 不支持单行记录操作，数据处理依靠 MapReduce，操作延时高 HBase 支持单行记录的 CRUD，并且是实时处理，效率比 Hive 高得多 HBase在 HDFS 上的存储路径 相同列族的数据存放在一个文件中。 [表数据的存储目录结构构成] hdfs://cs1:8020/hbase/data/${名字空间}/${表名}/${区域名称}/${列族名称}/${文件名} [WAL目录结构构成] hdfs://cs1:8020/hbase/WALs/${区域服务器名称,端口号,时间戳}/ Client 端 与 HBase 交互过程 HBase 简单集群结构 region:是 hbase 中对表进行切割的单元，由 regionserver 负责管理 region 分裂是逻辑概念?? :TODO hamster:hbase 的主节点，负责整个集群的状态感知，负载分配、负责用户表的元数据(schema)管理(可以配置多个用来实现 HA),hmaster 负载压力相对于 hdfs 的 namenode 会小很多 regionserver:hbase 中真正负责管理 region 的服务器，也就是负责为客户端进行表数据读写 的服务器每一台 regionserver 会管理很多的 region，同一个 regionserver 上面管理的所有的 region 不属于同一张表 zookeeper:整个 hbase 中的主从节点协调，主节点之间的选举，集群节点之间的上下线感 知„„都是通过 zookeeper 来实现 HDFS:用来存储 hbase 的系统文件，或者表的 region Hbase 顶层结构图 0.hbase集群启动时，master负责分配区域到指定区域服务器。 1.联系zk，找出meta表所在rs(regionserver)/hbase/meta-region-server 2.定位row key,找到对应region server 3.缓存信息在本地。 4.联系RegionServer 5.HRegionServer负责open HRegion对象，为每个列族创建Store对象，Store包含多个StoreFile实例，他们是对HFile的轻量级封装。每个Store还对应了一个MemStore，用于内存存储数据。 百万数据批量插入 代码long start = System.currentTimeMillis() ;Configuration conf = HBaseConfiguration.create();Connection conn = ConnectionFactory.createConnection(conf);TableName tname = TableName.valueOf(“ns1:t1”);HTable table = (HTable)conn.getTable(tname);//不要自动清理缓冲区table.setAutoFlush(false);for(int i = 4 ; i &lt; 1000000 ; i ++){Put put = new Put(Bytes.toBytes(&quot;row&quot; + i)) ; //关闭写前日志 put.setWriteToWAL(false); put.addColumn(Bytes.toBytes(&quot;f1&quot;),Bytes.toBytes(&quot;id&quot;),Bytes.toBytes(i)); put.addColumn(Bytes.toBytes(&quot;f1&quot;),Bytes.toBytes(&quot;name&quot;),Bytes.toBytes(&quot;tom&quot; + i)); put.addColumn(Bytes.toBytes(&quot;f1&quot;),Bytes.toBytes(&quot;age&quot;),Bytes.toBytes(i % 100)); table.put(put); if(i % 2000 == 0){ table.flushCommits(); } }//table.flushCommits();System.out.println(System.currentTimeMillis() - start ); HBase 切割文件 默认10G 进行切割 hbase.hregion.max.filesize 10737418240 hbase-default.xml HBase 合并文件 merge_region ‘1f4609ba4e2a9440aedde5d0e7123722’,’48277c27196a5c6e30d5ae679e9ec4f0’merge ‘前一个区域的 encoded’ , ‘后一个区域的 encoded’ HBase 手动移动文件 hbase(main):034:0&gt; move ‘8658ac9ea01f80c2cca32693397a1e70’,’cs3,16020,1529980599903’前面是 encoded region name , 后面是服务器名 预先切割 创建表时, 预先对表进行切割 切割线是 rowkeycreate ‘ns1:t2’,’f1’,SPLITS=&gt;[‘row300’, ‘row600’] 创建预先切割表之后, 存的值就会到对应的区域中去 注意: put 一次就是一条数据 指定版本数的问题 创建表时指定列族的版本数,该列族的所有列都具有相同数量版本 创建表时，指定列族的版本数。$hbase&gt;create ‘ns1:t3’,{NAME=&gt;’f1’,VERSIONS=&gt;3} 如果创建表的时候创的是3个版本, 查4个版本, 此时最多也就显示3个$hbase&gt;get ‘ns1:t3’,’row1’,{COLUMN=&gt;’f1’,VERSIONS=&gt;4}COLUMN CELL f1:name timestamp=1530019546709, value=tom3 f1:name timestamp=1530019542978, value=tom2 f1:name timestamp=1530019540164, value=tom1 原生扫描(expert) 原生扫描, 包含标记了delete的数据$hbase&gt;scan ‘ns1:t3’,{COLUMN=&gt;’f1’,RAW=&gt;true,VERSIONS=&gt;10}hbase(main):177:0&gt; scan ‘ns1:t3’,{COLUMN=&gt;’f1’,RAW=&gt;true,VERSIONS=&gt;10}ROW COLUMN+CELL row1 column=f1:name, timestamp=1530019546709, value=tom3 row1 column=f1:name, timestamp=1530019542978, value=tom2 row1 column=f1:name, timestamp=1530019540164, type=DeleteColumn row1 column=f1:name, timestamp=1530019540164, value=tom1 删除数据, 标记为删除, 小于该删除时间的数据都作废, flush 后, 用 RAW 似乎还会保留, 用非 RAW 的方式查看的画, 此删除数据, 及时间戳比这个小的数据, 都不会显示. $hbase&gt;delete ‘ns1:t3’,’row1’,’f1:name’,148989875645 这里是否要停掉 hbase 服务再起来看看? TTL time to live ,存活时间 影响所有的数据，包括没有删除的数据 超过该时间，原生扫描也扫不到数据 创建带 TTL参数的表$hbase&gt;create ‘ns1:tx’ , {NAME=&gt;’f1’,TTL=&gt;10,VERSIONS=&gt;10} KEEP_DELETED_CELLS 删除key之后，数据是否还保留, 未测试$hbase&gt;create ‘ns1:tx’ , {NAME=&gt;’f1’,TTL=&gt;10,VERSIONS,KEEP_DELETED_CELLS=&gt;true} TTL 的优先级高于 KEEP_DELETED_CELLS, 如果设置了 TTL, 时间一到就没了, 用 RAW 也找不回来 缓存 和 批处理 开启服务器端扫描器缓存 表层面(全局), 貌似是默认的? 再看看书上说的 hbase.client.scanner.caching 2147483647 hbase-default.xml 操作层面 注意: 貌似JavaAPI 中默认是没有设置 caching 的 设置方式scan.setCaching(10); 操作时间cache row nums : 1000 //632cache row nums : 5000 //423cache row nums : 1 //7359 扫描器缓存是 面向行级别的 批量扫描是 面向列级别的 控制每次next()服务器端返回的列的个数。 scan.setBatch(5);每次next返回5列。 过滤器 图示 代码 https://github.com/airpoet/bigdata/blob/master/HBaseDemo/src/test/java/com/rox/text/TestCRUD.java 计数器 shell 实现 $hbase&gt;incr ‘ns1:t8’,’row1’,’f1:click’,1后面可以跟任意数字, 正负,0都可以 $hbase&gt;get_counter ‘ns1:t8’,’row1’,’f1:click’获取计数器 Java 代码 https://github.com/airpoet/bigdata/blob/master/HBaseDemo/src/test/java/com/rox/text/TestCRUD.java 协处理器 coprocessor :TODO 有空再看 批处理的，等价于存储过程或者触发器 Observer RegionObserver //RegionServer区域观察者 MasterObserver //Master节点。 WAlObserver // Endpoint 终端,类似于存储过程。 HBase &amp; Mapreduce 整合 运行官方案例 [ap@cs1]~/apps/hbase% export HBASE_HOME=/home/ap/apps/hbase [ap@cs1]~/apps/hbase% export HADOOP_HOME=/home/ap/apps/hadoop [ap@cs1]~/apps/hbase% export HADOOP_CLASSPATH=${HBASE_HOME}/bin/hbase mapredcp 使用 MapReduce导入本地数据到Hbase apps/hadoop/bin/yarn jar apps/hbase/lib/hbase-server-1.2.6.1.jar importtsv \ -Dimporttsv.columns=HBASE_ROW_KEY,info:name,info:color fruit \ hdfs://cs1:8020/input_fruit这是 namenode 的 RPC 地址 HBase &amp; Hive 整合1.环境配置 12345export HBASE_HOME=/home/ap/apps/hbaseexport HADOOP_HOME=/home/ap/apps/hadoopexport HADOOP_CLASSPATH=`$&#123;HBASE_HOME&#125;/bin/hbase mapredcp` 剩下的hbase知识点 有空再补充]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[i-Zookeeper]]></title>
    <url>%2F2018%2F06%2F23%2FHadoop%2F4-Zookeeper%2Fi-Zookeeper%2F</url>
    <content type="text"><![CDATA[1.Zookeeper 的安装&amp;概述1.1. Zookeeper 简介Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。 zk提供的服务 Naming service //按名称区分集群中的节点. Configuration management //对加入节点的最新化处理。 Cluster management //实时感知集群中节点的增减. Leader election //leader选举 Locking and synchronization service //修改时锁定数据，实现容灾. Highly reliable data registry //节点宕机数据也是可用的。 1.2. ZK 的工作机制 1.4. ZK 架构 1.4.1 名词解释 1.Client ​ 从server获取信息，周期性发送数据给server，表示自己还活着。 ​ client连接时，server回传ack信息。 ​ 如果client没有收到reponse，自动重定向到另一个server. ​ 2.Server ​ zk集群中的一员，向client提供所有service，回传ack信息给client，表示自己还活着。 ​ 3.ensemble ​ 一组服务器。 ​ 最小节点数是3. ​ 4.Leader ​ 如果连接的节点失败，自定恢复，zk服务启动时，完成leader选举。 ​ 5.Follower ​ 追寻leader指令的节点。 1.4.2 整体解释 1）Zookeeper：一个领导者（leader），多个跟随者（follower）组成的集群。 2）Leader负责进行投票的发起和决议，更新系统状态 3）Follower用于接收客户请求并向客户端返回结果，在选举Leader过程中参与投票 4）集群中只要有半数以上节点存活，Zookeeper集群就能正常服务。 5）全局数据一致：每个server保存一份相同的数据副本，client无论连接到哪个server，数据都是一致的。 6）更新请求顺序进行，来自同一个client的更新请求按其发送顺序依次执行。 7）数据更新原子性，一次数据更新要么成功，要么失败。 8）实时性，在一定时间范围内，client能读到最新数据。 1.5. znode ZooKeeper数据模型的结构与Unix文件系统很类似，整体上可以看作是一棵树，每个节点称做一个ZNode。每一个ZNode默认能够存储1MB的数据，每个ZNode都可以通过其路径唯一标识 zk中的节点，维护了stat，由Version number, Action control list (ACL), Timestamp,Data length.构成. data version //数据写入的过程变化 ACL //action control list, 1.6. 节点类型​ 1.持久节点 ​ client结束，还存在。 ​ 2.临时节点 ​ 在client活动时有效，断开自动删除。临时节点不能有子节点。 ​ leader推选时使用。 ​ 3.序列节点 ​ 在节点名之后附加10个数字，主要用于同步和锁. 1.7. Session​ Session中的请求以FIFO执行，一旦client连接到server，session就建立了。sessionid分配client. ​ client以固定间隔向server发送心跳，表示session是valid的，zk集群如果在超时时候，没有收到心跳， ​ 判定为client挂了，与此同时，临时节点被删除。 1.8. Watches​ 观察。 ​ client能够通过watch机制在数据发生变化时收到通知。 ​ client可以在read 节点时设置观察者。watch机制会发送通知给注册的客户端。 ​ 观察模式只触发一次。 ​ session过期，watch机制删除了。 1.9. zk工作流程​ zk集群启动后，client连接到其中的一个节点，这个节点可以是leader，也可以是follower。 ​ 连通后，node分配一个id给client，发送ack信息给client。 ​ 如果客户端没有收到ack，连接到另一个节点。 ​ client周期性发送心跳信息给节点保证连接不会丢失。 ​ 如果client读取数据，发送请求给node，node读取自己数据库，返回节点数据给client. ​ 如果client想要在 zk 中存储数据，将路径和数据发送给server，server转发给leader。 ​ leader再补发请求给所有follower。只有大多数(超过半数)节点成功响应，则 ​ 写操作成功。 1.10 zk 应用场景1.10.1 统一命名服务 1.10.2 统一配管理 1.10.3 统一集群管理 1.10.4 服务器动态上下线 1.10.5 软负载均衡 2.Zookeeper 完全分布式的安装&amp;基本使用2.1 安装2.1.1 高可用安装PS: 高可用安装见这里 2.2.2 完全分布式安装123456789101112131415161718192021222324252627282930313233341.挑选3台主机cs1 ~ cs32.每台机器都安装zk &amp; 配置环境变量3.配置zk配置文件cs1 ~ cs3[/home/ap/apps/zk/conf/zoo.cfg]----------tickTime=2000initLimit=10syncLimit=5dataDir=/home/ap/zookeeperclientPort=2181server.1=cs1:2888:3888server.2=cs2:2888:3888server.3=cs3:2888:3888---------4.在每台主机的/home/ap/zookeeper中添加myid,内容分别是1,2,3[cs1]$&gt;echo 1 &gt; /home/ap/zookeeper/myid[cs2]$&gt;echo 2 &gt; /home/ap/zookeeper/myid[cs3]$&gt;echo 3 &gt; /home/ap/zookeeper/myid5.在cs1~cs3上,启动服务器集群 $&gt;zkServer.sh start6.查看每台服务器的状态$&gt;zkServer.sh status* 注意: 如果有3台机器, 只启动一台的话, 会显示如下, 因为根据配置, 有3台机器, 此时只有一台启动, 没有过半数, 整个集群是挂掉的* 只有启动的机器数量超过配置的半数, zk 集群才有效. 2.2.3 zoo.cfg 文件中配置参数的含义1234567891011121314151617181920212223242526271）tickTime=2000：通信心跳数 tickTime：通信心跳数，Zookeeper服务器心跳时间，单位毫秒 Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。 它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime)2）initLimit=10：LF初始通信时限 initLimit：LF初始通信时限 集群中的follower跟随者服务器(F)与leader领导者服务器(L)之间初始连接时能容忍的最多心跳数（tickTime的数量），用它来限定集群中的Zookeeper服务器连接到Leader的时限。 投票选举新leader的初始化时间 Follower在启动过程中，会从Leader同步所有最新数据，然后确定自己能够对外服务的起始状态。 Leader允许F在initLimit时间内完成这个工作。3）syncLimit=5：LF同步通信时限 syncLimit：LF同步通信时限 集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime， Leader认为Follwer死掉，从服务器列表中删除Follwer。 在运行过程中，Leader负责与ZK集群中所有机器进行通信，例如通过一些心跳检测机制，来检测机器的存活状态。 如果L发出心跳包在syncLimit之后，还没有从F那收到响应，那么就认为这个F已经不在线了。4）dataDir：数据文件目录+数据持久化路径 dataDir：数据文件目录+数据持久化路径 保存内存数据库快照信息的位置，如果没有其他说明，更新的事务日志也保存到数据库。5）clientPort=2181：客户端连接端口 监听客户端连接的端口6) Server.A=B:C:D。 A是一个数字，表示这个是第几号服务器； B是这个服务器的ip地址； C是这个服务器与集群中的Leader服务器交换信息的端口； D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。 # 集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。 2.2 ZK客户端的连接12345678910111213141516171819202122232425262728293031323334353637383940414243$&gt;zkCli.sh -server cs1:2181 //进入zk命令行$zk]help //查看帮助$zk]quit //退出$zk]create /a tom //创建节点$zk]get /a //查看数据$zk]ls / //列出节点$zk]ls2 / //查看当前节点数据并能看到更新次数等数据$zk]set /a tom //设置数据$zk]delete /a //删除一个节点$zk]rmr /a //递归删除所有节点。$zk]stat / //查看节点状态注意: * 创建节点不能递归创建, 目前只能一层一层创建* 每次创建都的写数据, 只创建目录的话会创建不成功* 如果 close 后, 并没有退出客户端, 只是访问不到数据, 此时如果想要连接, 可以用connect host:port================================================================================ZK 的帮助文档----------------help:ZooKeeper -server host:port cmd args stat path [watch] set path data [version] ls path [watch] delquota [-n|-b] path ls2 path [watch] setAcl path acl setquota -n|-b val path history redo cmdno printwatches on|off delete path [version] sync path listquota path rmr path get path [watch] create [-s] [-e] path data acl addauth scheme auth quit getAcl path close connect host:port 2.3 监听测试注意点: 注册的监听只能使用一次, 监听完毕后需要重新注册. 123456789101112131415161718192021222324=======节点值的变化的监听==========（1）在cs1主机上注册监听/app1节点数据变化[zk: localhost:2181(CONNECTED) 26] get /app1 watch （2）在cs2主机上修改/app1节点的数据[zk: localhost:2181(CONNECTED) 5] set /app1 777（3）观察cs2主机收到数据变化的监听WATCHER::WatchedEvent state:SyncConnected type:NodeDataChanged path:/app1=======节点的子节点变化的监听==========（1）在cs1主机上注册监听/app1节点的子节点变化[zk: localhost:2181(CONNECTED) 1] ls /app1 watch[aa0000000001, server101]（2）在cs2主机/app1节点上创建子节点[zk: localhost:2181(CONNECTED) 6] create /app1/bb 666Created /app1/bb（3）观察cs1主机收到子节点变化的监听WATCHER::WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/app1 3. ZK 内部机制3.1 选举机制3.1.1. zookeeper的选举机制（全新集群paxos）以一个简单的例子来说明整个选举的过程. 假设有五台服务器组成的zookeeper集群,它们的id从1-5,同时它们都是最新启动的,也就是没有历史数据,在存放数据量这一点上,都是一样的.假设这些服务器依序启动,来看看会发生什么. 1) 服务器1启动,此时只有它一台服务器启动了,它发出去的报没有任何响应,所以它的选举状态一直是LOOKING状态 2) 服务器2启动,它与最开始启动的服务器1进行通信,互相交换自己的选举结果,由于两者都没有历史数据,所以id值较大的服务器2胜出,但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3),所以服务器1,2还是继续保持LOOKING状态. 3) 服务器3启动,根据前面的理论分析,服务器3成为服务器1,2,3中的老大,而与上面不同的是,此时有三台服务器选举了它,所以它成为了这次选举的leader. 4) 服务器4启动,根据前面的分析,理论上服务器4应该是服务器1,2,3,4中最大的,但是由于前面已经有半数以上的服务器选举了服务器3,所以它只能接收当小弟的命了. 5) 服务器5启动,同4一样,当小弟. 3.1.2. 非全新集群的选举机制(数据恢复)那么，初始化的时候，是按照上述的说明进行选举的，但是当zookeeper运行了一段时间之后，有机器down掉，重新选举时，选举过程就相对复杂了。 需要加入数据id、leader id和逻辑时钟。 数据id：数据新的id就大，数据每次更新都会更新id。 Leader id：就是我们配置的myid中的值，每个机器一个。 逻辑时钟：这个值从0开始递增,每次选举对应一个值,也就是说: 如果在同一次选举中,那么这个值应该是一致的 ; 逻辑时钟值越大,说明这一次选举leader的进程更新. 选举的标准就变成： ​ 1、逻辑时钟小的选举结果被忽略，重新投票 ​ 2、统一逻辑时钟后，数据id大的胜出 ​ 3、数据id相同的情况下，leader id大的胜出 根据这个规则选出leader。 3.2 stat 的结构 1）czxid- 引起这个znode创建的zxid，创建节点的事务的zxid 每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于zxid2，那么zxid1在zxid2之前发生。 2）ctime - znode被创建的毫秒数(从1970年开始) 3）mzxid - znode最后更新的zxid 4）mtime - znode最后修改的毫秒数(从1970年开始) 5）pZxid-znode最后更新的子节点zxid 6）cversion - znode子节点变化号，znode子节点修改次数 7）dataversion - znode数据变化号 8）aclVersion - znode访问控制列表的变化号 9）ephemeralOwner- 如果是临时节点，这个是znode拥有者的session id。如果不是临时节点则是0。 10）dataLength- znode的数据长度 11）numChildren - znode子节点数量 4. 通过 JavaAPI 访问 ZK4.1 添加Maven 依赖123456789101112131415161718192021222324259.1[pom.xml]&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.it18zhang&lt;/groupId&gt; &lt;artifactId&gt;ZooKeeperDemo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.9&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 4.2 Java 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134* 对应关系:* Linux Java----------------------* ls getChildren* get getData* set setDAta* create create˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘package com.rox.zktest;import org.apache.zookeeper.*;import org.apache.zookeeper.data.ACL;import org.apache.zookeeper.data.Stat;import org.junit.Test;import java.io.IOException;import java.util.List;/** * 对应关系: * Linux Java * ls getChildren * get getData * set setDAta * create create */public class TestZK &#123; @Test public void ls() throws IOException, KeeperException, InterruptedException &#123; /** * 放三个就不行, 只能放2个目前来看 */ ZooKeeper zk = new ZooKeeper("cs1:2181,cs2:2181,cs3:2181", 5000, null); List&lt;String&gt; list = zk.getChildren("/", null); for (String s : list) &#123; System.out.println(s); &#125; &#125; @Test public void lsAll() &#123; try &#123; ls("/"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 列出指定 path 下的children * * @param path */ public void ls(String path) throws Exception &#123; System.out.println(path); ZooKeeper zk = new ZooKeeper("cs1:2181,cs2:2181,cs3:2181", 5000, null); List&lt;String&gt; list = zk.getChildren(path, null); if (list == null || list.isEmpty()) &#123; return; &#125; for (String s : list) &#123; // 先输出 children if (path.equals("/" )) &#123; ls(path + s); &#125; else &#123; ls(path + "/" + s); &#125; &#125; &#125; /** * 设置数据 */ @Test public void setData() throws Exception &#123; ZooKeeper zk = new ZooKeeper("cs1:2181", 5000, null); zk.setData("/a","tomaslee".getBytes(),0); &#125; /** * 创建临时节点 */ @Test public void createEPHEMERAL() throws Exception &#123; ZooKeeper zk = new ZooKeeper("cs1:2181", 5000, null); zk.create("/c/c1", "tom".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); System.out.println("hello"); &#125; /** * 创建观察者 */ @Test public void testWatch() throws Exception &#123; final ZooKeeper zk = new ZooKeeper("cs1:2181,cs2:2181,cs3:2181", 5000, null); Stat st = new Stat(); Watcher w = null; w = new Watcher() &#123; public void process(WatchedEvent event) &#123; try &#123; System.out.println("数据改了..."); zk.getData("/a",this,null); &#125;catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;; byte[] data = zk.getData("/a",w,st); System.out.println(new String(data)); while (true) &#123; Thread.sleep(1000); &#125; &#125;&#125;]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop-HA高可用集群QJM搭建]]></title>
    <url>%2F2018%2F06%2F22%2FHadoop%2F0-Hadoop%2FHadoop-HA%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[1.此教程默认已经搭建好完全分布式2. Zookeeper 集群搭建1234567891011121314151617181920212223242526272829配置完全分布式zk集群--------------------- 1.挑选3台主机 cs1 ~ cs3 2.每台机器都安装zk tar 环境变量 3.配置zk配置文件 cs1 ~ cs3 [/home/ap/apps/zk/conf/zoo.cfg] ... dataDir=/home/ap/zookeeper 4.在每台主机的/home/centos/zookeeper中添加myid,内容分别是1,2,3 [cs1] $&gt;echo 1 &gt; /home/ap/zookeeper/myid [cs2] $&gt;echo 2 &gt; /home/ap/zookeeper/myid [cs3] $&gt;echo 3 &gt; /home/ap/zookeeper/myid 5.启动服务器集群 $&gt;zkServer.sh start 6.查看每台服务器的状态 $&gt;zkServer.sh status* 注意: 如果有3台机器, 只启动一台的话, 会显示如下, 因为根据配置, 有3台机器, 此时只有一台启动, 没有过半数, 整个集群是挂掉的* 只有启动的机器数量超过配置的半数, zk 集群才有效. 3.HA 集群搭建首先声明笔者用的6台主机, 主机名 cs1-cs6, 用户名为ap, 可以对照改为自己的主机名&amp;用户名 另外, 搭建 HA 不会影响原来的完全分布式, 具体操作会在下面告知.hadoop 安装目录层级结构: /home/ap/apps/hadoop/etc/hadoop/hdfs-site.xml data 目录层级结构: cs1: /home/ap/hadoopdata/namenode/current/edits_00.... cs2: /home/ap/hadoopdata/datanode/current/BP-15... 可以对照参考 集群结构如下 开始搭建首先保证 各节点直接的 ssh 免密登录没问题 如果非生产环境, 可以同时把 .ssh删掉后, 全部重新生成 ssh-keygen, 同时相互发送, 这样操作最简单, 效率最高. 其次上代码了 把原本/home/ap/apps/hadoop/etc/hadoop中的 hadoop目录改为full,意思是完全分布式. cp -r full ha, 复制一份 full 为 ha, 在这份配置文件中配置 HA ln -s /home/ap/apps/hadoop/etc/ha /home/ap/apps/hadoop/etc/hadoop, 用一个软链接hadoop 指向 ha 配置 /home/ap/apps/hadoop/etc/ha/hdfs-site.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576[hdfs-site.xml]------------------------------------------------------------------&lt;configuration&gt; &lt;!-- 指定 2个 namenode 的命名空间 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;mycluster&lt;/value&gt; &lt;/property&gt; &lt;!-- myucluster下的名称节点两个id --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置每个nn的rpc地址。 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;cs1:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;cs6:8020&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置webui端口 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;cs1:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;cs6:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 名称节点共享编辑目录. --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://cs2:8485;cs3:8485;cs4:8485/mycluster&lt;/value&gt; &lt;/property&gt; &lt;!-- java类，client使用它判断哪个节点是激活态。 --&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!-- 脚本列表或者java类，在容灾保护激活态的nn. --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt; sshfence shell(/bin/true) &lt;/value&gt; &lt;/property&gt; &lt;!-- ssh免密登陆 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/ap/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置 sshfence 隔离机制超时时间(可不配) --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置JN存放edit的本地路径。 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/ap/hadoopdata/journal&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 配置 core-site.xml, 这里给出完整配置 (目前不包括 Hive 配置) 1234567891011121314151617181920212223[core-site.xml]------------------------------------------------------------&lt;configuration&gt; &lt;!-- 指定 hdfs 的 nameservice 为 mycluster --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://mycluster&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定 hadoop 工作目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/ap/hadoopdata&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zk集群访问地址 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;cs1:2181,cs2:2181,cs3:2181&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置mapred-site.xml 1234567891011121314151617181920212223[mapred-site.xml]-------------------&lt;configuration&gt; &lt;!-- 指定 mr 框架为 yarn 方式 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置 mapreduce 的历史服务器地址和端口号 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;cs1:10020&lt;/value&gt; &lt;/property&gt; &lt;!-- mapreduce 历史服务器的 web 访问地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;cs1:19888&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 把/home/ap/apps/hadoop/etc/*发给其他所有节点 2-6 注意: 软链接scp 的时候会有问题, 最终保证每个节点跟 cs1一样就可以了,可以每个节点单独修改, 也可以写脚本一起修改 ln -sfT /home/ap/apps/hadoop/etc/ha /home/ap/apps/hadoop/etc/hadoop 部署细节 12345678910111213141516171819202122232425262728293031323334350.在 zk 节点启动 zkServercs1-cs3: $&gt;zkServer.sh start1.在jn节点分别启动jn进程$&gt;hadoop-daemon.sh start journalnode2.启动jn之后，在两个NN之间进行disk元数据同步 a)如果是全新集群，先format文件系统,只需要在一个nn上执行。 [cs1] $&gt;hadoop namenode -format b)如果将非HA集群转换成HA集群，复制原NN的metadata到另一个nn. 1.步骤一 [cs1] $&gt;scp -r /home/centos/hadoop/dfs ap@cs6:/home/centos/hadoop/ 2.步骤二 在新的nn(未格式化的nn)上运行一下命令，实现待命状态引导。 [cs6] $&gt;hdfs namenode -bootstrapStandby //需要cs1为启动状态,提示是否格式化,选择N.3.在一个NN上执行以下命令，完成edit日志到jn节点的传输。$&gt;hdfs namenode -initializeSharedEdits#查看cs2,cs3,cs4 这几个 jn 节点是否有edit数据.4.启动所有节点.[cs1]$&gt;hadoop-daemon.sh start namenode //启动名称节点$&gt;hadoop-daemons.sh start datanode //启动所有数据节点[2,3,4]$&gt;hadoop-daemon.sh start journalnode[cs6]$&gt;hadoop-daemon.sh start namenode //启动名称节点 HA 管理 12345678910# 查看web 界面, 是否是2个 standby 状态http://cs1:50070/http://cs6:50070/hdfs haadmin : 查看 ha 帮助-----------------$&gt;hdfs haadmin -transitionToActive nn1 //切成激活态$&gt;hdfs haadmin -transitionToStandby nn1 //切成待命态$&gt;hdfs haadmin -transitionToActive --forceactive nn2//强行激活$&gt;hdfs haadmin -failover nn1 nn2 //模拟容灾演示,从nn1切换到nn2 加入 Zookeeper 容灾服务 zkfc 12345678910111213141516171819202122232425262728293031323334353637&lt;!-- 1.部署容灾 --&gt;------------------------------a.停止所有进程$&gt;stop-all.shb.配置hdfs-site.xml，启用自动容灾.[hdfs-site.xml]&lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;c.配置core-site.xml，指定zk的连接地址.&lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;cs1:2181,cs2:2181,cs3:2181&lt;/value&gt;&lt;/property&gt;d.分发以上两个文件到所有节点。&lt;!-- 2.登录其中的一台NN(s201),在ZK中初始化HA状态, 创建 namenode 的 命名空间节点 mycluster --&gt;------------------------------------$&gt;hdfs zkfc -formatZK&lt;!-- 3.启动hdfs进程. --&gt; $&gt;start-dfs.sh&lt;!-- 4.查看 webUI, 是否有一台自动切换为 active 状态了 --&gt;http://cs1:50070/http://cs6:50070/&lt;!-- 5.测试自动容灾(如果cs6是活跃节点) --&gt; $&gt;kill -9 cs6的 namenode进程号观察 cs1:50070的状态变化 配置RM的HA自动容灾 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021.配置yarn-site.xml&lt;configuration&gt; &lt;!-- 是否允许高可用 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- Identifies the cluster. Used by the elector to ensure an RM doesn’t take over as Active for another cluster. --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;cluster1&lt;/value&gt; &lt;/property&gt; &lt;!-- RM 的 id --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;!-- 2台 RM 的宿主 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;cs1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;cs6&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置web界面 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;cs1:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;cs6:8088&lt;/value&gt; &lt;/property&gt; &lt;!-- zookeeper 地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;cs1:2181,cs2:2181,cs3:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- YARN 集群为 MapReduce 程序提供的 shuffle 服务(原本的) --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- ===========以下是可选的============= --&gt; &lt;!-- 开启 YARN 集群的日志聚合功能 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- YARN 集群的聚合日志最长保留时长 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;86400&lt;/value&gt; &lt;/property&gt; &lt;!-- 启用自动恢复 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定 resourcemanager 的状态信息存储在 zookeeper 集群上--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;------------------------------------------------------------------2.使用管理命令&lt;!-- 查看状态 --&gt;$&gt;yarn rmadmin -getServiceState rm1&lt;!-- 切换状态到standby --&gt;$&gt;yarn rmadmin -transitionToStandby rm13.启动yarn集群$&gt;start-yarn.sh4.hadoop没有启动两个resourcemanager,需要手动启动另外一个$&gt;yarn-daemon.sh start resourcemanager5.查看webUI, 点击 About, 查看 active 或 standbyhttp://cs1:8088http://cs6:80886.做容灾模拟.kill -9 活跃的 RM 端口号7.注意: 如果容灾失败, 检查下每台主机时间是否同步$&gt;sudo ntpdate ntp1.aliyun.com 至此, 大功告成 4.HA 集群的启动/关闭4.1 HA 的启动一:单点启动 启动 zk 服务 QuorumPeerMain, 否则后面 RM 会起不来,连不上 ZK 服务cs1-cs3: $&gt; zkServer.sh start 启动 namenode/datanode cs1,cs6: $&gt; hadoop-daemon.sh start namenode cs1/cs6: $&gt; hadoop-daemons.sh start datanode cs6: $&gt; hadoop-daemon.sh start namenode 启动 journalnode cs2-cs4: $&gt; hadoop-daemon.sh start journalnode 启动RM (RM 会自动选出一个 active) cs1,cs6: $&gt; yarn-daemon.sh start resourcemanager cs1/cs6: $&gt; yarn-daemons.sh start nodemanager 启动 zk 的 DFSZKFailoverController cs1,cs6: $&gt; hadoop-daemon.sh start zkfc 此外可以启动MapReduce 的历史任务服务器 [ap@cs1]$&gt; mr-jobhistory-daemon.sh start historyserver 然后访问配置的 http://cs1:19888/jobhistory 二:懒汉启动 启动 zk 服务 QuorumPeerMain, 否则后面 RM 会起不来,连不上 ZK 服务cs1-cs3: $&gt; zkServer.sh start 执行启动全部 cs1: $&gt; start-all.sh(RM 节点) 或者, 使用新的启动方式 cs1: $&gt; start-dfs.sh(任意节点) cs1: $&gt; start-yarn.sh(在 RM 节点) 另一个 RM 节点不会自己启动,要手动启动 cs6: $&gt; yarn-daemon.sh start resourcemanager 三: 启动完成后 1234567891011121314151617181920212223242526272829303132============= cs1 jps =============5696 QuorumPeerMain6641 DFSZKFailoverController6338 NameNode6756 ResourceManager6873 Jps============= cs2 jps =============10849 Jps10722 NodeManager10631 JournalNode10541 DataNode127934 QuorumPeerMain============= cs3 jps =============630 NodeManager758 Jps535 JournalNode443 DataNode117503 QuorumPeerMain============= cs4 jps =============79589 Jps79462 NodeManager79368 JournalNode79278 DataNode============= cs5 jps =============23655 Jps23529 NodeManager23423 DataNode============= cs6 jps =============35680 Jps35506 NameNode35608 DFSZKFailoverController21455 ResourceManager 4.2 HA 的关闭在 一台NN 上stop-all.sh, 注意 zk 的 server- QuorumPeerMain不会停掉 5. 在集群实现时间同步(root 用户操作)思路:时间同步的方式：找一个机器，作为时间服务器，所有的机器与这台集群时间进行定时的同步，比如，每隔十分钟，同步一次时间。 5.1 在cs1上修改ntp服务12345678910111213141516171819202122232425262728293031323334353637383940414243444546471) 检查 ntp 是否安装#root&gt; rpm -qa|grep ntp---ntp-4.2.6p5-10.el6.centos.x86_64fontpackages-filesystem-1.41-1.1.el6.noarchntpdate-4.2.6p5-10.el6.centos.x86_64# 出现上面的3个文件, 就是安装了; 如果没有的话, 用 yum 装一下# root&gt; yum inatall -y ntp2) 修改 ntp 配置文件#root&gt; vi /etc/ntp.conf修改内容如下 a）修改1（设置本地网络上的主机不受限制。） #restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap 为 restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap b）修改2（设置为不采用公共的服务器） server 0.centos.pool.ntp.org iburst server 1.centos.pool.ntp.org iburst server 2.centos.pool.ntp.org iburst server 3.centos.pool.ntp.org iburst 为 #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst c）添加3（添加默认的一个内部时钟数据，使用它为局域网用户提供服务。） server 127.127.1.0 fudge 127.127.1.0 stratum 103）修改/etc/sysconfig/ntpd 文件#root&gt; vim /etc/sysconfig/ntpd增加内容如下（让硬件时间与系统时间一起同步）SYNC_HWCLOCK=yes4）重新启动ntpd#root&gt; service ntpd statusntpd 已停#root&gt; service ntpd start正在启动 ntpd： 5) 执行开机启动 ntpd 服务#root&gt; chkconfig ntpd on查看 ntpd 服务开机启动的状态#root&gt; chkconfig --list ntpd 5.2 其它机器上1234567891）在其他所有机器配置10分钟与时间服务器同步一次#编写定时任务脚本#root&gt; crontab -e */10 * * * * /usr/sbin/ntpdate hadoop1022) 修改任意机器时间#root&gt; date -s "2017-9-11 11:11:11"3) 十分钟后查看机器时间是否与cs1 同步]]></content>
      <categories>
        <category>Hadoop</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[i-Hive-Practice-1 影评练习]]></title>
    <url>%2F2018%2F06%2F19%2FHadoop%2F3-Hive%2Fi-Hive-Practice-1-%E5%BD%B1%E8%AF%84%E7%BB%83%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[现有如此三份数据：1、users.dat 数据格式为： 2::M::56::16::70072对应字段为：UserID BigInt, Gender String, Age Int, Occupation String, Zipcode String对应字段中文解释：用户id，性别，年龄，职业，邮政编码 2、movies.dat 数据格式为： 2::Jumanji (1995)::Adventure|Children’s|Fantasy对应字段为：MovieID BigInt, Title String, Genres String对应字段中文解释：电影ID，电影名字，电影类型 3、ratings.dat 数据格式为： 1::1193::5::978300760对应字段为：UserID BigInt, MovieID BigInt, Rating Double, Timestamped String对应字段中文解释：用户ID，电影ID，评分，评分时间戳 题目要求： 数据要求：（1）写shell脚本清洗数据。（hive不支持解析多字节的分隔符，也就是说hive只能解析’:’, 不支持解析’::’，所以用普通方式建表来使用是行不通的，要求对数据做一次简单清洗）（2）使用Hive能解析的方式进行 Hive要求：（1）正确建表，导入数据（三张表，三份数据），并验证是否正确 （2）求被评分次数最多的10部电影，并给出评分次数（电影名，评分次数） 123456789101112131415161718192021222324252627282930313233343536思路:1. 分组select movieid, count(movieid) rateCount from ratings group by movieid limit 10;2. 排序select movieid, count(movieid) rateCount from ratings group by movieid order by rateCount desc limit 10;3.joinselect a.movieid mvid, b.Title mvtitle, a.rateCount mvratecount from (select movieid, count(movieid) rateCount from ratings group by movieid order by rateCount desc limit 10) ajoin movies b where a.movieid=b.movieid;============================== 完整的------------select a.movieid mvid, b.Title mvtitle, a.rateCount mvratecount from (select movieid, count(movieid) rateCount from ratings group by movieid order by rateCount desc limit 10) ajoin movies b where a.movieid=b.movieid;--结果---+-------+----------------------------------------------------+--------------+| mvid | mvtitle | mvratecount |+-------+----------------------------------------------------+--------------+| 2858 | American Beauty (1999) | 3428 || 260 | Star Wars: Episode IV - A New Hope (1977) | 2991 || 1196 | Star Wars: Episode V - The Empire Strikes Back (1980) | 2990 || 1210 | Star Wars: Episode VI - Return of the Jedi (1983) | 2883 || 480 | Jurassic Park (1993) | 2672 || 2028 | Saving Private Ryan (1998) | 2653 || 589 | Terminator 2: Judgment Day (1991) | 2649 || 2571 | Matrix, The (1999) | 2590 || 1270 | Back to the Future (1985) | 2583 || 593 | Silence of the Lambs, The (1991) | 2578 |+-------+----------------------------------------------------+--------------+ （3）分别求男性，女性当中评分最高的10部电影（性别，电影名，影评分） 123456789101112131415161718192021222324252627# 注意,这里的 avg(r.rating) 是因为group by了 gender,title, 也就是# 说有 gender,title 分组, 此时可能对应此组的有多个值,# 这里就是 相同性别评价相同电影的, 评分有多个, 此时就要使用聚合函数,得出唯一值# 因此就使用了 avg(r.rating)select u.gender, m.title, avg(r.rating) rrfrom ratings r join users u on r.userid=u.userid join movies m on r.movieid=m.movieidwhere u.gender = 'M'group by gender,titleorder by rr desclimit 10;+-----------+--------------------------------------------+------+| u.gender | m.title | rr |+-----------+--------------------------------------------+------+| M | Schlafes Bruder (Brother of Sleep) (1995) | 5.0 || M | Small Wonders (1996) | 5.0 || M | Lured (1947) | 5.0 || M | Bells, The (1926) | 5.0 || M | Dangerous Game (1993) | 5.0 || M | Baby, The (1973) | 5.0 || M | Gate of Heavenly Peace, The (1995) | 5.0 || M | Follow the Bitch (1998) | 5.0 || M | Ulysses (Ulisse) (1954) | 5.0 || M | Angela (1995) | 5.0 |+-----------+--------------------------------------------+------+ （4）求movieid = 2116这部电影各年龄段（因为年龄就只有7个，就按这个7个分就好了）的平均影评（年龄段，影评分） 12345# 注意: 没有歧义的字段, 可以不用指明是谁的属性select age, avg(r.rating) avgrating from ratings r join users u on r.userid=u.useridwhere movieid=2116group by age; （5）求最喜欢看电影（影评次数最多）的那位女性评最高分的10部电影的平均影评分（观影者，电影名，影评分） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# 1.先拿到 影评次数最多的女性(id)0: jdbc:hive2://cs2:10000&gt; select UserID, count(UserID) count from ratings. . . . . . . . . . . . .&gt; group by UserID. . . . . . . . . . . . .&gt; order by count desc. . . . . . . . . . . . .&gt; limit 1;+---------+--------+| userid | count |+---------+--------+| 4169 | 2314 |+---------+--------+# 2.拿到此人评分最高的10部电影, 此人 userid, 电影 id, 此人评分## 一定要去重select distinct(MovieID) MovieID, Rating from ratings where UserID=4169 order by Rating desclimit 10;+----------+---------+| movieid | rating |+----------+---------+| 78 | 5.0 || 73 | 5.0 || 72 | 5.0 || 58 | 5.0 || 55 | 5.0 || 50 | 5.0 || 41 | 5.0 || 36 | 5.0 || 25 | 5.0 || 17 | 5.0 |+----------+---------+3.这10部电影的(观影者，电影名，平均影评分)观影者还没写select t.MovieID, m.Title, avg(r.rating) avgrating from topten t join ratings r on t.MovieID=r.MovieIDjoin movies m on t.MovieID=m.MovieIDgroup by t.MovieID,m.Title;+------------+-----------------------------------------------+---------------------+| t.movieid | m.title | avgrating |+------------+-----------------------------------------------+---------------------+| 3849 | Spiral Staircase, The (1946) | 4.046511627906977 || 3870 | Our Town (1940) | 3.857142857142857 || 3871 | Shane (1953) | 3.839344262295082 || 3893 | Nurse Betty (2000) | 3.5026833631484795 || 3897 | Almost Famous (2000) | 4.22635814889336 || 3910 | Dancer in the Dark (2000) | 3.82 || 3927 | Fantastic Voyage (1966) | 3.5804597701149423 || 3928 | Abbott and Costello Meet Frankenstein (1948) | 3.441747572815534 || 3929 | Bank Dick, The (1940) | 3.993197278911565 || 3932 | Invisible Man, The (1933) | 3.75 |+------------+-----------------------------------------------+---------------------+ （6）求好片（评分&gt;=4.0）最多的那个年份的最好看的10部电影（7）求1997年上映的电影中，评分最高的10部Comedy类电影（8）该影评库中各种类型电影中评价最高的5部电影（类型，电影名，平均影评分）（9）各年评分最高的电影类型（年份，类型，影评分）（10）每个地区最高评分的电影名，把结果存入HDFS（地区，电影名，影评分）]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA的简单使用]]></title>
    <url>%2F2018%2F06%2F18%2FTools%2FIDEA%2FIDEA%E7%9A%84%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1.安装 Scala 插件首先, 直接在 plugin 中搜索官方的 Scala 语言包插件是连不上的, 要验证一些 UUID之类的东西, 原因你懂的 于是乎在网上搜索, 有一篇博客说, 看搜索的时候显示的版本, 然后下载此版本安装, 如下图中画框的版本号 然后去官网找到此版本安装,下载下来 从磁盘安装 简直不要太坑 b 哦!!! 一直说版本不匹配. 我就一直没有怀疑版本的问题, 直到看到 怎么感觉这个 2013.3 在哪里见到过?? 查了一下我的 idea 版本, 不就是 2017.3.5嘛 ! 难道他们有关系?? 抱着试一试的态度, 随便下了一个2017.3.15的 居然能装上?? 好吧, 重启, OK 了! 由此可见, 也不要完全相信别人的博客啊!! 如果不行, 多方尝试! 尽信书不如无书!]]></content>
      <categories>
        <category>工具</category>
        <category>IDEA</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>工具</tag>
        <tag>IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive学习—2]]></title>
    <url>%2F2018%2F06%2F14%2FHadoop%2F3-Hive%2FHive%E5%AD%A6%E4%B9%A0-2%2F</url>
    <content type="text"><![CDATA[hive数据类型1) 原子数据类型 TinyInt：1byte有符号整数 SmallInt：2byte有符号整数 Int：4byte有符号整数 BigInt：8byte有符号整数 Float：单精度浮点数 Double：双精度浮点数 Boolean：布尔类型 String：字符串 TimeStamp：整数 2) 复杂数据类型 Array\&lt;Type> 由一系列相同数据类型的元素组成 这些元素可以通过 下标 来访问 查询时如果查到返回响应值，没查到则返回null建表：create table person(name string,work_locations array\&lt;string>)row format delimited fields terminated by ‘\t’collection items terminated by ‘,’;导入数据：load data local inpath ‘/home/sigeon/person.txt’ into table person;查询Select work_locations[0] from person; Map&lt;KType, VType&gt; 包含 key-value 键值对 可以通过 key 来访问元素建表语句：create table score(name string, scores map)row format delimited fields terminated by ‘\t’collection items terminated by ‘,’map keys terminated by ‘:’;导入数据：load data local inpath ‘/home/sigeon/score.txt’ into table score;查询语句：Select scores[‘Chinese’] from score; Struct&lt;Param1:Type1, Param1:Type1, … &gt; 可以包含不同数据类型的元素类似于c语言中的结构体 这些元素可以通过 点语法 的方式来得到建表语句：create table course(id int,course struct&lt;name:string, score:int&gt;)row format delimited fields terminated by ‘\t’collection items terminated by ‘,’;导入数据：load data local inpath ‘/ home/sigoen/course.txt’ into table course;查询语句：Select c.course.score from course c; 几个分隔符 指定分隔符要按从外向内的顺序，字段 -&gt; 集合元素 -&gt;map k-v ROW FORMAT：指定分隔符的关键字 DELIMITED FIELDS TERMINATED BY：字段分隔符 COLLECTION ITEMS TERMINATED BY：集合元素分隔符（Array 中的各元素、Struct 中的各元素、 Map 中的 key-value 对之间） MAP KEYS TERMINATED BY：Map 中 key 与 value 的分隔符 LINES TERMINATED BY：行之间的分隔符 hive视图 和关系型数据库一样，Hive也提供了视图的功能 Hive 的视图和关系型数据库的视图有很大的区别： 1、只有逻辑视图，没有物化视图； 2、视图只能查询，不能增删改 (Load|Insert/Update/Delete) 数据； 3、视图在创建时候，只是保存了一份元数据 (存在TBLS中)，当查询视图的时候，才开始执行视图对应的那些子查询视图元数据只存储了hql语句，而不是执行结果 视图操作 创建视图 1create view my_view as &lt;select * from mytable&gt; [limit 500]; 查看视图 show tables; // 显示所有表和视图 show views; //显示所有视图 desc [formatted] view_name; // 查看某个具体视图的(详细)信息视图类型：VIRTUAL_VIEW 删除视图 drop view [if exists] view_name 使用视图 select count(distinct uid) from my_view; hive函数 函数分类 UDF（自定义函数 User-Defined Function）作用于单个数据行，产生一个数据行作为输出。（数学函数，字 符串函数） UDAF（用户定义聚集函数 User-Defined Aggregation Funcation）：接收多个输入数据行，并产 生一个输出数据行。（count，max） UDTF（表格生成函数 User-Defined Table Function）：接收一行输入，输出多行（explode） 内置函数 查看函数命令 查看内置函数： show functions; 显示函数的详细信息： desc function [extended] fun_name;extended：显示扩展信息 分类 关系运算 分类\1. 等值比较: =\2. 等值比较:&lt;=&gt;\3. 不等值比较: &lt;&gt;和!=\4. 小于比较: &lt;\5. 小于等于比较: &lt;=\6. 大于比较: &gt;\7. 大于等于比较: &gt;=\8. 区间比较\9. 空值判断: IS NULL\10. 非空判断: IS NOT NULL\10. LIKE 比较: LIKE\11. JAVA 的 LIKE 操作: RLIKE\12. REGEXP 操作: REGEXP 数学运算 分类\1. 加法操作: +\2. 减法操作: –\3. 乘法操作: *\4. 除法操作: /\5. 取余操作: %\6. 位与操作: &amp;\7. 位或操作: |\8. 位异或操作: ^9．位取反操作: ~ 逻辑运算 分类\1. 逻辑与操作: AND 、&amp;&amp;\2. 逻辑或操作: OR 、||\3. 逻辑非操作: NOT、! 复合类型构造函数 分类 array 结构 map 结构\3. struct 结构\4. named_struct 结构\5. create_union 复合类型操作符 \1. 获取 array 中的元素 \2. 获取 map 中的元素 \3. 获取 struct 中的元素 集合操作函数 \1. map 类型大小：size \2. array 类型大小：size \3. 判断元素数组是否包含元素：array_contains \4. 获取 map 中所有 value 集合 \5. 获取 map 中所有 key 集合 \6. 数组排序 \7. 获取素组或map集合的单个元素（k-v对）：explode() 当同时查询炸裂字段和普通字段时，需要使用横向虚拟视图：lateral view 如：select name,addr.city from usr_addr lateral view(address) addr as city;这里一般需要给 查询结果 和 结果字段 起别名，不然没办法获得每个字段，如果只是查询所有的话就不需要了 类型转换函数 \1. 二进制转换：binary \2. 基础类型之间强制转换：cast 数值计算函数 \1. 取整函数: round \2. 指定精度取整函数: round \3. 向下取整函数: floor \4. 向上取整函数: ceil \5. 向上取整函数: ceiling \6. 取随机数函数: rand \7. 自然指数函数: exp \8. 以 10 为底对数函数: log10 \9. 以 2 为底对数函数: log2 \10. 对数函数: log \11. 幂运算函数: pow \12. 幂运算函数: power \13. 开平方函数: sqrt \14. 二进制函数: bin \15. 十六进制函数: hex \16. 反转十六进制函数: unhex \17. 进制转换函数: conv \18. 绝对值函数: abs \19. 正取余函数: pmod \20. 正弦函数: sin \21. 反正弦函数: asin \22. 余弦函数: cos \23. 反余弦函数: acos \24. positive 函数: positive \25. negative 函数: negative 字符串函数 \1. 字符 ascii 码函数：ascii \2. base64 字符串 \3. 字符串连接函数：concat \4. 带分隔符字符串连接函数：concat_ws \5. 数组转换成字符串的函数：concat_ws \6. 小数位格式化成字符串函数：format_number \7. 字符串截取函数：substr, substring序号从1开始，可以传负数，代表从右开始 \9. 字符串查找函数：instr找到返回一个正整数，未找到返回0 \10. 字符串长度函数：length \11. 字符串查找函数：locate \12. 字符串格式化函数：printf \13. 字符串转换成 map 函数：str_to_map \14. base64 解码函数：unbase64(string str) \15. 字符串转大写函数：upper,ucase \16. 字符串转小写函数：lower,lcase \17. 去空格函数：trim \18. 左边去空格函数：ltrim \19. 右边去空格函数：rtrim \20. 正则表达式替换函数：regexp_replace \21. 正则表达式解析函数：regexp_extract \22. URL 解析函数：parse_url \23. json 解析函数：get_json_object \24. 空格字符串函数：space \25. 重复字符串函数：repeat \26. 左补足函数：lpad \27. 右补足函数：rpad \28. 分割字符串函数: split \29. 集合查找函数: find_in_set \30. 分词函数：sentences \31. 分词后统计一起出现频次最高的 TOP-K \32. 分词后统计与指定单词一起出现频次最高的 TOP-K 日期函数 \1. UNIX 时间戳转日期函数: from_unixtime \2. 获取当前 UNIX 时间戳函数: unix_timestamp \3. 日期转 UNIX 时间戳函数: unix_timestamp \4. 指定格式日期转 UNIX 时间戳函数: unix_timestamp \5. 日期时间转日期函数: to_date \6. 日期转年函数: year \7. 日期转月函数: month \8. 日期转天函数: day \9. 日期转小时函数: hour \10. 日期转分钟函数: minute \11. 日期转秒函数: second \12. 日期转周函数: weekofyear \13. 日期比较函数: datediff \14. 日期增加函数: date_add \15. 日期减少函数: date_sub 条件函数 \1. If 函数: if( 条件 ，true返回参数，false返回参数 ) \2. 当param1不为null返回param1，否则返回param2：nvl(param1, param2) \2. 非空查找函数: coalesce \3. 条件判断函数：case 混合函数 分类\1. 调用 Java 函数：java_method\2. 调用 Java 函数：reflect\3. 字符串的 hash 值：hash XPath 解析 XML 函数 分类\1. xpath\2. xpath_string\3. xpath_boolean\4. xpath_short, xpath_int, xpath_long\5. xpath_float, xpath_double, xpath_number 汇总统计函数（UDAF） \1. 个数统计函数: count \2. 总和统计函数: sum \3. 平均值统计函数: avg \4. 最小值统计函数: min \5. 最大值统计函数: max \6. 非空集合总体变量函数: var_pop \7. 非空集合样本变量函数: var_samp \8. 总体标准偏离函数: stddev_pop \9. 样本标准偏离函数: stddev_samp 10．中位数函数: percentile \11. 中位数函数: percentile \12. 近似中位数函数: percentile_approx \13. 近似中位数函数: percentile_approx \14. 直方图: histogram_numeric \15. 集合去重数：collect_set \16. 集合不去重函数：collect_list 表格生成函数 Table-Generating Functions (UDTF) 分类1．数组拆分成多行：explode(array)2．Map 拆分成多行：explode(map) 自定义函数 \1. 需要继承 org.apache.hadoop.hive.ql.exec.UDF 类，实现一个或多个 evaluate() 方法 \2. 将hive的jar包放在hive的classpath路径下，进入hive客户端，执行命令：add jar \; \3. 检查jar包是否添加成功，执行命令：list jars; \4. 给自定义函数 添加别名，并在hive中 注册 该函数，执行命令：create temporary function myfunc as ‘主类全路径名’;myfunc：自定义函数别名该方法创建的是临时函数，当前客户端关闭就没有了，下次重复第3，4步；真实 生产中就是使用该方法！ \5. 查看hive函数库有没有成功添加，执行命令：show functions; \6. 调用函数时通过函数名和参数列表确定调用的是具体哪一个方法]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[i-Hive-1]]></title>
    <url>%2F2018%2F06%2F13%2FHadoop%2F3-Hive%2Fi-Hive-1%2F</url>
    <content type="text"><![CDATA[1. Hive 初探1.1 Hive 的数据存储 Hive的数据存储基于Hadoop HDFS Hive没有专门的数据存储格式 存储结构主要包括：数据库、文件、表、视图、索引 Hive默认可以直接加载文本文件（TextFile），还支持SequenceFile、RCFile 创建表时，指定Hive数据的列分隔符与行分隔符，Hive即可解析数据 1.2 Hive的系统架构 用户接口，包括 CLI，JDBC/ODBC，WebUI 元数据存储，通常是存储在关系数据库如 mysql, derby 中 解释器、编译器、优化器、执行器 Hadoop：用 HDFS 进行存储，利用 MapReduce 进行计算 1.3 Hive的系统架构 用户接口主要有三个：CLI，JDBC/ODBC和 WebUI CLI，即Shell命令行 JDBC/ODBC 是 Hive 的Java，与使用传统数据库JDBC的方式类似 WebGUI是通过浏览器访问 Hive Hive 将元数据存储在数据库中(metastore)，目前只支持 mysql、derby。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等 解释器、编译器、优化器完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划（plan）的生成。生成的查询计划存储在 HDFS 中，并在随后由 MapReduce 调用执行 Hive 的数据存储在 HDFS 中，大部分的查询由 MapReduce 完成（包含 的查询，比如 select from table 不会生成 MapRedcue 任务 1.4 Hive的metastore metastore是hive元数据的集中存放地。 metastore默认使用内嵌的derby数据库作为存储引擎 Derby引擎的缺点：一次只能打开一个会话 使用Mysql作为外置存储引擎，多用户同时访问 1.5 Hive 和 Hadoop 的调用关系 1、提交sql 交给驱动2、驱动编译 解析相关的字段表信息3、去metastore查询相关的信息 返回字段表信息4、编译返回信息 发给驱动5、驱动发送一个执行计划 交给执行引擎6.1、DDLs 对数据库表的操作的, 直接和metastore交互,create table t1(name string); 6.2、完成job返回数据信息、找namenode查数据6.3、namenode交互select count(1) from t1;7、返回结果信息集 1.6 Hive 参数配置使用 命名空间 使用权限 描述 hivevar 可读写 $ hive -d name=zhangsan; hiveconf 可读写 \$ hive –hiveconf hive.cli.print.current.db=true; $ hive –hiveconf hive.cli.print.header=true; system 可读写 java定义的配置属性，如system:user.name env 只读 shell环境变量，如env:USER hivevar 1234567# 使用场景: 起别名hive -d name=zhangsan #传参&gt; create table t2(name string,$&#123;name&#125; string); #取参数&gt; desc t2;---name stringzhangsan string hiveconf : 12345678910# 显示当前数据库名称[ap@cs2]~% hive --hiveconf hive.cli.print.current.db=true;hive (default)&gt; create database mydb;hive (default)&gt; use mydb;hive (mydb)&gt; # 显示表头(字段名)hive --hiveconf hive.cli.print.header=true;select * from t2;t2.name t2.zhangsan 1.7 Hive 的脚本执行 Hive -e “xx ” e 就是 edit, 在终端打印输出 Hive -e “show tables” &gt;&gt; a.txt 可以把执行结果重定向到文件中 Hive -S -e “show tables” &gt;&gt; a.txt -S : silence 安静的执行 hive -f file hive -f hql , hql 是文件, 执行文件 执行完了之后, 就离开 hive 命令行 hive -i /home/ap/hive-init.sql 执行完了,还在控制台, 可以继续操作 hive&gt;source file source + 文件名 : 直接执行当前目录文件 source /home/ap/xx.sql; 1.8 hive与依赖环境的交互 与linux交互命令 ！ !ls !pwd 与hdfs交互命令 dfs -ls / dfs -mkdir /hive hive (default)&gt; dfs -rm -r /user/hive/warehouse/t5; beeline 与 linux &amp; hdfs 交互 !help 查看帮助 1.9 Hive 的 JDBC 模式 JAVA API交互执行方式 hive 远程服务 (端口号1000 0) 启动方式 hive --service hiveserver2 org.apache.hive.jdbc.HiveDriver 在java代码中调用hive的JDBC建立连接 用 beeline 连接 方式1: 直接登录 注意: 这里的cs2是指的数据库所在的服务器, 如果mysql 安装在 cs2上, 那么不管在哪台机器上登录beeline , 都这样输入就行了 beeline -u jdbc:hive2://cs2:10000 -n ap 方式2: 输入用户名密码登录 !connect jdbc:hive2://cs2:10000 beeline注意点: 使用 beeline 连接时, 貌似无法与 Linux 目录交互 当前目录在/home/ap/apps/apache-hive-2.3.2-bin/bin/下 要传文件的话, 要使用全路径 1.10 SET命令使用 Hive 控制台set 命令 set; set -v; 显示所有的环境变量 set hive.cli.print.current.db=true; set hive.cli.print.header=true; set hive.metastore.warehouse.dir=/hive; hive参数初始化配置set命令: ~/.hiverc 创建此文件, 在此文件中配置初始化命令 补充：hive历史操作命令集~/.hivehistory 2. Hive数据类型2.1 基本数据类型 2.2 复合数据类型 创建学生表 123456CREATE TABLE student( id INT, name STRING, favors ARRAY\&lt;STRING&gt;, scores MAP&lt;STRING, FLOAT&gt;); 默认分隔符 描述 语句 \n 分隔行 LINES TERMINATED BY ‘\t’ ^A 分隔字段(列)，显示编码使用\001 FIELDS TERMINATED BY ‘\001’ ^B 分隔复合类型中的元素，显示编码使用\002 COLLECTION ITEMS TERMINATED BY ‘\002’ ^C 分隔map元素的key和value，显示编码使用\003 MAP KEYS TERMINATED BY ‘\003’ 2.2.1. Struct 使用Structs内部的数据可以通过DOT（.）来存取，例如，表中一列c的类型为STRUCT{a INT; b INT}，我们可以通过c.a来访问域a 123456789101112131415161718192021222324252627282930# 数据1001,zhangsan:241002,lisi:281003,wangwu:25# 1.创建表hive&gt; create table student_test(id INT, info struct&lt;name:STRING, age:INT&gt;)ROW FORMAT DELIMITED FIELDS TERMINATED BY ','COLLECTION ITEMS TERMINATED BY ':';# 2.加载表hive&gt; load data local inpath "student_test" into table student_test;# 3.顺便设置 显示表头,和当前数据库hive&gt; set hive.cli.print.header=true;hive&gt; set hive.cli.print.current.db=true;# 4. 展示所有的hive (default)&gt; select * from student_test;---student_test.id student_test.info1001 &#123;"name":"zhangsan","age":24&#125;1002 &#123;"name":"lisi","age":28&#125;1003 &#123;"name":"wangwu","age":25&#125;---# Struct -结构体-使用 . hive (default)&gt; select id,info.name,info.age from student_test;id name age1001 zhangsan 241002 lisi 281003 wangwu 25 2.2.2. Array 使用Array中的数据为相同类型，例如，假如array A中元素[&#39;a&#39;,&#39;b&#39;,&#39;c’]，则A[1]的值为’b’ 123456789101112131415161718192021222324# 原始数据zhangsan,24:25:27:37lisi,28:39:23:43wangwu,25:23:02:54# 创建表hive (default)&gt; create table class_test(name string,student_id_list array&lt;int&gt;) row format delimited fields terminated by ',' collection items terminated by ':';# 加载表hive (default)&gt; load data local inpath "class_test" into table class_test;# 查看表hive (default)&gt; select * from class_test;OKclass_test.name class_test.student_id_listzhangsan [24,25,27,37]lisi [28,39,23,43]wangwu [25,23,2,54]# 查看数据中某个元素hive (default)&gt; select name, student_id_list[0] from class_test where name='zhangsan';OKname _c1zhangsan 24 2.2.3. Map 使用访问指定域可以通过[“指定域名称”]进行，例如，一个Map M包含了一个group-&gt;gid的kv对，gid的值可以通过M[‘group’]来获取 1234567891011121314151617181920212223242526272829303132333435# 原始数据1001 job:80,team:60,person:701002 job:60,team:80,person:801003 job:90,team:70,person:100# 创建表hive (default)&gt; create table employee(id string,perf map&lt;string,int&gt;) row format delimited fields terminated by '\t' collection items terminated by ',' map keys terminated by ':';# 导入hive (default)&gt; load data local inpath "employee_data" into table employee;# 查看hive (default)&gt; select * from employee;---employee.id employee.perf1001 &#123;"job":80,"team":60,"person":70&#125;1002 &#123;"job":60,"team":80,"person":80&#125;1003 &#123;"job":90,"team":70,"person":100&#125;Time taken: 0.228 seconds, Fetched: 3 row(s)# 查看单个hive (default)&gt; select id,perf['job'],perf['team'],perf['person'] from employee;OKid _c1 _c2 _c31001 80 60 701002 60 80 801003 90 70 100# 显示别名hive (default)&gt; select id,perf['job'] as job,perf['team'] as team,perf['person'] as person from employee;OKid job team person1001 80 60 701002 60 80 801003 90 70 100 3. DDL , DML3.1 DDL3.1.1 数据库定义 默认数据库”default” 使用某个数据库 use &lt;数据库名&gt; 创建一个新库 12345678CREATE DATABASE [IF NOT EXISTS] mydb [LOCATION] '/.......' [COMMENT] '....’;hive&gt;SHOW DATABASES;hive&gt;DESCRIBE DATABASE [extended] mydb;hive&gt;DROP DATABASE [IF EXISTS] mydb [CASCADE]; 创建 create database db1; 删除 drop database if exists db1; 级联删除 drop database if exists db1 cascade; 3.1.2 表定义/修改 创建表 hive&gt;CREATE TABLE IF NOT EXISTS t1(…) [COMMENT ‘….’] [LOCATION ‘…’] hive (default)&gt; create table t4(name string,age int) row format delimited fields terminated by &quot;\t”; hive&gt; SHOW TABLES in mydb; show tables in mydb ‘’class*“ : 查看以 mydb 库中, 以 class 开头的表 hive&gt;CREATE TABLE t2 LIKE t1; 复制表 只会复制表结构 hive (default)&gt; create table t2 like t1; hive (mydb)&gt; create table t3 like default.employee; 复制其它库的表 hive&gt;DESCRIBE t2; desc t2; # 效果一样的 desc extended t1; # 查看更详细的表信息 hive (default)&gt; desc formatted t1; # 格式化查看表的详细信息 drop table xxx; 删除表 查看建表语句 show create table t_table; 修改表 重命名表 ALTER TABLE table_name RENAME TO new_table_name 增加/删除 分区 alter table student_p add partition(part=&#39;a&#39;) partition(part=&#39;b&#39;); 两个 partition中没有’,’ alter table student drop partition(stat_data=‘ffff’), partition(part=‘a’),partiton(part=‘b’); 两个 partition中有’,’ 增加/更新 列 ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...) 注：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段。 alter table student add columns (name1 string); ALTER TABLE table_name CHANGE c_name new_c_name new_c_type [FIRST | AFTER c_name] 3.1.3 列定义 修改列的名称、类型、位置、注释 ALTER TABLE t3 CHANGE COLUMN old_name new_name String COMMENT &#39;...&#39; AFTER column2; 123456789# 修改列名hive (default)&gt; alter table t1 change column name username string comment 'new name';# 查看表hive (default)&gt; desc t1;---col_name data_type commentusername string new nameage int--- 增加列 hive&gt; ALTER TABLE t3 ADD COLUMNS(gender int); 1234567891011121314# 查看表结构hive (default)&gt; desc t3;---col_name data_type commentname string---# 添加列hive (default)&gt; alter table t3 add columns(gender int);---hive (default)&gt; desc t3;---col_name data_type commentname stringgender int 删除列 replace 非常不建议使用, 会造成数据错乱, 一般采取重新创建一张表的方式. 3.1.4 显示命令 show tables show databases show partitions show functions desc extended t_name; desc formatted table_name; 3.2 DML3.2.1 Load 语法结构 LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] 说明： Load 操作只是单纯的复制/移动操作，将数据文件移动到 Hive 表对应的位置。 filepath： 相对路径，例如：project/data1 绝对路径，例如：/user/hive/project/data1 包含模式的完整 URI，例如： hdfs://namenode:9000/user/hive/project/data1 LOCAL关键字 如果指定了 LOCAL， load 命令会去查找本地文件系统中的 filepath。 如果没有指定 LOCAL 关键字，则根据inpath中的uri查找文件 OVERWRITE 关键字 如果使用了 OVERWRITE 关键字，则目标表（或者分区）中的内容会被删除，然后再将 filepath 指向的文件/目录中的内容添加到表/分区中。 如果目标表（分区）已经有一个文件，并且文件名和 filepath 中的文件名冲突，那么现有的文件会被新文件所替代。 3.2.2 Insert语法结构 普通插入 INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 …)] select_statement1 FROM from_statement Multiple inserts: 12345678910FROM from_statement [INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 ][INSERT OVERWRITE TABLE tablename2 [PARTITION ...] select_statement2] ...# 多重插入举例from studentinsert into table student_p partition(part='a')select * where id&lt;95011insert into table student_p partition(part='b')select * where id&gt;95011; Dynamic partition inserts: 不指定分区字段, 按照 from 表的分区字段插入 INSERT OVERWRITE TABLE tablename PARTITION (partcol1, partcol2 ...) select_statement FROM from_statement 导出表数据语法结构 INSERT OVERWRITE [LOCAL] DIRECTORY directory1 SELECT ... FROM ... 123456789101112131415# 导出到本地insert overwrite local directory '/home/ap/test/stucent1'select * from student1;--------------------例子:'查询学生信息，按性别分区，在分区内按年龄有序'0: jdbc:hive2://cs2:10000&gt; set mapred.reduce.tasks=2;No rows affected (0.015 seconds)0: jdbc:hive2://cs2:10000&gt; insert overwrite local directory '/home/ap/ihiveout'. . . . . . . . . . . . .&gt; select * from student distribute by Sex sort by Sage;--------------------# 导出到 HDFS (仅仅是少了一个 local)insert overwrite directory '/test/stucent1'select * from student1; multiple inserts: 123FROM from_statementINSERT OVERWRITE [LOCAL] DIRECTORY directory1 select_statement1[INSERT OVERWRITE [LOCAL] DIRECTORY directory2 select_statement2] ... 4. Hive的数据模型4.1 管理表 - 又称为内部表, 受控表基本操作 创建数据文件inner_table.dat 创建表 hive&gt;create table inner_table (key string); 加载数据 加载本地数据 hive&gt;load data local inpath &#39;/root/inner_table.dat&#39; into table inner_table; 加载HDFS 上数据 hive&gt;load data inpath ‘xxx’ into table xxx; 区别 加载 hdfs 上的数据没有 local 加载本地数据是 copy 一份, 加载 hdfs 上的数据是直接移动数据到加载的表目录下– mv 查看数据 select * from inner_table select count(*) from inner_table 删除表 drop table inner_table 清空表 truncate table table_name; 注意: 如果创建表的时候, 只指定了目录, 没有指定表名, 删除表的时候, 会把该目录下的所有表全部删掉 hive (mydb)&gt; create table t2(id int)location &#39;/home/t2&#39;; 内部表解释 管理表，也称作内部表,受控表 所有的 Table 数据（不包括 External Table）都保存在warehouse这个目录中。 删除表时，元数据与数据都会被删除 创建过程和数据加载过程（这两个过程可以在同一个语句中完成），在加载数据的过程中，实际数据会被移动到数据仓库目录中；之后对数据对访问将会直接在数据仓库目录中完成。删除表时，表中的数据和元数据将会被同时删除 内部表转为外部表, 外部表转为内部表 123456789101112131415161718192021222324252627hive (mydb)&gt; create table t1(id int);# manage_table 转换为 外部表 external_table ## 注意: 修改为外部表时, 后面2个都要大写hive (mydb)&gt; alter table t1 set tblproperties('EXTERNAL'='TRUE');## 修改为内部表hive (mydb)&gt; alter table t1 set tblproperties('EXTERNAL'='FALSE');# 查看t1详细信息desc formatted t1;---Location: hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1Table Type: EXTERNAL_TABLE-- # 删除t1hive (mydb)&gt; drop table t1;# 此时再查看, 已经没了hive (mydb)&gt; show tables;OKtab_name# 但是查看hdfs 路径会发现还在, 因为此表现在已经是外部表, 删除不会删除数据dfs -ls /user/hive/warehouse/mydb.db/t1# 如果此时再创建一个新表 t1, 表结构一样, 则数据会自动加载 4.2 ※ 外部表4.2.1基本操作 创建数据文件external_table.dat 创建表 hive&gt;create external table external_table1 (key string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39; location &#39;/home/external’; 在HDFS创建目录/home/external #hadoop fs -put /home/external_table.dat /home/external 在工作中, 一般都这样使用, 把数据上传到 hdfs 中 加载数据 LOAD DATA &#39;/home/external_table1.dat&#39; INTO TABLE external_table1; 查看数据 select * from external_table select count(*) from external_table 删除表 drop table external_table 4.2.2 外部表解释 包含External 的表叫外部表 删除外部表只删除metastore的元数据，不删除hdfs中的表数据 外部表 只有一个过程，加载数据和创建表同时完成，并不会移动到数据仓库目录中，只是与外部数据建立一个链接。当删除一个 外部表 时，仅删除该链接 指向已经在 HDFS 中存在的数据，可以创建 Partition 它和 内部表 在元数据的组织上是相同的，而实际数据的存储则有较大的差异 4.2.3 外部表语法123456789101112CREATE EXTERNAL TABLE page_view( viewTime INT, userid BIGINT, page_url STRING, referrer_url STRING, ip STRING COMMENT 'IP Address of the User', country STRING COMMENT 'country of origination‘) COMMENT 'This is the staging page view table' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\n' STORED AS TEXTFILE LOCATION 'hdfs://centos:9000/user/data/staging/page_view'; 4.2.4外部表注意点: 先创建外部表/内部表, 表名为t3, 再往t3传对应字段的数据, 就可以直接 select 数据了 删除外部表之后, 原本数据不会删除, 此时在相同的父路径创建与被删除表字段相同&amp;名称相同的内部/外部表, 数据也会直接加载 再看一个操作 12345678910# 在 mydb.db 库下创建一个外部表 t5, 指定路径为 '/external/t5'# 此时在 mydb.db 库的路径下是不存在 t5表的, 而是存在 '/external/t5' 中# 但是使用 'show tables' 是存在 t5 的hive (mydb)&gt; create external table t5(id int) location '/external/t5';# 往此目录传数据, 注意: 此时传过去, intdata 数据存在 t5目录下[ap@cs2]~% hadoop fs -put intdata /external/t5[ap@cs2]~% hadoop fs -put intdata /external/t5/i2# 继续传数据, 查询的时候, 就是传的全部数据'相当于所有的数据都在 t5表中'[ap@cs2]~% hadoop fs -put intdata /external/t5# 注意: 如果传到 t5 目录下, 没有指定数据文件名的话, 会默认采用数据的名称文件. 4.3 ※ 分区表4.3.1 基本概念和操作 分区可以理解为分类，通过分类把不同类型的数据放到不同的目录下。 分类的标准就是分区字段，可以一个，也可以多个。 分区表的意义在于优化查询。查询时尽量利用分区字段。如果不使用分区字段，就会全部扫描。 创建分区表, 指定分区字段 hive&gt;CREATE TABLE t3(...) PARTITIONED BY (province string); 创建表的时候, 指定分区字段 keyprovince 为分区字段添加一个值 hive&gt;ALTER TABLE t3 ADD [IF NOT EXISTS] PARTITION(...) LOCATION &#39;...’; alter table t3 add if not exists partition(province=&#39;hubei&#39;) partition(province=&#39;shanghai&#39;); alter table t3 add if not exists partition(province=&#39;jiangsu&#39;); 可为此分区字段添加多个值, 为 province 添加 hubei, hunan…. 查看表的分区字段&amp;值 hive&gt;SHOW PARTITIONS t3 [partition (province=&#39;beijing&#39;)]; 删除分区 hive&gt;ALTER TABLE t3 DROP PARTITION(province=‘beijing’.); 这里是删除北京的分区 (如果是内部表, 会连数据一起删除) 设置表不能被删除/查询 ——– 这里报语法错误, :TODO 防止分区被删除:alter table student_p partition (part=&#39;aa&#39;) enable no_drop; 防止分区被查询:alter table student_p partition (part=&#39;aa&#39;) enable offline; enable 和 disable 是反向操作 其它一些相关命令 SHOW TABLES; # 查看所有的表 SHOW TABLES ‘TMP‘; #支持模糊查询 SHOW PARTITIONS TMP_TABLE; #查看表有哪些分区 DESC TMP_TABLE; #查看表结构 4.3.2 创建分区表完整语法1234567891011CREATE TABLE tmp_table #表名(title string, # 字段名称 字段类型minimum_bid double,quantity bigint,have_invoice bigint)COMMENT '注释：XXX' #表注释 PARTITIONED BY(pt STRING) #分区表字段（如果你文件非常之大的话，采用分区表可以快过滤出按分区字段划分的数据） ROW FORMAT DELIMITED FIELDS TERMINATED BY '\001' # 字段是用什么分割开的STORED AS SEQUENCEFILE; #用哪种方式存储数据，SEQUENCEFILE是hadoop自带的文件压缩格式 4.3.3 分区表注意点(错误点) 1) 分区表在 load 数据的时候, 得指定分区, 否则会报错 123456789101112# 错误1: 分区表在 load 数据的时候, 得指定分区hive (mydb)&gt; load data local inpath '~/ihivedata/intdata' into table t6;FAILED: SemanticException [Error 10062]: Need to specify partition columns because the destination table is partitioned# 错误2: 导入本地数据的时候, 'path'是从当前所在路径开始的hive (mydb)&gt; load data local inpath '~/ihivedata/intdata' into table t6 partition(class='job1');FAILED: SemanticException Line 1:23 Invalid path ''&lt;sub&gt;/ihivedata/intdata'': No files matching path file:/home/ap/&lt;/sub&gt;/ihivedata/intdata# 这里就正确了hive (mydb)&gt; load data local inpath 'ihivedata/intdata' into table t6 partition(class='job1');Loading data to table mydb.t6 partition (class=job1)OK 本质原因: 分区表的分区, 就是在 hdfs 上, 原表的文件夹下面创建了一个子文件夹, 文件夹名就是分区名. 从本地 load 数据: hive (mydb)&gt; load data local inpath &#39;ihivedata/intdata&#39; into table t6 partition(class=&#39;job1&#39;); load 数据指定分区之后, 会直接 load 到数据文件夹里面 2) 没有添加分区时, 直接往不存在的分区导入数据, 分区会自动创建 12# 直接往不存在的分区load数据, 分区会自动创建load data local inpath 'ihivedata/intdata' into table t6 partition(class='job110'); 3) 手动在表中创建分区(文件夹), 并直接向此文件夹中导入数据 12345678910111213141516171819202122232425262728293031323334353637# 直接创建目录hive (mydb)&gt; dfs -mkdir /user/hive/warehouse/mydb.db/t6/class=job120;# 直接从 hadoop 端传数据hadoop fs -put ihivedata/intdata /user/hive/warehouse/mydb.db/t6/class=job120# 此时再 show partitions t6; 会发现并没有此分区---partitionclass=job1class=job110class=job2class=job3class=job4# 此时就需要手动'激活'此分区, 加入了就有了hive (mydb)&gt; alter table t6 add partition(class='job120');---hive (mydb)&gt; show partitions t6;---partitionclass=job1class=job110class=job120class=job2class=job3class=job4# 查看分区信息hive (mydb)&gt; select * from t6 where class='job110';OKt6.id t6.class1 job1102 job1103 job1104 job1105 job110 4.3.4 复合分区基本操作 创建数据文件partition_table.dat 创建表 create table t7(name string,age int)partitioned by(class string,city string)row format delimited fields terminated by &#39;\t&#39; stored as TEXTFILE; 在 Hive 下加载数据到分区 load data local inpath &#39;ihivedata/partidata&#39; into table t7 partition(class=&#39;job1&#39;,city=&#39;beijing&#39;); load data local inpath &#39;ihivedata/partidata&#39; into table t7 partition(class=&#39;job1&#39;,city=&#39;shanghai&#39;); load data local inpath &#39;ihivedata/partidata&#39; into table t7 partition(class=&#39;job2&#39;,city=&#39;ss&#39;); 注意: 多级分区其实就是多级目录 越靠近左边, 目录层级越高; 越靠近右边, 目录层级越低; load 数据到多级分区, load层级必须和整个层级数量相同 也就是说, 如果分区有2层, 传数据的时候, 也必须传2层分区, 并且层级顺序必须一致 从Linux 本地直接导数据到分区 可以直接在 hadoop UI 页面, 查看路径, 然后直接传到此路径中 hadoop fs -put ihivedata/partidata /user/hive/warehouse/mydb.db/t7/class=job1/city=beijing/p2 查看数据 select * from partition_table select count(*) from partition_table 删除表 drop table partition_table 工作中 用的最多的是 外部表 + 分区表 4.4 桶表 - 主要用于抽样查询桶表的基本操作 创建桶表完整过程 12345678910111213141516171819202122232425262728#创建分桶表drop table stu_buck;create table stu_buck(Sno int,Sname string,Sex string,Sage int,Sdept string)clustered by(Sno) sorted by(Sno DESC)into 4 bucketsrow format delimitedfields terminated by ',';#设置变量,设置分桶为true, 设置reduce数量是分桶的数量个数set hive.enforce.bucketing = true;set mapreduce.job.reduces=4;#开始往创建的分通表插入数据(插入数据需要是已分桶, 且排序的)#可以使用distribute by(sno) sort by(sno asc) 或是排序和分桶的字段相同的时候使用Cluster by(字段)#注意使用cluster by 就等同于分桶+排序(sort)insert into table stu_buckselect Sno,Sname,Sex,Sage,Sdept from student distribute by(Sno) sort by(Sno asc);insert overwrite table stu_buckselect * from student distribute by(Sno) sort by(Sno asc);insert overwrite table stu_buckselect * from student cluster by(Sno);-----以上3者效果一样的 保存select查询结果的几种方式： 将查询结果保存到一张新的hive表中 123create table t_tmpasselect * from t_p; 将查询结果保存到一张已经存在的hive表中 12insert into table t_tmpselect * from t_p; 将查询结果保存到指定的文件目录（可以是本地，也可以是hdfs） 12345insert overwrite local directory '/home/hadoop/test'select * from t_p;insert overwrite directory '/aaa/test'select * from t_p; 数据加载到桶表时，会对字段取hash值，然后与桶的数量取模。把数据放到对应的文件中。 所以顺序是打乱的, 不是原始 t1的数据顺序 查看数据 可以直接select * 查看全部 也可以直接单独查看每个桶的数据 123456789hive (mydb)&gt; dfs -cat /user/hive/warehouse/mydb.db/bucket_table/000000_0;4hive (mydb)&gt; dfs -cat /user/hive/warehouse/mydb.db/bucket_table/000001_0;51hive (mydb)&gt; dfs -cat /user/hive/warehouse/mydb.db/bucket_table/000002_0;2hive (mydb)&gt; dfs -cat /user/hive/warehouse/mydb.db/bucket_table/000003_0;3 修改桶的个数 alter table bucket_table clustered by (id) sorted by(id) into 10 buckets; 但是这样修改之后, 生成的是原来的 copy, 并且里面的数据也很奇怪, 不知道是按照什么来执行的? :TODO 注意： 物理上，每个桶就是表(或分区）目录里的一个文件 一个作业产生的桶(输出文件)和reduce任务个数相同 桶表工作中容易遇到的错误 向桶表中插入其它表查出的数据的时候, 必须指定字段名, 否则会报字段不匹配. 1234FAILED: SemanticException [Error 10044]: Line 1:12 Cannot insert into target table because column number/types are different 'bucket_table': Table insclause-0 has 1 columns, but query has 2 columns.# 应该是这样insert into bucket_table select id from t6; ※ 桶表的抽样查询 桶表的抽样查询 select * from bucket_table tablesample(bucket 1 out of 4 on id); tablesample是抽样语句 语法解析：TABLESAMPLE(BUCKET x OUT OF y) y必须是table总bucket数的倍数或者因子。 hive根据y的大小，决定抽样的比例。 例如，table总共分了64份，当y=32时，抽取(64/32=)2个bucket的数据，当y=128时，抽取(64/128=)1/2个bucket的数据。x表示从哪个bucket开始抽取。 例如，table总bucket数为32，tablesample(bucket 3 out of 16)，表示总共抽取（32/16=）2个bucket的数据，分别为第3个bucket和第（3+16=）19个bucket的数据。 5. Hive 视图的操作 使用视图可以降低查询的复杂度 视图的创建 create view v1 AS select t1.name from t1; 视图的删除 drop view if exists v1; 6. Hive 索引的操作 创建索引 create index t1_index on table t1(id) as &#39;org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler&#39; with deferred rebuild in table t1_index_table; t1_index: 索引名称 as: 指定索引器 t1_index_table: 要创建的索引表 显示索引 show formatted index on t1; 重建索引 alter index t1_index on t1 rebuild; 重建完索引之后, 查看 t1_index_table 这张表, 就存了t1表文件具体的位置, 最后一列t1_index_table._offsets是 索引的偏移量, 类似于指针, 偏移量是索引的精髓 12345678hive (mydb)&gt; select * from t1_index_table;OKt1_index_table.id t1_index_table._bucketname t1_index_table._offsets1 hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata [0]2 hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata [2]3 hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata [4]4 hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata [6]5 hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata [8] 分区字段本质上其实就是索引 7. 装载数据7.1 普通装载数据: 从本地 put 从 hive cp 从文件中装载数据 1hive&gt;LOAD DATA [LOCAL] INPATH '...' [OVERWRITE] INTO TABLE t2 [PARTITION (province='beijing')]; 通过查询表装载数据 12345678910111213141516# 方式1hive&gt;INSERT OVERWRITE TABLE t2 PARTITION (province='beijing') SELECT * FROM xxx WHERE xxx;# 方式2hive&gt;FROM t4 INSERT OVERWRITE TABLE t3 PARTITION (...) SELECT ...WHERE... INSERT OVERWRITE TABLE t3 PARTITION (...) SELECT ...WHERE... INSERT OVERWRITE TABLE t3 PARTITION (...) SELECT ...WHERE...; # 方式3 直接插入数据, 也会转化为文件的形式, 存在表的目录下- insert into table_name values(xxx); # 方式4 直接传文件 - load data (local) inpath ‘ xxx’ into table t_1; 7.2动态装载数据 不开启动态装载时 12hive&gt;INSERT OVERWRITE TABLE t3 PARTITION(province='bj', city='bj') SELECT t.province, t.city FROM temp t WHERE t.province='bj'; 开启动态分区支持 123hive&gt;set hive.exec.dynamic.partition=true;hive&gt;set hive.exec.dynamic.partition.mode=nostrict;hive&gt;set hive.exec.max.dynamic.partitions.pernode=1000; 把 t6 表的所有的字段 (包括分区字段) 加载进 t9 对应的分区 1hive (mydb)&gt; insert overwrite table t9 partition(class) select id,class from t6; 单语句建表并同时装载数据 1hive&gt;CREATE TABLE t4 AS SELECT .... 8. 导出数据 在hdfs之间复制文件(夹) hadoop fs -cp source destination hive&gt; dfs -cp source destination 案例: hive&gt; dfs -get /user/hive/warehouse/mydb.db/t9 /root/t9; 从 hdfs 复制到本地 使用DIRECTORY hive&gt;INSERT OVERWRITE 【LOCAL】 DIRECTORY &#39;...&#39; SELECT ...FROM...WHERE ...; 案例:通过查询导出到 t9, 走的 MapReduce 导到到 hdfs: insert overwrite directory &quot;/home/t9&quot; select * from t9; 导出到本地: insert overwrite local directory &quot;/home/ap/t9&quot; select * from t9; 9. 读模式&amp;写模式 RDBMS是写模式 Hive是读模式 10. 完整建表语句语法12345678910111213CREATE [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name [(col_name data_type [COMMENT col_comment], ...)] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [SKEWED BY (col_name, col_name, ...) ON ([(col_value, col_value, ...), ...|col_value, col_value, ...]) [STORED AS DIRECTORIES] ] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)] ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, ...)] [AS select_statement] (Note: not supported when creating external tables.) 11. 文件格式 TextFile SequenceFile RCFile ORC 11.1 默认存储格式就是 TextFile 存储空间消耗比较大， 并且压缩的text 无法分割和合并 查询的效率最低,可以直接存储， 加载数据的速度最高 11.2 使用SequenceFile存储 存储空间消耗大 压缩的文件可以分割和合并 查询效率高 需要通过text文件转化来加载 12345678910111213hive&gt; create table test2(str STRING) STORED AS SEQUENCEFILE; set hive.exec.compress.output=true;set mapred.output.compress=true;set mapred.output.compression.codec=com.hadoop.compression.lzo.LzoCodec;set io.seqfile.compression.type=BLOCK;set io.compression.codecs=com.hadoop.compression.lzo.LzoCodec;INSERT OVERWRITE TABLE test2 SELECT * FROM test1; 注意点: SequenceFile 类型的表, 不能直接导入数据文件, 只能通过从他表查询 insert overwrite table t2 select * from t1; 123# 查看此 'SequenceFile' 表hive (db2)&gt; dfs -cat /user/hive/warehouse/db2.db/t2/000000_0 ;SEQ"org.apache.hadoop.io.BytesWritableorg.apache.hadoop.io.Text*org.apache.hadoop.io.compress.DefaultCodec���/*&lt;bb�m�?x�c453x�c464x�c475x�c486x�c497hive (db2)&gt; 11.3 使用RCFile存储RCFILE是一种行列存储相结合的存储方式。首先，其将数据按行分块，保证同一个record在一个块上，避免读一个记录需要读取多个block。其次，块数据列式存储，有利于数据压缩和快速的列存取。 rcfile 存储空间最小 查询的效率最高 需要通过text文件转化来加载 加载的速度最低 123456789101112hive&gt; create table test3(str STRING) STORED AS RCFILE; set hive.exec.compress.output=true;set mapred.output.compress=true;set mapred.output.compression.codec=com.hadoop.compression.lzo.LzoCodec;set io.compression.codecs=com.hadoop.compression.lzo.LzoCodec;INSERT OVERWRITE TABLE test3 SELECT * FROM test1; 注意点: RCFile 也只能从其它表导入数据 12hive (db2)&gt; dfs -cat /user/hive/warehouse/db2.db/t3/000000_0;RCF*org.apache.hadoop.io.compress.DefaultCodechive.io.rcfile.column.number1 11.4 使用ORC存储(最好的一种格式)是一种针对 RCFile 优化的格式 主要特点: 压缩, 索引, 单文件输出 12345678910111213hive&gt; create table t1_orc(id int, name string) row format delimited fields terminated by '\t' stored as orc tblproperties("orc.compress"="ZLIB");ALTER TABLE ... [PARTITION partition_spec] SET FILEFORMAT ORC;# 也可以改为其它的, 修改的语法就是这样alter table t1 set fileformat textfile;SET hive.default.fileformat=Orc;insert overwrite table t1_orc select * from t1; 注意点: ORC 也只能从其它表导入数据 占用空间大, 一个 block 有256M, 之前2种都是128M 1234567891011hive (db2)&gt; dfs -cat /user/hive/warehouse/db2.db/t4/000000_0;ORCP+P �6�b�``���ь@�H� 1q01-PPK# (" id0P:P@�;��" (0��ORChive 12. 序列化 &amp; 反序列化 (Hive SerDe)12.1 SerDe What is a SerDe? SerDe 是 “Serializer and Deserializer.”的缩写 Hive 使用 SerDe和FileFormat进行行内容的读写. Hive序列化流程 从 HDFS 上读入文件 (反序列化) HDFS文件 --&gt; InputFileFormat --&gt; &lt;key, value&gt; --&gt; Deserializer --&gt; 行对象 写出到 HDFS (序列化) 行对象 --&gt; Serializer --&gt; &lt;key, value&gt; --&gt; OutputFileFormat --&gt; HDFS文件 注意: 数据全部存在在value中，key内容无意义 Hive 使用如下FileFormat 类读写 HDFS files: TextInputFormat/HiveIgnoreKeyTextOutputFormat: 读写普通HDFS文本文件. SequenceFileInputFormat/SequenceFileOutputFormat: 读写SequenceFile格式的HDFS文件 …. Hive 使用如下SerDe 类(反)序列化数据: MetadataTypedColumnsetSerDe: 读写csv、tsv文件和默认格式文件 ThriftSerDe: 读写Thrift 序列化后的对象. DynamicSerDe: 读写Thrift序列化后的对象, 不过不需要解读schema中的ddl. 12.2 使用CSV SerdeCSV格式的文件也称为逗号分隔值（Comma-Separated Values，CSV，有时也称为字符分隔值，因为分隔字符也可以不是逗号。在本文中的CSV格式的数据就不是简单的逗号分割的），其文件以纯文本形式存储表格数据（数字和文本）。CSV文件由任意数目的记录组成，记录间以某种换行符分隔；每条记录由字段组成，字段间的分隔符是其它字符或字符串，最常见的是逗号或制表符。通常，所有记录都有完全相同的字段序列。默认的分隔符是 DEFAULT_ESCAPE_CHARACTER \DEFAULT_QUOTE_CHARACTER “ —如果没有，则不需要指定DEFAULT_SEPARATOR , 12345678CREATE TABLE csv_table(a string, b string) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ("separatorChar"="\t", "quoteChar"="'", "escapeChar"="\\") STORED AS TEXTFILE;# separatorChar：分隔符# quoteChar：引号符# escapeChar：转义符&gt;&gt; :TODO 创建表没成功, 用到时再说 13. Lateral View 语法lateral view用于和split, explode等UDTF一起使用，它能够将一行数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。lateral view首先为原始表的每行调用UDTF，UTDF会把一行拆分成一或者多行，lateral view再把结果组合，产生一个支持别名表的虚拟表。 lateral: 侧面, 横切面 Lateral View: 切面表 创建表 create table t8(name string,nums array\&lt;int>)row format delimited fields terminated by “\t” COLLECTION ITEMS TERMINATED BY ‘:’; 数据切割 SELECT name,new_num FROM t8 LATERAL VIEW explode(nums) num AS new_num; select name,id from class_test lateral view explode(student_id_list) list as id; 注意: as 前面的 list 貌似是可以随表起名的 效果演示 12345678910111213141516171819202122hive (default)&gt; select * from class_test;OKclass_test.name class_test.student_id_listzhangsan [24,25,27,37]lisi [28,39,23,43]wangwu [25,23,2,54]---hive (default)&gt; select name,id from class_test lateral view explode(student_id_list) list as id;OKname idzhangsan 24zhangsan 25zhangsan 27zhangsan 37lisi 28lisi 39lisi 23lisi 43wangwu 25wangwu 23wangwu 2wangwu 54 14. Hive的高级函数14.1 简介 简单查询 select … from…where… 使用各种函数 hive&gt;show functions; 展示所有函数 hive&gt;describe function xxx; 详细描述函数用法 LIMIT语句 列别名 嵌套select语句 14.2 高级函数分类 标准函数 reverse() upper() 聚合函数 avg() sum() 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657select avg(id) from t3;select sum(id) from t3;# 最简答的嵌套hive (mydb)&gt; select t.id from (select * from t1 where id &gt; 3)t;# 最简单的 group by# if else 的效果select id, case when id &lt;= 2 then 'low'when id&gt;=3 and id &lt;4 then 'middle'when id&gt;=4 and id &lt;5 then 'high'else 'very high'end as id_highly from t1;----执行结果----id id_highly1 low2 low3 middle4 high5 very high--------------------select sid,case coursewhen 'yuwen' then scoreelse '0'end(别名) yuwencase coursewhen 'shuxue' then scoreelse '0'end(别名) shuxue# cast 转换函数, 大概是这么用hive (mydb)&gt; select id from t1 where cast(id AS FLOAT) &lt;3.0;OKid12 - ![image-20180619150824031](http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-070824.png) 首先当前不存在的列补0, 然后按照学号分组求和 ​ 然后按照 sid 分组求和/求最大值, 就可以了 同一列不同的放在不同的列上, 常用的方法 面试题4 array_contains() desc function array_contains() 自定义函数 UDF 15. Hive 性能调优15.1 什么时候可以避免执行MapReduce？ select * or select field1,field2 limite 10 where语句中只有分区字段 使用本地set hive.exec.mode.local.auto=true; 12345select * from t3;select name,age from t3;select name,age from t3 limit 2;select name,age from t3 where age=25;# 当 where 是本地字段(列中的字段), 是不走 MR的 group by语句： 通常和聚合函数一起使用，按照一个或者多个列对结果进行分组，然后对每组执行聚合操作 having语句： 限制结果的输出 hive将查询转化为MapReduce执行，hive的优化可以转化为mapreduce的优化！ 15.2 hive是如何将查询转化为MapReduce的？-EXPLAIN的使用 hive对sql的查询计划信息解析 EXPLAIN SELECT COUNT(1) FROM T1; EXPLAIN EXTENDED 显示详细扩展查询计划信息 EXPLAIN EXTENDED SELECT COUNT(1) FROM T1; 为啥我的 explain extended 只有固定的几行? 因为这个 count 没有调动 MR, 用 sum 就会启用 MR, 会出现长长的 log 12345678910111213141516171819---不会启用 MRhive (mydb)&gt; explain EXTENDED select count(1) from t9;---ExplainSTAGE DEPENDENCIES: Stage-0 is a root stage ...--------------------------------这里会启用 MRhive (mydb)&gt; explain EXTENDED select sum(id) from t9;---ExplainSTAGE DEPENDENCIES: Stage-1 is a root stage Stage-0 depends on stages: Stage-1STAGE PLANS: Stage: Stage-1 .... 15.3 性能调优15.3.1 本地mr 本地模式设置方式： set mapred.job.tracker=local; mapper 的本地模式, 只有在开发中才会使用 set hive.exec.mode.local.auto=true; Hive 的执行模式 可以用在生产中, 因为是自动模式, 可根据参数变化 设置这里才会转成 local hadoop 按照这里设定的规则hive.exec.mode.local.auto.input.files.max 测试 select 1 from wlan limit 5; 下面两个参数是local mr中常用的控制参数: hive.exec.mode.local.auto.inputbytes.max默认134217728 设置local mr的最大输入数据量,当输入数据量小于这个值的时候会采用local mr的方式 hive.exec.mode.local.auto.input.files.max默认是4 设置local mr的最大输入文件个数,当输入文件个数小于这个值的时候会采用local mr的方式 大于此数时, 就不会转化为 local hadoop 1Cannot run job locally: Number of Input Files (= 6) is larger than hive.exec.mode.local.auto.input.files.max(= 4) 可以这样修改local mr的最大输入文件个数值, 主要在调试阶段使用 1234hive (mydb)&gt; set hive.exec.mode.local.auto.input.files.max=8;# 这样设置了之后, 只要文件数&lt;=8, 就会在本地运行Job running in-process (local Hadoop) 15.3.2 开启并行计算 开启并行计算,增加集群的利用率 set hive.exec.parallel=true; 15.3.3 设置严格模式 设置严格模式 set hive.mapred.mode=strict; 设置非严格模式 set hive.mapred.mode=nonstrict; strict可以禁止三种类型的查询： 强制分区表的where条件过滤 Order by语句必须使用limit hive (mydb)&gt; select id from t9 where class=&#39;job110&#39; order by id limit 3; 限制笛卡尔积查询 15.3.4 调整mapper和reducer的数量 调整mapper和reducer的数量 太多map导致启动产生过多开销 marpred.min.split.size 按照输入数据量大小确定reducer数目, `set mapred.reduce.tasks= 默认3 dfs -count /分区目录/* hive.exec.reducers.max设置阻止资源过度消耗 JVM重用 小文件多或task多的业务场景 set mapred.job.reuse.jvm.num.task=10 会一直占用task槽 15.3.5 排序方面的优化 order by 语句：是全局排序, 用的比较多 123# 加个 desc 就是倒序排序hive (mydb)&gt; select id from bucket_table order by id desc limit 10;Automatically selecting local only mode for query sort by 语句： 是单reduce排序 一般与 distribute by结合使用 distribute by语句：类似于分桶，根据指定的字段将数据分到不同的 reducer，且分发算法是 hash 散列 与 sort by 结合用的比较多, 在每个分区内有序 12# 注意此处 distribute 的作用hive (mydb)&gt; select id from bucket_table distribute by id sort by id desc limit 10; cluster by语句： select * from t9 cluster by id; 可以确保类似的数据的分发到同一个reduce task中，并且保证数据有序, 防止所有的数据分发到同一个reduce上，导致整体的job时间延长 cluster by语句的等价语句： 如果分桶和sort字段是同一个时，此时，cluster by = distribute by + sort by distribute by Word sort by Word ASC 15.3.6 Map-side聚合可以直接在 .hiverc中配置 set hive.map.aggr=true; 这个设置可以将顶层的聚合操作放在Map阶段执行，从而减轻清洗阶段数据传输和Reduce阶段的执行时间，提升总体性能。:TODO 不太懂 缺点：该设置会消耗更多的内存。 执行select count(1) from wlan; 15.3.6 Join 优化 优先过滤后再进行 Join 操作，最大限度的减少参与 join 的数据量 小表 join 大表，最好启动 mapjoin 在使用写有 Join 操作的查询语句时有一条原则:应该将条目少的表/子查询放在 Join 操作 符的左边。 Join on 的条件相同的话，最好放入同一个 job，并且 join 表的排列顺序从小到大 在编写 Join 查询语句时，如果确定是由于 join 出现的数据倾斜，那么请做如下设置: set hive.skewjoin.key=100000; // 这个是 join 的键对应的记录条数超过这个值则会进行 分拆，值根据具体数据量设置 set hive.optimize.skewjoin=true; // 如果是 join 过程出现倾斜应该设置为 true 16. 表连接 (只支持等值连接) 16.1 简介 INNER JOIN 两张表中都有，且两表符合连接条件 select t1.name,t1.age,t9.age from t9 join t1 on t1.name=t9.name; LEFT OUTER JOIN 左表中符合where条件出现，右表可以为空 从左表返回所有的行(字段), 右表没有匹配where 条件的话返回null RIGHT OUTER JOIN 右表中符合where条件出现，左表可以为空 FULL OUTER JOIN 返回所有表符合where条件的所有记录，没有NULL替代 LEFT SEMI-JOIN 左表中符合右表on条件出现，右表不出现 Hive 当前没有实现 IN/EXISTS 子查询，可以用 LEFT SEMI JOIN 重写子查询语句。LEFT SEMI JOIN 的限制是， JOIN 子句中右边的表只能在 ON 子句中设置过滤条件，在 WHERE 子句、SELECT 子句或其他地方过滤都不行。 12345SELECT a.key, a.value FROM aWHERE a.key in (SELECT b.key FROM B);# 可以被重写为：SELECT a.key, a.val FROM a LEFT SEMI JOIN b on (a.key = b.key) 笛卡尔积 是m x n的结果 map-side JOIN 只有一张小表，在mapper的时候将小表完全放在内存中 select /+ mapjoin(t9) /t1.name,t1.age from t9 JOIN t1on t1.name=t9.name; 16.2 代码测试1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192关于hive中的各种join准备数据1,a2,b3,c4,d7,y8,u2,bb3,cc7,yy9,pp建表：create table a(id int,name string)row format delimited fields terminated by ',';create table b(id int,name string)row format delimited fields terminated by ',';导入数据：load data local inpath '/home/hadoop/a.txt' into table a;load data local inpath '/home/hadoop/b.txt' into table b;实验：** inner joinselect * from a inner join b on a.id=b.id;+-------+---------+-------+---------+--+| a.id | a.name | b.id | b.name |+-------+---------+-------+---------+--+| 2 | b | 2 | bb || 3 | c | 3 | cc || 7 | y | 7 | yy |+-------+---------+-------+---------+--+**left joinselect * from a left join b on a.id=b.id;+-------+---------+-------+---------+--+| a.id | a.name | b.id | b.name |+-------+---------+-------+---------+--+| 1 | a | NULL | NULL || 2 | b | 2 | bb || 3 | c | 3 | cc || 4 | d | NULL | NULL || 7 | y | 7 | yy || 8 | u | NULL | NULL |+-------+---------+-------+---------+--+**right joinselect * from a right join b on a.id=b.id;**select * from a full outer join b on a.id=b.id;+-------+---------+-------+---------+--+| a.id | a.name | b.id | b.name |+-------+---------+-------+---------+--+| 1 | a | NULL | NULL || 2 | b | 2 | bb || 3 | c | 3 | cc || 4 | d | NULL | NULL || 7 | y | 7 | yy || 8 | u | NULL | NULL || NULL | NULL | 9 | pp |+-------+---------+-------+---------+--+**select * from a left semi join b on a.id = b.id;+-------+---------+--+| a.id | a.name |+-------+---------+--+| 2 | b || 3 | c || 7 | y |+-------+---------+--+ 17. Hive自定义函数 &amp; Transform17.1 自定义函数类别 UDF 作用于单个数据行，产生一个数据行作为输出。（数学函数，字符串函数） UDAF (用户定义聚集函数)：接收多个输入数据行，并产生一个输出数据行。（count，max） 17.2 UDF 开发实例17.2.1 简单入门 先开发一个java类，继承UDF，并重载evaluate方法 12345678910package com.rox.hive.udf;import org.apache.hadoop.hive.ql.exec.UDF;public class ToLowerCase extends UDF &#123; // 必须是 public public String evaluate(String field) &#123; String res = field.toLowerCase(); return res; &#125;&#125; 打成jar包上传到服务器 将jar包添加到hive的classpath hive&gt;add JAR /home/ap/udf.jar; 创建临时函数与开发好的java class关联 Hive&gt;create temporary function tolowercase as &#39;com.rox.hive.udf.ToLowerCase&#39;; 即可在hql中使用自定义的函数strip select tolowercase(name) from t_1.. 17.2.2 稍稍复杂123456789101112131415161718192021222324252627282930313233343536373839404142# 需求: 通过一些手机号判断手机区域1364535532,101374535532,421384535532,341364535532,451384535532,22136-beijing137-shanghai138-guangzhou-----## 1.编写 UDF public static HashMap&lt;String, String&gt; provinceMap = new HashMap&lt;String, String&gt;(); static &#123; provinceMap.put("136", "beijing"); provinceMap.put("136", "shanghai"); provinceMap.put("136", "guangzhou"); &#125; public String evaluate(String phoneNum) &#123; return provinceMap.get(phoneNum.substring(0, 3)) == null ? "huoxing" : provinceMap.get(phoneNum.substring(0, 3)); &#125; ## 2.打包上传, 添加到 classpath, 创建临时函数## 3.创建表,加载数据create table flow_t(pnum string,flow int)row format delimited fields terminated by ',';load data local inpath '/home/ap/ihivedata/flow.tmp' into table flow_t;## 4.使用0: jdbc:hive2://cs2:10000&gt; select pnum,tolow(pnum),flow from flow_t;+-------------+------------+-------+| pnum | _c1 | flow |+-------------+------------+-------+| 1364535532 | guangzhou | 10 || 1374535532 | huoxing | 42 || 1384535532 | huoxing | 34 || 1364535532 | guangzhou | 45 || 1384535532 | huoxing | 22 |+-------------+------------+-------+ 17.2.3 有些复杂12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788# 解析 json 数据表 rating.json## 1. 写 udf// com.rox.json.MovieRateBean package com.rox.json;import lombok.Getter;import lombok.Setter;@Getter@Setterpublic class MovieRateBean &#123; private String movie; private String rate; private String timeStamp; private String uid; @Override public String toString() &#123; return movie + "\t" + rate + "\t" + timeStamp + "\t" + uid; &#125;&#125;// com.rox.json.JsonParser package com.rox.json;import java.io.IOException;import org.apache.hadoop.hive.ql.exec.UDF;import com.fasterxml.jackson.databind.ObjectMapper;public class JsonParser extends UDF &#123; public String evaluate(String jsonLine) &#123; ObjectMapper objectMapper = new ObjectMapper(); try &#123; MovieRateBean bean = objectMapper.readValue(jsonLine, MovieRateBean.class); return bean.toString(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return ""; &#125;&#125;====================================================## 2.打包上传, 添加到 classpath, 创建临时函数, 检查是否成功show functions;## 3.创建表,加载数据create table t_json(line string) row format delimited;load data local inpath '/home/ap/ihivedata/flow.log' into table t_json;## 4.检查数据select * from t_json limit 10;## 5.调用函数0: jdbc:hive2://cs2:10000&gt; select jsonparser(line)parsedline from t_json limit 10;+---------------------+| parsedline |+---------------------+| 1193 5 978300760 1 || 661 3 978302109 1 || 914 3 978301968 1 || 3408 4 978300275 1 || 2355 5 978824291 1 || 1197 3 978302268 1 || 1287 5 978302039 1 || 2804 5 978300719 1 || 594 4 978302268 1 || 919 4 978301368 1 |+---------------------+// 但是这样只是把每一行解析出来了, ## 6.删除原来的表 drop table if exists t_rating;## 7.重新创建一个表create table t_rating(movieid string,rate int,timestring string,uid string)row format delimited fields terminated by '\t';## 8.根据查出来的每一行, 按照 '\t'分割, 然后再插入到表中create table t_rating asselect split(jsonparser(line),'\t')[0]as movieid, split(jsonparser(line),'\t')[1] as rate, split(jsonparser(line),'\t')[2] as timestring, split(jsonparser(line),'\t')[3] as uid from t_json limit 10;// 但是执行结果会报错,不知道为啥,难道是 java 代码的问题? :TODO-------## 9.内置json函数select get_json_object(line,'$.movie') as moive,get_json_object(line,'$.rate') as rate from rat_json limit 10; 17.3 Transform实现12345678910111213141516171819202122232425262728293031323334353637383940414243# 1、先加载rating.json文件到hive的一个原始表 t_jsoncreate table rat_json(line string) row format delimited;load data local inpath '/home/ap/rating.json' into table t_json;2、需要解析json数据成四个字段，插入一张新的表 t_ratingdrop table if exists t_rating;# 创建表create table t_rating(movieid string,rate int,timestring string,uid string)row format delimited fields terminated by '\t';# 插入, 也可以直接创建 create table xx as + select...insert overwrite table t_ratingselect get_json_object(line,'$.movie') as moiveid, get_json_object(line,'$.rate') as rate, get_json_object(line,'$.timeStamp') as timestring,get_json_object(line,'$.uid') as uid from t_json;3. 写一个 python 脚本vi weekday_mapper.py#!/bin/pythonimport sysimport datetimefor line in sys.stdin: line = line.strip() movieid, rating, timestring,userid = line.split('\t') weekday = datetime.datetime.fromtimestamp(float(timestring)).isoweekday() print '\t'.join([movieid, rating, str(weekday),userid]) 4. 保存文件, 然后将文件加入 hive 的 classpathhive&gt;add FILE /home/hadoop/weekday_mapper.py;5. 此时可以直接创建新表hive&gt;create TABLE u_data_new asSELECT TRANSFORM (movieid, rate, timestring,uid) USING 'python weekday_mapper.py' AS (movieid, rate, weekday,uid)FROM t_rating;6. 查询结果## distinct看看有多少个不重复的数字select distinct(weekday) from u_data_new limit 10; 18. 案例18.1 广告推送-用户画像的介绍一个广告推送平台的项目结构示意图 18.2 累计报表实现套路(面试)题: 求每个月的累计访问次数 此处的访问次数可以换成工资,等等.. 有如下访客访问次数统计表 t_access_times 访客 月份 访问次数 A 2015-01 5 A 2015-01 15 B 2015-01 5 A 2015-01 8 B 2015-01 25 A 2015-01 5 A 2015-02 4 A 2015-02 6 B 2015-02 10 B 2015-02 5 …… …… …… 需要输出报表：t_access_times_accumulate 访客 月份 月访问总计 累计访问总计 A 2015-01 33 33 A 2015-02 10 43 ……. ……. ……. ……. B 2015-01 30 30 B 2015-02 15 45 ……. ……. ……. ……. 解题代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081# 准备数据vi t_access_timesA,2015-01,5A,2015-01,15B,2015-01,5A,2015-01,8B,2015-01,254A,2015-01,5A,2015-02,4A,2015-02,6B,2015-02,10B,2015-02,5----# 建表create table t_access_times(username string,month string,salary int)row format delimited fields terminated by ',';# 加载数据load data local inpath '/home/ap/t_access_times' into table t_access_times;1、第一步，先求个用户的月总金额select username,month,sum(salary) as salary from t_access_times group by username,month;+-----------+----------+---------+--+| username | month | salary |+-----------+----------+---------+--+| A | 2015-01 | 33 || A | 2015-02 | 10 || B | 2015-01 | 30 || B | 2015-02 | 15 |+-----------+----------+---------+--+2、第二步，将月总金额表 自己连接 自己连接create table aa as(select username,month,sum(salary) as salary from t_access_times group by username,month) A inner join (select username,month,sum(salary) as salary from t_access_times group by username,month) BA.username=B.username+-------------+----------+-----------+-------------+----------+-----------+--+| a.username | a.month | a.salary | b.username | b.month | b.salary |+-------------+----------+-----------+-------------+----------+-----------+--+| A | 2015-01 | 33 | A | 2015-01 | 33 || A | 2015-01 | 33 | A | 2015-02 | 10 || A | 2015-02 | 10 | A | 2015-01 | 33 || A | 2015-02 | 10 | A | 2015-02 | 10 || B | 2015-01 | 30 | B | 2015-01 | 30 || B | 2015-01 | 30 | B | 2015-02 | 15 || B | 2015-02 | 15 | B | 2015-01 | 30 || B | 2015-02 | 15 | B | 2015-02 | 15 |+-------------+----------+-----------+-------------+----------+-----------+--+3、第三步，从上一步的结果中进行分组查询，分组的字段是a.username a.month求月累计值： 将b.month &lt;= a.month的所有b.salary求和即可select A.username,A.month,max(A.salary) as salary,sum(B.salary) as accumulatefrom (select username,month,sum(salary) as salary from t_access_times group by username,month) A inner join (select username,month,sum(salary) as salary from t_access_times group by username,month) BonA.username=B.usernamewhere B.month &lt;= A.monthgroup by A.username,A.monthorder by A.username,A.month;+-------------+----------+---------+-------------+| a.username | a.month | salary | accumulate |+-------------+----------+---------+-------------+| A | 2015-01 | 33 | 33 || A | 2015-02 | 10 | 43 || B | 2015-01 | 259 | 259 || B | 2015-02 | 15 | 274 |+-------------+----------+---------+-------------+ 18.3 待做项目 19.注意点 使用聚合函数, 后面一定要分组(group by xxx), group by 会自动去重 如果 sql 语句中有 group by, 那么 select 后面必须有 group by 的字段, 或聚合函数 使用order by 排序某个字段时, 必须在 select中出现此字段, 否则会找不到 :TODO 待验证]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce-简单总结]]></title>
    <url>%2F2018%2F06%2F12%2FHadoop%2F2-MapReduce%2FMapReduce-%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[MapReduce在编程的时候，基本上一个固化的模式，没有太多可灵活改变的地方，除了以下几处： 输入数据接口：InputFormat FileInputFormat (文件类型数据读取的通用抽象类) DBInputFormat （数据库数据读取的通用抽象类） 默认使用的实现类是： TextInputFormat job.setInputFormatClass(TextInputFormat.class) TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回 逻辑处理接口： Mapper 完全需要用户自己去实现其中 map() setup() clean() map输出的结果在shuffle阶段会被partition以及sort，此处有两个接口可自定义： Partitioner 有默认实现 HashPartitioner，逻辑是根据key和numReduces来返回一个分区号:key.hashCode()&amp;Integer.MAXVALUE % numReduces 通常情况下，用默认的这个HashPartitioner就可以，如果业务上有特别的需求，可以自定义 继承 Partitioner, 重写getPartition方法, 具体见这里 Comparable 当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，override其中的compareTo()方法, 具体见这里 reduce端的数据分组比较接口 ： Groupingcomparator reduceTask拿到输入数据（一个partition的所有数据）后，首先需要对数据进行分组，其分组的默认原则是key相同，然后对每一组kv数据调用一次reduce()方法，并且将这一组kv中的第一个kv的key作为参数传给reduce的key，将这一组数据的value的迭代器传给reduce()的values参数 利用上述这个机制，我们可以实现一个高效的分组取最大值的逻辑： 自定义一个bean对象用来封装我们的数据，然后改写其compareTo方法产生倒序排序的效果 然后自定义一个Groupingcomparator，将bean对象的分组逻辑改成按照我们的业务分组id来分组（比如订单号） 这样，我们要取的最大值就是reduce()方法中传进来key 逻辑处理接口：Reducer 完全需要用户自己去实现其中 reduce() setup() clean() 输出数据接口： OutputFormat 有一系列子类 FileOutputformat DBoutputFormat ….. 默认实现类是TextOutputFormat 功能逻辑是： 将每一个KV对向目标文本文件中输出为一行 ​]]></content>
      <categories>
        <category>Hadoop</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive学习-1-安装]]></title>
    <url>%2F2018%2F06%2F12%2FHadoop%2F3-Hive%2FHive%E5%AD%A6%E4%B9%A0-1%2F</url>
    <content type="text"><![CDATA[Hive简介 Hive基本情况Hive 是建立在 Hadoop 上的数据仓库基础构架 由facebook开源，最初用于解决海量结构化的日志数据统 计问题; ETL(Extraction-Transformation-Loading)工具 构建在Hadoop之上的数据仓库; 数据计算使用MR，数据存储使用HDFS 数据库&amp;数据仓库 的区别： 概念上 数据库：用于管理精细化数据，一般情况下用于存储结果数据，分库分表进行存储 数据仓库：存储、查询、分析大规模数据 。更像一个打包的过程，里面存储的数据没有细化区分，粒度较粗 用途上： 数据库：OLTP，on line Transation Processing 联机事务处理，增删改 数据仓库：OLAP，on line analysis Processing 联机事务分析处理，查询，hive不支持删除、修改。 支持插入。 使用上： 数据库：标准sql, hbase: 非标准sql 数据仓库：方言版的sql， HQL 模式上： 数据库：写模式 数据仓库：读模式 可以将结构化的数据映射成一张数据库表 结构化数据映射成二维表 将文本中每一行数据映射为数据库的每一条数据 将文本中每一列数据映射为hive的表字段 提供HQL 查询功能 hive query language， 方言版sql 底层数据是存储在HDFS上 hive上建的表仅仅相当于对hdfs上的结构化数据进行映射管理 hive仅仅是一个管理数据的作用，而不会存储数据 hive想要管理hdfs上的数据，就要建立一个关联关系，关联hive上的表和hdfs上的数据路径 数据是依赖于一个元数据库 元数据库采用的是关系型数据库， 真实生产中一般使用mysql为hive的元数据库，hive内置默认的元数据库是 derby 元数据： HCalalog hive中的表和hdfs的映射关系，以及hive表属性（内部表，外部表，视图）和字段信息 元数据一旦修饰，hive的所有映射关系等都没了，就无法使用了 可与Pig、Presto等共享 Hive的是构建在Hadoop之上的数据仓库 数据计算使用MR，数据存储使用HDFS 通常用于进行离线数据处理(采用MapReduce) 可认为是一个HQL—-&gt;MR的语言翻译器 Hive优缺点： 优点： 简单，容易上手 提供了类SQL查询语言HQL 为超大数据集设计的计算/扩展能力 MR作为计算引擎，HDFS作为存储系统 统一的元数据管理(HCalalog) 可与Pig、Presto等共享 缺点 不支持 删除 &amp; 修改 delete&amp;update，不支持事务 因为是基于HDFS hive做的最多的是查询 Hive的HQL表达的能力有限 迭代式算法无法表达 有些复杂运算用HQL不易表达 Hive效率较低，查询延时高 Hive自动生成MapReduce作业，通常不够智能 HQL调优困难，粒度较粗 可控性差 Hive与传统关系型数据库(RDBMS）对比 Hive的架构 1) 用户接口 CLI：Command Line Interface，即Shell终端命令行，使用 Hive 命令行与 Hive 进行交互，最常用（学习，调试，生产），包括两种运行方式： hive命令方式：前提必须在hive安装节点上执行 hiveserver2方式：hive安装节点将hive启动为一个后台进程，客户机进行连接（类似于启动了一个hive服务端） 真实生产中常用！ - 1) 修改配置文件，允许远程连接; 第一：修改 hdfs-site.xml，加入一条配置信息，启用 webhdfs； 第二：修改 core-site.xml，加入两条配置信息，设置 hadoop的代理用户。 - 2) 启动服务进程 - 前台启动：hiveserver2 - 后台启动 - 记录日志：nohup hiveserver2 1&gt;/home/sigeon/hiveserver.log 2&gt;/home/sigeon/hiveserver.err &amp; 0：标准日志输入 1：标准日志输出 2：错误日志输出 如果我没有配置日志的输出路径，日志会生成在当前工作目录，默认的日志名称叫做： nohup.xxx - 不记录日志：nohup hiveserver2 1&gt;/dev/null 2&gt;/dev/null &amp; - [补充：] - nohup命令：no hang up的缩写，即不挂起，可以在你退出帐户/关闭终端之后继续运行相应的进程。 - 语法：nohup &lt;command&gt; &amp; - 3) 开启beenline客户端并连接： - 方法一： - beenline，开启beenline客户端; - !connect jdbc:&lt;hive2://master:10000&gt;，回车。然后输入用户名和密码，这个用户名是安装 hadoop 集群的用户名 - 方法二： - beeline -u jdbc:&lt;hive2://master:10000&gt; -n sigeon JDBC/ODBC：是 Hive 的基于 JDBC 操作提供的客户端，用户（开发员，运维人员）通过 这连接至 Hive server 服务 Web UI：通过浏览器访问 Hive，基本不会使用 2) 元数据库：保存元数据，一般会选用关系型数据库（如mysql，Hive 和 MySQL 之间通过 MetaStore 服务交互） 3) Thrift服务：Facebook 开发的一个软件框架，可以用来进行可扩展且跨语言的服务的开发， Hive 集成了该服务，能让不同的编程语言调用 Hive 的接口 4) 驱动Driver a. 解释器：解释器的作用是将 HiveSQL 语句转换为抽象语法树（AST） b. 编译器：编译器是将语法树编译为逻辑执行计划 c. 优化器：优化器是对逻辑执行计划进行优化 d. 执行器：执行器是调用底层的运行框架执行逻辑执行计划 Hive的数据组织格式 1)库：database 2) 表 a. 内部表（管理表：managed_table） b. 外部表（external_table） 内部表和外部表区别： 内部表和外部表是两个相对的概念，不可能有一个表同时是内部表又是外部表； 内部表删除表的时候会删除原始数据和元数据，而外部表删除表的时候只会删除元数据不会删除原始数据 一般情况下存储公共数据的表存放为外部表 大多数情况，他们的区别不明显。如果数据的所有处理都在 Hive 中进行，那么倾向于 选择内部表；但是如果 Hive 和其他工具要针对相同的数据集进行处理，外部表更合适。 使用外部表访问存储在 HDFS 上的初始数据，然后通过 Hive 转换数据并存到内部表中，使用外部表的场景是针对一个数据集有多个不同的 Schema。 c. 分区表 不同于hadoop中的分区，分区表是人为划分的 hive最终存储海量数据，海量数据查询一定注意避免全表扫描 查询的时候为了提升我们的查询性能，出现了分区表 将数据按照用户的业务存储到不同的目录下，在进行数据查询时只会对指定分区下的数据进行扫描一般情况下生产中用日期作为分区字段 d. 分桶表 类似于hadoop中的分区，是由程序决定的，只能指定桶的个数（分区的个数） 根据hash算法将余数不同的输出到不同的文件中 作用： 1）提升join的性能思考这个问题：select a.id,a.name,b.addr from a join b on a.id = b.id;如果 a 表和 b 表已经是 分桶表，而且分桶的字段是 id 字段做这个 join 操作时，还需要全表做笛卡尔积 2）提升数据样本的抽取效率，直接拿一个桶中的数据作为样本数据 分区表和分桶表的区别： Hive 数据表可以根据某些字段进行分区操作，细化数据管理，可以让部分查询更快。 同时表和分区也可以进一步被划分为 Buckets，分桶表的原理和 MapReduce 编程中的 HashPartitioner 的原理类似 分区和分桶都是细化数据管理，但是分区表是手动添加区分，由于 Hive 是读模式，所以对添加进分区的数据不做模式校验 分桶表中的数据是按照某些分桶字段进行 hash 散列 形成的多个文件，所以数据的准确性也高很多 3)视图： hive中的视图仅仅相当于一个sql语句的别名 在hive中仅仅存在逻辑视图，不存在物理视图 物理视图：讲sql语句的执行结果存在视图中 逻辑视图： 仅仅是对查询结果的引用 4)数据存储： 原始数据中存在HDFS 元数据存在mysql Hive安装装hive其实不难，主要是安装mysql，解决mysql的权限问题 MySql安装RPM 安装MySQl： 检查以前是否装过 MySQL 1rpm -qa|grep -i mysql 发现有的话就都卸载 12rpm -e --nodeps mysql-libs-5.1.73-5.el6_6.x86_64rpm -e --nodeps .... 删除老版本 mysql 的开发头文件和库 12rm -rf /usr/lib64/mysql# 在搜索 my.cnf 文件，有的话就删掉 上传mysql 安装包到 Linux中，解压 12345678tar -xvf mysql-5.6.26-1.linux_glibc2.5.x86_64.rpm-bundle.tar# 解压出来有这些文件MySQL-devel-5.6.26-1.linux_glibc2.5.x86_64.rpmMySQL-embedded-5.6.26-1.linux_glibc2.5.x86_64.rpmMySQL-server-5.6.26-1.linux_glibc2.5.x86_64.rpmMySQL-shared-5.6.26-1.linux_glibc2.5.x86_64.rpmMySQL-shared-compat-5.6.26-1.linux_glibc2.5.x86_64.rpmMySQL-test-5.6.26-1.linux_glibc2.5.x86_64.rpm 安装 server &amp; client 1234# serverrpm -ivh MySQL-server-5.6.26-1.linux_glibc2.5.x86_64.rpm# clientrpm -ivh MySQL-client-5.6.26-1.linux_glibc2.5.x86_64.rpm 启动Mysql 1sudo service mysql start 登录Mysql并改密码，等 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566# 初始密码在这个文件中cat /root/.mysql_sercert # 登录mysql -uroot -pxxxxx# 删除除了`%`之外的其他所有hostuse mysql;select host,user,password from user;delete from user where host in (&apos;localhost&apos;, &apos;127.0.0.1&apos;,&apos;::1&apos;, ...)# 修改密码 UPDATE user SET Password = PASSWORD(&apos;psd&apos;) WHERE user = &apos;root&apos;;# 为`%` 和 `*` 添加远程登录权限 #注意： 前面的 mysql 登录用户名， 123 是登录密码GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;psd&apos; WITH GRANT OPTION;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;*&apos; IDENTIFIED BY &apos;psd&apos; WITH GRANT OPTION;FLUSH PRIVILEGES;# 退出登录exit;# 此时可以再登录试试mysql -uroot -ppsd======================================# 修改字符集为 utf-8# 新建一个文件vi /etc/my.cnf # 添加以下内容[client]default-character-set=utf8[mysql]default-character-set=utf8[mysqld]character-set-server=utf8# 重启mysqlsudo service mysql restart======================================# 忘记密码，修改密码的方法# 停止mysql服务的运行service mysql stop# 跳过授权表访问mysqld_safe --user=mysql --skip-grant-tables --skip-networking &amp; # 登录mysqlmysql -u root mysql # 接下来可以修改密码了 ##在mysql5.7以下的版本如下：mysql&gt; UPDATE user SET Password=PASSWORD(&apos;newpassword&apos;) where USER=&apos;root’； ##在mysql5.7版本如下：update mysql.user set authentication_string=password(&apos;newpassword&apos;) ;# 修改完了重启service mysql restart====================================== MySql的其它错误在运行schematool -dbType mysql -initSchema手动初始化元数据库的时候 报了一个log4j重复加载的问题 解决：可以不用理睬 链接mysql 密码过期问题 Your password has expired. 解决： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950mysql -uroot -p123mysql&gt; use mysqlReading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; select host,user,password_expired from user;+-----------+------+------------------+| host | user | password_expired |+-----------+------+------------------+| % | root | N || cs1 | root | Y || 127.0.0.1 | root | Y || ::1 | root | Y |+-----------+------+------------------+4 rows in set (0.00 sec)-------------mysql&gt; update user set password_expired=&apos;N&apos; where host=&apos;::1&apos;;Query OK, 1 row affected (0.01 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; update user set password_expired=&apos;N&apos; where host=&apos;cs1&apos;;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; update user set password_expired=&apos;N&apos; where host=&apos;127.0.0.1&apos;;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; FLUSH PRIVILEGES;Query OK, 0 rows affected (0.00 sec)mysql&gt; select host,user,password_expired from user;+-----------+------+------------------+| host | user | password_expired |+-----------+------+------------------+| % | root | N || cs1 | root | N || 127.0.0.1 | root | N || ::1 | root | N |+-----------+------+------------------+4 rows in set (0.00 sec)mysql&gt; exitBye[ap@cs1]~/apps/hive% sudo service mysql restart# 再重新初始化就好了 Yum安装 Mysql 因为笔者没装过，所以这部分暂且不表 step2-安装HiveMySql安装好之后， 安装Hive就很简答了 注意: Hive 是操作 Mysql 的数据库, 一定要记得把 mysql 驱动文件放到 hive 下的 lib 中!!!启动beeline 前, 要把 hiveserver2开起来, 当然, hdfs, mysql 都要启动起来123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132# 1.上传到Linux# 2.解压安装包到安装目录tar -zxvf apache-hive-2.3.2-bin.tar.gz -C ~/apps/# 3.把MySQL驱动包(mysql-connector-java-5.1.40-bin.jar)放置在hive 的根路径下的 lib 目录，此处是 ~/apps/apache-hive-2.3.2-bin/lib# 4.修改配置文件cd ~/apps/apache-hive-2.3.2-bin/conf# 新建一个 hive-site.xmltouch hive-site.xmlvi hive-site.xml+++++++++++++++++++++++++++++++++++++++++++添加如下内容++++++++++++&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/hivedb?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;!-- 注意:如果mysql和hive 不在同一个服务器节点,需要使用mysql节点的 hostname 或 ip --&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt;+++++++++++++++++++++++++++++++++++++++++++添加如下内容+++++++++++ &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;description&gt;hive default warehouse, if nessecory, change it &lt;!-- 这个是配置hive在HDFS上 db 的路存储径的，不配默认默认就是上述路径 --&gt; &lt;/description&gt; &lt;/property&gt;++++++++++++++++++++++++++# 5.配置环境变量 &amp; source## 注意：ap是我的用户家目录，换上自己的用户家目录export HIVE_HOME=/home/ap/apps/apache-hive-2.3.2-bin export PATH=$PATH:$HIVE_HOME/bin# 用bash的找 .bash_profile， 配置所有环境变量source ~/.zshrc # 6.此时基本安装完成了，验证Hive安装hive --helo# 7.重点来了！ 初始化元数据库schematool -dbType mysql -initSchema&gt; 这里可能会遇到很多错误！！ &gt; 但是如果前面按照我的方法装的, 应该就问题不大了&gt; 主要是 mysql 连接权限的问题！！# 一定会出现的2个1&gt; 找不到 hive命令的一长串环境变量 (不用理会)2&gt; 两个log4j，jar包重复的问题 (不用理会)# 8.初始化完成后，可以看下数据库中有没有 hivedb 这个库，成功的话是会有的## 启动hivehive --service cli &gt;hive: show databases;# 如果能显示数据库，就没啥问题了# ❤️9.Hive的使用方式之 HiveServer2/beeline此处需要修改 hadoop 的配置文件# 9.1.首先关闭hdfs &amp; yarn服务 &amp; RunJar(hive服务)，修改hadoop配置文件# 9.2.修改 hadoop 集群的 hdfs-site.xml 配置文件:加入一条配置信息，表示启用 webhdfs&lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;# 9.3.修改 hadoop 集群的 core-site.xml 配置文件:加入两条配置信息:表示设置 hadoop 的代理用户## 注意： 此处的ap是配置 hadoop 的用户名&lt;property&gt; &lt;name&gt;hadoop.proxyuser.ap.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;!-- 表示任意节点使用 hadoop 集群的代理用户 hadoop 都能访问 hdfs 集群 --&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.ap.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;!-- 表示代理用户的组所属 --&gt;&lt;/property&gt; 注意 修改完成后， 发给hadoop集群其他主机scp xxx.xx xxx.ss cs2:$PWDscp xxx.xx xxx.ss cs3:$PWD...# 9.4.重启hdfs &amp; yarn服务# 9.5.启动 hiveserver2 服务## 后台启动：nohup hiveserver2 1&gt;/home/ap/hiveserver.log 2&gt;/home/ap/hiveserver.err &amp; # 或者:nohup hiveserver2 1&gt;/dev/null 2&gt;/dev/null &amp;# 或者:nohup hiveserver2 &gt;/dev/null 2&gt;&amp;1 &amp;# 以上 3 个命令是等价的，第一个表示记录日志，第二个和第三个表示不记录日志注意： nohup 可以在你退出帐户/关闭终端之后继续运行相应的进程。 nohup 就是不挂起的意思(no hang up)。该命令的一般形式为:nohup command &amp;# 9.6 启动 beeline 客户端去连接方式1：执行命令:beeline -u jdbc:hive2://cs2:10000 -n ap-u : 指定元数据库的链接信息 -n : 指定用户名和密码方式2：先执行 beeline然后按图所示输入:# 此处的cs2是只安装hive的hostname!connect jdbc:hive2://cs2:10000 按回车，然后输入用户名，密码，这个 用户名就是安装 hadoop 集群的用户名和密码 登录beeline :bee:方式1: 直接登录 登录前要开启 RunJar 进程, 也就是 开启hiveserver2 nohup hiveserver2 1&gt;/home/ap/hiveserver.log 2&gt;/home/ap/hiveserver.err &amp; beeline -u jdbc:hive2://cs2:10000 -n ap 方式2: 输入用户名密码登录 !connect jdbc:hive2://cs2:10000 登录 hivehive PS: Linux环境变量失效1234# 就是直接把环境变量设置为/bin:/usr/bin，因为常用的命令都在/bin这个文件夹中。PATH=/bin:/usr/bin# 接下来修改 .bashr_profile 或则 .zshrc中的内容即可， 修改完了重新source Hive – DDL库的操作12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758 库的操作 ============================================================================#  建库 CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment][LOCATION hdfs_path][WITH DBPROPERTIES (property_name=property_value, ...)];1、创建普通库create database dbname;2、创建库的时候检查存与否create databse if not exists dbname;3、创建库的时候带注释create database if not exists dbname comment &apos;create my db named dbname&apos;;4、查看创建库的详细语句show create database mydb;#  查看库 1、查看有哪些数据库 show databases;2、显示数据库的详细属性信息语法:desc database [extended] dbname; 示例:desc database extended myhive;3、查看正在使用哪个库 select current_database();4、查看创建库的详细语句 show create database mydb;5、查看以xx开头的库show databases like &apos;s*&apos;#  删除库 删除库操作: drop database dbname; drop database if exists dbname; 默认情况下，hive 不允许删除包含表的数据库，有两种解决办法:1、 手动删除库下所有表，然后删除库2、 使用 cascade 关键字 drop database if exists dbname cascade; 默认情况下就是 restrict（严格模式）, 后2行效果一样 drop database if exists myhive drop database if exists myhive restrict#  切换库 切换库操作:- 语法:use database_name - 实例:use myhive; 表的操作创建表建表语法 CREATE [EXTERNAL] TABLE [IF NOT EXISTS] &lt;table_name&gt;创建内部表；添加EXTERNAL参数会创建外部表 (col_name data_type [COMMENT col_comment], ...)添加字段和字段描述 [COMMENT table_comment]添加表描述 [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]指定分区字段和字段描述，分区字段不能为建表字段！ [CLUSTERED BY (col_name, col_name, ...)] SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] 指定分桶字段，分桶字段必须为建表字段！ 指定排序字段，此字段也必须为建表字段，指定的是分桶内的排序 指定分桶个数（hash后再取模） [ROW FORMAT row_format]指定分隔符，row_format 格式：列分隔符：`delimited fields terminated by ‘x&apos;` 行分隔符：`line terminated by ‘x&apos;` [STORED AS file_format]指定存储格式textfile：文本格式，默认 rcfile：行列结合格式 parquet：压缩格式 [LOCATION hdfs_path]指定表在hsfs上的存储路径，不指定的话就按配置的路径存储，如果也没指定就在hive默认的路经 /user/hive/warehouse 建表代码 a. 创建内部表 create table mytable (id int, name string) row format delimited fields terminated by &#39;,&#39; stored as textfile; b. 创建外部表 create external table mytable2 (id int, name string) row format delimited fields terminated by &#39;,&#39; location &#39;/user/hive/warehouse/mytable2&#39;; c. 创建分区表 create table table3(id int, name string) partitioned by(sex string) row format delimited fields terminated by &#39;,&#39; stored as textfile; 插入分区数据：load data local inpath &#39;/root/hivedata/mingxing.txt&#39; overwrite into table mytable3 partition(sex=&#39;girl’); 查询表分区： show partitions mytable3 d. 创建分桶表 create table stu_buck(Sno int,Sname string,Sex string,Sage int,Sdept string) clustered by(Sno) sorted by(Sno DESC) into 4 buckets row format delimited fields terminated by &#39;,’; e. 复制表 create [external] table [if not exists] new_table like table_name; f. 查询表 create table table_a as select * from teble_b; 查看表 desc &lt;table_name&gt;：显示表的字段信息 desc formatted &lt;table_name&gt;：格式化显示表的详细信息 desc extended &lt;table_name&gt;：显示表的详细信息 修改表 重命名 ALTER TABLE old_name RENAME TO new_name 修改属性 ALTER TABLE table_name SET TBLPROPERTIES (&#39;comment&#39; = &#39;my new students table’); 不支持修改表名，和表的数据存储目录 增加/修改/替换字段 ALTER TABLE table_name ADD COLUMNS (col_spec [, col_spec ...])新增的字段位置在所有列后面 ( partition 列前 ) ALTER TABLE table_name CHANGE c_name new_c_name new_c_type [FIRST | AFTER c_name]注意修改字段时，类型只能由小类型转为大类型，不让回报错；（在hive1.2.2中并没有此限制） ALTER TABLE table_name REPLACE COLUMNS (col_spec [, col_spec ...])REPLACE 表示替换表中所有字段 添加/删除分区 添加分区：ALTER TABLE table_name ADD [IF NOT EXISTS] PARTITION (partition_col = col_value1 [ ... ] ) [LOCATION &#39;location1’] 删除分区：ALTER TABLE table_name DROP [IF EXISTS] PARTITION (partition_col = col_value1 [ ... ] ) 修改分区路径：ALTER TABLE student_p PARTITION (part=&#39;bb&#39;) SET LOCATION &#39;/myhive_bbbbb’; [补充：] 1、 防止分区被删除：alter table student_p partition (part=’aa’) enable no_drop; 2、 防止分区被查询：alter table student_p partition (part=’aa’) enable offline;enable 和 disable 是反向操作 删除表 drop table if exists &lt;table_name&gt;; 清空表会保留表结构 truncate table table_name; truncate table table_name partition(city=&#39;beijing’); 其他辅助命令 a. 查看数据库列表 show databases; show databases like &#39;my*&#39;; b. 查看数据表 show tables; show tables in db_name; c. 查看数据表的建表语句 show create table table_name; d. 查看 hive 函数列表 show functions; e. 查看 hive 表的分区 show partitions table_name; show partitions table_name partition(city=&#39;beijing&#39;) f. 查看表的详细信息（元数据信息） desc table_name; desc extended table_name; g. 查看数据库的详细属性信息 desc formatted table_name; desc database db_name; desc database extended db_name; h. 清空数据表 truncate table table_name; Hive – DML装载数据 LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE table_name [PARTITION (partcol1=val1, partcol2=val2 ...)] 注意： LOAD 操作只是单纯的 复制（本地文件）或者 移动（hdfs文件，一般是公共数据，需要建立外部表）操作，将数据文件移动到 Hive 表对应的位置 如果指定了 LOCAL 就去本地文件系统中查找，否则按 inpath 中的 uri 在 hdfs 上查找 inpath 子句中的文件路径下，不能再有文件夹 如果使用了 OVERWRITE 关键字，则目标表（或者分区）中的内容会被删除，然后再将 filepath 指向的文件/目录中的内容添加到表/分区中。 如果目标表（分区）已经有一个文件，并且文件名和 filepath 中的文件名冲突，那么现有的文件会被新文件所替代。???不是自动重命名为xxx_copy_1 插入数据 a. 单条插入 INSERT INTO TABLE table_name VALUES(value1, value2, ...); b. 单重插入 INSERT INTO TABLE table_name [PARTITION (partcol1=val1, ...)] &lt;select_statement1 FROM from_statement&gt; c. 多重插入 FROM from_statement从基表中按不同的字段查询得到的结果分别插入不同的 hive 表只会扫描一次基表，提高查询性能 INSERT INTO TABLE table_name1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 [WHERE where_statement] INSERT INTO TABLE table_name2 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement2 [WHERE where_statement] [ … ]; d. 分区插入 分区插入有两种：一种是静态分区，另一种是动态分区。 如果混合使用静态分区和动态分区， 则静态分区必须出现在动态分区之前。 静态分区 A)、创建静态分区表 B)、从查询结果中导入数据（单重插入）记得加载数据前要添加分区，然后指定要加载到那个分区 C)、查看插入结果 动态分区 静态分区添加数据前需要指定分区，当分区个数不确定的时候就很不方便了，这个时候可以使用动态分区 重要且常用，尤其是按照日期分区时！ A)、创建分区表 B)、参数设置 hive-1.2版本 - set hive.exec.dynamic.partition=true; //动态分区开启状态，默认开启 - set hive.exec.dynamic.partition.mode=nonstrict; //动态分区执行模式，默认&quot;strict&quot;，在这种模式 下要求至少有一列分区字段是静态的，这有助于阻止因设计错误导致查询产生大量的分区 - \# 可选设置项 如果这些参数被更改了又想还原，则执行一次 reset 命令即可 - set hive.exec.max.dynamic.partitions.pernode=100; //每个节点生成动态分区最大个数 - set hive.exec.max.dynamic.partitions=1000; //生成动态分区最大个数，如果自动分区数大于这个参数，将会报错 - set hive.exec.max.created.files=100000; //一个任务最多可以创建的文件数目 - set dfs.datanode.max.xcievers=4096; //限定一次最多打开的文件数 set - hive.error.on.empty.partition=false; //表示当有空分区产生时，是否抛出异常 - C)、动态数据插入 - 单个分区字段 - insert into table test2 partition (age) select name,address,school,age from students; - 多个分区字段 多重分区中目录结构是按照分区字段顺序进行划分的 - insert into table student_ptn2 partition(department, age) select id, name, sex, department,age from students; 分区字段都是动态的 - insert into table student_ptn2 partition(city=&apos;sa&apos;, zipcode) select id, name, sex, age, department, department as zipcode from students; 第一个分区字段时静态的，第二字department字段动态的，重命名为zipcode？ - [注意：] - 查询语句 select 查询出来的动态分区 age 和 zipcode 必须放在 最后，和分区字段对应，不然结果会出错 - D)、查看插入结果 - select * from student_ptn2 where city=&apos;sa&apos; and zipcode=&apos;MA&apos;; e. 分桶插入 A)、创建分桶表 B)、从查询结果中导入数据只能使用insert方式 C)、查看插入结果 # 几个命令 set hive.exec.reducers.bytes.per.reducer= // 设置每个reducer的吞吐量，单位byte，默认256M set hive.exec.reducers.max= //reduceTask最多执行个数，默认1009 set mapreduce.job.reduces= //设置reducetask实际运行数，默认-1，代表没有设置，即reducetask默认数为1 set hive.exec.mode.local.auto=true //设置hive本地模式 导出数据（了解） 单模式导出 INSERT OVERWRITE [LOCAL] DIRECTORY directory1 &lt;select_statement&gt;; 多模式导出 FROM from_statement INSERT OVERWRITE [LOCAL] DIRECTORY directory1 &lt;select_statement1&gt; [INSERT OVERWRITE [LOCAL] DIRECTORY directory2 &lt;select_statement2&gt;] ... 查询数据 Hive 中的 SELECT 基础语法和标准 SQL 语法基本一致，支持 WHERE、DISTINCT、GROUP BY、 ORDER BY、HAVING、LIMIT、子查询等； 1、select * from db.table1虽然可以，但是要尽量避免 select * 这样的全表扫描操作，效率太低又费时 2、select count(distinct uid) from db.table1 3、支持 select、union all、join（left、right、full join）、like、where、having、各种聚合函数、 支持 json 解析 4、UDF/ UDAF/UDTF UDF：User Defined Function，自定义函数，一对一 UDAF：User Defined Aggregate Function，自定义聚合函数，多对一，如sum()，count() UDTF ：User Defined Table Function，自定义表函数，一对多，如explode() 5、不支持 update 和 delete 6、hive 虽然支持 in/exists（老版本是不支持的），但是 hive 推荐使用 semi join 的方式来代替 实现，而且效率更高。 半连接 左半连接：left semi join，以左表为基表，右表有的，只显示左表相应记录（即一半） 右半连接：right semi join，与左半连接相反 内连接：inner join，两表中都有的才会连接 外连接 左外连接：left outer join，坐标为基表，右表有的会关联，右表没有的以null表示；左表没有右表有的不会关联 右外连接：right outer join，与左外连接相反 全外连接：full outer join，两表合并 7、支持 case … when … 语法结构 SELECT [ALL | DISTINCT] select_ condition, select_ condition, ... FROM table_name a [JOIN table_other b ON a.id = b.id]表连接 [WHERE where_condition]过滤条件 [GROUP BY col_list [HAVING condition]]分组条件 [CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY col_list | ORDER BY col_list] [DESC]]排序条件：\1. order by：全局排序，默认升序，DESC表示降序。只有一个 reduce task 的结果，比如文件名是 000000_0，会导致当输入规模较大时，需要较长的计算时间。\2. sort by：局部排序，其在数据进入 reducer 前完成排序。因此，如果用 sort by 进行排序，并且设置 mapred.reduce.tasks &gt; 1，则 sort by 只保证每个 reducer 的输出有序，不保证全局有序。\3. distribute by：类似于分桶，根据指定的字段将数据分到不同的 reducer，且分发算法是 hash 散列\4. cluster by：除了具有 Distribute by 的功能外，还会对该字段进行排序。注意：如果 distribute 和 sort 字段是同一个时，cluster by = distribute by + sort by；如果分桶字段和排序字段不一样，那么就不能使用 clustered by [LIMIT number]显示结果的前几个记录]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[偶然发现的好歌]]></title>
    <url>%2F2018%2F06%2F11%2FSongs%2F%E5%81%B6%E7%84%B6%E5%8F%91%E7%8E%B0%E7%9A%84%E5%A5%BD%E6%AD%8C%2F</url>
    <content type="text"><![CDATA[Dealbreaker 专辑：Chesapeake 歌手：Rachael Yamagata 链接：http://music.163.com/#/m/song?id=18733198 You Won’t Let Me 专辑：Chesapeake 歌手：Rachael Yamagata 链接：http://music.163.com/#/song?id=18733192]]></content>
      <categories>
        <category>文艺</category>
        <category>Songs</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>文艺</tag>
        <tag>Songs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[_HDFS应用场景&原理&基本架构及使用方法]]></title>
    <url>%2F2018%2F06%2F11%2FHadoop%2F1-HDFS%2FHDFS%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%26%E5%8E%9F%E7%90%86%26%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84%E5%8F%8A%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[HDFS基本架构和原理HDFS设计思想 HDFS架构 HDFS数据块（block） 注意： Hadoop2.x，block默认大小是128MB HDFS写流程 创建Distributed FileSystem类 询问 NameNode 要写的文件对否存在 不存在就写入到 FSDataOutputStream 流中 流写出去到一个 DataNode … HDFS读流程 客户端向 NameNode 询问 block 的位置 按照客户端按照拿到的位置，向不同的DataNode 请求数据 …… HDFS典型物理拓扑 HDFS副本放置策略 HDFS可靠性策略 HDFS不适合存储小文件 HDFS程序设计HDFS访问方式 HDFS Shell命令概览 HDFS Shell命令—文件操作命令 HDFS Shell命令—文件操作命令 HDFS Shell命令—管理命令 HDFS Shell命令—管理脚本 HDFS Shell命令—文件管理命令fsck 查看帮助 用法示例 HDFS Shell命令—数据均衡器balancer 一般设置10% —— 15% 就差不多了 HDFS Shell命令—设置目录份额 ※ HDFS Shell命令—增加/移除节点 ※ HDFS JavaAPI介绍 HDFS Java程序举例 HDFS 多语言API—借助thrift也是Apache的顶级项目 thrift执行流程 hadoopfs.thrift接口定义 PHP语言访问HDFS Python语言访问HDFS Hadoop 2.0新特性 HA(高可用)与Federation(联邦) 异构层级存储结构背景 原理 HDFS ACL背景：现有权限管理的局限性 基于POSIX ACL的实现 HDFS快照背景 基本使用方法 HDFS缓存背景 原理 实现情况 总结]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于hexo的时序图插件 hexo-filter-sequence 的巨坑]]></title>
    <url>%2F2018%2F06%2F10%2FTools%2FHexo%2F%E5%85%B3%E4%BA%8Ehexo%E7%9A%84%E6%97%B6%E5%BA%8F%E5%9B%BE%E6%8F%92%E4%BB%B6-hexo-filter-sequence-%E7%9A%84%E5%B7%A8%E5%9D%91%2F</url>
    <content type="text"><![CDATA[前言在写代码的过程中，发现时序图还是挺管用的，简洁明了，于是想用一用。 结果发到站上，不显示。 在网上查了下，发现是hexo所使用的Markdown渲染语法不支持。 这里吐槽下，这里渲染的确实烂，作者为啥不改改.. 于是开始找解决方案，发现大多数都推荐了一个叫hexo-filter-sequence的插件，故安装之。 结果死活还是不行。 装了其它的几个flow图，却可以显示。 当flow图和sequence图同时存在的时候，sequence可以显示，但是sequence单独存在时，死活不显示。 难不成有啥依赖？必须先使用flow才能使用sequence？逻辑上不应该啊！ 但是事实却是这样！ 网上有一篇大神说要改源码才行，开始没注意，觉得应该不会吧，这么多人使用，源码还会有问题？？ 仔细一看源码，妈的！！心里千万匹草泥马呼啸而过！ 把初始化 sequence，写成了初始化 flow！！！ 把 flow 改成 sequence， 再把 js CDN源换成国内的！ 可以了！！ 再仔细一看，发现最后一次更新是在1年前！ 坑爹的作者，浪费了我至少3-5个小时！！ 下面为部分摘抄安装hexo-filter-sequence 插件: 1npm install --save hexo-filter-sequence 配置站点配置文件 _config.yml 中增加如下配置: 123456789sequence: webfont: https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js raphael: https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js underscore: https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js sequence: https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js css: # optional, the url for css, such as hand drawn theme options: theme: simple css_class: 源码源码修改后才能正常使用，进入插件目录作如下修改： 12345678910111213141516// index.jsvar assign = require('deep-assign');var renderer = require('./lib/renderer');hexo.config.sequence = assign(&#123; webfont: 'https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js', raphael: 'https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js', underscore: 'https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js', sequence: 'https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js', css: '', options: &#123; theme: 'simple' &#125;&#125;, hexo.config.sequence);hexo.extend.filter.register('before_post_render', renderer.render, 9); 12345678910// lib/renderer.js, 25 行if (sequences.length) &#123; var config = this.config.sequence; // resources data.content += '&lt;script src="' + config.webfont + '"&gt;&lt;/script&gt;'; data.content += '&lt;script src="' + config.raphael + '"&gt;&lt;/script&gt;'; data.content += '&lt;script src="' + config.underscore + '"&gt;&lt;/script&gt;'; data.content += '&lt;script src="' + config.sequence + '"&gt;&lt;/script&gt;'; ......&#125; 示例新建代码块，增加如下内容： 详情参考 Alice->Bob: Hello Bob, how are you? Note right of Bob: Bob thinks Bob-->Alice: I am good thanks!{"theme":"simple"} var code = document.getElementById("sequence-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value)); var diagram = Diagram.parse(code); diagram.drawSVG("sequence-0", options);]]></content>
      <categories>
        <category>工具</category>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>工具</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce-分组浅探]]></title>
    <url>%2F2018%2F06%2F10%2FHadoop%2F2-MapReduce%2FMapReduce-%E5%88%86%E7%BB%84%E6%B5%85%E6%8E%A2%2F</url>
    <content type="text"><![CDATA[前言最近学分组的时候，一直弄不明白，总感觉一头雾水，通过近3个小时的断点调试和log输出。 大概了解了一些表象，先记录。 案例是这个 求出每门课程参考学生成绩最高平均分的学生的信息： 课程，姓名和平均分，详细见MapReduce笔记-练习第二题第3小题 数据格式是这样的： 第一个是课程名称，总共四个课程，computer，math，english，algorithm， 第二个是学生姓名，后面是每次考试的分数 math,huangxiaoming,85,75,85,99,66,88,75,91 english,huanglei,85,75,85,99,66,88,75,91 … 结论 执行流程结论 map每读一行就 write 到 context 一次，按照指定的key进行分发 map 把所有的数据都读完了之后，大概执行到67%的时候，开始进入 CustomBean，执行CustomBean的compareTo()方法，会按照自己写的规则一条一条数据比较 上述都比较完毕之后，map阶段就结束了，此时来到了 reduce阶段，但是是到了67%了 到了reduce阶段，直接进入了MyGroup中自定义的compare方法。 MyGroup的compare()方法，如果返回非0， 就会进入 reduce 方法写出到context MyGroup进入Reduce的条件是 MyReduce中，如果compare的结果不等于0，也就是比较的2者不相同， 此时就进入Reduce， 写出到上下文 如果相同，会一直往下读，直到读到不同的， 此时写出读到上下文。 因为MyGroup会在Reduce阶段执行，而CustomBean中的compareTo()是在map阶段执行，所以需要在CustomBean中就把组排好序，此时分组功能才能正常运作 指定分组类MyGroup和不指定的区别指定与不指定是指：在Driver类中，是否加上job.setGroupingComparatorClass(MyGrouper.class);这一句。 指定分组类： 会按照分组类中，自定义的compare()方法比较，相同的为一组，分完一组就进入一次reduce方法 不指定分组类：（目前存疑） 是否是按照key进行分组 如果是自定义类为key，是否是按照此key中值相同的分为一组 如果是hadoop内置类，是否是按照此类的值分组（Text-String的值，IntWritable-int值等..） 依然是走得以上这套分组逻辑，一组的数据读完才进入到Reduce阶段做归并 Log信息CustomBean中没有进行分组, 组内排序的log123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129// ==================MyGroup中compare()方法=======================computer---MyGroup中比较---computercomputer---MyGroup中比较---mathmath---MyGroup中比较---englishenglish---MyGroup中比较---mathmath---MyGroup中比较---algorithmalgorithm---MyGroup中比较---algorithmalgorithm---MyGroup中比较---computercomputer---MyGroup中比较---englishenglish---MyGroup中比较---algorithmalgorithm---MyGroup中比较---mathmath---MyGroup中比较---algorithmalgorithm---MyGroup中比较---mathmath---MyGroup中比较---computercomputer---MyGroup中比较---englishenglish---MyGroup中比较---mathmath---MyGroup中比较---computercomputer---MyGroup中比较---mathmath---MyGroup中比较---englishenglish---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---englishenglish---MyGroup中比较---computercomputer---MyGroup中比较---algorithmalgorithm---MyGroup中比较---englishenglish---MyGroup中比较---computercomputer---MyGroup中比较---englishenglish---MyGroup中比较---englishenglish---MyGroup中比较---mathmath---MyGroup中比较---algorithmalgorithm---MyGroup中比较---computercomputer---MyGroup中比较---english// ======================reduce中的执行log============================================第1次进入reducecomputer huangjiaju 83.2---------in for write------computer liutao 83.0---------in for write------==================第2次进入reducemath huangxiaoming 83.0---------in for write------==================第3次进入reduceenglish huanglei 83.0---------in for write------==================第4次进入reducemath huangjiaju 82.28571428571429---------in for write------==================第5次进入reducealgorithm huangjiaju 82.28571428571429---------in for write------algorithm liutao 82.0---------in for write------==================第6次进入reducecomputer huanglei 74.42857142857143---------in for write------==================第7次进入reduceenglish liuyifei 74.42857142857143---------in for write------==================第8次进入reducealgorithm huanglei 74.42857142857143---------in for write------==================第9次进入reducemath huanglei 74.42857142857143---------in for write------==================第10次进入reducealgorithm huangzitao 72.75---------in for write------==================第11次进入reducemath liujialing 72.75---------in for write------==================第12次进入reducecomputer huangzitao 72.42857142857143---------in for write------==================第13次进入reduceenglish huangxiaoming 72.42857142857143---------in for write------==================第14次进入reducemath wangbaoqiang 72.42857142857143---------in for write------==================第15次进入reducecomputer huangxiaoming 72.42857142857143---------in for write------==================第16次进入reducemath xuzheng 69.28571428571429---------in for write------==================第17次进入reduceenglish zhaobenshan 69.28571428571429---------in for write------==================第18次进入reducecomputer huangbo 65.25---------in for write------computer xuzheng 65.0---------in for write------==================第19次进入reduceenglish zhouqi 64.18181818181819---------in for write------==================第20次进入reducecomputer liujialing 64.11111111111111---------in for write------==================第21次进入reducealgorithm liuyifei 62.142857142857146---------in for write------==================第22次进入reduceenglish liujialing 62.142857142857146---------in for write------==================第23次进入reducecomputer liuyifei 62.142857142857146---------in for write------==================第24次进入reduceenglish liuyifei 59.57142857142857---------in for write------english huangdatou 56.0---------in for write------==================第25次进入reducemath liutao 56.0---------in for write------==================第26次进入reducealgorithm huangdatou 56.0---------in for write------==================第27次进入reducecomputer huangdatou 56.0---------in for write------==================第28次进入reduceenglish huangbo 55.0---------in for write------ CustomBean中做了分组&amp;组内排序 的123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120// **Bean中相同的组进行分数降序， 组进行字典排序，此时MyGroup中执行的**algorithm---MyGroup中比较---algorithmalgorithm---MyGroup中比较---algorithmalgorithm---MyGroup中比较---algorithmalgorithm---MyGroup中比较---algorithmalgorithm---MyGroup中比较---algorithmalgorithm---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---englishenglish---MyGroup中比较---englishenglish---MyGroup中比较---englishenglish---MyGroup中比较---englishenglish---MyGroup中比较---englishenglish---MyGroup中比较---englishenglish---MyGroup中比较---englishenglish---MyGroup中比较---englishenglish---MyGroup中比较---englishenglish---MyGroup中比较---mathmath---MyGroup中比较---mathmath---MyGroup中比较---mathmath---MyGroup中比较---mathmath---MyGroup中比较---mathmath---MyGroup中比较---mathmath---MyGroup中比较---math// ======================reduce中执行============================================第1次进入reducealgorithm huangjiaju 82.28571428571429---------in for write------algorithm liutao 82.0---------in for write------algorithm huanglei 74.42857142857143---------in for write------algorithm huangzitao 72.75---------in for write------algorithm liuyifei 62.142857142857146---------in for write------algorithm huangdatou 56.0---------in for write------===================离开reduce==================第2次进入reducecomputer huangjiaju 83.2---------in for write------computer liutao 83.0---------in for write------computer huanglei 74.42857142857143---------in for write------computer huangzitao 72.42857142857143---------in for write------computer huangxiaoming 72.42857142857143---------in for write------computer huangbo 65.25---------in for write------computer xuzheng 65.0---------in for write------computer liujialing 64.11111111111111---------in for write------computer liuyifei 62.142857142857146---------in for write------computer huangdatou 56.0---------in for write------===================离开reduce==================第3次进入reduceenglish huanglei 83.0---------in for write------english liuyifei 74.42857142857143---------in for write------english huangxiaoming 72.42857142857143---------in for write------english zhaobenshan 69.28571428571429---------in for write------english zhouqi 64.18181818181819---------in for write------english liujialing 62.142857142857146---------in for write------english liuyifei 59.57142857142857---------in for write------english huangdatou 56.0---------in for write------english huangbo 55.0---------in for write------===================离开reduce==================第4次进入reducemath huangxiaoming 83.0---------in for write------math huangjiaju 82.28571428571429---------in for write------math huanglei 74.42857142857143---------in for write------math liujialing 72.75---------in for write------math wangbaoqiang 72.42857142857143---------in for write------math xuzheng 69.28571428571429---------in for write------math liutao 56.0---------in for write------===================离开reduce//  如果只取一个每次values的第一个的话 algorithm huangjiaju 82.28571428571429==================第1次进入reducecomputer huangjiaju 83.2==================第2次进入reduceenglish huanglei 83.0==================第3次进入reducemath huangxiaoming 83.0==================第4次进入reduce 其它疑点 通过log可以看出来， MyGroup是在读到了不同的数据，才会进入到 reduce 类中写出； 但是 通过 断点调试时， 现象是，第一次读到了2个相同的，就去reduce去写出了； 后面再读的就不做写出操作，直到下一次读到2个相同的，再在reduce中写出。 title: 执行流程时序图  Mapper(map)->ScoreBean: k:ScoreBean(courseName,avgScore) Mapper(map)->ScoreBean: v:Text(stuName) Mapper(ScoreBean)->Reducer(MyGroup): course按字典升序 Mapper(ScoreBean)->Reducer(MyGroup): course内成绩降序 Reducer(MyGroup)->Reducer(reduce): 根据自定义的分组规则按组输出 Reducer(MyGroup)->Reducer(reduce): 一组只调用reduce一次 Reducer(reduce)-->Reducer(reduce):{"theme":"simple"} var code = document.getElementById("sequence-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value)); var diagram = Diagram.parse(code); diagram.drawSVG("sequence-0", options);]]></content>
      <categories>
        <category>Hadoop</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[▍开始]]></title>
    <url>%2F2018%2F06%2F10%2FPoetry%2F%E5%BC%80%E5%A7%8B%2F</url>
    <content type="text"><![CDATA[月亮落下一两片羽毛在田野上。 黑暗中的麦子聆听着。 快静下来。 快。 就在那儿，月亮的孩子们正试着 挥动翅膀。 在两棵树之间，身材修长的女子抬起面庞， 美丽的剪影。接着，她步入空中，接着， 她完全消失在空中。 我独自站在一棵接骨木旁，不敢呼吸， 也不敢动。 我聆听着。 麦子向后靠着自己的黑暗， 而我靠着我的。 作者 / [美国] 詹姆斯·赖特 翻译 / 张文武 ▍Beginning The moon drops one or two feathers into the fields. The dark wheat listens. Be still. Now. There they are, the moon’s young, trying Their wings. Between trees, a slender woman lifts up the lovely shadow Of her face, and now she steps into the air, now she is gone Wholly, into the air. I stand alone by an elder tree, I do not dare breathe Or move. I listen. The wheat leans back toward its own darkness, And I lean toward mine. Author / James Wright]]></content>
      <categories>
        <category>文艺</category>
        <category>诗歌</category>
      </categories>
      <tags>
        <tag>诗歌</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown高阶语法]]></title>
    <url>%2F2018%2F06%2F10%2FTools%2FMarkdown%2FMarkdown%E9%AB%98%E9%98%B6%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[时序图的写法 流程图的写法 类图的写法 st=>start: Start|past:>http://www.google.com[blank] e=>end: End:>http://www.google.com op1=>operation: My Operation|past op2=>operation: Stuff|current sub1=>subroutine: My Subroutine|invalid cond=>condition: Yes or No?|approved:>http://www.google.com c2=>condition: Good idea|rejected io=>inputoutput: catch something...|request st->op1(right)->cond cond(yes, right)->c2 cond(no)->sub1(left)->op1 c2(yes)->io->e c2(no)->op2->e{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-0", options);mapper->Bob: Hello Bob, how are you? Note right of Bob: Bob thinks Bob-->reducer: I am good thanks!ad u? reducer->out: I'm fine too out->me: ok, you win me-->Bob: nono, not{"theme":"simple"} var code = document.getElementById("sequence-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value)); var diagram = Diagram.parse(code); diagram.drawSVG("sequence-0", options);]]></content>
      <categories>
        <category>工具</category>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>工具</tag>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[About Sublime Text3]]></title>
    <url>%2F2018%2F06%2F09%2FTools%2FSublime%2F%E5%AE%89%E8%A3%85%E4%B8%BB%E9%A2%98%2F</url>
    <content type="text"><![CDATA[主题详情参见这个网站 详细操作见此站]]></content>
      <categories>
        <category>工具</category>
        <category>Sublime</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>工具</tag>
        <tag>Sublime</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce笔记-Bug汇总]]></title>
    <url>%2F2018%2F06%2F09%2FHadoop%2F2-MapReduce%2FMapReduce-Bug%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[1、reduce 输出路径必须是新创建的。不能已经存在1Exception in thread "main" org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://cs1:9000/flowout01 already exists 2、在初始化 job 的时候， 没有传 conf ， 导致后面一直找不到文件， 因为不知道到哪里去找3、Text导包倒错, 导的时候要注意应该是这个 import org.apache.hadoop.io.Text; 4、进行字符串拼接的时候，把 StringBuilder 写到了 reduce 方法外， 这样导致 sb 会越来越多，当然，也可以每次拼接完了清空类似于这样 1234567891011121314A F,I,O,K,G,D,C,H,BB F,I,O,K,G,D,C,H,B,E,J,F,AC F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,FD F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,LE F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,HF F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,GG F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,MH F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,OI F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,CJ F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,OK F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,BL F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,B,D,EM F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,B,D,E,E,FO F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,B,D,E,E,F,A,H,I,J,F 4、mapreduce执行错误Mapper.\错误 Mapper &amp; Reducer 写成内部类的时候，有没有加上 static Bean类有没有无参构造 5、排序过程中，自定义了排序类，bean类的 compareTo()方法，只写了按照分数大小排序。会出现如下错误： 课程并没有分组 没有在相同的一组课程中比较分数， 而是比较的所有的分数 12345678computer huangjiaju 83.2math huangxiaoming 83.0english huanglei 83.0math huangjiaju 82.28571428571429algorithm huangjiaju 82.28571428571429computer huanglei 74.42857142857143english liuyifei 74.42857142857143... 此时应该在Bean对象中做如下事情 相同课程的按照分数降序排序 课程名按照自然（升序）排序 换言之，就是CustomBean 对象要输出的数据是 组名升序排序，组内按成绩降序排序 具体分析参阅]]></content>
      <categories>
        <category>Hadoop</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce笔记-练习]]></title>
    <url>%2F2018%2F06%2F09%2FHadoop%2F2-MapReduce%2FMapReduce-%E7%BB%83%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[求微博共同粉丝题目涉及知识点： 多 Job 串联 1234567891011121314A:B,C,D,F,E,OB:A,C,E,KC:F,A,D,ID:A,E,F,LE:B,C,D,M,LF:A,B,C,D,E,O,MG:A,C,D,E,FH:A,C,D,E,OI:A,OJ:B,OK:A,C,DL:D,E,FM:E,F,GO:A,H,I,J,K 以上是数据：A:B,C,D,F,E,O表示：A用户 关注B,C,D,E,F,O 求所有两两用户之间的共同关注对象 答案： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299package com.rox.mapreduce.mr3._01_多Job串联;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob;import org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class CommonFansDemo &#123; @SuppressWarnings("deprecation") public static void main(String[] args) throws Exception &#123; // Job 逻辑 // 指定 HDFS 相关的参数 Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://cs1:9000"); System.setProperty("HADOOP_USER_NAME", "ap"); // // 新建一个 job1 Job job1 = Job.getInstance(conf); // 设置 Jar 包所在路径 job1.setJarByClass(CommonFansDemo.class); // 指定 mapper 类和 reducer 类 job1.setMapperClass(MyMapper_Step1.class); job1.setReducerClass(MyReducer_Step1.class); // 指定 maptask 的输出类型 job1.setMapOutputKeyClass(Text.class); job1.setMapOutputValueClass(Text.class); // 指定最终的输出类型(reduce存在时,就是指 ReduceTask 的输出类型) job1.setOutputKeyClass(Text.class); job1.setOutputValueClass(Text.class); // 指定该 MapReduce 程序数据的输入输出路径 FileInputFormat.setInputPaths(job1, new Path("/in/commonfriend")); FileOutputFormat.setOutputPath(job1, new Path("/out/job1")); // // 新建一个 job2 Job job2 = Job.getInstance(conf); // 设置 Jar 包所在路径 job2.setJarByClass(CommonFansDemo.class); // 指定 mapper 类和 reducer 类 job2.setMapperClass(MyMapper_Step2.class); job2.setReducerClass(MyReducer_Step2.class); // 指定 maptask 的输出类型 job2.setMapOutputKeyClass(Text.class); job2.setMapOutputValueClass(Text.class); // 指定最终的输出类型(reduce存在时,就是指 ReduceTask 的输出类型) job2.setOutputKeyClass(Text.class); job2.setOutputValueClass(Text.class); // 指定该 MapReduce 程序数据的输入输出路径 FileInputFormat.setInputPaths(job2, new Path("/out/job1")); FileOutputFormat.setOutputPath(job2, new Path("/out/job2")); // /** * 将多个 job 当做一个组中的 job 提交, 参数名是组名 * 注意: JobControl 是实现了 Runnable 接口的 */ JobControl jControl = new JobControl("common_friend"); // 将原生的 job携带配置 转换为可控的 job ControlledJob aJob = new ControlledJob(job1.getConfiguration()); ControlledJob bJob = new ControlledJob(job2.getConfiguration()); // 添加依赖关系 bJob.addDependingJob(aJob); // 添加 job 到组中 jControl.addJob(aJob); jControl.addJob(bJob); // 启动一个线程 Thread jobThread = new Thread(jControl); jobThread.start(); while (!jControl.allFinished()) &#123; Thread.sleep(500); &#125; jobThread.stop(); &#125; static class MyMapper_Step1 extends Mapper&lt;LongWritable, Text, Text, Text&gt; &#123; String[] user_attentions; String[] attentions; Text k = new Text(); Text v = new Text(); @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; user_attentions = value.toString().split(":"); attentions = user_attentions[1].trim().split(","); for (String att : attentions) &#123; k.set(att); v.set(user_attentions[0].trim()); context.write(k, v); &#125; &#125; &#125; /** * @author shixuanji * 将两两粉丝(普通用户)拼接起来, 格式a-f:c =&gt; a,b 都共同关注了 c * * A F,I,O,K,G,D,C,H,B B E,J,F,A C B,E,K,A,H,G,F D H,C,G,F,E,A,K,L E A,B,L,G,M,F,D,H F C,M,L,A,D,G */ static class MyMapper_Step2 extends Mapper&lt;LongWritable, Text, Text, Text&gt; &#123; String[] attenion_users; String[] users; Text k = new Text(); Text v = new Text(); @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; attenion_users = value.toString().split("\t"); users = attenion_users[1].trim().split(","); for (String u1 : users) &#123; for (String u2 : users) &#123; if (u1.compareTo(u2) &lt; 0) &#123; String users = u1 + "-" + u2; k.set(users); v.set(attenion_users[0].trim()); context.write(k, v); &#125; &#125; &#125; &#125; &#125; /** * @author shixuanji * 需要统计的是, 某人拥有的全部粉丝 * key: 传过来的 key * value: 用,分割 */ static class MyReducer_Step1 extends Reducer&lt;Text, Text, Text, Text&gt; &#123; Text k = new Text(); Text v = new Text(); @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Reducer&lt;Text, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; // 注意: 这里 sb 不能写在外面,会不断的拼接 StringBuilder sb = new StringBuilder(); for (Text v : values) &#123; sb.append(v.toString()).append(","); &#125; k.set(key); v.set(sb.substring(0, sb.length() - 1)); context.write(k, v); &#125; &#125; /** * @author shixuanji * 拿到的数据: a-b c */ static class MyReducer_Step2 extends Reducer&lt;Text, Text, Text, Text&gt; &#123; Text k = new Text(); Text v = new Text(); @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Reducer&lt;Text, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; StringBuilder sb = new StringBuilder(); for (Text attention : values) &#123; sb.append(attention.toString()).append(","); &#125; k.set(key); v.set(sb.substring(0, sb.length() - 1)); context.write(k, v); &#125; &#125;&#125;// job1的输出A F,I,O,K,G,D,C,H,BB E,J,F,AC B,E,K,A,H,G,FD H,C,G,F,E,A,K,LE A,B,L,G,M,F,D,HF C,M,L,A,D,GG MH OI O,CJ OK O,BL D,EM E,FO A,H,I,J,F// job2的输出A-B E,CA-C D,FA-D F,EA-E C,D,BA-F O,B,E,D,CA-G E,F,D,CA-H O,E,D,CA-I OA-J B,OA-K D,CA-L D,F,EA-M E,FB-C AB-D E,AB-E CB-F A,E,CB-G C,A,EB-H A,E,CB-I AB-K C,AB-L EB-M EB-O A,KC-D A,FC-E DC-F D,AC-G F,A,DC-H D,AC-I AC-K A,DC-L F,DC-M FC-O I,AD-E LD-F E,AD-G A,F,ED-H E,AD-I AD-K AD-L F,ED-M F,ED-O AE-F C,B,M,DE-G C,DE-H C,DE-J BE-K D,CE-L DF-G A,D,C,EF-H A,E,C,D,OF-I O,AF-J O,BF-K C,A,DF-L E,DF-M EF-O AG-H A,C,D,EG-I AG-K C,A,DG-L D,E,FG-M F,EG-O AH-I O,AH-J OH-K A,D,CH-L E,DH-M EH-O AI-J OI-K AI-O AK-L DK-O AL-M F,E 求学生成绩题目1234567891011121314151617181920212223242526272829303132computer,huangxiaoming,85,86,41,75,93,42,85computer,xuzheng,54,52,86,91,42computer,huangbo,85,42,96,38english,zhaobenshan,54,52,86,91,42,85,75english,liuyifei,85,41,75,21,85,96,14algorithm,liuyifei,75,85,62,48,54,96,15computer,huangjiaju,85,75,86,85,85english,liuyifei,76,95,86,74,68,74,48english,huangdatou,48,58,67,86,15,33,85algorithm,huanglei,76,95,86,74,68,74,48algorithm,huangjiaju,85,75,86,85,85,74,86computer,huangdatou,48,58,67,86,15,33,85english,zhouqi,85,86,41,75,93,42,85,75,55,47,22english,huangbo,85,42,96,38,55,47,22algorithm,liutao,85,75,85,99,66computer,huangzitao,85,86,41,75,93,42,85math,wangbaoqiang,85,86,41,75,93,42,85computer,liujialing,85,41,75,21,85,96,14,74,86computer,liuyifei,75,85,62,48,54,96,15computer,liutao,85,75,85,99,66,88,75,91computer,huanglei,76,95,86,74,68,74,48english,liujialing,75,85,62,48,54,96,15math,huanglei,76,95,86,74,68,74,48math,huangjiaju,85,75,86,85,85,74,86math,liutao,48,58,67,86,15,33,85english,huanglei,85,75,85,99,66,88,75,91math,xuzheng,54,52,86,91,42,85,75math,huangxiaoming,85,75,85,99,66,88,75,91math,liujialing,85,86,41,75,93,42,85,75english,huangxiaoming,85,86,41,75,93,42,85algorithm,huangdatou,48,58,67,86,15,33,85algorithm,huangzitao,85,86,41,75,93,42,85,75 一、数据解释 数据字段个数不固定：第一个是课程名称，总共四个课程，computer，math，english，algorithm，第二个是学生姓名，后面是每次考试的分数 二、统计需求：1、统计每门课程的参加考试人数和课程平均分 2、统计每门课程参考学生的平均分，并且按课程存入不同的结果文件，要求一门课程一个结果文件 3、求出每门课程参考学生成绩最高平均分的学生的信息：课程，姓名和平均分 答案第1小题统计每门课程的参考人数和课程平均分 涉及知识点: 去重， 自定义类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177//  ScoreBean package com.rox.mapreduce.mr3._02_分组组件;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.WritableComparable;import lombok.AllArgsConstructor;import lombok.Getter;import lombok.NoArgsConstructor;import lombok.Setter;@Getter@Setter@AllArgsConstructor@NoArgsConstructorpublic class ScoreBean implements WritableComparable&lt;ScoreBean&gt; &#123; private String courseName; private String stuName; private Double score; @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(courseName); out.writeUTF(stuName); out.writeDouble(score); &#125; @Override public void readFields(DataInput in) throws IOException &#123; this.courseName = in.readUTF(); this.stuName = in.readUTF(); this.score = in.readDouble(); &#125; @Override /** * 如果是相同课程, 按照分数降序排列的 * 如果是不同课程, 按照课程名称升序排列 */ public int compareTo(ScoreBean o) &#123; // 测试一下只写按分数降序排序// return o.getScore().compareTo(this.getScore()); /*// 首先分组(只在相同的组内进行比较) int nameRes = this.getCourseName().compareTo(o.getCourseName()); if (nameRes == 0) &#123; // 课程相同的时候才进行降序排序 int scoreRes = return scoreRes; &#125; return nameRes;*/ return 0; &#125; public String toString1() &#123; return stuName + "\t" + score; &#125; @Override public String toString() &#123; return courseName + "\t" + stuName + "\t" + score; &#125; public ScoreBean(String stuName, Double score) &#123; super(); this.stuName = stuName; this.score = score; &#125;&#125;//  ScorePlusDemo1 package com.rox.mapreduce.mr3._02_分组组件;import java.io.IOException;import java.util.HashSet;import java.util.Set;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class ScorePlusDemo1 &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://cs1:9000"); System.setProperty("HADOOP_USER_NAME", "ap"); Job job = Job.getInstance(conf); job.setJarByClass(ScorePlusDemo1.class); job.setMapperClass(MyMapper.class); job.setReducerClass(MyReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(ScoreBean.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); String inP = "/in/newScoreIn"; String outP = "/out/ans1"; FileInputFormat.setInputPaths(job, new Path(inP)); FileOutputFormat.setOutputPath(job, new Path(outP)); Path mypath = new Path(outP); FileSystem hdfs = mypath.getFileSystem(conf); if (hdfs.isDirectory(mypath)) &#123; hdfs.delete(mypath, true); &#125; Boolean waitForComp = job.waitForCompletion(true); System.exit(waitForComp?0:1); &#125; static class MyMapper extends Mapper&lt;LongWritable, Text, Text, ScoreBean&gt; &#123; Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1.截取 String[] datas = value.toString().trim().split(","); String courseName = datas[0].trim(); String stuName = datas[1].trim(); int sum = 0; for (int i=2; i&lt;datas.length; i++) &#123; sum += Integer.parseInt(datas[i]); &#125; double avgScore = sum/(datas.length-2); ScoreBean sb = new ScoreBean(courseName, stuName, avgScore); k.set(courseName); context.write(k, sb); &#125; &#125; static class MyReducer extends Reducer&lt;Text, ScoreBean, Text, Text&gt; &#123; Text v = new Text(); @Override protected void reduce(Text key, Iterable&lt;ScoreBean&gt; values, Reducer&lt;Text, ScoreBean, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; Set&lt;String&gt; stuNames = new HashSet&lt;&gt;(); int count = 0; int sum = 0; for (ScoreBean sb : values) &#123; stuNames.add(sb.getStuName()); count ++; sum += sb.getScore(); &#125; int size = stuNames.size(); String val = size + "\t" + (double)sum/count; v.set(val); context.write(key, v); &#125; &#125;&#125;// 执行结果 algorithm 6 71.33333333333333computer 10 69.6english 8 66.0math 7 72.57142857142857 第2小题统计每门课程参考学生的平均分，并且按课程存入不同的结果文件，要求一门课程一个结果文件 涉及知识点： 分区, 字符串组合key， Partitioner 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207package com.rox.mapreduce.mr3._02_分组组件;import java.io.IOException;import java.util.HashMap;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.DoubleWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Partitioner;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;/** * @author shixuanji * 注意: 此题因为数据中有2条 course 和 stuName相同的数据(english liuyifei), 所以必须再在reduce中继续去重一下, 再计算一下平均分 * * 否则, 可以不用写reduce, 因为Mapper中已经把逻辑处理完了,可以直接输出 * 最终输出: * computer liuyifei 43 * computer huanglei 63 * math liutao 64 * ... */public class ScorePlusDemo2 &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 指定HDFS相关参数 Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://cs1:9000"); System.setProperty("HADOOP_USER_NAME", "ap"); //  创建/配置 Job Job job = Job.getInstance(conf); // 设置Jar包类型 job.setJarByClass(ScorePlusDemo2.class); // 设置Map Reduce执行类 job.setMapperClass(MyMapper.class); job.setReducerClass(MyReducer.class); // 设置Map输出类 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(DoubleWritable.class); // Reduce输出类 job.setOutputKeyClass(Text.class); job.setOutputValueClass(DoubleWritable.class); //  设置分区  job.setPartitionerClass(MyPartition.class); job.setNumReduceTasks(4); // 设置输入 输出路径 String inP = "/in/newScoreIn"; String outP = "/out/scorePlus2"; FileInputFormat.setInputPaths(job, new Path(inP)); FileOutputFormat.setOutputPath(job, new Path(outP)); // 设置如果存在路径就删除 Path mypath = new Path(outP); FileSystem hdfs = mypath.getFileSystem(conf); if (hdfs.isDirectory(mypath)) &#123; hdfs.delete(mypath, true); &#125; //  执行job boolean waitForCompletion = job.waitForCompletion(true); System.exit(waitForCompletion?0:-1); &#125; =============================================================== static class MyMapper extends Mapper&lt;LongWritable, Text, Text, DoubleWritable&gt; &#123; // 把 课程+学生 作为 key Text k = new Text(); //只有输出String类型的, 才需要在这里设置Text DoubleWritable v = new DoubleWritable(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] datas = value.toString().trim().split(","); String kStr = datas[0].trim() + "\t" + datas[1].trim(); int sum = 0; for (int i = 2; i &lt; datas.length; i++) &#123; sum += Integer.parseInt(datas[i]); &#125; double avg = sum / (datas.length - 2); k.set(kStr); v.set(avg); context.write(k, v); &#125; &#125;=============================================================== /** * @author shixuanji * 注意: 此题因为数据中有2条 course 和 stuName相同的数据, 所以必须再在reduce中 * 继续去重一下, 再计算一下平均分 * * 否则, 可以不用写reduce, 因为Mapper中已经把逻辑处理完了,可以直接输出 */ static class MyReducer extends Reducer&lt;Text, DoubleWritable, Text, DoubleWritable&gt; &#123; DoubleWritable v = new DoubleWritable(); @Override protected void reduce(Text key, Iterable&lt;DoubleWritable&gt; values, Reducer&lt;Text, DoubleWritable, Text, DoubleWritable&gt;.Context context) throws IOException, InterruptedException &#123; /** * 考虑到有 课程, 学生名相同, 后面的数据不同的情况, 这里再做一个平均求和 * 可以验证打印下 */ int count = 0; Double sum = 0.0; for (DoubleWritable avg : values) &#123; if (count &gt; 0) &#123; // 有key完全相同的情况才会进到这里 System.out.println("这是第" +count +"次, 说明课程和姓名有相同的两条数据\n课程姓名是: "+key.toString()); &#125; sum += avg.get(); count ++; &#125; Double finAvg = sum/count; v.set(finAvg); context.write(key, v); &#125; &#125;&#125;==============================================================================================================================/** * @author shixuanji * 继承 Partitioner, 实现自定义分区 */class MyPartition extends Partitioner&lt;Text, DoubleWritable&gt; &#123; private static HashMap&lt;String, Integer&gt; courseMap = new HashMap&lt;&gt;(); static &#123; courseMap.put("algorithm", 0); courseMap.put("computer", 1); courseMap.put("english", 2); courseMap.put("math", 3); &#125; @Override public int getPartition(Text key, DoubleWritable value, int numPartitions) &#123; // 取出Map输出的key中的前半部分--courseName Integer code = courseMap.get(key.toString().trim().split("\t")[0]); if (code != null) &#123; return code; &#125; return 5; &#125;&#125;=============================================================== ===============================================================  执行结果 algorithm huangdatou 56.0algorithm huangjiaju 82.0algorithm huanglei 74.0algorithm huangzitao 72.0algorithm liutao 82.0algorithm liuyifei 62.0----------computer huangbo 65.0computer huangdatou 56.0computer huangjiaju 83.0computer huanglei 74.0computer huangxiaoming 72.0computer huangzitao 72.0computer liujialing 64.0computer liutao 83.0computer liuyifei 62.0computer xuzheng 65.0---------english huangbo 55.0english huangdatou 56.0english huanglei 83.0english huangxiaoming 72.0english liujialing 62.0english liuyifei 66.5english zhaobenshan 69.0english zhouqi 64.0------------math huangjiaju 82.0math huanglei 74.0math huangxiaoming 83.0math liujialing 72.0math liutao 56.0math wangbaoqiang 72.0math xuzheng 69.0 第3小题求出 每门课程①参与考试的学生成绩 最高平局分② 的学生的信息：课程，姓名和平均分 解题思路： 通过题意得出2个结论 课程要分组 平均分要排序 排序的话，交给key来做无疑是最好的，因为MapReduce会自动对key进行分组&amp;排序 因此可以把 课程&amp;平均分 作为一个联合key 为了操作方便，可以封装到一个对象中去： ScoreBean 分组和排序需要在 ScoreBean重写的compareTo()方法中完成 因为最后结果是求每门课程的最高平均分，因此需要对课程进行分组。 此时原本的默认分组（以Bean对象整体分组）就不管用了，需要自定义分组 自定义分组要继承WritableComparator，重写compare()方法，指定分组的规则。 ScoreBean先按照组别进行排序，到reduce中时，已经是按照组，排好的数据，MyGroup 会把相同的比较结果放到同一个组中，分发到reduce. reduce中，只需要取出每组的第一个元素输出到上下文即可 图示 涉及知识点： mr中key的作用，自定义对象的用法，自定义分组，mr的执行流程 利用“班级和平均分”作为 key，可以将 map 阶段读取到的所有学生成绩数据按照班级 和成绩排倒序，发送到 reduce 在 reduce 端利用 GroupingComparator 将班级相同的 kv 聚合成组，然后取第一个即是最 大值 先贴个结论：执行流程结论 map每读一行就 write 到 context 一次，按照指定的key进行分发 map 把所有的数据都读完了之后，大概执行到67%的时候，开始进入 CustomBean，执行CustomBean的compareTo()方法，会按照自己写的规则一条一条数据比较 上述都比较完毕之后，map阶段就结束了，此时来到了 reduce阶段，但是是到了67%了 到了reduce阶段，直接进入了MyGroup中自定义的compare方法。 MyGroup的compare()方法，如果返回非0， 就会进入 reduce 方法写出到context MyGroup进入Reduce的条件是 MyReduce中，如果compare的结果不等于0，也就是比较的2者不相同， 此时就进入Reduce， 写出到上下文 如果相同，会一直往下读，直到读到不同的， 此时写出读到上下文。 因为MyGroup会在Reduce阶段执行，而CustomBean中的compareTo()是在map阶段执行，所以需要在CustomBean中就把组排好序，此时分组功能才能正常运作 指定分组类MyGroup和不指定的区别 指定与不指定是指：在Driver类中，是否加上job.setGroupingComparatorClass(MyGrouper.class);这一句。 指定分组类： 会按照分组类中，自定义的compare()方法比较，相同的为一组，分完一组就进入一次reduce方法 不指定分组类：（目前存疑） 是否是按照key进行分组 如果是自定义类为key，是否是按照此key中值相同的分为一组 如果是hadoop内置类，是否是按照此类的值分组（Text-String的值，IntWritable-int值等..） 依然是走得以上这套分组逻辑，一组的数据读完才进入到Reduce阶段做归并 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231// ScoreBean2 package com.rox.mapreduce.mr3._02_分组组件;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.WritableComparable;import lombok.Getter;import lombok.Setter;@Getter@Setterpublic class ScoreBean2 implements WritableComparable&lt;ScoreBean2&gt; &#123; private String courseName; private Double score; @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(courseName); out.writeDouble(score); &#125; @Override public void readFields(DataInput in) throws IOException &#123; this.courseName = in.readUTF(); this.score = in.readDouble(); &#125; @Override /** * 如果是相同课程, 按照分数降序排列的 * 如果是不同课程, 按照课程名称升序排列 */ public int compareTo(ScoreBean2 o) &#123; // 测试一下只写按分数降序排序// return o.getScore().compareTo(this.getScore()); // 首先分组(只在相同的组内进行比较) int nameRes = this.getCourseName().compareTo(o.getCourseName()); if (nameRes == 0) &#123; // 课程相同的时候才进行降序排序 int scoreRes = o.getScore().compareTo(this.getScore()); return scoreRes; &#125; return nameRes; &#125; /** * 实际上ScoreBean中是包含所有的参数的, 这里的输出可以自己设置 */ @Override public String toString() &#123; return courseName + "\t" + score; &#125; public ScoreBean2(String courseName, Double score) &#123; super(); this.courseName = courseName; this.score = score; &#125; public ScoreBean2() &#123; super(); &#125;&#125;// ScorePlusDemo3 package com.rox.mapreduce.mr3._02_分组组件;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class ScorePlusDemo3 &#123;  main  public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://cs1:9000"); System.setProperty("HADOOP_USER_NAME", "ap"); Job job = Job.getInstance(conf); job.setJarByClass(ScorePlusDemo3.class); job.setMapperClass(MyMapper.class); job.setReducerClass(MyReducer.class); job.setMapOutputKeyClass(ScoreBean2.class); job.setMapOutputValueClass(Text.class); job.setGroupingComparatorClass(MyGrouper.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); String outP = "/out/scorePlus3"; FileInputFormat.setInputPaths(job, new Path("/in/newScoreIn")); FileOutputFormat.setOutputPath(job, new Path(outP)); // 如果输出目录存在,就先删除 Path myPath = new Path(outP); FileSystem fs = myPath.getFileSystem(conf); if (fs.isDirectory(myPath)) &#123; fs.delete(myPath, true); &#125; boolean waitForCompletion = job.waitForCompletion(true); System.exit(waitForCompletion ? 0 : -1); &#125; Mapper  /** * @author shixuanji * 输出: key: course * value: score ... * 思路: * 1.不同课程要分开展示, 以 课程+分数 作为key, 在mapper中完成排序 * 2.在reduce中按照 MyGrouper 完成分组 */ static public class MyMapper extends Mapper&lt;LongWritable, Text, ScoreBean2, Text&gt; &#123; private String[] datas; Text v = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; datas = value.toString().trim().split(","); int sum = 0; for (int i = 2; i &lt; datas.length; i++) &#123; sum += Integer.parseInt(datas[i]); &#125; double avg = (double) sum / (datas.length - 2); ScoreBean2 sb = new ScoreBean2(datas[0].trim(), avg); v.set(datas[1].trim()); context.write(sb, v); &#125; &#125; Redecer  static public class MyReducer extends Reducer&lt;ScoreBean2, Text, Text, NullWritable&gt; &#123; Text k = new Text(); int count = 1; @Override protected void reduce(ScoreBean2 key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; /** * 如果没有其它问题 * 此时是按照课程分好组了, 同一个课程的所有学生都过来了, 并且学生成绩是排好的, * 如果此时求最大值, 只需要取出第一个即可 */ // 进来一次只取第一个 Text name = values.iterator().next(); k.set(key.getCourseName() + "\t" + name.toString() + "\t" + key.getScore()); context.write(k, NullWritable.get()); context.write(new Text("==================第"+count+"次进入reduce"), NullWritable.get()); /*context.write(new Text("==================第"+count+"次进入reduce"), NullWritable.get()); for (Text name : values) &#123; k.set(key.getCourseName() + "\t" + name.toString() + "\t" + key.getScore()); context.write(k, NullWritable.get()); context.write(new Text("---------in for write------"), NullWritable.get()); &#125;*/ count++; &#125; &#125;&#125; MyGrouper /** * @author shixuanji * 自定义分组 需要继承一个类WritableComparator * 重写compare方法 */class MyGrouper extends WritableComparator &#123; // WritableComparator 此方法的默认无参构造是不会创建对象的, 需要自己重写 public MyGrouper() &#123; // 中间省去的参数是 Configuration, 如果为空, 会创建一个新的 super(ScoreBean2.class, true); &#125; /** * 此处比较的是2个 WritableComparable 对象, 需要强转一下具体的类对象 */ @SuppressWarnings("rawtypes") @Override public int compare(WritableComparable a, WritableComparable b) &#123; ScoreBean2 aBean = (ScoreBean2) a; ScoreBean2 bBean = (ScoreBean2) b; // 返回分组规则 System.out.println(aBean.getCourseName()+"---MyGroup中比较---"+(bBean.getCourseName())); return aBean.getCourseName().compareTo(bBean.getCourseName()); &#125;&#125;================================================================================ 执行结果 ================================================================================algorithm huangjiaju 82.28571428571429==================第1次进入reducecomputer huangjiaju 83.2==================第2次进入reduceenglish huanglei 83.0==================第3次进入reducemath huangxiaoming 83.0==================第4次进入reduce MR实现两个表的数据关联Join题目 订单数据表t_order： flag=0id date pid amount1001 20150710 P0001 21002 20150710 P0001 31003 20150710 P0002 3Id:数据记录idDate 日期Pid 商品idAmount 库存数量 6.商品信息表t_product flag=1pid name category_id priceP0001 小米5 C01 2000P0002 锤子T1 C01 3500 mr实现两个表的数据关联id pid date amount name category_id price 答案1 : Reducer 端 实现 Join思路 map端 读取到当前路径下，所有文件的切片信息， 根据文件名判断是那张表 在setup中，从文件切片中获取到文件名 123456// 获取读取到的切片相关信息,一个切片对应一个 maptaskInputSplit inputSplit = context.getInputSplit();// 转换为文件切片FileSplit fs = (FileSplit)inputSplit;// 获取文件名filename = fs.getPath().getName(); 这里总共会获得2个文件名（指定目录存了2个指定文件），一个文件名对应一个切片 关联字段作为key， 其它的作为value，在value前面加上当前文件的名称标记 reduce端 通过标记区分两张表，把读取到的信息，分别存入2个list中 遍历大的表，与小表进行拼接（小表的相同pid记录只会有一条） 拼接完成后即可写出 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161package com.rox.mapreduce.mr3._03_join2表的数据关联;import java.io.IOException;import java.util.ArrayList;import java.util.List;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.FileSplit;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class ReduceJoinDemo &#123; public static void main(String[] args) throws Exception &#123; // 指定HDFS相关参数 Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://cs1:9000"); System.setProperty("HADOOP_USER_NAME", "ap"); //  创建/配置 Job Job job = Job.getInstance(conf); // 设置Jar包类型 job.setJarByClass(ReduceJoinDemo.class); // 设置Map Reduce执行类 job.setMapperClass(MyMapper.class); job.setReducerClass(MyReducer.class); // 设置Map输出类 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); // Reduce输出类 job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // 设置输入 输出路径 String inP = "/in/joindemo"; String outP = "/out/joinout1"; FileInputFormat.setInputPaths(job, new Path(inP)); FileOutputFormat.setOutputPath(job, new Path(outP)); // 设置如果存在路径就删除 Path mypath = new Path(outP); FileSystem hdfs = mypath.getFileSystem(conf); if (hdfs.isDirectory(mypath)) &#123; hdfs.delete(mypath, true); &#125; //  执行job boolean waitForCompletion = job.waitForCompletion(true); System.exit(waitForCompletion?0:-1); &#125; /** * @author shixuanji * 思路: 读取2个表中的数据,进行标记发送 * key: 两表需要关联的字段 * value: 其它值, 需要标记， 标记数据的来源 * * * **核心： 关联条件** - 想要在 reduce 端完成 join， 要在 reduce 端可以同时接收到两个表中的数据 - 要保证在 Map 端进行读文件的时候， 读到2个表的数据， 并且需要对2个表的数据进行区分 - 将2个表放在同一个目录下 解决: mapper 开始执行时, 在setup方法中, 从上下文中取到文件名, 根据文件名打标记 */ static class MyMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt; &#123; String filename = ""; Text k = new Text(); Text v = new Text(); @Override protected void setup( Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; // 获取读取到的切片相关信息,一个切片对应一个 maptask InputSplit inputSplit = context.getInputSplit(); // 转换为文件切片 FileSplit fs = (FileSplit)inputSplit; // 获取文件名 filename = fs.getPath().getName(); System.out.println("本次获取到的文件名为-----"+filename); &#125; @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; // 解析出来每一行内容, 打标记, 发送 String[] infos = value.toString().split("\t"); if (filename.equals("order")) &#123; k.set(infos[2]); // 设置标记前缀为 OR v.set("OR"+infos[0]+"\t"+infos[1]+"\t"+infos[3]); &#125;else &#123; k.set(infos[0]); // 设置标记前缀为 PR v.set("PR"+infos[1]+"\t"+infos[2]+"\t"+infos[3]); &#125; context.write(k, v); &#125; &#125; static class MyReducer extends Reducer&lt;Text, Text, Text, NullWritable&gt; &#123; Text k = new Text(); @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Reducer&lt;Text, Text, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; /** * 首先明确 product 和 order 是 一对多的关系 * 根据前缀不同,取到2个不同的表存进2个容器中 * 遍历多的表, 与一进行拼接 * 最后写出到上下文 * 最终的输出格式 id pid date amount name category_id price */ // 因为每次遍历到不同的pid, 都会走进来一次, list也会有新的输出,所以必须定义在里面,每次进来都要初始化 List&lt;String&gt; productList =new ArrayList&lt;&gt;(); List&lt;String&gt; orderList =new ArrayList&lt;&gt;(); for (Text v : values) &#123; String vStr = v.toString(); if (vStr.startsWith("OR")) &#123; orderList.add(vStr.substring(2)); &#125;else &#123; productList.add(vStr.substring(2)); &#125; &#125; // 此时2个list添加完了本次 相同的 key(pid) 的所有商品 // 遍历多的进行拼接 for (String or : orderList) &#123; // 相同的 pid的 product 只有一个, productList中的数量是1 // 但是相同pid 的 订单 可能有多个 String res = key.toString() + "\t" + or + productList.get(0); k.set(res); context.write(k, NullWritable.get()); &#125; &#125; &#125;&#125; ※ 答案2 ： Mapper 端实现 Join ※思路 创建job的时候,把小表加入缓存 在map的setup中, 读取缓存中的数据, 存入一个成员变量 map中 map方法中,只需要读一个表, 然后根据关联条件(关联key: pid)消除笛卡尔集,进行拼接 map直接输出, 甚至都不需要reduce 注意点: 需要达成jar包运行, 直接用Eclipse会找不到缓存 jar包执行方法 12# 如果代码内部指定了输入输出路径，后面的/in，/out参数可以不加hadoop jar xxxx.jar com.rox.xxx.xxxx(主方法) /in/xx /out/xx 如果没有Reduce方法 main方法中，设置map的写出key，value,应该用 setOutputKeyClass 123//// 设置Map输出类 (因为这里没有Reduce, 所以这里是最终输出,一定要注意!!!)//////////job.setOutputKeyClass(Text.class);job.setOutputValueClass(NullWritable.class); 要设置reduce task 的个数为0 1job.setNumReduceTasks(0); 把小文件加载到缓存中的方法 12////////////// 将小文件加载到缓存 job.addCacheFile(new URI("/in/joindemo/product")); ​ 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120package com.rox.mapreduce.mr3._03_join;import java.io.BufferedReader;import java.io.FileReader;import java.io.IOException;import java.net.URI;import java.util.HashMap;import java.util.Map;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class MapJoinDemo &#123; public static void main(String[] args) throws Exception &#123; // 指定HDFS相关参数 Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://cs1:9000"); System.setProperty("HADOOP_USER_NAME", "ap"); //  创建/配置 Job Job job = Job.getInstance(conf); // 设置Jar包类型:这里千万别写错了 job.setJarByClass(MapJoinDemo.class); // 设置Map Reduce执行类 job.setMapperClass(MyMapper.class); ///////////// 设置Map输出类 (因为这里没有Reduce, 所以这里是最终输出,一定要注意!!!)////////// job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); ////////////// 设置reduce执行个数为0 job.setNumReduceTasks(0); ////////////// 将小文件加载到缓存 job.addCacheFile(new URI("/in/joindemo/product")); // 设置输入 输出路径 String inP = "/in/joindemo/order"; String outP = "/out/joinout2"; FileInputFormat.setInputPaths(job, new Path(inP)); FileOutputFormat.setOutputPath(job, new Path(outP)); // 设置如果存在路径就删除 Path mypath = new Path(outP); FileSystem hdfs = mypath.getFileSystem(conf); if (hdfs.isDirectory(mypath)) &#123; hdfs.delete(mypath, true); &#125; //  执行job System.exit(job.waitForCompletion(true)?0:-1); &#125; /** * @author shixuanji * 思路: * 创建job的时候,把小表加入缓存 * 在map的setup中, 读取缓存中的数据, 存入一个成员变量 map中 * map方法中,只需要读一个表, 然后根据关联条件(关联key: pid)消除笛卡尔集,进行拼接 * 直接输出, 甚至都不需要reduce * * 注意点: * 需要达成jar包运行, 直接用Eclipse会找不到缓存 * 格式: hadoop jar包本地路径 jar包主方法全限定名 hadoop输入 hadoop输出 */ static class MyMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123; // 创建装载小表的map, key存储 关联键, value存其它 Map&lt;String, String&gt; proMap = new HashMap&lt;&gt;(); @Override protected void setup(Context context) throws IOException, InterruptedException &#123; // 获取缓存中存储的小表 (一般是 一对多中的 一), 因为只存了1个,所以直接取第0个 Path path = context.getLocalCacheFiles()[0]; String pString = path.toString(); // 开启in流, BufferedReader 逐行读取文件 BufferedReader br = new BufferedReader(new FileReader(pString)); String line = null; while ((line = br.readLine()) != null) &#123; // 成功读取一行 String[] infos = line.split("\t"); // 存进proMap proMap.put(infos[0], infos[1] + "\t" + infos[2] + "\t" + infos[3]); &#125;// br.close(); &#125; /** * 直接从路径读取大文件 */ Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] infos = value.toString().split("\t"); String pid = infos[2]; //进行关联 pid到map中匹配 如果包含 证明匹配上了 // 艹, 这里pid之前加了 "", 妈的,当然找不到啦!!! if (proMap.containsKey(pid)) &#123; String res = value.toString() + "\t" + proMap.get(pid); k.set(res); context.write(k, NullWritable.get()); &#125; &#125; &#125;&#125; title: 执行流程时序图  Mapper(map)->ScoreBean: k:ScoreBean(courseName,avgScore) Mapper(map)->ScoreBean: v:Text(stuName) Mapper(ScoreBean)->Reducer(MyGroup): course按字典升序 Mapper(ScoreBean)->Reducer(MyGroup): course内成绩降序 Reducer(MyGroup)->Reducer(reduce): 根据自定义的分组规则按组输出 Reducer(MyGroup)->Reducer(reduce): 一组只调用reduce一次{"theme":"simple"} var code = document.getElementById("sequence-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value)); var diagram = Diagram.parse(code); diagram.drawSVG("sequence-0", options);]]></content>
      <categories>
        <category>Hadoop</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce笔记-3]]></title>
    <url>%2F2018%2F06%2F08%2FHadoop%2F2-MapReduce%2FMapReduce%E7%AC%94%E8%AE%B0-3%2F</url>
    <content type="text"><![CDATA[1.多 Job 串联1.概念当程序中有多个 Job， 并且多个 job 之间相互依赖， a ， job 需要依赖另一个b，job 的执行结果时候， 此时需要使用多 job 串联 2. 涉及到昨天的微博求共同粉丝题目 A:B,C,D,F,E,OB:A,C,E,KC:F,A,D,ID:A,E,F,LE:B,C,D,M,LF:A,B,C,D,E,O,MG:A,C,D,E,FH:A,C,D,E,OI:A,OJ:B,OK:A,C,DL:D,E,FM:E,F,GO:A,H,I,J,K 以上是数据：A:B,C,D,F,E,O表示：A用户 关注B,C,D,E,F,O 求所有两两用户之间的共同关注对象 注意 要写2个MapReduce， 开启2个job 后一个job依赖于前一个的执行结果 后一个job的输入文件路径，就是前一个job的输出路径 2个job需要添加依赖 代码参考练习-第一题-求微博共同好友 多 Job 串联部分代码 基本的写到一起， job1， job2 用JobControl对象管理多 job， 会将多个 job 当做一个组中的 job 提交， 参数指的是组名， 随意起 原生的 job 要转为可控制的 job 123456789101112131415161718// 创建 JobControl 组JobControl jc = new JobControl("common_friend");// job 拿好配置， 加入 ControlledJob 管理, 变成可控制的 jobControlledJob ajob = new ControlledJob(job1.getConfiguration());ControlledJob bjob = new ControlledJob(job2.getConfiguration());// 添加依赖关系bjob.addDependingJob(ajob); // 添加 job进 JCjc.addJob(ajob);jc.addJob(bjob);// 启动线程Thread jobControlTread = new Thread(jc);jobControlTread.start();// 在线程完成之后关闭while(!jc.allFinished()) &#123; Thread.sleep(500);&#125;jobControl.stop(); 2. 分组组件map–分组–reduce reduce 接收到的数据是按照 map 输出的 key 进行分组的, 分组的时候按照 key 相同的时候为一组, 默认都实现了 WritableComparable接口， 其中的 compareTo（）方法返回为0的时候 默认为一组， 返回不为0， 则分到下一组 自定义分组使用场景： 默认的数据分组不能满足需求 一、数据解释 数据字段个数不固定：第一个是课程名称，总共四个课程，computer，math，english，algorithm，第二个是学生姓名，后面是每次考试的分数 二、统计需求：1、统计每门课程的参考人数和课程平均分 2、统计每门课程参考学生的平均分，并且按课程存入不同的结果文件，要求一门课程一个结果文件 3、求出每门课程参考学生成绩最高平均分的学生的信息：课程，姓名和平均分 第三题： 要求就是分组求最大值， 两件事情： 分组， 排序（shuffle） 总结： 1、利用“班级和平均分”作为 key，可以将 map 阶段读取到的所有学生成绩数据按照班级 和成绩排倒序，发送到 reduce 2、在 reduce 端利用 GroupingComparator 将班级相同的 kv 聚合成组，然后取第一个即是最 大值 具体参考：练习-求学生成绩-第三小题 3. Reduce 中的2个坑坑1Iterable\只能循环遍历一次 迭代器每次循环遍历完成， 指针都会移动到最后一个 系统类型，没事 自定义类型 ，有问题？ 坑2迭代器中所有对象公用同一个地址 4. Reduce 端的 Join牺牲效率换执行 思路： 核心： 关联条件 想要在 reduce 端完成 join， 要在 reduce 端可以同时接收到两个表中的数据 要保证在 Map 端进行读文件的时候， 读到2个表的数据， 并且需要对2个表的数据进行区分 将2个表放在同一个目录下 Map 端 读取两个表中的数据， 进行切分、发送 key ： 公共字段–关联字段–pid value： 剩下的字段， 标记数据的来源表 Reduce 端 通过编辑分离出2个表的数据 分别存到2个容器中（ArrayList） 遍历大表，拼接小表 代码参考练习-第三题MR实现2个表之间的Join 缺陷1. ReduceTask 的并行度问题： 建议0.95*datanode 的个数 并行度不高， 性能不高 2. 容器性能 list 等， 不提倡， reduce 接收的数据， 可能会很大 3. ReduceTask 容易产生数据倾斜 假设我们设置多个 ReduceTask， 根据分区规则， 默认 hash 以 key关联条件分， ReduceTask数据倾斜， 每个 ReduceTask 分工不均， 非常影响性能，没有合理的利用集群资源 在真实的生产中一定要尽量的避免数据倾斜 最好的做法：将分区设计的足够完美，难度比较大 因此，ReduceTask 一般不会完成 John工作 放在 Map 端完成就不会有这个问题了 补充：Mapper 中的源码分析123456789101112131415public void run(Context context) throws IOException, InterruptedException &#123; // 在 maptask 执行之前调用一次， 一个 maptask 只会调用一次。setup 中通常会帮 map 中初始化一些变量和资源， 比如数据库的连接等。 // 主要目的：减少资源的初始化次数而提升程序的性能 setup(context); try &#123; // 获取文件是否还有下一行， 一行只调用一次 while (context.nextKeyValue()) &#123; map(context.getCurrentKey(), context.getCurrentValue(), context); &#125; &#125; finally &#123; // maptask 任务执行完成之后会调用一次，一个 maptask 只会调用一次 // 帮 map 处理一些善后工作， 比如：资源的关闭 cleanup(context); &#125; &#125; 5. Map 端的 Join注意点：这种方式只能通过 Jar 包上传的方式，直接用 Eclipse 会找不到缓存 为了提升 Map 端 Join 性能， 我们的策略是， 将小表的数据加载到每个运行的 MapTask 的内存中。 如果小表被加载到了内存中， 我们每次在 Map 端只需要读取大表，当读取到大表的每一行数据，可以直接和内存中的小表进行关联。 这个时候，只需要 Map 就可以完成 Join 操作了。 1. 如何将小表加入到内存中？12// 将指定路径文件加载到缓存中job.addCacheFile(new URI("/xxx")); 2. Map 端怎样读取缓存中的数据想要在 Java 中使用缓存中的数据，缓存中的数据必须封装到 Java 的容器中 12// 获取缓存文件context.getLocalCacheFiles()[0] 3. 代码参考练习-第3题 代码注意点： setup：从缓存读取一文件（多对一的一）到 HashMap main 方法中注意点 12345// 指定文件加入缓存job.addCacheFile(new URI("/xxx")); // 如果没有ReduceTask， 要设置为0job.setNumReduceTasks(0); 示范代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import java.io.BufferedReader;import java.io.FileInputStream;import java.io.IOException;import java.io.InputStreamReader;import java.net.URI;import java.util.HashMap;import java.util.Map;import org.apache.commons.lang.StringUtils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class MapSideJoin &#123; public static class MapSideJoinMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123; // 用一个hashmap来加载保存产品信息表 Map&lt;String, String&gt; pdInfoMap = new HashMap&lt;String, String&gt;(); Text k = new Text(); /** * 通过阅读父类Mapper的源码，发现 setup方法是在maptask处理数据之前调用一次 可以用来做一些初始化工作 */ @Override protected void setup(Context context) throws IOException, InterruptedException &#123; BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream("pdts.txt"))); String line; while (StringUtils.isNotEmpty(line = br.readLine())) &#123; String[] fields = line.split(","); pdInfoMap.put(fields[0], fields[1]); &#125; br.close(); &#125; // 由于已经持有完整的产品信息表，所以在map方法中就能实现join逻辑了 @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String orderLine = value.toString(); String[] fields = orderLine.split("\t"); String pdName = pdInfoMap.get(fields[1]); k.set(orderLine + "\t" + pdName); context.write(k, NullWritable.get()); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(MapSideJoin.class); job.setMapperClass(MapSideJoinMapper.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); FileInputFormat.setInputPaths(job, new Path("D:/srcdata/mapjoininput")); FileOutputFormat.setOutputPath(job, new Path("D:/temp/output")); // 指定需要缓存一个文件到所有的maptask运行节点工作目录 /* job.addArchiveToClassPath(archive); */// 缓存jar包到task运行节点的classpath中 /* job.addFileToClassPath(file); */// 缓存普通文件到task运行节点的classpath中 /* job.addCacheArchive(uri); */// 缓存压缩包文件到task运行节点的工作目录 /* job.addCacheFile(uri) */// 缓存普通文件到task运行节点的工作目录 // 将产品表文件缓存到task工作节点的工作目录中去 job.addCacheFile(new URI("file:/D:/srcdata/mapjoincache/pdts.txt")); //map端join的逻辑不需要reduce阶段，设置reducetask数量为0 job.setNumReduceTasks(0); boolean res = job.waitForCompletion(true); System.exit(res ? 0 : 1); &#125;&#125; 6. 对比MapJoin 的方式： 大 &amp; 小表 因为有一个表需要加载到内存中，注定加载到内存中的表不能过大（hive 中默认是256M） 大表 &amp; 大表 如何设计 ReduceJoin ： 解决数据倾斜的问题，合理设计分区。 —很难做到 将其中一个大表进行切分，切分成小表， 最终执行 大表 &amp; 小表 优点 并行度高，不存在数据倾斜的问题，运行效率高 优先选择MapJoin :arrow_forward: 7. 排序算法 （待整理：TODO）1. 快速排序边界值始终是不变的。 2. 归并排序一般情况针对有序的，多个， 小数据集 应用场景：想到了多个Reduce 任务产生的多个文件的合并 1. 归并排序前传： 合并多个数组 2. 归并排序 之 一个大数据集—归———-切分成单个的数据集 —-并——— 两两相并， 并成新的数组， 小的先放入数组， 再放大的 新的数组再不断执行 上述的 合并多个数组 8. ※ Shuffle 过程 ※ mapper 阶段处理的数据如何传递给 reducer 阶段，是 MapReduce 框架中 最关键的一个流程，这个流程就叫 Shuffle Shuffle 即 数据混洗 —— 核心机制：数据分区，排序，局部聚合，缓存，拉取，再合并，排序； 环形缓冲区 内存中的一种首尾相连的数据结构（底层是字节数组），kvbuffer 包含原始数据区和元数据区，一个mapTask任务对应一个环形缓冲区 默认大小 100M，默认阈(yu)值 0.8，即当达到 80M 后，会触发 spill溢写 操作，将数据写入磁盘，此时mapper输出会继续向剩余20M中写数据缓冲区大小 mapred-site.xml：mapreduce.task.io.sort.mb阈值 mapred-site.xml：mapreduce.map.sort.spill.percent路径：mapred-site.xml：mapreduce.cluster.local.dir 如果此80M数据写入磁盘完成前，剩余20M缓冲区也写完，则会进入阻塞状态，直到是spill完成腾出缓冲区空间 赤道（equtor）：环形缓冲区中原始数据和元数据的边界 原始数据：mapTask输出的数据 元数据 记录原始数据的数据，包含4部分内容，占16*4字节； 每一条元数据占用空间是一样的，排序可以通过交换元数据实现 分类 a. 原始数据中key的起始位置 b. 原始数据中value的起始位置 c. value的长度 d. 分区信息，即该条信息属于哪个分区 核心操作 \1. 分区 partition（如果 reduceTask 只有一个或者没有，那么 partition 将不起作用） \2. Sort 根据 key 排序（MapReduce 编程中的 sort 是一定会做的，并且 只能按照 key排序， 当然 如果没有 reducer 阶段，那么就不会对 key 排序） \3. Combiner 进行局部 value 的合并（Combiner 是可选的组件，作用只是为了提高任务的执行效率） 详细过程 \1. 一个大文件需要处理，它在在 HDFS 上是以 block 块形式存放，每个 block 默认为 128M 存 3 份；运行时每个 map 任务会处理一个切块（split），如果 block 大和 split 相同，有多少个 block 就有多少个 map 任务；所以对整个文件处理时会有很多 map 任务进行并行计算。 \2. 每个 map 任务处理完输入的切块后会把结果写入到内存的一个 环形缓冲区，写入过程中会进行简单排序，当缓冲区的大小阀值，一个后台的线程就会启动把缓冲区中的数据溢写（spill）到本地磁盘中，同时Mapper继续时向环形缓冲区中写入数据。 数据溢写入到磁盘之前，首先会根据 reducer 的数量划分成同数量的分区（partition），每个分区中的都数据会有后台线程根据 map 任务的输出结果 key 进行排序； 如果有 combiner，它会在 缓冲区溢写到磁盘之前 和 mapTask排好序的输出上 运行，使写到本地磁盘和传给 reducer 的数据更少；Combiner即是把同一分区中的同一key的数据进行合并，整个shuffle过程会调用两个Combiner ! 最后在本地生成分好区且排好序的小文件。 注意：如果 map 向环形缓冲区写入数据的速度大于向本地写入数据的速度，环形缓冲区会被写满，向环形缓冲区写入数据的线程会阻塞直至缓冲区中的内容全部溢写到磁盘后再次启动，到阀值后会向本地磁盘新建一个溢写文件； \3. map 任务完成之前，会把本地磁盘溢写的所有文件 不停地 合并（merge）成得到一个结果文件，合并得到的结果文件会根据小溢写文件的分区而分区，每个分区的数据会再次根据 key 进行 排序，得到的结果文件是分好区且排好序的（可以合并成一个文件的溢写文件数量默认为10）；默认合并溢写文件数量 mapred-site.xml：mapreduce.task.io.sort.factor \4. reduce 任务启动，Reducer 中的一个线程定期向 MRAppMaster 询问 Mapper 输出结果文件位置，Mapper 结束后会向 MRAppMaster 汇报信息，从而 Reducer 会得知 Mapper 状态并得到 map 结果文件目录；reduce任务数配置a) mapred-site.xml：mapreduce.job.reducesb) job.setNumReduceTasks(num) \6. 当有一个 Mapper 结束时，reduce 任务进入复制阶段，reduce 任务通过 http 协议（hadoop 内置了netty容器）把所有 Mapper 结果文件的 对应的分区数据 拉取（fetch）过来，Reducer 可以并行复 制 Mapper 的 结果 ， 默认线程数为5； 所有 Reducer 复制完成 map 结果文件后，由于 Reducer 可能会失败，NodeManager 并不会在第一个 map 结果文件复制完成后就删除它，而是直到作业完成后 MRAppMaster 通知 NodeManager 进行删除； 另外如果 map 结果文件相当小，则会被直接复制到 reduce NodeManager 的内存；一旦缓冲区达到 reduce 的阈值大小 0.66 或 写入到 reduce NodeManager 内 存 中 文 件 个 数 达 到 map 输出阈值 1000，reduce 就会把 map 结果文件合并（merge）溢写到本地；默认线程数 mapred-site.xml：mapreduce.reduce.shuffle.parallelcopies缓冲区大小 mapred-site.xml:mapreduce.reduce.shuffle.input.buffer.percent，默认0.7阈值：mapred-site.xml:mapreduce.reduce.shuffle.merge.percent，默认0.66map输出阈值1000：mapred-site.xml:mapreduce.reduce.merge.inmem.threshold \7. 复制阶段完成后，Reducer 进入 Merge 阶段，循环地合并 map 结果文件，维持其顺序排序，合并因子默认为 10，经过不断地 Merge 后得到一个”最终文件”，可能存储在磁盘也可能存在内存中； \8. “最终文件”输入到 reduce 进行计算，计算结果输入到 HDFS。 [ 注意 ] 溢写前会先按照分区进行排序，再按key进行排序，采用 快速排序排序是按照原始数据排序，但是由于原始数据不好移动且原始数据包含了原始数据的位置信息，所以移动的其实是元数据；写入时读的是元数据，真正写入的时原始数据 最后的数据如果不够80M，也会被强制flush到磁盘 每个mapTask任务生成的磁盘小数据最后都会merge成一个大文件，采用 归并排序 Shuffle 中的缓冲区大小会影响到 mapreduce 程序的执行效率，原则上说，缓冲区越大，磁 盘io的次数越少，执行速度就越快。 10、自定义输入 InputFormat 默认的文件加载：TextInputFormat 默认的文件读取：LineRecordReader 源码追踪过程：context –&gt; mappercontext –&gt; mapcontext –&gt; reader –&gt; input –&gt; real –&gt; inputFormat.createRecordReader（split，taskContext），然后查找 inputFormat –&gt; createRecordReader（split，taskContext），inputFormat –&gt; TextInputFormat实例对象 案例：多个小文件合并 word1.txt ~word10.txt 每次读取一个小文件 自定义输入，需要创建两个类，并通过Job对象指定自定义输入 \1. 创建XxxInputFormat类，继承FileInputFormat&lt;&gt;，重写 createRecordReader() 方法 \2. 创建XxxRecordReader类，继承RecordReader&lt;&gt;，重写以下方法： initialize()：初始化方法，类似于setup()，对属性、链接或流进行初始化 getCurrentKey()：返回key getCurrentValue()：返回value getProgress()：返回文件执行进度 nextKeyValue()：返回文件是否读取结束 close()：进行一些资源的释放 \3. 在mapreduce类的main()方法中指定自定义输入：job.setInputFormatClass(XxxInputFormat.class); 代码 123456789101112131415161718192021222324252627282930313233package com.rox.mapreduce.mr4._02_inputformat;import java.io.IOException;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.JobContext;import org.apache.hadoop.mapreduce.RecordReader;import org.apache.hadoop.mapreduce.TaskAttemptContext;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;public class WholeFileInputFormat extends FileInputFormat&lt;NullWritable, Text&gt; &#123; /** * 设置每个小文件不可分片,保证一个小文件生成一个key-v键值对 */ @Override protected boolean isSplitable(JobContext context, Path filename) &#123; return false; &#125; @Override public RecordReader&lt;NullWritable, Text&gt; createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; WholeFileRecordReader reader = new WholeFileRecordReader(); reader.initialize(split, context); return reader; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package com.rox.mapreduce.mr4._02_inputformat;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FSDataInputStream;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.RecordReader;import org.apache.hadoop.mapreduce.TaskAttemptContext;import org.apache.hadoop.mapreduce.lib.input.FileSplit;public class WholeFileRecordReader extends RecordReader&lt;NullWritable, Text&gt;&#123; private FileSplit fileSplit; private Configuration conf; private Text value = new Text(); private boolean processed = false; // 标识文件是否读取完成 /** * 初始化方法 */ @Override public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; this.fileSplit=(FileSplit)split; this.conf = context.getConfiguration(); &#125; @Override public boolean nextKeyValue() throws IOException, InterruptedException &#123; if (!processed) &#123; byte[] contents = new byte[(int)fileSplit.getLength()]; Path file = fileSplit.getPath(); FileSystem fs = file.getFileSystem(conf); FSDataInputStream in = null; try &#123; in = fs.open(file); // 把输入流上的数据全部读取到contents字节数组中 IOUtils.readFully(in, contents, 0, contents.length); // 把读取到的数据设置到value里 value.set(contents,0,contents.length); &#125; finally &#123; IOUtils.closeStream(in); &#125; processed = true; return true; &#125; return false; &#125; @Override public NullWritable getCurrentKey() throws IOException, InterruptedException &#123; return NullWritable.get(); &#125; @Override public Text getCurrentValue() throws IOException, InterruptedException &#123; return value; &#125; @Override public float getProgress() throws IOException, InterruptedException &#123; return processed ? 1.0f : 0.0f; &#125; @Override public void close() throws IOException &#123; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114package com.rox.mapreduce.mr4._02_inputformat;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.FileSplit;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;public class SmallFilesConvertToBigMR extends Configured implements Tool &#123; public static void main(String[] args) throws Exception &#123; int exitCode = ToolRunner.run(new SmallFilesConvertToBigMR(), args); System.exit(exitCode); &#125; @Override public int run(String[] args) throws Exception &#123; Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://cs1:9000"); System.setProperty("HADOOP_USER_NAME", "ap"); Job job = Job.getInstance(conf, "combine small files to bigfile"); job.setJarByClass(SmallFilesConvertToBigMR.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); job.setMapperClass(SmallFilesConvertToBigMRMapper.class); job.setReducerClass(SmallFilesConvertToBigMRReducer.class); job.setOutputKeyClass(NullWritable.class); job.setOutputValueClass(Text.class);//////// job.setInputFormatClass(WholeFileInputFormat.class); // job.setOutputFormatClass(SequenceFileOutputFormat.class); Path input = new Path("/in/joindemo"); Path output = new Path("/out/bigfile"); FileInputFormat.setInputPaths(job, input); FileSystem fs = FileSystem.get(conf); if (fs.exists(output)) &#123; fs.delete(output, true); &#125; FileOutputFormat.setOutputPath(job, output); int status = job.waitForCompletion(true) ? 0 : 1; return status; &#125; static class SmallFilesConvertToBigMRMapper extends Mapper&lt;NullWritable, Text, Text, Text&gt; &#123; private Text filenameKey; @Override protected void setup( Mapper&lt;NullWritable, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; InputSplit split = context.getInputSplit(); Path path = ((FileSplit) split).getPath(); filenameKey = new Text(path.toString()); &#125; @Override protected void map(NullWritable key, Text value, Mapper&lt;NullWritable, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; context.write(filenameKey, value); &#125; &#125; static class SmallFilesConvertToBigMRReducer extends Reducer&lt;Text, Text, NullWritable, Text&gt; &#123; @Override protected void reduce(Text filename, Iterable&lt;Text&gt; bytes, Context context) throws IOException, InterruptedException &#123; context.write(NullWritable.get(), bytes.iterator().next()); &#125; &#125;&#125; 11、自定义输出 OutputFormat 默认的文件加载：TextOutputFormat 默认的文件读取：LineRecordWriter 源码追踪过程 略 案例：将考试成绩合格的输出到一个文件夹，不及格的输出到另一个文件夹（注意，不同于分区，分区只是量结果输出到同一文件夹下不同文件） 自定义输出，需要创建两个类，并通过Job对象指定自定义输入 \1. 创建XxxOutputFormat类，继承FileOutputFormat&lt;&gt;，重写getRecordWriter()方法 \2. 创建XxxRecordWriter类，继承RecordWriter&lt;&gt;，重写以下方法： write()：真正向外写出的方法，需要将结果输出到几个不同文件夹，就需要创建几个输出流 而输出流通过FileSystem对象获取，FileSystem对象获取需要配置文件 一般可以通过构造方法直接传入FileSystem对象 close()：释放资源 \3. 在mapreduce类的main()方法中指定自定义输入：job.setOutputFormatClass(XxxOutputFormat.class); 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package com.rox.mapreduce.outputformat;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class MultipleOutputMR &#123; public static void main(String[] args) throws Exception &#123; // 指定HDFS相关参数 Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://cs1:9000"); System.setProperty("HADOOP_USER_NAME", "ap"); //  创建/配置 Job Job job = Job.getInstance(conf); // 设置Jar包类型:这里千万别写错了 job.setJarByClass(MultipleOutputMR.class); // 设置Map Reduce执行类 job.setMapperClass(MultipleOutputMRMapper.class); // 设置Map输出类 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(NullWritable.class); ////////////// 设置reduce执行个数为0 job.setNumReduceTasks(0); ///////////// 设置MapOutputFormatClass job.setOutputFormatClass(MyOutputFormat.class); // 设置输入 输出路径 String inP = "/in/newScoreIn"; String outP = "/out/myoutformat/mulWriteSuc"; FileInputFormat.setInputPaths(job, new Path(inP)); FileOutputFormat.setOutputPath(job, new Path(outP)); // 设置如果存在路径就删除 Path mypath = new Path(outP); FileSystem hdfs = mypath.getFileSystem(conf); if (hdfs.exists(mypath)) &#123; hdfs.delete(mypath, true); &#125; //  执行job System.exit(job.waitForCompletion(true)?0:-1); &#125; static class MultipleOutputMRMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; // 参考次数&gt;7次 算合格 String[] splits = value.toString().split(","); if (splits.length &gt; 9) &#123; context.write(new Text("1::"+value.toString()), NullWritable.get()); &#125;else &#123; context.write(new Text("2::"+value.toString()), NullWritable.get()); &#125; &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233package com.rox.mapreduce.outputformat;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FSDataOutputStream;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.RecordWriter;import org.apache.hadoop.mapreduce.TaskAttemptContext;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class MyOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt; &#123; @Override public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter( TaskAttemptContext job) throws IOException, InterruptedException &#123; Configuration configuration = job.getConfiguration(); FileSystem fs = FileSystem.get(configuration); Path p1 = new Path("/out/myoutformat/out1"); Path p2 = new Path("/out/myoutformat/out2"); FSDataOutputStream out1 = fs.create(p1); FSDataOutputStream out2 = fs.create(p2); return new MyRecordWriter(out1,out2); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243package com.rox.mapreduce.outputformat;import java.io.IOException;import org.apache.hadoop.fs.FSDataOutputStream;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.RecordWriter;import org.apache.hadoop.mapreduce.TaskAttemptContext;public class MyRecordWriter extends RecordWriter&lt;Text, NullWritable&gt; &#123; FSDataOutputStream fsout = null; FSDataOutputStream fsout1 = null; public MyRecordWriter(FSDataOutputStream fsout, FSDataOutputStream fsout1) &#123; super(); this.fsout = fsout; this.fsout1 = fsout1; &#125; @Override public void write(Text key, NullWritable value) throws IOException, InterruptedException &#123; String[] strs = key.toString().split("::"); if (strs[0].equals("1")) &#123; fsout.write((strs[1]+"\n").getBytes()); &#125;else &#123; fsout1.write((strs[1]+"\n").getBytes()); &#125; &#125; @Override public void close(TaskAttemptContext context) throws IOException, InterruptedException &#123; IOUtils.closeStream(fsout); IOUtils.closeStream(fsout1); &#125;&#125; 12. 倒排索引建立概念： 倒排索引（Inverted Index），也常被称为反向索引、置入档案或反向档案，是一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。它是文档检索系统中最常用的数据结构。了解详情可自行百度 需求：有大量的文本（文档、网页），需要建立搜索索引 , 要求统计每一个单词在每个文件中出现的次数, 如下 思路 先根据单词&amp;文件名为 key, 输出一个文件 拆分文件行, 以单词为 key, value 再拼接 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140//  InverIndexStepOne package cn.itcast.bigdata.mr.inverindex;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.FileSplit;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class InverIndexStepOne &#123; static class InverIndexStepOneMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; Text k = new Text(); IntWritable v = new IntWritable(1); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] words = line.split(" "); FileSplit inputSplit = (FileSplit) context.getInputSplit(); String fileName = inputSplit.getPath().getName(); for (String word : words) &#123; k.set(word + "--" + fileName); context.write(k, v); &#125; &#125; &#125; static class InverIndexStepOneReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int count = 0; for (IntWritable value : values) &#123; count += value.get(); &#125; context.write(key, new IntWritable(count)); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(InverIndexStepOne.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.setInputPaths(job, new Path("D:/srcdata/inverindexinput")); FileOutputFormat.setOutputPath(job, new Path("D:/temp/out")); // FileInputFormat.setInputPaths(job, new Path(args[0])); // FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setMapperClass(InverIndexStepOneMapper.class); job.setReducerClass(InverIndexStepOneReducer.class); job.waitForCompletion(true); &#125;&#125;//  IndexStepTwo package cn.itcast.bigdata.mr.inverindex;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class IndexStepTwo &#123; public static class IndexStepTwoMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] files = line.split("--"); context.write(new Text(files[0]), new Text(files[1])); &#125; &#125; public static class IndexStepTwoReducer extends Reducer&lt;Text, Text, Text, Text&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; StringBuffer sb = new StringBuffer(); for (Text text : values) &#123; sb.append(text.toString().replace("\t", "--&gt;") + "\t"); &#125; context.write(key, new Text(sb.toString())); &#125; &#125; public static void main(String[] args) throws Exception &#123; if (args.length &lt; 1 || args == null) &#123; args = new String[]&#123;"D:/temp/out/part-r-00000", "D:/temp/out2"&#125;; &#125; Configuration config = new Configuration(); Job job = Job.getInstance(config); job.setMapperClass(IndexStepTwoMapper.class); job.setReducerClass(IndexStepTwoReducer.class);// job.setMapOutputKeyClass(Text.class);// job.setMapOutputValueClass(Text.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); System.exit(job.waitForCompletion(true) ? 1:0); &#125;&#125; 13、Yarn1、Yarn图示简介 在Hadoop1.x时， 只有两个主要组件：hdfs（文件存储）， MapReduce（计算） 所有的计算相关的全部放在MapReduce上 JobTracker: 整个计算程序的老大 资源调度：随机调度 监控程序运行的状态，启动运行程序 存在单点故障问题 TaskTracker：负责计算程序的执行 强行的将计算资源分成2部分 MapSlot ReduceSlot 每一部分资源只能跑对应的任务 缺陷： 单点故障 资源调度随机，会造成资源浪费 JobTracker的运行压力过大 Hadoop2.x： 分理出Yarn，专门负责集群的资源管理和调度 Yarn的进程：ResourceManager: 整个资源调度的老大 接受hadoop客户端的请求 接受NodeManager 的状态报告， NM的资源状态和存活状态 资源调度，整个计算程序的资源调度，调度的运行资源和节点 内部组件： ASM——ApplicationsManager 所有应用程序的管理者，负责调度应用程序 Scheduler——调度器概念 调度的是什么时候执行哪个计算程序 调度器： FIFO: first in first out 先提交的先执行，后提交的后执行 内部维护一个队列 FAIR: 公平调度器 大家平分资源运行 假设刚开始只有一个任务，占资源100%，此时又来了一个任务，这是进行资源平分，每人50% 内部也是维护一个队列 CAPACITY: 可以按需进行配置，使用资源 内部可维护多个队列，多个队列之间可以进行资源分配 例如：分配两个队列 队列1：60% 队列2：40% 每个队列中都是执行FIFO的 NodeManager： 负责真正的提供资源，运行计算程序 接受ResourceManager的命令 提供资源运行计算程序 MRAppMaster: 单个计算程序的老大, 类似于项目经理 负责帮助当前计算程序向ResourceManager申请资源 负责启动 MapTask 和 ReduceTask 任务 Container: 抽象资源容器，封装这一定的cpu，io 和网络资源（逻辑概念） 是运行MapTask，ReduceTask等的运行资源单位 1个split —— 1个MapTask (ReduceTask) —— 1个Container —— 显示为YarnChild，底层运行的资源单位就是Container 2、Yarn运行过程 MRAppMaster会在所有的MapTask执行到0.8的时候，开启ReduceTask任务 YARN 作业执行流程: 用户向 YARN 中提交应用程序，其中包括 MRAppMaster 程序，启动 MRAppMaster 的命令， 用户程序等。 ResourceManager 为该程序分配第一个 Container，并与对应的 NodeManager 通讯，要求 它在这个 Container 中启动应用程序 MRAppMaster。 MRAppMaster 首先向 ResourceManager 注册，这样用户可以直接通过 ResourceManager 查看应用程序的运行状态，然后将为各个任务申请资源，并监控它的运行状态，直到运行结 束，重复 4 到 7 的步骤。 MRAppMaster 采用轮询的方式通过 RPC 协议向 ResourceManager 申请和领取资源。 一旦 MRAppMaster 申请到资源后，便与对应的 NodeManager 通讯，要求它启动任务。 NodeManager 为任务设置好运行环境(包括环境变量、JAR 包、二进制程序等)后，将 任务启动命令写到一个脚本中，并通过运行该脚本启动任务。 各个任务通过某个 RPC 协议向 MRAppMaster 汇报自己的状态和进度，以让 MRAppMaster 随时掌握各个任务的运行状态，从而可以在任务败的时候重新启动任务。 8、应用程序运行完成后，MRAppMaster 向 ResourceManager 注销并关闭自己。 3、Job的提交过程(待整理) 客户端向rm发送 提交job请求 rm向客户端发送 共享资源路径 和 applicationId 客户端将程序运行需要的共享资源放进共享资源路径包括：程序jar包，xml配置文件，split切片信息 客户端向rm发送资源放置成功的报告，并真正 提交应用程序 rm接收到客户端的请求，会返回一个空闲的资源节点(比如：node01) 到资源节点(node01)上启动container 启动MRAppMaster 创建作业簿 记录 maptask 和 reducetask 的 运行状态和进度 等信息 mrappmaster去共享资源路径下 ,获取 切片 和 配置文件 等信息 mrappmaster 向 rm 申请maptask 和 reducetask的资源 rm 在处理 mrappmaster 请求时，会 优先处理有关maptask的请求 rm 向 mrappmaster 返回空闲节点（数据本地优先原则），运行maptask 或 reducetask优先返回有数据的节点。 对象节点需要到hdfs 共享路径下下载程序jar包等 共享资源 到本地 mrappmaster 到 对应的节点上，启动container 和 maptask maptask需要向 mrappmaster 汇报自身的 运行状态和进度 mrappmaster 监控到所有的maptask 运行进度到 80%，启动reducetask（启动前，也会下载共享资源路径下的响应文件，程序jar包，配置文件等） reducetask 时刻和 mrappmaster 通信，汇报自身的 运行状态和进度 整个运行过程中，maptask运行完成 ， 都会向mrappmaster 申请注销 自己 当所有的maptask 和 reducetask 运行完成 ， mrappmaster 就会向rm 申请注销，进行资源回收 4.MapReduce&amp;yarn的工作机制–job提交流程–吸星大法 疑点: MRAppMaster 的数量问题 经过研究, 应该是每个 job 会产生一个 MRAppMaster 5.MapReduce全原理剖析—六脉神剑全图 细节图 6. maptask任务分配切片机制 7. 完整流程图 14. 关于 Driver 配置的注意点123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;/** * 相当于一个yarn集群的客户端 * 需要在此封装我们的mr程序的相关运行参数，指定jar包 * 最后提交给yarn * @author * */public class WordcountDriver &#123; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); //是否运行为本地模式，就是看这个参数值是否为local，默认就是local /*conf.set("mapreduce.framework.name", "local");*/ //本地模式运行mr程序时，输入输出的数据可以在本地，也可以在hdfs上 //到底在哪里，就看以下两行配置你用哪行，默认就是file:/// /*conf.set("fs.defaultFS", "hdfs://mini1:9000/");*/ /*conf.set("fs.defaultFS", "file:///");*/ //运行集群模式，就是把程序提交到yarn中去运行 //要想运行为集群模式，以下3个参数要指定为集群上的值 /*conf.set("mapreduce.framework.name", "yarn"); conf.set("yarn.resourcemanager.hostname", "mini1"); conf.set("fs.defaultFS", "hdfs://mini1:9000/");*/ Job job = Job.getInstance(conf); job.setJar("c:/wc.jar"); //指定本程序的jar包所在的本地路径 /*job.setJarByClass(WordcountDriver.class);*/ //指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(WordcountMapper.class); job.setReducerClass(WordcountReducer.class); //指定mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); //指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); //指定需要使用combiner，以及用哪个类作为combiner的逻辑 /*job.setCombinerClass(WordcountCombiner.class);*/ job.setCombinerClass(WordcountReducer.class); //如果不设置InputFormat，它默认用的是TextInputformat.class job.setInputFormatClass(CombineTextInputFormat.class); CombineTextInputFormat.setMaxInputSplitSize(job, 4194304); CombineTextInputFormat.setMinInputSplitSize(job, 2097152); //指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); //指定job的输出结果所在目录 FileOutputFormat.setOutputPath(job, new Path(args[1])); // 指定需要缓存一个文件到所有的maptask运行节点工作目录 /* job.addArchiveToClassPath(archive); */// 缓存jar包到task运行节点的classpath中 /* job.addFileToClassPath(file); */// 缓存普通文件到task运行节点的classpath中 /* job.addCacheArchive(uri); */// 缓存压缩包文件到task运行节点的工作目录 /* job.addCacheFile(uri) */// 缓存普通文件到task运行节点的工作目录 // 将产品表文件缓存到task工作节点的工作目录中去 job.addCacheFile(new URI("file:/D:/srcdata/mapjoincache/pdts.txt")); //map端join的逻辑不需要reduce阶段，设置reducetask数量为0 job.setNumReduceTasks(0); //将job中配置的相关参数，以及job所用的java类所在的jar包，提交给yarn去运行 /*job.submit();*/ boolean res = job.waitForCompletion(true); System.exit(res?0:1); &#125;&#125; 15. GroupingComparator 分组排序见练习 或者github 注意点: 分区的数量 12345678910111213import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.Partitioner;public class ItemIdPartitioner extends Partitioner&lt;OrderBean, NullWritable&gt;&#123; @Override public int getPartition(OrderBean bean, NullWritable value, int numReduceTasks) &#123; //相同id的订单bean，会发往相同的partition //而且，产生的分区数，是会跟用户设置的reduce task数保持一致 return (bean.getItemid().hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.DoubleWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.io.WritableComparable;public class OrderBean implements WritableComparable&lt;OrderBean&gt;&#123; private Text itemid; private DoubleWritable amount; public OrderBean() &#123; &#125; public OrderBean(Text itemid, DoubleWritable amount) &#123; set(itemid, amount); &#125; public void set(Text itemid, DoubleWritable amount) &#123; this.itemid = itemid; this.amount = amount; &#125; public Text getItemid() &#123; return itemid; &#125; public DoubleWritable getAmount() &#123; return amount; &#125; @Override public int compareTo(OrderBean o) &#123; int cmp = this.itemid.compareTo(o.getItemid()); if (cmp == 0) &#123; cmp = -this.amount.compareTo(o.getAmount()); &#125; return cmp; &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(itemid.toString()); out.writeDouble(amount.get()); &#125; @Override public void readFields(DataInput in) throws IOException &#123; String readUTF = in.readUTF(); double readDouble = in.readDouble(); this.itemid = new Text(readUTF); this.amount= new DoubleWritable(readDouble); &#125; @Override public String toString() &#123; return itemid.toString() + "\t" + amount.get(); &#125;&#125; 1234567891011121314151617181920212223242526package cn.itcastcat.bigdata.secondarysort;import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;/** * 利用reduce端的GroupingComparator来实现将一组bean看成相同的key * @author duanhaitao@itcast.cn * */public class ItemidGroupingComparator extends WritableComparator &#123; //传入作为key的bean的class类型，以及制定需要让框架做反射获取实例对象 protected ItemidGroupingComparator() &#123; super(OrderBean.class, true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; OrderBean abean = (OrderBean) a; OrderBean bbean = (OrderBean) b; //比较两个bean时，指定只比较bean中的orderid return abean.getItemid().compareTo(bbean.getItemid( &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081import java.io.IOException;import org.apache.commons.lang.StringUtils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.DoubleWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import com.sun.xml.bind.v2.schemagen.xmlschema.List;/** * * @author duanhaitao@itcast.cn * */public class SecondarySort &#123; static class SecondarySortMapper extends Mapper&lt;LongWritable, Text, OrderBean, NullWritable&gt;&#123; OrderBean bean = new OrderBean(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] fields = StringUtils.split(line, ","); bean.set(new Text(fields[0]), new DoubleWritable(Double.parseDouble(fields[2]))); context.write(bean, NullWritable.get()); &#125; &#125; static class SecondarySortReducer extends Reducer&lt;OrderBean, NullWritable, OrderBean, NullWritable&gt;&#123; //到达reduce时，相同id的所有bean已经被看成一组，且金额最大的那个一排在第一位 @Override protected void reduce(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; context.write(key, NullWritable.get()); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(SecondarySort.class); job.setMapperClass(SecondarySortMapper.class); job.setReducerClass(SecondarySortReducer.class); job.setOutputKeyClass(OrderBean.class); job.setOutputValueClass(NullWritable.class); FileInputFormat.setInputPaths(job, new Path("c:/wordcount/gpinput")); FileOutputFormat.setOutputPath(job, new Path("c:/wordcount/gpoutput")); //在此设置自定义的Groupingcomparator类 job.setGroupingComparatorClass(ItemidGroupingComparator.class); //在此设置自定义的partitioner类 job.setPartitionerClass(ItemIdPartitioner.class); job.setNumReduceTasks(2); job.waitForCompletion(true); &#125;&#125;]]></content>
      <categories>
        <category>Hadoop</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce笔记-2 三大组件-Partitioner分区,sort排序,Combiner局部分区]]></title>
    <url>%2F2018%2F06%2F07%2FHadoop%2F2-MapReduce%2FMapReduce%E7%AC%94%E8%AE%B0-2%2F</url>
    <content type="text"><![CDATA[1. Combiner 组件1. 产生缘由：Combiner 是 MapReduce 程序中 Mapper 和 Reducer 之外的一种组件，它的作用是在 maptask 之后给 maptask 的结果进行局部汇总，以减轻 reducetask 的计算负载，减少网络传输 Combiner 组件的作用： 减少 reduce 端的数据量 减少 shuffle 过程的数据量 在 map 端做了一次合并，提高分布式计算程序的整体性能 Combiner 组件帮 reduce 分担压力， 因此其业务逻辑和 reduce 中的业务逻辑相似 2.自定义 Combiner 组件：默认情况下没有 Combiner 组件，Combiner 作用时间点 — map–combiner–reduce 继承 Reduce 类 public class MyCombiner extends Reducer&lt;前两个： map 的输出， 后两个： reduce 的输入&gt;{} 我们在写 MapReduce 程序的时候， map 的输出就是 reduce 的输入 也就是说， 这个 MyCombiner() 的前两个泛型和后两个泛型的类型一致 重写 reduce 方法 Combiner 本质上相当于 在 map 端进行了一次 reduce 操作， 通常情况下直接使用 reducer 的类作为 Combiner 的类，不再单独写 Combiner 代码逻辑 在 Job 中加上job.setCombinerClass(WorldcountReduce.class)， 就会调用 Combiner Combiner 使用原则 有或没有都不能影响业务逻辑，都不能影响最终结果。比如累加，最大值等，求平均值就不能用。 2、MapReduce 中的序列化2.1、概述Java 的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额 外的信息（各种校验信息，header，继承体系等），不便于在网络中高效传输； Hadoop 自己开发了一套序列化机制（参与序列化的对象的类都要实现 Writable 接口），精简，高效 Java 基本类型 &amp; Hadoop 类型对照表123456789101112131415161718// Java &amp; Hadoop类型参照hadoop数据类型 &lt;------------&gt; java数据类型: 布尔型： BooleanWritable &lt;------------&gt; boolean 整型： ByteWritable &lt;------------&gt; byte ShortWritable &lt;------------&gt; short IntWritable &lt;------------&gt; int LongWritable &lt;------------&gt; long 浮点型： FloatWritable &lt;------------&gt; float DoubleWritable &lt;------------&gt; double 字符串（文本）： Text &lt;------------&gt; String 数组： ArrayWritable &lt;------------&gt; Array map集合： MapWritable &lt;------------&gt; map 2.2、自定义对象实现 MapReduce 框架的序列化要实现WritableComparable接口，因为 MapReduce 框架中的 shuffle 过程一定会对 key 进行排序 123456789101112131415161718//序列化方法@Overridepublic void write(DataOutput out) throws IOException &#123; out.writeUTF(phone); out.writeLong(upfFlow); out.writeLong(downFlow); out.writeLong(sumFlow);&#125;//反序列化方法//注意： 字段的反序列化顺序与序列化时的顺序保持一致,並且类型也一致@Overridepublic void readFields(DataInput in) throws IOException &#123; this.phone = in.readUTF(); this.upfFlow = in.readLong(); this.downFlow = in.readLong(); this.sumFlow = in.readLong();&#125; 3. MapReduce中的Sort –TODO。。MapTask –&gt; ReduceTask 之间， 框架默认添加了排序 排序的规则是按照Map 端输出的 key 的字典顺序进行排序 1、 如果没有重写 WritableComparable 时 按单词统计中词频出现的此处进行排序， 按照出现的次数， 从低到高 如果想要对词频进行排序， 那么词频应该放在 map 输出 key 的位置 代码实现： 1234567 Map //词频为 key， 其它为 value Reduce // 将 map 输入的结果反转(k,v 换位置), 输出最终结果// 最后输出还是按照左边词, 右边次数// ps： 如果倒序排的时候, map 的时候发的时候 加上-, reduce 发的时候, 再加上-, 转成 IntWritable 2、自定义排序要实现WritableComparable接口 自定义的类必须放在 key 的位置 实现WritableComparable接口， 重写 compareTo()方法 待扩展… 作业： 增强需求： 按照总流量排序， 总流量相同时， 按照手机号码排序 4、MapReduce 中的数据分发组件 Partitioner（分区）需求： 根据归属地输出流量统计数据结果到不同文件，以便于在查询统计结果时可以定位到 省级范围进行 思路：MapReduce 中会将 map 输出的 kv 对，按照相同 key 分组，然后分发给不同的 reducetask 执行时机: 在Map输出 kv 对之后, 所携带的 k,v 参数，跟 Map 输出相同 MapReduce 默认的分发规则为：根据 key 的 hashcode%reducetask 数来分发，所以：如果要按照我们自 己的需求进行分组，则需要改写数据分发（分区）组件 Partitioner Partition重点总结： Partition 的 key value, 就是Mapper输出的key value public abstract int getPartition(KEY key, VALUE value, int numPartitions); 输入是Map的结果对&lt;key, value&gt;和Reducer的数目，输出则是分配的Reducer（整数编号）。就是指定Mappr输出的键值对到哪一个reducer上去。系统缺省的Partitioner是HashPartitioner，它以key的Hash值对Reducer的数目取模，得到对应的Reducer。这样保证如果有相同的key值，肯定被分配到同一个reducre上。如果有N个reducer，编号就为0,1,2,3……(N-1)。 MapReduce 中会将 map 输出的 kv 对，按照相同 key 分组，然后分发给不同的 reducetask 默认的分发规则为:根据 key 的 hashcode%reducetask 数来分发，所以:如果要按照我们自 己的需求进行分组，则需要改写数据分发(分组)组件 Partitioner, 自定义一个 CustomPartitioner 继承抽象类:Partitioner 因此， Partitioner 的执行时机， 是在Map输出 kv 对之后 Partitioner 实现过程 先分析一下具体的业务逻辑，确定大概有多少个分区 首先书写一个类，它要继承 org.apache.hadoop.mapreduce.Partitioner这个抽象类 重写public int getPartition这个方法，根据具体逻辑，读数据库或者配置返回相同的数字 在main方法中设置Partioner的类，job.setPartitionerClass(DataPartitioner.class); 设置Reducer的数量，job.setNumReduceTasks(6); 典型的 Partitioner 代码实现12345678910111213141516171819202122import java.util.HashMap;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; &#123; private static HashMap&lt;String, Integer&gt; provincMap = new HashMap&lt;String, Integer&gt;(); static &#123; provincMap.put("138", 0); provincMap.put("139", 1); provincMap.put("136", 2); provincMap.put("137", 3); provincMap.put("135", 4); &#125; @Override public int getPartition(Text key, FlowBean value, int numPartitions) &#123; Integer code = provincMap.get(key.toString().substring(0, 3)); if (code != null) &#123; return code; &#125; return 5; &#125;&#125; 5、全局计数器1. 框架内置计数器： Hadoop内置的计数器，主要用来记录作业的执行情况 内置计数器包括 MapReduce框架计数器（Map-Reduce Framework） 文件系统计数器（FielSystemCounters） 作业计数器（Job Counters） 文件输入格式计数器（File Output Format Counters） 文件输出格式计数器（File Input Format Counters) 计数器由相关的task进行维护，定期传递给tasktracker，再由tasktracker传给jobtracker； 最终的作业计数器实际上是有jobtracker维护，所以计数器可以被全局汇总，同时也不必在整个网络中传递 只有当一个作业执行成功后，最终的计数器的值才是完整可靠的； 2. 自定义的计数器应用场景 用来统计运行过程中的进度和状态， 类似于 job 运行的一个报告、日志 要将数据处理过程中遇到的不合规数据行进行全局计数，类似这 种需求可以借助 MapReduce 框架中提供的全局计数器来实现 计数器的值可以在mapper或reducer中增加 使用方式 定义枚举类 1234enum Temperature&#123; MISSING, TOTAL &#125; 在map或者reduce中使用计数器 123456// 1.自定义计数器Counter counter = context.getCounter(Temperature.TOTAL); // 2.为计数器赋初始值counter.setValue(long value);// 3.计数器工作counter.increment(long incr); 获取计数器 123Counters counters=job.getCounters(); Counter counter=counters.findCounter(LOG_PROCESSOR_COUNTER.BAD_RECORDS_LONG);// 查找枚举计数器，假如Enum的变量为BAD_RECORDS_LONG long value=counter.getValue();//获取计数值 计数器使用完整代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * @Description 假如一个文件，规范的格式是3个字段，“\t”作为分隔符，其中有2条异常数据，一条数据是只有2个字段，一条数据是有4个字段*/public class MyCounter &#123; // \t键 private static String TAB_SEPARATOR = "\t"; public static class MyCounterMap extends Mapper&lt;LongWritable, Text, Text, Text&gt; &#123; // 定义枚举对象 public static enum LOG_PROCESSOR_COUNTER &#123; BAD_RECORDS_LONG, BAD_RECORDS_SHORT &#125;; protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String arr_value[] = value.toString().split(TAB_SEPARATOR); if (arr_value.length &gt; 3) &#123; /* 自定义计数器 */ context.getCounter("ErrorCounter", "toolong").increment(1); /* 枚举计数器 */ context.getCounter(LOG_PROCESSOR_COUNTER.BAD_RECORDS_LONG).increment(1); &#125; else if (arr_value.length &lt; 3) &#123; // 自定义计数器 context.getCounter("ErrorCounter", "tooshort").increment(1); // 枚举计数器 context.getCounter(LOG_PROCESSOR_COUNTER.BAD_RECORDS_SHORT).increment(1); &#125; &#125; &#125; @SuppressWarnings("deprecation") public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException &#123; String[] args0 = &#123; "hdfs://hadoop2:9000/buaa/counter/counter.txt", "hdfs://hadoop2:9000/buaa/counter/out/" &#125;; // 读取配置文件 Configuration conf = new Configuration(); // 如果输出目录存在，则删除 Path mypath = new Path(args0[1]); FileSystem hdfs = mypath.getFileSystem(conf); if (hdfs.isDirectory(mypath)) &#123; hdfs.delete(mypath, true); &#125; // 新建一个任务 Job job = new Job(conf, "MyCounter"); // 主类 job.setJarByClass(MyCounter.class); // Mapper job.setMapperClass(MyCounterMap.class); // 输入目录 FileInputFormat.addInputPath(job, new Path(args0[0])); // 输出目录 FileOutputFormat.setOutputPath(job, new Path(args0[1])); // 提交任务，并退出 System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; 注意点：在没有 ReduceTask 的时候， job.setNumReduceTasks(0); 关于计数器，详情可参考]]></content>
      <categories>
        <category>Hadoop</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce笔记-1 WordCount, MapReduce运行机制]]></title>
    <url>%2F2018%2F06%2F06%2FHadoop%2F2-MapReduce%2FMapReduce%E7%AC%94%E8%AE%B0-1%2F</url>
    <content type="text"><![CDATA[参考链接: hdfs 笔记 mapreduce 笔记 1、MapReduce 入门1.1、MapReduce概念hadoop 的四大组件： HDFS：分布式存储系统 MapReduce：分布式计算系统 YARN：hadoop 的资源调度系统 Common：以上三大组件的底层支撑组件，主要提供基础工具包和 RPC 框架等 MapReduce 是一个分布式运算程序的编程框架，是用户开发“基于 Hadoop 的数据分析应用” 的核心框架 MapReduce 核心功能 ：将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布 式运算程序，并发运行在一个 Hadoop 集群上 1.2、为什么需要 MapReduce？引入 MapReduce 框架后，开发人员可以将绝大部分工作集中在业务逻辑的开发上，而将 分布式计算中的复杂性交由框架来处理 Hadoop 当中的 MapReduce 分布式程序运算框架的整体结构如下： MRAppMaster：MapReduce Application Master，分配任务，协调任务的运行 MapTask：阶段并发任，负责 mapper 阶段的任务处理 YARNChild ReduceTask：阶段汇总任务，负责 reducer 阶段的任务处理 YARNChild 1.3、MapReduce 的编写规范MapReduce 程序编写规范： 用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行 MR 程序的客户端) Mapper 的输入数据是 KV 对的形式（KV 的类型可自定义） Mapper 的输出数据是 KV 对的形式（KV 的类型可自定义） Mapper 中的业务逻辑写在 map()方法中 map()方法（maptask 进程）对每一个&lt;K,V&gt;调用一次 Reducer 的输入数据类型对应 Mapper 的输出数据类型，也是 KV 对的形式 Reducer 的业务逻辑写在 reduce()方法中 Reducetask 进程对每一组相同 k 的&lt;K,V&gt;组调用一次 reduce()方法 用户自定义的 Mapper 和 Reducer 都要继承各自的父类 整个程序需要一个 Drvier 来进行提交，提交的是一个描述了各种必要信息的 job 对象 1.4、WordCount 程序1、业务逻辑 maptask阶段处理每个数据分块的单词统计分析，思路是每遇到一个单词则把其转换成一个 key-value对，比如单词 hello，就转换成&lt;’hello’,1&gt;发送给 reducetask去汇总 reducetask阶段将接受 maptask的结果，来做汇总计数 2、具体代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465 Map static class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 计算任务代码：切割单词，输出每个单词计 1 的 key-value 对 String[] words = value.toString().split(" "); for (String word : words) &#123; context.write(new Text(word), new IntWritable(1)); &#125; &#125;&#125; Reduce static class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; // 汇总计算代码：对每个 key 相同的一组 key-value 做汇总统计 int sum = 0; for (IntWritable v : values) &#123; sum += v.get(); &#125; context.write(key, new IntWritable(sum)); &#125;&#125; main public static void main(String[] args) throws Exception &#123; // 指定 hdfs 相关的参数 Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://hadoop02:9000"); System.setProperty("HADOOP_USER_NAME", "hadoop"); // 新建一个 job 任务 Job job = Job.getInstance(conf); // 设置 jar 包所在路径 job.setJarByClass(WordCountMR.class); // 指定 mapper 类和 reducer 类 job.setMapperClass(WordCountMapper.class); job.setReducerClass(WordCountReducer.class); // 指定 maptask 的输出类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 指定 reducetask 的输出类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 指定该 mapreduce 程序数据的输入和输出路径 Path inputPath = new Path("/wordcount/input"); Path outputPath = new Path("/wordcount/output"); FileInputFormat.setInputPaths(job, inputPath); FileOutputFormat.setOutputPath(job, outputPath); // 最后提交任务 boolean waitForCompletion = job.waitForCompletion(true); System.exit(waitForCompletion?0:1);&#125; 1.5 小文件优化 需要设置最大值和最小值, 会切分文件, 在最大值与最小值之间 2、MapReduce 程序的核心运行机制2.1、概述一个完整的 MapReduce 程序在分布式运行时有两类实例进程： MRAppMaster：负责整个程序的过程调度及状态协调 Yarnchild：负责 map 阶段的整个数据处理流程 Yarnchild：负责 reduce 阶段的整个数据处理流程 以上两个阶段 MapTask 和 ReduceTask 的进程都是 YarnChild ， 并不是说这 MapTask 和 ReduceTask 就跑在同一个 YarnChild 进行里 2.2、MapReduce 程序的运行流程 一个 mr 程序启动的时候，最先启动的是 MRAppMaster，MRAppMaster 启动后根据本次 job 的描述信息，计算出需要的 maptask 实例数量，然后向集群申请机器启动相应数量的 maptask 进程 maptask 进程启动之后，根据给定的数据切片(哪个文件的哪个偏移量范围)范围进行数 据处理，主体流程为： 利用客户指定的 InputFormat 来获取 RecordReader 读取数据，形成输入 KV 对 将输入 KV 对传递给客户定义的 map()方法，做逻辑运算，并将 map()方法输出的 KV 对收 集到缓存 将缓存中的 KV 对按照 K 分区排序后不断溢写到磁盘文件 MRAppMaster 监控到所有 maptask 进程任务完成之后（真实情况是，某些 maptask 进 程处理完成后，就会开始启动 reducetask 去已完成的 maptask 处 fetch 数据），会根据客户指 定的参数启动相应数量的 reducetask 进程，并告知 reducetask 进程要处理的数据范围（数据 分区） Reducetask 进程启动之后，根据 MRAppMaster 告知的待处理数据所在位置，从若干台 maptask 运行所在机器上获取到若干个 maptask 输出结果文件，并在本地进行重新归并排序， 然后按照相同 key 的 KV 为一个组，调用客户定义的 reduce()方法进行逻辑运算，并收集运 算输出的结果 KV，然后调用客户指定的 OutputFormat 将结果数据输出到外部存储 2.3、MapTask 并行度决定机制将待处理数据执行逻辑切片（即按照一个特定切片大小，将待处理数据划分成逻辑上的多 个 split），然后每一个 split 分配一个 mapTask 并行实例处理。 这段逻辑及形成的切片规划描述文件，是由 FileInputFormat 实现类的 getSplits()方法完成的。 该方法返回的是 List，InputSplit 封装了每一个逻辑切片的信息，包括长度和位置 信息，而 getSplits()方法返回一组 InputSplit。 2.4、切片机制 FileInputFormat 中默认的切片机制 简单地按照文件的内容长度进行切片 切片大小，默认等于 block 大小 切片时不考虑数据集整体，而是逐个针对每一个文件单独切片 比如待处理数据有两个文件： File1.txt 200M File2.txt 100M 经过 getSplits()方法处理之后，形成的切片信息是： File1.txt-split1 0-128M File1.txt-split2 129M-200M File2.txt-split1 0-100M FileInputFormat 中切片的大小的参数配置1234567891011// 通过分析源码，在 FileInputFormat 中，计算切片大小的逻辑：long splitSize = computeSplitSize(blockSize, minSize, maxSize)，翻译一下就是求这三个值的中 间值// 切片主要由这几个值来运算决定：blocksize：默认是 128M，可通过 dfs.blocksize 修改minSize：默认是 1，可通过 mapreduce.input.fileinputformat.split.minsize 修改maxsize：默认是 Long.MaxValue，可通过 mapreduce.input.fileinputformat.split.maxsize 修改//因此，如果 maxsize 调的比 blocksize 小，则切片会小于 blocksize; 如果 minsize 调的比 blocksize 大，则切片会大于 blocksize// 但是，不论怎么调参数，都不能让多个小文件“划入”一个 split 2.5、MapTask 并行度经验之谈如果硬件配置为 2*12core + 64G，恰当的 map 并行度是大约每个节点 20-100 个 map，最好 每个 map 的执行时间至少一分钟。 如果 job 的每个 map 或者 reduce task 的运行时间都只有 30-40 秒钟，那么就减少该 job 的 map 或者 reduce 数，每一个 task(map|reduce)的 setup 和加入到调度器中进行调度，这个 中间的过程可能都要花费几秒钟，所以如果每个 task 都非常快就跑完了，就会在 task 的开 始和结束的时候浪费太多的时间。 配置 task 的 JVM 重用可以改善该问题： mapred.job.reuse.jvm.num.tasks，默认是 1，表示一个 JVM 上最多可以顺序执行的 task 数目（属于同一个 Job）是 1。也就是说一个 task 启一个 JVM。 这个值可以在 mapred-site.xml 中进行更改，当设置成多个，就意味着这多个 task 运行在同一个 JVM 上，但不是同时执行， 是排队顺序执行 如果 input 的文件非常的大，比如 1TB，可以考虑将 hdfs 上的每个 blocksize 设大，比如 设成 256MB 或者 512MB 2.6、ReduceTask 并行度决定机制reducetask 的并行度同样影响整个 job 的执行并发度和执行效率，但与 maptask 的并发数由 切片数决定不同，Reducetask 数量的决定是可以直接手动设置： 12// 设置 ReduceTask 的并行度job.setNumReduceTasks(4); 默认值是 1， 手动设置为 4，表示运行 4 个 reduceTask， 设置为 0，表示不运行 reduceTask 任务，也就是没有 reducer 阶段，只有 mapper 阶段 如果数据分布不均匀，就有可能在 reduce 阶段产生数据倾斜 注意：reducetask 数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有 1 个 reducetask 尽量不要运行太多的 reducetask。对大多数 job 来说，最好 rduce 的个数最多和集群中的 reduce 持平，或者比集群的 reduce slots 小。这个对于小集群而言，尤其重要。 最好的ReduceTask 个数是：datanode 个数 * 0.75~0.95 左右 2.7 客户端提交 MR 程序 Job 的流程 3. 昨日复习1.MapReduce 的 wc 编程 手写代码 Mapper Reducer Driver 2.MapTask 的并行度 在程序执行的时候运行的 maptask 的总个数 3.ReduceTask的并行度问题 ReduceTask 的并行度设置依赖于自己传入的参数 一般经验： ReduceTask 的个数应该 = datanode 的阶段数 * （0.75~0.95） ReduceTask 在设置的时候的并行度有一定的瓶颈 分区： 决定 ReduceTask 中的数据怎么分配的 默认分区方式 自定义分区]]></content>
      <categories>
        <category>Hadoop</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[▍Do Not Go Gentle into That Good Night]]></title>
    <url>%2F2018%2F06%2F04%2FPoetry%2F%E4%B8%8D%E8%A6%81%E6%B8%A9%E5%92%8C%E7%9A%84%E8%B5%B0%E8%BF%9B%E9%82%A3%E4%B8%AA%E8%89%AF%E5%A4%9C%2F</url>
    <content type="text"><![CDATA[Do not go gentle into that good night, Old age should burn and rave at close of the day; Rage, rage against the dying of the light. Though wise men at their end know dark is right, Because their words had forked no lightning they Do not go gentle into that good night. Good men, the last wave by, crying how bright Their frail deeds might have danced in a green bay, Rage, rage against the dying of the light. Wild men, who caught and sang the sun in flight, And learn, too late, they grieved it on its way, Do not go gentle into that good night. Grave men, near death, who see with blinding sight Blind eyes could blaze like meteors and be gay, Rage, rage against the dying of the light. And you, my father, there on the sad height, Curse, bless, me now with your fierce tears, I pray. Do not go gentle into that good night. Rage, rage against the dying of the light. 《不要温和地走进那个良夜》 -巫宁坤译本 ​ 不要温和地走进那个良夜， 老年应当在日暮时燃烧咆哮； 怒斥，怒斥光明的消逝。 虽然智慧的人临终时懂得黑暗有理， 因为他们的话没有进发出闪电，他们 也并不温和地走进那个良夜。 善良的人，当最后一浪过去，高呼他们脆弱的善行 可能曾会多么光辉地在绿色的海湾里舞蹈， 怒斥，怒斥光明的消逝。 狂暴的人抓住并歌唱过翱翔的太阳， 懂得，但为时太晚，他们使太阳在途中悲伤， 也并不温和地走进那个良夜。 严肃的人，接近死亡，用炫目的视觉看出 失明的跟睛可以像流星一样闪耀欢欣， 怒斥，恕斥光明的消逝。 您啊，我的父亲，在那悲哀的高处。 现在用您的热泪诅咒我，祝福我吧。我求您 不要温和地走进那个良夜。 怒斥，怒斥光明的消逝 《不要温顺地走入那长夜》 -和菜头译本 白日将尽，暮年仍应燃烧咆哮 狂怒吧，狂怒吧！ 对抗着光明渐逝 虽然智者深知 人之将死，黑暗自有其时 只因他们所言未曾裂天如电 他们不要温顺地走入那长夜 随着最后一浪，善人在哭喊 哭喊那脆弱的善行 它本应何其欢快 在绿色峡湾里起舞 狂怒吧，狂怒吧！ 对抗着光明渐逝。 狂人曾抓住飞驰的太阳 放声歌唱 太晚，他们才感到其中的伤感 不要温顺地走进那长夜 严肃的人行将死去时 用那渐渐失神的目光去看 盲瞳却如流星璀璨，欢欣溢满 狂怒吧，狂怒吧！ 对抗着光明渐逝 还有你啊，我的父亲，远在悲伤的高地 我恳请你现在 就让你诅咒，你的祝福 随着热泪落下 不要温顺地走进那长夜 狂怒吧，狂怒吧！ 对抗这光明渐逝 ​ 《 绝不向黑夜请安》 -高晓松译本 绝不向黑夜请安 老朽请于白日尽头涅槃 咆哮于光之消散 先哲虽败于幽暗 诗歌终不能将苍穹点燃 绝不向黑夜请安 贤者舞蹈于碧湾 为惊涛淹没的善行哭喊 咆哮于光之消散 狂者如夸父逐日 高歌中顿觉迟来的伤感 绝不向黑夜请安 逝者于临终迷幻 盲瞳怒放出流星的灿烂 咆哮于光之消散 那么您，我垂垂将死的父亲 请掬最后一捧热泪降临 请诅咒，请保佑 我祈愿，绝不向 黑夜请安，咆哮 于光之消散]]></content>
      <categories>
        <category>文艺</category>
        <category>诗歌</category>
      </categories>
      <tags>
        <tag>诗歌</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop学习笔记-3 HDFS 原理剖析]]></title>
    <url>%2F2018%2F06%2F03%2FHadoop%2F1-HDFS%2FHDFS-2-%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[1. HDFS体系结构主从。。。 2.NameNode概念 [x] 是整个文件系统的管理节点。它维护着整个文件系统的文件目录树，文件/目录的元信息和每个文件对应的数据块列表。接收用户的操作请求。 [x] 在hdfs-site.xml中的dfs.namenode.name.dir属性 [x] 文件包括： [x] 文件包括: ①fsimage:元数据镜像文件。存储某一时段NameNode内存元数据信息。 ②edits:操作日志文件。 ③fstime:保存最近一次checkpoint的时间 以上这些文件是保存在linux的文件系统中。 查看 fsimage 和 edits的内容 查看 NameNode中 fsimage 的内容 查看 fsimage镜像文件内容Usage: bin/hdfs oiv [OPTIONS] -i INPUTFILE -o OUTPUTFILE 1234567891011121314# 可以知道数据存在那个哪个 fsimage 镜像中------------------------------------# 使用离线的查看器 输出到网页查看oiv -i hadoopdata/namenode/current/fsimage_0000000000000000250 -o 0000000000000000250# 出现这样的提示INFO offlineImageViewer.WebImageViewer: WebImageViewer started. Listening on /127.0.0.1:5978. Press Ctrl+C to stop the viewer.# 另起一个窗口查看hadoop fs -ls -R webhdfs://127.0.0.1:5978------------------------------------# 也可以导出到 xml 文件bin/hdfs oiv -p XML -i tmp/dfs/name/current/fsimage_0000000000000000055 -o fsimage.xml 查看edits文件， 也可以导出到 xml 12# 查看edtis内容bin/hdfs oev -i tmp/dfs/name/current/edits_0000000000000000057-0000000000000000186 -o edits.xml 3. Datanode提供真实文件数据的存储服务 Datanode 节点的数据切块存储位置 ~/hadoopdata/datanode/current/BP-1070660005-192.168.170.131-1527865615372/current/finalized/subdir0/subdir0 12345678[ap@cs2]~/hadoopdata/datanode/current/BP-1070660005-192.168.170.131-1527865615372/current/finalized/subdir0/subdir0% ll总用量 213340-rw-r--r-- 1 ap ap 134217728 6月 2 13:35 blk_1073741842-rw-r--r-- 1 ap ap 1048583 6月 2 13:35 blk_1073741842_1018.meta-rw-r--r-- 1 ap ap 82527955 6月 2 13:35 blk_1073741843-rw-r--r-- 1 ap ap 644759 6月 2 13:35 blk_1073741843_1019.meta-rw-r--r-- 1 ap ap 13 6月 3 02:12 blk_1073741850-rw-r--r-- 1 ap ap 11 6月 3 02:12 blk_1073741850_1028.meta 文件块（block）：最基本的存储单位。对于文件内容而言，一个文件的长度大小是size，那么从文件的０偏移开始，按照固定的大小，顺序对文件进行划分并编号，划分好的每一个块称一个Block。HDFS默认Block大小是128MB，以一个256MB文件，共有256/128=2个Block. 不同于普通文件系统的是，HDFS中，如果一个文件小于一个数据块的大小，并不占用整个数据块存储空间, 按文件大小的实际容量存储 Replication。多复本。默认是三个。 hdfs-site.xml的dfs.replication属性 手动设置某个文件的副本数为3个 bin/hdfs dfs -setrep 3 /a.txt 4. 数据存储： 写文件解析 [x] 疑点： HDFS client上传数据到HDFS时，会首先在本地缓存数据，当数据达到一个block大小时，请求NameNode分配一个block。NameNode会把block所在的DataNode的地址告诉HDFS client。HDFS client会直接和DataNode通信，把数据写到DataNode节点一个block文件中。 问题： 如果一直写的数据都没有达到一个 block 大小， 那怎么存储？？ 写文件的过程： 首先调用FileSystem对象的open方法，其实是一个DistributedFileSystem的实例 DistributedFileSystem通过rpc获得文件的第一批个block的locations，同一block按照重复数会返回多个locations，这些locations按照hadoop拓扑结构排序，距离客户端近的排在前面. 前两步会返回一个FSDataInputStream对象，该对象会被封装成DFSInputStream对象，DFSInputStream可以方便的管理datanode和namenode数据流。客户端调用read方法，DFSInputStream最会找出离客户端最近的datanode并连接。 数据从datanode源源不断的流向客户端。 如果第一块的数据读完了，就会关闭指向第一块的datanode连接，接着读取下一块。这些操作对客户端来说是透明的，客户端的角度看来只是读一个持续不断的流。 如果第一批block都读完了，DFSInputStream就会去namenode拿下一批blocks的location，然后继续读，如果所有的块都读完，这时就会关闭掉所有的流。 如果在读数据的时候，DFSInputStream和datanode的通讯发生异常，就会尝试正在读的block的排第二近的datanode,并且会记录哪个datanode发生错误，剩余的blocks读的时候就会直接跳过该datanode。DFSInputStream也会检查block数据校验和，如果发现一个坏的block,就会先报告到namenode节点，然后DFSInputStream在其他的datanode上读该block的镜像 该设计的方向就是客户端直接连接datanode来检索数据并且namenode来负责为每一个block提供最优的datanode，namenode仅仅处理block location的请求，这些信息都加载在namenode的内存中，hdfs通过datanode集群可以承受大量客户端的并发访问。 5. 数据存储： 读文件解析 读文件的过程 首先调用FileSystem对象的open方法，其实是一个DistributedFileSystem的实例 DistributedFileSystem通过rpc获得文件的第一批个block的locations，同一block按照重复数会返回多个locations，这些locations按照hadoop拓扑结构排序，距离客户端近的排在前面. 前两步会返回一个FSDataInputStream对象，该对象会被封装成DFSInputStream对象，DFSInputStream可以方便的管理datanode和namenode数据流。客户端调用read方法，DFSInputStream最会找出离客户端最近的datanode并连接。 数据从datanode源源不断的流向客户端。 如果第一块的数据读完了，就会关闭指向第一块的datanode连接，接着读取下一块。这些操作对客户端来说是透明的，客户端的角度看来只是读一个持续不断的流。 如果第一批block都读完了，DFSInputStream就会去namenode拿下一批blocks的location，然后继续读，如果所有的块都读完，这时就会关闭掉所有的流。 如果在读数据的时候，DFSInputStream和datanode的通讯发生异常，就会尝试正在读的block的排第二近的datanode,并且会记录哪个datanode发生错误，剩余的blocks读的时候就会直接跳过该datanode。DFSInputStream也会检查block数据校验和，如果发现一个坏的block,就会先报告到namenode节点，然后DFSInputStream在其他的datanode上读该block的镜像 该设计的方向就是客户端直接连接datanode来检索数据并且namenode来负责为每一个block提供最优的datanode，namenode仅仅处理block location的请求，这些信息都加载在namenode的内存中，hdfs通过datanode集群可以承受大量客户端的并发访问。 6.Hadoop Archives （HAR files）Hadoop Archives (HAR files)是在0.18.0版本中引入的，它的出现就是为了缓解大量小文件消耗namenode内存的问题。HAR文件是通过在HDFS上构建一个层次化的文件系统来工作。一个HAR文件是通过hadoop的archive命令来创建，而这个命令实 际上也是运行了一个MapReduce任务来将小文件打包成HAR。对于client端来说，使用HAR文件没有任何影响。所有的原始文件都 （using har://URL）。但在HDFS端它内部的文件数减少了。 通过HAR来读取一个文件并不会比直接从HDFS中读取文件高效，而且实际上可能还会稍微低效一点，因为对每一个HAR文件的访问都需要完成两层 index文件的读取和文件本身数据的读取。并且尽管HAR文件可以被用来作为MapReduce job的input，但是并没有特殊的方法来使maps将HAR文件中打包的文件当作一个HDFS文件处理。 打包出来的 har 文件在xxx.har/part-0 中， contentz-size 跟原来的文件总大小一样 创建文件 hadoop archive -archiveName xxx.har -p /src /dest查看内容 hadoop fs -lsr har:///dest/xxx.har 可以原封不动的显示出来 123456789101112131415# 打包成 harhadoop archive -archiveName test.har -p /user/test /# 查看har 文件[ap@cs1]~% hadoop fs -count /test.har/part-0 0(目录数) 1(文件数) 72(文件大小) /test.har/part-0 (文件名)# 查看打包前文件[ap@cs1]~% hadoop fs -count /user/test 1 2 72 /user/test # 查看 har 文件， 把打包前的原本文件都显示出来了[ap@cs1]~% hadoop fs -ls -R har:///test.har-rw-r--r-- 3 ap supergroup 50 2018-06-03 04:24 har:///test.har/a.txt-rw-r--r-- 3 ap supergroup 22 2018-06-03 04:24 har:///test.har/b.txt 注意点： 存储层面：为了解决小文件过多导致的 Namenode 压力过大问题， 把很多小文件打包成一个 har 文件。 使用层面： 但是实际处理的时候， 还是会还原出原本的小文件进行处理， 不会把 har 文件当成一个 HDFS 文件处理。 HDFS 上不支持 tar， 只支持 har打包 7.HDFS 的 HA]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[▍回答]]></title>
    <url>%2F2018%2F06%2F03%2FPoetry%2F%E5%9B%9E%E7%AD%94%2F</url>
    <content type="text"><![CDATA[卑鄙是卑鄙者的通行证，高尚是高尚者的墓志铭，看吧，在那镀金的天空中，飘满了死者弯曲的倒影。 冰川纪过去了，为什么到处都是冰凌？好望角发现了，为什么死海里千帆相竞？ 我来到这个世界上，只带着纸、绳索和身影，为了在审判之前，宣读那些被判决的声音。 告诉你吧，世界我–不–相–信！纵使你脚下有一千名挑战者，那就把我算作第一千零一名。 我不相信天是蓝的，我不相信雷的回声，我不相信梦是假的，我不相信死无报应。 如果海洋注定要决堤，就让所有的苦水都注入我心中，如果陆地注定要上升，就让人类重新选择生存的峰顶。 新的转机和闪闪星斗，正在缀满没有遮拦的天空。那是五千年的象形文字，那是未来人们凝视的眼睛。 作者 / 北岛]]></content>
      <categories>
        <category>文艺</category>
        <category>诗歌</category>
      </categories>
      <tags>
        <tag>诗歌</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop学习笔记-2 HDFS基础入门]]></title>
    <url>%2F2018%2F06%2F02%2FHadoop%2F1-HDFS%2FHDFS-1-%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[Hadoop的核心组件 之 HDFSHDFS1. HDFS是什么: 分布式文件系统 2. HDFS 设计思想 分而治之, 切分存储, 当一个文件过大的时候, 一个节点存储不了, 采用切分存储 分块存储: 每一个块叫做 block 如果一个8T的数据, 这个怎么分合适??? 设置分块的时候要考虑一个事情 : 负载均衡 块的大小不能太大, 太大会造成负载不均衡 hadoop2.x 中默认的切分的块的大小是: 128M, 1.x中默认的是64M 如果一个文件不足128M, 也会单独存一个快, 快的大小就是存储数据的实际大小 这个分块存储思想中, 如果一个块的存储节点宕机了, 这个时候, 数据的安全性得不到保证了 HDFS中默认块的存储采用备份机制 默认的备份个数是3个(总共存的, 存到datanode上的, namenode不存), 之前自己配的是2个, 所有备份相同地位是相同的. 相同的数据块的备份一定存储在不同的节点上 如果节点总共2个, dfs.replication=3 副本个数是3个, 实际存储2个, 另一个进行记账, 当集群节点个数大于3个时, 会复制这个副本, 最终达到3个 假设集群中的节点4个, 副本3个, 有一个副本的机器宕机了, 这个时候发现副本的个数 小于 设定的个数, 就会进行复制, 达到3个副本, 如果 这个时候, 刚才宕机的节点又恢复了, 这个时候集群副本个数为4了, 集群会等待一段时间, 如果发现还是4个, 就会删除一个副本, 达到3个(设定值) 备份越多越好吗? 理论上副本数越多, 数据安全性越高 但是副本数越多, 会占用过多的存储资源, 会造成集群的维护变得越困难 100 个节点, 50个副本, 在这50个副本中, 随时都有可能宕机, hdfs就需要维护副本 一般情况下, 3个就可以了 hadoop是基于廉价的pc机设计的, 会造成机器随时可能宕机 HDFS的目录结构 hdfs的目录结构与linux 操作系统类似, 以 /为跟节点, 我们将这个目录🌲称为抽象目录树 因为hdfs的目录结构代表的是所有数据节点的抽象出来的目录, 不代表任何一个节点 hdfs: /hadoop.zip 500M 被分成4个块存储 hdfs中存储的数据块 是有编号的, blk_1, blk_2, blk_3, blk_4 /spark.zip 300M 3个块, blk_5 blk_6 blk_7 底层存储的时候, 每一个block都有一个唯一的id hdfs的数据底层存储的时候吗, 还是存在真正的物理节点上. 2. HDFS 的整体结构主从结构: 一个主节点, 多个从节点 namenode: 用于存储元数据, 包括: 抽象目录树 存储数据和block的对应关系 block存储的位置 处理客户端的读写请求 读: 下载 写: 上传 datanode 负责真正的数据存储, 存储数据的block 真正处理读写 secondarynamenode: 冷备份节点: 助理 当namenode宕机的时候, secondarynamenode不能主动切换为 namenode, 但是 secondarynamenode中存储的数据与namenode相同. 主要作用: namenode宕机的时候, 帮助namenode恢复 帮助namenode做一些事情, 分担namenode的压力 3. HDFS优缺点: 优点: 可构建在廉价机器上, 成本低, 通过多副本提高可靠性, 提供了容错和恢复机制 高容错性 容错性: 数据访问上, 一个节点数据丢失, 不影响整体的数据访问 数据自动保存多个副本, 副本丢失后, 自动恢复, 最终恢复到用户配置的副本个数 适合批处理, 适合离线数据处理 移动计算而非数据, 数据位置暴露给计算框架 适合大数据处理 GB, TB 甚至 PB 级数据, 百万规模以上的文件数量, 10k+ 节点规模 流式文件访问, 不支持数据修改, hdfs用于数据存储 一次性写入, 多次读取, 保证数据一致性 缺点: 不支持低延迟的数据访问, 不支持 实时/近实时 数据访问, 因为涉及到多轮RPC调用 向 NameNode 寻址.. 拿到地址后， 向 DataNode 请求数据.. 不擅长存储大量的小文件–kb级别的 寻址时间可能大于读取数据的时间, 不划算 进行数据访问的时候先找元数据 元数据是和block对应的, 1个block块对应一条元数据 1000w个1kb的文件, 存了1000w个块 — 1000w元数据 在进行数据访问的时候可能花了 1s 的时间, 总体上不划算 这样会造成元数据存储量过大, 增加namenode的压力 在hdfs中一般情况下, 一条元数据大小 150byte 左右 1000w条元数据 — 1000w * 150, 1.5G左右 不支持文件内容修改, 仅仅支持文件末尾追加 append， 一个文件同时只能有一个写者，不支持并发操作 ==4. HDFS 的 常用命令:==HDFS归根结底就是一个文件系统, 类似于 linux, 需要用命令来操作 1. hapdoop fs 命令 hadoop fs / hdfs dfs 效果是一样的 在hadoop中查看, 只有绝对路径的访问方式 查看帮助 hadoop fs -help 查看所有 hadoop fs的帮助 hadoop fs -help ls 查看 fs下的 ls的帮助 列出根目录: hadoop fs -ls / hadoop fs -ls -R / 递归展示 hadoop fs -ls -R -h /友好展示， 展示文件大小单位 如果不指定目录， 会默认找当前用户xx对应的/user/xx的目录 递归创建 -mkdir -p: hadoop fs -mkdir -p /aa/bb/cc/dd 不加 -p 为普通创建 创建空文件-touchz 类似于 Linux 下的 touch 上传 put: [-put [-f][-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;] 上传一个: hadoop fs -put hadoop-2.7.6.tar.gz /ss 上传多个: hadoop fs -put aa.txt bb.txt /ss 下载 get hadoop fs -get hdfs路径 本地路径 合并下载 getmerge hadoop fs -getmerge /ss/aa.txt /ss/bb.txt /home/ap/cc.txt 会将最后一个路径之前8的当做需要合并的文件, 最后一个路径中指定的文件就是合并生成的文件 查看文件内容 cat -cat 查看文件内容 -text也是类似 删除文件 rm rm -rf (错误的) rm -r(递归) -f(强制) 文件hadoop fs -rm -f /ss/aa.txt 文件夹 hadoop fs -rm -f -r /aa mv 修改名字, 移动 移动的文件从 hdfs 到 hdfs hadoop fs -mv .. .. cp 复制 hadoop fs -cp /hdfsfile /hdfsfile: 从 hdfs 复制到 hdfs 参数 -p ： 复制后保持文件的原本属性, 时间戳， 权限等 Passing -p preserves status [topax] (timestamps, ownership, permission, ACLs, XAttr). 参数 -f : 已有同名文件的话， 直接覆盖 在末尾追加: -appendToFile 本地文件 hdfs文件` 将本地文件bb.txt 追加到 htfd的 /aa/aa.txt 上 hadoop fs -appendToFile aa.txt /ss/bb.txt 从命令行追加 , 但是不知道怎么结束， 先存疑？？ hadoop fs -appendToFile - /a.txt` 这个追加是在原始块的末尾追加的. 会改变集群上的文件 如果超过128M才会进行切分, 但这个命令一般不会使用 查看文件，文件夹数量 count DIR_COUNT FILE_COUNT CONTENT_SIZE FILE_NAME 8 3 176.5 K /tmp hadoop fs -count -h /tmp: -h 是友好展示 hdfs dfs -count -h /tmp: 与上面效果一样 hdfs dfs -count -q -h /tmp: 查看文件配额， 具体看 help du： 展示文件大小， 如果参数是文件夹， 则展示文件夹下文件的大小 hadoop fs -du -h /tmp hadoop fs -du -s -h /tmp: s 应该是 sum 的意思， 展示所有文件大小的总和 展示文件最后1kb内容-tail Show the last 1KB of the file. hadoop fs -tail /dd.txt` -f Shows appended data as the file grows. 应用场景： 监控日志 修改文件权限 chmod 12345678910111213141516# 1. 直接使用十进制数字修改 [ap@cs2]~/test% hadoop fs -ls /drwxr-xr-x - ap supergroup 0 2018-06-01 08:55 /aa# -R： /aa 目录下所有的文件递归修改权限[ap@cs2]~/test% hadoop fs -chmod -R 777 /aa[ap@cs2]~/test% hadoop fs -ls /drwxrwxrwx - ap supergroup 0 2018-06-01 08:55 /aa# 2. 针对用户组修改，注意，修改2个不同组权限， 用，隔开hadoop fs -chmod u+x,g+x /a.txt # 3. 最常用的文件权限， 是 644(-rw-r--r--) 和 755(-rwxr-xr-x) 文件创建默认就是644# 4. u+x 与 +x 的区别 前者指定加在哪组用户上， 后者是所有组都加 修改用户权限 chown 1hadoop fs -chown -R 用户名:组名 hdfs目录/文件 2. hdfs dfsadmin命令 管理员对当前节点的一些操作 hdfs dfsadmin -report 报告当前的一些状态 -live 活跃的 -dead 死的 -decommissioning 退役的 **hdfs dfsadmin -safemode 安全模式 系统刚启动的时候， 会有30秒的安全模式开启状态， 过了30秒就关了 enter 进入 leave 离开 get 查看 hdfs dfsadmin 设置配额 -setQuota ： 配额是限定的文件&amp;文件夹的数量 A quota of 1 would force the directory to remain empty. 空文件本身算一个文件 bin/hdfs dfsadmin -setQuota 10 lisi -clrQuota -setSpaceQuota： 空间配额限定的是大小 bin/hdfs dfsadmin -setSpaceQuota 4k /lisi/ -clrSpaceQuota hdfs dfs -count -q -h /user: 加上 -q 是查看配额 3. httpFS访问使用 REST 的形式， 可以在浏览器上直接访问集群， 可以在非 Linux 平台访问 123456789101112131415161718192021# 编辑文件httpfs-env.sh# 打开此句注释, 使用内嵌的 tomcatexport HTTPFS_HTTP_PORT=14000# 编辑文件core-site.xml&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;编辑文件hdfs-site.xml&lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;重新启动namenode，执行 sbin/httpfs.sh start# 执行命令curl -i "http://cs1:14000/webhdfs/v1?user.name=root&amp;op=LISTSTATUS" 更多命令参考 相关知识点 这些命令在集群中的任何节点都可以做, hdfs文件系统中, 看到的目录结构只是一个抽象目录, 实际存储在集群中的节点上 aa.txt , 大小150M, hadoop fs -put aa.txt / 会在根目录下看到 /aa.txt, 但是 aa.txt 真实存储的时候, 会先进行分块, 分2块, 进行存储, 假设集群中5个存储节点, 这2个块存储在哪个节点, 由namenode进行分配 图形界面点进去, 可以看到存储的块 Linux的权限管理命令: 修改 文件/文件夹 权限的 chmod: 可读: r , =4 可写: w, =2 可执行: x, =1 最大权限是7 -rw-rw-r– 文件属性 d:目录 -:文件 l:链接 第一组: 本用户, 第二组: 本组用户, 第三组: 其它用户 chmod 711 改一个文件夹下所有文件权限为711 chmod -R 711 目录 修改文件所属用户和组 chown chown -R root:root ss/把ss的文件夹全部改成root用户和root组 5、Eclipse查看Hadoop文件信息详情可以查看 其中可能遇到的bug，参见 其中， Eclipse端无法直接删除文件的问题，似乎可以通过在hdfs-site.xml 中修改访问权限来实现， 还未尝试 1234&lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; 6. 通过 Java API的方式操作 HDFS]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap的实现原理]]></title>
    <url>%2F2018%2F06%2F02%2FJava%2FHashMap%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[1.什么是HashMap Hash: 散列将一个任意的长度通过某种(hash函数)算法转换成一个固定值 Map: 地图, (x,y)存储 底层就是一个数组结构, 数组中的每一项又是一个链表, 当新建一个HashMap的时候, 就会初始化一个数组 总结: 通过 hash 出来值, 然后通过值定位到某个 map, 然后value 存储到这个 map中, value只不过是 key 的附属. 2.源码分析先给出结论数据结构: 底层是数组 Entry 就是数组中的元素 每个Map.Entry其实就是一个key-value对, 它持有一个指向下一个元素的引用Entry&lt;K,V&gt; next;, 这就构成了链表 存取实现: 存储put : 过程 先根据 key 的 hashCode 重新计算 hash 值, 根据 hash 值得到这个元素在数组中的位置(下标) 如果数组该位置上已经存放有其他元素了, 那么在这个位置上的元素将以链表的形式存放, 新加入的放在链头, 最先加入的放在链尾. 如果数组该位置上没有元素, 就直接将该元素放到数组中的该位置上. 注意点 当系统决定存储 HashMap 中的 key-value 对时，完全没有考虑 Entry 中的 value，仅仅只是根据 key来计算并决定每个Entry的存储位置。当系统决定了 key的存储位置之后，value随之保存在那里即可。 对于于任意给定的对象，只要它的 hashCode()返回值相同，那么程序调用 hash(int h)方法所计算得到的hash 码值总是相同的。 本质上就是把 hash 值对数组长度取模运算， 这样一来，元素的分布相对来说是比较均匀的 但是系统是用的位运算， 方法更巧妙， 消耗更小 读取get 过程 首先计算 key 的 hashCode，找到数组中对应位置的某一元素，然后通过key 的 equals 方法在对应位置的链表中找到需要的元素。 存储实现总结: HashMap 在底层将 key-value 当成一个整体进行处理，这个整体就是一个 Entry 对象。 HashMap 底层采用一个 Entry[] 数组来保存所有的 key- value 对，当需要存储一个 Entry 对象时，会根据 hash 算法来决定其在数组中的存储位置，在根据 equals 方法决定其在该数组位置上的链表中的存储位置； 当需要取出一个 Entry 时，也会根据 hash 算法找到其在数组中的存储位置，再根据 equals 方法从该位置上的链表中取出该 Entry。 3. HashMap 的性能参数123456789101112131415161718192021222324252627282930313233343536/** * 桶表默认容量 16, 必须是 2 的倍数， 便于后面的 位运算 * 控制hashcode 不超16范围, a.hashcode = xx % 16 (hashcode 取模 桶个数) */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16/** * MUST be a power of two &lt;= 1&lt;&lt;30. * 桶表最大 2^30 */static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;/** * 扩容因子（负载因子）: 0.75 * 扩容: 每次2倍 */static final float DEFAULT_LOAD_FACTOR = 0.75f;/** * 链表: hash算法值相同的时候, 会把值相同的放在一个链表上, 链表上的元素个数 * 超过8个时, 链表转化为二叉树, 提升查询效率 */static final int TREEIFY_THRESHOLD = 8;/** * 小于6个， 又变回链表 */static final int UNTREEIFY_THRESHOLD = 6;/** * The smallest table capacity for which bins may be treeified. * (Otherwise the table is resized if too many nodes in a bin.) * Should be at least 4 * TREEIFY_THRESHOLD to avoid conflicts * between resizing and treeification thresholds. */static final int MIN_TREEIFY_CAPACITY = 64;]]></content>
      <categories>
        <category>Java</category>
        <category>知识点</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[▍无题]]></title>
    <url>%2F2018%2F06%2F02%2FPoetry%2F%E6%97%A0%E9%A2%98%2F</url>
    <content type="text"><![CDATA[在淮海中路看油画，要把天空调弱 让油菜花暗下去，看眉式清秀之人离开身体 穿过街巷中涂抹过的人群，悄悄投了水。 在淮海中路1411号，春光遮蔽了暗疾 鸟鸣带来逼仄和飞行感，一个人的身体像麻绳 裸露在新鲜空气中，骨头开裂出花朵。 眼底的云又白又黑，膝盖的青色愈爱愈深 穿过死后潭水的寂静，背部长出的鱼鳞 一年比一年薄，月亮一日比一日旧。 与春风交换身体，与素不相识之人抱头痛哭 与我，许下再死一次的诺言，这么多年了 她说，我爱你依旧，胜过画中人。 作者 / 隐居的事]]></content>
      <categories>
        <category>文艺</category>
        <category>诗歌</category>
      </categories>
      <tags>
        <tag>诗歌</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop相关书籍]]></title>
    <url>%2F2018%2F06%2F02%2FHadoop%2F0-Hadoop%2FHadoop%E7%9B%B8%E5%85%B3%E4%B9%A6%E7%B1%8D%2F</url>
    <content type="text"><![CDATA[Hadoop框架体系相关书籍推荐1. [Hadoop权威指南] 第四版 顾名思义, 很权威 🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘 🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘 2. [HBase权威指南] 祝你🐴到成功 🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴 🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴 3. [Hive编程指南] 小蜜蜂, 嗡嗡嗡. 🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝 🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝 4. [zookeeper分布式过程协同技术详解] 这是个啥动物?? 🐱?? 🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱 🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Books</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>Books</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop学习笔记-1 简介&安装]]></title>
    <url>%2F2018%2F05%2F31%2FHadoop%2F0-Hadoop%2FHadoop%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-%E7%AE%80%E4%BB%8B%26%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[引入面试题1. 有一个很大的(4T)的文件, 文件中存储的是ip, 每行存储一个, 要求求出出现此处最多的那个ip1234567891011121314151617181920212223242526272829303132333435如果这个文件是小文件： io流+集合 实现思路： 创建一个流进行文件读取 读取出来的数据存储到map集合中 key：ip value:次数 统计逻辑： 判断读取的ip是否已经存在在map中 存在：取出value+1 不存在：将ip作为key 1作为value 怎么求ip出现次数最多的 遍历map 遍历key，取出value找出最大值 value最大的key就是要找的ip我的文件足够大：大到一台机器装不下 数组 集合 变量------&gt;基于内存的 怎么办？？？？？ 服务器的2T 1.在最早的时候我们的思维模式就是纵向扩展，增加单个节点的性能 8T 摩尔定律：硬件性能18-24个月会提升一倍 4T------2h 4T------1h 前提是数据量不发生改变 但是往往数据量的变化速度远远大于服务器性能的提升速度 经过18个月 服务器性能提升了一倍 数据量------提升了10倍 4T------2h 40t-----20h 目前只需要10h 纵向扩展不可行？ 横向扩展：如果一台机器处理不了数据 使用多台机器 4T------2h 4t----4个机器----0.5小时 分而治之的思想： 一个机器计算性能有限 这个时候可以使用多台机器共同计算 每台机器承担一部分计算量 最终实现： 1.先将这个足够大的文件进行切分 切分成了多个小文件 2.将多个小文件分发给多个机器进行统计每个ip出现的次数 每个求出出现次数最多的ip 3.合并求出最终的最大值 2. 有两个很大的文件, 两个文件中存储的都是url, 求出两个文件中相同的url12345678910111213141516171819202122如果文件是小文件： io流+集合（set） 实现逻辑： 1.先创建两个文件读取流，用来读取两个文件 2.创建两个集合set1 set2 3.进行文件读取并分别set1 set2中 4.循环遍历其中一个set1，判断set1中取出的每个url是否在set2中 set2.contains(url)大文件的时候怎么办？ 我们也采用分而治之的思想：将两个大文件都进行切分，每个大文件都切成多个小文件 一个大任务=4*4个小任务 这样虽然可以达到目的但是效率太低？怎么办？ 排序，切分（规则同一） 最终将任务减少到4个 但是大文件排序仍然是一个非常消耗性能的事情，如果不需要排序就可做到这个效果尽量不要排序 怎么办？ hash算法的目的----》给每一个对象生成一个“唯一”的hash值0-Integer_MAX 是否可以运用hash算法解决这个问题 url.hashCode()%分段的个数 两个文件分段规则一定相同吗？ url.hashCode()肯定一样 分段个数一定相同吗？可以不一样 如果不一样的话 必须成倍数关系 最终的解决方案： 分而治之+分段规则 分段：分区 3. 有一个很小的文件, 存储的都是url, 每行一个, 怎样快速判断给定的一个url是否在这个文件中小文件: IO + 集合(set) 创建io 和 集合 进行文件读取放在 set集合中 set.contains(url) ==&gt; true:存在, false: 不存在 大文件: 思路1: 用hashCode() 进行分区, 然后用要查找的 url 取模定位 但是这样定位到了还是要一个个找 思路2: 数组的查询性能比较高, 数组可以通过下标 基数排序 数组的索引代表的是数据的原始值, 数组中存储的值, 是原始值出现的次数 放到对应下标的位置, 值只存出现的次数 如果数组中对应的下表存储的值为0, 代表此下标的值没有出现过, 就不需要输出 缺点: 数据范围过大时, 数组长度不好创建 数组的类型不好确定 如果数据比较分散时, 会造成资源浪费 练习: 写一个基数排序, 随机生成的20个数, 运用基数排序排序 对于本题 不需要统计次数, 存在标记为1, 不存在就是0 所以存的时候最好用boolean存, 用位数组 bit[] 可以设计多个hash算法, 用来校验某一种hashCode相同的情况 影响误判率3要素: hash算法个数 k - 数据量n - 数组长度 m 布隆过滤器 公式: k = 0.7*(m/n), 此时的误判率最小 大数据基本介绍数据 数据就是数值，也就是我们通过观察、实验或计算得出的结果。数据有很多种，最简单的就是数字。 数据也可以是文字、图像、声音等。数据可以用于科学研究、设计、查证等。 结构划分: 结构化 半结构化 非结构化 大数据特点: 4V 数据量大 1 Byte =8 bit1 KB = 1,024 Bytes = 8192 bit1 MB = 1,024 KB = 1,048,576 Bytes1 GB = 1,024 MB = 1,048,576 KB1 TB = 1,024 GB = 1,048,576 MB （普通用户数据级别）1 PB = 1,024 TB = 1,048,576 GB（企业级数据级别）1 EB = 1,024 PB = 1,048,576 TB1 ZB = 1,024 EB = 1,048,576 PB（全球数据总量级别） 数据增长速度快 数据种类多 文字 图片 音频 视频.. 数据的价值密度低 整体价值高 数据来源 公司中的自己的业务数据 淘宝 京东 第三方 爬虫 爬数据 数据处理 缺失数据的处理 考虑缺失数据是否影响整体的业务逻辑 不影响 删除 如果是和钱相关的数据 —-慎重 不能轻易删除 敏感数据 脱敏处理 – 加密 数据价值 人物画像 根据根据用户数据给用户做一个全方位的分析画像 属性： 人脉 消费水平 性格特点 …. 几个概念集群: 多个机器共同协作完成同一个任务, 每一个机器叫做节点, 多个机器共同组成的群体叫做集群 集群中的每个节点之间通过局域网或其他方式通讯 分布式: 分而治之 , 一个任务呗分成多个子任务模块, 每个任务跑在不同的节点上 原来一个人干的事情, 现在大家分工劳动 分布式的文件系统 , 分布式数据库, 分布式计算系统 负载均衡: Nginx 每个节点分配到的任务基本均衡 负载均衡是跟每个节点自身的配置等匹配的 不存在绝对的均衡 Hadoop概念 一个分布式的开源框架 支持成千上万的节点, 每个节点依靠本地的计算和存储 在应用层面提供高可用性 将硬件错误看成一个常态 Hadoop的模块 Common 支持其他 Hadoop 模块的公共实用程序 封装: 工具类, RPC框架 HDFS Hadoop的分布式文件系统, 负责海量数据的存储 将文件切分成指定大小的数据块并以多副本的存储形式存储在多个机器上 数据切分, 多副本, 容错等操作对用户是透明的 架构: 主从架构 ( Java进程) 主: namenode 一个 从: datenode 多个 助理: SecondaryNamenode 分担主进程的压力 YARN 集群的资源调度框架, 负责集群的资源管理 架构: 主从架构 主: ResourceManager – 负责统筹资源 从: NodeManager MapReduce 分布式计算框架, 有计算任务的时候才会有响应的进程 Hadoop的搭建搭建前的准备123456789101112131415161718192021222324252627282930313233343536373839404142搭建准备：1）ip配置2）主机名 vi /etc/sysconfig/network3）主机映射4）关闭防火墙和sellinux service iptables stop vi /etc/selinux/config SELINUX=disabled5）将系统的启动级别改为3 vi /etc/inittab6）创建普通用户，并为普通用户添加sudolers权限 创建用户：useradd 用户名 passwd 用户名 vi /etc/sudoers hadoop ALL=(ALL) ALL7）配置免密登录 先切换到普通用户 1）生成秘钥 ssh-keygen 2)发送秘钥 ssh-copy-id hadoop(主机名) 验证：ssh hadoop 8）安装jdk 卸载jdk： rpm -qa|grep jdk rpm -e java-1.7.0-openjdk-1.7.0.99-2.6.5.1.el6.x86_64 --nodeps9）时间同步 伪分布式不需要 分布式需要，必须做10)选择安装版本： 不选太陈旧的版本也不选最新的版本 2.7.6 11)安装 一定切换用户 普通用户 方式1: 伪分布式所有进程全部运行在同一个节点上 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162631）上传2）解压3）修改配置文件 配置文件的目录：HADOOP_HOME/etc/hadoop 需要修改6个配置文件： 1）hadoop-env.sh export JAVA_HOME=/home/hadoop/jdk1.8.0_73/ 2)core-site.xml 核心配置文件 &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop:9000&lt;/value&gt; &lt;/property&gt; 3)hdfs-site.xml &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; 4)yarn-site.xml &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; 5)mapred-site.xml &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 6)slaves 配置的是从节点的信息 7)配置环境变量 export JAVA_HOME=/home/hadoop/jdk1.8.0_73 export HADOOP_HOME=/home/hadoop/hadoop-2.7.6 export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin source /etc/rpofile 验证 hadoop version 8)先进行格式化 hadoop namenode -format 9)启动 start-all.sh 不建议 建议以下命令： start-dfs.sh start-yarn.sh 10）验证 jps 6个进程 3909 Jps 3736 ResourceManager 3401 DataNode 3306 NameNode 3836 NodeManager 3597 SecondaryNameNode 页面： hdfs：namenode的ip：50070 yarn:resourcemanager的ip：8088 方式2: 完全分布式参考文档 各个节点的安装的普通用户名必须相同 密码也得相同, 每个节点都需要操作 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879搭建准备： 1）ip配置 2）主机名 vi /etc/sysconfig/network 3）主机映射 4）关闭防火墙和sellinux service iptables stop vi /etc/selinux/config SELINUX=disabled 5）将系统的启动级别改为3 6）创建普通用户，并为普通用户添加sudolers权限 创建用户：useradd 用户名 passwd 用户名 vi /etc/sudoers hadoop ALL=(ALL) ALL 7）配置免密登录 先切换到普通用户 每台机器都需要执行下面的操作 各个节点之间都做一下 a. 生成秘钥 ssh-keygen b. 发送秘钥 ssh-copy-id hadoop(主机名) c. 验证：各个节点之间都需要做相互验证 ssh hadoop01 ssh hadoop02 ssh hadoop03 8）安装jdk 卸载jdk： rpm -qa|grep jdk rpm -e java-1.7.0-openjdk-1.7.0.99-2.6.5.1.el6.x86_64 --nodeps 9）时间同步 伪分布式不需要 分布式需要，必须做 1)不能联网的时候 手动指定 date -s 时间 或者手动搭建一个时间服务器 2）能联网的时候 找一个公网中的公用的时间服务器 所有节点的时间和公网中的时间服务器保持一致 ntpdate 公网的时间服务器地址 完全分布式必须要做 每个节点都需要执行 10) 选择安装版本： 不选太陈旧的版本也不选最新的版本 2.7.6 11) 安装 一定切换用户 普通用户 先在一个节点上执行所有的修改 在远程发送到其他节点 1）上传 2）解压d 3）配置环境变量 4）修改配置文件 6个配置文件 集群规划 .... .... 5)远程发送 scp -r hadoop-2.7.6 hadoop02:$PWD scp -r hadoop-2.7.6 hadoop03:$PWD scp -r /home/ap/apps/hadoop-2.7.6/etc/hadoop ap@cs1:/home/ap/apps/hadoop-2.7.6/etc 远程发送/etc/pofile 执行source /etc/pofile 6）进行格式化 必须在namenode的节点（hdfs的主节点） hadoop namenode -format 不配置目录默认/tmp 临时目录 可以随时回收的 7)启动 启动hdfs start-dfs.sh 在任意节点都可以 启动yarn start-yarn.sh 在yarn的主节点执行 jps命令查看 网页： hdfs: hadoop01:50070 yarn hadoop03:8088 8) 去掉警告（在/etc/profile或者 .bash_profile 或者 .zshrc中添加） export HADOOP_HOME_WARN_SUPPRESS=1 测试yarn集群是否启动成功 (提交MapReduce例子程序试跑) ls apps/hadoop-2.7.6/share/hadoop/mapreduce bin/hadoop jar hadoop-mapreduce-examples-2.6.5.jar pi 5 5 可能遇到的错误搭建过程中 主机找不到 /etc/sysconfig/network /etc/hosts 重启机器 何时化的时候报错 配置文件错误, 根据错误去相应文件进行调整, 修改完毕后, 重新格式化直到格式化成功 启动过程中某些进程启动不了措辞1: 暴力 全部关闭集群重新启动 stop-dfs.sh 在任意节点执行 stop-yarn.sh 在yarn的主节点启动 重新启动, 直接启动就可以了 start-dfs.sh start-yarn.sh 措施2: 单独启动某些进程单独启动hdfs的相关进程 hadoop-daemon.sh start hdfs 过程 hadoop-daemon.sh start namenode hadoop-daemon.sh start secondarynamenode 单独启动yarn的相关命令 yarn-daemon.sh start yarn 的相关过程 yarn-daemon.sh start resourcemanager ==搭建过程中的注意事项== 集群的只能成功的格式化一次, 不成功的要一直到格式化成功, 成功后就不能再次格式化 格式化的过程中: 创建出来namenode存储的相关目录 version文件: 记录仪集群的版本信息的, 每格式化一次, 就会产生一个新的版本信息 123456namespaceID=1163449973clusterID=CID-47f10077-2aef-4df6-a364-1a735515a100 #记录集群的版本信息的cTime=0storageType=NAME_NODEblockpoolID=BP-1527239677-192.168.75.162-1527817150436layoutVersion=-63 启动hdfs的时候: 生成datanode的相关数据信息 version: 记录datanode 相关版本的信息 1clusterID=CID-47f10077-2aef-4df6-a364-1a735515a100 两个文件中的clusterID相同的时候, datanode 才会认为是同一个集群的 想要重复格式化, 分3步走 停止所有服务 删除 namenode 的数据目录 rm -rf /home/ap/data/hadoopdata/name 删除 datanode 的数据目录 rm -rf /home/ap/data/hadoopdata/data 此时才可以重新格式化, 否则会造成 datanode 启动不了, 注意, 关闭防火墙, 关闭vpn 也可以一步到位 rm -rf /home/ap/data 再重新格式化 集群搭建过程中环境变量的配置(jdk. hadoop…) 在Linux中修改环境变量的地方有3个 /etc/profile 系统环境变量, 针对所有用户的 ~/.bashrc 用户环境变量 ~/.bash_profile 用户环境变量 这3个配置文件的加载顺序 /etc/profile &gt; .bashrc &gt; .bash_profile 生效: 最后一次加载的生效 时间同步问题 只要是完全分布式的, 多个节点之间一定要做时间同步, 目的: 要和 北京/上海 时间保持一致? no 集群背部各个节点之间时间保持一致 yes why ? 集群内部各个节点之间需要通信, 尤其是datanode 和 namenode之间, 他们之间的通信依靠心跳机制, 他们之间的心跳存在一个时间控制, 这个时间是 630s, 他们之前需要做时间同步 集群的安装模式1. 单机模式 直接解压的方式, 什么都不配置, 并且在一个节点上 没有分布式文件系统, 所有的文件都是来自本地, 只能对本地的文件进行读写 几乎没人用, 测试时偶尔会用 2. 伪分布式 可以看做跑在一个节点上的完全分布式 有分布式文件系统, 只不过这个文件系统只有一个节点 ==3. 完全分布式==参考文档 规划 目前疑点: NodeManager是根据什么配置到每台机器上的?? 根据表征, 可能是根据 slave文件 主机名 / IP HDFS YARN cts1 / 192.168.56.131 NameNode 空的 cts2 / 192.168.56.132 DataNode NodeManager cts3 / 192.168.56.133 DataNode + Secondary NameNode NodeManager cts4 / 192.168.56.134 DataNode NodeManager +ResourceManager hdfs 为例 : 在宏观看就是一个大的节点, 后台采用的硬件配置是三天机器的硬件配置之和, 但是对用户来讲完全感觉不到 在完全分布式中, 有主节点, 有从节点 主节点 namenode只有一个, 从节点有多个, 真实生产中, namenode会单独做一个节点 如果集群中namenode宕机, 整个集群还可以使用吗? 不可以 namenode: 主要作用存储元数据 (管理数据的数据, 存储的就是datanode存储数据的描述) datanode: 负责集群中真正处理数据存存储的 如果namenode 宕机, 集群无法使用, 这也是完全分布式的一大缺点, 存在单点故障问题 一般生产中不太使用, 学习, 测试, 节点个数比较少的时候, 有时候也会使用这种模式 节点数目越多, namenode宕机的可能性越大, 压力太大 助理secondarynamenode: 只是一个助理, 只是分担namenode的压力, 但是不能代替 架构: 一主多从 ==4. 高可用== 概念: 集群可以持续对外提供服务, 做到 7*24 小时不间断 依赖于zookeeper, 搭建放在 zookeeper课程之后 集群架构: 双主多从 有2个 namenode, 但是在同一时间只能有一个是 活跃的 namenode, 我们把这个活跃的namenode 成为 active 的, 另外一个是处理热备份状态, 我们将这个节点叫 standby, 但是2个主节点存储的元数据是一模一样的, 当 active namenode宕机的时候, standby的namenode 可以立马切换为 active 的namenode, 对外提供服务, 就可以做到 集群持续对外提供服务的功能 如果过一段时间, 宕机的 namenode 又活过来了, 宕机的 namenode 只能是变成 standby 的 缺陷: 在同一时间中, 集群中只有一个active 的 namenode, 也就是说 集群中有主节点能力的节点 只有一个, 如果集群中, 节点个数过多(1000) 的时候, 会造成namenode的崩溃, namenode存储的是元数据, 元数据过多的时候, 会造成namenode的崩溃(两个都崩溃), 没有真正的分担namenode 的压力 实际生产多使用高可用 5. 联邦机制 同一个集群中可以有多个主节点, 这些主节点的地位是一样的. 同一时间, 可以有多个活跃的 namenode 这些 namenode 共同使用集群中所有的 datanode, 每个namenode 只负责管理集群中的 datanode上的一部分数据 一般超大集群搭建的时候: 联邦 + 高可用 超大集群使用 每个namenode进行数据管理靠的Block Pool ID相同 不同的namenode管理的数据Block Pool ID 不同]]></content>
      <categories>
        <category>Hadoop</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop体系概览]]></title>
    <url>%2F2018%2F05%2F31%2FHadoop%2F0-Hadoop%2FHadoop%E4%BD%93%E7%B3%BB%E6%A6%82%E8%A7%88%2F</url>
    <content type="text"><![CDATA[Hadoop核心组件整体直观概览 1. 分布式文件系统HDFS1.1 基本概念 将文件切分成指定大小的数据块并以多副本的存储形式存储在多个机器上 数据切分, 多副本, 容错等操作对用户是透明的 1.2 图示 1.3 HDFS架构 Datanode 定期向 Namenode 发 Hearbeat 元数据信息： 多份备份 1.4 HDFS的 IO 操作 上面的是读 客户端先向 NameNode 寻址 然后再找 DataNode 拿数据 下面的是写 HDFS 不支持修改， 没有 leader 角色， 不支持并发写， 只能支持非并发的追加 HBase 支持并发写和修改 删除： 删除的是元数据（索引信息） Datanode 会定期向 Namenode 发送心跳， 同步信息， 当 Namenode 发现 Datanode 上没有自己存储的信息时，就会把这部分信息删除掉。 1.5 HDFS 副本存放策略 复制因子为3时的 存放策略 如果写入者在一个 datanode 上， 则把一份拷贝放在本地机器上， 否则随机放到一个 datande 上 另一个副本放在不同的（远程）机架的节点上， 最后一个副本存放在同一个机架的不同节点上 这一策略削减了机架间的写入流量，通常提高了写入性能 复制因子大于3， 则随机确定第4个和其它的副本位置，同时将每个拷贝的数目保持在上限以下(基本上是(副本数 - 1) / racks + 2)。 2. 资源调度系统 YARN2.1 基本概念 YARN: Yet Another Resource Negotiator 负责整个集群资源的管理和调度 YAEN特点: 扩展性 &amp; 容错性 &amp; 多框架资源统一调度 2.2 图示 3. 分布式计算框架 MapReduce3.1 基本概念 源于Google的MapReduce论文, 论文发表于2004年12月 MapReduce是Google MapReduce的克隆版 MapReduce的特点: 扩展性 &amp; 容错性 &amp; 海量数据离线处理 3.2 图示 Hadoop优势1. 高可靠性 数据存储: 数据块多副本 数据计算: 重新调度作业计算 2. 高扩展性 存储/计算资源不够时, 可以横向的线性扩展机器 一个集群中可以包含数以千计的节点 3. 其它 存储在廉价机器上, 降低成本 成熟的生态圈 Hadoop的发展史见文章: Hadoop十年解读与发展预测 Hadoop官网 Hadoop生态系统1. 图示 2. 特点 开源, 社区活跃 囊括了大数据处理的方方面面 成熟的生态圈 Hadoop常用发行版及选型 Apache Hadoop CDH : Cloudera Distributed Hadoop （国内用的比较多） HDP : Hortonworks Data Platform 使用: CDH使用占比 60-70 hadoop: hadoop-2.6.0-cdh5.7.0 hive : hive-1.1.0-cdh5.7.0]]></content>
      <categories>
        <category>Hadoop</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础增强-2 并发编程]]></title>
    <url>%2F2018%2F05%2F30%2FJava%2FJava%E5%9F%BA%E7%A1%80%E5%A2%9E%E5%BC%BA-2%2F</url>
    <content type="text"><![CDATA[1. 多线程基本知识1.1 多线程运行的原理 原理：CPU 在线程中做时间片的切换。 一个(多核中的一个) CPU 在在运行程序的过程中某个时刻点上，只能运行一个程序。而 CPU 可以在 多个程序之间进行高速的切换 (轮询制)。而切换频率和速度太快，导致人的肉眼看不到。 1.2 实现线程的两种方式 继承 Thread 声明实现Runnable接口 还可以实现Callable接口 1.3 线程的状态图解 新建状态（New）：新创建了一个线程对象。 就绪状态（Runnable）：线程对象创建后，其他线程调用了该对象的 start()方法。该状态 的线程位于可运行线程池中，变得可运行，等待获取 CPU 的使用权。 运行状态（Running）：就绪状态的线程获取了 CPU，执行程序代码。 阻塞状态（Blocked）：阻塞状态是线程因为某种原因放弃 CPU 使用权，暂时停止运行。 直到线程进入就绪状态，才有机会转到运行状态。阻塞的情况分三种： 等待阻塞：运行的线程执行 wait()方法，JVM 会把该线程放入等待池中。(wait 会释 放持有的锁) 同步阻塞：运行的线程在获取对象的同步锁时，若该同步锁被别的线程占用，则 JVM 会把该线程放入锁池中。 其他阻塞：运行的线程执行 sleep()或 join()方法，或者发出了 I/O 请求时，JVM 会把 该线程置为阻塞状态。当 sleep()状态超时、join()等待线程终止或者超时、或者 I/O 处理完毕 时，线程重新转入就绪状态。（注意：sleep 是不会释放持有的锁） 死亡状态（Dead）：线程执行完了或者因异常退出了 run()方法，该线程结束生命周期。 1.4 几个重要的方法的区别： sleep(timeout)：当前线程进入阻塞状态，暂停执行一定时间，不会释放锁标记 join()：join()方法会使当前线程等待调用 join()方法的线程结束后才能继续执行 yield()：调用该方法的线程重回可执行状态，不会释放锁标记，可以理解为交出 CPU 时间片， 但是不一定有效果，因为有可能又被马上执行。该方法的真正作用是使具有相同或者更高优 先级的方法得到执行机会。 wait(timeout)：wait 方法通常和 notify()/notifyAll()搭配使用，当前线程暂停执行，会释放锁 标记。进入对象等待池。直到调用 notify()方法之后，线程被移动到锁标记等待池。只有锁 标记等待池的线程才能获得锁 1.5 Join的用法联合线程: 线程的join方法表示一个线程等待另一个线程完成后才执行。有人也把这种方式称为联合线程，就是说把当前线程和当前线程所在的线程联合成一个线程。join方法被调用之后，线程对象处于阻塞状态。 适用于A线程需要等到B线程执行完毕,再拿B线程的结果再继续运行A线程. 说人话: A线程需要拿到B线程的执行结果,才能继续往下. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081class Join extends Thread&#123; @Override public void run() &#123; for (int i = 0; i &lt; 50; i++) &#123; System.out.println("This is join: " + i); &#125; &#125;public class JoinThread &#123; public static void main(String[] args) throws InterruptedException &#123; System.out.println("begin..."); Join joinThread = new Join();//创建join线程对象 for (int i = 0; i &lt; 50; i++) &#123; System.out.println("main: " +i); if (i == 10)&#123; //启动join对象 joinThread.start(); &#125; if (i == 20)&#123; System.out .println("------------------------------------------"); joinThread.join();//在此处强制运行该线程 &#125; &#125; &#125;&#125; ===================执行结果===================begin...main: 0main: 1main: 2main: 3main: 4main: 5main: 6main: 7main: 8main: 9main: 10main: 11main: 12main: 13This is join: 0main: 14This is join: 1This is join: 2This is join: 3main: 15This is join: 4main: 16main: 17main: 18main: 19main: 20------------------------------------------This is join: 5This is join: 6This is join: 7This is join: 8This is join: 9This is join: 10......This is join: 44This is join: 45This is join: 46This is join: 47This is join: 48This is join: 49main: 21main: 22main: 23main: 24main: 25main: 26main: 27main: 28main: 29main: 30 2. Java同步关键词解释2.1. synchronized属于 JVM 级别加锁，底层实现是： 在编译过程中，在指令级别加入一些标识来实现的。 1. 锁对象注意点: 必须是锁的同一个对象 2. 锁获取和释放 锁的获取是由JVM决定的, 用户无法操作 锁的释放也是由JVM决定的 Synchronized 无法中断正在阻塞队列或者等待队列的线程。 3. 什么时候会释放 获取锁的线程执行完了该代码块，然后线程释放对锁的占有； 线程执行发生异常，此时 JVM 会让线程自动释放锁。 4.格式 1234// 加同步格式：synchronized(需要一个任意的对象（锁）)&#123; 代码块中放操作共享数据的代码。&#125; 5.线程执行互斥代码的过程 获得互斥锁 清空工作内存 从主内存拷贝变量的最新副本到工作内存 执行代码 将更新后的共享变量的值刷新到主内存 释放互斥锁 Lock -&gt; 主内存 -&gt; 工作内存 -&gt; 主内存 -&gt; unlock 2.2 Lock手动获取或释放锁, 提供了比 synchronized 更多的功能 Lock 锁是 Java 代码级别来实现的，相对于 synchronized 在功能性上，有所加强，主要是，公平锁，轮 询锁，定时锁，可中断锁等，还增加了多路通知机制（Condition），可以用一个锁来管理多 个同步块。另外在使用的时候，必须手动的释放锁。Lock 锁的实现，主要是借助于队列同 步器（我们常常见到的 AQS）来实现。它包括一个 int 变量来表示状态；一个 FIFO 队列，来 存储获取资源的排队线程。 基本使用 12345678910111213class X &#123; // 创建一把锁 private final ReentrantLock lock = new ReentrantLock(); // 需要做同步的方法 public void m() &#123; lock.lock(); //获取🔐, 加锁 try &#123; // 代码 &#125; finally &#123; lock.unlock(); // 释放🔐 &#125; &#125;&#125; 1. lock 和 synchronized 的区别 Lock 不是 Java 语言内置的，synchronized 是 Java 语言的关键字，因此是内置特性。Lock 是一个类，通过这个类可以实现同步访问； Lock 和 synchronized 有一点非常大的不同，采用 synchronized 不需要用户去手动释放锁， 当 synchronized 方法或者 synchronized 代码块执行完之后，系统会自动让线程释放对锁的占 用；而 Lock 则必须要用户去手动释放锁，如果没有主动释放锁，就有可能导致出现死锁现 象。 2. Lock 接口中方法的使用 ReentrantLock 类 ReentrantLock 是唯一实现了 Lock 接口的类，并且 ReentrantLock 提供了更多的方法，ReentrantLock，意思是“可重入锁”。 lock()、tryLock()、tryLock(long time, TimeUnit unit)、lockInterruptibly()是用来获取锁的。 unLock()方法是用来释放锁的。 四个获取锁方法的区别 lock()，阻塞方法，该方法是平常使用得最多的一个方法，就是用来获取锁。如果锁已被 其他线程获取，则进行等待。由于在前面讲到如果采用 Lock，必须主动去释放锁，并且在 发生异常时，不会自动释放锁。因此一般来说，使用 Lock 必须在 try{}catch{}块中进行，并 且将释放锁的操作放在 finally 块中进行，以保证锁一定被被释放，防止死锁的发生。 tryLock()，非阻塞方法，该方法是有返回值的，它表示用来尝试获取锁，如果获取成功， 则返回 true，如果获取失败（即锁已被其他线程获取），则返回 false，也就说这个方法无论 如何都会立即返回。在拿不到锁时不会一直在那等待。 tryLock(long time, TimeUnit unit)，阻塞方法，阻塞给定时长，该方法和 tryLock()方法是 类似的，只不过区别在于这个方法在拿不到锁时会等待一定的时间，在时间期限之内如果还 拿不到锁，就返回 false。如果一开始拿到锁或者在等待期间内拿到了锁，则返回 true。 lockInterruptibly()这个方法比较特殊，当通过这个方法去获取锁时，如果线程正在等待 获取锁，则这个线程能够响应中断，即中断线程的等待状态。也就使说，当两个线程同时通 过 lock.lockInterruptibly()想获取某个锁时，假若此时线程 A 获取到了锁，而线程 B 只有在等 待，那么对线程 B 调用 threadB.interrupt()方法能够中断线程 B 的等待过程。 2.3 Lock 与 synchronized 的选择 Lock 是一个接口，而 synchronized 是 Java 中的关键字，synchronized 是内置的语言实现； synchronized 在发生异常时，会自动释放线程占有的锁，因此不会导致死锁现象发生； 而 Lock 在发生异常时，如果没有主动通过 unLock()去释放锁，则很可能造成死锁现象，因 此使用 Lock 时需要在 finally 块中释放锁； Lock 可以让等待锁的线程响应中断，而 synchronized 却不行，使用 synchronized 时，等 待的线程会一直等待下去，不能够响应中断； 通过 Lock 可以知道有没有成功获取锁，而 synchronized 却无法办到。 Lock 可以提高多个线程进行读操作的效率。 2.4 读写锁 线程进入读锁的前提条件： 没有其他线程的写锁, 没有写请求或者有写请求，但调用线程和持有锁的线程是同一个 线程进入写锁的前提条件： 没有其他线程的读锁, 没有其他线程的写锁 ReentrantReadWriteLock 与 ReentrantLock 都是单独的实现，彼此之间没有继承或实现的关系。 ReadWriteLock 类123456// API// 可以区别对待读、写的操作public interface ReadWriteLock &#123; Lock readLock(); Lock writeLock();&#125; ReentrantReadWriteLock 类 ReentrantReadWriteLock 里面提供了很多丰富的方法，不过最主要的有两个方法：readLock()和 writeLock()用来获取读锁和写锁。 注意：不过要注意的是，如果有一个线程已经占用了读锁，则此时其他线程如果要申请写 锁，则申请写锁的线程会一直等待释放读锁。 如果有一个线程已经占用了写锁，则此时其他线程如果申请写锁或者读锁，则申请的线程 会一直等待释放写锁。 2.5 死锁死锁是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象， 若无外力作用，它们都将无法推进下去。这是一个严重的问题，因为死锁会让你的程序挂起 无法完成任务。 死锁的发生必须满足以下四个条件： 互斥条件：一个资源每次只能被一个进程使用。 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 不剥夺条件：进程已获得的资源，在末使用完之前，不能强行剥夺。 循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。 避免死锁最简单的方法就是阻止循环等待条件，将系统中所有的资源设置标志位、排序，规 定所有的进程申请资源必须以一定的顺序（升序或降序）做操作来避免死锁 2.6 Volatile 特殊域变量多线程编程，我们要解决的问题集中在三个方面： 原子性。最简单的例子就是，i++,在多线程环境下，最终的结果是不确定的，为什么？就 是因为这么一个++操作，被编译为计算机底层指令后，是多个指令来完成的。那么遇到并发 的情况，就会导致彼此“覆盖”的情况。 可见性。通俗解释就是，在 A 线程对一个变量做了修改，在 B 线程中，能正确的读取到 修改后的结果。究其原理，是 cpu 不是直接和系统内存通信，而是把变量读取到 L1，L2 等 内部的缓存中，也叫作私有的数据工作栈。修改也是在内部缓存中，但是何时同步到系统内 存是不能确定的，有了这个时间差，在并发的时候，就可能会导致，读到的值，不是最新值。 指令重排。这里只说指令重排序，虚拟机在把代码编译为指令后执行，出于优化的目的， 在保证结果不变的情况下，可能会调整指令的执行顺序。 valotile，能满足上述的可见性和有序性。但是无法保证原子性。 可见性，是在修改后，强制把对变量的修改同步到系统内存。而其他 cpu 在读取自己的内部 缓存中的值的时候，发现是 valotile 修饰的，会把内部缓存中的值，置为无效，然后从系统 内存读取。 有序性，是通过内存屏障来实现的。所谓的内存屏障，可以理解为，在某些指令中，插入屏 障指令，用以确保，在向屏障指令后面继续执行的时候，其前面的所有指令已经执行完毕。 3. Java多线程中常见的面试题1. sleep(),wait(),join(),yield()四个方法的区别总结： 1）：sleep()，Thread 类中的方法，表示当前线程进入阻塞状态，不释放锁 2）：wait()，Object 类中的方法，表示线程进入等待状态，释放锁，所以一般能调用这个方 法的都是同步代码块，或者获取了锁的线程代码，通常和 notify()和 notifyAll()方法结合使用 3）：join()，Thread 类中的方法，假如在 a 线程中调用 b 线程对象的 join()方法，表示当前 a 线程阻塞，直到 b 线程运行结束 4）：yield()，Thread 类中的方法，表示线程回可执行状态。跟 sleep 方法一样，也不交出锁， 只不过不带时间参数，是指交出 cpu 2. Thread 和 Runnable 的区别总结： 实现 Runnable 接口比继承 Thread 类所具有的优势： 1）：适合多个相同的程序代码的线程去处理同一个资源 2）：可以避免 java 中的单继承的限制 3）：增加程序的健壮性，代码可以被多个线程共享，代码和数据独立 4）：线程池只能放入实现 Runable 或 callable 类线程，不能直接放入继承 Thread 的类 …]]></content>
      <categories>
        <category>Java</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础增强-1 集合反射设计模式排序]]></title>
    <url>%2F2018%2F05%2F29%2FJava%2FJava%E5%9F%BA%E7%A1%80%E5%A2%9E%E5%BC%BA-1%2F</url>
    <content type="text"><![CDATA[1、集合框架1.1 、集合框架体系图 Java 的集合框架主要分为五大类体系：1、Collection（常用的 List 和 Set，和不常用的 Queue 和 Vector 和 Stack），单元素集合2、Map（常用的 HashMap 和 TreeMap，不常用的 HashTable），Key-Value 映射3、Iterator（迭代器）4、工具类（Collections 和 Arrays）5、Comparable 和 Comparator 比较器 Java 中的集合和数组的区别:1、数组长度在初始化时指定，意味着只能保存定长的数据。而集合可以保存数量不确定的 数据。同时可以保存具有映射关系的数据（即关联数组，键值对 key-value）。 2、数组元素即可以是基本类型的值，也可以是对象。集合里只能保存对象（实际上只是保存对象的引用变量），基本数据类型的变量要转换成对应的包装类才能放入集合类中。 Collection 接口中的方法： Map 接口中的方法： 1.2、常用集合特性概述1.2.1 List 系 List 特点：元素有放入顺序，元素可重复 List 接口有三个实现类：LinkedList，ArrayList，Vector LinkedList：底层基于链表实现，链表内存是散乱的，每一个元素存储本身内存地址的同时还 存储下一个元素的地址。链表增删快，查找慢 ArrayList 和 Vector 底层都是基于数组实现的，查询快，增删慢，区别是 ArrayList 是非线程安全的，效率高；Vector 是基于线程安全的，效率低 ArrayList 的初始化大小是 10，扩容策略是 1.5 倍原元素数量的大小 数组 初始容量+扩容 (jdk10)12345678910111213141516171819// 初始容量private static final int DEFAULT_CAPACITY = 10;// 扩容private int newCapacity(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt;= 0) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) return Math.max(DEFAULT_CAPACITY, minCapacity); if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return minCapacity; &#125; return (newCapacity - MAX_ARRAY_SIZE &lt;= 0) ? newCapacity : hugeCapacity(minCapacity); &#125; 选择标准： 如果涉及到“动态数组”、“栈”、“队列”、“链表”等结构，应该考虑用 List，具体的选择哪 个 List，根据下面的标准来取舍。 1、对于需要快速插入，删除元素，应该使用 LinkedList。（增删改） 2、对于需要快速随机访问元素，应该使用 ArrayList。（查询） 3、对于“单线程环境”或者“多线程环境，但 List 仅仅只会被单个线程操作”，此时应该使 用非同步的类(如 ArrayList)。对于“多线程环境，且 List 可能同时被多个线程操作”，此时， 应该使用同步的类(如 Vector)。 LinkedList add(E e)1234567891011121314151617# add(E e) 源码public boolean add(E e) &#123; linkLast(e); return true;&#125; void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125; 1.2.2、Set 系 Set 特点：元素放入无顺序，元素不可重复 Set 接口的实现类：HashSet，TreeSet，LinkedHashSet HashSet（底层由 HashMap 实现）底层通过 hashCode()和 equals()进行去重。 HashSet 内部判断相等的标准HashSet 判断两个元素相等的标准： ​ 两个对象通过 equals()方法比较相等，并且两个对象的 hashCode()方法返回值也相等 HashSet 中判断集合元素相等，两个对象比较具体分为如下四个情况： 如果有两个元素通过 equal()方法比较返回 false，并且它们的 hashCode()方法返回不相等， HashSet 将会把它们存储在不同的位置。 如果有两个元素通过 equal()方法比较返回 true，并且它们的 hashCode()方法返回不相等， HashSet 将会把它们存储在不同的位置。 如果两个对象通过 equals()方法比较不相等，hashCode()方法比较相等，HashSet 将会把它们存储在相同的位置，在这个位置以链表式结构来保存多个对象。这是因为当向 HashSet 集合中存入一个元素时，HashSet 会调用对象的 hashCode()方法来得到对象的 hashCode 值， 然后根据该 hashCode 值来决定该对象存储在 HashSet 中存储位置。 如果有两个元素通过 equal()方法比较返回 true，并且它们的 hashCode()方法返回 true，HashSet 将不予添加。 LinkedHashSet，是 HashSet 的子类，在插入元素的时候，同时使用链表维持插入元素的顺序 SortedSet 接口有一个实现类：TreeSet（底层由平衡二叉树实现）确保集合中的元素都是出于排序状态 注意 LinkedHashSet 和 SortedSet 区别，前者存插入顺序，后者存插入之后的顺序 另外: JDK5 : 桶表 + 链表 JDK8 : 桶表 + 链表 + 二叉树 - **二叉树**: 检索深度 &gt; 8 的时候, 转化为二叉树, 减少查询深度 HashSet — HashMap 的源码实现123456789101112131415161718192021222324252627282930313233343536373839/** * The default initial capacity - MUST be a power of two. * 桶表默认容量 16 * 控制hashcode 不超16范围, a.hashcode = xx % 16 (hashcode 取模 桶个数) */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16/** * MUST be a power of two &lt;= 1&lt;&lt;30. * 桶表最大 2^30 */static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;/** * 扩容因子: 0.75 * 扩容: 每次2倍 */static final float DEFAULT_LOAD_FACTOR = 0.75f;/** * 链表: hash算法值相同的时候, 会把值相同的放在一个链表上, 链表上的元素个数 * 超过8个时, 转化为二叉树, 提升查询效率 */static final int TREEIFY_THRESHOLD = 8;/** * The bin count threshold for untreeifying a (split) bin during a * resize operation. Should be less than TREEIFY_THRESHOLD, and at * most 6 to mesh with shrinkage detection under removal. */static final int UNTREEIFY_THRESHOLD = 6;/** * The smallest table capacity for which bins may be treeified. * (Otherwise the table is resized if too many nodes in a bin.) * Should be at least 4 * TREEIFY_THRESHOLD to avoid conflicts * between resizing and treeification thresholds. */static final int MIN_TREEIFY_CAPACITY = 64; TreeSet的默认排序 TreeSet是有序的不可重复的, 有序是指元素值的大小 数值类型: 按照大小进行升序排序 字符串类型: 按照字典顺序进行升序排序 字符串从左到右, 一位一位的比较 自定义TreeSet类型: 实现 compareTo方法, 返回为0的情况会默认覆盖 1.2.2、Map 系Map 特点：存储的元素是键值对，在 JDK1.8 版本中是 Node，在老版本中是 Entry Map 接 口 有 五 个 常用 实 现 类 ： HashMap ， HashTable ， LinkeHashMap ， TreeMap ， ConcurrentHashMap 1. HashMap &amp; Hashtable 的区别HashMap 1. 非线程安全, 效率高 2. key不可以重复 3. key可以为null, 但只能有一个key为null Hashtable 线程安全, 效率低 key不可以重复 不可以为null 2. concurrentHash 简单分析是从 JDK1.5 之后提供的一个 HashTable 的替代实现，采 一个 map 中的元素分成很多的 segment，通过 lock 机制可以对每个 segment 加读写锁，从 而提高 map 的效率，底层实现采用数组+链表+红黑树的存储结构 Java并发包中的, 既是线程安全的, 又不至于效率过低 怎么实现: 分段锁机制 分段锁: 只加载在某一段数据上 MySql: 查询 - 95%, 增删改 - 5% 读锁: 共享锁, 一个线程进行操作的时候不应吸纳另一个线程的结构 写锁: 排它锁, 一个线程在进行操作的时候不允许其他任何线程的操作 3. put &amp; get 的流程put 的大致流程如下： 通过 hashcode 方法计算出 key 的 hash 值 通过 hash%length 计算出存储在 table 中的 index（源码中是使用 hash&amp;(length-1)，这样结 果相同，但是更快） 如果此时 table[index]的值为空，那么就直接存储，如果不为空那么就链接到这个数所在 的链表的头部。（在 JDK1.8 中，如果链表长度大于 8 就转化成红黑树） get 的大致流程如下： 通过 hashcode 计算出 key 的 hash 值 通过 hash%length 计算出存储在 table 中的 index（源码中是使用 hash&amp;(length-1)，这样结 果相同，但是更快） 遍历 table[index]所在的链表，只有当 key 与该节点中的 key 的值相同时才取出。 1.3 掌握重点List: ArrayList, LinkList Set: HashSet, TreeSet 需要掌握的方法: add , get, contains Map: HashMap, TreeMap 需要掌握的方法: put get map的循环遍历 containsKey…. 以上的都需要跟下源码 1.4 功能方法1.4.1 List 的功能方法ArrayList: 由数组实现的 List。允许对元素进行快速随机访问，但是向 List 中间插入与移 除元素的速度很慢。ListIterator 只应该用来由后向前遍历 ArrayList，而不是用来插入和移除 元素。因为那比 LinkedList 开销要大很多。 LinkedList : 对顺序访问进行了优化，向 List 中间插入与删除的开销并不大。随机访问则 相对较慢。(使用 ArrayList 代替。)还具有下列方 法：addFirst(), addLast(), getFirst(), getLast(), removeFirst() 和 removeLast(), 这些方法 (没有在任何接口或基类中定义过)使得 LinkedList 可以当作堆栈、队列和双向队列使用。 1.4.2 Set的功能方法Set : 存入 Set 的每个元素都必须是唯一的，因为 Set 不保存重复元素。加入 Set 的元素 必须定义 equals()方法以确保对象的唯一性。Set 与 Collection 有完全一样的接口。Set 接口 不保证维护元素的次序。 HashSet : 为快速查找设计的 Set。存入 HashSet 的对象必须定义 hashCode()。 TreeSet : 保存次序的 Set，底层为树结构。使用它可以从 Set 中提取有序的序列。 LinkedHashSet : 具有 HashSet 的查询速度，且内部使用链表维护元素的顺序(插入的次 序 。于是在使用迭代器遍历 Set 时，结果会按元素插入的次序显示。 1.4.3 Map 的功能方法Map : 维护“键值对”的关联性，使你可以通过“键”查找“值” HashMap : Map 基于散列表的实现。插入和查询“键值对”的开销是固定的。可以通过 构造器设置容量 capacity 和负载因子 load factor，以调整容器的性能。 LinkedHashMap : 类似于 HashMap，但是迭代遍历它时，取得“键值对”的顺序是其插 入次序，或者是最近最少使用(LRU)的次序。只比 HashMap 慢一点。而在迭代访问时发而更 快，因为它使用链表维护内部次序。 TreeMap : 基于红黑树数据结构的实现。查看“键”或“键值对”时，它们会被排序(次 序由 Comparabel 或 Comparator 决定)。TreeMap 的特点在 于，你得到的结果是经过排序的。 TreeMap 是唯一的带有 subMap()方法的 Map，它可以返回一个子树。 WeakHashMap : 弱键(weak key)Map，Map 中使用的对象也被允许释放: 这是为解决特 殊问题设计的。如果没有 map 之外的引用指向某个“键”，则此“键”可以被垃圾收集器回 收。 IdentifyHashMap : 使用==代替 equals()对“键”作比较的 hash map。专为解决特殊问题 而设计。 2、反射2.1 反射反射: 将 Java 类中的各个成分 (属性, 方法, 构造方法) 映射成对应的类 在运行时判断任意一个对象的所属的类 Class。 在运行时判断构造任意一个类的对象 Constructor。 在运行时判断任意一个类所具有的成员变量 Field 和方法 Method。 在运行时调用任意一个对象的方法。method.invoke(object, args) 反射的好处 提高了整个代码的灵活性 不需要知道细节 反射用的最多的时候, 就是写框架的时候 反射中需要掌握3个类: Constructor: 构造器的描述类 Field: 属性的描述类 Method: 方法的描述类 Java 预定义类型 是否是预定义类型: isPromitive(), 8种基本数据类型 + void 都是预定义类型 引用类型, 包装类不是预定义类型. 12System.out.println(int.class.isPrimitive()); // trueSystem.out.println(Integer.class.isPrimitive()); // false 2.2 ClassClass : 用于描述所有类的类, Class 类描述了类的属性信息，如类名、访问权限、包名、字 段名称列表、方法名称列表等, Class就是反射的基础. 获取Class的3种方式 1. `Class.forName`(&quot;类名字符串&quot;) (注意：类名字符串必须是全称，包名+类名) - 如果 `.class`已经被加载到内存了, 直接返回 - 如果没有的话, 就先加载到内存 2. `类名.class` 3. `实例对象.getClass()` 2.3 Constructor1234567891011121314151617181920212223242526// API// 补充: 可变参数 Class&lt;?&gt;... parameterTypespublic Constructor&lt;T&gt; getConstructor(Class&lt;?&gt;... parameterTypes) &#123;&#125;public Constructor&lt;?&gt;[] getConstructors() throws SecurityException &#123;&#125; ================//使用=============================////////获取构造方法//////////// 获取某个类的所有构造方法：Constructor[] constructor = Class.forName("java.lang.String").getConstructors();// 获取某个特殊（特定参数）的构造方法：Constructor constructor = Class.forName("java.lang.String").getConstructor(StringBuffer.class);////////创建实例对象//////////// 通常方式，直接调用构造方法：String str = new String("huangbo");// 反射方式：调用实参构造String str = (String)constructor.newInstance(new StringBuffer("huangbo"));// 反射方式：调用空参构造String obj = (String)Class.forName("java.lanng.String").newInstance();只有两个类拥有 newInstance()方法，分别是 Class 类和 Constructor 类 Class 类中的 newInstance() 方法是不带参数的，Constructor 类中的 newInstance()方法是带参数的(Object)，需要提供 必要的参数 2.4 FieldField类代表某个类中的一个成员变量，设有一个 obj 对象，Field 对象不是 obj 具体的变量值， 而是指代的是 obj 所属类的哪一个变量，可以通过 Field(对象).get(obj)获取相应的变量值 123456789101112131415// APIpublic Field[] getFields() throws SecurityException &#123;&#125;public Field getField(String name) &#123;&#125;================//使用=============================// getField 方法只能获取声明为 public 的变量，对于私有变量，可以通过 getDeclaredField()方法获 取 private 变量Field field = obj.getClass().getDeclaredField();// 将 private 变量设置为可访问；继承自父类AccessibleObject 的方法才可获取变量值field.setAccessible(true); // 获得对象值, 传入对象field.get(obj); // 反射替换,设置对象值// 传入对象,值// 把 obj 对象的 field 属性的值替换为 newValuefield.set(obj,newValue) 2.5 MethodMethod 类代表某个类中的成员方法 Method 对象不是具体的方法，而是来代表类中哪一个方法，与对象无关 123456789101112// 获取: 得到类中某一个方法：Method methodCharAt = Class.forName("java.lang.String").getMethod("charAt",int.class)// getMethod 方法用于得到一个方法对象，该方法接受的参数首先要有该方法名（String 类型），// 然后通过参数列表来区分重载那个方法，参数类型用 Class 对象来表示(如为 int 就用 int.class) // 调用方法：//普通方式：str.charAt(1)//反射方式：methodCharAt.invoke(str,1)// 以上两种调用方式等价 3. 设计模式设计模式（Design pattern）代表了面向对象编程中最佳的实践，通常被有经验的面向对象的 软件开发人员所采用。设计模式是软件开发人员在软件开发过程中面临的一般问题的解决方 案。这些解决方案是众多软件开发人员经过相当长的一段时间的试验和错误总结出来的。 设计模式只不过针对某些具体场景提供了一些效率较高的以复杂度换灵活性的手段而已 推荐学习站点 3.1 设计模式 – 六大原则总原则：开闭原则（Open Close Principle） 开闭原则就是说对扩展开放，对修改关闭。在程序需要进行拓展的时候，不能去修改原有的 代码，而是要扩展原有代码，实现一个热插拔的效果。所以一句话概括就是：为了使程序的 扩展性好，易于维护和升级。想要达到这样的效果，我们需要使用接口和抽象类等，后面的 具体设计中我们会提到这点。 六大原则： 单一职责原则 不要存在多于一个导致类变更的原因，也就是说每个类应该实现单一的职责，如若不然，就 应该把类拆分。 里氏替换原则（Liskov Substitution Principle） 里氏替换原则中，子类对父类的方法尽量不要重写和重载。因为父类代表了定义好的结构，通过这个规范的接口与外界交互，子类不应该随便破坏它。 依赖倒转原则（Dependence Inversion Principle） 这个是开闭原则的基础，具体内容：面向接口编程，依赖于抽象而不依赖于具体。写代码 时用到具体类时，不与具体类交互，而与具体类的上层接口交互。 接口隔离原则（Interface Segregation Principle） 这个原则的意思是：每个接口中不存在子类用不到却必须实现的方法，如果不然，就要将 接口拆分。使用多个隔离的接口，比使用单个接口（多个接口方法集合到一个的接口）要好。 迪米特法则（最少知道原则）（Demeter Principle） 就是说：一个类对自己依赖的类知道的越少越好 。也就是说无论被依赖的类多么复杂，都 应该将逻辑封装在方法的内部，通过 public 方法提供给外部。这样当被依赖的类变化时，才 能最小的影响该类。 合成复用原则（Composite Reuse Principle） 原则是尽量首先使用合成/聚合的方式，而不是使用继承。 3.2 设计模式 – 分类总体来说设计模式分为三大类： 创建型模式，共五种：工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。 结构型模式，共七种：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合 模式、享元模式。 行为型模式，共十一种：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模 式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。 3.3 常见设计模式3.3.1 单例模式(手写)单例模式（Singleton Pattern）是 Java 中最简单的,也是最最最常用的设计模式之一。这种类 型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。 注意: 单例类只能有一个实例。 单例类必须自己创建自己的唯一实例。 单例类必须给所有其他对象提供这一实例。 共有六种实现： 1、懒汉式，线程不安全 1234567891011public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125; 2、懒汉式，线程安全 12345678910public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125; 3、饿汉式 1234567public class Singleton &#123; private static Singleton instance = new Singleton(); private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; return instance; &#125; &#125; 4、双检锁/双重校验锁（DCL，即 double-checked locking） –面试必备 1234567891011121314public class Singleton &#123; private volatile static Singleton singleton; private Singleton ()&#123;&#125; public static Singleton getSingleton() &#123; if (singleton == null) &#123; synchronized (Singleton.class) &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125; &#125; 5、登记式/静态内部类 123456789public class Singleton &#123; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton ()&#123;&#125; public static final Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125; &#125; 6、枚举 12345public enum Singleton &#123; INSTANCE; public void whateverMethod() &#123; &#125; &#125; 详细请看 3.3.2 装饰器模式(手写)首先看一段代码 代码分析: 构造一个缓冲的字符输入流。包装了一个文件字符输入流。 事实上，BufferedReader 就是用来增强 FileReader 的读取的功能的。 FileReader 只有 read()方法， 但是 BufferedReader 中却增加了一个 readLine()的逐行读取 的功能 所以这就相当于是 BufferedReader 装饰了 FileReader，让 FileReader 变得更强大 装饰器模式概念 装饰器模式（Decorator Pattern）允许向一个现有的对象添加新的功能，同时又不改变其结 构。这种类型的设计模式属于结构型模式，它是作为现有的类的一个包装。 这种模式创建了一个装饰类，用来包装原有的类，并在保持类方法签名完整性的前提下， 提供了额外的功能。 装饰器模式实现方式1. 定义包装类 2. 将要装饰的模式作为参数传入包装类 3. 实现要加强的方法 3.3.2 代理模式1. 静态代理静态代理的缺点很明显：一个代理类只能对一个业务接口的实现类进行包装，如果有多个业 务接口的话就要定义很多实现类和代理类才行。 … 2. 动态代理第一种：JDK 动态代理实现 JDK 动态代理所用到的代理类在程序调用到代理类对象时才由 JVM 真正创建，JVM 根据传 进来的业务实现类对象以及方法名，动态地创建了一个代理类的 class 文件并被字节码引擎 执行，然后通过该代理类对象进行方法调用。我们需要做的，只需指定代理类的预处理、 调用后操作即可。 只能对实现了接口的类生成代理，而不是针对类，该目标类型实现的接口都将被代理。原理 是通过在运行期间创建一个接口的实现类来完成对目标对象的代理。具体实现步骤： 定义一个实现接口 InvocationHandler 的类 通过构造函数或者静态工厂方法等，注入被代理类 实现 invoke(Object proxy, Method method, Object[] args)方法 在主函数中获得被代理类的类加载器 使用 Proxy.newProxyInstance(classLoader, interfaces, args)产生一个代理对象 通过代理对象调用各种方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859=============//实现InvocationHandler====================import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;/** * @author shixuanji * 动态代理: JDK * 1. 实现一个接口 InvocationHandler * 2. 将代理对象作为属性传入 代理所有的类 * 3. 重写 invoke()方法 * */public class ProxyDynamicStudentDao implements InvocationHandler &#123; private Object o; public ProxyDynamicStudentDao(Object o) &#123; this.o = o; &#125; /* * @param proxy: 代理对象, 基本不用 ???应该是代理对象把, 老师写的被代理对象 * @param method: 拦截下来的被代理对象的方法 - 反射中描述方法的类 * @param args: 被代理对象业务方法的参数 */ public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; String methodName = method.getName().toString(); if (methodName.equals("insert")) &#123; // xxx &#125; else &#123; //ssssss &#125; // 做的事情就是对代理方法的方法的增强 // 增强 System.out.println("开始执行"); // 业务方法调用, 一定要调用被代理对象的 // obj: 对象 // args:方法的参数 Object res = method.invoke(o, args); // 增强 System.out.println("执行完了"); return res; &#125;&#125;============== // 使用========================/*** 参数1: 被代理对象的类加载器 * 参数2: 要实现的接口* 参数3: 代理类对象 * * 能.出来什么看 左边接收着* 运行时看 真正创建的对象*/// 目前看来 代理实例只能是接口BaseDAO newProxyInstance = (BaseDAO)Proxy.newProxyInstance(StudentDAO.class.getClassLoader(), StudentDAO.class.getInterfaces(), new ProxyDynamicStudentDao(new StudentDAO()));newProxyInstance.insert(new Teacher()); 第二种：CGLIB 动态代理实现： CGLIB 是针对类来实现代理的，原理是对指定的业务类生成一个子类，并覆盖其中业务方法 实现代理。因为采用的是继承，所以不能对 final 修饰的类进行代理，final 的方法也不能 针对类实现代理，对是否实现接口无要求。原理是对指定的类生成一个子类，覆盖其中的方 法，因为是继承，所以被代理的类或方法最好不要声明为 final 类型。具体实现步骤： 1、定义一个实现了 MethodInterceptor 接口的类 2、实现其 intercept()方法，在其中调用 proxy.invokeSuper() 3. 静态代理和动态代理的区别静态代理：自己编写创建代理类，然后再进行编译，在程序运行前，代理类的.class 文件就 已经存在了。 动态代理：在实现阶段不用关心代理谁，而在运行阶段（通过反射机制）才指定代理哪一个 对象。 3.4 重点掌握3.4.1. 装饰者模式 和 静态代理模式 区别在代码上的区别: 一般情况下, 装饰者模式被装饰的对象一般是从外部传入, 装饰的是一类的事务, 只要是某一类的(Class)都可以 静态代理模式被代理对象的初始化一般是内部创建的, 代理的是一个类的对象. 从功能上: 装饰者模式, 用于对被装饰者业务逻辑实现或增强, 对方法名没有要求 静态代理: 主要用于权限控制, 日志打印, 错误预警等功能 3.4.2. 三种设计模式必须掌握的 单例设计模式 装饰者模式 动态代理模式 3.4.3. 手写代码 冒泡排序 快速排序 设计模式 hadoop 的 wordcount scala 的 wordcount spark 的 wordcount 4. 排序算法 核心概念：算法复杂度、稳定性 算法复杂度：算法复杂度是指算法在编写成可执行程序后，运行时所需要的资源，资源包括 时间资源和内存资源。应用于数学和计算机导论。 稳定性：一个排序算法是稳定的，就是当有两个相等记录的关键字 R 和 S，且在原本的列表 中 R 出现在 S 之前，在排序过的列表中 R 也将会是在 S 之前。 4.1. 排序分类按照排序结果是否稳定性分类： 稳定排序：插入排序，冒泡排序，归并排序，计数排序，基数排序，桶排序（如果桶内 排序采用的是稳定性排序） 非稳定排序：选择排序，快速排序，堆排序。 按照排序过程中是否需要额外空间： 原地排序：插入排序，选择排序，冒泡排序，快速排序，堆排序。 非原地排序：归并排序，计数排序，基数排序，桶排序。 按照排序的主要操作分类： 交换类：冒泡排序、快速排序；此类的特点是通过不断的比较和交换进行排序； 插入类：简单插入排序、希尔排序；此类的特点是通过插入的手段进行排序； 选择类：简单选择排序、堆排序；此类的特点是看准了再移动； 归并类：归并排序；此类的特点是先分割后合并； 按照是否需要比较分类： 比较排序，时间复杂度 O(nlogn) ~ O(n^2)，主要有：冒泡排序，选择排序，插入排序， 归并排序，堆排序，快速排序等。 非比较排序，时间复杂度可以达到 O(n)，主要有：计数排序，基数排序，桶排序等。 4.2 常见排序的时间复杂度 有趣的排序算法视频 4.3 常见排序算法的核心实现4.3.1 冒泡排序 4.3.2 归并排序 4.3.3 快速排序]]></content>
      <categories>
        <category>Java</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习笔记-5]]></title>
    <url>%2F2018%2F05%2F28%2FLinux%2FLinux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-5%2F</url>
    <content type="text"><![CDATA[1.Shell操作日期时间date在类UNIX系统中，日期被存储为一个整数，其大小为自世界标准时间（UTC）1970年1月1日0时0分0秒起流逝的秒数。 语法 1date(选项)(参数) 选项 12345-d&lt;字符串&gt;：显示字符串所指的日期与时间。字符串前后必须加上双引号；-s&lt;字符串&gt;：根据字符串来设置日期与时间。字符串前后必须加上双引号；-u：显示GMT, 即目前的格林威治时间；--help：在线帮助；--version：显示版本信息。 参数 &lt;+时间日期格式&gt;：指定显示时使用的日期时间格式。 日期格式字符串列表 123456789101112131415161718192021222324%r 时间，12小时制%s 从1970年1月1日0点到目前经历的秒数%S 秒（00～59） %T 时间（24小时制）（hh:mm:ss）%X 显示时间的格式（％H时％M分％S秒）%Z 按字母表排序的时区缩写%a 星期名缩写%A 星期名全称%b 月名缩写%B 月名全称%c 日期和时间%d 按月计的日期（01～31）%D 日期（mm/dd/yy） %h 和%b选项相同%j 一年的第几天（001~366）%m 月份（01～12）%w 一个星期的第几天（0代表星期天）%W 一年的第几个星期（00～53，星期一为第一天）%x 显示日期的格式（mm/dd/yy）%y 年份的最后两个数字（1999则是99）%Y 年份（比如1970、1996等）%C 世纪，通常为省略当前年份的后两位数字%U 一年中的第几周，以周日为每星期第一天%e 按月计的日期，添加空格，等于%_d 实例 格式化输出 12date +"%Y-%m-%d"2009-12-07 输出昨天日期： 12date -d "1 day ago" +"%Y-%m-%d"2012-11-19 2秒后输出： 12date -d "2 second" +"%Y-%m-%d %H:%M.%S"2012-11-20 14:21.31 传说中的 1234567890 秒： 12date -d "1970-01-01 1234567890 seconds" +"%Y-%m-%d %H:%m:%S"2009-02-13 23:02:30 普通转格式： 12date -d "2009-12-12" +"%Y/%m/%d %H:%M.%S"2009/12/12 00:00.00 apache格式转换： 12date -d "Dec 5, 2009 12:00:37 AM" +"%Y-%m-%d %H:%M.%S"2009-12-05 00:00.37 格式转换后时间游走： 12date -d "Dec 5, 2009 12:00:37 AM 2 year ago" +"%Y-%m-%d %H:%M.%S"2007-12-05 00:00.37 加减操作： 1234567date +%Y%m%d //显示前天年月日date -d "+1 day" +%Y%m%d //显示前一天的日期date -d "-1 day" +%Y%m%d //显示后一天的日期date -d "-1 month" +%Y%m%d //显示上一月的日期date -d "+1 month" +%Y%m%d //显示下一月的日期date -d "-1 year" +%Y%m%d //显示前一年的日期date -d "+1 year" +%Y%m%d //显示下一年的日期 设定时间： 1234567date -s //设置当前时间，只有root权限才能设置，其他只能查看date -s 20120523 //设置成20120523，这样会把具体时间设置成空00:00:00date -s 01:01:01 //设置具体时间，不会对日期做更改date -s "01:01:01 2012-05-23" //这样可以设置全部时间date -s "01:01:01 20120523" //这样可以设置全部时间date -s "2012-05-23 01:01:01" //这样可以设置全部时间date -s "20120523 01:01:01" //这样可以设置全部时间 有时需要检查一组命令花费的时间，举例： 12345678#!/bin/bashstart=$(date +%s)nmap man.linuxde.net &amp;&gt; /dev/nullend=$(date +%s)difference=$(( end - start ))echo $difference seconds. 计算活了多少年 1echo $[($(date +%s -d $[date])-$(date +%s -d "19900318"))/86400/365] date -d其它的一些用法. 123456789101112131415161718192021222324252627## 获取下一天的时间[root@hadoop ~]# date -d next-day '+%Y-%m-%d %H:%M:%S'[root@hadoop ~]# date -d 'next day' '+%Y-%m-%d %H:%M:%S'另外一种写法：[root@hadoop ~]# date '+%Y-%m-%d %H:%M:%S' -d tomorrow## 获取上一天的时间 [root@hadoop ~]# date -d last-day '+%Y-%m-%d %H:%M:%S'另外一种写法：[root@hadoop ~]# date '+%Y-%m-%d %H:%M:%S' -d yesterday## 获取下一月的时间[root@hadoop ~]# date -d next-month '+%Y-%m-%d %H:%M:%S'## 获取上一月的时间 [root@hadoop ~]# date -d last-month '+%Y-%m-%d %H:%M:%S'## 获取下一年的时间[root@hadoop ~]# date -d next-year '+%Y-%m-%d %H:%M:%S'## 获取上一年的时间 [root@hadoop ~]# date -d last-year '+%Y-%m-%d %H:%M:%S'## 获取上一周的日期时间：[root@hadoop ~]# date -d next-week '+%Y-%m-%d %H:%M:%S'[root@hadoop ~]# date -d next-monday '+%Y-%m-%d %H:%M:%S'[root@hadoop ~]# date -d next-thursday '+%Y-%m-%d %H:%M:%S' 那么类似的，其实，last-year，last-month，last-day，last-week，last-hour，last-minute，last-second都有对应的实现。相反的，last对应next，自己可以根据实际情况灵活组织 接下来，我们来看‘–date’，它帮我实现任意时间前后的计算，来看具体的例子： 1234567## 获取一天以后的日期时间[root@hadoop ~]# date '+%Y-%m-%d %H:%M:%S' --date='1 day'[root@hadoop ~]# date '+%Y-%m-%d %H:%M:%S' --date='-1 day ago'## 获取一天以前的日期时间[root@hadoop ~]# date '+%Y-%m-%d %H:%M:%S' --date='-1 day'[root@hadoop ~]# date '+%Y-%m-%d %H:%M:%S' --date='1 day ago' 上面的例子显示出来了使用的格式，使用精髓在于改变前面的字符串显示格式，改变数据，改变要操作的日期对应字段，除了天也有对应的其他实现：year，month，week，day，hour，minute，second，monday（星期，七天都可） date 能用来显示或设定系统的日期和时间，在显示方面，使用者能设定欲显示的格式，格式设定为一个加号后接数个标记，其中可用的标记列表如下： 使用范例： 1[root@hadoop ~]# date '+%Y-%m-%d %H:%M:%S' 日期方面 12345678910111213141516%a : 星期几 (Sun..Sat) %A : 星期几 (Sunday..Saturday) %b : 月份 (Jan..Dec) %B : 月份 (January..December) %c : 直接显示日期和时间 %d : 日 (01..31) %D : 直接显示日期 (mm/dd/yy) %h : 同 %b %j : 一年中的第几天 (001..366) %m : 月份 (01..12) %U : 一年中的第几周 (00..53) (以 Sunday 为一周的第一天的情形) %w : 一周中的第几天 (0..6) %W : 一年中的第几周 (00..53) (以 Monday 为一周的第一天的情形) %x : 直接显示日期 (mm/dd/yyyy) %y : 年份的最后两位数字 (00.99) %Y : 完整年份 (0000..9999) 时间方面 123456789101112131415%%: 打印出%%n : 下一行%t : 跳格%H : 小时(00..23)%k : 小时(0..23)%l : 小时(1..12)%M : 分钟(00..59)%p : 显示本地AM或PM%P : 显示本地am或pm%r : 直接显示时间(12 小时制，格式为 hh:mm:ss [AP]M)%s : 从 1970 年 1 月 1 日 00:00:00 UTC 到目前为止的秒数%S : 秒(00..61)%T : 直接显示时间(24小时制)%X : 相当于%H:%M:%S %p%Z : 显示时区 若是不以加号作为开头，则表示要设定时间，而时间格式为 MMDDhhmm[[CC]YY][.ss] 1234567MM 为月份， DD 为日，hh 为小时，mm 为分钟，CC 为年份前两位数字，YY 为年份后两位数字，ss 为秒数 有用的小技巧12345678910111213141516171819202122## 获取相对某个日期前后的日期：cts1 ~ # date -d 'may 14 -2 weeks'2018年 04月 30日 星期一 00:00:00 CST## 把时间当中无用的0去掉，比如：01:02:25会变成1:2:25cts1 ~ # date '+%-H:%-M:%-S'19:18:22## 显示文件最后被更改的时间cts1 ~ # date "+%Y-%m-%d %H:%M:%S" -r install.log2018-05-23 10:11:14## 求两个字符串日期之间相隔的天数[root@hadoop ~]# expr '(' $(date +%s -d "2016-08-08") - $(date +%s -d "2016-09-09") ')' / 86400expr `expr $(date +%s -d "2016-08-08") - $(date +%s -d "2016-09-09")` / 86400## shell中加减指定间隔单位cts1 ~ # A=`date +%Y-%m-%d`cts1 ~ # B=`date +%Y-%m-%d -d "$A +48 hours"`cts1 ~ # echo $B2018-05-30 2. 文本处理wc功能： 统计文件行数、字节、字符数 选项1234567-c # 统计字节数，或--bytes或——chars：只显示Bytes数；。-l # 统计行数，或——lines：只显示列数；。-m # 统计字符数。这个标志不能与 -c 标志一起使用。-w # 统计字数，或——words：只显示字数。一个字被定义为由空白、跳格或换行字符分隔的字符串。-L # 打印最长行的长度。-help # 显示帮助信息--version # 显示版本信息 例子123456789101112131415161718192021222324wc -l * # 统计当前目录下的所有文件行数wc -l *.js # 统计当前目录下的所有 .js 后缀的文件行数find . * | xargs wc -l # 当前目录以及子目录的所有文件行数 wc test.txt # 查看文件的字节数、字数、行数# 查看文件的字节数、字数、行数wc test.txt# 输出结果7 8 70 test.txt行数 单词数 字节数 文件名# 用wc命令只打印统计文件行数不打印文件名wc -l test.txt # 输出结果7 test.txt# 用来统计当前目录下的文件数ls -l | wc -l# 输出结果8# 统计文件字数：cts1 ~ # wc -w date.txt30 date.txt sortsort命令 是在Linux里非常有用，它将文件进行排序，并将排序结果标准输出。sort命令既可以从特定的文件，也可以从stdin中获取输入。 选项123456789101112131415-b：忽略每行前面开始出的空格字符；-c：检查文件是否已经按照顺序排序；-d：排序时，处理英文字母、数字及空格字符外，忽略其他的字符；-f：排序时，将小写字母视为大写字母；-i：排序时，除了040至176之间的ASCII字符外，忽略其他的字符；-m：将几个排序号的文件进行合并；-M：将前面3个字母依照月份的缩写进行排序；-n：依照数值的大小排序；-o&lt;输出文件&gt;：将排序后的结果存入制定的文件；-r：以相反的顺序来排序；-t&lt;分隔字符&gt;：指定排序时所用的栏位分隔字符；-g：按照常规数值排序-k：位置1,位置2根据关键字排序，在从第位置1开始，位置2结束+&lt;起始栏位&gt;-&lt;结束栏位&gt;：以指定的栏位来排序，范围由起始栏位到结束栏位的前一栏位。 实例 sort将文件/文本的每一行作为一个单位，相互比较，比较原则是从首字符向后，依次按ASCII码值进行比较，最后将他们按升序输出。 123456789101112131415root@[mail text]# cat sort.txtaaa:10:1.1ccc:30:3.3ddd:40:4.4bbb:20:2.2eee:50:5.5eee:50:5.5[root@mail text]# sort sort.txtaaa:10:1.1bbb:20:2.2ccc:30:3.3ddd:40:4.4eee:50:5.5eee:50:5.5 忽略相同行使用-u选项或者uniq： 1234567891011121314151617181920212223[root@mail text]# cat sort.txtaaa:10:1.1ccc:30:3.3ddd:40:4.4bbb:20:2.2eee:50:5.5eee:50:5.5[root@mail text]# sort -u sort.txtaaa:10:1.1bbb:20:2.2ccc:30:3.3ddd:40:4.4eee:50:5.5或者[root@mail text]# uniq sort.txtaaa:10:1.1ccc:30:3.3ddd:40:4.4bbb:20:2.2eee:50:5.5 sort的-n、-r、-k、-t选项的使用： 123456789101112131415161718192021222324252627282930313233[root@mail text]# cat sort.txtAAA:BB:CCaaa:30:1.6ccc:50:3.3ddd:20:4.2bbb:10:2.5eee:40:5.4eee:60:5.1#将BB列按照数字从小到大顺序排列：[root@mail text]# sort -nk 2 -t: sort.txtAAA:BB:CCbbb:10:2.5ddd:20:4.2aaa:30:1.6eee:40:5.4ccc:50:3.3eee:60:5.1#将CC列数字从大到小顺序排列：[root@mail text]# sort -nrk 3 -t: sort.txteee:40:5.4eee:60:5.1ddd:20:4.2ccc:50:3.3bbb:10:2.5aaa:30:1.6AAA:BB:CC# -n是按照数字大小排序，-r是以相反顺序，-k是指定需要排序的栏位，-t指定栏位分隔符为冒号# 多列排序：以:分隔，按第二列数值排倒序，第三列正序[linux@linux ~]$ sort -n -t: -k2,2r -k3 sort.txt -k选项的具体语法格式： x,x 表示一个范围 123FStart.CStart Modifie,FEnd.CEnd Modifier-------Start--------,-------End-------- FStart.CStart 选项 , FEnd.CEnd 选项 这个语法格式可以被其中的逗号,分为两大部分， Start 部分和 End 部分。Start部分也由三部分组成，其中的Modifier部分就是我们之前说过的类似n和r的选项部分。我们重点说说Start部分的FStart和C.Start。C.Start也是可以省略的，省略的话就表示从本域的开头部分开始。FStart.CStart，其中FStart就是表示使用的域，而CStart则表示在FStart域中从第几个字符开始算“排序首字符”。同理，在End部分中，你可以设定FEnd.CEnd，如果你省略.CEnd，则表示结尾到“域尾”，即本域的最后一个字符。或者，如果你将CEnd设定为0(零)，也是表示结尾到“域尾”。 从公司英文名称的第二个字母开始进行排序： 1234567# 先创建此txt文件# 再排序$ sort -t ' ' -k 1.2 facebook.txtbaidu 100 5000sohu 100 4500google 110 5000guge 50 3000 -t &#39; &#39;, 首先用&#39; &#39;空格, 把字段分割成了3个域. 使用了-k 1.2，表示对第一个域的第二个字符开始到本域的最后一个字符为止的字符串进行排序。你会发现baidu因为第二个字母是a而名列榜首。sohu和 google第二个字符都是o，但sohu的h在google的o前面，所以两者分别排在第二和第三。guge只能屈居第四了。 只针对公司英文名称的第二个字母进行排序，如果相同的按照员工工资进行降序排序： 12345$ sort -t ' ' -k 1.2,1.2 -nrk 3,3 facebook.txtbaidu 100 5000google 110 5000sohu 100 4500guge 50 3000 由于只对第二个字母进行排序，所以我们使用了-k 1.2,1.2的表示方式，表示我们“只”对第二个字母进行排序。（如果你问“我使用-k 1.2怎么不行？”，当然不行，因为你省略了End部分，这就意味着你将对从第二个字母起到本域最后一个字符为止的字符串进行排序）。对于员工工资进行排 序，我们也使用了-k 3,3，这是最准确的表述，表示我们“只”对本域进行排序，因为如果你省略了后面的3，就变成了我们“对第3个域开始到最后一个域位置的内容进行排序” 了。 uniquniq命令 用于报告或忽略文件中的重复行，一般与sort命令结合使用。 选项123456-c或——count：在每列旁边显示该行重复出现的次数；-d或--repeated：仅显示重复出现的行列；-f&lt;栏位&gt;或--skip-fields=&lt;栏位&gt;：忽略比较指定的栏位；-s&lt;字符位置&gt;或--skip-chars=&lt;字符位置&gt;：忽略比较指定的字符；-u或——unique：仅显示出一次的行列；-w&lt;字符位置&gt;或--check-chars=&lt;字符位置&gt;：指定要比较的字符。 参数 输入文件：指定要去除的重复行文件。如果不指定此项，则从标准读取数据； 输出文件：指定要去除重复行后的内容要写入的输出文件。如果不指定此选项，则将内容显示到标准输出设备（显示终端）。 实例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758[root]# cat sort.txt : 原本行aaa:10:1.1ccc:30:3.3ddd:40:4.4bbb:20:2.2eee:50:5.5eee:50:5.5[root]# uniq sort.txt : 不展示重复行aaa:10:1.1ccc:30:3.3ddd:40:4.4bbb:20:2.2eee:50:5.5[cts1:Desktop][cts1:Desktop][root]# sort sort.txt | uniq : 不展示重复行aaa:10:1.1bbb:20:2.2ccc:30:3.3ddd:40:4.4eee:50:5.5[cts1:Desktop][root]# uniq -u sort.txt : 删除重复行aaa:10:1.1ccc:30:3.3ddd:40:4.4bbb:20:2.2[cts1:Desktop][root]# sort sort.txt | uniq -u : 排序并删除重复行aaa:10:1.1bbb:20:2.2ccc:30:3.3ddd:40:4.4[cts1:Desktop][root]# sort sort.txt | uniq -c : 展示每行出现的次数 1 aaa:10:1.1 1 bbb:20:2.2 1 ccc:30:3.3 1 ddd:40:4.4 2 eee:50:5.5[cts1:Desktop][root]# sort sort.txt | uniq -d : 只展示重复行eee:50:5.5# 求a.txt和b.txt的差集 ## 首先a b 去掉重复的, 再跟a b去重, 把b其它的部分也去掉cts1 Desktop # cat a b b | sort | uniq -uab# 求b.txt和a.txt的差集 ## 同上cts1 Desktop # cat b a a | sort | uniq -uef cutcut命令 用来显示行中的指定部分，删除文件中指定字段。cut经常用来显示文件的内容，类似于下的type命令。 说明：该命令有两项功能，其一是用来显示文件的内容，它依次读取由参数file所指 明的文件，将它们的内容输出到标准输出上；其二是连接两个或多个文件，如cut fl f2 &gt; f3将把文件fl和几的内容合并起来，然后通过输出重定向符“&gt;”的作用，将它们放入文件f3中。 当文件较大时，文本在屏幕上迅速闪过（滚屏），用户往往看不清所显示的内容。因此，一般用more等命令分屏显示。为了控制滚屏，可以按Ctrl+S键，停止滚屏；按Ctrl+Q键可以恢复滚屏。按Ctrl+C（中断）键可以终止该命令的执行，并且返回Shell提示符状态。 选项123456789-b：仅显示行中指定直接范围的内容；-c：仅显示行中指定范围的字符； -d：指定字段的分隔符，默认的字段分隔符为“TAB”； -f：显示指定字段的内容；-n：与“-b”选项连用，不分割多字节字符；--complement：补足被选择的字节、字符或字段；--out-delimiter=&lt;字段分隔符&gt;：指定输出内容是的字段分割符；--help：显示指令的帮助信息；--version：显示指令的版本信息。 参数文件：指定要进行内容过滤的文件。 实例例如有一个学生报表信息，包含No、Name、Mark、Percent： 12345[root@localhost text]# cat cut.txt No Name Mark Percent01 tom 69 9102 jack 71 8703 alex 68 98 -f使用-f 选项提取指定字段 12345[root@localhost text]# cut -d ' ' -f1 cut.txt No010203 12345[root@localhost text]# cut -d ' ' -f2,3 test.txt Name Marktom 69jack 71alex 68 –complement选项提取指定字段之外的列（打印除了第二列之外的列）： 12345cut -d ' ' -f2 --complement cut.txtNo Mark Percent01 69 9102 71 8703 68 98 指定字段的字符或者字节范围 &amp; 例子cut命令可以将一串字符作为列来显示，字符字段的记法： N- ：从第N个字节、字符、字段到结尾； N-M ：从第N个字节、字符、字段到第M个（包括M在内）字节、字符、字段； -M ：从第1个字节、字符、字段到第M个（包括M在内）字节、字符、字段。 上面是记法，结合下面选项将摸个范围的字节、字符指定为字段： -b 表示字节； -c 表示字符； -f 表示定义字段。 实例123456789101112131415161718192021222324252627282930[root@localhost text]# cat test.txt abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz# 打印第1个到第3个字符：cut -c1-3 test.txt abcabcabcabcabc# 打印前2个字符：cut -c-2 test.txt ababababab# 打印从第5个字符开始到结尾：cut -c5- test.txt efghijklmnopqrstuvwxyzefghijklmnopqrstuvwxyzefghijklmnopqrstuvwxyzefghijklmnopqrstuvwxyzefghijklmnopqrstuvwxyz grep(文本生成器)grep （global search regular expression(RE) and print out the line，全面搜索正则表达式并把行打印出来）是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。用于过滤/搜索的特定字符。可使用正则表达式能多种命令配合使用，使用上十分灵活。 选项123456789101112131415161718192021222324252627-a --text # 不要忽略二进制数据。-A &lt;显示行数&gt; --after-context=&lt;显示行数&gt; # 除了显示符合范本样式的那一行之外，并显示该行之后的内容。-b --byte-offset # 在显示符合范本样式的那一行之外，并显示该行之前的内容。-B&lt;显示行数&gt; --before-context=&lt;显示行数&gt; # 除了显示符合样式的那一行之外，并显示该行之前的内容。-c --count # 计算符合范本样式的列数。-C&lt;显示行数&gt; --context=&lt;显示行数&gt;或-&lt;显示行数&gt; # 除了显示符合范本样式的那一列之外，并显示该列之前后的内容。-d&lt;进行动作&gt; --directories=&lt;动作&gt; # 当指定要查找的是目录而非文件时，必须使用这项参数，否则grep命令将回报信息并停止动作。-e&lt;范本样式&gt; --regexp=&lt;范本样式&gt; # 指定字符串作为查找文件内容的范本样式。-E --extended-regexp # 将范本样式为延伸的普通表示法来使用，意味着使用能使用扩展正则表达式。-f&lt;范本文件&gt; --file=&lt;规则文件&gt; # 指定范本文件，其内容有一个或多个范本样式，让grep查找符合范本条件的文件内容，格式为每一列的范本样式。-F --fixed-regexp # 将范本样式视为固定字符串的列表。-G --basic-regexp # 将范本样式视为普通的表示法来使用。-h --no-filename # 在显示符合范本样式的那一列之前，不标示该列所属的文件名称。-H --with-filename # 在显示符合范本样式的那一列之前，标示该列的文件名称。-i --ignore-case # 忽略字符大小写的差别。-l --file-with-matches # 列出文件内容符合指定的范本样式的文件名称。-L --files-without-match # 列出文件内容不符合指定的范本样式的文件名称。-n --line-number # 在显示符合范本样式的那一列之前，标示出该列的编号。-q --quiet或--silent # 不显示任何信息。-R/-r --recursive # 此参数的效果和指定“-d recurse”参数相同。-s --no-messages # 不显示错误信息。-v --revert-match # 反转查找。-V --version # 显示版本信息。 -w --word-regexp # 只显示全字符合的列。-x --line-regexp # 只显示全列符合的列。-y # 此参数效果跟“-i”相同。-o # 只输出文件中匹配到的部分。 grep中的正则表达式12345678910111213141516171819202122232425&lt;sup&gt; # 锚定行的开始 如：'&lt;/sup&gt;grep'匹配所有以grep开头的行。 $ # 锚定行的结束 如：'grep$'匹配所有以grep结尾的行。 . # 匹配一个非换行符的字符 如：'gr.p'匹配gr后接一个任意字符，然后是p。 * # 匹配零个或多个先前字符 如：'*grep'匹配所有一个或多个空格后紧跟grep的行。 .* # 一起用代表任意字符。 [] # 匹配一个指定范围内的字符，如'[Gg]rep'匹配Grep和grep。 [&lt;sup&gt;] # 匹配一个不在指定范围内的字符，如：'[&lt;/sup&gt;A-FH-Z]rep'匹配不包含A-R和T-Z的一个字母开头，紧跟rep的行。 \ # 转义\(..\) # 标记匹配字符，如'\(love\)'，love被标记为1。 \&lt; # 锚定单词的开始，如:'\&lt;grep'匹配包含以grep开头的单词的行。 \&gt; # 锚定单词的结束，如'grep\&gt;'匹配包含以grep结尾的单词的行。 x\&#123;m\&#125; # 重复字符x，m次，如：'o\&#123;5\&#125;'匹配包含5个o的行。 x\&#123;m,\&#125; # 重复字符x,至少m次，如：'o\&#123;5,\&#125;'匹配至少有5个o的行。 x\&#123;m,n\&#125; # 重复字符x，至少m次，不多于n次，如：'o\&#123;5,10\&#125;'匹配5--10个o的行。 \w # 匹配文字和数字字符，也就是[A-Za-z0-9]，如：'G\w*p'匹配以G后跟零个或多个文字或数字字符，然后是p。 \W # \w的反置形式，匹配一个或多个非单词字符，如点号句号等。 \b # 单词锁定符，如: '\bgrep\b'只匹配grep。增录. # 任意一个字符a* # 任意多个a(0个或多个)a? # 0个或1个aa+ # 一个或多个a[A-Z][ABC] grep命令常见用法 在文件中搜索一个单词，命令会返回一个包含 “match_pattern” 的文本行： 12grep match_pattern file_namegrep "match_pattern" file_name 在多个文件中查找 1grep "match_pattern" file_1 file_2 file_3 ... 输出除 “match_pattern” 之外的所有行 &gt;&gt; -v 选项： 1grep -v "match_pattern" file_name 标记匹配颜色 –color=auto 选项： 12345678910grep "match_pattern" file_name --color=autoeg: 查找当前目录下所有文件中的 `a`cts1 Desktop # grep "\(a\)" ./* --color=auto---./a:a./abc.txt:abcdefghijklmnopqrstuvwxyz./abc.txt:abcdefghijklmnopqrstuvwxyz./abc.txt:abcdefghijklmnopqrstuvwxyz... 使用正则表达式 -E 选项：123grep -E "[1-9]+"或egrep "[1-9]+" 只输出文件中匹配到的部分 -o 选项： 12345echo this is a test line. | grep -o -E "[a-z]+\."line.echo this is a test line. | egrep -o "[a-z]+\."line. 统计文件或者文本中包含匹配字符串的行数 -c 选项： 1grep -c "text" file_name 输出包含匹配字符串的行数 -n 选项： 123456grep "text" -n file_name或cat file_name | grep "text" -n#多个文件grep "text" -n file_1 file_2 打印样式匹配所位于的字符或字节偏移 1234echo gun is not unix | grep -b -o "not"7:not#一行中字符串的字符便宜是从该行的第一个字符开始计算，起始值为0。选项 **-b -o** 一般总是配合使用。 搜索多个文件并查找匹配文本在哪些文件中 1grep -l "text" file1 file2 file3... grep递归搜索文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# 在多级目录中对文本进行递归搜索grep "text" . -r -n# .表示当前目录。# 忽略匹配样式中的字符大小写echo "hello world" | grep -i "HELLO"hello# 选项 -e 制动多个匹配样式：echo this is a text line | grep -e "is" -e "line" -oisline#也可以使用 **-f** 选项来匹配多个样式，在样式文件中逐行写出需要匹配的字符。cat patfileaaabbb# 用文件名称匹配, 实际上就是匹配文件中的内容echo aaa bbb ccc ddd eee | grep -f patfile -oaaabbb# 在grep搜索结果中包括或者排除指定文件：---#只在目录中所有的.php和.html文件中递归搜索字符"main()"grep "main()" . -r --include *.&#123;php,html&#125;#在搜索结果中排除所有README文件grep "main()" . -r --exclude "README"#在搜索结果中排除filelist文件列表里的文件grep "main()" . -r --exclude-from filelist---# 使用0值字节后缀的grep与xargs：---# 测试文件：echo "aaa" &gt; file1echo "bbb" &gt; file2echo "aaa" &gt; file3grep "aaa" file* -lZ | xargs -0 rm#执行后会删除file1和file3，grep输出用-Z选项来指定以0值字节作为终结符文件名（\0），xargs -0 读取输入并用0值字节终结符分隔文件名，然后删除匹配文件，-Z通常和-l结合使用。# 好吧, 暂时我也没懂是啥意思...---# grep静默输出：grep -q "test" filename# 不会输出任何信息，如果命令运行成功返回0，失败则返回非0值。一般用于条件测试。# 打印出匹配文本之前或者之后的行：# 显示匹配某个结果之后的3行，使用 -A 选项：seq 10 | grep "5" -A 35678# 显示匹配某个结果之前的3行，使用 -B 选项：seq 10 | grep "5" -B 32345# 显示匹配某个结果的前三行和后三行，使用 -C 选项：seq 10 | grep "5" -C 32345678# 如果匹配结果有多个，会用“--”作为各匹配结果之间的分隔符：echo -e "a\nb\nc\na\nb\nc" | grep a -A 1ab--ab grep其它常用用法12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 统计出现某个字符串的行的总行数 '-c'cts1 Desktop # echo "hello" &gt;&gt; hello.shcts1 Desktop # grep -c 'hello' hello.sh1# 查询不包含hello的行, 带有hello的整行都不会被查询到 '-v'cts1 Desktop # grep -v 'hello' hello.sh....# `.*`的用法, 前后都是 `.*` , 只要包含 parrten 的 整行 都会被查出来grep '.*hello.*' hello.sh# 任何由 'h..p'包含的都会被查出来. 查的当前行, .* 代表任意字符cts1 Desktop # grep 'h.*p' /etc/passwdgopher:x:13:30:gopher:/var/gopher:/sbin/nologinrpc:x:32:32:Rpcbind Daemon:/var/cache/rpcbind:/sbin/nolo# 正则表达以 ro 开头cts1 Desktop # grep '^ro' /etc/passwdroot:x:0:0:root:/root:/bin/zsh# 正则表达以 t 结尾cts1 Desktop # grep 't$' /etc/passwdhalt:x:7:0:halt:/sbin:/sbin/halt# '[Gg]rep'匹配Grep和grep## 此处是匹配 以 h || r 开头的cts1 Desktop # grep '^[hr]' /etc/passwdroot:x:0:0:root:/root:/bin/zshhalt:x:7:0:halt:/sbin:/sbin/haltrpc:x:32:32:Rpcbind Daemon:/var/cache/rpcbind:/sbin/nologin# 匹配非h之外其他的, 和 r 开头的, 也就是匹配 h以外的cts1 Desktop # grep '^[hr]' /etc/passwdroot:x:0:0:root:/root:/bin/zshhalt:x:7:0:halt:/sbin:/sbin/haltrpc:x:32:32:Rpcbind Daemon:/var/cache/rpcbind:/sbin/nologin# 匹配 h-r 以外的cts1 Desktop # grep '&lt;sup&gt;[&lt;/sup&gt;h-r]' /etc/passwdbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologin... sed(流编辑器)sed叫做流编辑器，在shell脚本和Makefile中作为过滤一使用非常普遍，也就是把前一个程序的输出引入sed的输入，经过一系列编辑命令转换成为另一种格式输出。sed是一种在线编辑器，它一次处理一行内容，处理时，把当前处理的行存储在临时缓冲区中，称为”模式空间”,接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。文件内容并没有改变，除非你使用重定向存储输出。 选项12345678910111213-n：一般sed命令会把所有数据都输出到屏幕，如果加入-n选项的话，则只会把经过sed命令处理的行输出到屏幕。-e&lt;script&gt;或--expression=&lt;script&gt;：以选项中的指定的script来处理输入的文本文件； 允许对输入数据应用多条sed命令编辑。-i：用sed的修改结果直接修改读取数据的文件，而不是由屏幕输出。-f&lt;script文件&gt;或--file=&lt;script文件&gt;：以选项中指定的script文件来处理输入的文本文件；-n或--quiet或——silent：仅显示script处理后的结果；--version：显示版本信息。# 动作:a：追加，在当前行后添加一行或多行。c：行替换，用c后面的字符串替换原数据行。i：插入，在当前行前插入一行或多行。p：打印，输出指定的行。s：字符串替换，用一个字符串替换另外一个字符串。格式为**\'**行范围s/旧字符串/新字符串/g**\'** (如果不加g的话，则表示只替换每行第一个匹配的串) sed元字符集(正则)12345678910111213**&lt;sup&gt;** 匹配行开始，如：/&lt;/sup&gt;sed/匹配所有以sed开头的行。**$** 匹配行结束，如：/sed$/匹配所有以sed结尾的行。**.** 匹配一个非换行符的任意字符，如：/s.d/匹配s后接一个任意字符，最后是d。**** * 匹配0个或多个字符，如：/*sed/匹配所有模板是一个或多个空格后紧跟sed的行。**[]** 匹配一个指定范围内的字符，如/[ss]ed/匹配sed和Sed。 **[&lt;sup&gt;]** 匹配一个不在指定范围内的字符，如：/[&lt;/sup&gt;A-RT-Z]ed/匹配不包含A-R和T-Z的一个字母开头，紧跟ed的行。**\(..\)** 匹配子串，保存匹配的字符，如s/\(love\)able/\1rs，loveable被替换成lovers。**&amp;** 保存搜索字符用来替换其他字符，如s/love/ **&amp;** /，love这成 **love** 。**\&lt;** 匹配单词的开始，如:/\&lt;love/匹配包含以love开头的单词的行。**\&gt;** 匹配单词的结束，如/love\&gt;/匹配包含以love结尾的单词的行。**x\&#123;m\&#125;** 重复字符x，m次，如：/0\&#123;5\&#125;/匹配包含5个0的行。**x\&#123;m,\&#125;** 重复字符x，至少m次，如：/0\&#123;5,\&#125;/匹配至少有5个0的行。**x\&#123;m,n\&#125;** 重复字符x，至少m次，不多于n次，如：/0\&#123;5,10\&#125;/匹配5~10个0的行。 实例 删除: d命令 sed ‘2d’ sed.txt —–删除sed.txt文件的第二行。 sed ‘2,$d’ sed.txt —–删除sed.txt文件的第二行到末尾所有行。 sed ‘$d’ sed.txt —–删除sed.txt文件的最后一行。 sed ‘/test/d ‘ sed.txt —–删除sed.txt文件所有包含test的行。 sed ‘/[A-Za-z]/d ‘ sed.txt —–删除sed.txt文件所有包含字母的行。 整行替换: c命令 将第二行替换成hello world sed \’2c hello world\’ sed.txt 字符串替换：s命令 sed ‘s/hello/hi/g’ sed.txt ## 在整行范围内把hello替换为hi。如果没有g标记，则只有每行第一个匹配的hello被替换成hi。 sed ‘s/hello/hi/2’ sed.txt ## 此种写法表示只替换每行的第2个hello为hi sed ‘s/hello/hi/2g’ sed.txt ## 此种写法表示只替换每行的第2个以后的hello为hi（包括第2个） sed -n ‘s/^hello/hi/p’ sed.txt ## (-n)选项和p标志一起使用表示只打印那些发生替换的行。也就是说，如果某一行开头的hello被替换成hi，就打印它。 sed -n ‘2,4p’ sed.txt ## 打印输出sed.txt中的第2行和第4行 sed -n ‘s/hello/&amp;-hi/gp’ sed.txt sed ‘s/^192.168.0.1/&amp;-localhost/‘ sed.txt sed ‘s/^192.168.0.1/[&amp;]/‘ sed.txt ## &amp;符号表示追加一个串到找到的串后, &amp;代表前一个串。 所有以192.168.0.1开头的行都会被替换成它自已加 -localhost，变成192.168.0.1-localhost。 第三句表示给IP地址添加中括号 sed -n ‘s/(liu)jialing/\1tao/p’ sed.txt sed -n ‘s/(liu)jia(ling)/\1tao\2ss/p’ sed.txt ## liu被标记为\1，所以liu会被保留下来（\1 == liu） ## ling被标记为\2，所以ling也会被保留下来（\2 == ling） ## 所以最后的结果就是\1tao\2ss == “liu” + “tao” + “ling” + “ss” 此处切记：\1代表的是被第一个()包含的内容，\1代表的是被第一个()包含的内容，…… 上面命令的意思就是：被括号包含的字符串会保留下来，然后跟其他的字符串比如tao和ss组成新的字符串liutaolingss sed ‘s#hello#hi#g’ sed.txt ## 不论什么字符，紧跟着s命令的都被认为是新的分隔符，所以，”#”在这里是分隔符，代替了默认的”/“分隔符。表示把所有hello替换成hi。 选定行的范围：逗号 sed -n ‘/today/,/hello/p’ sed.txt ## 所有在模板today和hello所确定的范围内的行都被打印。都找第一个，也就是说，从第一个today到第一个hello sed -n ‘5,/^hello/p’ sed.txt sed -n ‘/^hello/,8p’ sed.txt ## 打印从第五行开始到第一个包含以hello开始的行之间的所有行。 sed ‘/today/,/hello/s/$/www/‘ sed.txt ## 对于模板today和hello之间的行，每行的末尾用字符串www替换。 sed ‘/today/,/hello/s/^/www/‘ sed.txt ## 对于模板today和hello之间的行，每行的开头用字符串www替换。 sed ‘/^[A-Za-z]/s/5/five/g’ sed.txt ## 将以字母开头的行中的数字5替换成five 多点编辑：e命令 sed -e ‘1,5d’ -e ‘s/hello/hi/‘ sed.txt ## (-e)选项允许在同一行里执行多条命令。如例子所示，第一条命令删除1至5行，第二条命令用hello替换hi。命令的执行顺序对结果有影响。如果两个命令都是替换命令，那么第一个替换命令将影响第二个替换命令的结果。 sed –expression=’s/hello/hi/‘ –expression=’/today/d’ sed.txt ## 一个比-e更好的命令是–expression。它能给sed表达式赋值。 从文件读入：r命令 sed ‘/hello/r file’ sed.txt ## file里的内容被读进来，显示在与hello匹配的行下面，如果匹配多行，则file的内容将显示在所有匹配行的下面。 写入文件：w命令 sed -n ‘/hello/w file’ sed.txt ## 在huangbo.txt中所有包含hello的行都被写入file里。 追加命令：a命令 sed ‘/^hello/a\—&gt;this is a example’ sed.txt ## ‘—&gt;this is a example’被追加到以hello开头的行(另起一行)后面，sed要求命令a后面有一个反斜杠。 插入：i命令 sed ‘/will/i\some thing new ————————-‘ sed.txt ## 如果test被匹配，则把反斜杠后面的文本插入到匹配行的前面。 下一个：n命令 sed ‘/hello/{n; s/aa/bb/;}’ sed.txt 替换下一行的第一个aa sed ‘/hello/{n; s/aa/bb/g;}’ sed.txt 替换下一行的全部aa ## 如果hello被匹配，则移动到匹配行的下一行，替换这一行的aa，变为bb，并打印该行，然后继续。 退出：q命令 sed ‘10q’ sed.txt ## 打印完第10行后，退出sed。 同样的写法： sed -n ‘1,10p ‘ sed.txt awk(报表生成器)Awk是一个强大的处理文本的编程语言工具，其名称得自于它的创始人Alfred Aho、Peter Weinberger和Brian Kernighan 姓氏的首个字母，相对于grep的查找，sed的编辑，awk在其对数据分析并生成报告时，显得尤为强大。AWK 提供了极其强大的功能：可以进行样式装入、流控制、数学运算符、进程控制语句甚至于内置的变量和函数。简单来说awk就是扫描文件中的每一行，查找与命令行中所给定内容相匹配的模式。如果发现匹配内容，则进行下一个编程步骤。如果找不到匹配内容，则继续处理下一行。 语法12awk [options] 'script' var=value file(s)awk [options] -f scriptfile var=value file(s) … TODO 待补充 实例 假设last -n 5的输出如下: 123456root@cts1:~ # last -n 5root pts/1 192.168.170.1 Mon May 28 09:28 still logged inroot pts/1 192.168.170.1 Sun May 27 20:53 - 07:21 (10:28)root pts/0 192.168.170.1 Sun May 27 19:39 still logged inreboot system boot 2.6.32-573.el6.x Sun May 27 19:34 - 14:48 (19:13)root pts/1 192.168.170.1 Sat May 26 20:43 - 22:36 (01:53) 只显示五个最近登录的账号： 12345678root@cts1:~ # last -n 5 | awk '&#123;print $1&#125;'rootrootrootrebootroot# awk工作流程是这样的：读入有'\n'换行符分割的一条记录，然后将记录按指定的域分隔符划分域，填充域，$0则表示所有域,$1表示第一个域,$n表示第n个域。默认域分隔符是"空白键" 或 "[tab]键",所以$1表示登录用户，$3表示登录用户ip,以此类推 显示/etc/passwd的账户： 12345678root@cts1:~ # cat /etc/passwd | awk -F':' '&#123;print $1&#125;'rootbindaemon...# 这种是awk+action的示例，每行都会执行action&#123;print $1&#125;。# -F指定域分隔符为':' 显示/etc/passwd的账户和账户对应的shell,而账户与shell之间以tab键分割 12345root@cts1:~ # cat /etc/passwd | awk -F':' '&#123;print $1"\t"$7&#125;'root /bin/zshbin /sbin/nologindaemon /sbin/nologin... BEGIN and END 如果只是显示/etc/passwd的账户和账户对应的shell,而账户与shell之间以逗号分割,而且在所有行添加列名name,shell,在最后一行添加”blue,/bin/nosh”。 123456789101112131415161718root@cts1:~ # cat /etc/passwd |awk -F ':' 'BEGIN &#123;print "name,shell"&#125; &#123;print $1","$7&#125; END &#123;print "blue,/bin/nosh"&#125;'---name,shellroot,/bin/zshbin,/sbin/nologin...mysql,/bin/bashblue,/bin/nosh-------------------------root@cts1:~ # cat /etc/passwd | awk -F ':' 'BEGIN &#123;print "name \t shell"&#125; &#123;print$1"\t"$7&#125; END &#123;print "blue,/bin/bash"&#125;'---name shellroot /bin/zshbin /sbin/nologin...mysql /bin/bashblue,/bin/bash awk工作流程是这样的：先执行BEGIN，然后读取文件，读入有/n换行符分割的一条记录，然后将记录按指定的域分隔符划分域，填充域，$0则表示所有域,$1表示第一个域,$n表示第n个域,随后开始执行模式所对应的动作action。接着开始读入第二条记录••••••直到所有的记录都读完，最后执行END操作。 搜索/etc/passwd有root关键字的所有行 1234567891011121314151. root@cts1:~ # awk -F: '/root/' /etc/passwd --- root:x:0:0:root:/root:/bin/zsh operator:x:11:0:operator:/root:/sbin/nologin # 这种是pattern的使用示例，匹配了pattern(这里是root)的行才会执行action(没有指定action，默认输出每行的内容)。2. 搜索支持正则，例如找root开头的: awk -F: '/^root/' /etc/passwd# 搜索/etc/passwd有root关键字的所有行，并显示对应的shellroot@cts1:~ # awk -F ':' '/root/&#123;print $7&#125;' /etc/passwd/bin/zsh/sbin/nologin# 这里指定了action&#123;print $7&#125; awk常见内置变量 FILENAME：awk浏览的文件名 FNR：浏览文件的记录数，也就是行数。awk是以行为单位处理的，所以每行就是一个记录 NR：awk读取文件每行内容时的行号 NF：浏览记录的域的个数。可以用它来输出最后一个域 FS：设置输入域分隔符，等价于命令行-F选项 OFS：输出域分隔符 统计/etc/passwd:文件名，每行的行号，每行的列数，对应的完整行内容 1234root@cts1:~ # awk -F ':' '&#123;print "filename:" FILENAME ",linenumber:" NR ",columns:" NF ",linecontent:"$0&#125;' /etc/passwd---filename:/etc/passwd,linenumber:1,columns:7,linecontent:root:x:0:0:root:/root:/bin/zshfilename:/etc/passwd,linenumber:2,columns:7,linecontent:bin:x:1:1:bin:/bin:/sbin/nologin 使用printf替代print,可以让代码更加简洁，易读 1234root@cts1:~ # awk -F ':' '&#123;printf("filename:%s,linenumber:%s,columns:%s,linecontent:%s\n",FILENAME,NR,NF,$0)&#125;' /etc/passwd---filename:/etc/passwd,linenumber:1,columns:7,linecontent:root:x:0:0:root:/root:/bin/zshfilename:/etc/passwd,linenumber:2,columns:7,linecontent:bin:x:1:1:bin:/bin:/sbin/nologin 指定输入分隔符，指定输出分隔符： 12345root@cts1:~ # awk 'BEGIN &#123;FS=":"; OFS="\t"&#125; &#123;print $1, $2&#125;' /etc/passwd---root xbin x... 实用例子 1234567891011121314# A：打印最后一列：awk -F: '&#123;print $NF&#125;' /etc/passwdawk -F: '&#123;printf("%s\n",$NF);&#125;' /etc/passwd# B：统计文件行数：awk 'BEGIN &#123;x=0&#125; &#123;x++&#125; END &#123;print x&#125;' /etc/passwd# C：打印9*9乘法表：awk 'BEGIN&#123;for(n=0;n++&lt;9;)&#123;for(i=0;i++&lt;n;)printf i"*"n"="i*n" ";print ""&#125;&#125;'awk 'BEGIN &#123;for(i=1;i&lt;=9;i++)&#123;for(j=1;j&lt;=i;j++)&#123;printf i"*"j"="i*j" ";&#125;print ""&#125;&#125;'awk 'BEGIN &#123;for(i=9;i&gt;=1;i--)&#123;for(j=i;j&gt;=1;j--)&#123;printf i"*"j"="i*j" ";&#125;print ""&#125;&#125;'# D: 计算1-100 之和echo "sum" | awk 'BEGIN &#123;sum=0;&#125; &#123;i=0;while(i&lt;101)&#123;sum+=i;i++&#125;&#125; END &#123;print sum&#125;' 更多详细用法参见官网 find功能： 搜索文件目录层次结构 格式： find path -option actions find &lt;路径&gt; &lt;选项&gt; [表达式] 常用可选项：1234567891011121314151617-name 根据文件名查找，支持(\'\* \' , \'? \')-type 根据文件类型查找(f-普通文件，c-字符设备文件，b-块设备文件，l-链接文件，d-目录)-perm 根据文件的权限查找，比如 755-user 根据文件拥有者查找-group 根据文件所属组寻找文件-size 根据文件小大寻找文件 -o 表达式 或-a 表达式 与-not 表达式 非 类型参数列表：1234567- **f** 普通文件- **l** 符号连接- **d** 目录- **c** 字符设备- **b** 块设备- **s** 套接字- **p** Fifo 示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[linux@linux txt]$ ll ## 准备的测试文件total 248-rw-rw-r--. 1 linux linux 235373 Apr 18 00:10 hw.txt-rw-rw-r--. 1 linux linux 0 Apr 22 05:43 LINUX.pdf-rw-rw-r--. 1 linux linux 3 Apr 22 05:50 liujialing.jpg-rw-rw-r--. 1 linux linux 0 Apr 22 05:43 mingxing.pdf-rw-rw-r--. 1 linux linux 57 Apr 22 04:40 mingxing.txt-rw-rw-r--. 1 linux linux 66 Apr 22 05:15 sort.txt-rw-rw-r--. 1 linux linux 214 Apr 18 10:08 test.txt-rw-rw-r--. 1 linux linux 24 Apr 22 05:27 uniq.txt[linux@linux txt]$ find /home/linux/txt/ -name "*.txt" ## 查找文件名txt结尾的文件/home/linux/txt/uniq.txt/home/linux/txt/mingxing.txt/home/linux/txt/test.txt/home/linux/txt/hw.txt/home/linux/txt/sort.txt## 忽略大小写查找文件名包含linux[linux@linux txt]$ find /home/linux/txt -iname "*linux*" /home/linux/txt/LINUX.pdf## 查找文件名结尾是.txt或者.jpg的文件[linux@linux txt]$ find /home/linux/txt/ \( -name "*.txt" -o -name "*.jpg" \) /home/linux/txt/liujialing.jpg/home/linux/txt/uniq.txt/home/linux/txt/mingxing.txt/home/linux/txt/test.txt/home/linux/txt/hw.txt/home/linux/txt/sort.txt另一种写法：find /home/linux/txt/ -name "*.txt" -o -name "*.jpg"# 使用正则表达式的方式去查找上面条件的文件：[linux@linux txt]$ find /home/linux/txt/ -regex ".*\(\.txt\|\.jpg\)$"/home/linux/txt/liujialing.jpg/home/linux/txt/uniq.txt/home/linux/txt/mingxing.txt/home/linux/txt/test.txt/home/linux/txt/hw.txt/home/linux/txt/sort.txt## 查找.jpg结尾的文件，然后删掉[linux@linux txt]$ find /home/linux/txt -type f -name "*.jpg" -delete[linux@linux txt]$ lltotal 248-rw-rw-r--. 1 linux linux 235373 Apr 18 00:10 hw.txt-rw-rw-r--. 1 linux linux 0 Apr 22 05:43 LINUX.pdf-rw-rw-r--. 1 linux linux 0 Apr 22 05:43 mingxing.pdf-rw-rw-r--. 1 linux linux 57 Apr 22 04:40 mingxing.txt-rw-rw-r--. 1 linux linux 66 Apr 22 05:15 sort.txt-rw-rw-r--. 1 linux linux 214 Apr 18 10:08 test.txt-rw-rw-r--. 1 linux linux 24 Apr 22 05:27 uniq.txt## 查找5天以内创建的 .sh 文件, 并显示创建/更改时间find / -name "*sh" -mtime -5 |xargs ls -l 3. Shell操作字符串字符串截取Linux中操作字符串，也是一项必备的技能。其中尤以截取字符串更加频繁，下面为大家介绍几种常用方式，截取字符串 预先定义一个变量：WEBSITE=’http://hadoop//centos/huangbo.html&#39; #截取，*任意的, 删除//左边字符串（包括制定的分隔符），保留右边字符串 root@cts1:~ # echo ${WEBSITE#*//}hadoop//centos/huangbo.html ##截取，删除左边字符串（包括指定的分隔符），保留右边字符串，和上边一个#不同的是，它一直找到最后，而不是像一个#那样找到一个就满足条件退出了。 root@cts1:~ # echo ${WEBSITE##*//}centos/huangbo.html %截取，删除右边字符串（包括制定的分隔符），保留左边字符串 root@cts1:~ # echo ${WEBSITE%//*}http://hadoop %%截取，删除右边字符串（包括指定的分隔符），保留左边字符串，和上边一个%不同的是，它一直找到最前，而不是像一个%那样找到一个就满足条件退出了。 root@cts1:~ # echo ${WEBSITE%%//*}http: 从左边第几个字符开始，以及截取的字符的个数 root@cts1:~ # echo ${WEBSITE:2:2}tp 从左边第几个字符开始，一直到结束 root@cts1:~ # echo ${WEBSITE:2}tp://hadoop//centos/huangbo.html 从右边第几个字符开始，以及字符的个数, 从-4开始, 还是往右边截取 root@cts1:~ # echo ${WEBSITE:0-4:2 ht 从右边第几个字符开始，一直到结束 root@cts1:~ # echo ${WEBSITE:0-4}html 利用awk进行字符串截取 echo $WEBSITE | awk ‘{print substr($1,2,6)}’ ttp:// 利用cut进行字符串截取 root@cts1:~ # echo $WEBSITE | cut -b 1-4 http 获取最后几个字符 root@cts1:~ # echo ${WEBSITE:(-3)} tml 截取从倒数第3个字符后的2个字符 root@cts1:~ # echo ${WEBSITE:(-3):2}tm 字符串替换使用格式 ${parameter/pattern/string} 例子12345678910# 定义变量VAR：[linux@linux ~]$ VAR="hello tom, hello kitty, hello xiaoming"# 替换第一个hello, 用`/`：[linux@linux ~]$ echo $&#123;VAR/hello/hi&#125;hi tom, hello kitty, hello xiaoming# 替换所有hello, 用'//'：[linux@linux ~]$ echo $&#123;VAR//hello/hi&#125;hi tom, hi kitty, hi xiaoming 获取字符串长度在此为大家提供五种方式获取某字符串的长度 123456789101112131415161718192021222324252627282930313233343536# 在此为大家提供五种方式获取某字符串的长度# 1. 使用wc -L命令+----------------------------------------------------+| echo $&#123;WEBSITE&#125; |wc -L || || 35 |+----------------------------------------------------+# 2. 使用expr的方式去计算+---------------------------------------------------+| expr length $&#123;WEBSITE&#125; || || 35 |+---------------------------------------------------+# 3. 通过awk + length的方式获取字符串长度+---------------------------------------------------------------------------+|echo $&#123;WEBSITE&#125; | awk '&#123;print length($0)&#125;' || || 35 |+---------------------------------------------------------------------------+# 4. 通过awk的方式计算以**\"\"**分隔的字段个数+-------------------------------------------------------------------------+| echo $&#123;WEBSITE&#125; |awk -F "" '&#123;print NF&#125;' || || 35 |+-------------------------------------------------------------------------+# 5. 通过\#的方式获取字符串（最简单，最常用）+----------------------------------------------+| echo $&#123;#WEBSITE&#125; || || 35 |+----------------------------------------------+ 4. 脚本自动安装MySql这里先做个记录, 之后会整理一份更详细的文档出来.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#!/bin/bash## auto install mysql## 假如是第二次装，那么要先停掉服务，并且卸载之前的mysqlservice mysql stopEXISTS_RPMS=`rpm -qa | grep -i mysql`echo $&#123;EXISTS_RPMS&#125;for RPM in $&#123;EXISTS_RPMS&#125;do rpm -e --nodeps $&#123;RPM&#125;done## 删除残留文件rm -fr /usr/lib/mysqlrm -fr /usr/include/mysqlrm -f /etc/my.cnfrm -fr /var/lib/mysql## 从服务器获取安装mysql的rpm包wget http://linux/soft/MySQL-client-5.6.26-1.linux_glibc2.5.x86_64.rpmwget http://linux/soft/MySQL-server-5.6.26-1.linux_glibc2.5.x86_64.rpm## 删除之前的密码文件，以免产生干扰rm -rf /root/.mysql_secret## 安装服务器rpm -ivh MySQL-server-5.6.26-1.linux_glibc2.5.x86_64.rpm## 获取到生成的随机密码##PSWD=`cat /root/.mysql_secret | awk -F ':' '&#123;print substr($4,2,16)&#125;'`PSWD=` grep -v '^$' /root/.mysql_secret | awk -F ':' '&#123;print substr($4,2,16)&#125;'`##PSWD=$&#123;PWD:1:16&#125;## 安装客户端rpm -ivh MySQL-client-5.6.26-1.linux_glibc2.5.x86_64.rpm## 然后删除刚刚下下来的rpm包rm -rf MySQL-client-5.6.26-1.linux_glibc2.5.x86_64.rpmrm -rf MySQL-server-5.6.26-1.linux_glibc2.5.x86_64.rpm## 提示安装的步骤都完成了。echo "install mysql server and client is done .!!!!!!"## 打印出来刚刚生成的mysql初始密码echo "random password is:$&#123;PSWD&#125;"## 开启mysql服务service mysql start# 手动第一次登陆，然后改掉密码[root@hadoop bin]# mysql -uroot -pZjVIWvOGD18bT7oXmysql&gt; set PASSWORD=PASSWORD('root');# 现在就可以写脚本链接mysql进行操作了[root@hadoop bin]# vi initMysql.sh#!/bin/bashmysql -uroot -proot &lt;&lt; EOF GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION; FLUSH PRIVILEGES; use mysql; select host, user, password from user;EOF]]></content>
      <categories>
        <category>Linux</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习笔记-4]]></title>
    <url>%2F2018%2F05%2F28%2FLinux%2FLinux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4%2F</url>
    <content type="text"><![CDATA[1、Linux shell简介1.1、Shell概述Shell本身是一个用C语言编写的程序，它是用户使用Unix/Linux的桥梁，用户的大部分工作都是通过Shell完成的。 Shell既是一种命令语言，又是一种程序设计语言： 作为命令语言，它交互式地解释和执行用户输入的命令； 作为程序设计语言，它定义了各种变量和参数，并提供了许多在高级语言中才具有的控制结构，包括循环和分支。 Shell它虽然不是Unix/Linux系统内核的一部分，但它调用了系统核心的大部分功能来执行程序、建立文件并以并行的方式协调各个程序的运行。Shell是用户与内核进行交互操作的一种接口，目前最流行的Shell称为bash Shell（Bourne Again Shell） Shell是一门编程语言(解释型的编程语言)，即shell脚本(就是在用linux的shell命令编程)，Shell脚本程序从脚本中一行一行读取并执行这些命令，相当于一个用户把脚本中的命令一行一行敲到Shell提示符下执行 Shell是一种脚本语言，那么，就必须有解释器来执行这些脚本 Unix/Linux上常见的Shell脚本解释器有bash、sh、csh、ksh等，习惯上把它们称作一种Shell。我们常说有多少种Shell，其实说的是Shell脚本解释器，可以通过cat /etc/shells命令查看系统中安装的shell，不同的shell可能支持的命令语法是不相同的 sh是Unix 标准默认的shell，由Steve Bourne开发，是Bourne Shell的缩写。 bash是Linux标准默认的shell，本教程也基于bash讲解。bash由Brian Fox和Chet Ramey共同完成，是Bourne Again Shell的缩写。 Shell本身支持的命令并不多，内部命令一共有40个，但是它可以调用其他的程序，每个程序就是一个命令，这使得Shell命令的数量可以无限扩展，其结果就是Shell的功能非常强大，完全能够胜任Linux的日常管理工作，如文本或字符串检索、文件的查找或创建、大规模软件的自动部署、更改系统设置、监控服务器性能、发送报警邮件、抓取网页内容、压缩文件等。 1.2、Shell基本格式代码写在普通文本文件中，通常以.sh结尾，虽然不是强制要求，但希望大家最好这么做 12345vi helloworld.sh--------------------------#!/bin/bash ## 表示用哪一种shell解析器来解析执行我们的这个脚本程序，这句话只对自执行有效，对于使用sh helloworld.sh无效echo "hello world" ## 注释也可以写在这里 在这里，我们就写好了一个shell脚本，第一行是固定需要的，表明用哪一种shell解析器来执行我们的这个脚本程序。本质上，shell脚本里面的代码都是就是一些流程控制语句加一些特殊语法再加shell**命令**组成。其中，我们可以当做每一个命令就是shell编程当中的关键字。 1.3、Shell执行方式1 、sh 方式或者bash 方式 sh helloworld.sh bash helloworld.sh ## 直接指定用系统默认的bash shell解释执行 2 、source 方式或者. 方式 source命令也称为“点命令”，也就是一个点符号（.）,是bash的内部命令。 功能：使Shell读入指定的Shell程序文件并依次执行文件中的所有语句 source命令通常用于重新执行刚修改的初始化文件，使之立即生效，而不必注销并重新登录。 用法： . helloworld.sh source helloworld.sh 3 、直接执行该脚本文件 可以有两种方式，不过这两种方式的执行，都需要该文件有执行权限 所以在执行之前，我们要更改他的执行权限 1、 切换到该文件所在的路径然后执行命令： ./helloworld.sh 2、 直接以绝对路径方式执行 /home/linux/hellworld.sh 1.4、Shell注释单行注释：Shell脚本中以#开头的代码就是注释 # xxx 多行注释：Shell脚本中也可以使用多行注释：:&lt;&lt;! xxx ! 2、Shell基本语法2.1、变量2.1.1、系统变量Linux Shell中的变量分为“系统变量”和“用户自定义变量” 系统变量可以通过set命令查看，用户环境变量可以通过env查看： 常用系统变量：\$PWD \$SHELL \$USER $HOME 2.1.2、自定义变量12345678910111213141516171819# 1.注意, 变量中间不能有空格A=123 echo $A# 2.变量中间有空格的话要加引号 - 双引号中间可以引用变量 a=zs b="this is $a" echo $b this is zs - 单引号引用变量会原样输出 b='this is $a' echo $b this is $a # 3. 要在变量后直接连接字符, 要用&#123;&#125;把变量括起来 $&#123;变量名&#125;其它字符 echo $&#123;A&#125;ddd helloddd 2.1.3、变量高级用法 撤销变量：unset ABC 声明静态变量：readonly ABC= ‘abc’ 特点是这种变量是只读的，不能unset 在一个 .sh 中. 以绝对路径的形式调另一个 .sh 使用export关键字 export A=”A in a.sh” 意味着把变量提升为当前shell 进程中的全局环境变量，可供其他子shell 程序使用， A 变量就成了a.sh 脚本所在bash 进程的全局变量，该进程的所有子进程都能访问到变量A 通过 . /root/bin/b.sh 或 source /root/bin/b.sh 来调用 总结： a.sh中直接调用b.sh，会让b.sh在A所在的bash进程的“子进程”空间中执行 而子进程空间只能访问父进程中用export定义的变量 一个shell进程无法将自己定义的变量提升到父进程空间中去 source或者“.”号执行脚本时，会让脚本在调用者所在的shell进程空间中执行 2.1.4、反引号赋值 a=`ls -l /root/bin` ##反引号，运行里面的命令，并把结果返回给变量a 另外一种写法：a=$(ls -l /root/bin) 2.1.5、变量有用技巧 形式 说明 ${var} 变量本来的值 ${var:-word} 如果变量 var 为空或已被删除(unset)，那么返回 word，但不改变 var 的值 ${var:+word} 如果变量 var 被定义，那么返回word，但不改变 var 的值 ${var:=word} 如果变量 var 为空或已被删除(unset)，那么返回 word，并将 var 的值设置为 word ${var:?message} 如果变量 var 为空或已被删除(unset)，那么将消息 message 送到标准错误输出，可以用来检测变量 var 是否可以被正常赋值。 若此替换出现在Shell脚本中，那么脚本将停止运行 2.1.6 特殊变量 $? 表示上一个命令退出的状态码 $$ 表示当前进程编号 $0 表示当前脚本名称 $n 表示n 位置的输入参数（n**代表数字，n&gt;=1 ）** $# 表示参数的个数，常用于循环 $* 和$@ 都表示参数列表 注意：$*与$@区别 $* 和 $@ 都表示传递给函数或脚本的所有参数** 不被双引号” “包含时 $* 和 $@ 都以\$1 \$2 … \$n 的形式组成参数列表 当它们被双引号” “包含时 “$*“ 会将所有的参数作为一个整体，以”\$1 \$2 … \$n”的形式组成一个整串； $* 会将各个参数分开，以”\$1” “\$2” … “\$n” 的形式组成一个参数列表 2.1.7 变量的其他注意点 使用变量 使用一个定义过的变量, 只需要在变量名前面加 $ 符号 如果变量后面直接跟上了字符串, 就必须要加花括号 推荐给所有变量加上花括号, 这个是好的编程习惯 已定义的非只读变量, 可以被重新定义 只读变量 使用 readonly 命令可以将变量定义为只读变量，只读变量的值不能被改变。 1234567#!/bin/bashmyUrl="http://www.w3cschool.cc"readonly myUrlmyUrl="http://www.runoob.com"---# 会报错zsh: read-only variable: myUrl 删除变量, 只读变量不能删除, 变量被删除后不能再次使用 1unset variable_name 变量类型 1) 局部变量 局部变量在脚本或命令中定义，仅在当前shell实例中有效，其他shell启动的程序不能访问局部变量。 2) 环境变量 所有的程序，包括shell启动的程序，都能访问环境变量，有些程序需要环境变量来保证其正常运行。必要的时候shell脚本也可以定义环境变量。 3) shell变量 shell变量是由shell程序设置的特殊变量。shell变量中有一部分是环境变量，有一部分是局部变量，这些变量保证了shell的正常运行 2.18 字符串获取字符串长度12string="abcd"echo $&#123;#string&#125; #输出 4 提取子字符串以下实例从字符串第 2 个字符开始截取 4 个字符： 12string="runoob is a great site"echo $&#123;string:1:4&#125; # 输出 unoo 查找子字符串查找字符 “i 或 s“ 的位置： 12string="runoob is a great company"echo `expr index "$string" is` # 输出 8 2.19 Shell 数组 bash支持一维数组（不支持多维数组），并且没有限定数组的大小。 类似与C语言，数组元素的下标由0开始编号。获取数组中的元素要利用下标，下标可以是整数或算术表达式，其值应大于或等于0。 定义数组在Shell中，用括号来表示数组，数组元素用”空格”符号分割开。定义数组的一般形式为： 1数组名=(值1 值2 ... 值n) 还可以单独定义数组的各个分量： 123array_name[0]=value0array_name[1]=value1array_name[n]=valuen 可以不使用连续的下标，而且下标的范围没有限制。 获取数组的长度 获取数组长度的方法与获取字符串长度的方法相同，例如： 123456# 取得数组元素的个数length=$&#123;#array_name[@]&#125;# 或者length=$&#123;#array_name[*]&#125;# 取得数组单个元素的长度lengthn=$&#123;#array_name[n]&#125; 2.2、运算符2.2.1、算数运算符1. 用expr12345678格式 expr m + n 注意expr运算符间要有空格例如计算（2＋3）×4 的值1、分步计算 S=`expr 2 + 3` expr $S \* 4 ## *号需要转义2、一步完成计算 expr `expr 2 + 3 ` \* 4 echo `expr \`expr 2 + 3\` \* 4` 用expr还可以计算字符串的长度，子字符串出现的位置，截取字符串等等 123456789cts1 ~ # name='woshishuaige'cts1 ~ # expr length name4cts1 ~ # expr length $name12cts1 ~ # expr index $name shuai3cts1 ~ # expr substr $name 6 2sh 详情请翻阅: expr –help 2. 用(())12345678((1+2))(((2+3)*4))count=1((count++))((++count))echo $count# 但是要想取到运算结果，需要用$引用a=$((1+2)) 3. 用\$[]12345SS=$[2+3]echo $SSSS=$[2*3]echo $SSecho $[(2 + 3)*3] 4. 用let1234first=1second=2let third=first+secondecho $&#123;third&#125; 5. 用 | bc以上命令都只对整形数值有效，不适用于浮点数 如果有浮点数参与运算，可以将echo与bc命令结合起来使用，代码如下 123456echo "1.212*3" | bc ## 简单浮点运算echo "scale=2;3/8" | bc ##将输出结果设置为2位echo "obase=2;127" | bc ##输出运算结果的二进制echo "obase=10;ibase=2;101111111" | bc ##将二进制转换成十进制echo "10^10" | bc ##求幂指数echo "sqrt(100)" | bc ##开平方 除了用bc做尽职转换以外，还可以这样做： 12345echo $((base#number)) 表示把任意base进制的数number转换成十进制例子：echo $((8#377)) 返回255echo $((025)) 返回21 ， 八进制echo $((0xA4)) 返回164 ， 十六进制 使用bc还可以用来比较浮点数的大小： 12345678910111213[root@hadoop02 bin]# echo "1.2 &lt; 2" |bc1[root@hadoop02 bin]# echo "1.2 &gt; 2" |bc0[root@hadoop02 bin]# echo "1.2 == 2.2" |bc0[root@hadoop02 bin]# echo "1.2 != 2.2" |bc1看出规律了嘛？运算如果为真返回 1，否则返回 0，写一个例子：[root@hadoop02 bin]# [ $(echo "2.2 &gt; 2" |bc) -eq 1 ] &amp;&amp; echo yes || echo noyes[root@hadoop02 bin]# [ $(echo "2.2 &lt; 2" |bc) -eq 1 ] &amp;&amp; echo yes || echo nono 2.2.2、关系运算符下面给出一张关系运算符的列表： 运算符 等同运算符 说明 -eq = 检测两个数是否相等，相等返回true -ne != 检测两个数是否相等，不相等返回true -ge &gt;= 检测左边的数是否大等于右边的，如果是，则返回true -gt &gt; 检测左边的数是否大于右边的，如果是，则返回true -le &lt;= 检测左边的数是否小于等于右边的，如果是，则返回true -lt &lt; 检测左边的数是否小于右边的，如果是，则返回true 2.2.3、布尔运算符 运算符 等同运算符 说明 ! ! 非运算，表达式为 true 则返回false，否则返回true -a &amp;&amp; 与运算，两个表达式都为true 才返回true -o \ \ 或运算，有一个表达式为true 则返回true 2.2.4、字符串运算符 运算符 说明 = 检测两个字符串是否相等，相等返回true != 检测两个字符串是否相等，不相等返回true -z 检测字符串长度是否为0，为0返回true -n 检测字符串长度是否为0，不为0返回true str 检测字符串是否为空，不为空返回true 2.2.5、文件运算符 运算符 说明 -d 检测文件是否是目录，如果是，则返回true -f 检测文件是否是普通文件（既不是目录，也不是设备文件），如果是，则返回true -e 检测文件（包括目录）是否存在，如果是，则返回true -s 检测文件是否为空（文件大小是否大于0），不为空返回true -r 检测文件是否可读，如果是，则返回true -w 检测文件是否可写，如果是，则返回true -x 检测文件是否可执行，如果是，则返回true -b 检测文件是否是块设备文件，如果是，则返回true -c 检测文件是否是字符设备文件，如果是，则返回true 2.3、流程控制2.3.1、ifif.. then ..elif.. then.. else..fi123456789101112131415161718192021222324252627282930313233343536# if语法格式：if conditionthen statements [elif condition then statements. ..] [else statements ] fi# 示例程序：[root@hadoop02 bin]# vi g.sh#!/bin/bash## read a value for NAME from stdinread -p "please input your name:" NAME## printf '%s\n' $NAMEif [ $NAME = root ]thenecho "hello $&#123;NAME&#125;, welcome !"elif [ $NAME = hadoop ]then echo "hello $&#123;NAME&#125;, welcome !"else echo "I don’t know you !"fi## 规则解释[ condition ] (注：condition前后要有空格)#非空返回true，可使用$?验证（0为true，&gt;1为false）[ hadoop ]#空返回false[ ]# 注意[ ]内部的=周边的空格 三元运算符12[ condition ] &amp;&amp; echo OK || echo notok# 条件满足，执行&amp;&amp;后面的语句；条件不满足，执行||后面的语句 条件判断组合 条件判断组合有两种使用方式： [] 和 [[]] 注意它们的区别： [[ ]] 中逻辑组合可以使用 &amp;&amp; || 符号 [] 里面逻辑组合可以用 -a -o 常用判断运算符1. 字符串比较 = 判断相等!= 判断不相等-z 字符串长度是为0返回true-n 字符串长度是不为0返回true 2. 整数比较 -lt 小于 less than -le 小于等于 -eq 等于 -gt 大于 great than -ge 大于等于 -ne 不等于 3. 文件判断 -d 是否为目录 if [ -d /bin ]; then echo ok; else echo notok;fi -f 是否为文件 if [ -f /bin/ls ]; then echo ok; else echo notok;fi -e 是否存在 if [ -e /bin/ls ]; then echo ok; else echo notok;fi 2.3.2、while1234567891011121314151617181920212223242526while expressiondo command ……done----------------------i=1while ((i&lt;=3))do echo $i let i++done----------------------#!/bin/bashi=1while [ $i -le 3 ]do echo $i let i++done# 命令执行完毕，控制返回循环顶部，从头开始直至测试条件为假# 换种方式：循环体会一直执行，直到条件表达式expression为false# 注意：上述let i++ 可以写成 i=$(($i+1))或者i=$((i+1)) 2.3.3、caseCase语法（通过下面这个例子展示）： 12345678910case $1 instart) echo "starting" ;;stop) echo "stoping" ;;*) echo "Usage: &#123;start|stop&#125;"esac 2.3.4、for语法格式： 1234567for 变量 in 列表docommand……done# 列表是一组值（数字、字符串等）组成的序列，每个值通过空格分隔。每循环一次，就将列表中的下一个值赋给变量 三种方式 12345678# 方式1for N in 1 2 3; do echo $N; done# 方式2for N in &#123;1..3&#125;; do echo $N; done# 方式3for ((i=0; i&lt;=2; i++)); do echo "welcome $i times"; done 2.3.5、util语法结构： 123456789101112131415161718192021222324until expressiondo command ……done# expression一般为条件表达式，如果返回值为 false，则继续执行循环体内的语句，否则跳出循环。# 换种方式说：循环体会一直执行，直到条件表达式expression为true## 示例#!/bin/bash## vi util.sha=0until [ ! $a -lt 3 ]do echo $a a=`expr $a + 1`done----输出: 012 2.4、数组在Shell中，用括号来表示数组，数组元素用“空格”符号分割开。定义数组的一般形式为 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061array_name=(value1 ... valuen)例子：mingxing=(huangbo xuzheng wangbaoqiang)也可以单独定义：mingxing[3]=liujialing读取数组元素的格式是：$&#123;array_name[index]&#125;============================================# 获取数组下标： 前面加 `!`[linux@linux ~]$ echo $&#123;!mingxing[@]&#125; 或者[linux@linux ~]$ echo $&#123;!mingxing[*]&#125;---输出: 0 1 2 3============================================# 输出数组的所有元素：直接取值[linux@linux ~]$ echo $&#123;mingxing[*]&#125;[linux@linux ~]$ echo $&#123;mingxing[@]&#125;============================================# 获取数组的长度：`#` [linux@linux ~]$ echo $&#123;#mingxing[*]&#125;[linux@linux ~]$ echo $&#123;#mingxing[@]&#125;============================================# 数组对接: 添加新的值[linux@linux ~]$ mingxing+=(liuyifei liuyufeng)============================================# 删除数组元素，但是会保留数组对应位置，就是该值的下标依然会保留，会空着，之后，还可以填充其他的值进来。# 删除第一个元素：之后 $&#123;mingxing[0]&#125; 就是空值了[linux@linux ~]$ unset mingxing[0]============================================# 遍历数组：#!/bin/bashIP=(192.168.1.1 192.168.1.2 192.168.1.3)# 第一种方式for ((i=0;i&lt;$&#123;#IP[*]&#125;;i++))doecho $&#123;IP[$i]&#125;done#第二种方式for ip in $&#123;IP[*]&#125;do echo $ipdone============================================# 数组的分片：$&#123;arr[@]:number1:number2&#125;这里number1从下标number1开始取值，number2往后取几个元素，即取到的新的数组的长度---cts1 ~ # arr=(1 2 3 4 5 6 7)cts1 ~ # echo "&#123;arr[@]:0:3&#125; --- $&#123;arr[@]:0:3&#125; "&#123;arr[@]:0:3&#125; --- 1 2 3cts1 ~ # echo "&#123;arr[@]:3:3&#125; --- $&#123;arr[@]:3:3&#125; "&#123;arr[@]:3:3&#125; --- 4 5 6cts1 ~ # echo "&#123;arr[@]:4:3&#125; --- $&#123;arr[@]:4:3&#125; "&#123;arr[@]:4:3&#125; --- 5 6 7 2.5、函数使用函数的语法使用示例 : 1234567891011121314[root@hadoop02 bin]# vi i.sh #!/bin/shhello()&#123; echo "`date +%Y-%m-%d`" # return 2&#125;helloecho “huangbo” # echo $?A="mazhonghua"echo $A---执行结果----“huangbo”mazhonghua 函数的调用方式就是直接写函数名就OK了 123注意：1、必须在调用函数地方之前，先声明函数，shell脚本是逐行运行。不会像其它语言一样先预编译2、函数返回值，只能通过$? 系统变量获得，可以显示加：return 返回，如果不加，将以最后一条命令运行结果，作为返回值。 return后跟数值n(0-255) 脚本调试： 使用-x选项跟踪脚本调试shell脚本，能打印出所执行的每一行命令以及当前状态 sh -x i.sh 或者在代码中加入：set -x 12345678910111213cts1 Desktop # sh -x i.sh+ hello++ date +%Y-%m-%d+ echo 2018-05-292018-05-29+ return 2+ echo $'\342\200\234huangbo\342\200\235'“huangbo”+ echo 00+ A=mazhonghua+ echo mazhonghuamazhonghua 2.6、函数参数直接上代码 123456789101112131415161718192021222324vim funcWithParam.sh--------sh文件--------#!/bin/bash# filename=funcWithParamfuncWithParam()&#123; echo "第一个参数为 $1 !" echo "第二个参数为 $2 !" echo "第十个参数为 $10 !" echo "第十个参数为 $&#123;10&#125; !" echo "第十一个参数为 $&#123;11&#125; !" echo "参数总数有 $# 个!" echo "作为一个字符串输出所有参数 $* !"&#125;funcWithParam 1 2 3 4 5 6 7 8 9 34 73--------------------# 调用cts1 Desktop # sh funcWithParam.sh第一个参数为 1 !第二个参数为 2 !第十个参数为 10 !第十个参数为 34 !第十一个参数为 73 !参数总数有 11 个!作为一个字符串输出所有参数 1 2 3 4 5 6 7 8 9 34 73 ! 2.7、跨脚本调用函数编写一个base.sh脚本，里面放放一个test函数 123456vim base.sh------#!/bin/bashtest()&#123; echo "hello"&#125; 再编写一个other.sh脚本，里面引入base.sh脚本，并且调用test函数： 12345678910111213vim other.sh--------#!/bin/bashsource base.sh ## 引入脚本test ##调用引入脚本当中的test函数============================================# 执行cts1 Desktop # sh other.shother.sh: line 2: ./base.sh: 权限不够cts1 Desktop # chmod 755 other.sh base.shcts1 Desktop # ./other.shhello 3、Shell综合案例3.1、打印9*9乘法表示例代码： 12345678910111213141516171819202122232425#!/bin/bashfor((i=1;i&lt;=9;++i))do for((j=1;j&lt;=i;j++)) do echo -ne "$i*$j=$((i*j))\t" done echodone# 解释-n 不加换行符-e 解释转义符echo 换行效果: 1*1=12*1=2 2*2=43*1=3 3*2=6 3*3=94*1=4 4*2=8 4*3=12 4*4=165*1=5 5*2=10 5*3=15 5*4=20 5*5=256*1=6 6*2=12 6*3=18 6*4=24 6*5=30 6*6=367*1=7 7*2=14 7*3=21 7*4=28 7*5=35 7*6=42 7*7=498*1=8 8*2=16 8*3=24 8*4=32 8*5=40 8*6=48 8*7=56 8*8=649*1=9 9*2=18 9*3=27 9*4=36 9*5=45 9*6=54 9*7=63 9*8=72 9*9=81 3.2、自动部署集群的JDK1、 需求描述 12公司内有一个N个节点的集群，需要统一安装一些软件（jdk）需要开发一个脚本，实现对集群中的N台节点批量自动下载、安装jdk 2、 思路 思考一下：我们现在有一个JDK安装包在一台服务器上。那我们要实现这个目标： 1231、 把包传到每台服务器，或者通过本地yum源的方式去服务器取2、 给每台一台机器发送一个安装脚本，并且让脚本自己执行3. 要写一个启动脚本，用来执行以上两部操作 3、 Expect**的使用** 蛋疼点：假如在没有配置SSH免密登录的前提下，我们要要是scp命令从一台机器拷贝文件夹到另外的机器，会有人机交互过程，那我们怎么让机器自己实现人机交互？ 灵丹妙药：expect 命令 描述 set 可以设置超时，也可以设置变量 timeout 超时等待时间，默认 10s spawn 执行一个命令 send 执行交互，相当于手动输入 expect “”（expect内部命令） 匹配输出的内容 exp_continue 继续执行下面匹配 思路：模拟该人机交互过程，在需要交互的情况下，通过我们的检测给输入提前准备好的值即可 示例：观看配置SSH免密登录的过程 实现脚本： 123456789101112131415161718vi testExpect.sh## 定义一个函数sshcopyid()&#123; expect -c " spawn ssh-copy-id $1 expect &#123; \"(yes/no)?\" &#123;send \"yes\r\";exp_continue&#125; \"password:\" &#123;send \"$2\r\";exp_continue&#125; &#125; "&#125;## 调用函数执行sshcopyid $1 $2# 注意：如果机器没有expect，则请先安装expectyum install -y expect 4、 脚本实现 启动脚本initInstallJDK.sh 12345678910111213141516171819202122232425262728293031#!/bin/bashSERVERS="192.168.123.201"PASSWORD=hadoopBASE_SERVER=192.168.123.202auto_ssh_copy_id() &#123; expect -c "set timeout -1; spawn ssh-copy-id $1; expect &#123; *(yes/no)* &#123;send -- yes\r;exp_continue;&#125; *password:* &#123;send -- $2\r;exp_continue;&#125; eof &#123;exit 0;&#125; &#125;";&#125;ssh_copy_id_to_all() &#123; for SERVER in $SERVERS do auto_ssh_copy_id $SERVER $PASSWORD done&#125;ssh_copy_id_to_allfor SERVER in $SERVERSdo scp installJDK.sh root@$SERVER:/root ssh root@$SERVER chmod 755 installJDK.sh ssh root@$SERVER /root/installJDK.shdone 安装脚本installJDK.sh 1234567891011#!/bin/bashBASE_SERVER=192.168.123.202yum install -y wgetwget $BASE_SERVER/soft/jdk-8u73-linux-x64.tar.gztar -zxvf jdk-8u73-linux-x64.tar.gz -C /usr/localcat &gt;&gt; /etc/profile &lt;&lt;EOFexport JAVA_HOME=/usr/local/jdk1.8.0_73export PATH=\$PATH:\$JAVA_HOME/binEOF 4、总结写脚本注意事项： 1、开头加解释器： #!/bin/bash，和注释说明。 2、命名建议规则：变量名大写、局部变量小写，函数名小写，名字体现出实际作用。 3、默认变量是全局的，在函数中变量 local 指定为局部变量，避免污染其他作用域。 4、set -e 遇到执行非 0 时退出脚本， set -x 打印执行过程。 5、写脚本一定先测试再到生产上。]]></content>
      <categories>
        <category>Linux</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习笔记-2]]></title>
    <url>%2F2018%2F05%2F27%2FLinux%2FLinux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2%2F</url>
    <content type="text"><![CDATA[1. VI文本编辑器学会使用vi编辑器是学习Linux系统的必备技术之一，因为一般的Linux服务器是没有GUI界面的，Linux运维及开发人员基本上都是通过命令行的方式进行文本编辑或程序编写的。vi编辑器是Linux内置的文本编辑器，几乎所有的类unix系统中都内置了vi编辑器，而其它编辑器则不一定，另外很多软件会调用vi编辑进行内容编写，例如crontab定时任务。较之于其它编辑器或GUI编辑器，vi编辑速度是最快的。VIM是它的增强版本，VI有三种基本工作模式，分别是： 命令模式（command mode）、或者叫一般模式 插入模式（insert mode）、或者叫编辑模式 底行模式（last line mode）、或者叫命令行模式 1. 最基本用法 1、首先会进入“一般模式”，此模式只接受各种命令快捷键，不能编辑文件内容 2、按i键，就会从一般模式进入编辑模式，此模式下，敲入的都是文件内容 3、编辑完成之后，按Esc键退出编辑模式，回到一般模式 4、再按：，进入“底行命令模式”，输入wq命令，回车即可保存退出 2. 移动光标 (一般模式) 1、 使用上下左右键可以移动光标 2、 使用h,j,k,l，依次是向左，下，上，右移动 3、 w：将光标移动到下一个单词的首字母处 4、 W：利用空格向后移动光标，就是忽略标点 5、 b：利用word包括标点向前移动光标, 与w相对应 6、 B：利用空格向前移动光标，忽略标点 7、 e：将光标移动到下一个word的尾部，包括符号 8、 E: 将光标移动到下一个空格分隔字的尾部 9、 (：移动到句子开始 10、 )：移动到句子结束 11、 0：移动光标到本句句首 12、 $：移动光标到本行行尾 13、 {：移动到段落开始 14、 }：移动到段落结束 15、 H：屏幕顶端 16、 L：屏幕底端 17、 M：移动到屏幕中央位置 18、 gg：直接跳到文件的首行行首 19、 G：直接跳到文件的末行行首 20、 最强光标移动： % : 匹配括号移动，包括(, {, [.（你需要把光标先移到括号上） *: 匹配光标当前所在的单词，移动光标到下一个匹配单词 #: 匹配光标当前所在的单词，移动光标到上一个匹配单词 重点总结: h l : 左右, j k : 下上. w: 下一个单词 , b: 上一个单词. 0: 本行行首, $: 本行行尾 (: 句子开始, ): 句子结束 {: 段落开始 }: 段落结束 gg: 文件行首 G: 文件行尾 %: 下一个括号 *: 下一个单词 #: 上一个单词 3. 常用操作(一般模式 &gt;&gt; 插入模式) 单位 指令1 指令2 当前位 前一位/后一位 i: 在光标前一位开始插入 a: 光标后一位开始插入 当前行 最前/最后 I: 在该行的最前面插入 A: 在该行的最后插入 当前行 上一行/下一行 o: 小o, 当前行的下一行插入空行 O: 大o, 当前内容下移一行, 当前行插入空行 当前行 删除 dd: 删除当前行 3dd: ndd, 删除从当前行开始的 n行 当前行 复制 yy: 复制光标所在行 3yy: 复制从当前行开始的3行 粘贴 p: 粘贴到光标所在行的下一行 撤销 u: undo 撤销操作, 可一直撤销到最前面 重点总结: i : 当前光标前开始插入; a: 光标后一位开始插入 I: 在该行的最前面插入; A: 在该行的最后插入 o: 小o, 当前行的下一行插入空行 O: 大o, 当前内容下移一行, 当前行插入空行 dd: 删除当前行 3dd: ndd, 删除从当前行开始的 n行 yy: 复制光标所在行 3yy: 复制从当前行开始的3行 p: 粘贴到光标所在行的下一行 u: undo 撤销操作, 可一直撤销到最前面 4. 查找并替换 在底行命令模式中输入 1. 显示行号 :set nu 2. 隐藏行号 :set nonu 3 .查找关键字 :/you ## 效果：查找文件中出现的you，并定位到第一个找到的地方，按 n可以定位到下一个匹配位置（按N定位到上一个） 查询的时候被匹配上的字符串会被高亮，可以在命令模式下使用:noh取消高亮 4. 直接跳转到 3行 : 3 5. 替换操作 :1 s/sad/bbb 将第一行的第一个sad替换为bbb :1,5 s/sad/bbb 将第一行到第五行的第一个sad替换为bbb :1,. s/sad/bbb 将第一行到光标行的第一个sad替换为bbb :.,$ s/sad/bbb 将光标行到缓冲区最后一行的sad替换为bbb :s/sad/bbb 查找光标所在行的第一个sad，替换为bbb :s/sad/bbb/g 查找光标所在行的所有sad，替换为bbb :%s/sad/bbb 查找文件中所有行第一次出现的sad，替换为bbb :%s/sad/bbb/g 查找文件中所有的sad，替换为bbb 6. 屏幕翻滚类命令 Ctrl + u：向文件首翻半屏 Ctrl + d：向文件尾翻半屏 Ctrl + f：向文件尾翻一屏 Ctrl＋b：向文件首翻一屏 nz：将第n行滚至屏幕顶部，不指定n时将当前行滚至屏幕顶部 7. 其它的小技巧 r 替换光标处一个字符 R 进入替换模式，从光标处连续替换 s 删除当前字符，进入插入模式 S 删除当前行，进入插入模式 ≈ dd, dd也会删除当前行, 但是不会进入插入模式 f s 光标行内向后查找第一个出现的字符s (先输f , 再输 s, 向后找 s) F s 光标行内向前查找第一个出现的字符s (先输F , 再输 s, 向前找 s) ~ 大小写转换，只转换光标处字符 8. 速查网址 vim详解 2. 网络管理ifconfig命令ifconfig命令主要用于配置网络接口，如果不加任何参数，则ifconfig命令用于查看当前所有活动网络接口的状态信息，如下图： eth0 表示第一块网卡，其中HWaddr表示网卡的物理地址，可以看到目前这个网卡的物理地址(MAC地址）是00:0C:29:D6:C7:0E。 inet addr 用来表示网卡的IP地址，此网卡的IP地址是192.168.179.6，广播地址192.168.170.255，掩码地址Mask:255.255.255.0。 lo 是表示主机的回坏地址，这个一般是用来测试一个网络程序，但又不想让局域网或外网的用户能够查看，只能在此台主机上运行和查看所用的网络接口。比如把 httpd服务器的指定到回坏地址，在浏览器输入127.0.0.1就能看到你所架WEB网站了。但只是您能看得到，局域网的其它主机或用户无从知道。 ifconfig其他常用使用 -a 显示所有网络接口，包括停用的 -s 短格式显示网络信息，同netstat -i -v 显示详细信息，在网络出错的情况下适用 interface 指定网络接口 up 启用网络接口 down 关闭网络接口 启动关闭指定网卡&amp; 常用操作： 12345ifconfig eth0 up #启动网卡ifconfig eth0 down #关闭网卡 ifconfig #处于激活状态的网络接口ifconfig -a #所有配置的网络接口，不论其是否激活ifconfig eth0 #显示eth0的网卡信息 网络配置1. 三种通信模式Vmware中的虚拟机和宿主机进行通信有三种网络方式 一、Brigde――桥接：默认使用VMnet0 原理: Bridge 桥”就是一个主机，这个机器拥有两块网卡，分别处于两个局域网中，同时在”桥”上，运行着程序，让局域网A中的所有数据包原封不动的流入B，反之亦然。这样，局域网A和B就无缝的在链路层连接起来了，在桥接时，VMWare网卡和物理网卡应该处于同一IP网段 当然要保证两个局域网没有冲突的IP. VMWare 的桥也是同样的道理，只不过，本来作为硬件的一块网卡，现在由VMWare软件虚拟了！当采用桥接时，VMWare会虚拟一块网卡和真正的物理网卡就行桥接，这样，发到物理网卡的所有数据包就到了VMWare虚拟机，而由VMWare发出的数据包也会通过桥从物理网卡的那端发出。 所以，如果物理网卡可以上网，那么桥接的软网卡也没有问题了，这就是桥接上网的原理了。 联网方式： 这一种联网方式最简单，在局域网内，你的主机是怎么联网的，你在虚拟机里就怎么连网。把虚拟机看成局域网内的另一台电脑就行了！ 提示：主机网卡处在一个可以访问Internet的局域网中，虚拟机才能通过Bridge访问Internet。 二、NAT――网络地址转换 ：默认使用VMnet8 原理： NAT 是 Network address translate的简称。NAT技术应用在internet网关和路由器上，比如192.168.0.123这个地址要访问internet，它的数据包就要通过一个网关或者路由器，而网关或者路由器拥有一个能访问internet的ip地址，这样的网关和路由器就要在收发数据包时，对数据包的IP协议层数据进行更改（即 NAT），以使私有网段的主机能够顺利访问internet。此技术解决了IP地址稀缺的问题。同样的私有IP可以网关NAT 上网。 VMWare的NAT上网也是同样的道理，它在主机和虚拟机之间用软件伪造出一块网卡，这块网卡和虚拟机的ip处于一个地址段。同时，在这块网卡和主机的网络接口之间进行NAT。虚拟机发出的每一块数据包都会经过虚拟网卡，然后NAT，然后由主机的接口发出。 虚拟网卡和虚拟机处于一个地址段，虚拟机和主机不同一个地址段，主机相当于虚拟机的网关，所以虚拟机能ping到主机的IP，但是主机ping不到虚拟机的IP。 联网方式： 方法1、动态IP地址。 主机是静态IP或动态IP，都无所谓，将虚拟机设置成使用DHCP方式上网,Windows下选择“自动获取IP“，linux下开启DHCP服务即可。（这种方法最简单，不用过多的设置，但要在VMware中进行“编辑→虚拟网络设置”，将NAT和DHCP都开启了。一般NAT默认开启，DHCP默认关闭） 方法2、静态IP地址。 如果不想使用DHCP，也可以手动设置： IP设置与vmnet1同网段,网关设置成vmnet8的网关（在“虚拟网络设置”里的Net选项卡里能找到Gateway）通常是xxx.xxx.xxx.2。子网掩码设置与VMnet8相同（设置好IP地址后，子网掩码自动生成）DNS设置与主机相同。 例如：主机IP是10.70.54.31,设置虚拟机IP为10.70.54.22。Netmask,Gateway,DNS都与主机相同即可实现 虚拟机 —主机 虚拟机互联网 通信。 提示：使用NAT技术，主机能上网，虚拟机就可以访问Internet，但是主机不能访问虚拟机。 三、Host-Only――私有网络共享主机：默认使用VMnet1 联网方法： 方法1、动态IP地址。像上面那样开启DHCP后，虚拟机直接自动获取IP地址和DNS。就可以和主机相连了。当然，还要进行一些局域网共享的操作，这里不再赘述。 方法2、静态IP地址。 也可以手动设置，将虚拟机IP设置与VMnet1同网段,网关设置成VMnet1的网关相同,其余设置与VMnet1相同,DNS设置与主机相同。 例如：VMnet1 IP:172.16.249.1 Gateway :172.16.249.2 那么虚拟机 IP:172.16.249.100 Gateway: 172.16.249.2 这样、 虚拟机主机 可以通信但是、 虚拟机互联网 无法通信 提示：Host-only技术只用于主机和虚拟机互访，于访问internet无关。 2. NAT网络模式的配置见Linux学习笔记3, 第8条: Linux虚拟主机集群测试环境基本搭建 3. 其它的常用网络管理命令 ping命令: 常用来测试网络连接是否正常 先确定能ping通 ping www.baidu.com host命令: host命令用来进行DNS查询 然后用host命令可以查看到 host www.baidu.com 然后通过浏览器访问该地址：119.75.213.61 netstat命令: netstat命令可以显示网络接口的很多统计信息，包括打开的socket和路由表 以下是常用命令选项 123456789101112-a (all)显示所有选项，默认不显示LISTEN相关-t (tcp)仅显示tcp相关选项-u (udp)仅显示udp相关选项-n 拒绝显示别名，能显示数字的全部转化成数字-l 仅列出有在 Listen (监听) 的服務状态-p 显示建立相关链接的程序名-r 显示路由信息，路由表-e 显示扩展信息，例如uid等-s 按各个协议进行统计-c 每隔一个固定时间，执行该netstat命令 例子 1、列出所有端口，包括监听和未监听的：netstat -a 2、列出所有TCP端口：netstat -at 3、列出所有UDP端口：netstat -au 4、列出所有监听状态的TCP端口：该命令最重要用来查看哪个程序占用了哪个网络端口号 123# 比如查看谁占用了 tcp 的80 端口netstat -nltp | grep 80 [23:04:54]tcp 0 0 :::80 :::* LISTEN 3890/httpd 4. 防火墙防火墙根据配置文件/etc/sysconfig/iptables来控制本机的“出、入”网络访问行为 其对行为的配置策略有四个策略表 基础必备技能 | 查看防火墙状态 | service iptables status || ———————- | —————————– || 开启防火墙 | service iptables start || 关闭防火墙 | service iptables stop || 关闭防火墙开机自启 | chkconfig iptables off || 设置防火墙开机自启 | chkconfig iptables on || 查看防火墙开机启动状态 | chkconfig iptables –list | 扩展知识 1234567891011121314151617181920212223242526# 1、列出iptables规则iptables -L -n# 列出iptables规则并显示规则编号iptables -L -n --line-numbers# 2、列出iptables nat表规则（默认是filter表）iptables -L -n -t nat# 3、清除默认规则（注意默认是filter表，如果对nat表操作要加-t nat）#清除所有规则iptables -F#重启iptables发现规则依然存在，因为没有保存service iptables restart#保存配置service iptables save# 4、禁止SSH登陆（如果服务器在机房，一定要小心）iptables -A INPUT -p tcp --dport 22 -j DROP# 5、删除规则iptables -D INPUT -p tcp --dport 22 -j DROP # 6、加入一条INPUT规则开放80端口 iptables -I INPUT -p tcp --dport 80 -j ACCEPT 3. Linux系统启动级别管理使用runlevel命令可以查看系统运行的级别 12runlevel [23:12:37]N 3 修改系统默认启动级别 1234567891011121314# 启动级别在这个路径查看vi /etc/inittab-------------------# Default runlevel. The runlevels used are:# 0 - halt (Do NOT set initdefault to this)# 1 - Single user mode# 2 - Multiuser, without NFS (The same as 3, if you do not have networking)# 3 - Full multiuser mode# 4 - unused# 5 - X11# 6 - reboot (Do NOT set initdefault to this)#id:3:initdefault: id:3:initdefault: ## 配置默认启动级别 ## 通常将默认启动级别设置为：3 4. 用户和组1. 用户和组的概念Linux是一个多任务多用户的操作系统，当我们在使用ls -l命令的时候我们看到如下信息： 123drwxrwxr-x. 2 root root 4096 5月 24 00:15 test-rw-r--r--. 1 root root 58 5月 25 11:00 test.sh-rwxr-xr-x. 1 root root 35 5月 23 23:55 test.txt test：表示文件或者目录，具体的文件类型是由该行最前面的那个符号表示 12345- 当为[ *d* ]则是目录- 当为[ *-* ]则是文件；- 若是[ *l* ]则表示为链接文档(link file)；- 若是[ *b* ]则表示为装置文件里面的可供储存的接口设备(可随机存取装置)；- 若是[ *c* ]则表示为装置文件里面的串行端口设备，例如键盘、鼠标(一次性读取装置); drwxrwxr-x ：该文件的类型和权限信息 2 ：链接数，如果是文件则是1 ， 如果是文件夹则表示该文件夹下的子文件夹个数 第一个root ：文件或者目录的所属者** 第二个root ：所属用户组** 4096 ：文件或者目录的大小，是目录的话一般都是4096 5月 24 00:15：文件的最后编辑时间 通过以上信息得知，每个文件都设计到用户和组的权限问题 在Linux中，用户是能够获取系统资源的权限的集合，组是权限的容器 Linux 用户类型 用户类型 描述 管理员root 具有使用系统所有权限的用户,其UID 为0 系统用户 保障系统运行的用户,一般不提供密码登录系统,其UID为1-499之间 普通用户 即一般用户,其使用系统的权限受限,其UID为500-60000之间. 与Linux用户信息相关的文件有两个：分别是/etc/passwd和 /etc/shadow 123456789101112# 查看文件/etc/passwd文件的内容，选取第一行：root:x:0:0:root:/root:/bin/bashroot:用户名x:密码占位符，密码保存在shadow文件内0:用户id，UID0:组id，GIDroot:注释信息/root:用户家目录/bin/bash:用户默认使用shell登录名:加密口令:最后一次修改时间:最小时间间隔:最大时间间隔:警告时间:不活动时间:失效时间:标志 Linux用户组类型 用户组类型 描述 系统组 一般加入一些系统用户 普通用户组 可以加入多个用户 私有组/基本组 当创建用户时,如果没有为其指明所属组，则就为其定义一个私有的用户组，起名称与用户名同名，当把其他用户加入到该组中，则其就变成了普通组 与Linux用户组信息相关的文件有两个：分别是/etc/group和 /etc/gshadow 12345# 查看文件/etc/group文件内容，选取一个普通组行：hadoop:x:500:hadoop:组名x:组密码占位符500:组id 2. 用户操作Linux中的用户管理主要涉及到用户账号的添加、删除和修改。所有操作都影响/etc/passwd中的文件内容 选项12345678910111213-c&lt;备注&gt;：修改用户帐号的备注文字；-d&lt;登入目录&gt;：修改用户登入时的目录；-e&lt;有效期限&gt;：修改帐号的有效期限；-f&lt;缓冲天数&gt;：修改在密码过期后多少天即关闭该帐号；-g&lt;群组&gt;：修改用户所属的群组；-G&lt;群组&gt;；修改用户所属的附加群组；-l&lt;帐号名称&gt;：修改用户帐号名称；-L：锁定用户密码，使密码无效；-U:解除密码锁定。-s&lt;shell&gt;：修改用户登入后所使用的shell；-u&lt;uid&gt;：修改用户ID； 参数 登录名：指定要修改信息的用户登录名。 添加用户 useradd spark usermod -G bigdata spark ## 设置组, spark用户添加到bigdata组中 usermod -c “mylove spark” spark ## -c: 添加备注信息 一步完成：useradd -G bigdata -c “mylove” spark # spark用户添加到bigdata组中, 并设置备注为 mylover useradd -u 508 -g 514 -G 1001 user3 添加用户user3时, 指定用户uid 和 主组-g, 附属组-G, 组必须要先存在 useradd -u(添加的时候修改用户id) 508 -g 514(-g是添加组, 此组号码必须存在) 指定user4 的家目录 useradd -u 520 -g 1000 -d /home/user44 user4 1234567891011121314151617&gt; &gt; 不创建user4的家目录, user5刚创建的时候, 家就被毁了&gt; &gt; `useradd -u 508 -g 1000 -M user6&gt; &gt; &gt; &gt; 当user6没有家的时候, 这样显示&gt; &gt; `bash-4.1$ ....&gt; &gt; 此时应该这样操作&gt; &gt; cp -v /etc/skel/.b* /home/user6&gt; &gt; "/etc/skel/.bash_logout" -&gt; "/home/user6/.bash_logout"&gt; &gt; "/etc/skel/.bash_profile" -&gt; "/home/user6/.bash_profile"&gt; &gt; "/etc/skel/.bashrc" -&gt; "/home/user6/.bashrc"&gt; &gt; su user6&gt; &gt; [user6@cts1 shixuanji]$ &gt; &gt; &gt; &gt; --------&gt; &gt; 查看进程 ps -au | grep xxx&gt; &gt; 杀死进程 kill -9 xxx&gt; &gt; 123456&gt; &gt; # 不允许用户登录&gt; &gt; useradd -u 523 -g 1000 -s /sbin/nologin user7&gt; &gt; &gt; &gt; # 修改此用户登录权限为 /bin/bash&gt; &gt; usermod -s /bin/bash user7&gt; &gt; 设置密码 passwd spark 根据提示设置密码即可 创建密码工具 mkpasswd 下载： yum -y install expect mkpasswd -l 14 创建一个14位数的密码 mkpasswd -l 15 -s 0: -s是设置特殊符号的长度, 这里是不要特殊符号 123456789101112131415161718192021222324&gt; ### 修改密码&gt; # 此时只需要输入一遍密码&gt; passwd --stdin user4 &gt; &gt; # 通过管道可以直接更改&gt; echo "123" | passwd --stdin user4&gt; &gt; # 通过手动输入回车符 `\n` 来实现2遍确认 -e：激活转义字符。&gt; echo -e "123\n123\n" | passwd user4&gt; &gt; =======#额外知识点&gt; # 使用echo -e选项时，若字符串中出现以下字符，则特别加以处理，而不会将它当成一般文字输出：&gt; \a 发出警告声；&gt; \b 删除前一个字符；&gt; \c 最后不加上换行符号；&gt; \f 换行但光标仍旧停留在原来的位置；&gt; \n 换行且光标移至行首；&gt; \r 光标移至行首，但不换行；&gt; \t 插入tab；&gt; \v 与\f相同；&gt; \ 插入\字符；&gt; \nnn 插入nnn（八进制）所代表的ASCII字符；&gt; &gt; 修改用户 修改spark登录名为storm：usermod -l spark storm 将spark添加到bigdata和root组：usermod -G root,bigdata spark 查看spark的组信息：groups spark 1234567891011121314&gt; # usermod : usermodify&gt; # 使用的参数跟useradd 几乎一样&gt; &gt; chfn user4 : 修改用户的finger, 电话, 办公室等...&gt; # 修改后, 其实这里就是修改的备注选项&gt; $ ☞ grep shixuanji /etc/passwd&gt; shixuanji:x:500:500:airpoet,sh,110,110:/home/shixuanji:/bin/bash&gt; &gt; # 修改备注, 这里又把之前改chfn又改了&gt; $ ☞ usermod -c "哎呀我去" shixuanji&gt; $ ☞ grep shixuanji /etc/passwd &gt; shixuanji:x:500:500:哎呀我去:/home/shixuanji:/bin/bash&gt; &gt; 删除用户 userdel -r spark 加一个-r就表示把用户及用户的家目录, 信箱地址等都删除 查看用户的信息 id 用户名 root用户以其他用户的权限执行某条命令 123[~] su -c "touch /tmp/shixuanji.txt" - shixuanji 22:32:02[~] ls -l /tmp/shixuanji.txt 22:32:25-rw-rw-r--. 1 shixuanji shixuanji 0 5月 29 22:32 /tmp/shixuanji.txt 3. 组操作前面我们知道，组是权限的集合。在linux系统中，每个用户都有一个用户组，没有指定时都默认为私有组，私有组名同用户名一致，建立用户组的好处是系统能对一个用户组中的所有用户的操作权限进行集中管理。组管理涉及组的添加、删除和修改。组的增加、删除和修改实际上就对/etc/group文件的更新 添加一个叫bigdata的组 groupadd bigdata 查看系统当前有那些组 cat /etc/group 将spark用户添加到bigdata组中 usermod -G bigdata spark 或者 gpasswd -a spark bigdata 这两个命令的区分记忆技巧： 命令是什么，就证明对什么做操作，所以最后的参数就是命令的操作对象，中间的可选项表示要干嘛 将spark用户从bigdata组删除 gpasswd -d spark bigdata 将bigdata组名修改为bigspark groupmod -n bigspark bigdata 删除组 groupdel bigdata 4. 为用户配置sudoer权限普通情况下，使用普通用户进行一些简单的操作就OK，但是普通用户和root用户的区别就在于root用户能对系统做任何事，但是普通用户就不行。处处受限。那么假如在某些情况下，普通用户想拥有更大的权限做更多的事情，虽然有权限限制，但也不是不可以。部分操作还是可以赋予更高的权限让普通用户做一次。这就需要给普通用户配置root权限了。意思就是让普通用户使用root权限去做一些操作，这当然是需要配置的。 用root编辑 vi /etc/sudoers 直接用命令 visudo也可以直接编辑此文件 在文件的如下位置，为hadoop添加一行即可 root ALL=(ALL) ALL hadoop ALL=(ALL) ALL spark ALL=(ALL) ALL 然后，hadoop用户和spark用户就可以用sudo来执行系统级别的指令 [hadoop\@hadoop01 ~]\$ sudo useradd huangxiaoming 123456789&gt; ~ ➤ su shixuanji # 修改了配置文件之后, 用sudo可以访问root&gt; [shixuanji@cts1 root]$ sudo /bin/ls /root&gt; [sudo] password for shixuanji:&gt; anaconda-ks.cfg date.txt Documents git-2018-05-22 install.log.syslog Music Public Videos&gt; autojump Desktop Downloads install.log k-vim Pictures Templates wget-log&gt; [shixuanji@cts1 root]$ ls /root&gt; ls: 无法打开目录/root: 权限不够&gt; [shixuanji@cts1 root]$&gt; 让一个普通用户可以无密码登录到root用户, 还是修改配置文件 /etc/sudoers` 禁止远程登录 修改 vi /etc/ssh/sshd_config 此文件中的 #PermitRootLogin yes 为 no 这样就不能远程 ssh 登录了 查看系统日志 查看登录错误之类的日志 tail /var log/secure 有时候, 登录时, 要过几秒才让输入登录密码 修改 /etc/ssh/sshd_config中的UseDNS=no, 就不会去找DNS了 普通用户免密使用sudo &amp; 免密登录为root12345678vi /ect/sudoers# 设置不需要使用密码使用sudoap ALL=(ALL) NOPASSWD: ALL # 使用 sudo su - 免密登录到root账户User_Alias USER_SU=apCmnd_Alias SU=/bin/suUSER_SU ALL=(ALL) NOPASSWD: SU 5. 切换用户在linux的系统使用过程当中，免不了会有多个用户来回切换使用。 所以在此提供切换用户的使用操作：切换用户使用的命令是 su（switch user） 从普通用户切换到root用户 , 需要输密码 su root 或 su 从root用户切换到普通用户, 不需要输密码 su xxx 5. 文件权限1、linux文件权限的描述格式解读Linux用 户分为：拥有者、组群(Group)、其他（other），Linux系统中，预设的情況下，系统中所有的帐号与一般身份使用者，以及root的相关信 息， 都是记录在/etc/passwd文件中。每个人的密码则是记录在/etc/shadow文件下。 此外，所有的组群名称记录在/etc/group內 linux文件的用户权限的分析图 123456 -rw-r--r-- 1 user staff 651 Oct 12 12:53 .gitmodules# ↑╰┬╯╰┬╯╰┬╯# ┆ ┆ ┆ ╰┈ 0 其他人# ┆ ┆ ╰┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈ g 属组# ┆ ╰┈┈┈┈ u 属主# ╰┈┈ 第一个字母 `d` 代表目录，`-` 代表普通文件 d：标识节点类型（d：文件夹 -：文件 l：链接） r：可读 w：可写 x：可执行 文件 文件夹 r 可读取内容 可以ls w 可修改文件的内容 可以在其中创建或者删除子节点 x 能否运行这个文件 能否cd**进入这个目录** 2、 修改文件权限chmod命令 用来变更文件或目录的权限。在UNIX系统家族里，文件或目录权限的控制分别以读取、写入、执行3种一般权限来区分，另有3种特殊权限可供运用。用户可以使用chmod指令去变更文件与目录的权限，设置方式采用文字或数字代号皆可。符号连接的权限无法变更，如果用户对符号连接修改权限，其改变会作用在被连接的原始文件。 语法12345chmod(选项)(参数)r=读取属性 //值＝4 w=写入属性 //值＝2 x=执行属性 //值＝1 选项： u User，即文件或目录的拥有者；g Group，即文件或目录的所属群组；o Other，除了文件或目录拥有者或所属群组之外，其他用户皆属于这个范围；a All，即全部的用户，包含拥有者，所属群组以及其他用户；r 读取权限，数字代号为“4”;w 写入权限，数字代号为“2”；x 执行或切换权限，数字代号为“1”；- 不具任何权限，数字代号为“0”；s 特殊功能说明：变更文件或目录的权限。 参数: 权限模式：指定文件的权限模式；文件：要改变权限的文件。 示例变更 文件/目录 的权限 1234567891011# 直接修改chmod g-rw haha.dat ## 表示将haha.dat对所属组的rw权限取消chmod o-rw haha.dat ## 表示将haha.dat对其他人的rw权限取消chmod u+x haha.dat ## 表示将haha.dat对所属用户的权限增加xchmod a-x haha.dat ## 表示将haha.dat对所用户取消x权限# 也可以用数字的方式来修改权限(常用)chmod 664 haha.dat就会修改成 rw-rw-r--如果要将一个文件夹的所有内容权限统一修改，则可以-R参数chmod -R 770 aaa/ 变更 文件/目录 的拥有者 或 所属组 1234# &lt;只有root权限能执行&gt;chown angela aaa ## 改变所属用户chown :angela aaa ## 改变所属组chown angela:angela aaa/ ## 同时修改所属用户和所属组 6. 压缩打包gzipgzip是在Linux系统中经常使用的一个对文件进行压缩和解压缩的命令，既方便又好用。 gzip是个使用广泛的压缩程序，文件经它压缩过后，其名称后面会多处“.gz”扩展名。 语法 gzip (选项) (参数) 选项 123456789101112131415161718-a或——ascii：使用ASCII文字模式；-d或--decompress或----uncompress：解开压缩文件；-f或——force：强行压缩文件。不理会文件名称或硬连接是否存在以及该文件是否为符号连接；-h或——help：在线帮助；-l或——list：列出压缩文件的相关信息；-L或——license：显示版本与版权信息；-n或--no-name：压缩文件时，不保存原来的文件名称及时间戳记；-N或——name：压缩文件时，保存原来的文件名称及时间戳记；-q或——quiet：不显示警告信息；-r或——recursive：递归处理，将指定目录下的所有文件及子目录一并处理；-S或&lt;压缩字尾字符串&gt;或----suffix&lt;压缩字尾字符串&gt;：更改压缩字尾字符串；-t或——test：测试压缩文件是否正确无误；-v或——verbose：显示指令执行过程；-V或——version：显示版本信息；-&lt;压缩效率&gt;：压缩效率是一个介于1~9的数值，预设值为“6”，指定愈大的数值，压缩效率就会愈高；--best：此参数的效果和指定“-9”参数相同；--fast：此参数的效果和指定“-1”参数相同。-num 用指定的数字num调整压缩的速度，-1或--fast表示最快压缩方法（低压缩比），-9或--best表示最慢压缩方法（高压缩比）。系统缺省值为6。 参数 文件列表：指定要压缩的文件列表。 示例 12345678910111213# 把test6目录下的每个文件压缩成.gz文件gzip *# 把上例中每个压缩的文件解压，并列出详细的信息gzip -dv *# 详细显示例1中每个压缩的文件的信息，并不解压gzip -l *# 压缩一个tar备份文件，此时压缩文件的扩展名为.tar.gzgzip -r log.tar# 递归的压缩目录gzip -rv test6# 这样，所有test下面的文件都变成了.gz，目录依然存在只是目录里面的文件相应变成了.gz.这就是压缩，和打包不同。因为是对目录操作，所以需要加上-r选项，这样也可以对子目录进行递归了。# 递归地解压目录gzip -dr test6 bzipbzip2命令 用于创建和管理（包括解压缩）“.bz2”格式的压缩包。 语法 bzip2 (选项) (参数) 选项 123456789101112-c或——stdout：将压缩与解压缩的结果送到标准输出；-d或——decompress：执行解压缩；-f或-force：bzip2在压缩或解压缩时，若输出文件与现有文件同名，预设不会覆盖现有文件。若要覆盖。请使用此参数；-h或——help：在线帮助；-k或——keep：bzip2在压缩或解压缩后，会删除原始文件。若要保留原始文件，请使用此参数；-s或——small：降低程序执行时内存的使用量；-t或——test：测试.bz2压缩文件的完整性；-v或——verbose：压缩或解压缩文件时，显示详细的信息；-z或——compress：强制执行压缩；-V或——version：显示版本信息；--repetitive-best：若文件中有重复出现的资料时，可利用此参数提高压缩效果；--repetitive-fast：若文件中有重复出现的资料时，可利用此参数加快执行效果。 参数 文件：指定要压缩的文件。 示例 1234567891011## 压缩指定文件filename:bzip2 filename 或 bzip2 -z filename## 解压指定的文件filename.bz2:bzip2 -d filename.bz2 或 bunzip2 filename.bz2# 压缩解压的时候将结果也输出：bzip2 -v filename filename: 0.119:1, 67.200 bits/byte, -740.00% saved, 5 in, 42 out.# 压缩解压的时候，除了生成结果文件，将原来的文件也保存: bzip2 -k filename# 解压到标准输出, 输出文件的内容bzip2 -dc filename.bz2 tarLinux下的归档使用工具，用来打包和备份。 首先要弄清两个概念：打包和压缩。打包是指将一大堆文件或目录变成一个总的文件；压缩则是将一个大的文件通过一些压缩算法变成一个小文件。 为什么要区分这两个概念呢？这源于Linux中很多压缩程序只能针对一个文件进行压缩，这样当你想要压缩一大堆文件时，你得先将这一大堆文件先打成一个包（tar命令），然后再用压缩程序进行压缩（gzip bzip2命令）。 其实最简单的使用 tar 就只要记忆底下的方式即可： tar.gz 格式: 1234# 一次性打包并压缩、解压并解包打包并压缩： tar -zcvf [目标文件名].tar.gz [原文件名/目录名]解压并解包： tar -zxvf [原文件名].tar.gz -C [目标目录]注：z代表用gzip算法来压缩/解压。 tar.bz2格式: 1234# 一次性打包并压缩、解压并解包打包并压缩： tar -jcvf [目标文件名].tar.bz2 [原文件名/目录名]解压并解包： tar -jxvf [原文件名].tar.bz2注：小写j代表用bzip2算法来压缩/解压。 其实用不到的话就不用看了, 下面的. 语法 tar(选项)(参数) 选项 1234567891011121314151617181920212223-A或--catenate：新增文件到以存在的备份文件；-B：设置区块大小；-c或--create：建立新的备份文件；-C &lt;目录&gt;：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。-d：记录文件的差别；-x或--extract或--get：从备份文件中还原文件；-t或--list：列出备份文件的内容；-z或--gzip或--ungzip：通过gzip指令处理备份文件；-Z或--compress或--uncompress：通过compress指令处理备份文件；-f&lt;备份文件&gt;或--file=&lt;备份文件&gt;：指定备份文件；-v或--verbose：显示指令执行过程；-r：添加文件到已经压缩的文件；-u：添加改变了和现有的文件到已经存在的压缩文件；-j：支持bzip2解压文件；-v：显示操作过程；-l：文件系统边界设置；-k：保留原有文件不覆盖；-m：保留文件不被覆盖；-w：确认压缩文件的正确性；-p或--same-permissions：用原来的文件权限还原文件；-P或--absolute-names：文件名使用绝对名称，不移除文件名称前的“/”号；-N &lt;日期格式&gt; 或 --newer=&lt;日期时间&gt;：只将较指定日期更新的文件保存到备份文件里；--exclude=&lt;范本样式&gt;：排除符合范本样式的文件。 参数 文件或目录：指定要打包的文件或目录列表。 -f: 使用档案名字，切记，这个参数是最后一个参数，后面只能接档案名。 12345- z：有gzip属性的- j：有bz2属性的- Z：有compress属性的- v：显示所有过程- O：将文件解开到标准输出 实例 zip格式 123压缩： zip -r [目标文件名].zip [原文件/目录名]解压： unzip [原文件名].zip注： -r参数代表递归 tar格式（该格式仅仅打包，不压缩） 123打包：tar -cvf [目标文件名].tar [原文件名/目录名]解包：tar -xvf [原文件名].tar注：c参数代表create（创建），x参数代表extract（解包），v参数代表verbose（详细信息），f参数代表filename（文件名），所以f后必须接文件名。 tar.gz格式(方式二常用) 12345678# 方式一：利用前面已经打包好的tar文件，直接用压缩命令。压缩：gzip [原文件名].tar解压：gunzip [原文件名].tar.gz# 方式二：一次性打包并压缩、解压并解包打包并压缩： tar -zcvf [目标文件名].tar.gz [原文件名/目录名]解压并解包： tar -zxvf [原文件名].tar.gz注：z代表用gzip算法来压缩/解压。 tar.bz2格式 12345678方式一：利用已经打包好的tar文件，直接执行压缩命令：压缩：bzip2 [原文件名].tar解压：bunzip2 [原文件名].tar.bz2方式二：一次性打包并压缩、解压并解包打包并压缩： tar -jcvf [目标文件名].tar.bz2 [原文件名/目录名]解压并解包： tar -jxvf [原文件名].tar.bz2注：小写j代表用bzip2算法来压缩/解压。 jar格式 12345678910压缩：jar -cvf [目标文件名].jar [原文件名/目录名]解压：jar -xvf [原文件名].jar# 注：如果是打包的是Java类库，并且该类库中存在主类，那么需要写一个META-INF/MANIFEST.MF配置文件，内容如下：Manifest-Version: 1.0Created-By: 1.6.0_27 (Sun Microsystems Inc.)Main-class: the_name_of_the_main_class_should_be_put_here# 然后用如下命令打包：jar -cvfm [目标文件名].jar META-INF/MANIFEST.MF [原文件名/目录名] 这样以后就能用“java -jar [文件名].jar”命令直接运行主类中的public static void main方法了。 7z格式 12345压缩：7z a [目标文件名].7z [原文件名/目录名]解压：7z x [原文件名].7z注：这个7z解压命令支持rar格式，即：7z x [原文件名].rar 其它例子 参考网址 7. Linux开关机和重启 开机：开机键 关机：shutdown，halt，init 0，poweroff 重启：reboot，init 6 Shutdown命令详解： shutdown -h now ## 立刻关机 shutdown -h +10 ## 10分钟以后关机 shutdown -h 12:00:00 ##12点整的时候关机]]></content>
      <categories>
        <category>Linux</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习笔记-1]]></title>
    <url>%2F2018%2F05%2F26%2FLinux%2FLinux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1%2F</url>
    <content type="text"><![CDATA[1&gt; 初识Linux1. Linux介绍Linux系统是一套免费使用和自由传播的类UNIX操作系统（主要用在服务器上），是一个基于POSIX和UNIX的多用户、多任务、支持多线程和多CPU的操作系统。它能运行主要的UNIX工具软件、应用程序和网络协议。它支持32位和64位硬件。Linux继承了UNIX以网络为核心的设计思想，是一个性能稳定的多用户网络操作系统。 UNIX：操作系统，是美国AT&amp;T公司贝尔实验室于1969年完成的操作系统，最早由肯·汤普逊（Ken Thompson），丹尼斯·里奇（Dennis Ritchie）开发。在1971年首次对外发布，刚好在1971，丹尼斯·里奇（Dennis Ritchie）发明了C语言，后来在1973，Unix被他用C语言重新编写。Unix前身源自于MultiCS，叫UniCS，后来改名叫Unix。 POSIX：可移植操作系统接口（英语：Portable Operating System Interface of UNIX，缩写为POSIX），是IEEE（电气和电子工程师协会）为要在各种UNIX操作系统上运行软件，而定义API的一系列互相关联的标准的总称。 GNU：1983年，Richard Stallman（理查德·马修·斯托曼）创立GNU计划。一套完全自由的操作系统，其内容软件完全以GPL方式发布。这个操作系统是GNU计划的主要目标（发展出一套完整的开放源代码操作系统来取代Unix），名称来自GNU\’s Not Unix!的递归缩写。 GPL：一种GNU通用公共许可协议，为保证GNU软件可以自由的使用、复制、修改和发布，所有的GNU软件都有一份在禁止其他人添加任何限制的情况下授权所有权利给任何人的协议条款，是一个被广泛使用的自由软件许可协议条款，保证终端用户运行、学习、分享（复制）及编辑软件之自由，GPL是自由软件和开源软件的最流行许可证，特色表现： •取得软件与原始码：您可以根据自己的需求来执行这个自由软件 •复制：您可以自由的复制该软件 •修改：您可以将取得的原始码进行程序修改工作，使之适合您的工作 •再发行：您可以将您修改过的程序，再度的自由发行，而不会与原先的撰写者冲突 •回馈：您应该将您修改过的程序代码回馈于社群 ==不同许可证的区别== 1985年，Richard Stallman又创立了自由软件基金会（Free Software Foundation，FSF）来为GNU计划提供技术、法律以及财政支持。 1990年，GNU计划开发主要项目有Emacs（文本编辑器）、GCC（GUN Compiler Collection，GNU编译器集合）、Bash等，GCC是一套GNU开发的编程语言编译器。还有开发一些UNIX系统的程序库和工具。 Linux操作系统诞生于1991年10月5日（这是第一次正式向外公布时间），与UNIX兼容，并在GPL条款下发布。现在，Linux产生了许多不同的Linux发行版本，但它们都使用了Linux内核。Linux可安装在各种计算机硬件设备中，比如手机、平板电脑、路由器、视频游戏控制台、台式计算机、大型机和超级计算机。 1992年，Linux与其他GUN软件结合，完全自由的GNU/Linux操作系统正式诞生，简称Linux ==Linux的基本思想有两点== 第一，一切都是文件 第二，每个软件都有确定的用途 与Unix思想十分相近。 2. Linux特点1、分时的多用户、多任务操作系统 2、多数网络协议支持、方便的远程管理 3、强大的内存管理和文件系统管理 4、大量的可用的软件和免费的软件 5、优良的稳定性和安全性 6、良好的可移植性和灵活性 7、可供选择的厂商多 3. Linux操作系统架构 补充：linux内核必须加上一个”界面”软件，才能让用户去使用，”界面”分两类： a、命令行界面CLI SHELL（有很多种，最流行的一种是bash shell） b、图形界面GUI SHELL（也有很多种，目前最流行的有两种：gnome和kde） 4. Linux内核严格来讲，Linux不是一个操作系统，Linux只是一个操作系统中的内核。 内核建立了计算机软件与硬件之间通讯的平台，内核提供系统服务，比如文件管理、虚拟内存、设备I/O、进程管理等。 内核官网：[http://www.kernel.org/]{.underline}。目前最新的内核版本：4.13.5 要注意区分linux发型版本和linux内核版本。两者不是同一个事物 下面这位是Linux内核的作者： 这是Linux的logo 5. 常见发行版红帽企业系统（RedHat Enterprise Linux, RHEL） 全球最大的开源技术厂商，全世界内使用最广泛的Linux发布套件， 提供性能与稳定性极强的Linux套件系统并拥有完善的全球技术支持。 官网：[http://www.redhat.com]{.underline} 社区企业操作系统（CentOS） 最初是将红帽企业系统”重新编译/发布”给用户免费使用而广泛使用， 当前已正式加入红帽公司并继续保持免费（随RHEL更新而更新）。 官网：[http://www.centos.org/]{.underline} 红帽用户桌面版（Fedora [Linux]） 最初由红帽公司发起的桌面版系统套件（目前已经不限于桌面版）， 用户可免费体验到最新的技术或工具，而功能成熟后加入到RHEL中。 官网：[http://fedora.redhat.com]{.underline} 国际化组织的开源操作系统（Debian） 提供超过37500种不同的自由软件且拥有很高的认可度， 对于各类内核架构支持性良好，稳定性、安全性强更有免费的技术支持。 官网：[http://www.debian.org/]{.underline} 基于Debian的桌面版（Ubuntu） Ubuntu是一款基于Debian派生的产品，对新款硬件具有极强的兼容能力。 普遍认为Ubuntu与Fedora都是极其出色的LINUX桌面系统。 官网：[http://www.ubuntulinux.org/]{.underline} 当然还有国内的国防科技大学发行麒麟kylin和中科院发行红旗RedFlag 2&gt; Linux文件系统1. CentOS的目录结构 2. 根目录下每个目录的简单解释 /**：**根目录，一般根目录下只存放目录，不要存放文件，/etc、/bin、/dev、/lib、/sbin应该和根目录放置在一个分区中 /bin:/usr/bin: 可执行二进制文件的目录，如常用的命令ls、tar、mv、cat等 /boot**：**放置linux系统启动时用到的一些文件。/boot/vmlinuz为linux的内核文件，以及/boot/gurb。建议单独分区，分区大小100M即可 /dev**：**存放linux系统下的设备文件，访问该目录下某个文件，相当于访问某个设备，常用的是挂载光驱mount /dev/cdrom /mnt /etc**：**系统配置文件存放的目录，不建议在此目录下存放可执行文件，重要的配置文件有/etc/inittab、/etc/gateways、/etc/resolv.conf、/etc/fstab、/etc/init.d、/etc/X11、/etc/sysconfig、/etc/xinetd.d修改配置文件之前记得备份。注：/etc/X11存放与x windows有关的设置 /home**：**系统默认的用户家目录，新增用户账号时，用户的家目录都存放在此目录下，表示当前用户的家目录，test表示用户test的家目录。建议单独分区，并设置较大的磁盘空间，方便用户存放数据 /lib:/lib64:/usr/lib:/usr/local/lib**：**系统使用的函数库的目录，程序在执行过程中，需要调用一些额外的参数时需要函数库的协助，比较重要的目录为/lib/modules /lost+fount**：**系统异常产生错误时，会将一些遗失的片段放置于此目录下，通常这个目录会自动出现在装置目录下。如加载硬盘于/disk 中，此目录下就会自动产生目录/disk/lost+found /mnt:/media**：**光盘默认挂载点，通常光盘挂载于/mnt/cdrom下，也不一定，可以选择任意位置进行挂载 /opt**：**给主机额外安装软件所摆放的目录。如：FC4使用的Fedora 社群开发软件，如果想要自行安装新的KDE桌面软件，可以将该软件安装在该目录下。以前的Linux系统中，习惯放置在 /usr/local目录下option /proc**：*此目录的数据都在内存中，如系统核心，外部设备，网络状态，由于数据都存放于内存中，所以不占用磁盘空间，比较重要的目录有/proc/cpuinfo、/proc/interrupts、/proc/dma、/proc/ioports、/proc/net/等process /root**：**系统管理员root的家目录，系统第一个启动的分区为/，所以最好将/root和/放置在一个分区下 /sbin:/usr/sbin:/usr/local/sbin**：**放置系统管理员使用的可执行命令，如fdisk、shutdown、mount等。与/bin不同的是，这几个目录是给系统管理员root使用的命令，一般用户只能”查看”而不能设置和使用。 /selinux**：**selinux软件目录，用于保证系统安全 /srv**：**服务启动之后需要访问的数据目录，如www服务需要访问的网页数据存放在/srv/www内service /sys**：**类似于/proc的特殊文件系统，存放内核数据信息 /tmp**：**一般用户或正在执行的程序临时存放文件的目录,任何人都可以访问,重要数据不可放置在此目录下 /usr**：应用程序存放目录， /usr/bin 存放应用程序 /usr/share 存放共享数据 /usr/lib 存放不能直接运行的，却是许多程序运行所必需的一些函数库文件 /usr/local:存放软件升级包 /usr/share/doc: 系统说明文件存放目录 /usr/share/man: 程序说明文件存放目录，使用man ls时会查询/usr/share/man/man1/ls.1.gz的内容建议单独分区，设置较大的磁盘空间 usr**：user share resources/unix share resouces /var**：**放置系统执行过程中经常变化的文件，如： /var/log：随时更改的日志文件 /var/log/message：所有的登录文件存放目录 /var/spool/mail：邮件存放的目录 /var/run：程序或服务启动 使用建议： 用户应该将文件存储在自己的主目录及其子目录下 系统绝大多数设置都在/etc目录下 不要修改/或者/usr目录下的任何内容，除非你真的清楚你在做什么，也就是说/目录最好和安装好系统之初保持一致 大多数工具和应用软件程序都安装在/bin，/sbin，/usr/bin，/usr/sbin，/usr/local/bin 文件或者目录都有唯一的绝对路径，没有盘符的概念 3&gt; Linux命令终端1. Linux 的命令格式：命令选项 命令参数注意：三者之间要空格隔开，其中命令选项分为长格式和短格式。 短格式用’-‘表示，比如：-l， 长格式用”–”表示，比如：–help， 也可以使用组合格式，比如：-a -l 等价于-la或者-al 2. Linux的默认命令提示符：#：管理员用户 $：普通用户 PS: Linux以回车键表示命令结束，如果 linux命令需要折行输入，那么可以以 \表示每行结束 4&gt; 常用命令归纳分类基本命令 文件管理 mkdir, rmdir, mv, rm, cp, touch, cat, tac, echo, more, less, head, tail, file, find, rename, ln, pwd, scp, alias 磁盘管理 ls, cd, du, df, mount, unmounts, fdisk 文档处理 wc, sort, uniq, cut, sed, awk, grep, vi, diff 用户和组 useradd, usermod, passwd, userdel, groupadd, groupdel, chgrp, su 文件传输 get, put, wget 网络通信 telnet, nc, ifconfig, ping, netstat, ip, host 备份压缩 gzip, bzip2, bunzip2, tar, zip 系统管理 exit, kill, last, ps, top, free, pstree, reboot, halt, shutdown, sudo, who, w, whoami, whereis, which, last, whatis 系统设置 clear, set, unset, hwclock, time, date, 其他 history, hostname, nohup, service, init, rpm, ssh, cal, yum 网站速查http://man.linuxde.net/ http://www.jb51.net/linux/ https://jaywcjlove.github.io/linux-command ps: 直接在命令行中, 用 man xx, 也可以查看命令信息 5&gt; 常用文件系统命令详解磁盘管理 cd change directory 常使用方式： cd sourcedir 进入用户主目录 cd ~ 进入用户主目录 cd - 返回进入此目录之前所在的目录 cd .. 返回上级目录（若当前目录为”/“，则执行完后还在”/“；”..”为上级目录的意思） cd ../.. 返回上两级目录 pwd print working directory pwd 显示当前工作目录 ls list：显示目录内容列表 使用格式：ls 选项 目录或文件名 常用选项 -l：详细信息显示 -a：显示所有子目录和文件的信息，包括隐藏文件 -A：类似于“-a”，但不显示“.”和“..”目录的信息 -R：递归显示内容 -h：以友好方式显示文件大小 例子： ls -l ## 列出文件详细信息， 也可以写作 ll ls -lah ## 以友好方式显示包括隐藏文件的详细信息 du disk usage：显示每个文件和目录的磁盘使用空间 使用格式：du 选项 目录或文件名 常用选项： -a：统计时包括所有的文件，而不仅仅只统计目录 -h：以更易读的字节单位（K、M等）显示信息 -s：只统计每个参数所占用空间总的大小 例子： du -ah ## df disk free：显示磁盘相关信息 常用选项： -h：以更易读的字节单位（K、M等）显示信息 -T：显示分区格式 例子： df -h ## 显示磁盘信息，以友好方式 df -T -h ## 以友好格式显示磁盘信息，并且附加磁盘格式 文件管理 touch 创建空文件 或更新文件时间标记 使用格式： touch 文件名 file 查看文件类型 使用格式： file 文件名 根据文件内容格式判断文件类型。而不是根据后缀名 mkdir 创建文件夹 make directory 使用格式：mkdir 选项 参数 常用选项： -p：已级联的方式创建文件夹 例子： mkdir -p /root/ma/niu/zhu/dagou ## 上级目录不存在自动创建上一级目录，常用 cp 复制文件 使用格式： cp 选项 源文件或目录… 目标文件或目录 常用选项： -r：递归复制整个目录树 -p：保持源文件的属性不变 -i：需要覆盖文件或目录时进行提醒 rm 删除文件或目录 使用格式：rm [选项] 文件或目录 常用选项： -f：强行删除文件或目录，不进行提醒 -i：删除文件或目录时提醒用户确认 -r：递归删除整个目录树 例子： rm -rf /root/ma/ ## 不提醒递归删除整个目录，慎用慎用慎用 mv 移动文件 如果与源文件位置一样，则相当于重命名 使用格式： mv [选项]… 源文件或目录… 目标文件或目录 常用选项： -f：若目标文件或目录与现有的文件或目录重复，则直接覆盖现有的文件或目录 -u：当源文件比目标文件新或者目标文件不存在时，才执行移动操作 rmdir 删除空文件夹 常用选项： -p或–parents：删除指定目录后，若该目录的上层目录已变成空目录，则将其一并删除； rmdir -p /root/aa/bb/cc/dd/ee 删除文件夹ee，如果删除ee后，dd变为空，则删除dd，依次类推 cat 连接文件并打印到标准输出设备上 常用选项： -n或–number：由1开始对所有输出的行数编号 cat /home/hadoop/data.txt ## 查看文件内容 tac 倒序输出文件内容 tac /home/hadoop/data.txt echo 输出指定的字符串或者变量 常用选项： -e：若字符串中出现以下字符，则特别加以处理，而不会将它当成一般文字输出： \a 发出警告声； \b 删除前一个字符； \c 最后不加上换行符号； \f 换行但光标仍旧停留在原来的位置； \n 换行且光标移至行首； \r 光标移至行首，但不换行； \t 插入tab； \v 与\f相同； \ 插入\字符； \nnn 插入nnn（八进制）所代表的ASCII字符； 例子： echo ‘ma’ ## 输出ma echo -e ‘ma\n’ ## 打印ma之后换行 echo -ne ‘ma’ ‘zhonghua’ ## 打印完不换行 echo ‘ma’ &gt; ma.dat ## 覆盖 echo ‘ma’ &gt;&gt; ma.dat ## 追加 echo $PWD ## 输出变量内容 head 在屏幕上显示指定文件的开头若干行 默认显示10行 常用选项： -n&lt;数字&gt;：指定显示头部内容的行数； 例子： head -n 5 install.log ## 显示该文件前五行内容 tail 在屏幕上显示指定文件的末尾若干行 常用选项： -f：显示文件最新追加的内容 tail -f install.log ## 显示最新追加的内容 ## 显示文件file的最后10行 tail -1 file ## 显示文件file最后一行的内容 tail -c 10 file ## 显示文件file的最后10个字符 more 显示文件内容，每次显示一屏 使用方式： 按Space键：显示文本的下一屏内容。 按Enter键：只显示文本的下一行内容。 按h键：显示帮助屏，该屏上有相关的帮助信息。 按b键：显示上一屏内容。 按q键：退出more命令。 less 分屏上下翻页浏览文件内容 和more使用方式基本类似 按e键：向上滚动一行 按y键：向下滚动一行 G：跳到文件末尾 gg：跳到文件首行 ln 用来为文件创件连接 软链接 -s 和 硬链接 ln /mnt/cdrom1 /var/www/html/centos/ —&gt;硬链接 ln -s /mnt/cdrom2 /var/www/html/centos/ –&gt;软链接-符号链接 alias 别名 查看别名：alias 定义别名：alias la=&#39;ll -a 取消别名：unalias la 软/硬 链接相关软链接, ln -s 文件/文件夹 产生的链接 创建文件的软链接 1ln -s /tmp/yum.log /root/yuntest 创建文件夹的软链接 1234# 后面的链接不指定名字, 就默认用前面的源文件/文件夹名字ln -s /tmp /root ---lrwxrwxrwx. 1 root root 4 5月 29 20:53 tmp -&gt; /tmp 如果软链接是指向目录的话, 是可以直接cd进去的, cd进去的是真实的目录! 12pwd -P: 查看软链接的真实指向某目录(物理目录)pwd -L: 逻辑目录 硬链接, ln 不加s, 一般不会用 不能链接目录 不能跨分区做硬链接 其它有用的命令123456789101112131415161718man：显示命令帮助信息clear：清屏，或者按ctrl + l也行ctrl + c：退出当前进程ctrl + z：挂起当前前台进程whatis：命令是什么whereis：在标准路径下搜索与名称相关的文件，whereis将所有搜索到的文件都显示which：which在设定的搜索路径下进行目录搜索，只显示搜索到的第一个文件su：切换用户history：显示历史命令hostname：显示主机名set：查看系统变量get：下载文件put：上传文件sudo：以root用户权限执行一次命令exit：退出登录状态w：显示当前连接的用户who：显示当前会话信息uptime：查看系统运行时间 使用小技巧(重点)12345678910111213141516ctrl + u：清除光标前的命令，相当于剪切 # !ctrl + k：清除光标后的命令，相当于剪切 # !ctrl + y：粘贴 # !ctrl + t：把光标前面的那个字符往后挪动一位ctrl + l：清屏ctrl + a: 移到命令行首 # !ctrl + e: 移到命令行尾 # !ctrl + ← →: 光标移动一个单词 # !!!：执行上一次命令 # !!$：上个命令的最后一个单词ctrl + w：删除光标前一个单词cd data; cat sed.txt：表示先执行cd，然后执行cat，工作目录会切换(cd data; cat sed.txt)：跟上个命令相比，不切换工作目录|：管道符，表示把前面命令内容的输出当做后面命令的输入&gt;：表示内容覆盖&gt;&gt;：表示内容追加]]></content>
      <categories>
        <category>Linux</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习笔记-3]]></title>
    <url>%2F2018%2F05%2F25%2FLinux%2FLinux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3%2F</url>
    <content type="text"><![CDATA[1. 系统服务管理 检查本机httpd服务是否开启, 使用命令service httpd status 开启/关闭/重启 httpd服务 service httpd start/stop/restart 查看所有的服务状态 service --status-all 过滤出某个服务service –status-all | grep httpd` 防火墙服务 sevice iptables status/stop/start/restart 配置后台服务进程开机自启 12345# 开启开机自启http进程-&gt; chkconfig httpd on# 查看httpd的开机自启状态-&gt; chkconfig --list | grep httpdhttpd 0:关闭 1:关闭 2:启用 3:启用 4:启用 5:启用 6:关闭 缺省系统运行级别 0 为停机，机器关闭。 1 为单用户模式，就像Win9x下的安全模式类似。 2 为多用户模式，但是没有NFS支持。 3 为完整的多用户模式，是标准的运行级。 4 一般不用，在一些特殊情况下可以用它来做一些事情。例如在笔记本 电脑的电池用尽时，可以切换到这个模式来做一些设置。 5 就是X11，进到X Window系统了。 6 为重启，运行init 6机器就会重启。 Centos中时区 当前正在使用的时d区文件位于 /etc/localtime 其他时区文件则位于 /usr/share/zoneinfo 中国时区使用 /usr/share/zoneinfo/Asia/Shanghai 更改时区 cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 如果没有 Asia/Shanghai 时区文件，请使用tzselect命令去生成时区文件，生成好的时区文件就在 /usr/share/zoneinfo目录下 修改系统时间 date : 直接查看时间 date -s : 手动设置时间 sudo ntpdate ntp1.aliyun.com : 同步时间服务器时间 2. 简单磁盘管理 df：列出文件系统的整体磁盘使用量 -h 人性化的列出文件的大小等 123-&gt; # df -h /dev/sr0Filesystem Size Used Avail Use% Mounted on/dev/sr0 3.7G 3.7G 0 100% /media/cdrom du：检查磁盘空间使用量 fdisk：用于磁盘分区 3. 文件的基本属性前缀的含义 1234567# 查看目录的权限-&gt; # ll -d /var/www/html/localyum ls -ld 也是一样, 查看目录的权限drwxr-xr-x. 7 root root 4096 5月 25 07:50 autojump-rw-r--r--. 1 root root 50434 5月 23 10:11 install.loglrwxrwxrwx. 1 root root 13 5月 25 21:04 localyum -&gt; /media/cdrom/ 当为[ d ]则是目录 当为[ - ]则是文件； 若是[ l ]则表示为链接文档(link file)； 若是[ b ]则表示为装置文件里面的可供储存的接口设备(可随机存取装置)； 若是[ c ]则表示为装置文件里面的串行端口设备，例如键盘、鼠标(一次性读取装置); r、w、x 对于文件和目录的含义 权限 对文件的含义 对目录的含义 r 读权限 可以查看文件内容 可以列出目录中的内容 w 写权限 可以修改文件内容 可以再目录中创建、删除文件 x 执行权限 可以执行文件 可以进入目录 4. 软件安装 TODO1. 二进制发布包安装 TODO:软件已经针对具体平台编译打包发布，只要解压，修改配置即可 安装jdk , 安装tomcat也一样 TODO: 通过 ftp工具把 jdk传到 linux服务器 创建一个 /var/www/html/soft/jdk8 的软链接, 指向服务器中的安装包 —— 这一步失败了, 先搁置把, 先copy过去. 2. 源码编译安装 TODO:软件以源码工程的形式发布，需要获取到源码工程后用相应开发工具进行编译打包部署 安装/卸载 redis 卸载: 首先查看redis-server是否启动 ps aux | grep redis 有的话, 关闭这些进程 kill -9 进程pid 删除redis相应的文件夹就可以了。 find / -name redis 安装: 拷贝到/usr/local, 解压, 删掉原安装包 12tar -zxvf redis-3.0.0.tar.gz rm redis-3.0.0.tar.gz 检查运行环境 123# 检测一下是否可以安装makemake test 安装到指定目录 1make PREFIX=/root/apps/redis install 拷贝配置信息 1cp /usr/local/redis-3.0.0/redis.conf /usr/local/redis/bin 启动 前端模式 bin/redis-server 后端模式启动 修改redis.conf配置文件，daemonize yes` 以后端模式启动 : TODO 3. RPM发布包软件已经按照RedHat（Redhat Package Manager）的包管理工具规范RPM进行打包发布，需要获取到相应的软件RPM发布包，然后用rpm命令进行安装 Mysql安装 1234567891011121314151617181920# rpm安装命令1、安装包：rpm -ivh 包名参数：-i ：安装的意思-v ：可视化-h ：显示安装进度另外在安装一个rpm包时常用的附带参数有：--force 强制安装，即使覆盖属于其他包的文件也要安装--nodeps 当要安装的rpm包依赖其他包时，即使其他包没有安装，也要安装这个包2、升级包：rpm -Uvh filename-U 升级3、卸载包rpm -e filename （这里的filename是通过rpm的查询功能所查询到的）4、查询一个包是否安装：rpm -q 包名（这里的包名，是不带有平台信息以及后缀名的）5.查询当前安装的所有rpm包：rpm -qa查询当前安装的和sql相关的包：rpm -qa | grep 'sql'查询sqlite安装路径：rpm -ql sqlite 5. 设置本地/网络yum源 首先检查虚拟机的 CD/DVD驱动器是否有挂载iso镜像文件 找到挂载源的位置 在 /dev/sr0下, 将其挂载到/mnt下创建的cdrom文件夹下 mount -t iso9660 -o ro /dev/sr0 /media/cdrom 配置开机挂载 , vi /etc/fstab, 增加一行 /dev/cdrom /media/cdrom iso9660 defaults 0 0 创建软连接, 设置可以通过web访问 ln -s /mnt/cdrom/ /var/www/html/yumsorurce 修改etc/yum.repos.d中的文件entOS-Media.repo中的enabled=1, 开启从本地寻找, 因为文件中, 原本就配置了baseurl: file:///media/cdrom/, 而我们自己创建了media/cdrom这个目录, 所以就可以从这里面读取了 执行yum repolist, 发现已经能读取出media中的repo了, 大功告成 c6-media CentOS-6 - Media 6,575 tips: 个人觉得还是配置个网络源比较好, 比如163/阿里的. 12345678910111213141516171819# 操作步骤# 1.备份原来的Base源cd /etc/yum.repos.d/mv CentOS-Base.repo CentOS-Base.repo_bak# 2. 下载网易/阿里源 到/etc/yum.repos.d下, 替换掉原本的# 网易源wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.163.com/.help/CentOS6-Base-163.repo# 阿里源wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repoyum clean allyum makecache# yum使用yum update 􏻦􏻧􏷉􏷊升级系统yum install -y xxx 直接安装, 不用确认􏻈yum update ~ 􏻦􏻧􏸻􏹺􏷜 升级指定软件包yum remove ~ 􏺌􏷎􏸻􏹺􏷜􏷕 卸载指定软件 挂载, 配置源的其他说明 12345678910111213141516171819202122232425# 关于挂载# lrwxrwxrwx. 1 root root 3 5月 25 16:47 cdrom -&gt; sr0cdrom其实是sr0的软链接, 因此直接找sr0即可# 挂载的基本语法mont -t iso9660 -o ro(只读) /dev/sr0 /mnt/cdrom (挂载类型) (挂载方式) (挂载源) (挂载点)# 卸载的语法umount /mnt/cdrom 如果卸载时遇到 umount: /mnt/cdrom: device is busy.解决方式: 1. 查找哪个进程在用: fuser /mnt/cdrom 2. 查找进程: ps -ef | grep 进程号 3. 插死进程: kill -9 进程号, 如果是root用户, 可能会断开连接, 需要重连 4. 然后继续 umount, 如果不行, 就强制卸载 umount -f /mnt/cdrom# 关于etc/yum.repos.d 中文件的说明 CentOS-Base.repo：有网的环境下默认使用这个，这个是第一优先级。因为没网，所以修改文件名，设置成备份文件。这样系统就会使用第二优先级的文件。 CentOS-Media.repo：没网的环境下使用这个，在上图中会发现他默认配置了4个路径，第4个yumsource是我自己加的。意思是说，如果系统检测yum使用了离线安装，那么会从上到下从这4个路径中查找安装软件。所以我们只要把光盘挂载在这四个目录下的任意一个目录即可。同时，该配置文件默认是不启用的，如果想使用需要修改倒数第二行的enabled为1，否则该文件无效。 # /mnt &amp; /media 目录的区别media：挂载一些移动设备，例如光盘，U盘等。mnt： 挂载一些硬盘等设备。所以我们的光盘应该挂载在media目录下，从yum给的默认配置文件也能看出。 6. 进程相关1. ps命令ps命令用于报告当前系统的进程状态。可以搭配kill指令随时中断、删除不必要的程序。 常用选项包括： 123456ps -1、-a显示所有用户的进程2、-u显示用户名和启动时间3、-x显示所有进程，包括没有控制终端的进程4、-e显示所有进程，包括没有控制终端的进程，较x选项，信息更为简略5、-l显示进程详细信息，按长格式显示 常用组合 ps -au 显示所有用户进程，并给出用户名和启动时间等详细信息 ps -aux 显示所有用户进程，包括没有控制终端的进程，并给出用户和和启动埋单等详细信息 ps -el 按长格式显示进程详细信息 1234567891011121314151617181920212223242526272829303132# 上述命令可能出现的字段含义USER: 进程所有者PID: 进程号PPID: 进程的父进程ID%CPU: CPU占用率C: 进程的CPU占用率%MEM: 内存占用率VSZ: 表示如果一个程序完全驻留在内存的话需要占用多少内存空间;RSS: 指明了当前实际占用了多少内存;TTY: 终端的次要装置号码 (minor device number of tty)F：进程的标志S：进程的状态STAT: 该进程程的状态，有以下值D: 不可中断的静止R: 正在执行中S: 静止状态T: 暂停执行Z: 不存在但暂时无法消除W: 没有足够的记忆体分页可分配&lt;: 高优先序的进程N: 低优先序的进程L: 有记忆体分页分配并锁在记忆体内PRI：进程的优先权NI：进程的Nice值ADDR：进程的地址空间SZ：进程占用内存的大小WCHAN：进程当前是否在运行TTY：进程所属终端START: 进程开始时间TIME: 执行的运行时间COMMAND：所执行的指令CMD：进程的命令 2. kill / pidof / pkill 命令有时候某个进程可能会长期占用CPU资源或无法正常执行或超出运行时间等，此时可能希望人工干预直接将进程杀死，这时候kill命令可以派上用场 12341、kill pid 直接杀死进程，但不能保证一定能杀死2、kill -9 pid 强制杀死进程3、pidof命令用于查看某个进程的进程号（例如：pidof mysqld）4、pkill命令可以按照进程名杀死进程。pkill和killall应用方法差不多，也是直接杀死运行中的程序；如果您想杀掉单个进程，请用kill来杀掉 3. 进程切换前台进程指的是进程在执行时会将命令行阻塞，直到进程执行完毕；后台进程指的是进程在执行时不会阻塞当前命令行，而是在系统后台执行 123451、ctrl + c 终止进程2、ctrl + z 挂起进程3、fg命令将进程转换到前台执行4、bg命令将进程转换到后台执行5、jobs命令查看任务 4. top命令top 命令可以定期显示所有正在运行和实际运行并且更新到列表中，它显示出 CPU 的使用、内存的使用、交换内存、缓存大小、缓冲区大小、过程控制、用户和更多命令。它也会显示内存和 CPU 使用率过高的正在运行的进程。 按q键退出查看. 5. pstree命令将进程间的关系以树结构的形式展示，能清楚看各进程之间的父子关系 12pstree ：以树状形式显示进程pstree -p ： 以树状形式显示进程，并且显示进程号 6. JPS命令JPS命令是JDK提供的一个检查系统是否启动了JVM进程的一个进程。不是linux系统自带的。主要任务就是用来检查java进程的。 7. 计划任务概念计划任务在Linux的体现主要分为at和crontab，其中： at：通过at命令安排任务在某一时刻执行一次 crontab：通过crontab 命令，我们可以在固定的间隔时间执行指定的系统指令或 shell script脚本。时间间隔的单位可以是分钟、小时、日、月、周及以上的任意组合。这个命令非常适合周期性的日志分析或数据备份等工作。 命令服务管理crontab在CentOS系统上，crontab服务的名称叫做crond 安装 yum -y install crontabs 服务操作说明 1234567891011service crond start #启动服务service crond stop #关闭服务service crond restart #重启服务service crond reload #重新载入配置service crond status #服务状态#查看crontab服务是否已设置为开机启动，执行命令：chkconfig --list#加入开机自动启动：chkconfig --level 35 crond on crontab功能使用 命令格式 123456789101112131415161718192021222324252627282930313233crontab [-u user] filecrontab [-u user] [ -e | -l | -r ]# crontab 参数说明：-u user：用来设定某个用户的crontab服务，例如，”-u ixdba”表示设定ixdba用户的crontab服务，此参数一般有root用户来运行。file：file是命令文件的名字,表示将file做为crontab的任务列表文件并载入crontab。-e：编辑某个用户的crontab文件内容。如果不指定用户，则表示编辑当前用户的crontab文件。-l：显示某个用户的crontab文件内容，如果不指定用户，则表示显示当前用户的crontab文件内容。-r：删除定时任务配置，从/var/spool/cron目录中删除某个用户的crontab文件，如果不指定用户，则默认删除当前用户的crontab文件。-i：在删除用户的crontab文件时给确认提示。# 命令示例：crontab file [-u user] ## 用指定的文件替代目前的crontab。 # 必须掌握：crontab -l [-u user] ## 列出用户目前的crontab. crontab -e [-u user] ## 编辑用户目前的crontab.# 通过crontab添加的计划任务都会存储在/var/spool/cron/目录里# 查看当前服务状态service crond status# 操作服务/sbin/service crond start //启动服务/sbin/service crond stop //关闭服务/sbin/service crond restart //重启服务/sbin/service crond reload //重新载入配置# 查看开机启动服务ntsysv # 退出时, 按tab切换# 加入开机自动启动chkconfig –level 35 crond on 配置说明 123456789101112131415161718# 基本格式 : * * * * * command 分 时 日 月 周 命令 # 每个小时的第几分钟执行该任务. 其它的类似第1列表示分钟1～59 每分钟用*或者 */1表示 第2列表示小时0～23（0表示0点） 7-9表示：8点到10点之间第3列表示日期1～31 第4列表示月份1～12 第5列标识号星期0～6（0表示星期天） 第6列要运行的命令# 记住几个特殊符号的含义:“*”代表取值范围内的数字,“/”代表”每”,“-”代表从某个数字到某个数字,“,”分开几个离散的数字 配置示例 123456789101112131415161718192021222324252627282930313233# * * * * * command # 分 时 日 月 周 命令 */1 * * * * date &gt;&gt; /root/date.txt上面的例子表示每分钟执行一次date命令可用 tail -f查看30 21 * * * /usr/local/etc/rc.d/httpd restart上面的例子表示每晚的21:30重启apache45 4 1,10,22 * * /usr/local/etc/rc.d/httpd restart上面的例子表示每月1、10、22日的4 : 45重启apache10 1 * * 6,0 /usr/local/etc/rc.d/httpd restart 上面的例子表示每周六、周日的1 : 10重启apache0,30 18-23 * * * /usr/local/etc/rc.d/httpd restart上面的例子表示在每天18 : 00至23 : 00之间每隔30分钟重启apache0 23 * * 6 /usr/local/etc/rc.d/httpd restart上面的例子表示每星期六的11 : 00 pm重启apache* */1 * * * /usr/local/etc/rc.d/httpd restart上面的例子每一小时重启apache* 23-7/1 * * * /usr/local/etc/rc.d/httpd restart上面的例子晚上11点到早上7点之间，每隔一小时重启apache0 11 4 * mon-wed /usr/local/etc/rc.d/httpd restart上面的例子每月的4号与每周一到周三的11点重启apache0 4 1 jan * /usr/local/etc/rc.d/httpd restart 上面的例子一月一号的4点重启apache 更详细的见这里 8. Linux虚拟主机集群测试环境基本搭建 注意点: 首次使用 NAT 模式装好CentOS之后, 使用ifconfig查看ip, 虚拟机是没有ip的, 需要手动开启ip服务, 命令是 dhclient, 如果已开启就不能再次开启 1. 第一台虚拟主机的静态ip配置(针对于mac环境) 执行ifconfig命令, 如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667shixuanji@x:~|⇒ ifconfiglo0: flags=8049&lt;UP,LOOPBACK,RUNNING,MULTICAST&gt; mtu 16384 options=1203&lt;RXCSUM,TXCSUM,TXSTATUS,SW_TIMESTAMP&gt; inet 127.0.0.1 netmask 0xff000000 inet6 ::1 prefixlen 128 inet6 fe80::1%lo0 prefixlen 64 scopeid 0x1 nd6 options=201&lt;PERFORMNUD,DAD&gt;gif0: flags=8010&lt;POINTOPOINT,MULTICAST&gt; mtu 1280stf0: flags=0&lt;&gt; mtu 1280EHC29: flags=0&lt;&gt; mtu 0EHC26: flags=0&lt;&gt; mtu 0XHC20: flags=0&lt;&gt; mtu 0en0: flags=8823&lt;UP,BROADCAST,SMART,SIMPLEX,MULTICAST&gt; mtu 1500 ether 60:03:08:a1:ac:ee nd6 options=201&lt;PERFORMNUD,DAD&gt; media: autoselect (&lt;unknown type&gt;) status: inactivep2p0: flags=8802&lt;BROADCAST,SIMPLEX,MULTICAST&gt; mtu 2304 ether 02:03:08:a1:ac:ee media: autoselect status: inactiveawdl0: flags=8902&lt;BROADCAST,PROMISC,SIMPLEX,MULTICAST&gt; mtu 1484 ether 72:25:b2:c8:2a:03 nd6 options=201&lt;PERFORMNUD,DAD&gt; media: autoselect status: inactiveen1: flags=8963&lt;UP,BROADCAST,SMART,RUNNING,PROMISC,SIMPLEX,MULTICAST&gt; mtu 1500 options=60&lt;TSO4,TSO6&gt; ether 32:00:1a:0d:12:00 media: autoselect &lt;full-duplex&gt; status: inactiveen2: flags=8963&lt;UP,BROADCAST,SMART,RUNNING,PROMISC,SIMPLEX,MULTICAST&gt; mtu 1500 options=60&lt;TSO4,TSO6&gt; ether 32:00:1a:0d:12:01 media: autoselect &lt;full-duplex&gt; status: inactivebridge0: flags=8822&lt;BROADCAST,SMART,SIMPLEX,MULTICAST&gt; mtu 1500 options=63&lt;RXCSUM,TXCSUM,TSO4,TSO6&gt; ether 32:00:1a:0d:12:00 Configuration: id 0:0:0:0:0:0 priority 0 hellotime 0 fwddelay 0 maxage 0 holdcnt 0 proto stp maxaddr 100 timeout 1200 root id 0:0:0:0:0:0 priority 0 ifcost 0 port 0 ipfilter disabled flags 0x2 member: en1 flags=3&lt;LEARNING,DISCOVER&gt; ifmaxaddr 0 port 10 priority 0 path cost 0 member: en2 flags=3&lt;LEARNING,DISCOVER&gt; ifmaxaddr 0 port 11 priority 0 path cost 0 media: &lt;unknown type&gt; status: inactiveutun0: flags=8051&lt;UP,POINTOPOINT,RUNNING,MULTICAST&gt; mtu 2000 inet6 fe80::2ed8:c28:27b2:f5d2%utun0 prefixlen 64 scopeid 0xd nd6 options=201&lt;PERFORMNUD,DAD&gt;vmnet1: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500 ether 00:50:56:c0:00:01 inet 172.16.63.1 netmask 0xffffff00 broadcast 172.16.63.255vmnet8: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500 ether 00:50:56:c0:00:08 inet 192.168.170.1 netmask 0xffffff00 broadcast 192.168.170.255en4: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500 options=3&lt;RXCSUM,TXCSUM&gt; ether 00:0e:c6:cc:ae:d7 inet6 fe80::1c60:7fe2:3947:4ec0%en4 prefixlen 64 secured scopeid 0x11 inet 192.168.63.148 netmask 0xffffff00 broadcast 192.168.63.255 nd6 options=201&lt;PERFORMNUD,DAD&gt; media: autoselect (100baseTX &lt;full-duplex,flow-control&gt;) status: active 找到最后的vmnet8, 其中的 inet 就是虚拟主机的网段, 配置虚拟主机的静态ip的时候, 就配置此网段内的. 广播地址 broadcast也是虚拟主机NAT的广播地址, 配置时可以不填 123vmnet8: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500 ether 00:50:56:c0:00:08 inet 192.168.170.1 netmask 0xffffff00 broadcast 192.168.170.255 修改虚拟机网卡配置vi /etc/sysconfig/network-scripts/ifcfg-eth0, 做如下配置 123456789101112DEVICE=eth0HWADDR=00:0C:29:D6:C7:0ETYPE=EthernetUUID=bebc1b63-4f20-405a-860a-32d0d8211582ONBOOT=yesNM_CONTROLLED=yesBOOTPROTO=static # ip类型IPADDR=192.168.170.6 # ip地址, 与 vmnet8 在同一网段NETMASK=255.255.255.0 # 子网掩码GATEWAY=192.168.170.2 # 网关, 跟 ip在同一网段DNS1=192.168.170.2 # 与 ip 同一网段 DNS2=8.8.8.8 # google的 dns 重启网络服务(这里重启也没啥用，等后面hostname改完了，重启一下) service network restart 2. 复制原本的虚拟主机 复制虚拟主机1到2 按照原虚拟主机的root用户名&amp;密码登录2 3. 修改新机网卡 修改网卡vi /etc/udev/rules.d/70-persistent-net.rules, 删除eth0所在的整个段落, 把下面的eth1改为eth0, 保存退出 修改网卡配置vi /etc/sysconfig/network-scripts/ifcfg-eth0, 如果有UUID, HARDDR, 删掉, IPADDR改为与原虚拟主机不同的地址, 但要在同一网段, BOOTPROTO改为static4. 修改新机hostname ​ 修改hostname, vi /etc/sysconfig/network, 把HOSTNAME改为要修改的名字 5.到这里配置完了，重启一下reboot 6. SSH免密登录原理 注意点：如果遇到需要添加新机器之类的，最好是先把 .ssh文件删掉，然后所有的机器一起操作看，全部都重新生成ssh-keygen， 全部重新发送 ssh-copy-id ，这样最简单，不易出错 在mac端，传ssh-copy-id时候，要制定用户名 ssh-copy-id ap@cs1；类似这样 新机生成自己公钥 ssh-keygen, 注意: 如果原本主机中已经生成, 此处在提示verwrite (y/n)?的时候要选择y, 才会重新生成覆盖 把新主机公钥发送给其它机器 ssh-copy-id root@xxx(其它主机ip), 此命令相当于 下面2条命令的效果 12cat id_rsa.pub &gt; authorized_keysscp -r authorized_keys root@192.168.123.202:/root/.ssh/ 其他主机也把公钥发给新主机, 此时就可以实现主机间的免密登录了. 7. 功能增强(可选) 可以在 每台机器中设置host别名, vi /etc/hosts, 加上xxx.xxx.xx.xxx cts1/2/3..., 这样在访问其他主机时, 可以直接用别名替代域名 如果是用的zsh的shell, 可以在所有主机的 &lt;sub&gt;.zshrc中, 添加alias login1=&#39;ssh root@cts1&#39; &gt;&gt; &lt;/sub&gt;/.zshrc, 这样可以直接 用 login1登录到对应的主机. 9. 安装, 使用 zsh &amp; oh-my-zsh &amp;相关插件主骨架安装&amp;介绍 安装zsh套件 1yum install zsh -y 安装 oh-my-zsh套件 1sh -c "$(wget https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)" zsh 的一些基本配置操作, 常用插件安装见这里! 12345678910111213141516171819202122# 查看oh-my-zsh 的主题ls ~/.oh-my-zsh/themes------#这里都是 .zshrc中的配置------# 可以更改为random, 这样会随机显示, 很有乐趣, 直接输入zsh也会切换vi ~/.zshrcZSH_THEME="random"# 添加plugin, 按照对应方式安装plugins=(git ... ... )# 添加alias 到 ~/.zshrcalias vi='vim'alias zshconfig='vi ~/.zshrc'alias vimconfig='vi ~/.vimrc'-----------------------# 设置当前用户使用zsh为默认的shellchsh -s /bin/zsh# 卸载 oh-my-zshuninstall_oh_my_zsh zsh zsh 的一些骚气操作 12345671. 兼容 bash, 这个就不用说了2. 输入某条命令, 比如 cat, 然后用上下键, 可以翻阅所有执行过的命令3. 各种补全, 输入任何命令, 按 2下 tab键, 下面会出现所有可能的补全, 可以 tab, 或 上下左右切换.4. 比如要杀掉进程java, 原来是需要 ps aux | grep java, 查进程的 PID，然后 kill PID; 现在只需要 kill java, 然后按下 tab, java会被替换为 对应的 PID, 点回车, kill !5. 目录浏览和跳转, 输入 d, 可以列出在这个回话中访问过的目录列表, 再输入列表前的序号, 即可直接跳转.6. 在当前目录下输入 .. 或 ... , 或者直接输入目录名, 都可以直接跳转, 甚至都不需要使用 cd命令了.7. 通配符搜索：ls -l */.sh，可以递归显示当前目录下的 shell 文件，文件少时可以代替 find，文件太多就歇菜了. zsh主题介绍, zsh插件介绍 zsh常用插件安装安装 zsh-autosuggestions1234567891011121314151617181920方式1: # 下载到本地git clone git://github.com/zsh-users/zsh-autosuggestions ~/.zsh/zsh-autosuggestions# 添加到.zshrc, 这样就不用每次source了添加 source ~/.zsh/zsh-autosuggestions/zsh-autosuggestions.zsh 到 .zshrc 尾部plugins=() 中添加上 zsh-autosuggestions, 用空格隔开即可======================方式2: 官方建议# 少了添加source到 ~/.zshrc这一步, 猜想是会按照默认的路径加载? ==&gt; 是的, 是可行的, 建议这个# 猜想, oh-my-zsh会自动对安装到 &lt;sub&gt;/.oh-my-zsh/custom&#125;/plugins/ 此路径下的插件source, 就不需要手动在 &lt;/sub&gt;/.zshrc中添加source了, 其它的插件就先不折腾了, 以后有机会再试试# 1.Clone this repository into $ZSH_CUSTOM/plugins (by default ~/.oh-my-zsh/custom/plugins)git clone https://github.com/zsh-users/zsh-autosuggestions $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/zsh-autosuggestions# 2.Add the plugin to the list of plugins for Oh My Zsh to load:plugins=(zsh-autosuggestions)# 3.Start a new terminal session. 安装 autojump123456789101112131415161718# 下载到本地git clone git://github.com/joelthelion/autojump.git# 执行安装脚本cd autojump./install.py# 安装完成在~/下面有.autojump目录, 在.zshrc中加一句[[ -s &lt;sub&gt;/.autojump/etc/profile.d/autojump.sh ]] &amp;&amp; . &lt;/sub&gt;/.autojump/etc/profile.d/autojump.sh# 在plugins=(git zsh-autosuggestions autojump) 加上autojump, 与前者用空格隔开PS: 添加一条快捷键：j -a s '/Users/XXX/Desktop/code/shark’这句代码的含义：j -a 你定义的快捷命令 ‘需要跳转的目录位置’此后要是想进入shark目录，除了传统的cd一级一级的进入，还可以直接使用命令：j s # 接下来可以愉悦的使用 j 了 安装 zsh-syntax-highlighting 123456789101112131415# 到 ~/.zshrc 目录, 克隆仓库# 这里默认的是主目录, 当然可以下载到其它目录# ps: 在那个目录, git clone就会下载到哪个目录git clone https://github.com/zsh-users/zsh-syntax-highlighting.git# source the script 到 ~/.zshrc# 这个在哪个目录下echo的, 就会把当前目录拼到前面?? 结果好像是这样echo "source $&#123;(q-)PWD&#125;/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh" &gt;&gt; $&#123;ZDOTDIR:-$HOME&#125;/.zshrc# 在plugins=(git zsh-autosuggestions autojump zsh-syntax-highlighting) zsh-syntax-highlighting, 与前者用空格隔开# 其实这里不加好像也没事, 还是加上为好# 在当前 shell生效# 看这个文件在哪个目录, 在哪个目录就source 哪个目录, 立即生效source ./zsh-syntax-highlighting/zsh-syntax-highlighting.zsh]]></content>
      <categories>
        <category>Linux</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iTerm2上使用 lrzsz黑科技 上传下载文件到远端]]></title>
    <url>%2F2018%2F05%2F23%2FTools%2FiTerm2%2FiTerm2%E4%B8%8A%E4%BD%BF%E7%94%A8lrzsz%E4%B8%8A%E4%BC%A0%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[ZModem integration for iTerm 2This script can be used to automate ZModem transfers from your OSX desktop to a server that can run lrzsz (in theory, any machinethat supports SSH), and vice-versa. The minimum supported iTerm2 version is 1.0.0.20120108 Troubleshooting Sending a directory may fail: this is a known issue If you are using tmux or some other terminal multiplexer (ie: screen), try using the -e option to sz and/or rz on your server to force escaping of more characters during transmission. This tool may also fail if you are using expect or rlogin as it expects a mostly-clean 8-bit connection between the two parties. Setup Install lrzsz on OSX: brew install lrzsz Save the iterm2-send-zmodem.sh and iterm2-recv-zmodem.sh scripts in /usr/local/bin/ Set up Triggers in iTerm 2 like so: Regular expression: rz waiting to receive.\*\*B0100 Action: Run Silent Coprocess Parameters: /usr/local/bin/iterm2-send-zmodem.sh Instant: checked Regular expression: \*\*B00000000000000 Action: Run Silent Coprocess Parameters: /usr/local/bin/iterm2-recv-zmodem.sh Instant: checked To send a file to a remote machine: Type rz on the remote machine Select the file(s) on the local machine to send Wait for the coprocess indicator to disappear The receive a file from a remote machine Type sz filename1 filename2 … filenameN on the remote machine Select the folder to receive to on the local machine Wait for the coprocess indicator to disappear Future plans (patches welcome) Visual progress bar indicator]]></content>
      <categories>
        <category>工具</category>
        <category>iTerm2</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>工具</tag>
        <tag>iTerm2</tag>
      </tags>
  </entry>
</search>
