<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hive学习——2]]></title>
    <url>%2F2018%2F06%2F13%2FHadoop%2FStudy%2F3-Hive%2FHive%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%942%2F</url>
    <content type="text"><![CDATA[1) Hive 初探Hive 的数据存储 Hive的数据存储基于Hadoop HDFS Hive没有专门的数据存储格式 存储结构主要包括：数据库、文件、表、视图、索引 Hive默认可以直接加载文本文件（TextFile），还支持SequenceFile、RCFile 创建表时，指定Hive数据的列分隔符与行分隔符，Hive即可解析数据 Hive的系统架构 用户接口，包括 CLI，JDBC/ODBC，WebUI 元数据存储，通常是存储在关系数据库如 mysql, derby 中 解释器、编译器、优化器、执行器 Hadoop：用 HDFS 进行存储，利用 MapReduce 进行计算 Hive的系统架构 用户接口主要有三个：CLI，JDBC/ODBC和 WebUI CLI，即Shell命令行 JDBC/ODBC 是 Hive 的Java，与使用传统数据库JDBC的方式类似 WebGUI是通过浏览器访问 Hive Hive 将元数据存储在数据库中(metastore)，目前只支持 mysql、derby。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等 解释器、编译器、优化器完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划（plan）的生成。生成的查询计划存储在 HDFS 中，并在随后由 MapReduce 调用执行 Hive 的数据存储在 HDFS 中，大部分的查询由 MapReduce 完成（包含 的查询，比如 select from table 不会生成 MapRedcue 任务 Hive的metastore metastore是hive元数据的集中存放地。 metastore默认使用内嵌的derby数据库作为存储引擎 Derby引擎的缺点：一次只能打开一个会话 使用Mysql作为外置存储引擎，多用户同时访问 Hive 和 Hadoop 的调用关系 1、提交sql 交给驱动2、驱动编译 解析相关的字段表信息3、去metastore查询相关的信息 返回字段表信息4、编译返回信息 发给驱动5、驱动发送一个执行计划 交给执行引擎6.1、DDLs 对数据库表的操作的, 直接和metastore交互,create table t1(name string); 6.2、完成job返回数据信息、找namenode查数据6.3、namenode交互select count(1) from t1;7、返回结果信息集 Hive 参数配置使用 命名空间 使用权限 描述 hivevar 可读写 $ hive -d name=zhangsan; hiveconf 可读写 \$ hive –hiveconf hive.cli.print.current.db=true; $ hive –hiveconf hive.cli.print.header=true; system 可读写 java定义的配置属性，如system:user.name env 只读 shell环境变量，如env:USER hivevar 1234567# 使用场景: 起别名hive -d name=zhangsan #传参&gt; create table t2(name string,$&#123;name&#125; string); #取参数&gt; desc t2;---name stringzhangsan string hiveconf : 12345678910# 显示当前数据库名称[ap@cs2]~% hive --hiveconf hive.cli.print.current.db=true;hive (default)&gt; create database mydb;hive (default)&gt; use mydb;hive (mydb)&gt; # 显示表头(字段名)hive --hiveconf hive.cli.print.header=true;select * from t2;t2.name t2.zhangsan Hive 的脚本执行 Hive -e “xx ” e 就是 edit, 在终端打印输出 Hive -e “show tables” &gt;&gt; a.txt 可以把执行结果重定向到文件中 Hive -S -e “show tables” &gt;&gt; a.txt -S : silence 安静的执行 hive -f file hive -f hql , hql 是文件, 执行文件 执行完了之后, 就离开 hive 命令行 hive -i /home/ap/hive-init.sql 执行完了,还在控制台, 可以继续操作 hive&gt;source file source + 文件名 : 直接执行当前目录文件 hive与依赖环境的交互 与linux交互命令 ！ !ls !pwd 与hdfs交互命令 dfs -ls / dfs -mkdir /hive hive (default)&gt; dfs -rm -r /user/hive/warehouse/t5; Hive 的 JDBC 模式 JAVA API交互执行方式 hive 远程服务 (端口号1000 0) 启动方式 #hive –service hiveserver2 org.apache.hive.jdbc.HiveDriver 在java代码中调用hive的JDBC建立连接 可以用 beeline 连接 set命令使用 Hive 控制台set 命令 set; set -v; 显示所有的环境变量 set hive.cli.print.current.db=true; set hive.cli.print.header=true; set hive.metastore.warehouse.dir=/hive; hive参数初始化配置set命令: ~/.hiverc 创建此文件, 在此文件中配置初始化命令 补充：hive历史操作命令集~/.hivehistory 2) Hive数据类型a) 基本数据类型 b) 复合数据类型 创建学生表hive&gt;CREATE TABLE student(id INT,name STRING,favors ARRAY\,scores MAP&lt;STRING, FLOAT&gt;); 默认分隔符 描述 语句 \n 分隔行 LINES TERMINATED BY ‘\t’ ^A 分隔字段(列)，显示编码使用\001 FIELDS TERMINATED BY ‘\001’ ^B 分隔复合类型中的元素，显示编码使用\002 COLLECTION ITEMS TERMINATED BY ‘\002’ ^C 分隔map元素的key和value，显示编码使用\003 MAP KEYS TERMINATED BY ‘\003’ a. Struct 使用Structs内部的数据可以通过DOT（.）来存取，例如，表中一列c的类型为STRUCT{a INT; b INT}，我们可以通过c.a来访问域a 123456789101112131415161718192021222324252627282930# 数据1001,zhangsan:241002,lisi:281003,wangwu:25# 1.创建表hive&gt; create table student_test(id INT, info struct&lt;name:STRING, age:INT&gt;)ROW FORMAT DELIMITED FIELDS TERMINATED BY ','COLLECTION ITEMS TERMINATED BY ':';# 2.加载表hive&gt; load data local inpath "student_test" into table student_test;# 3.顺便设置 显示表头,和当前数据库hive&gt; set hive.cli.print.header=true;hive&gt; set hive.cli.print.current.db=true;# 4. 展示所有的hive (default)&gt; select * from student_test;---student_test.id student_test.info1001 &#123;"name":"zhangsan","age":24&#125;1002 &#123;"name":"lisi","age":28&#125;1003 &#123;"name":"wangwu","age":25&#125;---# Struct -结构体-使用 . hive (default)&gt; select id,info.name,info.age from student_test;id name age1001 zhangsan 241002 lisi 281003 wangwu 25 b. Array 使用Array中的数据为相同类型，例如，假如array A中元素[‘a’,’b’,’c’]，则A[1]的值为’b’ 123456789101112131415161718192021222324# 原始数据zhangsan,24:25:27:37lisi,28:39:23:43wangwu,25:23:02:54# 创建表hive (default)&gt; create table class_test(name string,student_id_list array&lt;int&gt;) row format delimited fields terminated by ',' collection items terminated by ':';# 加载表hive (default)&gt; load data local inpath "class_test" into table class_test;# 查看表hive (default)&gt; select * from class_test;OKclass_test.name class_test.student_id_listzhangsan [24,25,27,37]lisi [28,39,23,43]wangwu [25,23,2,54]# 查看数据中某个元素hive (default)&gt; select name, student_id_list[0] from class_test where name='zhangsan';OKname _c1zhangsan 24 c. Map 使用访问指定域可以通过[“指定域名称”]进行，例如，一个Map M包含了一个group-&gt;gid的kv对，gid的值可以通过M[‘group’]来获取 1234567891011121314151617181920212223242526272829303132333435# 原始数据1001 job:80,team:60,person:701002 job:60,team:80,person:801003 job:90,team:70,person:100# 创建表hive (default)&gt; create table employee(id string,perf map&lt;string,int&gt;) row format delimited fields terminated by '\t' collection items terminated by ',' map keys terminated by ':';# 导入hive (default)&gt; load data local inpath "employee_data" into table employee;# 查看hive (default)&gt; select * from employee;---employee.id employee.perf1001 &#123;"job":80,"team":60,"person":70&#125;1002 &#123;"job":60,"team":80,"person":80&#125;1003 &#123;"job":90,"team":70,"person":100&#125;Time taken: 0.228 seconds, Fetched: 3 row(s)# 查看单个hive (default)&gt; select id,perf['job'],perf['team'],perf['person'] from employee;OKid _c1 _c2 _c31001 80 60 701002 60 80 801003 90 70 100# 显示别名hive (default)&gt; select id,perf['job'] as job,perf['team'] as team,perf['person'] as person from employee;OKid job team person1001 80 60 701002 60 80 801003 90 70 100 3) 数据定义a). 数据库定义 默认数据库”default” 使用某个数据库 use &lt;数据库名&gt; 创建一个新库 CREATE DATABASE [IF NOT EXISTS] mydb[LOCATION] ‘/…….’[COMMENT] ‘….’; hive&gt;SHOW DATABASES; hive&gt;DESCRIBE DATABASE [extended] mydb; hive&gt;DROP DATABASE [IF EXISTS] mydb [CASCADE]; 创建 create database db1; 删除 drop database if exists db1; 级联 drop database if exists db1 cascade;]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[王石松-hadoop测试-1]]></title>
    <url>%2F2018%2F06%2F12%2FHadoop%2FTest%2F%E7%8E%8B%E7%9F%B3%E6%9D%BEhadoop%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[第一题：简答题 （30分）1、简述hdfs的读写流程读流程： 使用 HDFS 提供的客户端 Client，向远程的 namenode 发起 RPC 请求; namenode 会视情况返回文件的全部 block 列表，对于每个 block，namenode 都会返回有 该 block 拷贝的 datanode 地址; 客户端 Client 会选取离客户端最近的 datanode 来读取 block;如果客户端本身就是 datanode， 那么将从本地直接获取数据; 读取完当前 block 的数据后，关闭当前的 datanode 链接，并为读取下一个 block 寻找最 佳的 datanode; 当读完列表 block 后，且文件读取还没有结束，客户端会继续向 namenode 获取下一批的 block 列表; 读取完一个 block 都会进行 checksum 验证，如果读取 datanode 时出现错误，客户端会 通知 namenode，然后再从下一个拥有该 block 拷贝的 datanode 继续读。 写流程： 使用 HDFS 提供的客户端 Client，向远程的 namenode 发起 RPC 请求 namenode 会检查要创建的文件是否已经存在，创建者是否有权限进行操作，成功则会为文件创建一个记录，否则会让客户端抛出异常; 当客户端开始写入文件的时候，客户端会将文件切分成多个 packets，并在内部以数据队列“data queue(数据队列)”的形式管理这些 packets，并向 namenode 申请 blocks，获 取用来存储 replicas 的合适的 datanode 列表，列表的大小根据 namenode 中 replication 的设定而定; 开始以 pipeline(管道)的形式将 packet 写入所有的 replicas 中。客户端把 packet 以流的 方式写入第一个 datanode，该 datanode 把该 packet 存储之后，再将其传递给在此 pipeline 中的下一个 datanode，直到最后一个 datanode，这种写数据的方式呈流水线的形式 最后一个 datanode 成功存储之后会返回一个 ack packet(确认队列)，在 pipeline 里传递 至客户端，在客户端的开发库内部维护着”ack queue”，成功收到 datanode 返回的 ack packet 后会从”data queue”移除相应的 packet。 如果传输过程中，有某个 datanode 出现了故障，那么当前的 pipeline 会被关闭，出现故 障的 datanode 会从当前的 pipeline 中移除，剩余的 block 会继续剩下的 datanode 中继续 以 pipeline 的形式传输，同时 namenode 会分配一个新的 datanode，保持 replicas 设定的 数量。 客户端完成数据的写入后，会对数据流调用 close()方法，关闭数据流; 只要写入了 dfs.replication.min(最小写入成功的副本数)的复本数(默认为 1)，写操作 就会成功，并且这个块可以在集群中异步复制，直到达到其目标复本数(dfs.replication 的默认值为 3)，因为 namenode 已经知道文件由哪些块组成，所以它在返回成功前只需 要等待数据块进行最小量的复制。 2、详细写出mapreduce的shuffle过程 20分 Map: mapTask 收集我们的 map()方法输出的 kv 对，放到内存缓冲区 kvbuffer(环形缓冲区:内 存中的一种首尾相连的数据结构，kvbuffer 包含数据区和索引区)中 从内存缓冲区中的数据区的数据不断溢出本地磁盘文件 file.out，可能会溢出多次，则会 有多个文件，相应的内存缓冲区中的索引区数据溢出为磁盘索引文件 file.out.index 多个溢出文件会被合并成大的溢出文件 在溢出过程中，及合并的过程中，都要调用 partitoner 进行分区和针对 key 进行排序 Reduce: reduceTask 根据自己的分区号，去各个 mapTask 机器上取相应的结果分区数据 reduceTask 会取到同一个分区的来自不同 mapTask 的结果文件，reduceTask 会将这些文 件再进行合并(归并排序) 合并成大文件后，shuffle 的过程也就结束了，后面进入 reduceTask 的逻辑运算过程(从 文件中取出一个一个的键值对 group，调用用户自定义的 reduce()方法) 第二题：编程题 （70分）第一题、HDFS题综合练习（10分） 编写程序统计出HDFS文件系统中的平均数据块数（数据块总数/文件总数） 比如：一个文件有5个块，一个文件有3个块，那么平均数据块数为4 如果还有一个文件，并且数据块就1个，那么整个HDFS的平均数据块数就是3 代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.rox.mapreduce.test._01;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.LocatedFileStatus;import org.apache.hadoop.fs.Path;import org.apache.hadoop.fs.RemoteIterator;/** * @author shixuanji * 编写程序统计出HDFS文件系统中的平均数据块数（数据块总数/文件总数）比如：一个文件有5个块，一个文件有3个块，那么平均数据块数为4如果还有一个文件，并且数据块就1个，那么整个HDFS的平均数据块数就是3 */public class BlockCount &#123; public static void main(String[] args) throws Exception &#123; Configuration configuration = new Configuration(); configuration.set("fs.defaultFS", "hdfs://cs1:9000"); System.setProperty("HADOOP_USER_NAME", "ap"); FileSystem fSystem = FileSystem.get(configuration); // 数据块总数 int sumBlockNum = 0; // 副本总数 int sumReplicationNum = 0; // 递归获取HDFS上所有文件 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fSystem.listFiles(new Path("/"), true); while (listFiles.hasNext()) &#123; LocatedFileStatus next = listFiles.next(); // 所有的的block块数目 int currentBlockNum = next.getBlockLocations().length; sumBlockNum += currentBlockNum; //计算所有的副本数 副本总数=当全前副本数*block块数 sumReplicationNum += next.getReplication() * currentBlockNum; &#125; float avgReplicationNum = (float)sumReplicationNum / sumBlockNum; System.out.println("当前平均块数为: "+avgReplicationNum); &#125;&#125;---运行结果------2018-06-12 19:23:10,696 WARN [main] util.NativeCodeLoader (NativeCodeLoader.java:&lt;clinit&gt;(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable当前平均块数为: 3.0 第二题、MapReduce题–基础复习（40分 = 20 分 + 20分）下面是三种商品的销售数据 要求：根据以上数据，用MapReduce统计出如下数据： 1、每种商品的销售总金额，并降序排序 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.rox.mapreduce.test._02;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.WritableComparable;import lombok.AllArgsConstructor;import lombok.Getter;import lombok.NoArgsConstructor;import lombok.Setter;@Setter@Getter@AllArgsConstructor@NoArgsConstructorpublic class ProductBean implements WritableComparable&lt;ProductBean&gt; &#123; private String proId; private Double sales; @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(proId); out.writeDouble(sales); &#125; @Override public void readFields(DataInput in) throws IOException &#123; this.proId = in.readUTF(); this.sales = in.readDouble(); &#125; @Override public int compareTo(ProductBean o) &#123; return o.getSales().compareTo(this.getSales()); &#125; @Override public String toString() &#123; return proId + "\t" + sales; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152package com.rox.mapreduce.test._02;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.DoubleWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob;import org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class Question2_1 &#123; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://cs1:9000"); System.setProperty("HADOOP_USER_NAME", "ap"); Job job = Job.getInstance(conf); job.setJarByClass(Question2_1.class); job.setMapperClass(MyMapper.class); job.setReducerClass(MyReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(DoubleWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(DoubleWritable.class); String inP = "/in/productsales"; String outP = "/out/Question2_1"; FileInputFormat.setInputPaths(job, new Path(inP)); FileOutputFormat.setOutputPath(job, new Path(outP)); Job job2 = Job.getInstance(conf); job2.setJarByClass(Question2_1.class); job2.setMapperClass(MyMapper2.class); job2.setReducerClass(MyReducer2.class); job2.setMapOutputKeyClass(ProductBean.class); job2.setMapOutputValueClass(NullWritable.class); job.setOutputKeyClass(ProductBean.class); job.setOutputValueClass(NullWritable.class); String inP2 = "/out/Question2_1"; String outP2 = "/out/Question2_1_2"; FileInputFormat.setInputPaths(job2, new Path(inP2)); FileOutputFormat.setOutputPath(job2, new Path(outP2)); JobControl control = new JobControl("jc"); ControlledJob conjob1 = new ControlledJob(job.getConfiguration()); ControlledJob conjob2 = new ControlledJob(job2.getConfiguration()); conjob2.addDependingJob(conjob1); control.addJob(conjob1); control.addJob(conjob2); Thread t = new Thread(control); t.start(); while (!control.allFinished()) &#123; Thread.sleep(1000); &#125; System.exit(0); &#125; static class MyMapper extends Mapper&lt;LongWritable, Text, Text, DoubleWritable&gt; &#123; Text k = new Text(); DoubleWritable v = new DoubleWritable(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] line = value.toString().split("\t"); //根据商品ID作为Key这样在reduce阶段我们就能对每一种商品的销售额进行累加 k.set(line[1]); //根据数据格式，求出每条记录的销售总金额 double sum = Double.parseDouble(line[2]) * Double.parseDouble(line[3]); v.set(sum); context.write(k, v); &#125; &#125; static class MyReducer extends Reducer&lt;Text, DoubleWritable, Text, DoubleWritable&gt; &#123; DoubleWritable v = new DoubleWritable(); @Override protected void reduce(Text key, Iterable&lt;DoubleWritable&gt; values, Context context) throws IOException, InterruptedException &#123; double sum = 0D; //相同的商品会在一个reduceTask中进行处理， //这里能够对每一种商品的销售额进行累加 for (DoubleWritable num : values) &#123; sum += num.get(); &#125; v.set(sum); context.write(key, v); &#125; &#125; /** * 对商品进行排序的时候这里选择使用自定义数据类型的方式进行 * 使用compareTo方法对数据进行排序 */ static class MyMapper2 extends Mapper&lt;LongWritable, Text, ProductBean, NullWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] line = value.toString().split("\t"); ProductBean p = new ProductBean(line[0], Double.parseDouble(line[1])); context.write(p, NullWritable.get()); &#125; &#125; static class MyReducer2 extends Reducer&lt;ProductBean, NullWritable, ProductBean, NullWritable&gt; &#123; @Override protected void reduce(ProductBean key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; context.write(key, NullWritable.get()); &#125; &#125;&#125; 执行结果 B0001 1681.0A0003 1217.5A0001 753.8A0002 738.5 2、每种商品销售额最多的三周思路： 按商品名分组 自定义ProductBean, 实现WritableComparable, compareTo()方法中实现 商品名相同的，按照销售额降序排序 商品名不同的，按照商品名降序排序 自定义MyGroup，继承WritableComparator, 重写compare()中实现 比价商品编号， 按照商品编号分组 在job中，启用自定义分组类 job.setGroupingComparatorClass(MyGroup.class); Mapper Mapper&lt;LongWritable, Text, ProductBean, NullWritable&gt; Reducer Reducer&lt;ProductBean, NullWritable, ProductBean, NullWritable&gt; Reducer中，写出前三次接收到的ProductBean 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.rox.mapreduce.test._02_2;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.WritableComparable;import lombok.AllArgsConstructor;import lombok.Getter;import lombok.NoArgsConstructor;import lombok.Setter;@Setter@Getter@AllArgsConstructor@NoArgsConstructorpublic class ProductBean implements WritableComparable&lt;ProductBean&gt; &#123; private String proId; private Double sales; private String week; @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(proId); out.writeDouble(sales); out.writeUTF(week); &#125; @Override public void readFields(DataInput in) throws IOException &#123; this.proId = in.readUTF(); this.sales = in.readDouble(); this.week = in.readUTF(); &#125; @Override public int compareTo(ProductBean o) &#123; // 如果商品编号相同 if (this.getProId().compareTo(o.getProId()) == 0) &#123; // 就按照价格降序排序 return o.getSales().compareTo( this.getSales()); &#125; // 否则按照商品编号排序 return o.getProId().compareTo(this.getProId()); &#125; @Override public String toString() &#123; return proId + "\t" + sales + "\t" + week; &#125;&#125; 1234567891011121314151617181920package com.rox.mapreduce.test._02_2;import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;public class MyGroup extends WritableComparator &#123; public MyGroup() &#123; super(ProductBean.class,true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; ProductBean aBean = (ProductBean)a; ProductBean bBean = (ProductBean)b; // 根据商品编号分组 return aBean.getProId().compareTo(bBean.getProId()); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102package com.rox.mapreduce.test._02_2;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class Question2_2 &#123; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://cs1:9000"); System.setProperty("HADOOP_USER_NAME", "ap"); Job job = Job.getInstance(conf); job.setJarByClass(Question2_2.class); job.setMapperClass(MyMapper.class); job.setReducerClass(MyReducer.class); job.setMapOutputKeyClass(ProductBean.class); job.setMapOutputValueClass(NullWritable.class); // 设置分组组件 job.setGroupingComparatorClass(MyGroup.class); job.setOutputKeyClass(ProductBean.class); job.setOutputValueClass(NullWritable.class); String inP = "/in/productsales"; String outP = "/out/Question2_2"; FileInputFormat.setInputPaths(job, new Path(inP)); FileOutputFormat.setOutputPath(job, new Path(outP)); FileSystem fs = FileSystem.get(conf); Path out = new Path(outP); if (fs.exists(out)) &#123; fs.delete(out,true); &#125; System.exit(job.waitForCompletion(true)?0:-1); &#125; /** * @author shixuanji * 商品名分组 * 商品当周销售额降序排序 * 取出对应的三周 */ static class MyMapper extends Mapper&lt;LongWritable, Text, ProductBean, NullWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, ProductBean, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; String[] datas = value.toString().trim().split("\t"); Double sales = Double.parseDouble(datas[2]) * Double.parseDouble(datas[3]); ProductBean pBean = new ProductBean(datas[1], sales, datas[0]); context.write(pBean, NullWritable.get()); &#125; &#125; static class MyReducer extends Reducer&lt;ProductBean, NullWritable, ProductBean, NullWritable&gt; &#123; @Override protected void reduce(ProductBean key, Iterable&lt;NullWritable&gt; values, Reducer&lt;ProductBean, NullWritable, ProductBean, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; int count = 0; for (NullWritable nullWritable : values) &#123; if (count &lt; 3) &#123; context.write(key, nullWritable); &#125; count ++; &#125; &#125; &#125;&#125;------运行结果--- B0001 550.0 week2B0001 525.0 week1B0001 522.0 week3A0003 463.50000000000006 week4A0003 294.0 week3A0003 276.0 week1A0002 264.0 week3A0002 187.0 week4A0002 160.0 week2A0001 330.0 week2A0001 200.0 week1A0001 128.79999999999998 week4 第三题：MapReduce题–倒排索引（ 20分）:unamused:]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Test</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>Test</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce-简单总结]]></title>
    <url>%2F2018%2F06%2F12%2FHadoop%2FStudy%2F2-MapReduce%2FMapReduce-%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[MapReduce在编程的时候，基本上一个固化的模式，没有太多可灵活改变的地方，除了以下几处： 1、输入数据接口：InputFormat —&gt; FileInputFormat(文件类型数据读取的通用抽象类) DBInputFormat （数据库数据读取的通用抽象类） 默认使用的实现类是： TextInputFormat job.setInputFormatClass(TextInputFormat.class) TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回 2、逻辑处理接口： Mapper 完全需要用户自己去实现其中 map() setup() clean() 3、map输出的结果在shuffle阶段会被partition以及sort，此处有两个接口可自定义： Partitioner 有默认实现 HashPartitioner，逻辑是 根据key和numReduces来返回一个分区号； key.hashCode()&amp;Integer.MAXVALUE % numReduces 通常情况下，用默认的这个HashPartitioner就可以，如果业务上有特别的需求，可以自定义 Comparable 当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，override其中的compareTo()方法 4、reduce端的数据分组比较接口 ： Groupingcomparator reduceTask拿到输入数据（一个partition的所有数据）后，首先需要对数据进行分组，其分组的默认原则是key相同，然后对每一组kv数据调用一次reduce()方法，并且将这一组kv中的第一个kv的key作为参数传给reduce的key，将这一组数据的value的迭代器传给reduce()的values参数 利用上述这个机制，我们可以实现一个高效的分组取最大值的逻辑： 自定义一个bean对象用来封装我们的数据，然后改写其compareTo方法产生倒序排序的效果 然后自定义一个Groupingcomparator，将bean对象的分组逻辑改成按照我们的业务分组id来分组（比如订单号） 这样，我们要取的最大值就是reduce()方法中传进来key 5、逻辑处理接口：Reducer 完全需要用户自己去实现其中 reduce() setup() clean() 6、输出数据接口： OutputFormat —&gt; 有一系列子类 FileOutputformat DBoutputFormat ….. 默认实现类是TextOutputFormat，功能逻辑是： 将每一个KV对向目标文本文件中输出为一行]]></content>
      <categories>
        <category>Hadoop</category>
        <category>MapReduce</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive学习——1]]></title>
    <url>%2F2018%2F06%2F12%2FHadoop%2FStudy%2F3-Hive%2FHive%E2%80%94%E2%80%941%2F</url>
    <content type="text"><![CDATA[Hive简介 Hive基本情况Hive 是建立在 Hadoop 上的数据仓库基础构架 由facebook开源，最初用于解决海量结构化的日志数据统 计问题; ETL(Extraction-Transformation-Loading)工具 构建在Hadoop之上的数据仓库; 数据计算使用MR，数据存储使用HDFS 数据库&amp;数据仓库 的区别： 概念上 数据库：用于管理精细化数据，一般情况下用于存储结果数据，分库分表进行存储 数据仓库：存储、查询、分析大规模数据 。更像一个打包的过程，里面存储的数据没有细化区分，粒度较粗 用途上： 数据库：OLTP，on line Transation Processing 联机事务处理，增删改 数据仓库：OLAP，on line analysis Processing 联机事务分析处理，查询，hive不支持删除、修改。 支持插入。 使用上： 数据库：标准sql, hbase: 非标准sql 数据仓库：方言版的sql， HQL 模式上： 数据库：写模式 数据仓库：读模式 可以将结构化的数据映射成一张数据库表 结构化数据映射成二维表 将文本中每一行数据映射为数据库的每一条数据 将文本中每一列数据映射为hive的表字段 提供HQL 查询功能 hive query language， 方言版sql 底层数据是存储在HDFS上 hive上建的表仅仅相当于对hdfs上的结构化数据进行映射管理 hive仅仅是一个管理数据的作用，而不会存储数据 hive想要管理hdfs上的数据，就要建立一个关联关系，关联hive上的表和hdfs上的数据路径 数据是依赖于一个元数据库 元数据库采用的是关系型数据库， 真实生产中一般使用mysql为hive的元数据库，hive内置默认的元数据库是 derby 元数据： HCalalog hive中的表和hdfs的映射关系，以及hive表属性（内部表，外部表，视图）和字段信息 元数据一旦修饰，hive的所有映射关系等都没了，就无法使用了 可与Pig、Presto等共享 Hive的是构建在Hadoop之上的数据仓库 数据计算使用MR，数据存储使用HDFS 通常用于进行离线数据处理(采用MapReduce) 可认为是一个HQL—-&gt;MR的语言翻译器 Hive优缺点： 优点： 简单，容易上手 提供了类SQL查询语言HQL 为超大数据集设计的计算/扩展能力 MR作为计算引擎，HDFS作为存储系统 统一的元数据管理(HCalalog) 可与Pig、Presto等共享 缺点 不支持 删除 &amp; 修改 delete&amp;update，不支持事务 因为是基于HDFS hive做的最多的是查询 Hive的HQL表达的能力有限 迭代式算法无法表达 有些复杂运算用HQL不易表达 Hive效率较低，查询延时高 Hive自动生成MapReduce作业，通常不够智能 HQL调优困难，粒度较粗 可控性差 Hive与传统关系型数据库(RDBMS）对比 Hive的架构 1) 用户接口 CLI：Command Line Interface，即Shell终端命令行，使用 Hive 命令行与 Hive 进行交互，最常用（学习，调试，生产），包括两种运行方式： hive命令方式：前提必须在hive安装节点上执行 hiveserver2方式：hive安装节点将hive启动为一个后台进程，客户机进行连接（类似于启动了一个hive服务端） 真实生产中常用！ - 1) 修改配置文件，允许远程连接; 第一：修改 hdfs-site.xml，加入一条配置信息，启用 webhdfs； 第二：修改 core-site.xml，加入两条配置信息，设置 hadoop的代理用户。 - 2) 启动服务进程 - 前台启动：hiveserver2 - 后台启动 - 记录日志：nohup hiveserver2 1&gt;/home/sigeon/hiveserver.log 2&gt;/home/sigeon/hiveserver.err &amp; 0：标准日志输入 1：标准日志输出 2：错误日志输出 如果我没有配置日志的输出路径，日志会生成在当前工作目录，默认的日志名称叫做： nohup.xxx - 不记录日志：nohup hiveserver2 1&gt;/dev/null 2&gt;/dev/null &amp; - [补充：] - nohup命令：no hang up的缩写，即不挂起，可以在你退出帐户/关闭终端之后继续运行相应的进程。 - 语法：nohup &lt;command&gt; &amp; - 3) 开启beenline客户端并连接： - 方法一： - beenline，开启beenline客户端; - !connect jdbc:&lt;hive2://master:10000&gt;，回车。然后输入用户名和密码，这个用户名是安装 hadoop 集群的用户名 - 方法二： - beeline -u jdbc:&lt;hive2://master:10000&gt; -n sigeon JDBC/ODBC：是 Hive 的基于 JDBC 操作提供的客户端，用户（开发员，运维人员）通过 这连接至 Hive server 服务 Web UI：通过浏览器访问 Hive，基本不会使用 2) 元数据库：保存元数据，一般会选用关系型数据库（如mysql，Hive 和 MySQL 之间通过 MetaStore 服务交互） 3) Thrift服务：Facebook 开发的一个软件框架，可以用来进行可扩展且跨语言的服务的开发， Hive 集成了该服务，能让不同的编程语言调用 Hive 的接口 4) 驱动Driver a. 解释器：解释器的作用是将 HiveSQL 语句转换为抽象语法树（AST） b. 编译器：编译器是将语法树编译为逻辑执行计划 c. 优化器：优化器是对逻辑执行计划进行优化 d. 执行器：执行器是调用底层的运行框架执行逻辑执行计划 Hive的数据组织格式 1)库：database 2) 表 a. 内部表（管理表：managed_table） b. 外部表（external_table） 内部表和外部表区别： 内部表和外部表是两个相对的概念，不可能有一个表同时是内部表又是外部表； 内部表删除表的时候会删除原始数据和元数据，而外部表删除表的时候只会删除元数据不会删除原始数据 一般情况下存储公共数据的表存放为外部表 大多数情况，他们的区别不明显。如果数据的所有处理都在 Hive 中进行，那么倾向于 选择内部表；但是如果 Hive 和其他工具要针对相同的数据集进行处理，外部表更合适。 使用外部表访问存储在 HDFS 上的初始数据，然后通过 Hive 转换数据并存到内部表中，使用外部表的场景是针对一个数据集有多个不同的 Schema。 c. 分区表 不同于hadoop中的分区，分区表是人为划分的 hive最终存储海量数据，海量数据查询一定注意避免全表扫描 查询的时候为了提升我们的查询性能，出现了分区表 将数据按照用户的业务存储到不同的目录下，在进行数据查询时只会对指定分区下的数据进行扫描一般情况下生产中用日期作为分区字段 d. 分桶表 类似于hadoop中的分区，是由程序决定的，只能指定桶的个数（分区的个数） 根据hash算法将余数不同的输出到不同的文件中 作用： 1）提升join的性能思考这个问题：select a.id,a.name,b.addr from a join b on a.id = b.id;如果 a 表和 b 表已经是 分桶表，而且分桶的字段是 id 字段做这个 join 操作时，还需要全表做笛卡尔积 2）提升数据样本的抽取效率，直接拿一个桶中的数据作为样本数据 分区表和分桶表的区别： Hive 数据表可以根据某些字段进行分区操作，细化数据管理，可以让部分查询更快。 同时表和分区也可以进一步被划分为 Buckets，分桶表的原理和 MapReduce 编程中的 HashPartitioner 的原理类似 分区和分桶都是细化数据管理，但是分区表是手动添加区分，由于 Hive 是读模式，所以对添加进分区的数据不做模式校验 分桶表中的数据是按照某些分桶字段进行 hash 散列 形成的多个文件，所以数据的准确性也高很多 3)视图： hive中的视图仅仅相当于一个sql语句的别名 在hive中仅仅存在逻辑视图，不存在物理视图 物理视图：讲sql语句的执行结果存在视图中 逻辑视图： 仅仅是对查询结果的引用 4)数据存储： 原始数据中存在HDFS 元数据存在mysql Hive安装装hive其实不难，主要是安装mysql，解决mysql的权限问题 MySql安装RPM 安装MySQl： 检查以前是否装过 MySQL 1rpm -qa|grep -i mysql 发现有的话就都卸载 12rpm -e --nodeps mysql-libs-5.1.73-5.el6_6.x86_64rpm -e --nodeps .... 删除老版本 mysql 的开发头文件和库 12rm -rf /usr/lib64/mysql# 在搜索 my.cnf 文件，有的话就删掉 上传mysql 安装包到 Linux中，解压 12345678tar -xvf mysql-5.6.26-1.linux_glibc2.5.x86_64.rpm-bundle.tar# 解压出来有这些文件MySQL-devel-5.6.26-1.linux_glibc2.5.x86_64.rpmMySQL-embedded-5.6.26-1.linux_glibc2.5.x86_64.rpmMySQL-server-5.6.26-1.linux_glibc2.5.x86_64.rpmMySQL-shared-5.6.26-1.linux_glibc2.5.x86_64.rpmMySQL-shared-compat-5.6.26-1.linux_glibc2.5.x86_64.rpmMySQL-test-5.6.26-1.linux_glibc2.5.x86_64.rpm 安装 server &amp; client 1234# serverrpm -ivh MySQL-server-5.6.26-1.linux_glibc2.5.x86_64.rpm# clientrpm -ivh MySQL-client-5.6.26-1.linux_glibc2.5.x86_64.rpm 启动Mysql 1sudo service mysql start 登录Mysql并改密码，等 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566# 初始密码在这个文件中cat /root/.mysql_sercert # 登录mysql -uroot -pxxxxx# 删除除了`%`之外的其他所有hostuse mysql;select host,user,password from user;delete from user where host in (&apos;localhost&apos;, &apos;127.0.0.1&apos;,&apos;::1&apos;, ...)# 修改密码 UPDATE user SET Password = PASSWORD(&apos;psd&apos;) WHERE user = &apos;root&apos;;# 为`%` 和 `*` 添加远程登录权限 #注意： 前面的 mysql 登录用户名， 123 是登录密码GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;psd&apos; WITH GRANT OPTION;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;*&apos; IDENTIFIED BY &apos;psd&apos; WITH GRANT OPTION;FLUSH PRIVILEGES;# 退出登录exit;# 此时可以再登录试试mysql -uroot -ppsd======================================# 修改字符集为 utf-8# 新建一个文件vi /etc/my.cnf # 添加以下内容[client]default-character-set=utf8[mysql]default-character-set=utf8[mysqld]character-set-server=utf8# 重启mysqlsudo service mysql restart======================================# 忘记密码，修改密码的方法# 停止mysql服务的运行service mysql stop# 跳过授权表访问mysqld_safe --user=mysql --skip-grant-tables --skip-networking &amp; # 登录mysqlmysql -u root mysql # 接下来可以修改密码了 ##在mysql5.7以下的版本如下：mysql&gt; UPDATE user SET Password=PASSWORD(&apos;newpassword&apos;) where USER=&apos;root’； ##在mysql5.7版本如下：update mysql.user set authentication_string=password(&apos;newpassword&apos;) ;# 修改完了重启service mysql restart====================================== MySql的其它错误在运行schematool -dbType mysql -initSchema手动初始化元数据库的时候 报了一个log4j重复加载的问题 解决：可以不用理睬 链接mysql 密码过期问题 Your password has expired. 解决： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950mysql -uroot -p123mysql&gt; use mysqlReading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; select host,user,password_expired from user;+-----------+------+------------------+| host | user | password_expired |+-----------+------+------------------+| % | root | N || cs1 | root | Y || 127.0.0.1 | root | Y || ::1 | root | Y |+-----------+------+------------------+4 rows in set (0.00 sec)-------------mysql&gt; update user set password_expired=&apos;N&apos; where host=&apos;::1&apos;;Query OK, 1 row affected (0.01 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; update user set password_expired=&apos;N&apos; where host=&apos;cs1&apos;;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; update user set password_expired=&apos;N&apos; where host=&apos;127.0.0.1&apos;;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; FLUSH PRIVILEGES;Query OK, 0 rows affected (0.00 sec)mysql&gt; select host,user,password_expired from user;+-----------+------+------------------+| host | user | password_expired |+-----------+------+------------------+| % | root | N || cs1 | root | N || 127.0.0.1 | root | N || ::1 | root | N |+-----------+------+------------------+4 rows in set (0.00 sec)mysql&gt; exitBye[ap@cs1]~/apps/hive% sudo service mysql restart# 再重新初始化就好了 Yum安装 Mysql 因为笔者没装过，所以这部分暂且不表 step2-安装HiveMySql安装好之后， 安装Hive就很简答了 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132# 1.上传到Linux# 2.解压安装包到安装目录tar -zxvf apache-hive-2.3.2-bin.tar.gz -C ~/apps/# 3.把MySQL驱动包(mysql-connector-java-5.1.40-bin.jar)放置在hive 的根路径下的 lib 目录，此处是 ~/apps/apache-hive-2.3.2-bin/lib# 4.修改配置文件cd ~/apps/apache-hive-2.3.2-bin/conf# 新建一个 hive-site.xmltouch hive-site.xmlvi hive-site.xml+++++++++++++++++++++++++++++++++++++++++++添加如下内容++++++++++++&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/hivedb?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;!-- 注意:如果mysql和hive 不在同一个服务器节点,需要使用mysql节点的 hostname 或 ip --&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt;+++++++++++++++++++++++++++++++++++++++++++添加如下内容+++++++++++ &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;description&gt;hive default warehouse, if nessecory, change it &lt;!-- 这个是配置hive在HDFS上 db 的路存储径的，不配默认默认就是上述路径 --&gt; &lt;/description&gt; &lt;/property&gt;++++++++++++++++++++++++++# 5.配置环境变量 &amp; source## 注意：ap是我的用户家目录，换上自己的用户家目录export HIVE_HOME=/home/ap/apps/apache-hive-2.3.2-bin export PATH=$PATH:$HIVE_HOME/bin# 用bash的找 .bash_profile， 配置所有环境变量source ~/.zshrc # 6.此时基本安装完成了，验证Hive安装hive --helo# 7.重点来了！ 初始化元数据库chematool -dbType mysql -initSchema&gt; 这里可能会遇到很多错误！！ &gt; 但是如果前面按照我的方法装的, 应该就问题不大了&gt; 主要是 mysql 连接权限的问题！！# 一定会出现的2个1&gt; 找不到 hive命令的一长串环境变量 (不用理会)2&gt; 两个log4j，jar包重复的问题 (不用理会)# 8.初始化完成后，可以看下数据库中有没有 hivedb 这个库，成功的话是会有的## 启动hivehive --service cli &gt;hive: show databases;# 如果能显示数据库，就没啥问题了# ❤️9.Hive的使用方式之 HiveServer2/beeline此处需要修改 hadoop 的配置文件# 9.1.首先关闭hdfs &amp; yarn服务 &amp; RunJar(hive服务)，修改hadoop配置文件# 9.2.修改 hadoop 集群的 hdfs-site.xml 配置文件:加入一条配置信息，表示启用 webhdfs&lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;# 9.3.修改 hadoop 集群的 core-site.xml 配置文件:加入两条配置信息:表示设置 hadoop 的代理用户## 注意： 此处的ap是配置 hadoop 的用户名&lt;property&gt; &lt;name&gt;hadoop.proxyuser.ap.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;!-- 表示任意节点使用 hadoop 集群的代理用户 hadoop 都能访问 hdfs 集群 --&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.ap.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;!-- 表示代理用户的组所属 --&gt;&lt;/property&gt; 注意 修改完成后， 发给hadoop集群其他主机scp xxx.xx xxx.ss cs2:$PWDscp xxx.xx xxx.ss cs3:$PWD...# 9.4.重启hdfs &amp; yarn服务# 9.5.启动 hiveserver2 服务## 后台启动：nohup hiveserver2 1&gt;/home/hadoop/hiveserver.log 2&gt;/home/hadoop/hiveserver.err &amp; # 或者:nohup hiveserver2 1&gt;/dev/null 2&gt;/dev/null &amp;# 或者:nohup hiveserver2 &gt;/dev/null 2&gt;&amp;1 &amp;# 以上 3 个命令是等价的，第一个表示记录日志，第二个和第三个表示不记录日志注意： nohup 可以在你退出帐户/关闭终端之后继续运行相应的进程。 nohup 就是不挂起的意思(no hang up)。该命令的一般形式为:nohup command &amp;# 9.6 启动 beeline 客户端去连接方式1：执行命令:beeline -u jdbc:hive2://cs2:10000 -n ap-u : 指定元数据库的链接信息 -n : 指定用户名和密码方式2：先执行 beeline然后按图所示输入:# 此处的cs2是只安装hive的hostname!connect jdbc:hive2://cs2:10000 按回车，然后输入用户名，密码，这个 用户名就是安装 hadoop 集群的用户名和密码 PS: Linux环境变量失效1234# 就是直接把环境变量设置为/bin:/usr/bin，因为常用的命令都在/bin这个文件夹中。PATH=/bin:/usr/bin# 接下来修改 .bashr_profile 或则 .zshrc中的内容即可， 修改完了重新source Hive – DDL库的操作12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758 库的操作 ============================================================================#  建库 CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment][LOCATION hdfs_path][WITH DBPROPERTIES (property_name=property_value, ...)];1、创建普通库create database dbname;2、创建库的时候检查存与否create databse if not exists dbname;3、创建库的时候带注释create database if not exists dbname comment &apos;create my db named dbname&apos;;4、查看创建库的详细语句show create database mydb;#  查看库 1、查看有哪些数据库 show databases;2、显示数据库的详细属性信息语法:desc database [extended] dbname; 示例:desc database extended myhive;3、查看正在使用哪个库 select current_database();4、查看创建库的详细语句 show create database mydb;5、查看以xx开头的库show databases like &apos;s*&apos;#  删除库 删除库操作: drop database dbname; drop database if exists dbname; 默认情况下，hive 不允许删除包含表的数据库，有两种解决办法:1、 手动删除库下所有表，然后删除库2、 使用 cascade 关键字 drop database if exists dbname cascade; 默认情况下就是 restrict（严格模式）, 后2行效果一样 drop database if exists myhive drop database if exists myhive restrict#  切换库 切换库操作:- 语法:use database_name - 实例:use myhive; 表的操作创建表 语法结构 建表语法 CREATE [EXTERNAL] TABLE [IF NOT EXISTS] &lt;table_name&gt;创建内部表；添加EXTERNAL参数会创建外部表 (col_name data_type [COMMENT col_comment], ...)添加字段和字段描述 [COMMENT table_comment]添加表描述 [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]指定分区字段和字段描述，分区字段不能为建表字段！ [CLUSTERED BY (col_name, col_name, ...)] SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] 指定分桶字段，分桶字段必须为建表字段！ 指定排序字段，此字段也必须为建表字段，指定的是分桶内的排序 指定分桶个数（hash后再取模） [ROW FORMAT row_format]指定分隔符，row_format 格式：列分隔符：`delimited fields terminated by ‘x&apos;` 行分隔符：`line terminated by ‘x&apos;` [STORED AS file_format]指定存储格式textfile：文本格式，默认 rcfile：行列结合格式 parquet：压缩格式 [LOCATION hdfs_path]指定表在hsfs上的存储路径，不指定的话就按配置的路径存储，如果也没指定就在hive默认的路经 /user/hive/warehouse 建表代码 a. 创建内部表 create table mytable (id int, name string) row format delimited fields terminated by &#39;,&#39; stored as textfile; b. 创建外部表 create external table mytable2 (id int, name string) row format delimited fields terminated by &#39;,&#39; location &#39;/user/hive/warehouse/mytable2&#39;; c. 创建分区表 create table table3(id int, name string) partitioned by(sex string) row format delimited fields terminated by &#39;,&#39; stored as textfile; 插入分区数据：load data local inpath &#39;/root/hivedata/mingxing.txt&#39; overwrite into table mytable3 partition(sex=&#39;girl’); 查询表分区： show partitions mytable3 d. 创建分桶表 create table stu_buck(Sno int,Sname string,Sex string,Sage int,Sdept string) clustered by(Sno) sorted by(Sno DESC) into 4 buckets row format delimited fields terminated by &#39;,’; e. 复制表 create [external] table [if not exists] new_table like table_name; f. 查询表 create table table_a as select * from teble_b; 查看表 desc &lt;table_name&gt;：显示表的字段信息 desc formatted &lt;table_name&gt;：格式化显示表的详细信息 desc extended &lt;table_name&gt;：显示表的详细信息 修改表 重命名 ALTER TABLE old_name RENAME TO new_name 修改属性 ALTER TABLE table_name SET TBLPROPERTIES (&#39;comment&#39; = &#39;my new students table’); 不支持修改表名，和表的数据存储目录 增加/修改/替换字段 ALTER TABLE table_name ADD COLUMNS (col_spec [, col_spec ...])新增的字段位置在所有列后面 ( partition 列前 ) ALTER TABLE table_name CHANGE c_name new_c_name new_c_type [FIRST | AFTER c_name]注意修改字段时，类型只能由小类型转为大类型，不让回报错；（在hive1.2.2中并没有此限制） ALTER TABLE table_name REPLACE COLUMNS (col_spec [, col_spec ...])REPLACE 表示替换表中所有字段 添加/删除分区 添加分区：ALTER TABLE table_name ADD [IF NOT EXISTS] PARTITION (partition_col = col_value1 [ ... ] ) [LOCATION &#39;location1’] 删除分区：ALTER TABLE table_name DROP [IF EXISTS] PARTITION (partition_col = col_value1 [ ... ] ) 修改分区路径：ALTER TABLE student_p PARTITION (part=&#39;bb&#39;) SET LOCATION &#39;/myhive_bbbbb’; [补充：] 1、 防止分区被删除：alter table student_p partition (part=’aa’) enable no_drop; 2、 防止分区被查询：alter table student_p partition (part=’aa’) enable offline;enable 和 disable 是反向操作 删除表 drop table if exists &lt;table_name&gt;; 清空表会保留表结构 truncate table table_name; truncate table table_name partition(city=&#39;beijing’); 其他辅助命令 a. 查看数据库列表 show databases; show databases like &#39;my*&#39;; b. 查看数据表 show tables; show tables in db_name; c. 查看数据表的建表语句 show create table table_name; d. 查看 hive 函数列表 show functions; e. 查看 hive 表的分区 show partitions table_name; show partitions table_name partition(city=&#39;beijing&#39;) f. 查看表的详细信息（元数据信息） desc table_name; desc extended table_name; g. 查看数据库的详细属性信息 desc formatted table_name; desc database db_name; desc database extended db_name; h. 清空数据表 truncate table table_name; Hive – DML装载数据 LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE table_name [PARTITION (partcol1=val1, partcol2=val2 ...)] 注意： LOAD 操作只是单纯的 复制（本地文件）或者 移动（hdfs文件，一般是公共数据，需要建立外部表）操作，将数据文件移动到 Hive 表对应的位置 如果指定了 LOCAL 就去本地文件系统中查找，否则按 inpath 中的 uri 在 hdfs 上查找 inpath 子句中的文件路径下，不能再有文件夹 如果使用了 OVERWRITE 关键字，则目标表（或者分区）中的内容会被删除，然后再将 filepath 指向的文件/目录中的内容添加到表/分区中。 如果目标表（分区）已经有一个文件，并且文件名和 filepath 中的文件名冲突，那么现有的文件会被新文件所替代。???不是自动重命名为xxx_copy_1 插入数据 a. 单条插入 INSERT INTO TABLE table_name VALUES(value1, value2, ...); b. 单重插入 INSERT INTO TABLE table_name [PARTITION (partcol1=val1, ...)] &lt;select_statement1 FROM from_statement&gt; c. 多重插入 FROM from_statement从基表中按不同的字段查询得到的结果分别插入不同的 hive 表只会扫描一次基表，提高查询性能 INSERT INTO TABLE table_name1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 [WHERE where_statement] INSERT INTO TABLE table_name2 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement2 [WHERE where_statement] [ … ]; d. 分区插入 分区插入有两种：一种是静态分区，另一种是动态分区。 如果混合使用静态分区和动态分区， 则静态分区必须出现在动态分区之前。 静态分区 A)、创建静态分区表 B)、从查询结果中导入数据（单重插入）记得加载数据前要添加分区，然后指定要加载到那个分区 C)、查看插入结果 动态分区 静态分区添加数据前需要指定分区，当分区个数不确定的时候就很不方便了，这个时候可以使用动态分区 重要且常用，尤其是按照日期分区时！ A)、创建分区表 B)、参数设置 hive-1.2版本 - set hive.exec.dynamic.partition=true; //动态分区开启状态，默认开启 - set hive.exec.dynamic.partition.mode=nonstrict; //动态分区执行模式，默认&quot;strict&quot;，在这种模式 下要求至少有一列分区字段是静态的，这有助于阻止因设计错误导致查询产生大量的分区 - \# 可选设置项 如果这些参数被更改了又想还原，则执行一次 reset 命令即可 - set hive.exec.max.dynamic.partitions.pernode=100; //每个节点生成动态分区最大个数 - set hive.exec.max.dynamic.partitions=1000; //生成动态分区最大个数，如果自动分区数大于这个参数，将会报错 - set hive.exec.max.created.files=100000; //一个任务最多可以创建的文件数目 - set dfs.datanode.max.xcievers=4096; //限定一次最多打开的文件数 set - hive.error.on.empty.partition=false; //表示当有空分区产生时，是否抛出异常 - C)、动态数据插入 - 单个分区字段 - insert into table test2 partition (age) select name,address,school,age from students; - 多个分区字段 多重分区中目录结构是按照分区字段顺序进行划分的 - insert into table student_ptn2 partition(department, age) select id, name, sex, department,age from students; 分区字段都是动态的 - insert into table student_ptn2 partition(city=&apos;sa&apos;, zipcode) select id, name, sex, age, department, department as zipcode from students; 第一个分区字段时静态的，第二字department字段动态的，重命名为zipcode？ - [注意：] - 查询语句 select 查询出来的动态分区 age 和 zipcode 必须放在 最后，和分区字段对应，不然结果会出错 - D)、查看插入结果 - select * from student_ptn2 where city=&apos;sa&apos; and zipcode=&apos;MA&apos;; e. 分桶插入 A)、创建分桶表 B)、从查询结果中导入数据只能使用insert方式 C)、查看插入结果 # 几个命令 set hive.exec.reducers.bytes.per.reducer= // 设置每个reducer的吞吐量，单位byte，默认256M set hive.exec.reducers.max= //reduceTask最多执行个数，默认1009 set mapreduce.job.reduces= //设置reducetask实际运行数，默认-1，代表没有设置，即reducetask默认数为1 set hive.exec.mode.local.auto=true //设置hive本地模式 导出数据（了解） 单模式导出 INSERT OVERWRITE [LOCAL] DIRECTORY directory1 &lt;select_statement&gt;; 多模式导出 FROM from_statement INSERT OVERWRITE [LOCAL] DIRECTORY directory1 &lt;select_statement1&gt; [INSERT OVERWRITE [LOCAL] DIRECTORY directory2 &lt;select_statement2&gt;] ... 查询数据 Hive 中的 SELECT 基础语法和标准 SQL 语法基本一致，支持 WHERE、DISTINCT、GROUP BY、 ORDER BY、HAVING、LIMIT、子查询等； 1、select * from db.table1虽然可以，但是要尽量避免 select * 这样的全表扫描操作，效率太低又费时 2、select count(distinct uid) from db.table1 3、支持 select、union all、join（left、right、full join）、like、where、having、各种聚合函数、 支持 json 解析 4、UDF/ UDAF/UDTF UDF：User Defined Function，自定义函数，一对一 UDAF：User Defined Aggregate Function，自定义聚合函数，多对一，如sum()，count() UDTF ：User Defined Table Function，自定义表函数，一对多，如explode() 5、不支持 update 和 delete 6、hive 虽然支持 in/exists（老版本是不支持的），但是 hive 推荐使用 semi join 的方式来代替 实现，而且效率更高。 半连接 左半连接：left semi join，以左表为基表，右表有的，只显示左表相应记录（即一半） 右半连接：right semi join，与左半连接相反 内连接：inner join，两表中都有的才会连接 外连接 左外连接：left outer join，坐标为基表，右表有的会关联，右表没有的以null表示；左表没有右表有的不会关联 右外连接：right outer join，与左外连接相反 全外连接：full outer join，两表合并 7、支持 case … when … 语法结构 SELECT [ALL | DISTINCT] select_ condition, select_ condition, ... FROM table_name a [JOIN table_other b ON a.id = b.id]表连接 [WHERE where_condition]过滤条件 [GROUP BY col_list [HAVING condition]]分组条件 [CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY col_list | ORDER BY col_list] [DESC]]排序条件：\1. order by：全局排序，默认升序，DESC表示降序。只有一个 reduce task 的结果，比如文件名是 000000_0，会导致当输入规模较大时，需要较长的计算时间。\2. sort by：局部排序，其在数据进入 reducer 前完成排序。因此，如果用 sort by 进行排序，并且设置 mapred.reduce.tasks &gt; 1，则 sort by 只保证每个 reducer 的输出有序，不保证全局有序。\3. distribute by：类似于分桶，根据指定的字段将数据分到不同的 reducer，且分发算法是 hash 散列\4. cluster by：除了具有 Distribute by 的功能外，还会对该字段进行排序。注意：如果 distribute 和 sort 字段是同一个时，cluster by = distribute by + sort by；如果分桶字段和排序字段不一样，那么就不能使用 clustered by [LIMIT number]显示结果的前几个记录]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[_数据分析系统——Hive]]></title>
    <url>%2F2018%2F06%2F12%2FHadoop%2FStudy%2F3-Hive%2F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9FHive%2F</url>
    <content type="text"><![CDATA[Hive背景及应用场景Hive是什么？看一下MR的 wordcount 和 Hive 的 wordcount Hive典型应用场景 为什么使用Hive？ 有了Hive，还需要自己写MR程序吗？ Hive基本架构 Hive各模块组成 Hive部署架构-实验环境 Hive部署架构-生产环境 Hive部署架构-metastore服务 Hive使用方式CLI（Command Line Interface） CLI—hive外部资源 Hive Web UI Hive客户端程序 HQL查询语句数据模型 数据类型（不断增加中……） 自有的特殊类型 数据定义语句（DDL） 分隔符开发中这样写 Hive Partition与Bucket Hive数据格式 应用举例：日志清理 数据操作语句（DML） 数据加载与插入语句 几个实例 数据查询语句SELECT 两种分布式Join算法 HQL中两种特殊Join Order By与Sort By Distribute by与Cluster by Transform语法（Streaming） 用户自定义函数（UDF） UDAF http://svn.apache.org/repos/asf/hive/branches/branch0.13/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFAverage.java UDTF http://svn.apache.org/repos/asf/hive/branches/branch0.13/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFExplode.java 其它问题 Hive On HBase Hive总结及其类似开源系统Stinger Hive-MR vs Hive-Tez Shark Impala 性能比较]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[偶然发现的好歌]]></title>
    <url>%2F2018%2F06%2F11%2FSongs%2F%E5%81%B6%E7%84%B6%E5%8F%91%E7%8E%B0%E7%9A%84%E5%A5%BD%E6%AD%8C%2F</url>
    <content type="text"><![CDATA[Dealbreaker 专辑：Chesapeake 歌手：Rachael Yamagata 链接：http://music.163.com/#/m/song?id=18733198 You Won’t Let Me 专辑：Chesapeake 歌手：Rachael Yamagata 链接：http://music.163.com/#/song?id=18733192]]></content>
      <categories>
        <category>文艺</category>
        <category>Songs</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>文艺</tag>
        <tag>Songs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[_HDFS应用场景&原理&基本架构及使用方法]]></title>
    <url>%2F2018%2F06%2F11%2FHadoop%2FStudy%2F1-HDFS%2FHDFS%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%26%E5%8E%9F%E7%90%86%26%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84%E5%8F%8A%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[HDFS基本架构和原理HDFS设计思想 HDFS架构 HDFS数据块（block） 注意： Hadoop2.x，block默认大小是128MB HDFS写流程 创建Distributed FileSystem类 询问 NameNode 要写的文件对否存在 不存在就写入到 FSDataOutputStream 流中 流写出去到一个 DataNode … HDFS读流程 客户端向 NameNode 询问 block 的位置 按照客户端按照拿到的位置，向不同的DataNode 请求数据 …… HDFS典型物理拓扑 HDFS副本放置策略 HDFS可靠性策略 HDFS不适合存储小文件 HDFS程序设计HDFS访问方式 HDFS Shell命令概览 HDFS Shell命令—文件操作命令 HDFS Shell命令—文件操作命令 HDFS Shell命令—管理命令 HDFS Shell命令—管理脚本 HDFS Shell命令—文件管理命令fsck 查看帮助 用法示例 HDFS Shell命令—数据均衡器balancer 一般设置10% —— 15% 就差不多了 HDFS Shell命令—设置目录份额 ※ HDFS Shell命令—增加/移除节点 ※ HDFS JavaAPI介绍 HDFS Java程序举例 HDFS 多语言API—借助thrift也是Apache的顶级项目 thrift执行流程 hadoopfs.thrift接口定义 PHP语言访问HDFS Python语言访问HDFS Hadoop 2.0新特性 HA(高可用)与Federation(联邦) 异构层级存储结构背景 原理 HDFS ACL背景：现有权限管理的局限性 基于POSIX ACL的实现 HDFS快照背景 基本使用方法 HDFS缓存背景 原理 实现情况 总结]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于hexo的时序图插件 hexo-filter-sequence 的巨坑]]></title>
    <url>%2F2018%2F06%2F10%2FTools%2FHexo%2F%E5%85%B3%E4%BA%8Ehexo%E7%9A%84%E6%97%B6%E5%BA%8F%E5%9B%BE%E6%8F%92%E4%BB%B6-hexo-filter-sequence-%E7%9A%84%E5%B7%A8%E5%9D%91%2F</url>
    <content type="text"><![CDATA[前言在写代码的过程中，发现时序图还是挺管用的，简洁明了，于是想用一用。 结果发到站上，不显示。 在网上查了下，发现是hexo所使用的Markdown渲染语法不支持。 这里吐槽下，这里渲染的确实烂，作者为啥不改改.. 于是开始找解决方案，发现大多数都推荐了一个叫hexo-filter-sequence的插件，故安装之。 结果死活还是不行。 装了其它的几个flow图，却可以显示。 当flow图和sequence图同时存在的时候，sequence可以显示，但是sequence单独存在时，死活不显示。 难不成有啥依赖？必须先使用flow才能使用sequence？逻辑上不应该啊！ 但是事实却是这样！ 网上有一篇大神说要改源码才行，开始没注意，觉得应该不会吧，这么多人使用，源码还会有问题？？ 仔细一看源码，妈的！！心里千万匹草泥马呼啸而过！ 把初始化 sequence，写成了初始化 flow！！！ 把 flow 改成 sequence， 再把 js CDN源换成国内的！ 可以了！！ 再仔细一看，发现最后一次更新是在1年前！ 坑爹的作者，浪费了我至少3-5个小时！！ 下面为部分摘抄安装hexo-filter-sequence 插件: 1npm install --save hexo-filter-sequence 配置站点配置文件 _config.yml 中增加如下配置: 123456789sequence: webfont: https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js raphael: https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js underscore: https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js sequence: https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js css: # optional, the url for css, such as hand drawn theme options: theme: simple css_class: 源码源码修改后才能正常使用，进入插件目录作如下修改： 12345678910111213141516// index.jsvar assign = require('deep-assign');var renderer = require('./lib/renderer');hexo.config.sequence = assign(&#123; webfont: 'https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js', raphael: 'https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js', underscore: 'https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js', sequence: 'https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js', css: '', options: &#123; theme: 'simple' &#125;&#125;, hexo.config.sequence);hexo.extend.filter.register('before_post_render', renderer.render, 9); 12345678910// lib/renderer.js, 25 行if (sequences.length) &#123; var config = this.config.sequence; // resources data.content += '&lt;script src="' + config.webfont + '"&gt;&lt;/script&gt;'; data.content += '&lt;script src="' + config.raphael + '"&gt;&lt;/script&gt;'; data.content += '&lt;script src="' + config.underscore + '"&gt;&lt;/script&gt;'; data.content += '&lt;script src="' + config.sequence + '"&gt;&lt;/script&gt;'; ......&#125; 示例新建代码块，增加如下内容： 详情参考 Alice->Bob: Hello Bob, how are you? Note right of Bob: Bob thinks Bob-->Alice: I am good thanks!{"theme":"simple"} var code = document.getElementById("sequence-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value)); var diagram = Diagram.parse(code); diagram.drawSVG("sequence-0", options);]]></content>
      <categories>
        <category>工具</category>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>工具</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce-分组浅探]]></title>
    <url>%2F2018%2F06%2F10%2FHadoop%2FStudy%2F2-MapReduce%2FMapReduce-%E5%88%86%E7%BB%84%E6%B5%85%E6%8E%A2%2F</url>
    <content type="text"><![CDATA[前言最近学分组的时候，一直弄不明白，总感觉一头雾水，通过近3个小时的断点调试和log输出。 大概了解了一些表象，先记录。 案例是这个 求出每门课程参考学生成绩最高平均分的学生的信息： 课程，姓名和平均分，详细见MapReduce笔记-练习第二题第3小题 数据格式是这样的： 第一个是课程名称，总共四个课程，computer，math，english，algorithm， 第二个是学生姓名，后面是每次考试的分数 math,huangxiaoming,85,75,85,99,66,88,75,91 english,huanglei,85,75,85,99,66,88,75,91 … 结论 执行流程结论 map每读一行就 write 到 context 一次，按照指定的key进行分发 map 把所有的数据都读完了之后，大概执行到67%的时候，开始进入 CustomBean，执行CustomBean的compareTo()方法，会按照自己写的规则一条一条数据比较 上述都比较完毕之后，map阶段就结束了，此时来到了 reduce阶段，但是是到了67%了 到了reduce阶段，直接进入了MyGroup中自定义的compare方法。 MyGroup的compare()方法，如果返回非0， 就会进入 reduce 方法写出到context MyGroup进入Reduce的条件是 MyReduce中，如果compare的结果不等于0，也就是比较的2者不相同， 此时就进入Reduce， 写出到上下文 如果相同，会一直往下读，直到读到不同的， 此时写出读到上下文。 因为MyGroup会在Reduce阶段执行，而CustomBean中的compareTo()是在map阶段执行，所以需要在CustomBean中就把组排好序，此时分组功能才能正常运作 指定分组类MyGroup和不指定的区别指定与不指定是指：在Driver类中，是否加上job.setGroupingComparatorClass(MyGrouper.class);这一句。 指定分组类： 会按照分组类中，自定义的compare()方法比较，相同的为一组，分完一组就进入一次reduce方法 不指定分组类：（目前存疑） 是否是按照key进行分组 如果是自定义类为key，是否是按照此key中值相同的分为一组 如果是hadoop内置类，是否是按照此类的值分组（Text-String的值，IntWritable-int值等..） 依然是走得以上这套分组逻辑，一组的数据读完才进入到Reduce阶段做归并 Log信息CustomBean中没有进行分组, 组内排序的log123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129// ==================MyGroup中compare()方法=======================computer---MyGroup中比较---computercomputer---MyGroup中比较---mathmath---MyGroup中比较---englishenglish---MyGroup中比较---mathmath---MyGroup中比较---algorithmalgorithm---MyGroup中比较---algorithmalgorithm---MyGroup中比较---computercomputer---MyGroup中比较---englishenglish---MyGroup中比较---algorithmalgorithm---MyGroup中比较---mathmath---MyGroup中比较---algorithmalgorithm---MyGroup中比较---mathmath---MyGroup中比较---computercomputer---MyGroup中比较---englishenglish---MyGroup中比较---mathmath---MyGroup中比较---computercomputer---MyGroup中比较---mathmath---MyGroup中比较---englishenglish---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---englishenglish---MyGroup中比较---computercomputer---MyGroup中比较---algorithmalgorithm---MyGroup中比较---englishenglish---MyGroup中比较---computercomputer---MyGroup中比较---englishenglish---MyGroup中比较---englishenglish---MyGroup中比较---mathmath---MyGroup中比较---algorithmalgorithm---MyGroup中比较---computercomputer---MyGroup中比较---english// ======================reduce中的执行log============================================第1次进入reducecomputer huangjiaju 83.2---------in for write------computer liutao 83.0---------in for write------==================第2次进入reducemath huangxiaoming 83.0---------in for write------==================第3次进入reduceenglish huanglei 83.0---------in for write------==================第4次进入reducemath huangjiaju 82.28571428571429---------in for write------==================第5次进入reducealgorithm huangjiaju 82.28571428571429---------in for write------algorithm liutao 82.0---------in for write------==================第6次进入reducecomputer huanglei 74.42857142857143---------in for write------==================第7次进入reduceenglish liuyifei 74.42857142857143---------in for write------==================第8次进入reducealgorithm huanglei 74.42857142857143---------in for write------==================第9次进入reducemath huanglei 74.42857142857143---------in for write------==================第10次进入reducealgorithm huangzitao 72.75---------in for write------==================第11次进入reducemath liujialing 72.75---------in for write------==================第12次进入reducecomputer huangzitao 72.42857142857143---------in for write------==================第13次进入reduceenglish huangxiaoming 72.42857142857143---------in for write------==================第14次进入reducemath wangbaoqiang 72.42857142857143---------in for write------==================第15次进入reducecomputer huangxiaoming 72.42857142857143---------in for write------==================第16次进入reducemath xuzheng 69.28571428571429---------in for write------==================第17次进入reduceenglish zhaobenshan 69.28571428571429---------in for write------==================第18次进入reducecomputer huangbo 65.25---------in for write------computer xuzheng 65.0---------in for write------==================第19次进入reduceenglish zhouqi 64.18181818181819---------in for write------==================第20次进入reducecomputer liujialing 64.11111111111111---------in for write------==================第21次进入reducealgorithm liuyifei 62.142857142857146---------in for write------==================第22次进入reduceenglish liujialing 62.142857142857146---------in for write------==================第23次进入reducecomputer liuyifei 62.142857142857146---------in for write------==================第24次进入reduceenglish liuyifei 59.57142857142857---------in for write------english huangdatou 56.0---------in for write------==================第25次进入reducemath liutao 56.0---------in for write------==================第26次进入reducealgorithm huangdatou 56.0---------in for write------==================第27次进入reducecomputer huangdatou 56.0---------in for write------==================第28次进入reduceenglish huangbo 55.0---------in for write------ CustomBean中做了分组&amp;组内排序 的123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120// **Bean中相同的组进行分数降序， 组进行字典排序，此时MyGroup中执行的**algorithm---MyGroup中比较---algorithmalgorithm---MyGroup中比较---algorithmalgorithm---MyGroup中比较---algorithmalgorithm---MyGroup中比较---algorithmalgorithm---MyGroup中比较---algorithmalgorithm---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---computercomputer---MyGroup中比较---englishenglish---MyGroup中比较---englishenglish---MyGroup中比较---englishenglish---MyGroup中比较---englishenglish---MyGroup中比较---englishenglish---MyGroup中比较---englishenglish---MyGroup中比较---englishenglish---MyGroup中比较---englishenglish---MyGroup中比较---englishenglish---MyGroup中比较---mathmath---MyGroup中比较---mathmath---MyGroup中比较---mathmath---MyGroup中比较---mathmath---MyGroup中比较---mathmath---MyGroup中比较---mathmath---MyGroup中比较---math// ======================reduce中执行============================================第1次进入reducealgorithm huangjiaju 82.28571428571429---------in for write------algorithm liutao 82.0---------in for write------algorithm huanglei 74.42857142857143---------in for write------algorithm huangzitao 72.75---------in for write------algorithm liuyifei 62.142857142857146---------in for write------algorithm huangdatou 56.0---------in for write------===================离开reduce==================第2次进入reducecomputer huangjiaju 83.2---------in for write------computer liutao 83.0---------in for write------computer huanglei 74.42857142857143---------in for write------computer huangzitao 72.42857142857143---------in for write------computer huangxiaoming 72.42857142857143---------in for write------computer huangbo 65.25---------in for write------computer xuzheng 65.0---------in for write------computer liujialing 64.11111111111111---------in for write------computer liuyifei 62.142857142857146---------in for write------computer huangdatou 56.0---------in for write------===================离开reduce==================第3次进入reduceenglish huanglei 83.0---------in for write------english liuyifei 74.42857142857143---------in for write------english huangxiaoming 72.42857142857143---------in for write------english zhaobenshan 69.28571428571429---------in for write------english zhouqi 64.18181818181819---------in for write------english liujialing 62.142857142857146---------in for write------english liuyifei 59.57142857142857---------in for write------english huangdatou 56.0---------in for write------english huangbo 55.0---------in for write------===================离开reduce==================第4次进入reducemath huangxiaoming 83.0---------in for write------math huangjiaju 82.28571428571429---------in for write------math huanglei 74.42857142857143---------in for write------math liujialing 72.75---------in for write------math wangbaoqiang 72.42857142857143---------in for write------math xuzheng 69.28571428571429---------in for write------math liutao 56.0---------in for write------===================离开reduce//  如果只取一个每次values的第一个的话 algorithm huangjiaju 82.28571428571429==================第1次进入reducecomputer huangjiaju 83.2==================第2次进入reduceenglish huanglei 83.0==================第3次进入reducemath huangxiaoming 83.0==================第4次进入reduce 其它疑点 通过log可以看出来， MyGroup是在读到了不同的数据，才会进入到 reduce 类中写出； 但是 通过 断点调试时， 现象是，第一次读到了2个相同的，就去reduce去写出了； 后面再读的就不做写出操作，直到下一次读到2个相同的，再在reduce中写出。 title: 执行流程时序图  Mapper(map)->ScoreBean: k:ScoreBean(courseName,avgScore) Mapper(map)->ScoreBean: v:Text(stuName) Mapper(ScoreBean)->Reducer(MyGroup): course按字典升序 Mapper(ScoreBean)->Reducer(MyGroup): course内成绩降序 Reducer(MyGroup)->Reducer(reduce): 根据自定义的分组规则按组输出 Reducer(MyGroup)->Reducer(reduce): 一组只调用reduce一次 Reducer(reduce)-->Reducer(reduce):{"theme":"simple"} var code = document.getElementById("sequence-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value)); var diagram = Diagram.parse(code); diagram.drawSVG("sequence-0", options);]]></content>
      <categories>
        <category>Hadoop</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[▍开始]]></title>
    <url>%2F2018%2F06%2F10%2FPoetry%2F%E5%BC%80%E5%A7%8B%2F</url>
    <content type="text"><![CDATA[月亮落下一两片羽毛在田野上。 黑暗中的麦子聆听着。 快静下来。 快。 就在那儿，月亮的孩子们正试着 挥动翅膀。 在两棵树之间，身材修长的女子抬起面庞， 美丽的剪影。接着，她步入空中，接着， 她完全消失在空中。 我独自站在一棵接骨木旁，不敢呼吸， 也不敢动。 我聆听着。 麦子向后靠着自己的黑暗， 而我靠着我的。 作者 / [美国] 詹姆斯·赖特 翻译 / 张文武 ▍Beginning The moon drops one or two feathers into the fields. The dark wheat listens. Be still. Now. There they are, the moon’s young, trying Their wings. Between trees, a slender woman lifts up the lovely shadow Of her face, and now she steps into the air, now she is gone Wholly, into the air. I stand alone by an elder tree, I do not dare breathe Or move. I listen. The wheat leans back toward its own darkness, And I lean toward mine. Author / James Wright]]></content>
      <categories>
        <category>文艺</category>
        <category>诗歌</category>
      </categories>
      <tags>
        <tag>诗歌</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown高阶语法]]></title>
    <url>%2F2018%2F06%2F10%2FTools%2FMarkdown%2FMarkdown%E9%AB%98%E9%98%B6%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[时序图的写法 流程图的写法 类图的写法 st=>start: Start|past:>http://www.google.com[blank] e=>end: End:>http://www.google.com op1=>operation: My Operation|past op2=>operation: Stuff|current sub1=>subroutine: My Subroutine|invalid cond=>condition: Yes or No?|approved:>http://www.google.com c2=>condition: Good idea|rejected io=>inputoutput: catch something...|request st->op1(right)->cond cond(yes, right)->c2 cond(no)->sub1(left)->op1 c2(yes)->io->e c2(no)->op2->e{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-0", options);mapper->Bob: Hello Bob, how are you? Note right of Bob: Bob thinks Bob-->reducer: I am good thanks!ad u? reducer->out: I'm fine too out->me: ok, you win me-->Bob: nono, not{"theme":"simple"} var code = document.getElementById("sequence-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value)); var diagram = Diagram.parse(code); diagram.drawSVG("sequence-0", options);]]></content>
      <categories>
        <category>工具</category>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>工具</tag>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[About Sublime Text3]]></title>
    <url>%2F2018%2F06%2F09%2FTools%2FSublime%2F%E5%AE%89%E8%A3%85%E4%B8%BB%E9%A2%98%2F</url>
    <content type="text"><![CDATA[主题详情参见这个网站 详细操作见此站]]></content>
      <categories>
        <category>工具</category>
        <category>Sublime</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>工具</tag>
        <tag>Sublime</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce笔记-Bug汇总]]></title>
    <url>%2F2018%2F06%2F09%2FHadoop%2FStudy%2F2-MapReduce%2FMapReduce-Bug%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[1、reduce 输出路径必须是新创建的。不能已经存在1Exception in thread "main" org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://cs1:9000/flowout01 already exists 2、在初始化 job 的时候， 没有传 conf ， 导致后面一直找不到文件， 因为不知道到哪里去找3、Text导包倒错, 导的时候要注意应该是这个 import org.apache.hadoop.io.Text; 4、进行字符串拼接的时候，把 StringBuilder 写到了 reduce 方法外， 这样导致 sb 会越来越多，当然，也可以每次拼接完了清空类似于这样 1234567891011121314A F,I,O,K,G,D,C,H,BB F,I,O,K,G,D,C,H,B,E,J,F,AC F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,FD F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,LE F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,HF F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,GG F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,MH F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,OI F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,CJ F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,OK F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,BL F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,B,D,EM F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,B,D,E,E,FO F,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,B,D,E,E,F,A,H,I,J,F 4、mapreduce执行错误Mapper.\错误 Mapper &amp; Reducer 写成内部类的时候，有没有加上 static Bean类有没有无参构造 5、排序过程中，自定义了排序类，bean类的 compareTo()方法，只写了按照分数大小排序。会出现如下错误： 课程并没有分组 没有在相同的一组课程中比较分数， 而是比较的所有的分数 12345678computer huangjiaju 83.2math huangxiaoming 83.0english huanglei 83.0math huangjiaju 82.28571428571429algorithm huangjiaju 82.28571428571429computer huanglei 74.42857142857143english liuyifei 74.42857142857143... 此时应该在Bean对象中做如下事情 相同课程的按照分数降序排序 课程名按照自然（升序）排序 换言之，就是CustomBean 对象要输出的数据是 组名升序排序，组内按成绩降序排序 具体分析参阅]]></content>
      <categories>
        <category>Hadoop</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce笔记-练习]]></title>
    <url>%2F2018%2F06%2F09%2FHadoop%2FStudy%2F2-MapReduce%2FMapReduce-%E7%BB%83%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[求微博共同粉丝题目涉及知识点： 多 Job 串联 1234567891011121314A:B,C,D,F,E,OB:A,C,E,KC:F,A,D,ID:A,E,F,LE:B,C,D,M,LF:A,B,C,D,E,O,MG:A,C,D,E,FH:A,C,D,E,OI:A,OJ:B,OK:A,C,DL:D,E,FM:E,F,GO:A,H,I,J,K 以上是数据：A:B,C,D,F,E,O表示：A用户 关注B,C,D,E,F,O 求所有两两用户之间的共同关注对象 答案： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299package com.rox.mapreduce.mr3._01_多Job串联;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob;import org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class CommonFansDemo &#123; @SuppressWarnings("deprecation") public static void main(String[] args) throws Exception &#123; // Job 逻辑 // 指定 HDFS 相关的参数 Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://cs1:9000"); System.setProperty("HADOOP_USER_NAME", "ap"); // // 新建一个 job1 Job job1 = Job.getInstance(conf); // 设置 Jar 包所在路径 job1.setJarByClass(CommonFansDemo.class); // 指定 mapper 类和 reducer 类 job1.setMapperClass(MyMapper_Step1.class); job1.setReducerClass(MyReducer_Step1.class); // 指定 maptask 的输出类型 job1.setMapOutputKeyClass(Text.class); job1.setMapOutputValueClass(Text.class); // 指定最终的输出类型(reduce存在时,就是指 ReduceTask 的输出类型) job1.setOutputKeyClass(Text.class); job1.setOutputValueClass(Text.class); // 指定该 MapReduce 程序数据的输入输出路径 FileInputFormat.setInputPaths(job1, new Path("/in/commonfriend")); FileOutputFormat.setOutputPath(job1, new Path("/out/job1")); // // 新建一个 job2 Job job2 = Job.getInstance(conf); // 设置 Jar 包所在路径 job2.setJarByClass(CommonFansDemo.class); // 指定 mapper 类和 reducer 类 job2.setMapperClass(MyMapper_Step2.class); job2.setReducerClass(MyReducer_Step2.class); // 指定 maptask 的输出类型 job2.setMapOutputKeyClass(Text.class); job2.setMapOutputValueClass(Text.class); // 指定最终的输出类型(reduce存在时,就是指 ReduceTask 的输出类型) job2.setOutputKeyClass(Text.class); job2.setOutputValueClass(Text.class); // 指定该 MapReduce 程序数据的输入输出路径 FileInputFormat.setInputPaths(job2, new Path("/out/job1")); FileOutputFormat.setOutputPath(job2, new Path("/out/job2")); // /** * 将多个 job 当做一个组中的 job 提交, 参数名是组名 * 注意: JobControl 是实现了 Runnable 接口的 */ JobControl jControl = new JobControl("common_friend"); // 将原生的 job携带配置 转换为可控的 job ControlledJob aJob = new ControlledJob(job1.getConfiguration()); ControlledJob bJob = new ControlledJob(job2.getConfiguration()); // 添加依赖关系 bJob.addDependingJob(aJob); // 添加 job 到组中 jControl.addJob(aJob); jControl.addJob(bJob); // 启动一个线程 Thread jobThread = new Thread(jControl); jobThread.start(); while (!jControl.allFinished()) &#123; Thread.sleep(500); &#125; jobThread.stop(); &#125; static class MyMapper_Step1 extends Mapper&lt;LongWritable, Text, Text, Text&gt; &#123; String[] user_attentions; String[] attentions; Text k = new Text(); Text v = new Text(); @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; user_attentions = value.toString().split(":"); attentions = user_attentions[1].trim().split(","); for (String att : attentions) &#123; k.set(att); v.set(user_attentions[0].trim()); context.write(k, v); &#125; &#125; &#125; /** * @author shixuanji * 将两两粉丝(普通用户)拼接起来, 格式a-f:c =&gt; a,b 都共同关注了 c * * A F,I,O,K,G,D,C,H,B B E,J,F,A C B,E,K,A,H,G,F D H,C,G,F,E,A,K,L E A,B,L,G,M,F,D,H F C,M,L,A,D,G */ static class MyMapper_Step2 extends Mapper&lt;LongWritable, Text, Text, Text&gt; &#123; String[] attenion_users; String[] users; Text k = new Text(); Text v = new Text(); @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; attenion_users = value.toString().split("\t"); users = attenion_users[1].trim().split(","); for (String u1 : users) &#123; for (String u2 : users) &#123; if (u1.compareTo(u2) &lt; 0) &#123; String users = u1 + "-" + u2; k.set(users); v.set(attenion_users[0].trim()); context.write(k, v); &#125; &#125; &#125; &#125; &#125; /** * @author shixuanji * 需要统计的是, 某人拥有的全部粉丝 * key: 传过来的 key * value: 用,分割 */ static class MyReducer_Step1 extends Reducer&lt;Text, Text, Text, Text&gt; &#123; Text k = new Text(); Text v = new Text(); @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Reducer&lt;Text, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; // 注意: 这里 sb 不能写在外面,会不断的拼接 StringBuilder sb = new StringBuilder(); for (Text v : values) &#123; sb.append(v.toString()).append(","); &#125; k.set(key); v.set(sb.substring(0, sb.length() - 1)); context.write(k, v); &#125; &#125; /** * @author shixuanji * 拿到的数据: a-b c */ static class MyReducer_Step2 extends Reducer&lt;Text, Text, Text, Text&gt; &#123; Text k = new Text(); Text v = new Text(); @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Reducer&lt;Text, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; StringBuilder sb = new StringBuilder(); for (Text attention : values) &#123; sb.append(attention.toString()).append(","); &#125; k.set(key); v.set(sb.substring(0, sb.length() - 1)); context.write(k, v); &#125; &#125;&#125;// job1的输出A F,I,O,K,G,D,C,H,BB E,J,F,AC B,E,K,A,H,G,FD H,C,G,F,E,A,K,LE A,B,L,G,M,F,D,HF C,M,L,A,D,GG MH OI O,CJ OK O,BL D,EM E,FO A,H,I,J,F// job2的输出A-B E,CA-C D,FA-D F,EA-E C,D,BA-F O,B,E,D,CA-G E,F,D,CA-H O,E,D,CA-I OA-J B,OA-K D,CA-L D,F,EA-M E,FB-C AB-D E,AB-E CB-F A,E,CB-G C,A,EB-H A,E,CB-I AB-K C,AB-L EB-M EB-O A,KC-D A,FC-E DC-F D,AC-G F,A,DC-H D,AC-I AC-K A,DC-L F,DC-M FC-O I,AD-E LD-F E,AD-G A,F,ED-H E,AD-I AD-K AD-L F,ED-M F,ED-O AE-F C,B,M,DE-G C,DE-H C,DE-J BE-K D,CE-L DF-G A,D,C,EF-H A,E,C,D,OF-I O,AF-J O,BF-K C,A,DF-L E,DF-M EF-O AG-H A,C,D,EG-I AG-K C,A,DG-L D,E,FG-M F,EG-O AH-I O,AH-J OH-K A,D,CH-L E,DH-M EH-O AI-J OI-K AI-O AK-L DK-O AL-M F,E 求学生成绩题目1234567891011121314151617181920212223242526272829303132computer,huangxiaoming,85,86,41,75,93,42,85computer,xuzheng,54,52,86,91,42computer,huangbo,85,42,96,38english,zhaobenshan,54,52,86,91,42,85,75english,liuyifei,85,41,75,21,85,96,14algorithm,liuyifei,75,85,62,48,54,96,15computer,huangjiaju,85,75,86,85,85english,liuyifei,76,95,86,74,68,74,48english,huangdatou,48,58,67,86,15,33,85algorithm,huanglei,76,95,86,74,68,74,48algorithm,huangjiaju,85,75,86,85,85,74,86computer,huangdatou,48,58,67,86,15,33,85english,zhouqi,85,86,41,75,93,42,85,75,55,47,22english,huangbo,85,42,96,38,55,47,22algorithm,liutao,85,75,85,99,66computer,huangzitao,85,86,41,75,93,42,85math,wangbaoqiang,85,86,41,75,93,42,85computer,liujialing,85,41,75,21,85,96,14,74,86computer,liuyifei,75,85,62,48,54,96,15computer,liutao,85,75,85,99,66,88,75,91computer,huanglei,76,95,86,74,68,74,48english,liujialing,75,85,62,48,54,96,15math,huanglei,76,95,86,74,68,74,48math,huangjiaju,85,75,86,85,85,74,86math,liutao,48,58,67,86,15,33,85english,huanglei,85,75,85,99,66,88,75,91math,xuzheng,54,52,86,91,42,85,75math,huangxiaoming,85,75,85,99,66,88,75,91math,liujialing,85,86,41,75,93,42,85,75english,huangxiaoming,85,86,41,75,93,42,85algorithm,huangdatou,48,58,67,86,15,33,85algorithm,huangzitao,85,86,41,75,93,42,85,75 一、数据解释 数据字段个数不固定：第一个是课程名称，总共四个课程，computer，math，english，algorithm，第二个是学生姓名，后面是每次考试的分数 二、统计需求：1、统计每门课程的参加考试人数和课程平均分 2、统计每门课程参考学生的平均分，并且按课程存入不同的结果文件，要求一门课程一个结果文件 3、求出每门课程参考学生成绩最高平均分的学生的信息：课程，姓名和平均分 答案第1小题统计每门课程的参考人数和课程平均分 涉及知识点: 去重， 自定义类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177//  ScoreBean package com.rox.mapreduce.mr3._02_分组组件;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.WritableComparable;import lombok.AllArgsConstructor;import lombok.Getter;import lombok.NoArgsConstructor;import lombok.Setter;@Getter@Setter@AllArgsConstructor@NoArgsConstructorpublic class ScoreBean implements WritableComparable&lt;ScoreBean&gt; &#123; private String courseName; private String stuName; private Double score; @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(courseName); out.writeUTF(stuName); out.writeDouble(score); &#125; @Override public void readFields(DataInput in) throws IOException &#123; this.courseName = in.readUTF(); this.stuName = in.readUTF(); this.score = in.readDouble(); &#125; @Override /** * 如果是相同课程, 按照分数降序排列的 * 如果是不同课程, 按照课程名称升序排列 */ public int compareTo(ScoreBean o) &#123; // 测试一下只写按分数降序排序// return o.getScore().compareTo(this.getScore()); /*// 首先分组(只在相同的组内进行比较) int nameRes = this.getCourseName().compareTo(o.getCourseName()); if (nameRes == 0) &#123; // 课程相同的时候才进行降序排序 int scoreRes = return scoreRes; &#125; return nameRes;*/ return 0; &#125; public String toString1() &#123; return stuName + "\t" + score; &#125; @Override public String toString() &#123; return courseName + "\t" + stuName + "\t" + score; &#125; public ScoreBean(String stuName, Double score) &#123; super(); this.stuName = stuName; this.score = score; &#125;&#125;//  ScorePlusDemo1 package com.rox.mapreduce.mr3._02_分组组件;import java.io.IOException;import java.util.HashSet;import java.util.Set;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class ScorePlusDemo1 &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://cs1:9000"); System.setProperty("HADOOP_USER_NAME", "ap"); Job job = Job.getInstance(conf); job.setJarByClass(ScorePlusDemo1.class); job.setMapperClass(MyMapper.class); job.setReducerClass(MyReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(ScoreBean.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); String inP = "/in/newScoreIn"; String outP = "/out/ans1"; FileInputFormat.setInputPaths(job, new Path(inP)); FileOutputFormat.setOutputPath(job, new Path(outP)); Path mypath = new Path(outP); FileSystem hdfs = mypath.getFileSystem(conf); if (hdfs.isDirectory(mypath)) &#123; hdfs.delete(mypath, true); &#125; Boolean waitForComp = job.waitForCompletion(true); System.exit(waitForComp?0:1); &#125; static class MyMapper extends Mapper&lt;LongWritable, Text, Text, ScoreBean&gt; &#123; Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1.截取 String[] datas = value.toString().trim().split(","); String courseName = datas[0].trim(); String stuName = datas[1].trim(); int sum = 0; for (int i=2; i&lt;datas.length; i++) &#123; sum += Integer.parseInt(datas[i]); &#125; double avgScore = sum/(datas.length-2); ScoreBean sb = new ScoreBean(courseName, stuName, avgScore); k.set(courseName); context.write(k, sb); &#125; &#125; static class MyReducer extends Reducer&lt;Text, ScoreBean, Text, Text&gt; &#123; Text v = new Text(); @Override protected void reduce(Text key, Iterable&lt;ScoreBean&gt; values, Reducer&lt;Text, ScoreBean, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; Set&lt;String&gt; stuNames = new HashSet&lt;&gt;(); int count = 0; int sum = 0; for (ScoreBean sb : values) &#123; stuNames.add(sb.getStuName()); count ++; sum += sb.getScore(); &#125; int size = stuNames.size(); String val = size + "\t" + (double)sum/count; v.set(val); context.write(key, v); &#125; &#125;&#125;// 执行结果 algorithm 6 71.33333333333333computer 10 69.6english 8 66.0math 7 72.57142857142857 第2小题统计每门课程参考学生的平均分，并且按课程存入不同的结果文件，要求一门课程一个结果文件 涉及知识点： 分区, 字符串组合key， Partitioner 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207package com.rox.mapreduce.mr3._02_分组组件;import java.io.IOException;import java.util.HashMap;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.DoubleWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Partitioner;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;/** * @author shixuanji * 注意: 此题因为数据中有2条 course 和 stuName相同的数据(english liuyifei), 所以必须再在reduce中继续去重一下, 再计算一下平均分 * * 否则, 可以不用写reduce, 因为Mapper中已经把逻辑处理完了,可以直接输出 * 最终输出: * computer liuyifei 43 * computer huanglei 63 * math liutao 64 * ... */public class ScorePlusDemo2 &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 指定HDFS相关参数 Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://cs1:9000"); System.setProperty("HADOOP_USER_NAME", "ap"); //  创建/配置 Job Job job = Job.getInstance(conf); // 设置Jar包类型 job.setJarByClass(ScorePlusDemo2.class); // 设置Map Reduce执行类 job.setMapperClass(MyMapper.class); job.setReducerClass(MyReducer.class); // 设置Map输出类 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(DoubleWritable.class); // Reduce输出类 job.setOutputKeyClass(Text.class); job.setOutputValueClass(DoubleWritable.class); //  设置分区  job.setPartitionerClass(MyPartition.class); job.setNumReduceTasks(4); // 设置输入 输出路径 String inP = "/in/newScoreIn"; String outP = "/out/scorePlus2"; FileInputFormat.setInputPaths(job, new Path(inP)); FileOutputFormat.setOutputPath(job, new Path(outP)); // 设置如果存在路径就删除 Path mypath = new Path(outP); FileSystem hdfs = mypath.getFileSystem(conf); if (hdfs.isDirectory(mypath)) &#123; hdfs.delete(mypath, true); &#125; //  执行job boolean waitForCompletion = job.waitForCompletion(true); System.exit(waitForCompletion?0:-1); &#125; =============================================================== static class MyMapper extends Mapper&lt;LongWritable, Text, Text, DoubleWritable&gt; &#123; // 把 课程+学生 作为 key Text k = new Text(); //只有输出String类型的, 才需要在这里设置Text DoubleWritable v = new DoubleWritable(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] datas = value.toString().trim().split(","); String kStr = datas[0].trim() + "\t" + datas[1].trim(); int sum = 0; for (int i = 2; i &lt; datas.length; i++) &#123; sum += Integer.parseInt(datas[i]); &#125; double avg = sum / (datas.length - 2); k.set(kStr); v.set(avg); context.write(k, v); &#125; &#125;=============================================================== /** * @author shixuanji * 注意: 此题因为数据中有2条 course 和 stuName相同的数据, 所以必须再在reduce中 * 继续去重一下, 再计算一下平均分 * * 否则, 可以不用写reduce, 因为Mapper中已经把逻辑处理完了,可以直接输出 */ static class MyReducer extends Reducer&lt;Text, DoubleWritable, Text, DoubleWritable&gt; &#123; DoubleWritable v = new DoubleWritable(); @Override protected void reduce(Text key, Iterable&lt;DoubleWritable&gt; values, Reducer&lt;Text, DoubleWritable, Text, DoubleWritable&gt;.Context context) throws IOException, InterruptedException &#123; /** * 考虑到有 课程, 学生名相同, 后面的数据不同的情况, 这里再做一个平均求和 * 可以验证打印下 */ int count = 0; Double sum = 0.0; for (DoubleWritable avg : values) &#123; if (count &gt; 0) &#123; // 有key完全相同的情况才会进到这里 System.out.println("这是第" +count +"次, 说明课程和姓名有相同的两条数据\n课程姓名是: "+key.toString()); &#125; sum += avg.get(); count ++; &#125; Double finAvg = sum/count; v.set(finAvg); context.write(key, v); &#125; &#125;&#125;==============================================================================================================================/** * @author shixuanji * 继承 Partitioner, 实现自定义分区 */class MyPartition extends Partitioner&lt;Text, DoubleWritable&gt; &#123; private static HashMap&lt;String, Integer&gt; courseMap = new HashMap&lt;&gt;(); static &#123; courseMap.put("algorithm", 0); courseMap.put("computer", 1); courseMap.put("english", 2); courseMap.put("math", 3); &#125; @Override public int getPartition(Text key, DoubleWritable value, int numPartitions) &#123; // 取出Map输出的key中的前半部分--courseName Integer code = courseMap.get(key.toString().trim().split("\t")[0]); if (code != null) &#123; return code; &#125; return 5; &#125;&#125;=============================================================== ===============================================================  执行结果 algorithm huangdatou 56.0algorithm huangjiaju 82.0algorithm huanglei 74.0algorithm huangzitao 72.0algorithm liutao 82.0algorithm liuyifei 62.0----------computer huangbo 65.0computer huangdatou 56.0computer huangjiaju 83.0computer huanglei 74.0computer huangxiaoming 72.0computer huangzitao 72.0computer liujialing 64.0computer liutao 83.0computer liuyifei 62.0computer xuzheng 65.0---------english huangbo 55.0english huangdatou 56.0english huanglei 83.0english huangxiaoming 72.0english liujialing 62.0english liuyifei 66.5english zhaobenshan 69.0english zhouqi 64.0------------math huangjiaju 82.0math huanglei 74.0math huangxiaoming 83.0math liujialing 72.0math liutao 56.0math wangbaoqiang 72.0math xuzheng 69.0 第3小题求出 每门课程①参与考试的学生成绩 最高平局分② 的学生的信息：课程，姓名和平均分 解题思路： 通过题意得出2个结论 课程要分组 平均分要排序 排序的话，交给key来做无疑是最好的，因为MapReduce会自动对key进行分组&amp;排序 因此可以把 课程&amp;平均分 作为一个联合key 为了操作方便，可以封装到一个对象中去： ScoreBean 分组和排序需要在 ScoreBean重写的compareTo()方法中完成 因为最后结果是求每门课程的最高平均分，因此需要对课程进行分组。 此时原本的默认分组（以Bean对象整体分组）就不管用了，需要自定义分组 自定义分组要继承WritableComparator，重写compare()方法，指定分组的规则。 ScoreBean先按照组别进行排序，到reduce中时，已经是按照组，排好的数据，MyGroup 会把相同的比较结果放到同一个组中，分发到reduce. reduce中，只需要取出每组的第一个元素输出到上下文即可 图示 涉及知识点： mr中key的作用，自定义对象的用法，自定义分组，mr的执行流程 利用“班级和平均分”作为 key，可以将 map 阶段读取到的所有学生成绩数据按照班级 和成绩排倒序，发送到 reduce 在 reduce 端利用 GroupingComparator 将班级相同的 kv 聚合成组，然后取第一个即是最 大值 先贴个结论：执行流程结论 map每读一行就 write 到 context 一次，按照指定的key进行分发 map 把所有的数据都读完了之后，大概执行到67%的时候，开始进入 CustomBean，执行CustomBean的compareTo()方法，会按照自己写的规则一条一条数据比较 上述都比较完毕之后，map阶段就结束了，此时来到了 reduce阶段，但是是到了67%了 到了reduce阶段，直接进入了MyGroup中自定义的compare方法。 MyGroup的compare()方法，如果返回非0， 就会进入 reduce 方法写出到context MyGroup进入Reduce的条件是 MyReduce中，如果compare的结果不等于0，也就是比较的2者不相同， 此时就进入Reduce， 写出到上下文 如果相同，会一直往下读，直到读到不同的， 此时写出读到上下文。 因为MyGroup会在Reduce阶段执行，而CustomBean中的compareTo()是在map阶段执行，所以需要在CustomBean中就把组排好序，此时分组功能才能正常运作 指定分组类MyGroup和不指定的区别 指定与不指定是指：在Driver类中，是否加上job.setGroupingComparatorClass(MyGrouper.class);这一句。 指定分组类： 会按照分组类中，自定义的compare()方法比较，相同的为一组，分完一组就进入一次reduce方法 不指定分组类：（目前存疑） 是否是按照key进行分组 如果是自定义类为key，是否是按照此key中值相同的分为一组 如果是hadoop内置类，是否是按照此类的值分组（Text-String的值，IntWritable-int值等..） 依然是走得以上这套分组逻辑，一组的数据读完才进入到Reduce阶段做归并 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231// ScoreBean2 package com.rox.mapreduce.mr3._02_分组组件;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.WritableComparable;import lombok.Getter;import lombok.Setter;@Getter@Setterpublic class ScoreBean2 implements WritableComparable&lt;ScoreBean2&gt; &#123; private String courseName; private Double score; @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(courseName); out.writeDouble(score); &#125; @Override public void readFields(DataInput in) throws IOException &#123; this.courseName = in.readUTF(); this.score = in.readDouble(); &#125; @Override /** * 如果是相同课程, 按照分数降序排列的 * 如果是不同课程, 按照课程名称升序排列 */ public int compareTo(ScoreBean2 o) &#123; // 测试一下只写按分数降序排序// return o.getScore().compareTo(this.getScore()); // 首先分组(只在相同的组内进行比较) int nameRes = this.getCourseName().compareTo(o.getCourseName()); if (nameRes == 0) &#123; // 课程相同的时候才进行降序排序 int scoreRes = o.getScore().compareTo(this.getScore()); return scoreRes; &#125; return nameRes; &#125; /** * 实际上ScoreBean中是包含所有的参数的, 这里的输出可以自己设置 */ @Override public String toString() &#123; return courseName + "\t" + score; &#125; public ScoreBean2(String courseName, Double score) &#123; super(); this.courseName = courseName; this.score = score; &#125; public ScoreBean2() &#123; super(); &#125;&#125;// ScorePlusDemo3 package com.rox.mapreduce.mr3._02_分组组件;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class ScorePlusDemo3 &#123;  main  public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://cs1:9000"); System.setProperty("HADOOP_USER_NAME", "ap"); Job job = Job.getInstance(conf); job.setJarByClass(ScorePlusDemo3.class); job.setMapperClass(MyMapper.class); job.setReducerClass(MyReducer.class); job.setMapOutputKeyClass(ScoreBean2.class); job.setMapOutputValueClass(Text.class); job.setGroupingComparatorClass(MyGrouper.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); String outP = "/out/scorePlus3"; FileInputFormat.setInputPaths(job, new Path("/in/newScoreIn")); FileOutputFormat.setOutputPath(job, new Path(outP)); // 如果输出目录存在,就先删除 Path myPath = new Path(outP); FileSystem fs = myPath.getFileSystem(conf); if (fs.isDirectory(myPath)) &#123; fs.delete(myPath, true); &#125; boolean waitForCompletion = job.waitForCompletion(true); System.exit(waitForCompletion ? 0 : -1); &#125; Mapper  /** * @author shixuanji * 输出: key: course * value: score ... * 思路: * 1.不同课程要分开展示, 以 课程+分数 作为key, 在mapper中完成排序 * 2.在reduce中按照 MyGrouper 完成分组 */ static public class MyMapper extends Mapper&lt;LongWritable, Text, ScoreBean2, Text&gt; &#123; private String[] datas; Text v = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; datas = value.toString().trim().split(","); int sum = 0; for (int i = 2; i &lt; datas.length; i++) &#123; sum += Integer.parseInt(datas[i]); &#125; double avg = (double) sum / (datas.length - 2); ScoreBean2 sb = new ScoreBean2(datas[0].trim(), avg); v.set(datas[1].trim()); context.write(sb, v); &#125; &#125; Redecer  static public class MyReducer extends Reducer&lt;ScoreBean2, Text, Text, NullWritable&gt; &#123; Text k = new Text(); int count = 1; @Override protected void reduce(ScoreBean2 key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; /** * 如果没有其它问题 * 此时是按照课程分好组了, 同一个课程的所有学生都过来了, 并且学生成绩是排好的, * 如果此时求最大值, 只需要取出第一个即可 */ // 进来一次只取第一个 Text name = values.iterator().next(); k.set(key.getCourseName() + "\t" + name.toString() + "\t" + key.getScore()); context.write(k, NullWritable.get()); context.write(new Text("==================第"+count+"次进入reduce"), NullWritable.get()); /*context.write(new Text("==================第"+count+"次进入reduce"), NullWritable.get()); for (Text name : values) &#123; k.set(key.getCourseName() + "\t" + name.toString() + "\t" + key.getScore()); context.write(k, NullWritable.get()); context.write(new Text("---------in for write------"), NullWritable.get()); &#125;*/ count++; &#125; &#125;&#125; MyGrouper /** * @author shixuanji * 自定义分组 需要继承一个类WritableComparator * 重写compare方法 */class MyGrouper extends WritableComparator &#123; // WritableComparator 此方法的默认无参构造是不会创建对象的, 需要自己重写 public MyGrouper() &#123; // 中间省去的参数是 Configuration, 如果为空, 会创建一个新的 super(ScoreBean2.class, true); &#125; /** * 此处比较的是2个 WritableComparable 对象, 需要强转一下具体的类对象 */ @SuppressWarnings("rawtypes") @Override public int compare(WritableComparable a, WritableComparable b) &#123; ScoreBean2 aBean = (ScoreBean2) a; ScoreBean2 bBean = (ScoreBean2) b; // 返回分组规则 System.out.println(aBean.getCourseName()+"---MyGroup中比较---"+(bBean.getCourseName())); return aBean.getCourseName().compareTo(bBean.getCourseName()); &#125;&#125;================================================================================ 执行结果 ================================================================================algorithm huangjiaju 82.28571428571429==================第1次进入reducecomputer huangjiaju 83.2==================第2次进入reduceenglish huanglei 83.0==================第3次进入reducemath huangxiaoming 83.0==================第4次进入reduce MR实现两个表的数据关联Join题目 订单数据表t_order： flag=0id date pid amount1001 20150710 P0001 21002 20150710 P0001 31003 20150710 P0002 3Id:数据记录idDate 日期Pid 商品idAmount 库存数量 6.商品信息表t_product flag=1pid name category_id priceP0001 小米5 C01 2000P0002 锤子T1 C01 3500 mr实现两个表的数据关联id pid date amount name category_id price 答案1 : Reducer 端 实现 Join思路 map端 读取到当前路径下，所有文件的切片信息， 根据文件名判断是那张表 在setup中，从文件切片中获取到文件名 123456// 获取读取到的切片相关信息,一个切片对应一个 maptaskInputSplit inputSplit = context.getInputSplit();// 转换为文件切片FileSplit fs = (FileSplit)inputSplit;// 获取文件名filename = fs.getPath().getName(); 这里总共会获得2个文件名（指定目录存了2个指定文件），一个文件名对应一个切片 关联字段作为key， 其它的作为value，在value前面加上当前文件的名称标记 reduce端 通过标记区分两张表，把读取到的信息，分别存入2个list中 遍历大的表，与小表进行拼接（小表的相同pid记录只会有一条） 拼接完成后即可写出 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161package com.rox.mapreduce.mr3._03_join2表的数据关联;import java.io.IOException;import java.util.ArrayList;import java.util.List;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.FileSplit;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class ReduceJoinDemo &#123; public static void main(String[] args) throws Exception &#123; // 指定HDFS相关参数 Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://cs1:9000"); System.setProperty("HADOOP_USER_NAME", "ap"); //  创建/配置 Job Job job = Job.getInstance(conf); // 设置Jar包类型 job.setJarByClass(ReduceJoinDemo.class); // 设置Map Reduce执行类 job.setMapperClass(MyMapper.class); job.setReducerClass(MyReducer.class); // 设置Map输出类 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); // Reduce输出类 job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // 设置输入 输出路径 String inP = "/in/joindemo"; String outP = "/out/joinout1"; FileInputFormat.setInputPaths(job, new Path(inP)); FileOutputFormat.setOutputPath(job, new Path(outP)); // 设置如果存在路径就删除 Path mypath = new Path(outP); FileSystem hdfs = mypath.getFileSystem(conf); if (hdfs.isDirectory(mypath)) &#123; hdfs.delete(mypath, true); &#125; //  执行job boolean waitForCompletion = job.waitForCompletion(true); System.exit(waitForCompletion?0:-1); &#125; /** * @author shixuanji * 思路: 读取2个表中的数据,进行标记发送 * key: 两表需要关联的字段 * value: 其它值, 需要标记， 标记数据的来源 * * * **核心： 关联条件** - 想要在 reduce 端完成 join， 要在 reduce 端可以同时接收到两个表中的数据 - 要保证在 Map 端进行读文件的时候， 读到2个表的数据， 并且需要对2个表的数据进行区分 - 将2个表放在同一个目录下 解决: mapper 开始执行时, 在setup方法中, 从上下文中取到文件名, 根据文件名打标记 */ static class MyMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt; &#123; String filename = ""; Text k = new Text(); Text v = new Text(); @Override protected void setup( Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; // 获取读取到的切片相关信息,一个切片对应一个 maptask InputSplit inputSplit = context.getInputSplit(); // 转换为文件切片 FileSplit fs = (FileSplit)inputSplit; // 获取文件名 filename = fs.getPath().getName(); System.out.println("本次获取到的文件名为-----"+filename); &#125; @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; // 解析出来每一行内容, 打标记, 发送 String[] infos = value.toString().split("\t"); if (filename.equals("order")) &#123; k.set(infos[2]); // 设置标记前缀为 OR v.set("OR"+infos[0]+"\t"+infos[1]+"\t"+infos[3]); &#125;else &#123; k.set(infos[0]); // 设置标记前缀为 PR v.set("PR"+infos[1]+"\t"+infos[2]+"\t"+infos[3]); &#125; context.write(k, v); &#125; &#125; static class MyReducer extends Reducer&lt;Text, Text, Text, NullWritable&gt; &#123; Text k = new Text(); @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Reducer&lt;Text, Text, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; /** * 首先明确 product 和 order 是 一对多的关系 * 根据前缀不同,取到2个不同的表存进2个容器中 * 遍历多的表, 与一进行拼接 * 最后写出到上下文 * 最终的输出格式 id pid date amount name category_id price */ // 因为每次遍历到不同的pid, 都会走进来一次, list也会有新的输出,所以必须定义在里面,每次进来都要初始化 List&lt;String&gt; productList =new ArrayList&lt;&gt;(); List&lt;String&gt; orderList =new ArrayList&lt;&gt;(); for (Text v : values) &#123; String vStr = v.toString(); if (vStr.startsWith("OR")) &#123; orderList.add(vStr.substring(2)); &#125;else &#123; productList.add(vStr.substring(2)); &#125; &#125; // 此时2个list添加完了本次 相同的 key(pid) 的所有商品 // 遍历多的进行拼接 for (String or : orderList) &#123; // 相同的 pid的 product 只有一个, productList中的数量是1 // 但是相同pid 的 订单 可能有多个 String res = key.toString() + "\t" + or + productList.get(0); k.set(res); context.write(k, NullWritable.get()); &#125; &#125; &#125;&#125; ※ 答案2 ： Mapper 端实现 Join ※思路 创建job的时候,把小表加入缓存 在map的setup中, 读取缓存中的数据, 存入一个成员变量 map中 map方法中,只需要读一个表, 然后根据关联条件(关联key: pid)消除笛卡尔集,进行拼接 map直接输出, 甚至都不需要reduce 注意点: 需要达成jar包运行, 直接用Eclipse会找不到缓存 jar包执行方法 12# 如果代码内部指定了输入输出路径，后面的/in，/out参数可以不加hadoop jar xxxx.jar com.rox.xxx.xxxx(主方法) /in/xx /out/xx 如果没有Reduce方法 main方法中，设置map的写出key，value,应该用 setOutputKeyClass 123//// 设置Map输出类 (因为这里没有Reduce, 所以这里是最终输出,一定要注意!!!)//////////job.setOutputKeyClass(Text.class);job.setOutputValueClass(NullWritable.class); 要设置reduce task 的个数为0 1job.setNumReduceTasks(0); 把小文件加载到缓存中的方法 12////////////// 将小文件加载到缓存 job.addCacheFile(new URI("/in/joindemo/product")); ​ 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120package com.rox.mapreduce.mr3._03_join;import java.io.BufferedReader;import java.io.FileReader;import java.io.IOException;import java.net.URI;import java.util.HashMap;import java.util.Map;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class MapJoinDemo &#123; public static void main(String[] args) throws Exception &#123; // 指定HDFS相关参数 Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://cs1:9000"); System.setProperty("HADOOP_USER_NAME", "ap"); //  创建/配置 Job Job job = Job.getInstance(conf); // 设置Jar包类型:这里千万别写错了 job.setJarByClass(MapJoinDemo.class); // 设置Map Reduce执行类 job.setMapperClass(MyMapper.class); ///////////// 设置Map输出类 (因为这里没有Reduce, 所以这里是最终输出,一定要注意!!!)////////// job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); ////////////// 设置reduce执行个数为0 job.setNumReduceTasks(0); ////////////// 将小文件加载到缓存 job.addCacheFile(new URI("/in/joindemo/product")); // 设置输入 输出路径 String inP = "/in/joindemo/order"; String outP = "/out/joinout2"; FileInputFormat.setInputPaths(job, new Path(inP)); FileOutputFormat.setOutputPath(job, new Path(outP)); // 设置如果存在路径就删除 Path mypath = new Path(outP); FileSystem hdfs = mypath.getFileSystem(conf); if (hdfs.isDirectory(mypath)) &#123; hdfs.delete(mypath, true); &#125; //  执行job System.exit(job.waitForCompletion(true)?0:-1); &#125; /** * @author shixuanji * 思路: * 创建job的时候,把小表加入缓存 * 在map的setup中, 读取缓存中的数据, 存入一个成员变量 map中 * map方法中,只需要读一个表, 然后根据关联条件(关联key: pid)消除笛卡尔集,进行拼接 * 直接输出, 甚至都不需要reduce * * 注意点: * 需要达成jar包运行, 直接用Eclipse会找不到缓存 * 格式: hadoop jar包本地路径 jar包主方法全限定名 hadoop输入 hadoop输出 */ static class MyMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123; // 创建装载小表的map, key存储 关联键, value存其它 Map&lt;String, String&gt; proMap = new HashMap&lt;&gt;(); @Override protected void setup(Context context) throws IOException, InterruptedException &#123; // 获取缓存中存储的小表 (一般是 一对多中的 一), 因为只存了1个,所以直接取第0个 Path path = context.getLocalCacheFiles()[0]; String pString = path.toString(); // 开启in流, BufferedReader 逐行读取文件 BufferedReader br = new BufferedReader(new FileReader(pString)); String line = null; while ((line = br.readLine()) != null) &#123; // 成功读取一行 String[] infos = line.split("\t"); // 存进proMap proMap.put(infos[0], infos[1] + "\t" + infos[2] + "\t" + infos[3]); &#125;// br.close(); &#125; /** * 直接从路径读取大文件 */ Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] infos = value.toString().split("\t"); String pid = infos[2]; //进行关联 pid到map中匹配 如果包含 证明匹配上了 // 艹, 这里pid之前加了 "", 妈的,当然找不到啦!!! if (proMap.containsKey(pid)) &#123; String res = value.toString() + "\t" + proMap.get(pid); k.set(res); context.write(k, NullWritable.get()); &#125; &#125; &#125;&#125; title: 执行流程时序图  Mapper(map)->ScoreBean: k:ScoreBean(courseName,avgScore) Mapper(map)->ScoreBean: v:Text(stuName) Mapper(ScoreBean)->Reducer(MyGroup): course按字典升序 Mapper(ScoreBean)->Reducer(MyGroup): course内成绩降序 Reducer(MyGroup)->Reducer(reduce): 根据自定义的分组规则按组输出 Reducer(MyGroup)->Reducer(reduce): 一组只调用reduce一次{"theme":"simple"} var code = document.getElementById("sequence-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value)); var diagram = Diagram.parse(code); diagram.drawSVG("sequence-0", options);]]></content>
      <categories>
        <category>Hadoop</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce笔记-3]]></title>
    <url>%2F2018%2F06%2F08%2FHadoop%2FStudy%2F2-MapReduce%2FMapReduce%E7%AC%94%E8%AE%B0-3%2F</url>
    <content type="text"><![CDATA[1.多 Job 串联1.概念当程序中有多个 Job， 并且多个 job 之间相互依赖， a ， job 需要依赖另一个b，job 的执行结果时候， 此时需要使用多 job 串联 2. 涉及到昨天的微博求共同粉丝题目 A:B,C,D,F,E,OB:A,C,E,KC:F,A,D,ID:A,E,F,LE:B,C,D,M,LF:A,B,C,D,E,O,MG:A,C,D,E,FH:A,C,D,E,OI:A,OJ:B,OK:A,C,DL:D,E,FM:E,F,GO:A,H,I,J,K 以上是数据：A:B,C,D,F,E,O表示：A用户 关注B,C,D,E,F,O 求所有两两用户之间的共同关注对象 注意 要写2个MapReduce， 开启2个job 后一个job依赖于前一个的执行结果 后一个job的输入文件路径，就是前一个job的输出路径 2个job需要添加依赖 代码参考练习-第一题-求微博共同好友 多 Job 串联部分代码 基本的写到一起， job1， job2 用JobControl对象管理多 job， 会将多个 job 当做一个组中的 job 提交， 参数指的是组名， 随意起 原生的 job 要转为可控制的 job 123456789101112131415161718// 创建 JobControl 组JobControl jc = new JobControl("common_friend");// job 拿好配置， 加入 ControlledJob 管理, 变成可控制的 jobControlledJob ajob = new ControlledJob(job1.getConfiguration());ControlledJob bjob = new ControlledJob(job2.getConfiguration());// 添加依赖关系bjob.addDependingJob(ajob); // 添加 job进 JCjc.addJob(ajob);jc.addJob(bjob);// 启动线程Thread jobControlTread = new Thread(jc);jobControlTread.start();// 在线程完成之后关闭while(!jc.allFinished()) &#123; Thread.sleep(500);&#125;jobControl.stop(); 2. 分组组件map–分组–reduce reduce 接收到的数据是按照 map 输出的 key 进行分组的, 分组的时候按照 key 相同的时候为一组, 默认都实现了 WritableComparable接口， 其中的 compareTo（）方法返回为0的时候 默认为一组， 返回不为0， 则分到下一组 自定义分组使用场景： 默认的数据分组不能满足需求 一、数据解释 数据字段个数不固定：第一个是课程名称，总共四个课程，computer，math，english，algorithm，第二个是学生姓名，后面是每次考试的分数 二、统计需求：1、统计每门课程的参考人数和课程平均分 2、统计每门课程参考学生的平均分，并且按课程存入不同的结果文件，要求一门课程一个结果文件 3、求出每门课程参考学生成绩最高平均分的学生的信息：课程，姓名和平均分 第三题： 要求就是分组求最大值， 两件事情： 分组， 排序（shuffle） 总结： 1、利用“班级和平均分”作为 key，可以将 map 阶段读取到的所有学生成绩数据按照班级 和成绩排倒序，发送到 reduce 2、在 reduce 端利用 GroupingComparator 将班级相同的 kv 聚合成组，然后取第一个即是最 大值 具体参考：练习-求学生成绩-第三小题 3. Reduce 中的2个坑坑1Iterable\只能循环遍历一次 迭代器每次循环遍历完成， 指针都会移动到最后一个 系统类型，没事 自定义类型 ，有问题？ 坑2迭代器中所有对象公用同一个地址 4. Reduce 端的 Join牺牲效率换执行 思路： 核心： 关联条件 想要在 reduce 端完成 join， 要在 reduce 端可以同时接收到两个表中的数据 要保证在 Map 端进行读文件的时候， 读到2个表的数据， 并且需要对2个表的数据进行区分 将2个表放在同一个目录下 Map 端 读取两个表中的数据， 进行切分、发送 key ： 公共字段–关联字段–pid value： 剩下的字段， 标记数据的来源表 Reduce 端 通过编辑分离出2个表的数据 分别存到2个容器中（ArrayList） 遍历大表，拼接小表 代码参考练习-第三题MR实现2个表之间的Join 缺陷1. ReduceTask 的并行度问题： 建议0.95*datanode 的个数 并行度不高， 性能不高 2. 容器性能 list 等， 不提倡， reduce 接收的数据， 可能会很大 3. ReduceTask 容易产生数据倾斜 假设我们设置多个 ReduceTask， 根据分区规则， 默认 hash 以 key关联条件分， ReduceTask数据倾斜， 每个 ReduceTask 分工不均， 非常影响性能，没有合理的利用集群资源 在真实的生产中一定要尽量的避免数据倾斜 最好的做法：将分区设计的足够完美，难度比较大 因此，ReduceTask 一般不会完成 John工作 放在 Map 端完成就不会有这个问题了 补充：Mapper 中的源码分析123456789101112131415public void run(Context context) throws IOException, InterruptedException &#123; // 在 maptask 执行之前调用一次， 一个 maptask 只会调用一次。setup 中通常会帮 map 中初始化一些变量和资源， 比如数据库的连接等。 // 主要目的：减少资源的初始化次数而提升程序的性能 setup(context); try &#123; // 获取文件是否还有下一行， 一行只调用一次 while (context.nextKeyValue()) &#123; map(context.getCurrentKey(), context.getCurrentValue(), context); &#125; &#125; finally &#123; // maptask 任务执行完成之后会调用一次，一个 maptask 只会调用一次 // 帮 map 处理一些善后工作， 比如：资源的关闭 cleanup(context); &#125; &#125; 5. Map 端的 Join注意点：这种方式只能通过 Jar 包上传的方式，直接用 Eclipse 会找不到缓存 为了提升 Map 端 Join 性能， 我们的策略是， 将小表的数据加载到每个运行的 MapTask 的内存中。 如果小表被加载到了内存中， 我们每次在 Map 端只需要读取大表，当读取到大表的每一行数据，可以直接和内存中的小表进行关联。 这个时候，只需要 Map 就可以完成 Join 操作了。 1. 如何将小表加入到内存中？12// 将指定路径文件加载到缓存中job.addCacheFile(new URI("/xxx")); 2. Map 端怎样读取缓存中的数据想要在 Java 中使用缓存中的数据，缓存中的数据必须封装到 Java 的容器中 12// 获取缓存文件context.getLocalCacheFiles()[0] 3. 代码参考练习-第3题 代码注意点： setup：从缓存读取一文件（多对一的一）到 HashMap main 方法中注意点 12345// 指定文件加入缓存job.addCacheFile(new URI("/xxx")); // 如果没有ReduceTask， 要设置为0job.setNumReduceTasks(0); 6. 对比MapJoin 的方式： 大 &amp; 小表 因为有一个表需要加载到内存中，注定加载到内存中的表不能过大（hive 中默认是256M） 大表 &amp; 大表 如何设计 ReduceJoin ： 解决数据倾斜的问题，合理设计分区。 —很难做到 将其中一个大表进行切分，切分成小表， 最终执行 大表 &amp; 小表 优点 并行度高，不存在数据倾斜的问题，运行效率高 优先选择MapJoin :arrow_forward: 7. 排序算法 （待整理：TODO）1. 快速排序边界值始终是不变的。 2. 归并排序一般情况针对有序的，多个， 小数据集 应用场景：想到了多个Reduce 任务产生的多个文件的合并 1. 归并排序前传： 合并多个数组 2. 归并排序 之 一个大数据集—归———-切分成单个的数据集 —-并——— 两两相并， 并成新的数组， 小的先放入数组， 再放大的 新的数组再不断执行 上述的 合并多个数组 8. ※ Shuffle 过程 ※ mapper 阶段处理的数据如何传递给 reducer 阶段，是 MapReduce 框架中 最关键的一个流程，这个流程就叫 Shuffle Shuffle 即 数据混洗 —— 核心机制：数据分区，排序，局部聚合，缓存，拉取，再合并，排序； 环形缓冲区 内存中的一种首尾相连的数据结构（底层是字节数组），kvbuffer 包含原始数据区和元数据区，一个mapTask任务对应一个环形缓冲区 默认大小 100M，默认阈(yu)值 0.8，即当达到 80M 后，会触发 spill溢写 操作，将数据写入磁盘，此时mapper输出会继续向剩余20M中写数据缓冲区大小 mapred-site.xml：mapreduce.task.io.sort.mb阈值 mapred-site.xml：mapreduce.map.sort.spill.percent路径：mapred-site.xml：mapreduce.cluster.local.dir 如果此80M数据写入磁盘完成前，剩余20M缓冲区也写完，则会进入阻塞状态，直到是spill完成腾出缓冲区空间 赤道（equtor）：环形缓冲区中原始数据和元数据的边界 原始数据：mapTask输出的数据 元数据 记录原始数据的数据，包含4部分内容，占16*4字节； 每一条元数据占用空间是一样的，排序可以通过交换元数据实现 分类 a. 原始数据中key的起始位置 b. 原始数据中value的起始位置 c. value的长度 d. 分区信息，即该条信息属于哪个分区 核心操作 \1. 分区 partition（如果 reduceTask 只有一个或者没有，那么 partition 将不起作用） \2. Sort 根据 key 排序（MapReduce 编程中的 sort 是一定会做的，并且 只能按照 key排序， 当然 如果没有 reducer 阶段，那么就不会对 key 排序） \3. Combiner 进行局部 value 的合并（Combiner 是可选的组件，作用只是为了提高任务的执行效率） 详细过程 \1. 一个大文件需要处理，它在在 HDFS 上是以 block 块形式存放，每个 block 默认为 128M 存 3 份；运行时每个 map 任务会处理一个切块（split），如果 block 大和 split 相同，有多少个 block 就有多少个 map 任务；所以对整个文件处理时会有很多 map 任务进行并行计算。 \2. 每个 map 任务处理完输入的切块后会把结果写入到内存的一个 环形缓冲区，写入过程中会进行简单排序，当缓冲区的大小阀值，一个后台的线程就会启动把缓冲区中的数据溢写（spill）到本地磁盘中，同时Mapper继续时向环形缓冲区中写入数据。 数据溢写入到磁盘之前，首先会根据 reducer 的数量划分成同数量的分区（partition），每个分区中的都数据会有后台线程根据 map 任务的输出结果 key 进行排序； 如果有 combiner，它会在 缓冲区溢写到磁盘之前 和 mapTask排好序的输出上 运行，使写到本地磁盘和传给 reducer 的数据更少；Combiner即是把同一分区中的同一key的数据进行合并，整个shuffle过程会调用两个Combiner ! 最后在本地生成分好区且排好序的小文件。 注意：如果 map 向环形缓冲区写入数据的速度大于向本地写入数据的速度，环形缓冲区会被写满，向环形缓冲区写入数据的线程会阻塞直至缓冲区中的内容全部溢写到磁盘后再次启动，到阀值后会向本地磁盘新建一个溢写文件； \3. map 任务完成之前，会把本地磁盘溢写的所有文件 不停地 合并（merge）成得到一个结果文件，合并得到的结果文件会根据小溢写文件的分区而分区，每个分区的数据会再次根据 key 进行 排序，得到的结果文件是分好区且排好序的（可以合并成一个文件的溢写文件数量默认为10）；默认合并溢写文件数量 mapred-site.xml：mapreduce.task.io.sort.factor \4. reduce 任务启动，Reducer 中的一个线程定期向 MRAppMaster 询问 Mapper 输出结果文件位置，Mapper 结束后会向 MRAppMaster 汇报信息，从而 Reducer 会得知 Mapper 状态并得到 map 结果文件目录；reduce任务数配置a) mapred-site.xml：mapreduce.job.reducesb) job.setNumReduceTasks(num) \6. 当有一个 Mapper 结束时，reduce 任务进入复制阶段，reduce 任务通过 http 协议（hadoop 内置了netty容器）把所有 Mapper 结果文件的 对应的分区数据 拉取（fetch）过来，Reducer 可以并行复 制 Mapper 的 结果 ， 默认线程数为5； 所有 Reducer 复制完成 map 结果文件后，由于 Reducer 可能会失败，NodeManager 并不会在第一个 map 结果文件复制完成后就删除它，而是直到作业完成后 MRAppMaster 通知 NodeManager 进行删除； 另外如果 map 结果文件相当小，则会被直接复制到 reduce NodeManager 的内存；一旦缓冲区达到 reduce 的阈值大小 0.66 或 写入到 reduce NodeManager 内 存 中 文 件 个 数 达 到 map 输出阈值 1000，reduce 就会把 map 结果文件合并（merge）溢写到本地；默认线程数 mapred-site.xml：mapreduce.reduce.shuffle.parallelcopies缓冲区大小 mapred-site.xml:mapreduce.reduce.shuffle.input.buffer.percent，默认0.7阈值：mapred-site.xml:mapreduce.reduce.shuffle.merge.percent，默认0.66map输出阈值1000：mapred-site.xml:mapreduce.reduce.merge.inmem.threshold \7. 复制阶段完成后，Reducer 进入 Merge 阶段，循环地合并 map 结果文件，维持其顺序排序，合并因子默认为 10，经过不断地 Merge 后得到一个”最终文件”，可能存储在磁盘也可能存在内存中； \8. “最终文件”输入到 reduce 进行计算，计算结果输入到 HDFS。 [ 注意 ] 溢写前会先按照分区进行排序，再按key进行排序，采用 快速排序排序是按照原始数据排序，但是由于原始数据不好移动且原始数据包含了原始数据的位置信息，所以移动的其实是元数据；写入时读的是元数据，真正写入的时原始数据 最后的数据如果不够80M，也会被强制flush到磁盘 每个mapTask任务生成的磁盘小数据最后都会merge成一个大文件，采用 归并排序 Shuffle 中的缓冲区大小会影响到 mapreduce 程序的执行效率，原则上说，缓冲区越大，磁 盘io的次数越少，执行速度就越快。 10、自定义输入 InputFormat 默认的文件加载：TextInputFormat 默认的文件读取：LineRecordReader 源码追踪过程：context –&gt; mappercontext –&gt; mapcontext –&gt; reader –&gt; input –&gt; real –&gt; inputFormat.createRecordReader（split，taskContext），然后查找 inputFormat –&gt; createRecordReader（split，taskContext），inputFormat –&gt; TextInputFormat实例对象 案例：多个小文件合并 word1.txt ~word10.txt 每次读取一个小文件 自定义输入，需要创建两个类，并通过Job对象指定自定义输入 \1. 创建XxxInputFormat类，继承FileInputFormat&lt;&gt;，重写 createRecordReader() 方法 \2. 创建XxxRecordReader类，继承RecordReader&lt;&gt;，重写以下方法： initialize()：初始化方法，类似于setup()，对属性、链接或流进行初始化 getCurrentKey()：返回key getCurrentValue()：返回value getProgress()：返回文件执行进度 nextKeyValue()：返回文件是否读取结束 close()：进行一些资源的释放 \3. 在mapreduce类的main()方法中指定自定义输入：job.setInputFormatClass(XxxInputFormat.class); 代码 123456789101112131415161718192021222324252627282930313233package com.rox.mapreduce.mr4._02_inputformat;import java.io.IOException;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.JobContext;import org.apache.hadoop.mapreduce.RecordReader;import org.apache.hadoop.mapreduce.TaskAttemptContext;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;public class WholeFileInputFormat extends FileInputFormat&lt;NullWritable, Text&gt; &#123; /** * 设置每个小文件不可分片,保证一个小文件生成一个key-v键值对 */ @Override protected boolean isSplitable(JobContext context, Path filename) &#123; return false; &#125; @Override public RecordReader&lt;NullWritable, Text&gt; createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; WholeFileRecordReader reader = new WholeFileRecordReader(); reader.initialize(split, context); return reader; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package com.rox.mapreduce.mr4._02_inputformat;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FSDataInputStream;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.RecordReader;import org.apache.hadoop.mapreduce.TaskAttemptContext;import org.apache.hadoop.mapreduce.lib.input.FileSplit;public class WholeFileRecordReader extends RecordReader&lt;NullWritable, Text&gt;&#123; private FileSplit fileSplit; private Configuration conf; private Text value = new Text(); private boolean processed = false; // 标识文件是否读取完成 /** * 初始化方法 */ @Override public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; this.fileSplit=(FileSplit)split; this.conf = context.getConfiguration(); &#125; @Override public boolean nextKeyValue() throws IOException, InterruptedException &#123; if (!processed) &#123; byte[] contents = new byte[(int)fileSplit.getLength()]; Path file = fileSplit.getPath(); FileSystem fs = file.getFileSystem(conf); FSDataInputStream in = null; try &#123; in = fs.open(file); // 把输入流上的数据全部读取到contents字节数组中 IOUtils.readFully(in, contents, 0, contents.length); // 把读取到的数据设置到value里 value.set(contents,0,contents.length); &#125; finally &#123; IOUtils.closeStream(in); &#125; processed = true; return true; &#125; return false; &#125; @Override public NullWritable getCurrentKey() throws IOException, InterruptedException &#123; return NullWritable.get(); &#125; @Override public Text getCurrentValue() throws IOException, InterruptedException &#123; return value; &#125; @Override public float getProgress() throws IOException, InterruptedException &#123; return processed ? 1.0f : 0.0f; &#125; @Override public void close() throws IOException &#123; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114package com.rox.mapreduce.mr4._02_inputformat;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.FileSplit;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;public class SmallFilesConvertToBigMR extends Configured implements Tool &#123; public static void main(String[] args) throws Exception &#123; int exitCode = ToolRunner.run(new SmallFilesConvertToBigMR(), args); System.exit(exitCode); &#125; @Override public int run(String[] args) throws Exception &#123; Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://cs1:9000"); System.setProperty("HADOOP_USER_NAME", "ap"); Job job = Job.getInstance(conf, "combine small files to bigfile"); job.setJarByClass(SmallFilesConvertToBigMR.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); job.setMapperClass(SmallFilesConvertToBigMRMapper.class); job.setReducerClass(SmallFilesConvertToBigMRReducer.class); job.setOutputKeyClass(NullWritable.class); job.setOutputValueClass(Text.class);//////// job.setInputFormatClass(WholeFileInputFormat.class); // job.setOutputFormatClass(SequenceFileOutputFormat.class); Path input = new Path("/in/joindemo"); Path output = new Path("/out/bigfile"); FileInputFormat.setInputPaths(job, input); FileSystem fs = FileSystem.get(conf); if (fs.exists(output)) &#123; fs.delete(output, true); &#125; FileOutputFormat.setOutputPath(job, output); int status = job.waitForCompletion(true) ? 0 : 1; return status; &#125; static class SmallFilesConvertToBigMRMapper extends Mapper&lt;NullWritable, Text, Text, Text&gt; &#123; private Text filenameKey; @Override protected void setup( Mapper&lt;NullWritable, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; InputSplit split = context.getInputSplit(); Path path = ((FileSplit) split).getPath(); filenameKey = new Text(path.toString()); &#125; @Override protected void map(NullWritable key, Text value, Mapper&lt;NullWritable, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; context.write(filenameKey, value); &#125; &#125; static class SmallFilesConvertToBigMRReducer extends Reducer&lt;Text, Text, NullWritable, Text&gt; &#123; @Override protected void reduce(Text filename, Iterable&lt;Text&gt; bytes, Context context) throws IOException, InterruptedException &#123; context.write(NullWritable.get(), bytes.iterator().next()); &#125; &#125;&#125; 11、自定义输出 OutputFormat 默认的文件加载：TextOutputFormat 默认的文件读取：LineRecordWriter 源码追踪过程 略 案例：将考试成绩合格的输出到一个文件夹，不及格的输出到另一个文件夹（注意，不同于分区，分区只是量结果输出到同一文件夹下不同文件） 自定义输出，需要创建两个类，并通过Job对象指定自定义输入 \1. 创建XxxOutputFormat类，继承FileOutputFormat&lt;&gt;，重写getRecordWriter()方法 \2. 创建XxxRecordWriter类，继承RecordWriter&lt;&gt;，重写以下方法： write()：真正向外写出的方法，需要将结果输出到几个不同文件夹，就需要创建几个输出流 而输出流通过FileSystem对象获取，FileSystem对象获取需要配置文件 一般可以通过构造方法直接传入FileSystem对象 close()：释放资源 \3. 在mapreduce类的main()方法中指定自定义输入：job.setOutputFormatClass(XxxOutputFormat.class); 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package com.rox.mapreduce.outputformat;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class MultipleOutputMR &#123; public static void main(String[] args) throws Exception &#123; // 指定HDFS相关参数 Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://cs1:9000"); System.setProperty("HADOOP_USER_NAME", "ap"); //  创建/配置 Job Job job = Job.getInstance(conf); // 设置Jar包类型:这里千万别写错了 job.setJarByClass(MultipleOutputMR.class); // 设置Map Reduce执行类 job.setMapperClass(MultipleOutputMRMapper.class); // 设置Map输出类 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(NullWritable.class); ////////////// 设置reduce执行个数为0 job.setNumReduceTasks(0); ///////////// 设置MapOutputFormatClass job.setOutputFormatClass(MyOutputFormat.class); // 设置输入 输出路径 String inP = "/in/newScoreIn"; String outP = "/out/myoutformat/mulWriteSuc"; FileInputFormat.setInputPaths(job, new Path(inP)); FileOutputFormat.setOutputPath(job, new Path(outP)); // 设置如果存在路径就删除 Path mypath = new Path(outP); FileSystem hdfs = mypath.getFileSystem(conf); if (hdfs.exists(mypath)) &#123; hdfs.delete(mypath, true); &#125; //  执行job System.exit(job.waitForCompletion(true)?0:-1); &#125; static class MultipleOutputMRMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; // 参考次数&gt;7次 算合格 String[] splits = value.toString().split(","); if (splits.length &gt; 9) &#123; context.write(new Text("1::"+value.toString()), NullWritable.get()); &#125;else &#123; context.write(new Text("2::"+value.toString()), NullWritable.get()); &#125; &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233package com.rox.mapreduce.outputformat;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FSDataOutputStream;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.RecordWriter;import org.apache.hadoop.mapreduce.TaskAttemptContext;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class MyOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt; &#123; @Override public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter( TaskAttemptContext job) throws IOException, InterruptedException &#123; Configuration configuration = job.getConfiguration(); FileSystem fs = FileSystem.get(configuration); Path p1 = new Path("/out/myoutformat/out1"); Path p2 = new Path("/out/myoutformat/out2"); FSDataOutputStream out1 = fs.create(p1); FSDataOutputStream out2 = fs.create(p2); return new MyRecordWriter(out1,out2); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243package com.rox.mapreduce.outputformat;import java.io.IOException;import org.apache.hadoop.fs.FSDataOutputStream;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.RecordWriter;import org.apache.hadoop.mapreduce.TaskAttemptContext;public class MyRecordWriter extends RecordWriter&lt;Text, NullWritable&gt; &#123; FSDataOutputStream fsout = null; FSDataOutputStream fsout1 = null; public MyRecordWriter(FSDataOutputStream fsout, FSDataOutputStream fsout1) &#123; super(); this.fsout = fsout; this.fsout1 = fsout1; &#125; @Override public void write(Text key, NullWritable value) throws IOException, InterruptedException &#123; String[] strs = key.toString().split("::"); if (strs[0].equals("1")) &#123; fsout.write((strs[1]+"\n").getBytes()); &#125;else &#123; fsout1.write((strs[1]+"\n").getBytes()); &#125; &#125; @Override public void close(TaskAttemptContext context) throws IOException, InterruptedException &#123; IOUtils.closeStream(fsout); IOUtils.closeStream(fsout1); &#125;&#125; 12、Yarn1、Yarn图示简介 在Hadoop1.x时， 只有两个主要组件：hdfs（文件存储）， MapReduce（计算） 所有的计算相关的全部放在MapReduce上 JobTracker: 整个计算程序的老大 资源调度：随机调度 监控程序运行的状态，启动运行程序 存在单点故障问题 TaskTracker：负责计算程序的执行 强行的将计算资源分成2部分 MapSlot ReduceSlot 每一部分资源只能跑对应的任务 缺陷： 单点故障 资源调度随机，会造成资源浪费 JobTracker的运行压力过大 Hadoop2.x： 分理出Yarn，专门负责集群的资源管理和调度 Yarn的进程：ResourceManager: 整个资源调度的老大 接受hadoop客户端的请求 接受NodeManager 的状态报告， NM的资源状态和存活状态 资源调度，整个计算程序的资源调度，调度的运行资源和节点 内部组件： ASM——ApplicationsManager 所有应用程序的管理者，负责调度应用程序 Scheduler——调度器概念 调度的是什么时候执行哪个计算程序 调度器： FIFO: first in first out 先提交的先执行，后提交的后执行 内部维护一个队列 FAIR: 公平调度器 大家平分资源运行 假设刚开始只有一个任务，占资源100%，此时又来了一个任务，这是进行资源平分，每人50% 内部也是维护一个队列 CAPICITPY: 可以按需进行配置，使用资源 内部可维护多个队列，多个队列之间可以进行资源分配 例如：分配两个队列 队列1：60% 队列2：40% 每个队列中都是执行FIFO的 NodeManager： 负责真正的提供资源，运行计算程序 接受ResourceManager的命令 提供资源运行计算程序 MRAppMaster: 单个计算程序的老大, 类似于项目经理 负责帮助当前计算程序向ResourceManager申请资源 负责启动 MapTask 和 ReduceTask 任务 Container: 抽象资源容器，封装这一定的cpu，io 和网络资源（逻辑概念） 是运行MapTask，ReduceTask等的运行资源单位 1个split —— 1个MapTask (ReduceTask) —— 1个Container —— 显示为YarnChild，底层运行的资源单位就是Container 2、Yarn运行过程 MRAppMaster会在所有的MapTask执行到0.8的时候，开启ReduceTask任务 YARN 作业执行流程: 用户向 YARN 中提交应用程序，其中包括 MRAppMaster 程序，启动 MRAppMaster 的命令， 用户程序等。 ResourceManager 为该程序分配第一个 Container，并与对应的 NodeManager 通讯，要求 它在这个 Container 中启动应用程序 MRAppMaster。 MRAppMaster 首先向 ResourceManager 注册，这样用户可以直接通过 ResourceManager 查看应用程序的运行状态，然后将为各个任务申请资源，并监控它的运行状态，直到运行结 束，重复 4 到 7 的步骤。 MRAppMaster 采用轮询的方式通过 RPC 协议向 ResourceManager 申请和领取资源。 一旦 MRAppMaster 申请到资源后，便与对应的 NodeManager 通讯，要求它启动任务。 NodeManager 为任务设置好运行环境(包括环境变量、JAR 包、二进制程序等)后，将 任务启动命令写到一个脚本中，并通过运行该脚本启动任务。 各个任务通过某个 RPC 协议向 MRAppMaster 汇报自己的状态和进度，以让 MRAppMaster 随时掌握各个任务的运行状态，从而可以在任务败的时候重新启动任务。 8、应用程序运行完成后，MRAppMaster 向 ResourceManager 注销并关闭自己。 3、Job的提交过程(待整理) 客户端向rm发送 提交job请求 rm向客户端发送 共享资源路径 和 applicationId 客户端将程序运行需要的共享资源放进共享资源路径包括：程序jar包，xml配置文件，split切片信息 客户端向rm发送资源放置成功的报告，并真正 提交应用程序 rm接收到客户端的请求，会返回一个空闲的资源节点(比如：node01) 到资源节点(node01)上启动container 启动MRAppMaster 创建作业簿 记录 maptask 和 reducetask 的 运行状态和进度 等信息 mrappmaster去共享资源路径下 ,获取 切片 和 配置文件 等信息 mrappmaster 向 rm 申请maptask 和 reducetask的资源 rm 在处理 mrappmaster 请求时，会 优先处理有关maptask的请求 rm 向 mrappmaster 返回空闲节点（数据本地优先原则），运行maptask 或 reducetask优先返回有数据的节点。 对象节点需要到hdfs 共享路径下下载程序jar包等 共享资源 到本地 mrappmaster 到 对应的节点上，启动container 和 maptask maptask需要向 mrappmaster 汇报自身的 运行状态和进度 mrappmaster 监控到所有的maptask 运行进度到 80%，启动reducetask（启动前，也会下载共享资源路径下的响应文件，程序jar包，配置文件等） reducetask 时刻和 mrappmaster 通信，汇报自身的 运行状态和进度 整个运行过程中，maptask运行完成 ， 都会向mrappmaster 申请注销 自己 当所有的maptask 和 reducetask 运行完成 ， mrappmaster 就会向rm 申请注销，进行资源回收]]></content>
      <categories>
        <category>Hadoop</category>
        <category>MapReduce</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce笔记-2 三大组件-Partitioner分区,sort排序,Combiner局部分区]]></title>
    <url>%2F2018%2F06%2F07%2FHadoop%2FStudy%2F2-MapReduce%2FMapReduce%E7%AC%94%E8%AE%B0-2%2F</url>
    <content type="text"><![CDATA[1. Combiner 组件1. 产生缘由：Combiner 是 MapReduce 程序中 Mapper 和 Reducer 之外的一种组件，它的作用是在 maptask 之后给 maptask 的结果进行局部汇总，以减轻 reducetask 的计算负载，减少网络传输 Combiner 组件的作用： 减少 reduce 端的数据量 减少 shuffle 过程的数据量 在 map 端做了一次合并，提高分布式计算程序的整体性能 Combiner 组件帮 reduce 分担压力， 因此其业务逻辑和 reduce 中的业务逻辑相似 2.自定义 Combiner 组件：默认情况下没有 Combiner 组件，Combiner 作用时间点 — map–combiner–reduce 继承 Reduce 类 public class MyCombiner extends Reducer&lt;前两个： map 的输出， 后两个： reduce 的输入&gt;{} 我们在写 MapReduce 程序的时候， map 的输出就是 reduce 的输入 也就是说， 这个 MyCombiner() 的前两个泛型和后两个泛型的类型一致 重写 reduce 方法 Combiner 本质上相当于 在 map 端进行了一次 reduce 操作， 通常情况下直接使用 reducer 的类作为 Combiner 的类，不再单独写 Combiner 代码逻辑 在 Job 中加上job.setCombinerClass(WorldcountReduce.class)， 就会调用 Combiner Combiner 使用原则 有或没有都不能影响业务逻辑，都不能影响最终结果。比如累加，最大值等，求平均值就不能用。 2、MapReduce 中的序列化2.1、概述Java 的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额 外的信息（各种校验信息，header，继承体系等），不便于在网络中高效传输； Hadoop 自己开发了一套序列化机制（参与序列化的对象的类都要实现 Writable 接口），精简，高效 Java 基本类型 &amp; Hadoop 类型对照表123456789101112131415161718// Java &amp; Hadoop类型参照hadoop数据类型 &lt;------------&gt; java数据类型: 布尔型： BooleanWritable &lt;------------&gt; boolean 整型： ByteWritable &lt;------------&gt; byte ShortWritable &lt;------------&gt; short IntWritable &lt;------------&gt; int LongWritable &lt;------------&gt; long 浮点型： FloatWritable &lt;------------&gt; float DoubleWritable &lt;------------&gt; double 字符串（文本）： Text &lt;------------&gt; String 数组： ArrayWritable &lt;------------&gt; Array map集合： MapWritable &lt;------------&gt; map 2.2、自定义对象实现 MapReduce 框架的序列化要实现WritableComparable接口，因为 MapReduce 框架中的 shuffle 过程一定会对 key 进行排序 123456789101112131415161718//序列化方法@Overridepublic void write(DataOutput out) throws IOException &#123; out.writeUTF(phone); out.writeLong(upfFlow); out.writeLong(downFlow); out.writeLong(sumFlow);&#125;//反序列化方法//注意： 字段的反序列化顺序与序列化时的顺序保持一致,並且类型也一致@Overridepublic void readFields(DataInput in) throws IOException &#123; this.phone = in.readUTF(); this.upfFlow = in.readLong(); this.downFlow = in.readLong(); this.sumFlow = in.readLong();&#125; 3. MapReduce中的Sort –TODO。。MapTask –&gt; ReduceTask 之间， 框架默认添加了排序 排序的规则是按照Map 端输出的 key 的字典顺序进行排序 1、 如果没有重写 WritableComparable 时 按单词统计中词频出现的此处进行排序， 按照出现的次数， 从低到高 如果想要对词频进行排序， 那么词频应该放在 map 输出 key 的位置 代码实现： 1234567 Map //词频为 key， 其它为 value Reduce // 将 map 输入的结果反转(k,v 换位置), 输出最终结果// 最后输出还是按照左边词, 右边次数// ps： 如果倒序排的时候, map 的时候发的时候 加上-, reduce 发的时候, 再加上-, 转成 IntWritable 2、自定义排序要实现WritableComparable接口 自定义的类必须放在 key 的位置 实现WritableComparable接口， 重写 compareTo()方法 待扩展… 作业： 增强需求： 按照总流量排序， 总流量相同时， 按照手机号码排序 4、MapReduce 中的数据分发组件 Partitioner（分区）需求： 根据归属地输出流量统计数据结果到不同文件，以便于在查询统计结果时可以定位到 省级范围进行 思路：MapReduce 中会将 map 输出的 kv 对，按照相同 key 分组，然后分发给不同的 reducetask 执行时机: 在Map输出 kv 对之后, 所携带的 k,v 参数，跟 Map 输出相同 MapReduce 默认的分发规则为：根据 key 的 hashcode%reducetask 数来分发，所以：如果要按照我们自 己的需求进行分组，则需要改写数据分发（分区）组件 Partitioner Partition重点总结： Partition 的 key value, 就是Mapper输出的key value public abstract int getPartition(KEY key, VALUE value, int numPartitions); 输入是Map的结果对&lt;key, value&gt;和Reducer的数目，输出则是分配的Reducer（整数编号）。就是指定Mappr输出的键值对到哪一个reducer上去。系统缺省的Partitioner是HashPartitioner，它以key的Hash值对Reducer的数目取模，得到对应的Reducer。这样保证如果有相同的key值，肯定被分配到同一个reducre上。如果有N个reducer，编号就为0,1,2,3……(N-1)。 MapReduce 中会将 map 输出的 kv 对，按照相同 key 分组，然后分发给不同的 reducetask 默认的分发规则为:根据 key 的 hashcode%reducetask 数来分发，所以:如果要按照我们自 己的需求进行分组，则需要改写数据分发(分组)组件 Partitioner, 自定义一个 CustomPartitioner 继承抽象类:Partitioner 因此， Partitioner 的执行时机， 是在Map输出 kv 对之后 Partitioner 实现过程 先分析一下具体的业务逻辑，确定大概有多少个分区 首先书写一个类，它要继承 org.apache.hadoop.mapreduce.Partitioner这个抽象类 重写public int getPartition这个方法，根据具体逻辑，读数据库或者配置返回相同的数字 在main方法中设置Partioner的类，job.setPartitionerClass(DataPartitioner.class); 设置Reducer的数量，job.setNumReduceTasks(6); 典型的 Partitioner 代码实现12345678910111213141516171819202122import java.util.HashMap;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; &#123; private static HashMap&lt;String, Integer&gt; provincMap = new HashMap&lt;String, Integer&gt;(); static &#123; provincMap.put("138", 0); provincMap.put("139", 1); provincMap.put("136", 2); provincMap.put("137", 3); provincMap.put("135", 4); &#125; @Override public int getPartition(Text key, FlowBean value, int numPartitions) &#123; Integer code = provincMap.get(key.toString().substring(0, 3)); if (code != null) &#123; return code; &#125; return 5; &#125;&#125; 5、全局计数器1. 框架内置计数器： Hadoop内置的计数器，主要用来记录作业的执行情况 内置计数器包括 MapReduce框架计数器（Map-Reduce Framework） 文件系统计数器（FielSystemCounters） 作业计数器（Job Counters） 文件输入格式计数器（File Output Format Counters） 文件输出格式计数器（File Input Format Counters) 计数器由相关的task进行维护，定期传递给tasktracker，再由tasktracker传给jobtracker； 最终的作业计数器实际上是有jobtracker维护，所以计数器可以被全局汇总，同时也不必在整个网络中传递 只有当一个作业执行成功后，最终的计数器的值才是完整可靠的； 2. 自定义的计数器应用场景 用来统计运行过程中的进度和状态， 类似于 job 运行的一个报告、日志 要将数据处理过程中遇到的不合规数据行进行全局计数，类似这 种需求可以借助 MapReduce 框架中提供的全局计数器来实现 计数器的值可以在mapper或reducer中增加 使用方式 定义枚举类 1234enum Temperature&#123; MISSING, TOTAL &#125; 在map或者reduce中使用计数器 123456// 1.自定义计数器Counter counter = context.getCounter(Temperature.TOTAL); // 2.为计数器赋初始值counter.setValue(long value);// 3.计数器工作counter.increment(long incr); 获取计数器 123Counters counters=job.getCounters(); Counter counter=counters.findCounter(LOG_PROCESSOR_COUNTER.BAD_RECORDS_LONG);// 查找枚举计数器，假如Enum的变量为BAD_RECORDS_LONG long value=counter.getValue();//获取计数值 计数器使用完整代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * @Description 假如一个文件，规范的格式是3个字段，“\t”作为分隔符，其中有2条异常数据，一条数据是只有2个字段，一条数据是有4个字段*/public class MyCounter &#123; // \t键 private static String TAB_SEPARATOR = "\t"; public static class MyCounterMap extends Mapper&lt;LongWritable, Text, Text, Text&gt; &#123; // 定义枚举对象 public static enum LOG_PROCESSOR_COUNTER &#123; BAD_RECORDS_LONG, BAD_RECORDS_SHORT &#125;; protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String arr_value[] = value.toString().split(TAB_SEPARATOR); if (arr_value.length &gt; 3) &#123; /* 自定义计数器 */ context.getCounter("ErrorCounter", "toolong").increment(1); /* 枚举计数器 */ context.getCounter(LOG_PROCESSOR_COUNTER.BAD_RECORDS_LONG).increment(1); &#125; else if (arr_value.length &lt; 3) &#123; // 自定义计数器 context.getCounter("ErrorCounter", "tooshort").increment(1); // 枚举计数器 context.getCounter(LOG_PROCESSOR_COUNTER.BAD_RECORDS_SHORT).increment(1); &#125; &#125; &#125; @SuppressWarnings("deprecation") public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException &#123; String[] args0 = &#123; "hdfs://hadoop2:9000/buaa/counter/counter.txt", "hdfs://hadoop2:9000/buaa/counter/out/" &#125;; // 读取配置文件 Configuration conf = new Configuration(); // 如果输出目录存在，则删除 Path mypath = new Path(args0[1]); FileSystem hdfs = mypath.getFileSystem(conf); if (hdfs.isDirectory(mypath)) &#123; hdfs.delete(mypath, true); &#125; // 新建一个任务 Job job = new Job(conf, "MyCounter"); // 主类 job.setJarByClass(MyCounter.class); // Mapper job.setMapperClass(MyCounterMap.class); // 输入目录 FileInputFormat.addInputPath(job, new Path(args0[0])); // 输出目录 FileOutputFormat.setOutputPath(job, new Path(args0[1])); // 提交任务，并退出 System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; 注意点：在没有 ReduceTask 的时候， job.setNumReduceTasks(0); 关于计数器，详情可参考]]></content>
      <categories>
        <category>Hadoop</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce笔记-1 WordCount, MapReduce运行机制]]></title>
    <url>%2F2018%2F06%2F06%2FHadoop%2FStudy%2F2-MapReduce%2FMapReduce%E7%AC%94%E8%AE%B0-1%2F</url>
    <content type="text"><![CDATA[参考链接: hdfs 笔记 mapreduce 笔记 1、MapReduce 入门1.1、MapReduce概念hadoop 的四大组件： HDFS：分布式存储系统 MapReduce：分布式计算系统 YARN：hadoop 的资源调度系统 Common：以上三大组件的底层支撑组件，主要提供基础工具包和 RPC 框架等 MapReduce 是一个分布式运算程序的编程框架，是用户开发“基于 Hadoop 的数据分析应用” 的核心框架 MapReduce 核心功能 ：将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布 式运算程序，并发运行在一个 Hadoop 集群上 1.2、为什么需要 MapReduce？引入 MapReduce 框架后，开发人员可以将绝大部分工作集中在业务逻辑的开发上，而将 分布式计算中的复杂性交由框架来处理 Hadoop 当中的 MapReduce 分布式程序运算框架的整体结构如下： MRAppMaster：MapReduce Application Master，分配任务，协调任务的运行 MapTask：阶段并发任，负责 mapper 阶段的任务处理 YARNChild ReduceTask：阶段汇总任务，负责 reducer 阶段的任务处理 YARNChild 1.3、MapReduce 的编写规范MapReduce 程序编写规范： 用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行 MR 程序的客户端) Mapper 的输入数据是 KV 对的形式（KV 的类型可自定义） Mapper 的输出数据是 KV 对的形式（KV 的类型可自定义） Mapper 中的业务逻辑写在 map()方法中 map()方法（maptask 进程）对每一个&lt;K,V&gt;调用一次 Reducer 的输入数据类型对应 Mapper 的输出数据类型，也是 KV 对的形式 Reducer 的业务逻辑写在 reduce()方法中 Reducetask 进程对每一组相同 k 的&lt;K,V&gt;组调用一次 reduce()方法 用户自定义的 Mapper 和 Reducer 都要继承各自的父类 整个程序需要一个 Drvier 来进行提交，提交的是一个描述了各种必要信息的 job 对象 1.4、WordCount 程序1、业务逻辑 maptask阶段处理每个数据分块的单词统计分析，思路是每遇到一个单词则把其转换成一个 key-value对，比如单词 hello，就转换成&lt;’hello’,1&gt;发送给 reducetask去汇总 reducetask阶段将接受 maptask的结果，来做汇总计数 2、具体代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465 Map static class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 计算任务代码：切割单词，输出每个单词计 1 的 key-value 对 String[] words = value.toString().split(" "); for (String word : words) &#123; context.write(new Text(word), new IntWritable(1)); &#125; &#125;&#125; Reduce static class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; // 汇总计算代码：对每个 key 相同的一组 key-value 做汇总统计 int sum = 0; for (IntWritable v : values) &#123; sum += v.get(); &#125; context.write(key, new IntWritable(sum)); &#125;&#125; main public static void main(String[] args) throws Exception &#123; // 指定 hdfs 相关的参数 Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://hadoop02:9000"); System.setProperty("HADOOP_USER_NAME", "hadoop"); // 新建一个 job 任务 Job job = Job.getInstance(conf); // 设置 jar 包所在路径 job.setJarByClass(WordCountMR.class); // 指定 mapper 类和 reducer 类 job.setMapperClass(WordCountMapper.class); job.setReducerClass(WordCountReducer.class); // 指定 maptask 的输出类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 指定 reducetask 的输出类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 指定该 mapreduce 程序数据的输入和输出路径 Path inputPath = new Path("/wordcount/input"); Path outputPath = new Path("/wordcount/output"); FileInputFormat.setInputPaths(job, inputPath); FileOutputFormat.setOutputPath(job, outputPath); // 最后提交任务 boolean waitForCompletion = job.waitForCompletion(true); System.exit(waitForCompletion?0:1);&#125; 2、MapReduce 程序的核心运行机制2.1、概述一个完整的 MapReduce 程序在分布式运行时有两类实例进程： MRAppMaster：负责整个程序的过程调度及状态协调 Yarnchild：负责 map 阶段的整个数据处理流程 Yarnchild：负责 reduce 阶段的整个数据处理流程 以上两个阶段 MapTask 和 ReduceTask 的进程都是 YarnChild ， 并不是说这 MapTask 和 ReduceTask 就跑在同一个 YarnChild 进行里 2.2、MapReduce 程序的运行流程 一个 mr 程序启动的时候，最先启动的是 MRAppMaster，MRAppMaster 启动后根据本次 job 的描述信息，计算出需要的 maptask 实例数量，然后向集群申请机器启动相应数量的 maptask 进程 maptask 进程启动之后，根据给定的数据切片(哪个文件的哪个偏移量范围)范围进行数 据处理，主体流程为： 利用客户指定的 InputFormat 来获取 RecordReader 读取数据，形成输入 KV 对 将输入 KV 对传递给客户定义的 map()方法，做逻辑运算，并将 map()方法输出的 KV 对收 集到缓存 将缓存中的 KV 对按照 K 分区排序后不断溢写到磁盘文件 MRAppMaster 监控到所有 maptask 进程任务完成之后（真实情况是，某些 maptask 进 程处理完成后，就会开始启动 reducetask 去已完成的 maptask 处 fetch 数据），会根据客户指 定的参数启动相应数量的 reducetask 进程，并告知 reducetask 进程要处理的数据范围（数据 分区） Reducetask 进程启动之后，根据 MRAppMaster 告知的待处理数据所在位置，从若干台 maptask 运行所在机器上获取到若干个 maptask 输出结果文件，并在本地进行重新归并排序， 然后按照相同 key 的 KV 为一个组，调用客户定义的 reduce()方法进行逻辑运算，并收集运 算输出的结果 KV，然后调用客户指定的 OutputFormat 将结果数据输出到外部存储 2.3、MapTask 并行度决定机制将待处理数据执行逻辑切片（即按照一个特定切片大小，将待处理数据划分成逻辑上的多 个 split），然后每一个 split 分配一个 mapTask 并行实例处理。 这段逻辑及形成的切片规划描述文件，是由 FileInputFormat 实现类的 getSplits()方法完成的。 该方法返回的是 List，InputSplit 封装了每一个逻辑切片的信息，包括长度和位置 信息，而 getSplits()方法返回一组 InputSplit。 2.4、切片机制FileInputFormat 中默认的切片机制 简单地按照文件的内容长度进行切片 切片大小，默认等于 block 大小 切片时不考虑数据集整体，而是逐个针对每一个文件单独切片 比如待处理数据有两个文件： File1.txt 200M File2.txt 100M 经过 getSplits()方法处理之后，形成的切片信息是： File1.txt-split1 0-128M File1.txt-split2 129M-200M File2.txt-split1 0-100M FileInputFormat 中切片的大小的参数配置1234567891011// 通过分析源码，在 FileInputFormat 中，计算切片大小的逻辑：long splitSize = computeSplitSize(blockSize, minSize, maxSize)，翻译一下就是求这三个值的中 间值// 切片主要由这几个值来运算决定：blocksize：默认是 128M，可通过 dfs.blocksize 修改minSize：默认是 1，可通过 mapreduce.input.fileinputformat.split.minsize 修改maxsize：默认是 Long.MaxValue，可通过 mapreduce.input.fileinputformat.split.maxsize 修改//因此，如果 maxsize 调的比 blocksize 小，则切片会小于 blocksize; 如果 minsize 调的比 blocksize 大，则切片会大于 blocksize// 但是，不论怎么调参数，都不能让多个小文件“划入”一个 split 2.5、MapTask 并行度经验之谈如果硬件配置为 2*12core + 64G，恰当的 map 并行度是大约每个节点 20-100 个 map，最好 每个 map 的执行时间至少一分钟。 如果 job 的每个 map 或者 reduce task 的运行时间都只有 30-40 秒钟，那么就减少该 job 的 map 或者 reduce 数，每一个 task(map|reduce)的 setup 和加入到调度器中进行调度，这个 中间的过程可能都要花费几秒钟，所以如果每个 task 都非常快就跑完了，就会在 task 的开 始和结束的时候浪费太多的时间。 配置 task 的 JVM 重用可以改善该问题： mapred.job.reuse.jvm.num.tasks，默认是 1，表示一个 JVM 上最多可以顺序执行的 task 数目（属于同一个 Job）是 1。也就是说一个 task 启一个 JVM。 这个值可以在 mapred-site.xml 中进行更改，当设置成多个，就意味着这多个 task 运行在同一个 JVM 上，但不是同时执行， 是排队顺序执行 如果 input 的文件非常的大，比如 1TB，可以考虑将 hdfs 上的每个 blocksize 设大，比如 设成 256MB 或者 512MB 2.6、ReduceTask 并行度决定机制reducetask 的并行度同样影响整个 job 的执行并发度和执行效率，但与 maptask 的并发数由 切片数决定不同，Reducetask 数量的决定是可以直接手动设置： 12// 设置 ReduceTask 的并行度job.setNumReduceTasks(4); 默认值是 1， 手动设置为 4，表示运行 4 个 reduceTask， 设置为 0，表示不运行 reduceTask 任务，也就是没有 reducer 阶段，只有 mapper 阶段 如果数据分布不均匀，就有可能在 reduce 阶段产生数据倾斜 注意：reducetask 数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有 1 个 reducetask 尽量不要运行太多的 reducetask。对大多数 job 来说，最好 rduce 的个数最多和集群中的 reduce 持平，或者比集群的 reduce slots 小。这个对于小集群而言，尤其重要。 最好的ReduceTask 个数是：datanode 个数 * 0.75~0.95 左右 3. 昨日复习1.MapReduce 的 wc 编程 手写代码 Mapper Reducer Driver 2.MapTask 的并行度 在程序执行的时候运行的 maptask 的总个数 3.ReduceTask的并行度问题 ReduceTask 的并行度设置依赖于自己传入的参数 一般经验： ReduceTask 的个数应该 = datanode 的阶段数 * （0.75~0.95） ReduceTask 在设置的时候的并行度有一定的瓶颈 分区： 决定 ReduceTask 中的数据怎么分配的 默认分区方式 自定义分区]]></content>
      <categories>
        <category>Hadoop</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[▍Do Not Go Gentle into That Good Night]]></title>
    <url>%2F2018%2F06%2F04%2FPoetry%2F%E4%B8%8D%E8%A6%81%E6%B8%A9%E5%92%8C%E7%9A%84%E8%B5%B0%E8%BF%9B%E9%82%A3%E4%B8%AA%E8%89%AF%E5%A4%9C%2F</url>
    <content type="text"><![CDATA[Do not go gentle into that good night, Old age should burn and rave at close of the day; Rage, rage against the dying of the light. Though wise men at their end know dark is right, Because their words had forked no lightning they Do not go gentle into that good night. Good men, the last wave by, crying how bright Their frail deeds might have danced in a green bay, Rage, rage against the dying of the light. Wild men, who caught and sang the sun in flight, And learn, too late, they grieved it on its way, Do not go gentle into that good night. Grave men, near death, who see with blinding sight Blind eyes could blaze like meteors and be gay, Rage, rage against the dying of the light. And you, my father, there on the sad height, Curse, bless, me now with your fierce tears, I pray. Do not go gentle into that good night. Rage, rage against the dying of the light. 《不要温和地走进那个良夜》 -巫宁坤译本 ​ 不要温和地走进那个良夜， 老年应当在日暮时燃烧咆哮； 怒斥，怒斥光明的消逝。 虽然智慧的人临终时懂得黑暗有理， 因为他们的话没有进发出闪电，他们 也并不温和地走进那个良夜。 善良的人，当最后一浪过去，高呼他们脆弱的善行 可能曾会多么光辉地在绿色的海湾里舞蹈， 怒斥，怒斥光明的消逝。 狂暴的人抓住并歌唱过翱翔的太阳， 懂得，但为时太晚，他们使太阳在途中悲伤， 也并不温和地走进那个良夜。 严肃的人，接近死亡，用炫目的视觉看出 失明的跟睛可以像流星一样闪耀欢欣， 怒斥，恕斥光明的消逝。 您啊，我的父亲，在那悲哀的高处。 现在用您的热泪诅咒我，祝福我吧。我求您 不要温和地走进那个良夜。 怒斥，怒斥光明的消逝 《不要温顺地走入那长夜》 -和菜头译本 白日将尽，暮年仍应燃烧咆哮 狂怒吧，狂怒吧！ 对抗着光明渐逝 虽然智者深知 人之将死，黑暗自有其时 只因他们所言未曾裂天如电 他们不要温顺地走入那长夜 随着最后一浪，善人在哭喊 哭喊那脆弱的善行 它本应何其欢快 在绿色峡湾里起舞 狂怒吧，狂怒吧！ 对抗着光明渐逝。 狂人曾抓住飞驰的太阳 放声歌唱 太晚，他们才感到其中的伤感 不要温顺地走进那长夜 严肃的人行将死去时 用那渐渐失神的目光去看 盲瞳却如流星璀璨，欢欣溢满 狂怒吧，狂怒吧！ 对抗着光明渐逝 还有你啊，我的父亲，远在悲伤的高地 我恳请你现在 就让你诅咒，你的祝福 随着热泪落下 不要温顺地走进那长夜 狂怒吧，狂怒吧！ 对抗这光明渐逝 ​ 《 绝不向黑夜请安》 -高晓松译本 绝不向黑夜请安 老朽请于白日尽头涅槃 咆哮于光之消散 先哲虽败于幽暗 诗歌终不能将苍穹点燃 绝不向黑夜请安 贤者舞蹈于碧湾 为惊涛淹没的善行哭喊 咆哮于光之消散 狂者如夸父逐日 高歌中顿觉迟来的伤感 绝不向黑夜请安 逝者于临终迷幻 盲瞳怒放出流星的灿烂 咆哮于光之消散 那么您，我垂垂将死的父亲 请掬最后一捧热泪降临 请诅咒，请保佑 我祈愿，绝不向 黑夜请安，咆哮 于光之消散]]></content>
      <categories>
        <category>文艺</category>
        <category>诗歌</category>
      </categories>
      <tags>
        <tag>诗歌</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop学习笔记-3 HDFS 原理剖析]]></title>
    <url>%2F2018%2F06%2F03%2FHadoop%2FStudy%2F1-HDFS%2FHDFS-2-%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[1. HDFS体系结构主从。。。 2.NameNode概念 [x] 是整个文件系统的管理节点。它维护着整个文件系统的文件目录树，文件/目录的元信息和每个文件对应的数据块列表。接收用户的操作请求。 [x] 在hdfs-site.xml中的dfs.namenode.name.dir属性 [x] 文件包括： [x] 文件包括: ①fsimage:元数据镜像文件。存储某一时段NameNode内存元数据信息。 ②edits:操作日志文件。 ③fstime:保存最近一次checkpoint的时间 以上这些文件是保存在linux的文件系统中。 查看 fsimage 和 edits的内容 查看 NameNode中 fsimage 的内容 查看 fsimage镜像文件内容Usage: bin/hdfs oiv [OPTIONS] -i INPUTFILE -o OUTPUTFILE 1234567891011121314# 可以知道数据存在那个哪个 fsimage 镜像中------------------------------------# 使用离线的查看器 输出到网页查看oiv -i hadoopdata/namenode/current/fsimage_0000000000000000250 -o 0000000000000000250# 出现这样的提示INFO offlineImageViewer.WebImageViewer: WebImageViewer started. Listening on /127.0.0.1:5978. Press Ctrl+C to stop the viewer.# 另起一个窗口查看hadoop fs -ls -R webhdfs://127.0.0.1:5978------------------------------------# 也可以导出到 xml 文件bin/hdfs oiv -p XML -i tmp/dfs/name/current/fsimage_0000000000000000055 -o fsimage.xml 查看edits文件， 也可以导出到 xml 12# 查看edtis内容bin/hdfs oev -i tmp/dfs/name/current/edits_0000000000000000057-0000000000000000186 -o edits.xml 3. Datanode提供真实文件数据的存储服务 Datanode 节点的数据切块存储位置 ~/hadoopdata/datanode/current/BP-1070660005-192.168.170.131-1527865615372/current/finalized/subdir0/subdir0 12345678[ap@cs2]~/hadoopdata/datanode/current/BP-1070660005-192.168.170.131-1527865615372/current/finalized/subdir0/subdir0% ll总用量 213340-rw-r--r-- 1 ap ap 134217728 6月 2 13:35 blk_1073741842-rw-r--r-- 1 ap ap 1048583 6月 2 13:35 blk_1073741842_1018.meta-rw-r--r-- 1 ap ap 82527955 6月 2 13:35 blk_1073741843-rw-r--r-- 1 ap ap 644759 6月 2 13:35 blk_1073741843_1019.meta-rw-r--r-- 1 ap ap 13 6月 3 02:12 blk_1073741850-rw-r--r-- 1 ap ap 11 6月 3 02:12 blk_1073741850_1028.meta 文件块（block）：最基本的存储单位。对于文件内容而言，一个文件的长度大小是size，那么从文件的０偏移开始，按照固定的大小，顺序对文件进行划分并编号，划分好的每一个块称一个Block。HDFS默认Block大小是128MB，以一个256MB文件，共有256/128=2个Block. 不同于普通文件系统的是，HDFS中，如果一个文件小于一个数据块的大小，并不占用整个数据块存储空间, 按文件大小的实际容量存储 Replication。多复本。默认是三个。 hdfs-site.xml的dfs.replication属性 手动设置某个文件的副本数为3个 bin/hdfs dfs -setrep 3 /a.txt 4. 数据存储： 写文件解析 [x] 疑点： HDFS client上传数据到HDFS时，会首先在本地缓存数据，当数据达到一个block大小时，请求NameNode分配一个block。NameNode会把block所在的DataNode的地址告诉HDFS client。HDFS client会直接和DataNode通信，把数据写到DataNode节点一个block文件中。 问题： 如果一直写的数据都没有达到一个 block 大小， 那怎么存储？？ 写文件的过程： 首先调用FileSystem对象的open方法，其实是一个DistributedFileSystem的实例 DistributedFileSystem通过rpc获得文件的第一批个block的locations，同一block按照重复数会返回多个locations，这些locations按照hadoop拓扑结构排序，距离客户端近的排在前面. 前两步会返回一个FSDataInputStream对象，该对象会被封装成DFSInputStream对象，DFSInputStream可以方便的管理datanode和namenode数据流。客户端调用read方法，DFSInputStream最会找出离客户端最近的datanode并连接。 数据从datanode源源不断的流向客户端。 如果第一块的数据读完了，就会关闭指向第一块的datanode连接，接着读取下一块。这些操作对客户端来说是透明的，客户端的角度看来只是读一个持续不断的流。 如果第一批block都读完了，DFSInputStream就会去namenode拿下一批blocks的location，然后继续读，如果所有的块都读完，这时就会关闭掉所有的流。 如果在读数据的时候，DFSInputStream和datanode的通讯发生异常，就会尝试正在读的block的排第二近的datanode,并且会记录哪个datanode发生错误，剩余的blocks读的时候就会直接跳过该datanode。DFSInputStream也会检查block数据校验和，如果发现一个坏的block,就会先报告到namenode节点，然后DFSInputStream在其他的datanode上读该block的镜像 该设计的方向就是客户端直接连接datanode来检索数据并且namenode来负责为每一个block提供最优的datanode，namenode仅仅处理block location的请求，这些信息都加载在namenode的内存中，hdfs通过datanode集群可以承受大量客户端的并发访问。 5. 数据存储： 读文件解析 读文件的过程 首先调用FileSystem对象的open方法，其实是一个DistributedFileSystem的实例 DistributedFileSystem通过rpc获得文件的第一批个block的locations，同一block按照重复数会返回多个locations，这些locations按照hadoop拓扑结构排序，距离客户端近的排在前面. 前两步会返回一个FSDataInputStream对象，该对象会被封装成DFSInputStream对象，DFSInputStream可以方便的管理datanode和namenode数据流。客户端调用read方法，DFSInputStream最会找出离客户端最近的datanode并连接。 数据从datanode源源不断的流向客户端。 如果第一块的数据读完了，就会关闭指向第一块的datanode连接，接着读取下一块。这些操作对客户端来说是透明的，客户端的角度看来只是读一个持续不断的流。 如果第一批block都读完了，DFSInputStream就会去namenode拿下一批blocks的location，然后继续读，如果所有的块都读完，这时就会关闭掉所有的流。 如果在读数据的时候，DFSInputStream和datanode的通讯发生异常，就会尝试正在读的block的排第二近的datanode,并且会记录哪个datanode发生错误，剩余的blocks读的时候就会直接跳过该datanode。DFSInputStream也会检查block数据校验和，如果发现一个坏的block,就会先报告到namenode节点，然后DFSInputStream在其他的datanode上读该block的镜像 该设计的方向就是客户端直接连接datanode来检索数据并且namenode来负责为每一个block提供最优的datanode，namenode仅仅处理block location的请求，这些信息都加载在namenode的内存中，hdfs通过datanode集群可以承受大量客户端的并发访问。 6.Hadoop Archives （HAR files）Hadoop Archives (HAR files)是在0.18.0版本中引入的，它的出现就是为了缓解大量小文件消耗namenode内存的问题。HAR文件是通过在HDFS上构建一个层次化的文件系统来工作。一个HAR文件是通过hadoop的archive命令来创建，而这个命令实 际上也是运行了一个MapReduce任务来将小文件打包成HAR。对于client端来说，使用HAR文件没有任何影响。所有的原始文件都 （using har://URL）。但在HDFS端它内部的文件数减少了。 通过HAR来读取一个文件并不会比直接从HDFS中读取文件高效，而且实际上可能还会稍微低效一点，因为对每一个HAR文件的访问都需要完成两层 index文件的读取和文件本身数据的读取。并且尽管HAR文件可以被用来作为MapReduce job的input，但是并没有特殊的方法来使maps将HAR文件中打包的文件当作一个HDFS文件处理。 打包出来的 har 文件在xxx.har/part-0 中， contentz-size 跟原来的文件总大小一样 创建文件 hadoop archive -archiveName xxx.har -p /src /dest查看内容 hadoop fs -lsr har:///dest/xxx.har 可以原封不动的显示出来 123456789101112131415# 打包成 harhadoop archive -archiveName test.har -p /user/test /# 查看har 文件[ap@cs1]~% hadoop fs -count /test.har/part-0 0(目录数) 1(文件数) 72(文件大小) /test.har/part-0 (文件名)# 查看打包前文件[ap@cs1]~% hadoop fs -count /user/test 1 2 72 /user/test # 查看 har 文件， 把打包前的原本文件都显示出来了[ap@cs1]~% hadoop fs -ls -R har:///test.har-rw-r--r-- 3 ap supergroup 50 2018-06-03 04:24 har:///test.har/a.txt-rw-r--r-- 3 ap supergroup 22 2018-06-03 04:24 har:///test.har/b.txt 注意点： 存储层面：为了解决小文件过多导致的 Namenode 压力过大问题， 把很多小文件打包成一个 har 文件。 使用层面： 但是实际处理的时候， 还是会还原出原本的小文件进行处理， 不会把 har 文件当成一个 HDFS 文件处理。 HDFS 上不支持 tar， 只支持 har打包 7.HDFS 的 HA]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[▍回答]]></title>
    <url>%2F2018%2F06%2F03%2FPoetry%2F%E5%9B%9E%E7%AD%94%2F</url>
    <content type="text"><![CDATA[卑鄙是卑鄙者的通行证，高尚是高尚者的墓志铭，看吧，在那镀金的天空中，飘满了死者弯曲的倒影。 冰川纪过去了，为什么到处都是冰凌？好望角发现了，为什么死海里千帆相竞？ 我来到这个世界上，只带着纸、绳索和身影，为了在审判之前，宣读那些被判决的声音。 告诉你吧，世界我–不–相–信！纵使你脚下有一千名挑战者，那就把我算作第一千零一名。 我不相信天是蓝的，我不相信雷的回声，我不相信梦是假的，我不相信死无报应。 如果海洋注定要决堤，就让所有的苦水都注入我心中，如果陆地注定要上升，就让人类重新选择生存的峰顶。 新的转机和闪闪星斗，正在缀满没有遮拦的天空。那是五千年的象形文字，那是未来人们凝视的眼睛。 作者 / 北岛]]></content>
      <categories>
        <category>文艺</category>
        <category>诗歌</category>
      </categories>
      <tags>
        <tag>诗歌</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop学习笔记-2 HDFS基础入门]]></title>
    <url>%2F2018%2F06%2F02%2FHadoop%2FStudy%2F1-HDFS%2FHDFS-1-%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[Hadoop的核心组件 之 HDFSHDFS1. HDFS是什么: 分布式文件系统 2. HDFS 设计思想 分而治之, 切分存储, 当一个文件过大的时候, 一个节点存储不了, 采用切分存储 分块存储: 每一个块叫做 block 如果一个8T的数据, 这个怎么分合适??? 设置分块的时候要考虑一个事情 : 负载均衡 块的大小不能太大, 太大会造成负载不均衡 hadoop2.x 中默认的切分的块的大小是: 128M, 1.x中默认的是64M 如果一个文件不足128M, 也会单独存一个快, 快的大小就是存储数据的实际大小 这个分块存储思想中, 如果一个块的存储节点宕机了, 这个时候, 数据的安全性得不到保证了 HDFS中默认块的存储采用备份机制 默认的备份个数是3个(总共存的, 存到datanode上的, namenode不存), 之前自己配的是2个, 所有备份相同地位是相同的. 相同的数据块的备份一定存储在不同的节点上 如果节点总共2个, dfs.replication=3 副本个数是3个, 实际存储2个, 另一个进行记账, 当集群节点个数大于3个时, 会复制这个副本, 最终达到3个 假设集群中的节点4个, 副本3个, 有一个副本的机器宕机了, 这个时候发现副本的个数 小于 设定的个数, 就会进行复制, 达到3个副本, 如果 这个时候, 刚才宕机的节点又恢复了, 这个时候集群副本个数为4了, 集群会等待一段时间, 如果发现还是4个, 就会删除一个副本, 达到3个(设定值) 备份越多越好吗? 理论上副本数越多, 数据安全性越高 但是副本数越多, 会占用过多的存储资源, 会造成集群的维护变得越困难 100 个节点, 50个副本, 在这50个副本中, 随时都有可能宕机, hdfs就需要维护副本 一般情况下, 3个就可以了 hadoop是基于廉价的pc机设计的, 会造成机器随时可能宕机 HDFS的目录结构 hdfs的目录结构与linux 操作系统类似, 以 /为跟节点, 我们将这个目录🌲称为抽象目录树 因为hdfs的目录结构代表的是所有数据节点的抽象出来的目录, 不代表任何一个节点 hdfs: /hadoop.zip 500M 被分成4个块存储 hdfs中存储的数据块 是有编号的, blk_1, blk_2, blk_3, blk_4 /spark.zip 300M 3个块, blk_5 blk_6 blk_7 底层存储的时候, 每一个block都有一个唯一的id hdfs的数据底层存储的时候吗, 还是存在真正的物理节点上. 2. HDFS 的整体结构主从结构: 一个主节点, 多个从节点 namenode: 用于存储元数据, 包括: 抽象目录树 存储数据和block的对应关系 block存储的位置 处理客户端的读写请求 读: 下载 写: 上传 datanode 负责真正的数据存储, 存储数据的block 真正处理读写 secondarynamenode: 冷备份节点: 助理 当namenode宕机的时候, secondarynamenode不能主动切换为 namenode, 但是 secondarynamenode中存储的数据与namenode相同. 主要作用: namenode宕机的时候, 帮助namenode恢复 帮助namenode做一些事情, 分担namenode的压力 3. HDFS优缺点: 优点: 可构建在廉价机器上, 成本低, 通过多副本提高可靠性, 提供了容错和恢复机制 高容错性 容错性: 数据访问上, 一个节点数据丢失, 不影响整体的数据访问 数据自动保存多个副本, 副本丢失后, 自动恢复, 最终恢复到用户配置的副本个数 适合批处理, 适合离线数据处理 移动计算而非数据, 数据位置暴露给计算框架 适合大数据处理 GB, TB 甚至 PB 级数据, 百万规模以上的文件数量, 10k+ 节点规模 流式文件访问, 不支持数据修改, hdfs用于数据存储 一次性写入, 多次读取, 保证数据一致性 缺点: 不支持低延迟的数据访问, 不支持 实时/近实时 数据访问, 因为涉及到多轮RPC调用 向 NameNode 寻址.. 拿到地址后， 向 DataNode 请求数据.. 不擅长存储大量的小文件–kb级别的 寻址时间可能大于读取数据的时间, 不划算 进行数据访问的时候先找元数据 元数据是和block对应的, 1个block块对应一条元数据 1000w个1kb的文件, 存了1000w个块 — 1000w元数据 在进行数据访问的时候可能花了 1s 的时间, 总体上不划算 这样会造成元数据存储量过大, 增加namenode的压力 在hdfs中一般情况下, 一条元数据大小 150byte 左右 1000w条元数据 — 1000w * 150, 1.5G左右 不支持文件内容修改, 仅仅支持文件末尾追加 append， 一个文件同时只能有一个写者，不支持并发操作 ==4. HDFS 的 常用命令:==HDFS归根结底就是一个文件系统, 类似于 linux, 需要用命令来操作 1. hapdoop fs 命令 hadoop fs / hdfs dfs 效果是一样的 在hadoop中查看, 只有绝对路径的访问方式 查看帮助 hadoop fs -help 查看所有 hadoop fs的帮助 hadoop fs -help ls 查看 fs下的 ls的帮助 列出根目录: hadoop fs -ls / hadoop fs -ls -R / 递归展示 hadoop fs -ls -R -h /友好展示， 展示文件大小单位 如果不指定目录， 会默认找当前用户xx对应的/user/xx的目录 递归创建 -mkdir -p: hadoop fs -mkdir -p /aa/bb/cc/dd 不加 -p 为普通创建 创建空文件-touchz 类似于 Linux 下的 touch 上传 put: [-put [-f][-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;] 上传一个: hadoop fs -put hadoop-2.7.6.tar.gz /ss 上传多个: hadoop fs -put aa.txt bb.txt /ss 下载 get hadoop fs -get hdfs路径 本地路径 合并下载 getmerge hadoop fs -getmerge /ss/aa.txt /ss/bb.txt /home/ap/cc.txt 会将最后一个路径之前8的当做需要合并的文件, 最后一个路径中指定的文件就是合并生成的文件 查看文件内容 cat -cat 查看文件内容 -text也是类似 删除文件 rm rm -rf (错误的) rm -r(递归) -f(强制) 文件hadoop fs -rm -f /ss/aa.txt 文件夹 hadoop fs -rm -f -r /aa mv 修改名字, 移动 移动的文件从 hdfs 到 hdfs hadoop fs -mv .. .. cp 复制 hadoop fs -cp /hdfsfile /hdfsfile: 从 hdfs 复制到 hdfs 参数 -p ： 复制后保持文件的原本属性, 时间戳， 权限等 Passing -p preserves status [topax] (timestamps, ownership, permission, ACLs, XAttr). 参数 -f : 已有同名文件的话， 直接覆盖 在末尾追加: -appendToFile 本地文件 hdfs文件` 将本地文件bb.txt 追加到 htfd的 /aa/aa.txt 上 hadoop fs -appendToFile aa.txt /ss/bb.txt 从命令行追加 , 但是不知道怎么结束， 先存疑？？ hadoop fs -appendToFile - /a.txt` 这个追加是在原始块的末尾追加的. 会改变集群上的文件 如果超过128M才会进行切分, 但这个命令一般不会使用 查看文件，文件夹数量 count DIR_COUNT FILE_COUNT CONTENT_SIZE FILE_NAME 8 3 176.5 K /tmp hadoop fs -count -h /tmp: -h 是友好展示 hdfs dfs -count -h /tmp: 与上面效果一样 hdfs dfs -count -q -h /tmp: 查看文件配额， 具体看 help du： 展示文件大小， 如果参数是文件夹， 则展示文件夹下文件的大小 hadoop fs -du -h /tmp hadoop fs -du -s -h /tmp: s 应该是 sum 的意思， 展示所有文件大小的总和 展示文件最后1kb内容-tail Show the last 1KB of the file. hadoop fs -tail /dd.txt` -f Shows appended data as the file grows. 应用场景： 监控日志 修改文件权限 chmod 12345678910111213141516# 1. 直接使用十进制数字修改 [ap@cs2]~/test% hadoop fs -ls /drwxr-xr-x - ap supergroup 0 2018-06-01 08:55 /aa# -R： /aa 目录下所有的文件递归修改权限[ap@cs2]~/test% hadoop fs -chmod -R 777 /aa[ap@cs2]~/test% hadoop fs -ls /drwxrwxrwx - ap supergroup 0 2018-06-01 08:55 /aa# 2. 针对用户组修改，注意，修改2个不同组权限， 用，隔开hadoop fs -chmod u+x,g+x /a.txt # 3. 最常用的文件权限， 是 644(-rw-r--r--) 和 755(-rwxr-xr-x) 文件创建默认就是644# 4. u+x 与 +x 的区别 前者指定加在哪组用户上， 后者是所有组都加 修改用户权限 chown 1hadoop fs -chown -R 用户名:组名 hdfs目录/文件 2. hdfs dfsadmin命令 管理员对当前节点的一些操作 hdfs dfsadmin -report 报告当前的一些状态 -live 活跃的 -dead 死的 -decommissioning 退役的 **hdfs dfsadmin -safemode 安全模式 系统刚启动的时候， 会有30秒的安全模式开启状态， 过了30秒就关了 enter 进入 leave 离开 get 查看 hdfs dfsadmin 设置配额 -setQuota ： 配额是限定的文件&amp;文件夹的数量 A quota of 1 would force the directory to remain empty. 空文件本身算一个文件 bin/hdfs dfsadmin -setQuota 10 lisi -clrQuota -setSpaceQuota： 空间配额限定的是大小 bin/hdfs dfsadmin -setSpaceQuota 4k /lisi/ -clrSpaceQuota hdfs dfs -count -q -h /user: 加上 -q 是查看配额 3. httpFS访问使用 REST 的形式， 可以在浏览器上直接访问集群， 可以在非 Linux 平台访问 123456789101112131415161718192021# 编辑文件httpfs-env.sh# 打开此句注释, 使用内嵌的 tomcatexport HTTPFS_HTTP_PORT=14000# 编辑文件core-site.xml&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;编辑文件hdfs-site.xml&lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;重新启动namenode，执行 sbin/httpfs.sh start# 执行命令curl -i "http://cs1:14000/webhdfs/v1?user.name=root&amp;op=LISTSTATUS" 更多命令参考 相关知识点 这些命令在集群中的任何节点都可以做, hdfs文件系统中, 看到的目录结构只是一个抽象目录, 实际存储在集群中的节点上 aa.txt , 大小150M, hadoop fs -put aa.txt / 会在根目录下看到 /aa.txt, 但是 aa.txt 真实存储的时候, 会先进行分块, 分2块, 进行存储, 假设集群中5个存储节点, 这2个块存储在哪个节点, 由namenode进行分配 图形界面点进去, 可以看到存储的块 Linux的权限管理命令: 修改 文件/文件夹 权限的 chmod: 可读: r , =4 可写: w, =2 可执行: x, =1 最大权限是7 -rw-rw-r– 文件属性 d:目录 -:文件 l:链接 第一组: 本用户, 第二组: 本组用户, 第三组: 其它用户 chmod 711 改一个文件夹下所有文件权限为711 chmod -R 711 目录 修改文件所属用户和组 chown chown -R root:root ss/把ss的文件夹全部改成root用户和root组 5、Eclipse查看Hadoop文件信息详情可以查看 其中可能遇到的bug，参见 其中， Eclipse端无法直接删除文件的问题，似乎可以通过在hdfs-site.xml 中修改访问权限来实现， 还未尝试 1234&lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; 6. 通过 Java API的方式操作 HDFS]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap的实现原理]]></title>
    <url>%2F2018%2F06%2F02%2FJava%2FHashMap%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[1.什么是HashMap Hash: 散列将一个任意的长度通过某种(hash函数)算法转换成一个固定值 Map: 地图, (x,y)存储 底层就是一个数组结构, 数组中的每一项又是一个链表, 当新建一个HashMap的时候, 就会初始化一个数组 总结: 通过 hash 出来值, 然后通过值定位到某个 map, 然后value 存储到这个 map中, value只不过是 key 的附属. 2.源码分析先给出结论数据结构: 底层是数组 Entry 就是数组中的元素 每个Map.Entry其实就是一个key-value对, 它持有一个指向下一个元素的引用Entry&lt;K,V&gt; next;, 这就构成了链表 存取实现: 存储put : 过程 先根据 key 的 hashCode 重新计算 hash 值, 根据 hash 值得到这个元素在数组中的位置(下标) 如果数组该位置上已经存放有其他元素了, 那么在这个位置上的元素将以链表的形式存放, 新加入的放在链头, 最先加入的放在链尾. 如果数组该位置上没有元素, 就直接将该元素放到数组中的该位置上. 注意点 当系统决定存储 HashMap 中的 key-value 对时，完全没有考虑 Entry 中的 value，仅仅只是根据 key来计算并决定每个Entry的存储位置。当系统决定了 key的存储位置之后，value随之保存在那里即可。 对于于任意给定的对象，只要它的 hashCode()返回值相同，那么程序调用 hash(int h)方法所计算得到的hash 码值总是相同的。 本质上就是把 hash 值对数组长度取模运算， 这样一来，元素的分布相对来说是比较均匀的 但是系统是用的位运算， 方法更巧妙， 消耗更小 读取get 过程 首先计算 key 的 hashCode，找到数组中对应位置的某一元素，然后通过key 的 equals 方法在对应位置的链表中找到需要的元素。 存储实现总结: HashMap 在底层将 key-value 当成一个整体进行处理，这个整体就是一个 Entry 对象。 HashMap 底层采用一个 Entry[] 数组来保存所有的 key- value 对，当需要存储一个 Entry 对象时，会根据 hash 算法来决定其在数组中的存储位置，在根据 equals 方法决定其在该数组位置上的链表中的存储位置； 当需要取出一个 Entry 时，也会根据 hash 算法找到其在数组中的存储位置，再根据 equals 方法从该位置上的链表中取出该 Entry。 3. HashMap 的性能参数123456789101112131415161718192021222324252627282930313233343536/** * 桶表默认容量 16, 必须是 2 的倍数， 便于后面的 位运算 * 控制hashcode 不超16范围, a.hashcode = xx % 16 (hashcode 取模 桶个数) */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16/** * MUST be a power of two &lt;= 1&lt;&lt;30. * 桶表最大 2^30 */static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;/** * 扩容因子（负载因子）: 0.75 * 扩容: 每次2倍 */static final float DEFAULT_LOAD_FACTOR = 0.75f;/** * 链表: hash算法值相同的时候, 会把值相同的放在一个链表上, 链表上的元素个数 * 超过8个时, 链表转化为二叉树, 提升查询效率 */static final int TREEIFY_THRESHOLD = 8;/** * 小于6个， 又变回链表 */static final int UNTREEIFY_THRESHOLD = 6;/** * The smallest table capacity for which bins may be treeified. * (Otherwise the table is resized if too many nodes in a bin.) * Should be at least 4 * TREEIFY_THRESHOLD to avoid conflicts * between resizing and treeification thresholds. */static final int MIN_TREEIFY_CAPACITY = 64;]]></content>
      <categories>
        <category>Java</category>
        <category>知识点</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[▍无题]]></title>
    <url>%2F2018%2F06%2F02%2FPoetry%2F%E6%97%A0%E9%A2%98%2F</url>
    <content type="text"><![CDATA[在淮海中路看油画，要把天空调弱 让油菜花暗下去，看眉式清秀之人离开身体 穿过街巷中涂抹过的人群，悄悄投了水。 在淮海中路1411号，春光遮蔽了暗疾 鸟鸣带来逼仄和飞行感，一个人的身体像麻绳 裸露在新鲜空气中，骨头开裂出花朵。 眼底的云又白又黑，膝盖的青色愈爱愈深 穿过死后潭水的寂静，背部长出的鱼鳞 一年比一年薄，月亮一日比一日旧。 与春风交换身体，与素不相识之人抱头痛哭 与我，许下再死一次的诺言，这么多年了 她说，我爱你依旧，胜过画中人。 作者 / 隐居的事]]></content>
      <categories>
        <category>文艺</category>
        <category>诗歌</category>
      </categories>
      <tags>
        <tag>诗歌</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop相关书籍]]></title>
    <url>%2F2018%2F06%2F02%2FHadoop%2FStudy%2F0-Hadoop%2FHadoop%E7%9B%B8%E5%85%B3%E4%B9%A6%E7%B1%8D%2F</url>
    <content type="text"><![CDATA[Hadoop框架体系相关书籍推荐1. [Hadoop权威指南] 第四版 顾名思义, 很权威 🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘 🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘 2. [HBase权威指南] 祝你🐴到成功 🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴 🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴 3. [Hive编程指南] 小蜜蜂, 嗡嗡嗡. 🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝 🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝 4. [zookeeper分布式过程协同技术详解] 这是个啥动物?? 🐱?? 🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱 🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Books</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>Books</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop学习笔记-1 简介&安装]]></title>
    <url>%2F2018%2F05%2F31%2FHadoop%2FStudy%2F0-Hadoop%2FHadoop%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-%E7%AE%80%E4%BB%8B%26%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[引入面试题1. 有一个很大的(4T)的文件, 文件中存储的是ip, 每行存储一个, 要求求出出现此处最多的那个ip1234567891011121314151617181920212223242526272829303132333435如果这个文件是小文件： io流+集合 实现思路： 创建一个流进行文件读取 读取出来的数据存储到map集合中 key：ip value:次数 统计逻辑： 判断读取的ip是否已经存在在map中 存在：取出value+1 不存在：将ip作为key 1作为value 怎么求ip出现次数最多的 遍历map 遍历key，取出value找出最大值 value最大的key就是要找的ip我的文件足够大：大到一台机器装不下 数组 集合 变量------&gt;基于内存的 怎么办？？？？？ 服务器的2T 1.在最早的时候我们的思维模式就是纵向扩展，增加单个节点的性能 8T 摩尔定律：硬件性能18-24个月会提升一倍 4T------2h 4T------1h 前提是数据量不发生改变 但是往往数据量的变化速度远远大于服务器性能的提升速度 经过18个月 服务器性能提升了一倍 数据量------提升了10倍 4T------2h 40t-----20h 目前只需要10h 纵向扩展不可行？ 横向扩展：如果一台机器处理不了数据 使用多台机器 4T------2h 4t----4个机器----0.5小时 分而治之的思想： 一个机器计算性能有限 这个时候可以使用多台机器共同计算 每台机器承担一部分计算量 最终实现： 1.先将这个足够大的文件进行切分 切分成了多个小文件 2.将多个小文件分发给多个机器进行统计每个ip出现的次数 每个求出出现次数最多的ip 3.合并求出最终的最大值 2. 有两个很大的文件, 两个文件中存储的都是url, 求出两个文件中相同的url12345678910111213141516171819202122如果文件是小文件： io流+集合（set） 实现逻辑： 1.先创建两个文件读取流，用来读取两个文件 2.创建两个集合set1 set2 3.进行文件读取并分别set1 set2中 4.循环遍历其中一个set1，判断set1中取出的每个url是否在set2中 set2.contains(url)大文件的时候怎么办？ 我们也采用分而治之的思想：将两个大文件都进行切分，每个大文件都切成多个小文件 一个大任务=4*4个小任务 这样虽然可以达到目的但是效率太低？怎么办？ 排序，切分（规则同一） 最终将任务减少到4个 但是大文件排序仍然是一个非常消耗性能的事情，如果不需要排序就可做到这个效果尽量不要排序 怎么办？ hash算法的目的----》给每一个对象生成一个“唯一”的hash值0-Integer_MAX 是否可以运用hash算法解决这个问题 url.hashCode()%分段的个数 两个文件分段规则一定相同吗？ url.hashCode()肯定一样 分段个数一定相同吗？可以不一样 如果不一样的话 必须成倍数关系 最终的解决方案： 分而治之+分段规则 分段：分区 3. 有一个很小的文件, 存储的都是url, 每行一个, 怎样快速判断给定的一个url是否在这个文件中小文件: IO + 集合(set) 创建io 和 集合 进行文件读取放在 set集合中 set.contains(url) ==&gt; true:存在, false: 不存在 大文件: 思路1: 用hashCode() 进行分区, 然后用要查找的 url 取模定位 但是这样定位到了还是要一个个找 思路2: 数组的查询性能比较高, 数组可以通过下标 基数排序 数组的索引代表的是数据的原始值, 数组中存储的值, 是原始值出现的次数 放到对应下标的位置, 值只存出现的次数 如果数组中对应的下表存储的值为0, 代表此下标的值没有出现过, 就不需要输出 缺点: 数据范围过大时, 数组长度不好创建 数组的类型不好确定 如果数据比较分散时, 会造成资源浪费 练习: 写一个基数排序, 随机生成的20个数, 运用基数排序排序 对于本题 不需要统计次数, 存在标记为1, 不存在就是0 所以存的时候最好用boolean存, 用位数组 bit[] 可以设计多个hash算法, 用来校验某一种hashCode相同的情况 影响误判率3要素: hash算法个数 k - 数据量n - 数组长度 m 布隆过滤器 公式: k = 0.7*(m/n), 此时的误判率最小 大数据基本介绍数据 数据就是数值，也就是我们通过观察、实验或计算得出的结果。数据有很多种，最简单的就是数字。 数据也可以是文字、图像、声音等。数据可以用于科学研究、设计、查证等。 结构划分: 结构化 半结构化 非结构化 大数据特点: 4V 数据量大 1 Byte =8 bit1 KB = 1,024 Bytes = 8192 bit1 MB = 1,024 KB = 1,048,576 Bytes1 GB = 1,024 MB = 1,048,576 KB1 TB = 1,024 GB = 1,048,576 MB （普通用户数据级别）1 PB = 1,024 TB = 1,048,576 GB（企业级数据级别）1 EB = 1,024 PB = 1,048,576 TB1 ZB = 1,024 EB = 1,048,576 PB（全球数据总量级别） 数据增长速度快 数据种类多 文字 图片 音频 视频.. 数据的价值密度低 整体价值高 数据来源 公司中的自己的业务数据 淘宝 京东 第三方 爬虫 爬数据 数据处理 缺失数据的处理 考虑缺失数据是否影响整体的业务逻辑 不影响 删除 如果是和钱相关的数据 —-慎重 不能轻易删除 敏感数据 脱敏处理 – 加密 数据价值 人物画像 根据根据用户数据给用户做一个全方位的分析画像 属性： 人脉 消费水平 性格特点 …. 几个概念集群: 多个机器共同协作完成同一个任务, 每一个机器叫做节点, 多个机器共同组成的群体叫做集群 集群中的每个节点之间通过局域网或其他方式通讯 分布式: 分而治之 , 一个任务呗分成多个子任务模块, 每个任务跑在不同的节点上 原来一个人干的事情, 现在大家分工劳动 分布式的文件系统 , 分布式数据库, 分布式计算系统 负载均衡: Nginx 每个节点分配到的任务基本均衡 负载均衡是跟每个节点自身的配置等匹配的 不存在绝对的均衡 Hadoop概念 一个分布式的开源框架 支持成千上万的节点, 每个节点依靠本地的计算和存储 在应用层面提供高可用性 将硬件错误看成一个常态 Hadoop的模块 Common 支持其他 Hadoop 模块的公共实用程序 封装: 工具类, RPC框架 HDFS Hadoop的分布式文件系统, 负责海量数据的存储 将文件切分成指定大小的数据块并以多副本的存储形式存储在多个机器上 数据切分, 多副本, 容错等操作对用户是透明的 架构: 主从架构 ( Java进程) 主: namenode 一个 从: datenode 多个 助理: SecondaryNamenode 分担主进程的压力 YARN 集群的资源调度框架, 负责集群的资源管理 架构: 主从架构 主: ResourceManager – 负责统筹资源 从: NodeManager MapReduce 分布式计算框架, 有计算任务的时候才会有响应的进程 Hadoop的搭建搭建前的准备123456789101112131415161718192021222324252627282930313233343536373839404142搭建准备：1）ip配置2）主机名 vi /etc/sysconfig/network3）主机映射4）关闭防火墙和sellinux service iptables stop vi /etc/selinux/config SELINUX=disabled5）将系统的启动级别改为3 vi /etc/inittab6）创建普通用户，并为普通用户添加sudolers权限 创建用户：useradd 用户名 passwd 用户名 vi /etc/sudoers hadoop ALL=(ALL) ALL7）配置免密登录 先切换到普通用户 1）生成秘钥 ssh-keygen 2)发送秘钥 ssh-copy-id hadoop(主机名) 验证：ssh hadoop 8）安装jdk 卸载jdk： rpm -qa|grep jdk rpm -e java-1.7.0-openjdk-1.7.0.99-2.6.5.1.el6.x86_64 --nodeps9）时间同步 伪分布式不需要 分布式需要，必须做10)选择安装版本： 不选太陈旧的版本也不选最新的版本 2.7.6 11)安装 一定切换用户 普通用户 方式1: 伪分布式所有进程全部运行在同一个节点上 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162631）上传2）解压3）修改配置文件 配置文件的目录：HADOOP_HOME/etc/hadoop 需要修改6个配置文件： 1）hadoop-env.sh export JAVA_HOME=/home/hadoop/jdk1.8.0_73/ 2)core-site.xml 核心配置文件 &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop:9000&lt;/value&gt; &lt;/property&gt; 3)hdfs-site.xml &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; 4)yarn-site.xml &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; 5)mapred-site.xml &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 6)slaves 配置的是从节点的信息 7)配置环境变量 export JAVA_HOME=/home/hadoop/jdk1.8.0_73 export HADOOP_HOME=/home/hadoop/hadoop-2.7.6 export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin source /etc/rpofile 验证 hadoop version 8)先进行格式化 hadoop namenode -format 9)启动 start-all.sh 不建议 建议以下命令： start-dfs.sh start-yarn.sh 10）验证 jps 6个进程 3909 Jps 3736 ResourceManager 3401 DataNode 3306 NameNode 3836 NodeManager 3597 SecondaryNameNode 页面： hdfs：namenode的ip：50070 yarn:resourcemanager的ip：8088 方式2: 完全分布式参考文档 各个节点的安装的普通用户名必须相同 密码也得相同, 每个节点都需要操作 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879搭建准备： 1）ip配置 2）主机名 vi /etc/sysconfig/network 3）主机映射 4）关闭防火墙和sellinux service iptables stop vi /etc/selinux/config SELINUX=disabled 5）将系统的启动级别改为3 6）创建普通用户，并为普通用户添加sudolers权限 创建用户：useradd 用户名 passwd 用户名 vi /etc/sudoers hadoop ALL=(ALL) ALL 7）配置免密登录 先切换到普通用户 每台机器都需要执行下面的操作 各个节点之间都做一下 a. 生成秘钥 ssh-keygen b. 发送秘钥 ssh-copy-id hadoop(主机名) c. 验证：各个节点之间都需要做相互验证 ssh hadoop01 ssh hadoop02 ssh hadoop03 8）安装jdk 卸载jdk： rpm -qa|grep jdk rpm -e java-1.7.0-openjdk-1.7.0.99-2.6.5.1.el6.x86_64 --nodeps 9）时间同步 伪分布式不需要 分布式需要，必须做 1)不能联网的时候 手动指定 date -s 时间 或者手动搭建一个时间服务器 2）能联网的时候 找一个公网中的公用的时间服务器 所有节点的时间和公网中的时间服务器保持一致 ntpdate 公网的时间服务器地址 完全分布式必须要做 每个节点都需要执行 10) 选择安装版本： 不选太陈旧的版本也不选最新的版本 2.7.6 11) 安装 一定切换用户 普通用户 先在一个节点上执行所有的修改 在远程发送到其他节点 1）上传 2）解压d 3）配置环境变量 4）修改配置文件 6个配置文件 集群规划 .... .... 5)远程发送 scp -r hadoop-2.7.6 hadoop02:$PWD scp -r hadoop-2.7.6 hadoop03:$PWD scp -r /home/ap/apps/hadoop-2.7.6/etc/hadoop ap@cs1:/home/ap/apps/hadoop-2.7.6/etc 远程发送/etc/pofile 执行source /etc/pofile 6）进行格式化 必须在namenode的节点（hdfs的主节点） hadoop namenode -format 不配置目录默认/tmp 临时目录 可以随时回收的 7)启动 启动hdfs start-dfs.sh 在任意节点都可以 启动yarn start-yarn.sh 在yarn的主节点执行 jps命令查看 网页： hdfs: hadoop01:50070 yarn hadoop03:8088 8) 去掉警告（在/etc/profile或者 .bash_profile 或者 .zshrc中添加） export HADOOP_HOME_WARN_SUPPRESS=1 测试yarn集群是否启动成功 (提交MapReduce例子程序试跑) ls apps/hadoop-2.7.6/share/hadoop/mapreduce bin/hadoop jar hadoop-mapreduce-examples-2.6.5.jar pi 5 5 可能遇到的错误搭建过程中 主机找不到 /etc/sysconfig/network /etc/hosts 重启机器 何时化的时候报错 配置文件错误, 根据错误去相应文件进行调整, 修改完毕后, 重新格式化直到格式化成功 启动过程中某些进程启动不了措辞1: 暴力 全部关闭集群重新启动 stop-dfs.sh 在任意节点执行 stop-yarn.sh 在yarn的主节点启动 重新启动, 直接启动就可以了 start-dfs.sh start-yarn.sh 措施2: 单独启动某些进程单独启动hdfs的相关进程 hadoop-daemon.sh start hdfs 过程 hadoop-daemon.sh start namenode hadoop-daemon.sh start secondarynamenode 单独启动yarn的相关命令 yarn-daemon.sh start yarn 的相关过程 yarn-daemon.sh start resourcemanager ==搭建过程中的注意事项== 集群的只能成功的格式化一次, 不成功的要一直到格式化成功, 成功后就不能再次格式化 格式化的过程中: 创建出来namenode存储的相关目录 version文件: 记录仪集群的版本信息的, 每格式化一次, 就会产生一个新的版本信息 123456namespaceID=1163449973clusterID=CID-47f10077-2aef-4df6-a364-1a735515a100 #记录集群的版本信息的cTime=0storageType=NAME_NODEblockpoolID=BP-1527239677-192.168.75.162-1527817150436layoutVersion=-63 启动hdfs的时候: 生成datanode的相关数据信息 version: 记录datanode 相关版本的信息 1clusterID=CID-47f10077-2aef-4df6-a364-1a735515a100 两个文件中的clusterID相同的时候, datanode 才会认为是同一个集群的 想要重复格式化, 分3步走 停止所有服务 删除 namenode 的数据目录 rm -rf /home/ap/data/hadoopdata/name 删除 datanode 的数据目录 rm -rf /home/ap/data/hadoopdata/data 此时才可以重新格式化, 否则会造成 datanode 启动不了, 注意, 关闭防火墙, 关闭vpn 也可以一步到位 rm -rf /home/ap/data 再重新格式化 集群搭建过程中环境变量的配置(jdk. hadoop…) 在Linux中修改环境变量的地方有3个 /etc/profile 系统环境变量, 针对所有用户的 ~/.bashrc 用户环境变量 ~/.bash_profile 用户环境变量 这3个配置文件的加载顺序 /etc/profile &gt; .bashrc &gt; .bash_profile 生效: 最后一次加载的生效 时间同步问题 只要是完全分布式的, 多个节点之间一定要做时间同步, 目的: 要和 北京/上海 时间保持一致? no 集群背部各个节点之间时间保持一致 yes why ? 集群内部各个节点之间需要通信, 尤其是datanode 和 namenode之间, 他们之间的通信依靠心跳机制, 他们之间的心跳存在一个时间控制, 这个时间是 630s, 他们之前需要做时间同步 集群的安装模式1. 单机模式 直接解压的方式, 什么都不配置, 并且在一个节点上 没有分布式文件系统, 所有的文件都是来自本地, 只能对本地的文件进行读写 几乎没人用, 测试时偶尔会用 2. 伪分布式 可以看做跑在一个节点上的完全分布式 有分布式文件系统, 只不过这个文件系统只有一个节点 ==3. 完全分布式==参考文档 规划 目前疑点: NodeManager是根据什么配置到每台机器上的?? 根据表征, 可能是根据 slave文件 主机名 / IP HDFS YARN cts1 / 192.168.56.131 NameNode 空的 cts2 / 192.168.56.132 DataNode NodeManager cts3 / 192.168.56.133 DataNode + Secondary NameNode NodeManager cts4 / 192.168.56.134 DataNode NodeManager +ResourceManager hdfs 为例 : 在宏观看就是一个大的节点, 后台采用的硬件配置是三天机器的硬件配置之和, 但是对用户来讲完全感觉不到 在完全分布式中, 有主节点, 有从节点 主节点 namenode只有一个, 从节点有多个, 真实生产中, namenode会单独做一个节点 如果集群中namenode宕机, 整个集群还可以使用吗? 不可以 namenode: 主要作用存储元数据 (管理数据的数据, 存储的就是datanode存储数据的描述) datanode: 负责集群中真正处理数据存存储的 如果namenode 宕机, 集群无法使用, 这也是完全分布式的一大缺点, 存在单点故障问题 一般生产中不太使用, 学习, 测试, 节点个数比较少的时候, 有时候也会使用这种模式 节点数目越多, namenode宕机的可能性越大, 压力太大 助理secondarynamenode: 只是一个助理, 只是分担namenode的压力, 但是不能代替 架构: 一主多从 ==4. 高可用== 概念: 集群可以持续对外提供服务, 做到 7*24 小时不间断 依赖于zookeeper, 搭建放在 zookeeper课程之后 集群架构: 双主多从 有2个 namenode, 但是在同一时间只能有一个是 活跃的 namenode, 我们把这个活跃的namenode 成为 active 的, 另外一个是处理热备份状态, 我们将这个节点叫 standby, 但是2个主节点存储的元数据是一模一样的, 当 active namenode宕机的时候, standby的namenode 可以立马切换为 active 的namenode, 对外提供服务, 就可以做到 集群持续对外提供服务的功能 如果过一段时间, 宕机的 namenode 又活过来了, 宕机的 namenode 只能是变成 standby 的 缺陷: 在同一时间中, 集群中只有一个active 的 namenode, 也就是说 集群中有主节点能力的节点 只有一个, 如果集群中, 节点个数过多(1000) 的时候, 会造成namenode的崩溃, namenode存储的是元数据, 元数据过多的时候, 会造成namenode的崩溃(两个都崩溃), 没有真正的分担namenode 的压力 实际生产多使用高可用 5. 联邦机制 同一个集群中可以有多个主节点, 这些主节点的地位是一样的. 同一时间, 可以有多个活跃的 namenode 这些 namenode 共同使用集群中所有的 datanode, 每个namenode 只负责管理集群中的 datanode上的一部分数据 一般超大集群搭建的时候: 联邦 + 高可用 超大集群使用 每个namenode进行数据管理靠的Block Pool ID相同 不同的namenode管理的数据Block Pool ID 不同]]></content>
      <categories>
        <category>Hadoop</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop体系概览]]></title>
    <url>%2F2018%2F05%2F31%2FHadoop%2FStudy%2F0-Hadoop%2FHadoop%E4%BD%93%E7%B3%BB%E6%A6%82%E8%A7%88%2F</url>
    <content type="text"><![CDATA[Hadoop核心组件整体直观概览 1. 分布式文件系统HDFS1.1 基本概念 将文件切分成指定大小的数据块并以多副本的存储形式存储在多个机器上 数据切分, 多副本, 容错等操作对用户是透明的 1.2 图示 1.3 HDFS架构 Datanode 定期向 Namenode 发 Hearbeat 元数据信息： 多份备份 1.4 HDFS的 IO 操作 上面的是读 客户端先向 NameNode 寻址 然后再找 DataNode 拿数据 下面的是写 HDFS 不支持修改， 没有 leader 角色， 不支持并发写， 只能支持非并发的追加 HBase 支持并发写和修改 删除： 删除的是元数据（索引信息） Datanode 会定期向 Namenode 发送心跳， 同步信息， 当 Namenode 发现 Datanode 上没有自己存储的信息时，就会把这部分信息删除掉。 1.5 HDFS 副本存放策略 复制因子为3时的 存放策略 如果写入者在一个 datanode 上， 则把一份拷贝放在本地机器上， 否则随机放到一个 datande 上 另一个副本放在不同的（远程）机架的节点上， 最后一个副本存放在同一个机架的不同节点上 这一策略削减了机架间的写入流量，通常提高了写入性能 复制因子大于3， 则随机确定第4个和其它的副本位置，同时将每个拷贝的数目保持在上限以下(基本上是(副本数 - 1) / racks + 2)。 2. 资源调度系统 YARN2.1 基本概念 YARN: Yet Another Resource Negotiator 负责整个集群资源的管理和调度 YAEN特点: 扩展性 &amp; 容错性 &amp; 多框架资源统一调度 2.2 图示 3. 分布式计算框架 MapReduce3.1 基本概念 源于Google的MapReduce论文, 论文发表于2004年12月 MapReduce是Google MapReduce的克隆版 MapReduce的特点: 扩展性 &amp; 容错性 &amp; 海量数据离线处理 3.2 图示 Hadoop优势1. 高可靠性 数据存储: 数据块多副本 数据计算: 重新调度作业计算 2. 高扩展性 存储/计算资源不够时, 可以横向的线性扩展机器 一个集群中可以包含数以千计的节点 3. 其它 存储在廉价机器上, 降低成本 成熟的生态圈 Hadoop的发展史见文章: Hadoop十年解读与发展预测 Hadoop官网 Hadoop生态系统1. 图示 2. 特点 开源, 社区活跃 囊括了大数据处理的方方面面 成熟的生态圈 Hadoop常用发行版及选型 Apache Hadoop CDH : Cloudera Distributed Hadoop （国内用的比较多） HDP : Hortonworks Data Platform 使用: CDH使用占比 60-70 hadoop: hadoop-2.6.0-cdh5.7.0 hive : hive-1.1.0-cdh5.7.0]]></content>
      <categories>
        <category>Hadoop</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Hadoop</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础增强-2 并发编程]]></title>
    <url>%2F2018%2F05%2F30%2FJava%2FJava%E5%9F%BA%E7%A1%80%E5%A2%9E%E5%BC%BA-2%2F</url>
    <content type="text"><![CDATA[1. 多线程基本知识1.1 多线程运行的原理 原理：CPU 在线程中做时间片的切换。 一个(多核中的一个) CPU 在在运行程序的过程中某个时刻点上，只能运行一个程序。而 CPU 可以在 多个程序之间进行高速的切换 (轮询制)。而切换频率和速度太快，导致人的肉眼看不到。 1.2 实现线程的两种方式 继承 Thread 声明实现Runnable接口 还可以实现Callable接口 1.3 线程的状态图解 新建状态（New）：新创建了一个线程对象。 就绪状态（Runnable）：线程对象创建后，其他线程调用了该对象的 start()方法。该状态 的线程位于可运行线程池中，变得可运行，等待获取 CPU 的使用权。 运行状态（Running）：就绪状态的线程获取了 CPU，执行程序代码。 阻塞状态（Blocked）：阻塞状态是线程因为某种原因放弃 CPU 使用权，暂时停止运行。 直到线程进入就绪状态，才有机会转到运行状态。阻塞的情况分三种： 等待阻塞：运行的线程执行 wait()方法，JVM 会把该线程放入等待池中。(wait 会释 放持有的锁) 同步阻塞：运行的线程在获取对象的同步锁时，若该同步锁被别的线程占用，则 JVM 会把该线程放入锁池中。 其他阻塞：运行的线程执行 sleep()或 join()方法，或者发出了 I/O 请求时，JVM 会把 该线程置为阻塞状态。当 sleep()状态超时、join()等待线程终止或者超时、或者 I/O 处理完毕 时，线程重新转入就绪状态。（注意：sleep 是不会释放持有的锁） 死亡状态（Dead）：线程执行完了或者因异常退出了 run()方法，该线程结束生命周期。 1.4 几个重要的方法的区别： sleep(timeout)：当前线程进入阻塞状态，暂停执行一定时间，不会释放锁标记 join()：join()方法会使当前线程等待调用 join()方法的线程结束后才能继续执行 yield()：调用该方法的线程重回可执行状态，不会释放锁标记，可以理解为交出 CPU 时间片， 但是不一定有效果，因为有可能又被马上执行。该方法的真正作用是使具有相同或者更高优 先级的方法得到执行机会。 wait(timeout)：wait 方法通常和 notify()/notifyAll()搭配使用，当前线程暂停执行，会释放锁 标记。进入对象等待池。直到调用 notify()方法之后，线程被移动到锁标记等待池。只有锁 标记等待池的线程才能获得锁 1.5 Join的用法联合线程: 线程的join方法表示一个线程等待另一个线程完成后才执行。有人也把这种方式称为联合线程，就是说把当前线程和当前线程所在的线程联合成一个线程。join方法被调用之后，线程对象处于阻塞状态。 适用于A线程需要等到B线程执行完毕,再拿B线程的结果再继续运行A线程. 说人话: A线程需要拿到B线程的执行结果,才能继续往下. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081class Join extends Thread&#123; @Override public void run() &#123; for (int i = 0; i &lt; 50; i++) &#123; System.out.println("This is join: " + i); &#125; &#125;public class JoinThread &#123; public static void main(String[] args) throws InterruptedException &#123; System.out.println("begin..."); Join joinThread = new Join();//创建join线程对象 for (int i = 0; i &lt; 50; i++) &#123; System.out.println("main: " +i); if (i == 10)&#123; //启动join对象 joinThread.start(); &#125; if (i == 20)&#123; System.out .println("------------------------------------------"); joinThread.join();//在此处强制运行该线程 &#125; &#125; &#125;&#125; ===================执行结果===================begin...main: 0main: 1main: 2main: 3main: 4main: 5main: 6main: 7main: 8main: 9main: 10main: 11main: 12main: 13This is join: 0main: 14This is join: 1This is join: 2This is join: 3main: 15This is join: 4main: 16main: 17main: 18main: 19main: 20------------------------------------------This is join: 5This is join: 6This is join: 7This is join: 8This is join: 9This is join: 10......This is join: 44This is join: 45This is join: 46This is join: 47This is join: 48This is join: 49main: 21main: 22main: 23main: 24main: 25main: 26main: 27main: 28main: 29main: 30 2. Java同步关键词解释2.1. synchronized属于 JVM 级别加锁，底层实现是： 在编译过程中，在指令级别加入一些标识来实现的。 1. 锁对象注意点: 必须是锁的同一个对象 2. 锁获取和释放 锁的获取是由JVM决定的, 用户无法操作 锁的释放也是由JVM决定的 Synchronized 无法中断正在阻塞队列或者等待队列的线程。 3. 什么时候会释放 获取锁的线程执行完了该代码块，然后线程释放对锁的占有； 线程执行发生异常，此时 JVM 会让线程自动释放锁。 4.格式 1234// 加同步格式：synchronized(需要一个任意的对象（锁）)&#123; 代码块中放操作共享数据的代码。&#125; 5.线程执行互斥代码的过程 获得互斥锁 清空工作内存 从主内存拷贝变量的最新副本到工作内存 执行代码 将更新后的共享变量的值刷新到主内存 释放互斥锁 Lock -&gt; 主内存 -&gt; 工作内存 -&gt; 主内存 -&gt; unlock 2.2 Lock手动获取或释放锁, 提供了比 synchronized 更多的功能 Lock 锁是 Java 代码级别来实现的，相对于 synchronized 在功能性上，有所加强，主要是，公平锁，轮 询锁，定时锁，可中断锁等，还增加了多路通知机制（Condition），可以用一个锁来管理多 个同步块。另外在使用的时候，必须手动的释放锁。Lock 锁的实现，主要是借助于队列同 步器（我们常常见到的 AQS）来实现。它包括一个 int 变量来表示状态；一个 FIFO 队列，来 存储获取资源的排队线程。 基本使用 12345678910111213class X &#123; // 创建一把锁 private final ReentrantLock lock = new ReentrantLock(); // 需要做同步的方法 public void m() &#123; lock.lock(); //获取🔐, 加锁 try &#123; // 代码 &#125; finally &#123; lock.unlock(); // 释放🔐 &#125; &#125;&#125; 1. lock 和 synchronized 的区别 Lock 不是 Java 语言内置的，synchronized 是 Java 语言的关键字，因此是内置特性。Lock 是一个类，通过这个类可以实现同步访问； Lock 和 synchronized 有一点非常大的不同，采用 synchronized 不需要用户去手动释放锁， 当 synchronized 方法或者 synchronized 代码块执行完之后，系统会自动让线程释放对锁的占 用；而 Lock 则必须要用户去手动释放锁，如果没有主动释放锁，就有可能导致出现死锁现 象。 2. Lock 接口中方法的使用 ReentrantLock 类 ReentrantLock 是唯一实现了 Lock 接口的类，并且 ReentrantLock 提供了更多的方法，ReentrantLock，意思是“可重入锁”。 lock()、tryLock()、tryLock(long time, TimeUnit unit)、lockInterruptibly()是用来获取锁的。 unLock()方法是用来释放锁的。 四个获取锁方法的区别 lock()，阻塞方法，该方法是平常使用得最多的一个方法，就是用来获取锁。如果锁已被 其他线程获取，则进行等待。由于在前面讲到如果采用 Lock，必须主动去释放锁，并且在 发生异常时，不会自动释放锁。因此一般来说，使用 Lock 必须在 try{}catch{}块中进行，并 且将释放锁的操作放在 finally 块中进行，以保证锁一定被被释放，防止死锁的发生。 tryLock()，非阻塞方法，该方法是有返回值的，它表示用来尝试获取锁，如果获取成功， 则返回 true，如果获取失败（即锁已被其他线程获取），则返回 false，也就说这个方法无论 如何都会立即返回。在拿不到锁时不会一直在那等待。 tryLock(long time, TimeUnit unit)，阻塞方法，阻塞给定时长，该方法和 tryLock()方法是 类似的，只不过区别在于这个方法在拿不到锁时会等待一定的时间，在时间期限之内如果还 拿不到锁，就返回 false。如果一开始拿到锁或者在等待期间内拿到了锁，则返回 true。 lockInterruptibly()这个方法比较特殊，当通过这个方法去获取锁时，如果线程正在等待 获取锁，则这个线程能够响应中断，即中断线程的等待状态。也就使说，当两个线程同时通 过 lock.lockInterruptibly()想获取某个锁时，假若此时线程 A 获取到了锁，而线程 B 只有在等 待，那么对线程 B 调用 threadB.interrupt()方法能够中断线程 B 的等待过程。 2.3 Lock 与 synchronized 的选择 Lock 是一个接口，而 synchronized 是 Java 中的关键字，synchronized 是内置的语言实现； synchronized 在发生异常时，会自动释放线程占有的锁，因此不会导致死锁现象发生； 而 Lock 在发生异常时，如果没有主动通过 unLock()去释放锁，则很可能造成死锁现象，因 此使用 Lock 时需要在 finally 块中释放锁； Lock 可以让等待锁的线程响应中断，而 synchronized 却不行，使用 synchronized 时，等 待的线程会一直等待下去，不能够响应中断； 通过 Lock 可以知道有没有成功获取锁，而 synchronized 却无法办到。 Lock 可以提高多个线程进行读操作的效率。 2.4 读写锁 线程进入读锁的前提条件： 没有其他线程的写锁, 没有写请求或者有写请求，但调用线程和持有锁的线程是同一个 线程进入写锁的前提条件： 没有其他线程的读锁, 没有其他线程的写锁 ReentrantReadWriteLock 与 ReentrantLock 都是单独的实现，彼此之间没有继承或实现的关系。 ReadWriteLock 类123456// API// 可以区别对待读、写的操作public interface ReadWriteLock &#123; Lock readLock(); Lock writeLock();&#125; ReentrantReadWriteLock 类 ReentrantReadWriteLock 里面提供了很多丰富的方法，不过最主要的有两个方法：readLock()和 writeLock()用来获取读锁和写锁。 注意：不过要注意的是，如果有一个线程已经占用了读锁，则此时其他线程如果要申请写 锁，则申请写锁的线程会一直等待释放读锁。 如果有一个线程已经占用了写锁，则此时其他线程如果申请写锁或者读锁，则申请的线程 会一直等待释放写锁。 2.5 死锁死锁是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象， 若无外力作用，它们都将无法推进下去。这是一个严重的问题，因为死锁会让你的程序挂起 无法完成任务。 死锁的发生必须满足以下四个条件： 互斥条件：一个资源每次只能被一个进程使用。 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 不剥夺条件：进程已获得的资源，在末使用完之前，不能强行剥夺。 循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。 避免死锁最简单的方法就是阻止循环等待条件，将系统中所有的资源设置标志位、排序，规 定所有的进程申请资源必须以一定的顺序（升序或降序）做操作来避免死锁 2.6 Volatile 特殊域变量多线程编程，我们要解决的问题集中在三个方面： 原子性。最简单的例子就是，i++,在多线程环境下，最终的结果是不确定的，为什么？就 是因为这么一个++操作，被编译为计算机底层指令后，是多个指令来完成的。那么遇到并发 的情况，就会导致彼此“覆盖”的情况。 可见性。通俗解释就是，在 A 线程对一个变量做了修改，在 B 线程中，能正确的读取到 修改后的结果。究其原理，是 cpu 不是直接和系统内存通信，而是把变量读取到 L1，L2 等 内部的缓存中，也叫作私有的数据工作栈。修改也是在内部缓存中，但是何时同步到系统内 存是不能确定的，有了这个时间差，在并发的时候，就可能会导致，读到的值，不是最新值。 指令重排。这里只说指令重排序，虚拟机在把代码编译为指令后执行，出于优化的目的， 在保证结果不变的情况下，可能会调整指令的执行顺序。 valotile，能满足上述的可见性和有序性。但是无法保证原子性。 可见性，是在修改后，强制把对变量的修改同步到系统内存。而其他 cpu 在读取自己的内部 缓存中的值的时候，发现是 valotile 修饰的，会把内部缓存中的值，置为无效，然后从系统 内存读取。 有序性，是通过内存屏障来实现的。所谓的内存屏障，可以理解为，在某些指令中，插入屏 障指令，用以确保，在向屏障指令后面继续执行的时候，其前面的所有指令已经执行完毕。 3. Java多线程中常见的面试题1. sleep(),wait(),join(),yield()四个方法的区别总结： 1）：sleep()，Thread 类中的方法，表示当前线程进入阻塞状态，不释放锁 2）：wait()，Object 类中的方法，表示线程进入等待状态，释放锁，所以一般能调用这个方 法的都是同步代码块，或者获取了锁的线程代码，通常和 notify()和 notifyAll()方法结合使用 3）：join()，Thread 类中的方法，假如在 a 线程中调用 b 线程对象的 join()方法，表示当前 a 线程阻塞，直到 b 线程运行结束 4）：yield()，Thread 类中的方法，表示线程回可执行状态。跟 sleep 方法一样，也不交出锁， 只不过不带时间参数，是指交出 cpu 2. Thread 和 Runnable 的区别总结： 实现 Runnable 接口比继承 Thread 类所具有的优势： 1）：适合多个相同的程序代码的线程去处理同一个资源 2）：可以避免 java 中的单继承的限制 3）：增加程序的健壮性，代码可以被多个线程共享，代码和数据独立 4）：线程池只能放入实现 Runable 或 callable 类线程，不能直接放入继承 Thread 的类 …]]></content>
      <categories>
        <category>Java</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础增强-1 集合反射设计模式排序]]></title>
    <url>%2F2018%2F05%2F29%2FJava%2FJava%E5%9F%BA%E7%A1%80%E5%A2%9E%E5%BC%BA-1%2F</url>
    <content type="text"><![CDATA[1、集合框架1.1 、集合框架体系图 Java 的集合框架主要分为五大类体系：1、Collection（常用的 List 和 Set，和不常用的 Queue 和 Vector 和 Stack），单元素集合2、Map（常用的 HashMap 和 TreeMap，不常用的 HashTable），Key-Value 映射3、Iterator（迭代器）4、工具类（Collections 和 Arrays）5、Comparable 和 Comparator 比较器 Java 中的集合和数组的区别:1、数组长度在初始化时指定，意味着只能保存定长的数据。而集合可以保存数量不确定的 数据。同时可以保存具有映射关系的数据（即关联数组，键值对 key-value）。 2、数组元素即可以是基本类型的值，也可以是对象。集合里只能保存对象（实际上只是保存对象的引用变量），基本数据类型的变量要转换成对应的包装类才能放入集合类中。 Collection 接口中的方法： Map 接口中的方法： 1.2、常用集合特性概述1.2.1 List 系 List 特点：元素有放入顺序，元素可重复 List 接口有三个实现类：LinkedList，ArrayList，Vector LinkedList：底层基于链表实现，链表内存是散乱的，每一个元素存储本身内存地址的同时还 存储下一个元素的地址。链表增删快，查找慢 ArrayList 和 Vector 底层都是基于数组实现的，查询快，增删慢，区别是 ArrayList 是非线程安全的，效率高；Vector 是基于线程安全的，效率低 ArrayList 的初始化大小是 10，扩容策略是 1.5 倍原元素数量的大小 数组 初始容量+扩容 (jdk10)12345678910111213141516171819// 初始容量private static final int DEFAULT_CAPACITY = 10;// 扩容private int newCapacity(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt;= 0) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) return Math.max(DEFAULT_CAPACITY, minCapacity); if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return minCapacity; &#125; return (newCapacity - MAX_ARRAY_SIZE &lt;= 0) ? newCapacity : hugeCapacity(minCapacity); &#125; 选择标准： 如果涉及到“动态数组”、“栈”、“队列”、“链表”等结构，应该考虑用 List，具体的选择哪 个 List，根据下面的标准来取舍。 1、对于需要快速插入，删除元素，应该使用 LinkedList。（增删改） 2、对于需要快速随机访问元素，应该使用 ArrayList。（查询） 3、对于“单线程环境”或者“多线程环境，但 List 仅仅只会被单个线程操作”，此时应该使 用非同步的类(如 ArrayList)。对于“多线程环境，且 List 可能同时被多个线程操作”，此时， 应该使用同步的类(如 Vector)。 LinkedList add(E e)1234567891011121314151617# add(E e) 源码public boolean add(E e) &#123; linkLast(e); return true;&#125; void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125; 1.2.2、Set 系 Set 特点：元素放入无顺序，元素不可重复 Set 接口的实现类：HashSet，TreeSet，LinkedHashSet HashSet（底层由 HashMap 实现）底层通过 hashCode()和 equals()进行去重。 HashSet 内部判断相等的标准HashSet 判断两个元素相等的标准： ​ 两个对象通过 equals()方法比较相等，并且两个对象的 hashCode()方法返回值也相等 HashSet 中判断集合元素相等，两个对象比较具体分为如下四个情况： 如果有两个元素通过 equal()方法比较返回 false，并且它们的 hashCode()方法返回不相等， HashSet 将会把它们存储在不同的位置。 如果有两个元素通过 equal()方法比较返回 true，并且它们的 hashCode()方法返回不相等， HashSet 将会把它们存储在不同的位置。 如果两个对象通过 equals()方法比较不相等，hashCode()方法比较相等，HashSet 将会把它们存储在相同的位置，在这个位置以链表式结构来保存多个对象。这是因为当向 HashSet 集合中存入一个元素时，HashSet 会调用对象的 hashCode()方法来得到对象的 hashCode 值， 然后根据该 hashCode 值来决定该对象存储在 HashSet 中存储位置。 如果有两个元素通过 equal()方法比较返回 true，并且它们的 hashCode()方法返回 true，HashSet 将不予添加。 LinkedHashSet，是 HashSet 的子类，在插入元素的时候，同时使用链表维持插入元素的顺序 SortedSet 接口有一个实现类：TreeSet（底层由平衡二叉树实现）确保集合中的元素都是出于排序状态 注意 LinkedHashSet 和 SortedSet 区别，前者存插入顺序，后者存插入之后的顺序 另外: JDK5 : 桶表 + 链表 JDK8 : 桶表 + 链表 + 二叉树 - **二叉树**: 检索深度 &gt; 8 的时候, 转化为二叉树, 减少查询深度 HashSet — HashMap 的源码实现123456789101112131415161718192021222324252627282930313233343536373839/** * The default initial capacity - MUST be a power of two. * 桶表默认容量 16 * 控制hashcode 不超16范围, a.hashcode = xx % 16 (hashcode 取模 桶个数) */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16/** * MUST be a power of two &lt;= 1&lt;&lt;30. * 桶表最大 2^30 */static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;/** * 扩容因子: 0.75 * 扩容: 每次2倍 */static final float DEFAULT_LOAD_FACTOR = 0.75f;/** * 链表: hash算法值相同的时候, 会把值相同的放在一个链表上, 链表上的元素个数 * 超过8个时, 转化为二叉树, 提升查询效率 */static final int TREEIFY_THRESHOLD = 8;/** * The bin count threshold for untreeifying a (split) bin during a * resize operation. Should be less than TREEIFY_THRESHOLD, and at * most 6 to mesh with shrinkage detection under removal. */static final int UNTREEIFY_THRESHOLD = 6;/** * The smallest table capacity for which bins may be treeified. * (Otherwise the table is resized if too many nodes in a bin.) * Should be at least 4 * TREEIFY_THRESHOLD to avoid conflicts * between resizing and treeification thresholds. */static final int MIN_TREEIFY_CAPACITY = 64; TreeSet的默认排序 TreeSet是有序的不可重复的, 有序是指元素值的大小 数值类型: 按照大小进行升序排序 字符串类型: 按照字典顺序进行升序排序 字符串从左到右, 一位一位的比较 自定义TreeSet类型: 实现 compareTo方法, 返回为0的情况会默认覆盖 1.2.2、Map 系Map 特点：存储的元素是键值对，在 JDK1.8 版本中是 Node，在老版本中是 Entry Map 接 口 有 五 个 常用 实 现 类 ： HashMap ， HashTable ， LinkeHashMap ， TreeMap ， ConcurrentHashMap 1. HashMap &amp; Hashtable 的区别HashMap 1. 非线程安全, 效率高 2. key不可以重复 3. key可以为null, 但只能有一个key为null Hashtable 线程安全, 效率低 key不可以重复 不可以为null 2. concurrentHash 简单分析是从 JDK1.5 之后提供的一个 HashTable 的替代实现，采 一个 map 中的元素分成很多的 segment，通过 lock 机制可以对每个 segment 加读写锁，从 而提高 map 的效率，底层实现采用数组+链表+红黑树的存储结构 Java并发包中的, 既是线程安全的, 又不至于效率过低 怎么实现: 分段锁机制 分段锁: 只加载在某一段数据上 MySql: 查询 - 95%, 增删改 - 5% 读锁: 共享锁, 一个线程进行操作的时候不应吸纳另一个线程的结构 写锁: 排它锁, 一个线程在进行操作的时候不允许其他任何线程的操作 3. put &amp; get 的流程put 的大致流程如下： 通过 hashcode 方法计算出 key 的 hash 值 通过 hash%length 计算出存储在 table 中的 index（源码中是使用 hash&amp;(length-1)，这样结 果相同，但是更快） 如果此时 table[index]的值为空，那么就直接存储，如果不为空那么就链接到这个数所在 的链表的头部。（在 JDK1.8 中，如果链表长度大于 8 就转化成红黑树） get 的大致流程如下： 通过 hashcode 计算出 key 的 hash 值 通过 hash%length 计算出存储在 table 中的 index（源码中是使用 hash&amp;(length-1)，这样结 果相同，但是更快） 遍历 table[index]所在的链表，只有当 key 与该节点中的 key 的值相同时才取出。 1.3 掌握重点List: ArrayList, LinkList Set: HashSet, TreeSet 需要掌握的方法: add , get, contains Map: HashMap, TreeMap 需要掌握的方法: put get map的循环遍历 containsKey…. 以上的都需要跟下源码 1.4 功能方法1.4.1 List 的功能方法ArrayList: 由数组实现的 List。允许对元素进行快速随机访问，但是向 List 中间插入与移 除元素的速度很慢。ListIterator 只应该用来由后向前遍历 ArrayList，而不是用来插入和移除 元素。因为那比 LinkedList 开销要大很多。 LinkedList : 对顺序访问进行了优化，向 List 中间插入与删除的开销并不大。随机访问则 相对较慢。(使用 ArrayList 代替。)还具有下列方 法：addFirst(), addLast(), getFirst(), getLast(), removeFirst() 和 removeLast(), 这些方法 (没有在任何接口或基类中定义过)使得 LinkedList 可以当作堆栈、队列和双向队列使用。 1.4.2 Set的功能方法Set : 存入 Set 的每个元素都必须是唯一的，因为 Set 不保存重复元素。加入 Set 的元素 必须定义 equals()方法以确保对象的唯一性。Set 与 Collection 有完全一样的接口。Set 接口 不保证维护元素的次序。 HashSet : 为快速查找设计的 Set。存入 HashSet 的对象必须定义 hashCode()。 TreeSet : 保存次序的 Set，底层为树结构。使用它可以从 Set 中提取有序的序列。 LinkedHashSet : 具有 HashSet 的查询速度，且内部使用链表维护元素的顺序(插入的次 序 。于是在使用迭代器遍历 Set 时，结果会按元素插入的次序显示。 1.4.3 Map 的功能方法Map : 维护“键值对”的关联性，使你可以通过“键”查找“值” HashMap : Map 基于散列表的实现。插入和查询“键值对”的开销是固定的。可以通过 构造器设置容量 capacity 和负载因子 load factor，以调整容器的性能。 LinkedHashMap : 类似于 HashMap，但是迭代遍历它时，取得“键值对”的顺序是其插 入次序，或者是最近最少使用(LRU)的次序。只比 HashMap 慢一点。而在迭代访问时发而更 快，因为它使用链表维护内部次序。 TreeMap : 基于红黑树数据结构的实现。查看“键”或“键值对”时，它们会被排序(次 序由 Comparabel 或 Comparator 决定)。TreeMap 的特点在 于，你得到的结果是经过排序的。 TreeMap 是唯一的带有 subMap()方法的 Map，它可以返回一个子树。 WeakHashMap : 弱键(weak key)Map，Map 中使用的对象也被允许释放: 这是为解决特 殊问题设计的。如果没有 map 之外的引用指向某个“键”，则此“键”可以被垃圾收集器回 收。 IdentifyHashMap : 使用==代替 equals()对“键”作比较的 hash map。专为解决特殊问题 而设计。 2、反射2.1 反射反射: 将 Java 类中的各个成分 (属性, 方法, 构造方法) 映射成对应的类 在运行时判断任意一个对象的所属的类 Class。 在运行时判断构造任意一个类的对象 Constructor。 在运行时判断任意一个类所具有的成员变量 Field 和方法 Method。 在运行时调用任意一个对象的方法。method.invoke(object, args) 反射的好处 提高了整个代码的灵活性 不需要知道细节 反射用的最多的时候, 就是写框架的时候 反射中需要掌握3个类: Constructor: 构造器的描述类 Field: 属性的描述类 Method: 方法的描述类 Java 预定义类型 是否是预定义类型: isPromitive(), 8种基本数据类型 + void 都是预定义类型 引用类型, 包装类不是预定义类型. 12System.out.println(int.class.isPrimitive()); // trueSystem.out.println(Integer.class.isPrimitive()); // false 2.2 ClassClass : 用于描述所有类的类, Class 类描述了类的属性信息，如类名、访问权限、包名、字 段名称列表、方法名称列表等, Class就是反射的基础. 获取Class的3种方式 1. `Class.forName`(&quot;类名字符串&quot;) (注意：类名字符串必须是全称，包名+类名) - 如果 `.class`已经被加载到内存了, 直接返回 - 如果没有的话, 就先加载到内存 2. `类名.class` 3. `实例对象.getClass()` 2.3 Constructor1234567891011121314151617181920212223242526// API// 补充: 可变参数 Class&lt;?&gt;... parameterTypespublic Constructor&lt;T&gt; getConstructor(Class&lt;?&gt;... parameterTypes) &#123;&#125;public Constructor&lt;?&gt;[] getConstructors() throws SecurityException &#123;&#125; ================//使用=============================////////获取构造方法//////////// 获取某个类的所有构造方法：Constructor[] constructor = Class.forName("java.lang.String").getConstructors();// 获取某个特殊（特定参数）的构造方法：Constructor constructor = Class.forName("java.lang.String").getConstructor(StringBuffer.class);////////创建实例对象//////////// 通常方式，直接调用构造方法：String str = new String("huangbo");// 反射方式：调用实参构造String str = (String)constructor.newInstance(new StringBuffer("huangbo"));// 反射方式：调用空参构造String obj = (String)Class.forName("java.lanng.String").newInstance();只有两个类拥有 newInstance()方法，分别是 Class 类和 Constructor 类 Class 类中的 newInstance() 方法是不带参数的，Constructor 类中的 newInstance()方法是带参数的(Object)，需要提供 必要的参数 2.4 FieldField类代表某个类中的一个成员变量，设有一个 obj 对象，Field 对象不是 obj 具体的变量值， 而是指代的是 obj 所属类的哪一个变量，可以通过 Field(对象).get(obj)获取相应的变量值 123456789101112131415// APIpublic Field[] getFields() throws SecurityException &#123;&#125;public Field getField(String name) &#123;&#125;================//使用=============================// getField 方法只能获取声明为 public 的变量，对于私有变量，可以通过 getDeclaredField()方法获 取 private 变量Field field = obj.getClass().getDeclaredField();// 将 private 变量设置为可访问；继承自父类AccessibleObject 的方法才可获取变量值field.setAccessible(true); // 获得对象值, 传入对象field.get(obj); // 反射替换,设置对象值// 传入对象,值// 把 obj 对象的 field 属性的值替换为 newValuefield.set(obj,newValue) 2.5 MethodMethod 类代表某个类中的成员方法 Method 对象不是具体的方法，而是来代表类中哪一个方法，与对象无关 123456789101112// 获取: 得到类中某一个方法：Method methodCharAt = Class.forName("java.lang.String").getMethod("charAt",int.class)// getMethod 方法用于得到一个方法对象，该方法接受的参数首先要有该方法名（String 类型），// 然后通过参数列表来区分重载那个方法，参数类型用 Class 对象来表示(如为 int 就用 int.class) // 调用方法：//普通方式：str.charAt(1)//反射方式：methodCharAt.invoke(str,1)// 以上两种调用方式等价 3. 设计模式设计模式（Design pattern）代表了面向对象编程中最佳的实践，通常被有经验的面向对象的 软件开发人员所采用。设计模式是软件开发人员在软件开发过程中面临的一般问题的解决方 案。这些解决方案是众多软件开发人员经过相当长的一段时间的试验和错误总结出来的。 设计模式只不过针对某些具体场景提供了一些效率较高的以复杂度换灵活性的手段而已 推荐学习站点 3.1 设计模式 – 六大原则总原则：开闭原则（Open Close Principle） 开闭原则就是说对扩展开放，对修改关闭。在程序需要进行拓展的时候，不能去修改原有的 代码，而是要扩展原有代码，实现一个热插拔的效果。所以一句话概括就是：为了使程序的 扩展性好，易于维护和升级。想要达到这样的效果，我们需要使用接口和抽象类等，后面的 具体设计中我们会提到这点。 六大原则： 单一职责原则 不要存在多于一个导致类变更的原因，也就是说每个类应该实现单一的职责，如若不然，就 应该把类拆分。 里氏替换原则（Liskov Substitution Principle） 里氏替换原则中，子类对父类的方法尽量不要重写和重载。因为父类代表了定义好的结构，通过这个规范的接口与外界交互，子类不应该随便破坏它。 依赖倒转原则（Dependence Inversion Principle） 这个是开闭原则的基础，具体内容：面向接口编程，依赖于抽象而不依赖于具体。写代码 时用到具体类时，不与具体类交互，而与具体类的上层接口交互。 接口隔离原则（Interface Segregation Principle） 这个原则的意思是：每个接口中不存在子类用不到却必须实现的方法，如果不然，就要将 接口拆分。使用多个隔离的接口，比使用单个接口（多个接口方法集合到一个的接口）要好。 迪米特法则（最少知道原则）（Demeter Principle） 就是说：一个类对自己依赖的类知道的越少越好 。也就是说无论被依赖的类多么复杂，都 应该将逻辑封装在方法的内部，通过 public 方法提供给外部。这样当被依赖的类变化时，才 能最小的影响该类。 合成复用原则（Composite Reuse Principle） 原则是尽量首先使用合成/聚合的方式，而不是使用继承。 3.2 设计模式 – 分类总体来说设计模式分为三大类： 创建型模式，共五种：工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。 结构型模式，共七种：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合 模式、享元模式。 行为型模式，共十一种：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模 式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。 3.3 常见设计模式3.3.1 单例模式(手写)单例模式（Singleton Pattern）是 Java 中最简单的,也是最最最常用的设计模式之一。这种类 型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。 注意: 单例类只能有一个实例。 单例类必须自己创建自己的唯一实例。 单例类必须给所有其他对象提供这一实例。 共有六种实现： 1、懒汉式，线程不安全 1234567891011public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125; 2、懒汉式，线程安全 12345678910public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125; 3、饿汉式 1234567public class Singleton &#123; private static Singleton instance = new Singleton(); private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; return instance; &#125; &#125; 4、双检锁/双重校验锁（DCL，即 double-checked locking） –面试必备 1234567891011121314public class Singleton &#123; private volatile static Singleton singleton; private Singleton ()&#123;&#125; public static Singleton getSingleton() &#123; if (singleton == null) &#123; synchronized (Singleton.class) &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125; &#125; 5、登记式/静态内部类 123456789public class Singleton &#123; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton ()&#123;&#125; public static final Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125; &#125; 6、枚举 12345public enum Singleton &#123; INSTANCE; public void whateverMethod() &#123; &#125; &#125; 详细请看 3.3.2 装饰器模式(手写)首先看一段代码 代码分析: 构造一个缓冲的字符输入流。包装了一个文件字符输入流。 事实上，BufferedReader 就是用来增强 FileReader 的读取的功能的。 FileReader 只有 read()方法， 但是 BufferedReader 中却增加了一个 readLine()的逐行读取 的功能 所以这就相当于是 BufferedReader 装饰了 FileReader，让 FileReader 变得更强大 装饰器模式概念 装饰器模式（Decorator Pattern）允许向一个现有的对象添加新的功能，同时又不改变其结 构。这种类型的设计模式属于结构型模式，它是作为现有的类的一个包装。 这种模式创建了一个装饰类，用来包装原有的类，并在保持类方法签名完整性的前提下， 提供了额外的功能。 装饰器模式实现方式1. 定义包装类 2. 将要装饰的模式作为参数传入包装类 3. 实现要加强的方法 3.3.2 代理模式1. 静态代理静态代理的缺点很明显：一个代理类只能对一个业务接口的实现类进行包装，如果有多个业 务接口的话就要定义很多实现类和代理类才行。 … 2. 动态代理第一种：JDK 动态代理实现 JDK 动态代理所用到的代理类在程序调用到代理类对象时才由 JVM 真正创建，JVM 根据传 进来的业务实现类对象以及方法名，动态地创建了一个代理类的 class 文件并被字节码引擎 执行，然后通过该代理类对象进行方法调用。我们需要做的，只需指定代理类的预处理、 调用后操作即可。 只能对实现了接口的类生成代理，而不是针对类，该目标类型实现的接口都将被代理。原理 是通过在运行期间创建一个接口的实现类来完成对目标对象的代理。具体实现步骤： 定义一个实现接口 InvocationHandler 的类 通过构造函数或者静态工厂方法等，注入被代理类 实现 invoke(Object proxy, Method method, Object[] args)方法 在主函数中获得被代理类的类加载器 使用 Proxy.newProxyInstance(classLoader, interfaces, args)产生一个代理对象 通过代理对象调用各种方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859=============//实现InvocationHandler====================import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;/** * @author shixuanji * 动态代理: JDK * 1. 实现一个接口 InvocationHandler * 2. 将代理对象作为属性传入 代理所有的类 * 3. 重写 invoke()方法 * */public class ProxyDynamicStudentDao implements InvocationHandler &#123; private Object o; public ProxyDynamicStudentDao(Object o) &#123; this.o = o; &#125; /* * @param proxy: 代理对象, 基本不用 ???应该是代理对象把, 老师写的被代理对象 * @param method: 拦截下来的被代理对象的方法 - 反射中描述方法的类 * @param args: 被代理对象业务方法的参数 */ public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; String methodName = method.getName().toString(); if (methodName.equals("insert")) &#123; // xxx &#125; else &#123; //ssssss &#125; // 做的事情就是对代理方法的方法的增强 // 增强 System.out.println("开始执行"); // 业务方法调用, 一定要调用被代理对象的 // obj: 对象 // args:方法的参数 Object res = method.invoke(o, args); // 增强 System.out.println("执行完了"); return res; &#125;&#125;============== // 使用========================/*** 参数1: 被代理对象的类加载器 * 参数2: 要实现的接口* 参数3: 代理类对象 * * 能.出来什么看 左边接收着* 运行时看 真正创建的对象*/// 目前看来 代理实例只能是接口BaseDAO newProxyInstance = (BaseDAO)Proxy.newProxyInstance(StudentDAO.class.getClassLoader(), StudentDAO.class.getInterfaces(), new ProxyDynamicStudentDao(new StudentDAO()));newProxyInstance.insert(new Teacher()); 第二种：CGLIB 动态代理实现： CGLIB 是针对类来实现代理的，原理是对指定的业务类生成一个子类，并覆盖其中业务方法 实现代理。因为采用的是继承，所以不能对 final 修饰的类进行代理，final 的方法也不能 针对类实现代理，对是否实现接口无要求。原理是对指定的类生成一个子类，覆盖其中的方 法，因为是继承，所以被代理的类或方法最好不要声明为 final 类型。具体实现步骤： 1、定义一个实现了 MethodInterceptor 接口的类 2、实现其 intercept()方法，在其中调用 proxy.invokeSuper() 3. 静态代理和动态代理的区别静态代理：自己编写创建代理类，然后再进行编译，在程序运行前，代理类的.class 文件就 已经存在了。 动态代理：在实现阶段不用关心代理谁，而在运行阶段（通过反射机制）才指定代理哪一个 对象。 3.4 重点掌握3.4.1. 装饰者模式 和 静态代理模式 区别在代码上的区别: 一般情况下, 装饰者模式被装饰的对象一般是从外部传入, 装饰的是一类的事务, 只要是某一类的(Class)都可以 静态代理模式被代理对象的初始化一般是内部创建的, 代理的是一个类的对象. 从功能上: 装饰者模式, 用于对被装饰者业务逻辑实现或增强, 对方法名没有要求 静态代理: 主要用于权限控制, 日志打印, 错误预警等功能 3.4.2. 三种设计模式必须掌握的 单例设计模式 装饰者模式 动态代理模式 3.4.3. 手写代码 冒泡排序 快速排序 设计模式 hadoop 的 wordcount scala 的 wordcount spark 的 wordcount 4. 排序算法 核心概念：算法复杂度、稳定性 算法复杂度：算法复杂度是指算法在编写成可执行程序后，运行时所需要的资源，资源包括 时间资源和内存资源。应用于数学和计算机导论。 稳定性：一个排序算法是稳定的，就是当有两个相等记录的关键字 R 和 S，且在原本的列表 中 R 出现在 S 之前，在排序过的列表中 R 也将会是在 S 之前。 4.1. 排序分类按照排序结果是否稳定性分类： 稳定排序：插入排序，冒泡排序，归并排序，计数排序，基数排序，桶排序（如果桶内 排序采用的是稳定性排序） 非稳定排序：选择排序，快速排序，堆排序。 按照排序过程中是否需要额外空间： 原地排序：插入排序，选择排序，冒泡排序，快速排序，堆排序。 非原地排序：归并排序，计数排序，基数排序，桶排序。 按照排序的主要操作分类： 交换类：冒泡排序、快速排序；此类的特点是通过不断的比较和交换进行排序； 插入类：简单插入排序、希尔排序；此类的特点是通过插入的手段进行排序； 选择类：简单选择排序、堆排序；此类的特点是看准了再移动； 归并类：归并排序；此类的特点是先分割后合并； 按照是否需要比较分类： 比较排序，时间复杂度 O(nlogn) ~ O(n^2)，主要有：冒泡排序，选择排序，插入排序， 归并排序，堆排序，快速排序等。 非比较排序，时间复杂度可以达到 O(n)，主要有：计数排序，基数排序，桶排序等。 4.2 常见排序的时间复杂度 有趣的排序算法视频 4.3 常见排序算法的核心实现4.3.1 冒泡排序 4.3.2 归并排序 4.3.3 快速排序]]></content>
      <categories>
        <category>Java</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习笔记-5]]></title>
    <url>%2F2018%2F05%2F28%2FLinux%2FLinux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-5%2F</url>
    <content type="text"><![CDATA[1.Shell操作日期时间date在类UNIX系统中，日期被存储为一个整数，其大小为自世界标准时间（UTC）1970年1月1日0时0分0秒起流逝的秒数。 语法 1date(选项)(参数) 选项 12345-d&lt;字符串&gt;：显示字符串所指的日期与时间。字符串前后必须加上双引号；-s&lt;字符串&gt;：根据字符串来设置日期与时间。字符串前后必须加上双引号；-u：显示GMT, 即目前的格林威治时间；--help：在线帮助；--version：显示版本信息。 参数 &lt;+时间日期格式&gt;：指定显示时使用的日期时间格式。 日期格式字符串列表 123456789101112131415161718192021222324%r 时间，12小时制%s 从1970年1月1日0点到目前经历的秒数%S 秒（00～59） %T 时间（24小时制）（hh:mm:ss）%X 显示时间的格式（％H时％M分％S秒）%Z 按字母表排序的时区缩写%a 星期名缩写%A 星期名全称%b 月名缩写%B 月名全称%c 日期和时间%d 按月计的日期（01～31）%D 日期（mm/dd/yy） %h 和%b选项相同%j 一年的第几天（001~366）%m 月份（01～12）%w 一个星期的第几天（0代表星期天）%W 一年的第几个星期（00～53，星期一为第一天）%x 显示日期的格式（mm/dd/yy）%y 年份的最后两个数字（1999则是99）%Y 年份（比如1970、1996等）%C 世纪，通常为省略当前年份的后两位数字%U 一年中的第几周，以周日为每星期第一天%e 按月计的日期，添加空格，等于%_d 实例 格式化输出 12date +"%Y-%m-%d"2009-12-07 输出昨天日期： 12date -d "1 day ago" +"%Y-%m-%d"2012-11-19 2秒后输出： 12date -d "2 second" +"%Y-%m-%d %H:%M.%S"2012-11-20 14:21.31 传说中的 1234567890 秒： 12date -d "1970-01-01 1234567890 seconds" +"%Y-%m-%d %H:%m:%S"2009-02-13 23:02:30 普通转格式： 12date -d "2009-12-12" +"%Y/%m/%d %H:%M.%S"2009/12/12 00:00.00 apache格式转换： 12date -d "Dec 5, 2009 12:00:37 AM" +"%Y-%m-%d %H:%M.%S"2009-12-05 00:00.37 格式转换后时间游走： 12date -d "Dec 5, 2009 12:00:37 AM 2 year ago" +"%Y-%m-%d %H:%M.%S"2007-12-05 00:00.37 加减操作： 1234567date +%Y%m%d //显示前天年月日date -d "+1 day" +%Y%m%d //显示前一天的日期date -d "-1 day" +%Y%m%d //显示后一天的日期date -d "-1 month" +%Y%m%d //显示上一月的日期date -d "+1 month" +%Y%m%d //显示下一月的日期date -d "-1 year" +%Y%m%d //显示前一年的日期date -d "+1 year" +%Y%m%d //显示下一年的日期 设定时间： 1234567date -s //设置当前时间，只有root权限才能设置，其他只能查看date -s 20120523 //设置成20120523，这样会把具体时间设置成空00:00:00date -s 01:01:01 //设置具体时间，不会对日期做更改date -s "01:01:01 2012-05-23" //这样可以设置全部时间date -s "01:01:01 20120523" //这样可以设置全部时间date -s "2012-05-23 01:01:01" //这样可以设置全部时间date -s "20120523 01:01:01" //这样可以设置全部时间 有时需要检查一组命令花费的时间，举例： 12345678#!/bin/bashstart=$(date +%s)nmap man.linuxde.net &amp;&gt; /dev/nullend=$(date +%s)difference=$(( end - start ))echo $difference seconds. 计算活了多少年 1echo $[($(date +%s -d $[date])-$(date +%s -d "19900318"))/86400/365] date -d其它的一些用法. 123456789101112131415161718192021222324252627## 获取下一天的时间[root@hadoop ~]# date -d next-day '+%Y-%m-%d %H:%M:%S'[root@hadoop ~]# date -d 'next day' '+%Y-%m-%d %H:%M:%S'另外一种写法：[root@hadoop ~]# date '+%Y-%m-%d %H:%M:%S' -d tomorrow## 获取上一天的时间 [root@hadoop ~]# date -d last-day '+%Y-%m-%d %H:%M:%S'另外一种写法：[root@hadoop ~]# date '+%Y-%m-%d %H:%M:%S' -d yesterday## 获取下一月的时间[root@hadoop ~]# date -d next-month '+%Y-%m-%d %H:%M:%S'## 获取上一月的时间 [root@hadoop ~]# date -d last-month '+%Y-%m-%d %H:%M:%S'## 获取下一年的时间[root@hadoop ~]# date -d next-year '+%Y-%m-%d %H:%M:%S'## 获取上一年的时间 [root@hadoop ~]# date -d last-year '+%Y-%m-%d %H:%M:%S'## 获取上一周的日期时间：[root@hadoop ~]# date -d next-week '+%Y-%m-%d %H:%M:%S'[root@hadoop ~]# date -d next-monday '+%Y-%m-%d %H:%M:%S'[root@hadoop ~]# date -d next-thursday '+%Y-%m-%d %H:%M:%S' 那么类似的，其实，last-year，last-month，last-day，last-week，last-hour，last-minute，last-second都有对应的实现。相反的，last对应next，自己可以根据实际情况灵活组织 接下来，我们来看‘–date’，它帮我实现任意时间前后的计算，来看具体的例子： 1234567## 获取一天以后的日期时间[root@hadoop ~]# date '+%Y-%m-%d %H:%M:%S' --date='1 day'[root@hadoop ~]# date '+%Y-%m-%d %H:%M:%S' --date='-1 day ago'## 获取一天以前的日期时间[root@hadoop ~]# date '+%Y-%m-%d %H:%M:%S' --date='-1 day'[root@hadoop ~]# date '+%Y-%m-%d %H:%M:%S' --date='1 day ago' 上面的例子显示出来了使用的格式，使用精髓在于改变前面的字符串显示格式，改变数据，改变要操作的日期对应字段，除了天也有对应的其他实现：year，month，week，day，hour，minute，second，monday（星期，七天都可） date 能用来显示或设定系统的日期和时间，在显示方面，使用者能设定欲显示的格式，格式设定为一个加号后接数个标记，其中可用的标记列表如下： 使用范例： 1[root@hadoop ~]# date '+%Y-%m-%d %H:%M:%S' 日期方面 12345678910111213141516%a : 星期几 (Sun..Sat) %A : 星期几 (Sunday..Saturday) %b : 月份 (Jan..Dec) %B : 月份 (January..December) %c : 直接显示日期和时间 %d : 日 (01..31) %D : 直接显示日期 (mm/dd/yy) %h : 同 %b %j : 一年中的第几天 (001..366) %m : 月份 (01..12) %U : 一年中的第几周 (00..53) (以 Sunday 为一周的第一天的情形) %w : 一周中的第几天 (0..6) %W : 一年中的第几周 (00..53) (以 Monday 为一周的第一天的情形) %x : 直接显示日期 (mm/dd/yyyy) %y : 年份的最后两位数字 (00.99) %Y : 完整年份 (0000..9999) 时间方面 123456789101112131415%%: 打印出%%n : 下一行%t : 跳格%H : 小时(00..23)%k : 小时(0..23)%l : 小时(1..12)%M : 分钟(00..59)%p : 显示本地AM或PM%P : 显示本地am或pm%r : 直接显示时间(12 小时制，格式为 hh:mm:ss [AP]M)%s : 从 1970 年 1 月 1 日 00:00:00 UTC 到目前为止的秒数%S : 秒(00..61)%T : 直接显示时间(24小时制)%X : 相当于%H:%M:%S %p%Z : 显示时区 若是不以加号作为开头，则表示要设定时间，而时间格式为 MMDDhhmm[[CC]YY][.ss] 1234567MM 为月份， DD 为日，hh 为小时，mm 为分钟，CC 为年份前两位数字，YY 为年份后两位数字，ss 为秒数 有用的小技巧12345678910111213141516171819202122## 获取相对某个日期前后的日期：cts1 ~ # date -d 'may 14 -2 weeks'2018年 04月 30日 星期一 00:00:00 CST## 把时间当中无用的0去掉，比如：01:02:25会变成1:2:25cts1 ~ # date '+%-H:%-M:%-S'19:18:22## 显示文件最后被更改的时间cts1 ~ # date "+%Y-%m-%d %H:%M:%S" -r install.log2018-05-23 10:11:14## 求两个字符串日期之间相隔的天数[root@hadoop ~]# expr '(' $(date +%s -d "2016-08-08") - $(date +%s -d "2016-09-09") ')' / 86400expr `expr $(date +%s -d "2016-08-08") - $(date +%s -d "2016-09-09")` / 86400## shell中加减指定间隔单位cts1 ~ # A=`date +%Y-%m-%d`cts1 ~ # B=`date +%Y-%m-%d -d "$A +48 hours"`cts1 ~ # echo $B2018-05-30 2. 文本处理wc功能： 统计文件行数、字节、字符数 选项1234567-c # 统计字节数，或--bytes或——chars：只显示Bytes数；。-l # 统计行数，或——lines：只显示列数；。-m # 统计字符数。这个标志不能与 -c 标志一起使用。-w # 统计字数，或——words：只显示字数。一个字被定义为由空白、跳格或换行字符分隔的字符串。-L # 打印最长行的长度。-help # 显示帮助信息--version # 显示版本信息 例子123456789101112131415161718192021222324wc -l * # 统计当前目录下的所有文件行数wc -l *.js # 统计当前目录下的所有 .js 后缀的文件行数find . * | xargs wc -l # 当前目录以及子目录的所有文件行数 wc test.txt # 查看文件的字节数、字数、行数# 查看文件的字节数、字数、行数wc test.txt# 输出结果7 8 70 test.txt行数 单词数 字节数 文件名# 用wc命令只打印统计文件行数不打印文件名wc -l test.txt # 输出结果7 test.txt# 用来统计当前目录下的文件数ls -l | wc -l# 输出结果8# 统计文件字数：cts1 ~ # wc -w date.txt30 date.txt sortsort命令 是在Linux里非常有用，它将文件进行排序，并将排序结果标准输出。sort命令既可以从特定的文件，也可以从stdin中获取输入。 选项123456789101112131415-b：忽略每行前面开始出的空格字符；-c：检查文件是否已经按照顺序排序；-d：排序时，处理英文字母、数字及空格字符外，忽略其他的字符；-f：排序时，将小写字母视为大写字母；-i：排序时，除了040至176之间的ASCII字符外，忽略其他的字符；-m：将几个排序号的文件进行合并；-M：将前面3个字母依照月份的缩写进行排序；-n：依照数值的大小排序；-o&lt;输出文件&gt;：将排序后的结果存入制定的文件；-r：以相反的顺序来排序；-t&lt;分隔字符&gt;：指定排序时所用的栏位分隔字符；-g：按照常规数值排序-k：位置1,位置2根据关键字排序，在从第位置1开始，位置2结束+&lt;起始栏位&gt;-&lt;结束栏位&gt;：以指定的栏位来排序，范围由起始栏位到结束栏位的前一栏位。 实例 sort将文件/文本的每一行作为一个单位，相互比较，比较原则是从首字符向后，依次按ASCII码值进行比较，最后将他们按升序输出。 123456789101112131415root@[mail text]# cat sort.txtaaa:10:1.1ccc:30:3.3ddd:40:4.4bbb:20:2.2eee:50:5.5eee:50:5.5[root@mail text]# sort sort.txtaaa:10:1.1bbb:20:2.2ccc:30:3.3ddd:40:4.4eee:50:5.5eee:50:5.5 忽略相同行使用-u选项或者uniq： 1234567891011121314151617181920212223[root@mail text]# cat sort.txtaaa:10:1.1ccc:30:3.3ddd:40:4.4bbb:20:2.2eee:50:5.5eee:50:5.5[root@mail text]# sort -u sort.txtaaa:10:1.1bbb:20:2.2ccc:30:3.3ddd:40:4.4eee:50:5.5或者[root@mail text]# uniq sort.txtaaa:10:1.1ccc:30:3.3ddd:40:4.4bbb:20:2.2eee:50:5.5 sort的-n、-r、-k、-t选项的使用： 123456789101112131415161718192021222324252627282930313233[root@mail text]# cat sort.txtAAA:BB:CCaaa:30:1.6ccc:50:3.3ddd:20:4.2bbb:10:2.5eee:40:5.4eee:60:5.1#将BB列按照数字从小到大顺序排列：[root@mail text]# sort -nk 2 -t: sort.txtAAA:BB:CCbbb:10:2.5ddd:20:4.2aaa:30:1.6eee:40:5.4ccc:50:3.3eee:60:5.1#将CC列数字从大到小顺序排列：[root@mail text]# sort -nrk 3 -t: sort.txteee:40:5.4eee:60:5.1ddd:20:4.2ccc:50:3.3bbb:10:2.5aaa:30:1.6AAA:BB:CC# -n是按照数字大小排序，-r是以相反顺序，-k是指定需要排序的栏位，-t指定栏位分隔符为冒号# 多列排序：以:分隔，按第二列数值排倒序，第三列正序[linux@linux ~]$ sort -n -t: -k2,2r -k3 sort.txt -k选项的具体语法格式： x,x 表示一个范围 123FStart.CStart Modifie,FEnd.CEnd Modifier-------Start--------,-------End-------- FStart.CStart 选项 , FEnd.CEnd 选项 这个语法格式可以被其中的逗号,分为两大部分， Start 部分和 End 部分。Start部分也由三部分组成，其中的Modifier部分就是我们之前说过的类似n和r的选项部分。我们重点说说Start部分的FStart和C.Start。C.Start也是可以省略的，省略的话就表示从本域的开头部分开始。FStart.CStart，其中FStart就是表示使用的域，而CStart则表示在FStart域中从第几个字符开始算“排序首字符”。同理，在End部分中，你可以设定FEnd.CEnd，如果你省略.CEnd，则表示结尾到“域尾”，即本域的最后一个字符。或者，如果你将CEnd设定为0(零)，也是表示结尾到“域尾”。 从公司英文名称的第二个字母开始进行排序： 1234567# 先创建此txt文件# 再排序$ sort -t ' ' -k 1.2 facebook.txtbaidu 100 5000sohu 100 4500google 110 5000guge 50 3000 -t &#39; &#39;, 首先用&#39; &#39;空格, 把字段分割成了3个域. 使用了-k 1.2，表示对第一个域的第二个字符开始到本域的最后一个字符为止的字符串进行排序。你会发现baidu因为第二个字母是a而名列榜首。sohu和 google第二个字符都是o，但sohu的h在google的o前面，所以两者分别排在第二和第三。guge只能屈居第四了。 只针对公司英文名称的第二个字母进行排序，如果相同的按照员工工资进行降序排序： 12345$ sort -t ' ' -k 1.2,1.2 -nrk 3,3 facebook.txtbaidu 100 5000google 110 5000sohu 100 4500guge 50 3000 由于只对第二个字母进行排序，所以我们使用了-k 1.2,1.2的表示方式，表示我们“只”对第二个字母进行排序。（如果你问“我使用-k 1.2怎么不行？”，当然不行，因为你省略了End部分，这就意味着你将对从第二个字母起到本域最后一个字符为止的字符串进行排序）。对于员工工资进行排 序，我们也使用了-k 3,3，这是最准确的表述，表示我们“只”对本域进行排序，因为如果你省略了后面的3，就变成了我们“对第3个域开始到最后一个域位置的内容进行排序” 了。 uniquniq命令 用于报告或忽略文件中的重复行，一般与sort命令结合使用。 选项123456-c或——count：在每列旁边显示该行重复出现的次数；-d或--repeated：仅显示重复出现的行列；-f&lt;栏位&gt;或--skip-fields=&lt;栏位&gt;：忽略比较指定的栏位；-s&lt;字符位置&gt;或--skip-chars=&lt;字符位置&gt;：忽略比较指定的字符；-u或——unique：仅显示出一次的行列；-w&lt;字符位置&gt;或--check-chars=&lt;字符位置&gt;：指定要比较的字符。 参数 输入文件：指定要去除的重复行文件。如果不指定此项，则从标准读取数据； 输出文件：指定要去除重复行后的内容要写入的输出文件。如果不指定此选项，则将内容显示到标准输出设备（显示终端）。 实例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758[root]# cat sort.txt : 原本行aaa:10:1.1ccc:30:3.3ddd:40:4.4bbb:20:2.2eee:50:5.5eee:50:5.5[root]# uniq sort.txt : 不展示重复行aaa:10:1.1ccc:30:3.3ddd:40:4.4bbb:20:2.2eee:50:5.5[cts1:Desktop][cts1:Desktop][root]# sort sort.txt | uniq : 不展示重复行aaa:10:1.1bbb:20:2.2ccc:30:3.3ddd:40:4.4eee:50:5.5[cts1:Desktop][root]# uniq -u sort.txt : 删除重复行aaa:10:1.1ccc:30:3.3ddd:40:4.4bbb:20:2.2[cts1:Desktop][root]# sort sort.txt | uniq -u : 排序并删除重复行aaa:10:1.1bbb:20:2.2ccc:30:3.3ddd:40:4.4[cts1:Desktop][root]# sort sort.txt | uniq -c : 展示每行出现的次数 1 aaa:10:1.1 1 bbb:20:2.2 1 ccc:30:3.3 1 ddd:40:4.4 2 eee:50:5.5[cts1:Desktop][root]# sort sort.txt | uniq -d : 只展示重复行eee:50:5.5# 求a.txt和b.txt的差集 ## 首先a b 去掉重复的, 再跟a b去重, 把b其它的部分也去掉cts1 Desktop # cat a b b | sort | uniq -uab# 求b.txt和a.txt的差集 ## 同上cts1 Desktop # cat b a a | sort | uniq -uef cutcut命令 用来显示行中的指定部分，删除文件中指定字段。cut经常用来显示文件的内容，类似于下的type命令。 说明：该命令有两项功能，其一是用来显示文件的内容，它依次读取由参数file所指 明的文件，将它们的内容输出到标准输出上；其二是连接两个或多个文件，如cut fl f2 &gt; f3将把文件fl和几的内容合并起来，然后通过输出重定向符“&gt;”的作用，将它们放入文件f3中。 当文件较大时，文本在屏幕上迅速闪过（滚屏），用户往往看不清所显示的内容。因此，一般用more等命令分屏显示。为了控制滚屏，可以按Ctrl+S键，停止滚屏；按Ctrl+Q键可以恢复滚屏。按Ctrl+C（中断）键可以终止该命令的执行，并且返回Shell提示符状态。 选项123456789-b：仅显示行中指定直接范围的内容；-c：仅显示行中指定范围的字符； -d：指定字段的分隔符，默认的字段分隔符为“TAB”； -f：显示指定字段的内容；-n：与“-b”选项连用，不分割多字节字符；--complement：补足被选择的字节、字符或字段；--out-delimiter=&lt;字段分隔符&gt;：指定输出内容是的字段分割符；--help：显示指令的帮助信息；--version：显示指令的版本信息。 参数文件：指定要进行内容过滤的文件。 实例例如有一个学生报表信息，包含No、Name、Mark、Percent： 12345[root@localhost text]# cat cut.txt No Name Mark Percent01 tom 69 9102 jack 71 8703 alex 68 98 -f使用-f 选项提取指定字段 12345[root@localhost text]# cut -d ' ' -f1 cut.txt No010203 12345[root@localhost text]# cut -d ' ' -f2,3 test.txt Name Marktom 69jack 71alex 68 –complement选项提取指定字段之外的列（打印除了第二列之外的列）： 12345cut -d ' ' -f2 --complement cut.txtNo Mark Percent01 69 9102 71 8703 68 98 指定字段的字符或者字节范围 &amp; 例子cut命令可以将一串字符作为列来显示，字符字段的记法： N- ：从第N个字节、字符、字段到结尾； N-M ：从第N个字节、字符、字段到第M个（包括M在内）字节、字符、字段； -M ：从第1个字节、字符、字段到第M个（包括M在内）字节、字符、字段。 上面是记法，结合下面选项将摸个范围的字节、字符指定为字段： -b 表示字节； -c 表示字符； -f 表示定义字段。 实例123456789101112131415161718192021222324252627282930[root@localhost text]# cat test.txt abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz# 打印第1个到第3个字符：cut -c1-3 test.txt abcabcabcabcabc# 打印前2个字符：cut -c-2 test.txt ababababab# 打印从第5个字符开始到结尾：cut -c5- test.txt efghijklmnopqrstuvwxyzefghijklmnopqrstuvwxyzefghijklmnopqrstuvwxyzefghijklmnopqrstuvwxyzefghijklmnopqrstuvwxyz grep(文本生成器)grep （global search regular expression(RE) and print out the line，全面搜索正则表达式并把行打印出来）是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。用于过滤/搜索的特定字符。可使用正则表达式能多种命令配合使用，使用上十分灵活。 选项123456789101112131415161718192021222324252627-a --text # 不要忽略二进制数据。-A &lt;显示行数&gt; --after-context=&lt;显示行数&gt; # 除了显示符合范本样式的那一行之外，并显示该行之后的内容。-b --byte-offset # 在显示符合范本样式的那一行之外，并显示该行之前的内容。-B&lt;显示行数&gt; --before-context=&lt;显示行数&gt; # 除了显示符合样式的那一行之外，并显示该行之前的内容。-c --count # 计算符合范本样式的列数。-C&lt;显示行数&gt; --context=&lt;显示行数&gt;或-&lt;显示行数&gt; # 除了显示符合范本样式的那一列之外，并显示该列之前后的内容。-d&lt;进行动作&gt; --directories=&lt;动作&gt; # 当指定要查找的是目录而非文件时，必须使用这项参数，否则grep命令将回报信息并停止动作。-e&lt;范本样式&gt; --regexp=&lt;范本样式&gt; # 指定字符串作为查找文件内容的范本样式。-E --extended-regexp # 将范本样式为延伸的普通表示法来使用，意味着使用能使用扩展正则表达式。-f&lt;范本文件&gt; --file=&lt;规则文件&gt; # 指定范本文件，其内容有一个或多个范本样式，让grep查找符合范本条件的文件内容，格式为每一列的范本样式。-F --fixed-regexp # 将范本样式视为固定字符串的列表。-G --basic-regexp # 将范本样式视为普通的表示法来使用。-h --no-filename # 在显示符合范本样式的那一列之前，不标示该列所属的文件名称。-H --with-filename # 在显示符合范本样式的那一列之前，标示该列的文件名称。-i --ignore-case # 忽略字符大小写的差别。-l --file-with-matches # 列出文件内容符合指定的范本样式的文件名称。-L --files-without-match # 列出文件内容不符合指定的范本样式的文件名称。-n --line-number # 在显示符合范本样式的那一列之前，标示出该列的编号。-q --quiet或--silent # 不显示任何信息。-R/-r --recursive # 此参数的效果和指定“-d recurse”参数相同。-s --no-messages # 不显示错误信息。-v --revert-match # 反转查找。-V --version # 显示版本信息。 -w --word-regexp # 只显示全字符合的列。-x --line-regexp # 只显示全列符合的列。-y # 此参数效果跟“-i”相同。-o # 只输出文件中匹配到的部分。 grep中的正则表达式12345678910111213141516171819202122232425&lt;sup&gt; # 锚定行的开始 如：'&lt;/sup&gt;grep'匹配所有以grep开头的行。 $ # 锚定行的结束 如：'grep$'匹配所有以grep结尾的行。 . # 匹配一个非换行符的字符 如：'gr.p'匹配gr后接一个任意字符，然后是p。 * # 匹配零个或多个先前字符 如：'*grep'匹配所有一个或多个空格后紧跟grep的行。 .* # 一起用代表任意字符。 [] # 匹配一个指定范围内的字符，如'[Gg]rep'匹配Grep和grep。 [&lt;sup&gt;] # 匹配一个不在指定范围内的字符，如：'[&lt;/sup&gt;A-FH-Z]rep'匹配不包含A-R和T-Z的一个字母开头，紧跟rep的行。 \ # 转义\(..\) # 标记匹配字符，如'\(love\)'，love被标记为1。 \&lt; # 锚定单词的开始，如:'\&lt;grep'匹配包含以grep开头的单词的行。 \&gt; # 锚定单词的结束，如'grep\&gt;'匹配包含以grep结尾的单词的行。 x\&#123;m\&#125; # 重复字符x，m次，如：'o\&#123;5\&#125;'匹配包含5个o的行。 x\&#123;m,\&#125; # 重复字符x,至少m次，如：'o\&#123;5,\&#125;'匹配至少有5个o的行。 x\&#123;m,n\&#125; # 重复字符x，至少m次，不多于n次，如：'o\&#123;5,10\&#125;'匹配5--10个o的行。 \w # 匹配文字和数字字符，也就是[A-Za-z0-9]，如：'G\w*p'匹配以G后跟零个或多个文字或数字字符，然后是p。 \W # \w的反置形式，匹配一个或多个非单词字符，如点号句号等。 \b # 单词锁定符，如: '\bgrep\b'只匹配grep。增录. # 任意一个字符a* # 任意多个a(0个或多个)a? # 0个或1个aa+ # 一个或多个a[A-Z][ABC] grep命令常见用法 在文件中搜索一个单词，命令会返回一个包含 “match_pattern” 的文本行： 12grep match_pattern file_namegrep "match_pattern" file_name 在多个文件中查找 1grep "match_pattern" file_1 file_2 file_3 ... 输出除 “match_pattern” 之外的所有行 &gt;&gt; -v 选项： 1grep -v "match_pattern" file_name 标记匹配颜色 –color=auto 选项： 12345678910grep "match_pattern" file_name --color=autoeg: 查找当前目录下所有文件中的 `a`cts1 Desktop # grep "\(a\)" ./* --color=auto---./a:a./abc.txt:abcdefghijklmnopqrstuvwxyz./abc.txt:abcdefghijklmnopqrstuvwxyz./abc.txt:abcdefghijklmnopqrstuvwxyz... 使用正则表达式 -E 选项：123grep -E "[1-9]+"或egrep "[1-9]+" 只输出文件中匹配到的部分 -o 选项： 12345echo this is a test line. | grep -o -E "[a-z]+\."line.echo this is a test line. | egrep -o "[a-z]+\."line. 统计文件或者文本中包含匹配字符串的行数 -c 选项： 1grep -c "text" file_name 输出包含匹配字符串的行数 -n 选项： 123456grep "text" -n file_name或cat file_name | grep "text" -n#多个文件grep "text" -n file_1 file_2 打印样式匹配所位于的字符或字节偏移 1234echo gun is not unix | grep -b -o "not"7:not#一行中字符串的字符便宜是从该行的第一个字符开始计算，起始值为0。选项 **-b -o** 一般总是配合使用。 搜索多个文件并查找匹配文本在哪些文件中 1grep -l "text" file1 file2 file3... grep递归搜索文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# 在多级目录中对文本进行递归搜索grep "text" . -r -n# .表示当前目录。# 忽略匹配样式中的字符大小写echo "hello world" | grep -i "HELLO"hello# 选项 -e 制动多个匹配样式：echo this is a text line | grep -e "is" -e "line" -oisline#也可以使用 **-f** 选项来匹配多个样式，在样式文件中逐行写出需要匹配的字符。cat patfileaaabbb# 用文件名称匹配, 实际上就是匹配文件中的内容echo aaa bbb ccc ddd eee | grep -f patfile -oaaabbb# 在grep搜索结果中包括或者排除指定文件：---#只在目录中所有的.php和.html文件中递归搜索字符"main()"grep "main()" . -r --include *.&#123;php,html&#125;#在搜索结果中排除所有README文件grep "main()" . -r --exclude "README"#在搜索结果中排除filelist文件列表里的文件grep "main()" . -r --exclude-from filelist---# 使用0值字节后缀的grep与xargs：---# 测试文件：echo "aaa" &gt; file1echo "bbb" &gt; file2echo "aaa" &gt; file3grep "aaa" file* -lZ | xargs -0 rm#执行后会删除file1和file3，grep输出用-Z选项来指定以0值字节作为终结符文件名（\0），xargs -0 读取输入并用0值字节终结符分隔文件名，然后删除匹配文件，-Z通常和-l结合使用。# 好吧, 暂时我也没懂是啥意思...---# grep静默输出：grep -q "test" filename# 不会输出任何信息，如果命令运行成功返回0，失败则返回非0值。一般用于条件测试。# 打印出匹配文本之前或者之后的行：# 显示匹配某个结果之后的3行，使用 -A 选项：seq 10 | grep "5" -A 35678# 显示匹配某个结果之前的3行，使用 -B 选项：seq 10 | grep "5" -B 32345# 显示匹配某个结果的前三行和后三行，使用 -C 选项：seq 10 | grep "5" -C 32345678# 如果匹配结果有多个，会用“--”作为各匹配结果之间的分隔符：echo -e "a\nb\nc\na\nb\nc" | grep a -A 1ab--ab grep其它常用用法12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 统计出现某个字符串的行的总行数 '-c'cts1 Desktop # echo "hello" &gt;&gt; hello.shcts1 Desktop # grep -c 'hello' hello.sh1# 查询不包含hello的行, 带有hello的整行都不会被查询到 '-v'cts1 Desktop # grep -v 'hello' hello.sh....# `.*`的用法, 前后都是 `.*` , 只要包含 parrten 的 整行 都会被查出来grep '.*hello.*' hello.sh# 任何由 'h..p'包含的都会被查出来. 查的当前行, .* 代表任意字符cts1 Desktop # grep 'h.*p' /etc/passwdgopher:x:13:30:gopher:/var/gopher:/sbin/nologinrpc:x:32:32:Rpcbind Daemon:/var/cache/rpcbind:/sbin/nolo# 正则表达以 ro 开头cts1 Desktop # grep '^ro' /etc/passwdroot:x:0:0:root:/root:/bin/zsh# 正则表达以 t 结尾cts1 Desktop # grep 't$' /etc/passwdhalt:x:7:0:halt:/sbin:/sbin/halt# '[Gg]rep'匹配Grep和grep## 此处是匹配 以 h || r 开头的cts1 Desktop # grep '^[hr]' /etc/passwdroot:x:0:0:root:/root:/bin/zshhalt:x:7:0:halt:/sbin:/sbin/haltrpc:x:32:32:Rpcbind Daemon:/var/cache/rpcbind:/sbin/nologin# 匹配非h之外其他的, 和 r 开头的, 也就是匹配 h以外的cts1 Desktop # grep '^[hr]' /etc/passwdroot:x:0:0:root:/root:/bin/zshhalt:x:7:0:halt:/sbin:/sbin/haltrpc:x:32:32:Rpcbind Daemon:/var/cache/rpcbind:/sbin/nologin# 匹配 h-r 以外的cts1 Desktop # grep '&lt;sup&gt;[&lt;/sup&gt;h-r]' /etc/passwdbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologin... sed(流编辑器)sed叫做流编辑器，在shell脚本和Makefile中作为过滤一使用非常普遍，也就是把前一个程序的输出引入sed的输入，经过一系列编辑命令转换成为另一种格式输出。sed是一种在线编辑器，它一次处理一行内容，处理时，把当前处理的行存储在临时缓冲区中，称为”模式空间”,接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。文件内容并没有改变，除非你使用重定向存储输出。 选项12345678910111213-n：一般sed命令会把所有数据都输出到屏幕，如果加入-n选项的话，则只会把经过sed命令处理的行输出到屏幕。-e&lt;script&gt;或--expression=&lt;script&gt;：以选项中的指定的script来处理输入的文本文件； 允许对输入数据应用多条sed命令编辑。-i：用sed的修改结果直接修改读取数据的文件，而不是由屏幕输出。-f&lt;script文件&gt;或--file=&lt;script文件&gt;：以选项中指定的script文件来处理输入的文本文件；-n或--quiet或——silent：仅显示script处理后的结果；--version：显示版本信息。# 动作:a：追加，在当前行后添加一行或多行。c：行替换，用c后面的字符串替换原数据行。i：插入，在当前行前插入一行或多行。p：打印，输出指定的行。s：字符串替换，用一个字符串替换另外一个字符串。格式为**\'**行范围s/旧字符串/新字符串/g**\'** (如果不加g的话，则表示只替换每行第一个匹配的串) sed元字符集(正则)12345678910111213**&lt;sup&gt;** 匹配行开始，如：/&lt;/sup&gt;sed/匹配所有以sed开头的行。**$** 匹配行结束，如：/sed$/匹配所有以sed结尾的行。**.** 匹配一个非换行符的任意字符，如：/s.d/匹配s后接一个任意字符，最后是d。**** * 匹配0个或多个字符，如：/*sed/匹配所有模板是一个或多个空格后紧跟sed的行。**[]** 匹配一个指定范围内的字符，如/[ss]ed/匹配sed和Sed。 **[&lt;sup&gt;]** 匹配一个不在指定范围内的字符，如：/[&lt;/sup&gt;A-RT-Z]ed/匹配不包含A-R和T-Z的一个字母开头，紧跟ed的行。**\(..\)** 匹配子串，保存匹配的字符，如s/\(love\)able/\1rs，loveable被替换成lovers。**&amp;** 保存搜索字符用来替换其他字符，如s/love/ **&amp;** /，love这成 **love** 。**\&lt;** 匹配单词的开始，如:/\&lt;love/匹配包含以love开头的单词的行。**\&gt;** 匹配单词的结束，如/love\&gt;/匹配包含以love结尾的单词的行。**x\&#123;m\&#125;** 重复字符x，m次，如：/0\&#123;5\&#125;/匹配包含5个0的行。**x\&#123;m,\&#125;** 重复字符x，至少m次，如：/0\&#123;5,\&#125;/匹配至少有5个0的行。**x\&#123;m,n\&#125;** 重复字符x，至少m次，不多于n次，如：/0\&#123;5,10\&#125;/匹配5~10个0的行。 实例 删除: d命令 sed ‘2d’ sed.txt —–删除sed.txt文件的第二行。 sed ‘2,$d’ sed.txt —–删除sed.txt文件的第二行到末尾所有行。 sed ‘$d’ sed.txt —–删除sed.txt文件的最后一行。 sed ‘/test/d ‘ sed.txt —–删除sed.txt文件所有包含test的行。 sed ‘/[A-Za-z]/d ‘ sed.txt —–删除sed.txt文件所有包含字母的行。 整行替换: c命令 将第二行替换成hello world sed \’2c hello world\’ sed.txt 字符串替换：s命令 sed ‘s/hello/hi/g’ sed.txt ## 在整行范围内把hello替换为hi。如果没有g标记，则只有每行第一个匹配的hello被替换成hi。 sed ‘s/hello/hi/2’ sed.txt ## 此种写法表示只替换每行的第2个hello为hi sed ‘s/hello/hi/2g’ sed.txt ## 此种写法表示只替换每行的第2个以后的hello为hi（包括第2个） sed -n ‘s/^hello/hi/p’ sed.txt ## (-n)选项和p标志一起使用表示只打印那些发生替换的行。也就是说，如果某一行开头的hello被替换成hi，就打印它。 sed -n ‘2,4p’ sed.txt ## 打印输出sed.txt中的第2行和第4行 sed -n ‘s/hello/&amp;-hi/gp’ sed.txt sed ‘s/^192.168.0.1/&amp;-localhost/‘ sed.txt sed ‘s/^192.168.0.1/[&amp;]/‘ sed.txt ## &amp;符号表示追加一个串到找到的串后, &amp;代表前一个串。 所有以192.168.0.1开头的行都会被替换成它自已加 -localhost，变成192.168.0.1-localhost。 第三句表示给IP地址添加中括号 sed -n ‘s/(liu)jialing/\1tao/p’ sed.txt sed -n ‘s/(liu)jia(ling)/\1tao\2ss/p’ sed.txt ## liu被标记为\1，所以liu会被保留下来（\1 == liu） ## ling被标记为\2，所以ling也会被保留下来（\2 == ling） ## 所以最后的结果就是\1tao\2ss == “liu” + “tao” + “ling” + “ss” 此处切记：\1代表的是被第一个()包含的内容，\1代表的是被第一个()包含的内容，…… 上面命令的意思就是：被括号包含的字符串会保留下来，然后跟其他的字符串比如tao和ss组成新的字符串liutaolingss sed ‘s#hello#hi#g’ sed.txt ## 不论什么字符，紧跟着s命令的都被认为是新的分隔符，所以，”#”在这里是分隔符，代替了默认的”/“分隔符。表示把所有hello替换成hi。 选定行的范围：逗号 sed -n ‘/today/,/hello/p’ sed.txt ## 所有在模板today和hello所确定的范围内的行都被打印。都找第一个，也就是说，从第一个today到第一个hello sed -n ‘5,/^hello/p’ sed.txt sed -n ‘/^hello/,8p’ sed.txt ## 打印从第五行开始到第一个包含以hello开始的行之间的所有行。 sed ‘/today/,/hello/s/$/www/‘ sed.txt ## 对于模板today和hello之间的行，每行的末尾用字符串www替换。 sed ‘/today/,/hello/s/^/www/‘ sed.txt ## 对于模板today和hello之间的行，每行的开头用字符串www替换。 sed ‘/^[A-Za-z]/s/5/five/g’ sed.txt ## 将以字母开头的行中的数字5替换成five 多点编辑：e命令 sed -e ‘1,5d’ -e ‘s/hello/hi/‘ sed.txt ## (-e)选项允许在同一行里执行多条命令。如例子所示，第一条命令删除1至5行，第二条命令用hello替换hi。命令的执行顺序对结果有影响。如果两个命令都是替换命令，那么第一个替换命令将影响第二个替换命令的结果。 sed –expression=’s/hello/hi/‘ –expression=’/today/d’ sed.txt ## 一个比-e更好的命令是–expression。它能给sed表达式赋值。 从文件读入：r命令 sed ‘/hello/r file’ sed.txt ## file里的内容被读进来，显示在与hello匹配的行下面，如果匹配多行，则file的内容将显示在所有匹配行的下面。 写入文件：w命令 sed -n ‘/hello/w file’ sed.txt ## 在huangbo.txt中所有包含hello的行都被写入file里。 追加命令：a命令 sed ‘/^hello/a\—&gt;this is a example’ sed.txt ## ‘—&gt;this is a example’被追加到以hello开头的行(另起一行)后面，sed要求命令a后面有一个反斜杠。 插入：i命令 sed ‘/will/i\some thing new ————————-‘ sed.txt ## 如果test被匹配，则把反斜杠后面的文本插入到匹配行的前面。 下一个：n命令 sed ‘/hello/{n; s/aa/bb/;}’ sed.txt 替换下一行的第一个aa sed ‘/hello/{n; s/aa/bb/g;}’ sed.txt 替换下一行的全部aa ## 如果hello被匹配，则移动到匹配行的下一行，替换这一行的aa，变为bb，并打印该行，然后继续。 退出：q命令 sed ‘10q’ sed.txt ## 打印完第10行后，退出sed。 同样的写法： sed -n ‘1,10p ‘ sed.txt awk(报表生成器)Awk是一个强大的处理文本的编程语言工具，其名称得自于它的创始人Alfred Aho、Peter Weinberger和Brian Kernighan 姓氏的首个字母，相对于grep的查找，sed的编辑，awk在其对数据分析并生成报告时，显得尤为强大。AWK 提供了极其强大的功能：可以进行样式装入、流控制、数学运算符、进程控制语句甚至于内置的变量和函数。简单来说awk就是扫描文件中的每一行，查找与命令行中所给定内容相匹配的模式。如果发现匹配内容，则进行下一个编程步骤。如果找不到匹配内容，则继续处理下一行。 语法12awk [options] 'script' var=value file(s)awk [options] -f scriptfile var=value file(s) … TODO 待补充 实例 假设last -n 5的输出如下: 123456root@cts1:~ # last -n 5root pts/1 192.168.170.1 Mon May 28 09:28 still logged inroot pts/1 192.168.170.1 Sun May 27 20:53 - 07:21 (10:28)root pts/0 192.168.170.1 Sun May 27 19:39 still logged inreboot system boot 2.6.32-573.el6.x Sun May 27 19:34 - 14:48 (19:13)root pts/1 192.168.170.1 Sat May 26 20:43 - 22:36 (01:53) 只显示五个最近登录的账号： 12345678root@cts1:~ # last -n 5 | awk '&#123;print $1&#125;'rootrootrootrebootroot# awk工作流程是这样的：读入有'\n'换行符分割的一条记录，然后将记录按指定的域分隔符划分域，填充域，$0则表示所有域,$1表示第一个域,$n表示第n个域。默认域分隔符是"空白键" 或 "[tab]键",所以$1表示登录用户，$3表示登录用户ip,以此类推 显示/etc/passwd的账户： 12345678root@cts1:~ # cat /etc/passwd | awk -F':' '&#123;print $1&#125;'rootbindaemon...# 这种是awk+action的示例，每行都会执行action&#123;print $1&#125;。# -F指定域分隔符为':' 显示/etc/passwd的账户和账户对应的shell,而账户与shell之间以tab键分割 12345root@cts1:~ # cat /etc/passwd | awk -F':' '&#123;print $1"\t"$7&#125;'root /bin/zshbin /sbin/nologindaemon /sbin/nologin... BEGIN and END 如果只是显示/etc/passwd的账户和账户对应的shell,而账户与shell之间以逗号分割,而且在所有行添加列名name,shell,在最后一行添加”blue,/bin/nosh”。 123456789101112131415161718root@cts1:~ # cat /etc/passwd |awk -F ':' 'BEGIN &#123;print "name,shell"&#125; &#123;print $1","$7&#125; END &#123;print "blue,/bin/nosh"&#125;'---name,shellroot,/bin/zshbin,/sbin/nologin...mysql,/bin/bashblue,/bin/nosh-------------------------root@cts1:~ # cat /etc/passwd | awk -F ':' 'BEGIN &#123;print "name \t shell"&#125; &#123;print$1"\t"$7&#125; END &#123;print "blue,/bin/bash"&#125;'---name shellroot /bin/zshbin /sbin/nologin...mysql /bin/bashblue,/bin/bash awk工作流程是这样的：先执行BEGIN，然后读取文件，读入有/n换行符分割的一条记录，然后将记录按指定的域分隔符划分域，填充域，$0则表示所有域,$1表示第一个域,$n表示第n个域,随后开始执行模式所对应的动作action。接着开始读入第二条记录••••••直到所有的记录都读完，最后执行END操作。 搜索/etc/passwd有root关键字的所有行 1234567891011121314151. root@cts1:~ # awk -F: '/root/' /etc/passwd --- root:x:0:0:root:/root:/bin/zsh operator:x:11:0:operator:/root:/sbin/nologin # 这种是pattern的使用示例，匹配了pattern(这里是root)的行才会执行action(没有指定action，默认输出每行的内容)。2. 搜索支持正则，例如找root开头的: awk -F: '/^root/' /etc/passwd# 搜索/etc/passwd有root关键字的所有行，并显示对应的shellroot@cts1:~ # awk -F ':' '/root/&#123;print $7&#125;' /etc/passwd/bin/zsh/sbin/nologin# 这里指定了action&#123;print $7&#125; awk常见内置变量 FILENAME：awk浏览的文件名 FNR：浏览文件的记录数，也就是行数。awk是以行为单位处理的，所以每行就是一个记录 NR：awk读取文件每行内容时的行号 NF：浏览记录的域的个数。可以用它来输出最后一个域 FS：设置输入域分隔符，等价于命令行-F选项 OFS：输出域分隔符 统计/etc/passwd:文件名，每行的行号，每行的列数，对应的完整行内容 1234root@cts1:~ # awk -F ':' '&#123;print "filename:" FILENAME ",linenumber:" NR ",columns:" NF ",linecontent:"$0&#125;' /etc/passwd---filename:/etc/passwd,linenumber:1,columns:7,linecontent:root:x:0:0:root:/root:/bin/zshfilename:/etc/passwd,linenumber:2,columns:7,linecontent:bin:x:1:1:bin:/bin:/sbin/nologin 使用printf替代print,可以让代码更加简洁，易读 1234root@cts1:~ # awk -F ':' '&#123;printf("filename:%s,linenumber:%s,columns:%s,linecontent:%s\n",FILENAME,NR,NF,$0)&#125;' /etc/passwd---filename:/etc/passwd,linenumber:1,columns:7,linecontent:root:x:0:0:root:/root:/bin/zshfilename:/etc/passwd,linenumber:2,columns:7,linecontent:bin:x:1:1:bin:/bin:/sbin/nologin 指定输入分隔符，指定输出分隔符： 12345root@cts1:~ # awk 'BEGIN &#123;FS=":"; OFS="\t"&#125; &#123;print $1, $2&#125;' /etc/passwd---root xbin x... 实用例子 1234567891011121314# A：打印最后一列：awk -F: '&#123;print $NF&#125;' /etc/passwdawk -F: '&#123;printf("%s\n",$NF);&#125;' /etc/passwd# B：统计文件行数：awk 'BEGIN &#123;x=0&#125; &#123;x++&#125; END &#123;print x&#125;' /etc/passwd# C：打印9*9乘法表：awk 'BEGIN&#123;for(n=0;n++&lt;9;)&#123;for(i=0;i++&lt;n;)printf i"*"n"="i*n" ";print ""&#125;&#125;'awk 'BEGIN &#123;for(i=1;i&lt;=9;i++)&#123;for(j=1;j&lt;=i;j++)&#123;printf i"*"j"="i*j" ";&#125;print ""&#125;&#125;'awk 'BEGIN &#123;for(i=9;i&gt;=1;i--)&#123;for(j=i;j&gt;=1;j--)&#123;printf i"*"j"="i*j" ";&#125;print ""&#125;&#125;'# D: 计算1-100 之和echo "sum" | awk 'BEGIN &#123;sum=0;&#125; &#123;i=0;while(i&lt;101)&#123;sum+=i;i++&#125;&#125; END &#123;print sum&#125;' 更多详细用法参见官网 find功能： 搜索文件目录层次结构 格式： find path -option actions find &lt;路径&gt; &lt;选项&gt; [表达式] 常用可选项：1234567891011121314151617-name 根据文件名查找，支持(\'\* \' , \'? \')-type 根据文件类型查找(f-普通文件，c-字符设备文件，b-块设备文件，l-链接文件，d-目录)-perm 根据文件的权限查找，比如 755-user 根据文件拥有者查找-group 根据文件所属组寻找文件-size 根据文件小大寻找文件 -o 表达式 或-a 表达式 与-not 表达式 非 类型参数列表：1234567- **f** 普通文件- **l** 符号连接- **d** 目录- **c** 字符设备- **b** 块设备- **s** 套接字- **p** Fifo 示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[linux@linux txt]$ ll ## 准备的测试文件total 248-rw-rw-r--. 1 linux linux 235373 Apr 18 00:10 hw.txt-rw-rw-r--. 1 linux linux 0 Apr 22 05:43 LINUX.pdf-rw-rw-r--. 1 linux linux 3 Apr 22 05:50 liujialing.jpg-rw-rw-r--. 1 linux linux 0 Apr 22 05:43 mingxing.pdf-rw-rw-r--. 1 linux linux 57 Apr 22 04:40 mingxing.txt-rw-rw-r--. 1 linux linux 66 Apr 22 05:15 sort.txt-rw-rw-r--. 1 linux linux 214 Apr 18 10:08 test.txt-rw-rw-r--. 1 linux linux 24 Apr 22 05:27 uniq.txt[linux@linux txt]$ find /home/linux/txt/ -name "*.txt" ## 查找文件名txt结尾的文件/home/linux/txt/uniq.txt/home/linux/txt/mingxing.txt/home/linux/txt/test.txt/home/linux/txt/hw.txt/home/linux/txt/sort.txt## 忽略大小写查找文件名包含linux[linux@linux txt]$ find /home/linux/txt -iname "*linux*" /home/linux/txt/LINUX.pdf## 查找文件名结尾是.txt或者.jpg的文件[linux@linux txt]$ find /home/linux/txt/ \( -name "*.txt" -o -name "*.jpg" \) /home/linux/txt/liujialing.jpg/home/linux/txt/uniq.txt/home/linux/txt/mingxing.txt/home/linux/txt/test.txt/home/linux/txt/hw.txt/home/linux/txt/sort.txt另一种写法：find /home/linux/txt/ -name "*.txt" -o -name "*.jpg"# 使用正则表达式的方式去查找上面条件的文件：[linux@linux txt]$ find /home/linux/txt/ -regex ".*\(\.txt\|\.jpg\)$"/home/linux/txt/liujialing.jpg/home/linux/txt/uniq.txt/home/linux/txt/mingxing.txt/home/linux/txt/test.txt/home/linux/txt/hw.txt/home/linux/txt/sort.txt## 查找.jpg结尾的文件，然后删掉[linux@linux txt]$ find /home/linux/txt -type f -name "*.jpg" -delete[linux@linux txt]$ lltotal 248-rw-rw-r--. 1 linux linux 235373 Apr 18 00:10 hw.txt-rw-rw-r--. 1 linux linux 0 Apr 22 05:43 LINUX.pdf-rw-rw-r--. 1 linux linux 0 Apr 22 05:43 mingxing.pdf-rw-rw-r--. 1 linux linux 57 Apr 22 04:40 mingxing.txt-rw-rw-r--. 1 linux linux 66 Apr 22 05:15 sort.txt-rw-rw-r--. 1 linux linux 214 Apr 18 10:08 test.txt-rw-rw-r--. 1 linux linux 24 Apr 22 05:27 uniq.txt## 查找5天以内创建的 .sh 文件, 并显示创建/更改时间find / -name "*sh" -mtime -5 |xargs ls -l 3. Shell操作字符串字符串截取Linux中操作字符串，也是一项必备的技能。其中尤以截取字符串更加频繁，下面为大家介绍几种常用方式，截取字符串 预先定义一个变量：WEBSITE=’http://hadoop//centos/huangbo.html&#39; #截取，*任意的, 删除//左边字符串（包括制定的分隔符），保留右边字符串 root@cts1:~ # echo ${WEBSITE#*//}hadoop//centos/huangbo.html ##截取，删除左边字符串（包括指定的分隔符），保留右边字符串，和上边一个#不同的是，它一直找到最后，而不是像一个#那样找到一个就满足条件退出了。 root@cts1:~ # echo ${WEBSITE##*//}centos/huangbo.html %截取，删除右边字符串（包括制定的分隔符），保留左边字符串 root@cts1:~ # echo ${WEBSITE%//*}http://hadoop %%截取，删除右边字符串（包括指定的分隔符），保留左边字符串，和上边一个%不同的是，它一直找到最前，而不是像一个%那样找到一个就满足条件退出了。 root@cts1:~ # echo ${WEBSITE%%//*}http: 从左边第几个字符开始，以及截取的字符的个数 root@cts1:~ # echo ${WEBSITE:2:2}tp 从左边第几个字符开始，一直到结束 root@cts1:~ # echo ${WEBSITE:2}tp://hadoop//centos/huangbo.html 从右边第几个字符开始，以及字符的个数, 从-4开始, 还是往右边截取 root@cts1:~ # echo ${WEBSITE:0-4:2 ht 从右边第几个字符开始，一直到结束 root@cts1:~ # echo ${WEBSITE:0-4}html 利用awk进行字符串截取 echo $WEBSITE | awk ‘{print substr($1,2,6)}’ ttp:// 利用cut进行字符串截取 root@cts1:~ # echo $WEBSITE | cut -b 1-4 http 获取最后几个字符 root@cts1:~ # echo ${WEBSITE:(-3)} tml 截取从倒数第3个字符后的2个字符 root@cts1:~ # echo ${WEBSITE:(-3):2}tm 字符串替换使用格式 ${parameter/pattern/string} 例子12345678910# 定义变量VAR：[linux@linux ~]$ VAR="hello tom, hello kitty, hello xiaoming"# 替换第一个hello, 用`/`：[linux@linux ~]$ echo $&#123;VAR/hello/hi&#125;hi tom, hello kitty, hello xiaoming# 替换所有hello, 用'//'：[linux@linux ~]$ echo $&#123;VAR//hello/hi&#125;hi tom, hi kitty, hi xiaoming 获取字符串长度在此为大家提供五种方式获取某字符串的长度 123456789101112131415161718192021222324252627282930313233343536# 在此为大家提供五种方式获取某字符串的长度# 1. 使用wc -L命令+----------------------------------------------------+| echo $&#123;WEBSITE&#125; |wc -L || || 35 |+----------------------------------------------------+# 2. 使用expr的方式去计算+---------------------------------------------------+| expr length $&#123;WEBSITE&#125; || || 35 |+---------------------------------------------------+# 3. 通过awk + length的方式获取字符串长度+---------------------------------------------------------------------------+|echo $&#123;WEBSITE&#125; | awk '&#123;print length($0)&#125;' || || 35 |+---------------------------------------------------------------------------+# 4. 通过awk的方式计算以**\"\"**分隔的字段个数+-------------------------------------------------------------------------+| echo $&#123;WEBSITE&#125; |awk -F "" '&#123;print NF&#125;' || || 35 |+-------------------------------------------------------------------------+# 5. 通过\#的方式获取字符串（最简单，最常用）+----------------------------------------------+| echo $&#123;#WEBSITE&#125; || || 35 |+----------------------------------------------+ 4. 脚本自动安装MySql这里先做个记录, 之后会整理一份更详细的文档出来.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#!/bin/bash## auto install mysql## 假如是第二次装，那么要先停掉服务，并且卸载之前的mysqlservice mysql stopEXISTS_RPMS=`rpm -qa | grep -i mysql`echo $&#123;EXISTS_RPMS&#125;for RPM in $&#123;EXISTS_RPMS&#125;do rpm -e --nodeps $&#123;RPM&#125;done## 删除残留文件rm -fr /usr/lib/mysqlrm -fr /usr/include/mysqlrm -f /etc/my.cnfrm -fr /var/lib/mysql## 从服务器获取安装mysql的rpm包wget http://linux/soft/MySQL-client-5.6.26-1.linux_glibc2.5.x86_64.rpmwget http://linux/soft/MySQL-server-5.6.26-1.linux_glibc2.5.x86_64.rpm## 删除之前的密码文件，以免产生干扰rm -rf /root/.mysql_secret## 安装服务器rpm -ivh MySQL-server-5.6.26-1.linux_glibc2.5.x86_64.rpm## 获取到生成的随机密码##PSWD=`cat /root/.mysql_secret | awk -F ':' '&#123;print substr($4,2,16)&#125;'`PSWD=` grep -v '^$' /root/.mysql_secret | awk -F ':' '&#123;print substr($4,2,16)&#125;'`##PSWD=$&#123;PWD:1:16&#125;## 安装客户端rpm -ivh MySQL-client-5.6.26-1.linux_glibc2.5.x86_64.rpm## 然后删除刚刚下下来的rpm包rm -rf MySQL-client-5.6.26-1.linux_glibc2.5.x86_64.rpmrm -rf MySQL-server-5.6.26-1.linux_glibc2.5.x86_64.rpm## 提示安装的步骤都完成了。echo "install mysql server and client is done .!!!!!!"## 打印出来刚刚生成的mysql初始密码echo "random password is:$&#123;PSWD&#125;"## 开启mysql服务service mysql start# 手动第一次登陆，然后改掉密码[root@hadoop bin]# mysql -uroot -pZjVIWvOGD18bT7oXmysql&gt; set PASSWORD=PASSWORD('root');# 现在就可以写脚本链接mysql进行操作了[root@hadoop bin]# vi initMysql.sh#!/bin/bashmysql -uroot -proot &lt;&lt; EOF GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION; FLUSH PRIVILEGES; use mysql; select host, user, password from user;EOF]]></content>
      <categories>
        <category>Linux</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习笔记-4]]></title>
    <url>%2F2018%2F05%2F28%2FLinux%2FLinux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4%2F</url>
    <content type="text"><![CDATA[1、Linux shell简介1.1、Shell概述Shell本身是一个用C语言编写的程序，它是用户使用Unix/Linux的桥梁，用户的大部分工作都是通过Shell完成的。 Shell既是一种命令语言，又是一种程序设计语言： 作为命令语言，它交互式地解释和执行用户输入的命令； 作为程序设计语言，它定义了各种变量和参数，并提供了许多在高级语言中才具有的控制结构，包括循环和分支。 Shell它虽然不是Unix/Linux系统内核的一部分，但它调用了系统核心的大部分功能来执行程序、建立文件并以并行的方式协调各个程序的运行。Shell是用户与内核进行交互操作的一种接口，目前最流行的Shell称为bash Shell（Bourne Again Shell） Shell是一门编程语言(解释型的编程语言)，即shell脚本(就是在用linux的shell命令编程)，Shell脚本程序从脚本中一行一行读取并执行这些命令，相当于一个用户把脚本中的命令一行一行敲到Shell提示符下执行 Shell是一种脚本语言，那么，就必须有解释器来执行这些脚本 Unix/Linux上常见的Shell脚本解释器有bash、sh、csh、ksh等，习惯上把它们称作一种Shell。我们常说有多少种Shell，其实说的是Shell脚本解释器，可以通过cat /etc/shells命令查看系统中安装的shell，不同的shell可能支持的命令语法是不相同的 sh是Unix 标准默认的shell，由Steve Bourne开发，是Bourne Shell的缩写。 bash是Linux标准默认的shell，本教程也基于bash讲解。bash由Brian Fox和Chet Ramey共同完成，是Bourne Again Shell的缩写。 Shell本身支持的命令并不多，内部命令一共有40个，但是它可以调用其他的程序，每个程序就是一个命令，这使得Shell命令的数量可以无限扩展，其结果就是Shell的功能非常强大，完全能够胜任Linux的日常管理工作，如文本或字符串检索、文件的查找或创建、大规模软件的自动部署、更改系统设置、监控服务器性能、发送报警邮件、抓取网页内容、压缩文件等。 1.2、Shell基本格式代码写在普通文本文件中，通常以.sh结尾，虽然不是强制要求，但希望大家最好这么做 12345vi helloworld.sh--------------------------#!/bin/bash ## 表示用哪一种shell解析器来解析执行我们的这个脚本程序，这句话只对自执行有效，对于使用sh helloworld.sh无效echo "hello world" ## 注释也可以写在这里 在这里，我们就写好了一个shell脚本，第一行是固定需要的，表明用哪一种shell解析器来执行我们的这个脚本程序。本质上，shell脚本里面的代码都是就是一些流程控制语句加一些特殊语法再加shell**命令**组成。其中，我们可以当做每一个命令就是shell编程当中的关键字。 1.3、Shell执行方式1 、sh 方式或者bash 方式 sh helloworld.sh bash helloworld.sh ## 直接指定用系统默认的bash shell解释执行 2 、source 方式或者. 方式 source命令也称为“点命令”，也就是一个点符号（.）,是bash的内部命令。 功能：使Shell读入指定的Shell程序文件并依次执行文件中的所有语句 source命令通常用于重新执行刚修改的初始化文件，使之立即生效，而不必注销并重新登录。 用法： . helloworld.sh source helloworld.sh 3 、直接执行该脚本文件 可以有两种方式，不过这两种方式的执行，都需要该文件有执行权限 所以在执行之前，我们要更改他的执行权限 1、 切换到该文件所在的路径然后执行命令： ./helloworld.sh 2、 直接以绝对路径方式执行 /home/linux/hellworld.sh 1.4、Shell注释单行注释：Shell脚本中以#开头的代码就是注释 # xxx 多行注释：Shell脚本中也可以使用多行注释：:&lt;&lt;! xxx ! 2、Shell基本语法2.1、变量2.1.1、系统变量Linux Shell中的变量分为“系统变量”和“用户自定义变量” 系统变量可以通过set命令查看，用户环境变量可以通过env查看： 常用系统变量：\$PWD \$SHELL \$USER $HOME 2.1.2、自定义变量12345678910111213141516171819# 1.注意, 变量中间不能有空格A=123 echo $A# 2.变量中间有空格的话要加引号 - 双引号中间可以引用变量 a=zs b="this is $a" echo $b this is zs - 单引号引用变量会原样输出 b='this is $a' echo $b this is $a # 3. 要在变量后直接连接字符, 要用&#123;&#125;把变量括起来 $&#123;变量名&#125;其它字符 echo $&#123;A&#125;ddd helloddd 2.1.3、变量高级用法 撤销变量：unset ABC 声明静态变量：readonly ABC= ‘abc’ 特点是这种变量是只读的，不能unset 在一个 .sh 中. 以绝对路径的形式调另一个 .sh 使用export关键字 export A=”A in a.sh” 意味着把变量提升为当前shell 进程中的全局环境变量，可供其他子shell 程序使用， A 变量就成了a.sh 脚本所在bash 进程的全局变量，该进程的所有子进程都能访问到变量A 通过 . /root/bin/b.sh 或 source /root/bin/b.sh 来调用 总结： a.sh中直接调用b.sh，会让b.sh在A所在的bash进程的“子进程”空间中执行 而子进程空间只能访问父进程中用export定义的变量 一个shell进程无法将自己定义的变量提升到父进程空间中去 source或者“.”号执行脚本时，会让脚本在调用者所在的shell进程空间中执行 2.1.4、反引号赋值 a=`ls -l /root/bin` ##反引号，运行里面的命令，并把结果返回给变量a 另外一种写法：a=$(ls -l /root/bin) 2.1.5、变量有用技巧 形式 说明 ${var} 变量本来的值 ${var:-word} 如果变量 var 为空或已被删除(unset)，那么返回 word，但不改变 var 的值 ${var:+word} 如果变量 var 被定义，那么返回word，但不改变 var 的值 ${var:=word} 如果变量 var 为空或已被删除(unset)，那么返回 word，并将 var 的值设置为 word ${var:?message} 如果变量 var 为空或已被删除(unset)，那么将消息 message 送到标准错误输出，可以用来检测变量 var 是否可以被正常赋值。 若此替换出现在Shell脚本中，那么脚本将停止运行 2.1.6 特殊变量 $? 表示上一个命令退出的状态码 $$ 表示当前进程编号 $0 表示当前脚本名称 $n 表示n 位置的输入参数（n**代表数字，n&gt;=1 ）** $# 表示参数的个数，常用于循环 $* 和$@ 都表示参数列表 注意：$*与$@区别 $* 和 $@ 都表示传递给函数或脚本的所有参数** 不被双引号” “包含时 $* 和 $@ 都以\$1 \$2 … \$n 的形式组成参数列表 当它们被双引号” “包含时 “$*“ 会将所有的参数作为一个整体，以”\$1 \$2 … \$n”的形式组成一个整串； $* 会将各个参数分开，以”\$1” “\$2” … “\$n” 的形式组成一个参数列表 2.1.7 变量的其他注意点 使用变量 使用一个定义过的变量, 只需要在变量名前面加 $ 符号 如果变量后面直接跟上了字符串, 就必须要加花括号 推荐给所有变量加上花括号, 这个是好的编程习惯 已定义的非只读变量, 可以被重新定义 只读变量 使用 readonly 命令可以将变量定义为只读变量，只读变量的值不能被改变。 1234567#!/bin/bashmyUrl="http://www.w3cschool.cc"readonly myUrlmyUrl="http://www.runoob.com"---# 会报错zsh: read-only variable: myUrl 删除变量, 只读变量不能删除, 变量被删除后不能再次使用 1unset variable_name 变量类型 1) 局部变量 局部变量在脚本或命令中定义，仅在当前shell实例中有效，其他shell启动的程序不能访问局部变量。 2) 环境变量 所有的程序，包括shell启动的程序，都能访问环境变量，有些程序需要环境变量来保证其正常运行。必要的时候shell脚本也可以定义环境变量。 3) shell变量 shell变量是由shell程序设置的特殊变量。shell变量中有一部分是环境变量，有一部分是局部变量，这些变量保证了shell的正常运行 2.18 字符串获取字符串长度12string="abcd"echo $&#123;#string&#125; #输出 4 提取子字符串以下实例从字符串第 2 个字符开始截取 4 个字符： 12string="runoob is a great site"echo $&#123;string:1:4&#125; # 输出 unoo 查找子字符串查找字符 “i 或 s“ 的位置： 12string="runoob is a great company"echo `expr index "$string" is` # 输出 8 2.19 Shell 数组 bash支持一维数组（不支持多维数组），并且没有限定数组的大小。 类似与C语言，数组元素的下标由0开始编号。获取数组中的元素要利用下标，下标可以是整数或算术表达式，其值应大于或等于0。 定义数组在Shell中，用括号来表示数组，数组元素用”空格”符号分割开。定义数组的一般形式为： 1数组名=(值1 值2 ... 值n) 还可以单独定义数组的各个分量： 123array_name[0]=value0array_name[1]=value1array_name[n]=valuen 可以不使用连续的下标，而且下标的范围没有限制。 获取数组的长度 获取数组长度的方法与获取字符串长度的方法相同，例如： 123456# 取得数组元素的个数length=$&#123;#array_name[@]&#125;# 或者length=$&#123;#array_name[*]&#125;# 取得数组单个元素的长度lengthn=$&#123;#array_name[n]&#125; 2.2、运算符2.2.1、算数运算符1. 用expr12345678格式 expr m + n 注意expr运算符间要有空格例如计算（2＋3）×4 的值1、分步计算 S=`expr 2 + 3` expr $S \* 4 ## *号需要转义2、一步完成计算 expr `expr 2 + 3 ` \* 4 echo `expr \`expr 2 + 3\` \* 4` 用expr还可以计算字符串的长度，子字符串出现的位置，截取字符串等等 123456789cts1 ~ # name='woshishuaige'cts1 ~ # expr length name4cts1 ~ # expr length $name12cts1 ~ # expr index $name shuai3cts1 ~ # expr substr $name 6 2sh 详情请翻阅: expr –help 2. 用(())12345678((1+2))(((2+3)*4))count=1((count++))((++count))echo $count# 但是要想取到运算结果，需要用$引用a=$((1+2)) 3. 用\$[]12345SS=$[2+3]echo $SSSS=$[2*3]echo $SSecho $[(2 + 3)*3] 4. 用let1234first=1second=2let third=first+secondecho $&#123;third&#125; 5. 用 | bc以上命令都只对整形数值有效，不适用于浮点数 如果有浮点数参与运算，可以将echo与bc命令结合起来使用，代码如下 123456echo "1.212*3" | bc ## 简单浮点运算echo "scale=2;3/8" | bc ##将输出结果设置为2位echo "obase=2;127" | bc ##输出运算结果的二进制echo "obase=10;ibase=2;101111111" | bc ##将二进制转换成十进制echo "10^10" | bc ##求幂指数echo "sqrt(100)" | bc ##开平方 除了用bc做尽职转换以外，还可以这样做： 12345echo $((base#number)) 表示把任意base进制的数number转换成十进制例子：echo $((8#377)) 返回255echo $((025)) 返回21 ， 八进制echo $((0xA4)) 返回164 ， 十六进制 使用bc还可以用来比较浮点数的大小： 12345678910111213[root@hadoop02 bin]# echo "1.2 &lt; 2" |bc1[root@hadoop02 bin]# echo "1.2 &gt; 2" |bc0[root@hadoop02 bin]# echo "1.2 == 2.2" |bc0[root@hadoop02 bin]# echo "1.2 != 2.2" |bc1看出规律了嘛？运算如果为真返回 1，否则返回 0，写一个例子：[root@hadoop02 bin]# [ $(echo "2.2 &gt; 2" |bc) -eq 1 ] &amp;&amp; echo yes || echo noyes[root@hadoop02 bin]# [ $(echo "2.2 &lt; 2" |bc) -eq 1 ] &amp;&amp; echo yes || echo nono 2.2.2、关系运算符下面给出一张关系运算符的列表： 运算符 等同运算符 说明 -eq = 检测两个数是否相等，相等返回true -ne != 检测两个数是否相等，不相等返回true -ge &gt;= 检测左边的数是否大等于右边的，如果是，则返回true -gt &gt; 检测左边的数是否大于右边的，如果是，则返回true -le &lt;= 检测左边的数是否小于等于右边的，如果是，则返回true -lt &lt; 检测左边的数是否小于右边的，如果是，则返回true 2.2.3、布尔运算符 运算符 等同运算符 说明 ! ! 非运算，表达式为 true 则返回false，否则返回true -a &amp;&amp; 与运算，两个表达式都为true 才返回true -o \ \ 或运算，有一个表达式为true 则返回true 2.2.4、字符串运算符 运算符 说明 = 检测两个字符串是否相等，相等返回true != 检测两个字符串是否相等，不相等返回true -z 检测字符串长度是否为0，为0返回true -n 检测字符串长度是否为0，不为0返回true str 检测字符串是否为空，不为空返回true 2.2.5、文件运算符 运算符 说明 -d 检测文件是否是目录，如果是，则返回true -f 检测文件是否是普通文件（既不是目录，也不是设备文件），如果是，则返回true -e 检测文件（包括目录）是否存在，如果是，则返回true -s 检测文件是否为空（文件大小是否大于0），不为空返回true -r 检测文件是否可读，如果是，则返回true -w 检测文件是否可写，如果是，则返回true -x 检测文件是否可执行，如果是，则返回true -b 检测文件是否是块设备文件，如果是，则返回true -c 检测文件是否是字符设备文件，如果是，则返回true 2.3、流程控制2.3.1、ifif.. then ..elif.. then.. else..fi123456789101112131415161718192021222324252627282930313233343536# if语法格式：if conditionthen statements [elif condition then statements. ..] [else statements ] fi# 示例程序：[root@hadoop02 bin]# vi g.sh#!/bin/bash## read a value for NAME from stdinread -p "please input your name:" NAME## printf '%s\n' $NAMEif [ $NAME = root ]thenecho "hello $&#123;NAME&#125;, welcome !"elif [ $NAME = hadoop ]then echo "hello $&#123;NAME&#125;, welcome !"else echo "I don’t know you !"fi## 规则解释[ condition ] (注：condition前后要有空格)#非空返回true，可使用$?验证（0为true，&gt;1为false）[ hadoop ]#空返回false[ ]# 注意[ ]内部的=周边的空格 三元运算符12[ condition ] &amp;&amp; echo OK || echo notok# 条件满足，执行&amp;&amp;后面的语句；条件不满足，执行||后面的语句 条件判断组合 条件判断组合有两种使用方式： [] 和 [[]] 注意它们的区别： [[ ]] 中逻辑组合可以使用 &amp;&amp; || 符号 [] 里面逻辑组合可以用 -a -o 常用判断运算符1. 字符串比较 = 判断相等!= 判断不相等-z 字符串长度是为0返回true-n 字符串长度是不为0返回true 2. 整数比较 -lt 小于 less than -le 小于等于 -eq 等于 -gt 大于 great than -ge 大于等于 -ne 不等于 3. 文件判断 -d 是否为目录 if [ -d /bin ]; then echo ok; else echo notok;fi -f 是否为文件 if [ -f /bin/ls ]; then echo ok; else echo notok;fi -e 是否存在 if [ -e /bin/ls ]; then echo ok; else echo notok;fi 2.3.2、while1234567891011121314151617181920212223242526while expressiondo command ……done----------------------i=1while ((i&lt;=3))do echo $i let i++done----------------------#!/bin/bashi=1while [ $i -le 3 ]do echo $i let i++done# 命令执行完毕，控制返回循环顶部，从头开始直至测试条件为假# 换种方式：循环体会一直执行，直到条件表达式expression为false# 注意：上述let i++ 可以写成 i=$(($i+1))或者i=$((i+1)) 2.3.3、caseCase语法（通过下面这个例子展示）： 12345678910case $1 instart) echo "starting" ;;stop) echo "stoping" ;;*) echo "Usage: &#123;start|stop&#125;"esac 2.3.4、for语法格式： 1234567for 变量 in 列表docommand……done# 列表是一组值（数字、字符串等）组成的序列，每个值通过空格分隔。每循环一次，就将列表中的下一个值赋给变量 三种方式 12345678# 方式1for N in 1 2 3; do echo $N; done# 方式2for N in &#123;1..3&#125;; do echo $N; done# 方式3for ((i=0; i&lt;=2; i++)); do echo "welcome $i times"; done 2.3.5、util语法结构： 123456789101112131415161718192021222324until expressiondo command ……done# expression一般为条件表达式，如果返回值为 false，则继续执行循环体内的语句，否则跳出循环。# 换种方式说：循环体会一直执行，直到条件表达式expression为true## 示例#!/bin/bash## vi util.sha=0until [ ! $a -lt 3 ]do echo $a a=`expr $a + 1`done----输出: 012 2.4、数组在Shell中，用括号来表示数组，数组元素用“空格”符号分割开。定义数组的一般形式为 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061array_name=(value1 ... valuen)例子：mingxing=(huangbo xuzheng wangbaoqiang)也可以单独定义：mingxing[3]=liujialing读取数组元素的格式是：$&#123;array_name[index]&#125;============================================# 获取数组下标： 前面加 `!`[linux@linux ~]$ echo $&#123;!mingxing[@]&#125; 或者[linux@linux ~]$ echo $&#123;!mingxing[*]&#125;---输出: 0 1 2 3============================================# 输出数组的所有元素：直接取值[linux@linux ~]$ echo $&#123;mingxing[*]&#125;[linux@linux ~]$ echo $&#123;mingxing[@]&#125;============================================# 获取数组的长度：`#` [linux@linux ~]$ echo $&#123;#mingxing[*]&#125;[linux@linux ~]$ echo $&#123;#mingxing[@]&#125;============================================# 数组对接: 添加新的值[linux@linux ~]$ mingxing+=(liuyifei liuyufeng)============================================# 删除数组元素，但是会保留数组对应位置，就是该值的下标依然会保留，会空着，之后，还可以填充其他的值进来。# 删除第一个元素：之后 $&#123;mingxing[0]&#125; 就是空值了[linux@linux ~]$ unset mingxing[0]============================================# 遍历数组：#!/bin/bashIP=(192.168.1.1 192.168.1.2 192.168.1.3)# 第一种方式for ((i=0;i&lt;$&#123;#IP[*]&#125;;i++))doecho $&#123;IP[$i]&#125;done#第二种方式for ip in $&#123;IP[*]&#125;do echo $ipdone============================================# 数组的分片：$&#123;arr[@]:number1:number2&#125;这里number1从下标number1开始取值，number2往后取几个元素，即取到的新的数组的长度---cts1 ~ # arr=(1 2 3 4 5 6 7)cts1 ~ # echo "&#123;arr[@]:0:3&#125; --- $&#123;arr[@]:0:3&#125; "&#123;arr[@]:0:3&#125; --- 1 2 3cts1 ~ # echo "&#123;arr[@]:3:3&#125; --- $&#123;arr[@]:3:3&#125; "&#123;arr[@]:3:3&#125; --- 4 5 6cts1 ~ # echo "&#123;arr[@]:4:3&#125; --- $&#123;arr[@]:4:3&#125; "&#123;arr[@]:4:3&#125; --- 5 6 7 2.5、函数使用函数的语法使用示例 : 1234567891011121314[root@hadoop02 bin]# vi i.sh #!/bin/shhello()&#123; echo "`date +%Y-%m-%d`" # return 2&#125;helloecho “huangbo” # echo $?A="mazhonghua"echo $A---执行结果----“huangbo”mazhonghua 函数的调用方式就是直接写函数名就OK了 123注意：1、必须在调用函数地方之前，先声明函数，shell脚本是逐行运行。不会像其它语言一样先预编译2、函数返回值，只能通过$? 系统变量获得，可以显示加：return 返回，如果不加，将以最后一条命令运行结果，作为返回值。 return后跟数值n(0-255) 脚本调试： 使用-x选项跟踪脚本调试shell脚本，能打印出所执行的每一行命令以及当前状态 sh -x i.sh 或者在代码中加入：set -x 12345678910111213cts1 Desktop # sh -x i.sh+ hello++ date +%Y-%m-%d+ echo 2018-05-292018-05-29+ return 2+ echo $'\342\200\234huangbo\342\200\235'“huangbo”+ echo 00+ A=mazhonghua+ echo mazhonghuamazhonghua 2.6、函数参数直接上代码 123456789101112131415161718192021222324vim funcWithParam.sh--------sh文件--------#!/bin/bash# filename=funcWithParamfuncWithParam()&#123; echo "第一个参数为 $1 !" echo "第二个参数为 $2 !" echo "第十个参数为 $10 !" echo "第十个参数为 $&#123;10&#125; !" echo "第十一个参数为 $&#123;11&#125; !" echo "参数总数有 $# 个!" echo "作为一个字符串输出所有参数 $* !"&#125;funcWithParam 1 2 3 4 5 6 7 8 9 34 73--------------------# 调用cts1 Desktop # sh funcWithParam.sh第一个参数为 1 !第二个参数为 2 !第十个参数为 10 !第十个参数为 34 !第十一个参数为 73 !参数总数有 11 个!作为一个字符串输出所有参数 1 2 3 4 5 6 7 8 9 34 73 ! 2.7、跨脚本调用函数编写一个base.sh脚本，里面放放一个test函数 123456vim base.sh------#!/bin/bashtest()&#123; echo "hello"&#125; 再编写一个other.sh脚本，里面引入base.sh脚本，并且调用test函数： 12345678910111213vim other.sh--------#!/bin/bashsource base.sh ## 引入脚本test ##调用引入脚本当中的test函数============================================# 执行cts1 Desktop # sh other.shother.sh: line 2: ./base.sh: 权限不够cts1 Desktop # chmod 755 other.sh base.shcts1 Desktop # ./other.shhello 3、Shell综合案例3.1、打印9*9乘法表示例代码： 12345678910111213141516171819202122232425#!/bin/bashfor((i=1;i&lt;=9;++i))do for((j=1;j&lt;=i;j++)) do echo -ne "$i*$j=$((i*j))\t" done echodone# 解释-n 不加换行符-e 解释转义符echo 换行效果: 1*1=12*1=2 2*2=43*1=3 3*2=6 3*3=94*1=4 4*2=8 4*3=12 4*4=165*1=5 5*2=10 5*3=15 5*4=20 5*5=256*1=6 6*2=12 6*3=18 6*4=24 6*5=30 6*6=367*1=7 7*2=14 7*3=21 7*4=28 7*5=35 7*6=42 7*7=498*1=8 8*2=16 8*3=24 8*4=32 8*5=40 8*6=48 8*7=56 8*8=649*1=9 9*2=18 9*3=27 9*4=36 9*5=45 9*6=54 9*7=63 9*8=72 9*9=81 3.2、自动部署集群的JDK1、 需求描述 12公司内有一个N个节点的集群，需要统一安装一些软件（jdk）需要开发一个脚本，实现对集群中的N台节点批量自动下载、安装jdk 2、 思路 思考一下：我们现在有一个JDK安装包在一台服务器上。那我们要实现这个目标： 1231、 把包传到每台服务器，或者通过本地yum源的方式去服务器取2、 给每台一台机器发送一个安装脚本，并且让脚本自己执行3. 要写一个启动脚本，用来执行以上两部操作 3、 Expect**的使用** 蛋疼点：假如在没有配置SSH免密登录的前提下，我们要要是scp命令从一台机器拷贝文件夹到另外的机器，会有人机交互过程，那我们怎么让机器自己实现人机交互？ 灵丹妙药：expect 命令 描述 set 可以设置超时，也可以设置变量 timeout 超时等待时间，默认 10s spawn 执行一个命令 send 执行交互，相当于手动输入 expect “”（expect内部命令） 匹配输出的内容 exp_continue 继续执行下面匹配 思路：模拟该人机交互过程，在需要交互的情况下，通过我们的检测给输入提前准备好的值即可 示例：观看配置SSH免密登录的过程 实现脚本： 123456789101112131415161718vi testExpect.sh## 定义一个函数sshcopyid()&#123; expect -c " spawn ssh-copy-id $1 expect &#123; \"(yes/no)?\" &#123;send \"yes\r\";exp_continue&#125; \"password:\" &#123;send \"$2\r\";exp_continue&#125; &#125; "&#125;## 调用函数执行sshcopyid $1 $2# 注意：如果机器没有expect，则请先安装expectyum install -y expect 4、 脚本实现 启动脚本initInstallJDK.sh 12345678910111213141516171819202122232425262728293031#!/bin/bashSERVERS="192.168.123.201"PASSWORD=hadoopBASE_SERVER=192.168.123.202auto_ssh_copy_id() &#123; expect -c "set timeout -1; spawn ssh-copy-id $1; expect &#123; *(yes/no)* &#123;send -- yes\r;exp_continue;&#125; *password:* &#123;send -- $2\r;exp_continue;&#125; eof &#123;exit 0;&#125; &#125;";&#125;ssh_copy_id_to_all() &#123; for SERVER in $SERVERS do auto_ssh_copy_id $SERVER $PASSWORD done&#125;ssh_copy_id_to_allfor SERVER in $SERVERSdo scp installJDK.sh root@$SERVER:/root ssh root@$SERVER chmod 755 installJDK.sh ssh root@$SERVER /root/installJDK.shdone 安装脚本installJDK.sh 1234567891011#!/bin/bashBASE_SERVER=192.168.123.202yum install -y wgetwget $BASE_SERVER/soft/jdk-8u73-linux-x64.tar.gztar -zxvf jdk-8u73-linux-x64.tar.gz -C /usr/localcat &gt;&gt; /etc/profile &lt;&lt;EOFexport JAVA_HOME=/usr/local/jdk1.8.0_73export PATH=\$PATH:\$JAVA_HOME/binEOF 4、总结写脚本注意事项： 1、开头加解释器： #!/bin/bash，和注释说明。 2、命名建议规则：变量名大写、局部变量小写，函数名小写，名字体现出实际作用。 3、默认变量是全局的，在函数中变量 local 指定为局部变量，避免污染其他作用域。 4、set -e 遇到执行非 0 时退出脚本， set -x 打印执行过程。 5、写脚本一定先测试再到生产上。]]></content>
      <categories>
        <category>Linux</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习笔记-2]]></title>
    <url>%2F2018%2F05%2F27%2FLinux%2FLinux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2%2F</url>
    <content type="text"><![CDATA[1. VI文本编辑器学会使用vi编辑器是学习Linux系统的必备技术之一，因为一般的Linux服务器是没有GUI界面的，Linux运维及开发人员基本上都是通过命令行的方式进行文本编辑或程序编写的。vi编辑器是Linux内置的文本编辑器，几乎所有的类unix系统中都内置了vi编辑器，而其它编辑器则不一定，另外很多软件会调用vi编辑进行内容编写，例如crontab定时任务。较之于其它编辑器或GUI编辑器，vi编辑速度是最快的。VIM是它的增强版本，VI有三种基本工作模式，分别是： 命令模式（command mode）、或者叫一般模式 插入模式（insert mode）、或者叫编辑模式 底行模式（last line mode）、或者叫命令行模式 1. 最基本用法 1、首先会进入“一般模式”，此模式只接受各种命令快捷键，不能编辑文件内容 2、按i键，就会从一般模式进入编辑模式，此模式下，敲入的都是文件内容 3、编辑完成之后，按Esc键退出编辑模式，回到一般模式 4、再按：，进入“底行命令模式”，输入wq命令，回车即可保存退出 2. 移动光标 (一般模式) 1、 使用上下左右键可以移动光标 2、 使用h,j,k,l，依次是向左，下，上，右移动 3、 w：将光标移动到下一个单词的首字母处 4、 W：利用空格向后移动光标，就是忽略标点 5、 b：利用word包括标点向前移动光标, 与w相对应 6、 B：利用空格向前移动光标，忽略标点 7、 e：将光标移动到下一个word的尾部，包括符号 8、 E: 将光标移动到下一个空格分隔字的尾部 9、 (：移动到句子开始 10、 )：移动到句子结束 11、 0：移动光标到本句句首 12、 $：移动光标到本行行尾 13、 {：移动到段落开始 14、 }：移动到段落结束 15、 H：屏幕顶端 16、 L：屏幕底端 17、 M：移动到屏幕中央位置 18、 gg：直接跳到文件的首行行首 19、 G：直接跳到文件的末行行首 20、 最强光标移动： % : 匹配括号移动，包括(, {, [.（你需要把光标先移到括号上） *: 匹配光标当前所在的单词，移动光标到下一个匹配单词 #: 匹配光标当前所在的单词，移动光标到上一个匹配单词 重点总结: h l : 左右, j k : 下上. w: 下一个单词 , b: 上一个单词. 0: 本行行首, $: 本行行尾 (: 句子开始, ): 句子结束 {: 段落开始 }: 段落结束 gg: 文件行首 G: 文件行尾 %: 下一个括号 *: 下一个单词 #: 上一个单词 3. 常用操作(一般模式 &gt;&gt; 插入模式) 单位 指令1 指令2 当前位 前一位/后一位 i: 在光标前一位开始插入 a: 光标后一位开始插入 当前行 最前/最后 I: 在该行的最前面插入 A: 在该行的最后插入 当前行 上一行/下一行 o: 小o, 当前行的下一行插入空行 O: 大o, 当前内容下移一行, 当前行插入空行 当前行 删除 dd: 删除当前行 3dd: ndd, 删除从当前行开始的 n行 当前行 复制 yy: 复制光标所在行 3yy: 复制从当前行开始的3行 粘贴 p: 粘贴到光标所在行的下一行 撤销 u: undo 撤销操作, 可一直撤销到最前面 重点总结: i : 当前光标前开始插入; a: 光标后一位开始插入 I: 在该行的最前面插入; A: 在该行的最后插入 o: 小o, 当前行的下一行插入空行 O: 大o, 当前内容下移一行, 当前行插入空行 dd: 删除当前行 3dd: ndd, 删除从当前行开始的 n行 yy: 复制光标所在行 3yy: 复制从当前行开始的3行 p: 粘贴到光标所在行的下一行 u: undo 撤销操作, 可一直撤销到最前面 4. 查找并替换 在底行命令模式中输入 1. 显示行号 :set nu 2. 隐藏行号 :set nonu 3 .查找关键字 :/you ## 效果：查找文件中出现的you，并定位到第一个找到的地方，按 n可以定位到下一个匹配位置（按N定位到上一个） 查询的时候被匹配上的字符串会被高亮，可以在命令模式下使用:noh取消高亮 4. 直接跳转到 3行 : 3 5. 替换操作 :1 s/sad/bbb 将第一行的第一个sad替换为bbb :1,5 s/sad/bbb 将第一行到第五行的第一个sad替换为bbb :1,. s/sad/bbb 将第一行到光标行的第一个sad替换为bbb :.,$ s/sad/bbb 将光标行到缓冲区最后一行的sad替换为bbb :s/sad/bbb 查找光标所在行的第一个sad，替换为bbb :s/sad/bbb/g 查找光标所在行的所有sad，替换为bbb :%s/sad/bbb 查找文件中所有行第一次出现的sad，替换为bbb :%s/sad/bbb/g 查找文件中所有的sad，替换为bbb 6. 屏幕翻滚类命令 Ctrl + u：向文件首翻半屏 Ctrl + d：向文件尾翻半屏 Ctrl + f：向文件尾翻一屏 Ctrl＋b：向文件首翻一屏 nz：将第n行滚至屏幕顶部，不指定n时将当前行滚至屏幕顶部 7. 其它的小技巧 r 替换光标处一个字符 R 进入替换模式，从光标处连续替换 s 删除当前字符，进入插入模式 S 删除当前行，进入插入模式 ≈ dd, dd也会删除当前行, 但是不会进入插入模式 f s 光标行内向后查找第一个出现的字符s (先输f , 再输 s, 向后找 s) F s 光标行内向前查找第一个出现的字符s (先输F , 再输 s, 向前找 s) ~ 大小写转换，只转换光标处字符 8. 速查网址 vim详解 2. 网络管理ifconfig命令ifconfig命令主要用于配置网络接口，如果不加任何参数，则ifconfig命令用于查看当前所有活动网络接口的状态信息，如下图： eth0 表示第一块网卡，其中HWaddr表示网卡的物理地址，可以看到目前这个网卡的物理地址(MAC地址）是00:0C:29:D6:C7:0E。 inet addr 用来表示网卡的IP地址，此网卡的IP地址是192.168.179.6，广播地址192.168.170.255，掩码地址Mask:255.255.255.0。 lo 是表示主机的回坏地址，这个一般是用来测试一个网络程序，但又不想让局域网或外网的用户能够查看，只能在此台主机上运行和查看所用的网络接口。比如把 httpd服务器的指定到回坏地址，在浏览器输入127.0.0.1就能看到你所架WEB网站了。但只是您能看得到，局域网的其它主机或用户无从知道。 ifconfig其他常用使用 -a 显示所有网络接口，包括停用的 -s 短格式显示网络信息，同netstat -i -v 显示详细信息，在网络出错的情况下适用 interface 指定网络接口 up 启用网络接口 down 关闭网络接口 启动关闭指定网卡&amp; 常用操作： 12345ifconfig eth0 up #启动网卡ifconfig eth0 down #关闭网卡 ifconfig #处于激活状态的网络接口ifconfig -a #所有配置的网络接口，不论其是否激活ifconfig eth0 #显示eth0的网卡信息 网络配置1. 三种通信模式Vmware中的虚拟机和宿主机进行通信有三种网络方式 一、Brigde――桥接：默认使用VMnet0 原理: Bridge 桥”就是一个主机，这个机器拥有两块网卡，分别处于两个局域网中，同时在”桥”上，运行着程序，让局域网A中的所有数据包原封不动的流入B，反之亦然。这样，局域网A和B就无缝的在链路层连接起来了，在桥接时，VMWare网卡和物理网卡应该处于同一IP网段 当然要保证两个局域网没有冲突的IP. VMWare 的桥也是同样的道理，只不过，本来作为硬件的一块网卡，现在由VMWare软件虚拟了！当采用桥接时，VMWare会虚拟一块网卡和真正的物理网卡就行桥接，这样，发到物理网卡的所有数据包就到了VMWare虚拟机，而由VMWare发出的数据包也会通过桥从物理网卡的那端发出。 所以，如果物理网卡可以上网，那么桥接的软网卡也没有问题了，这就是桥接上网的原理了。 联网方式： 这一种联网方式最简单，在局域网内，你的主机是怎么联网的，你在虚拟机里就怎么连网。把虚拟机看成局域网内的另一台电脑就行了！ 提示：主机网卡处在一个可以访问Internet的局域网中，虚拟机才能通过Bridge访问Internet。 二、NAT――网络地址转换 ：默认使用VMnet8 原理： NAT 是 Network address translate的简称。NAT技术应用在internet网关和路由器上，比如192.168.0.123这个地址要访问internet，它的数据包就要通过一个网关或者路由器，而网关或者路由器拥有一个能访问internet的ip地址，这样的网关和路由器就要在收发数据包时，对数据包的IP协议层数据进行更改（即 NAT），以使私有网段的主机能够顺利访问internet。此技术解决了IP地址稀缺的问题。同样的私有IP可以网关NAT 上网。 VMWare的NAT上网也是同样的道理，它在主机和虚拟机之间用软件伪造出一块网卡，这块网卡和虚拟机的ip处于一个地址段。同时，在这块网卡和主机的网络接口之间进行NAT。虚拟机发出的每一块数据包都会经过虚拟网卡，然后NAT，然后由主机的接口发出。 虚拟网卡和虚拟机处于一个地址段，虚拟机和主机不同一个地址段，主机相当于虚拟机的网关，所以虚拟机能ping到主机的IP，但是主机ping不到虚拟机的IP。 联网方式： 方法1、动态IP地址。 主机是静态IP或动态IP，都无所谓，将虚拟机设置成使用DHCP方式上网,Windows下选择“自动获取IP“，linux下开启DHCP服务即可。（这种方法最简单，不用过多的设置，但要在VMware中进行“编辑→虚拟网络设置”，将NAT和DHCP都开启了。一般NAT默认开启，DHCP默认关闭） 方法2、静态IP地址。 如果不想使用DHCP，也可以手动设置： IP设置与vmnet1同网段,网关设置成vmnet8的网关（在“虚拟网络设置”里的Net选项卡里能找到Gateway）通常是xxx.xxx.xxx.2。子网掩码设置与VMnet8相同（设置好IP地址后，子网掩码自动生成）DNS设置与主机相同。 例如：主机IP是10.70.54.31,设置虚拟机IP为10.70.54.22。Netmask,Gateway,DNS都与主机相同即可实现 虚拟机 —主机 虚拟机互联网 通信。 提示：使用NAT技术，主机能上网，虚拟机就可以访问Internet，但是主机不能访问虚拟机。 三、Host-Only――私有网络共享主机：默认使用VMnet1 联网方法： 方法1、动态IP地址。像上面那样开启DHCP后，虚拟机直接自动获取IP地址和DNS。就可以和主机相连了。当然，还要进行一些局域网共享的操作，这里不再赘述。 方法2、静态IP地址。 也可以手动设置，将虚拟机IP设置与VMnet1同网段,网关设置成VMnet1的网关相同,其余设置与VMnet1相同,DNS设置与主机相同。 例如：VMnet1 IP:172.16.249.1 Gateway :172.16.249.2 那么虚拟机 IP:172.16.249.100 Gateway: 172.16.249.2 这样、 虚拟机主机 可以通信但是、 虚拟机互联网 无法通信 提示：Host-only技术只用于主机和虚拟机互访，于访问internet无关。 2. NAT网络模式的配置见Linux学习笔记3, 第8条: Linux虚拟主机集群测试环境基本搭建 3. 其它的常用网络管理命令 ping命令: 常用来测试网络连接是否正常 先确定能ping通 ping www.baidu.com host命令: host命令用来进行DNS查询 然后用host命令可以查看到 host www.baidu.com 然后通过浏览器访问该地址：119.75.213.61 netstat命令: netstat命令可以显示网络接口的很多统计信息，包括打开的socket和路由表 以下是常用命令选项 123456789101112-a (all)显示所有选项，默认不显示LISTEN相关-t (tcp)仅显示tcp相关选项-u (udp)仅显示udp相关选项-n 拒绝显示别名，能显示数字的全部转化成数字-l 仅列出有在 Listen (监听) 的服務状态-p 显示建立相关链接的程序名-r 显示路由信息，路由表-e 显示扩展信息，例如uid等-s 按各个协议进行统计-c 每隔一个固定时间，执行该netstat命令 例子 1、列出所有端口，包括监听和未监听的：netstat -a 2、列出所有TCP端口：netstat -at 3、列出所有UDP端口：netstat -au 4、列出所有监听状态的TCP端口：该命令最重要用来查看哪个程序占用了哪个网络端口号 123# 比如查看谁占用了 tcp 的80 端口netstat -nltp | grep 80 [23:04:54]tcp 0 0 :::80 :::* LISTEN 3890/httpd 4. 防火墙防火墙根据配置文件/etc/sysconfig/iptables来控制本机的“出、入”网络访问行为 其对行为的配置策略有四个策略表 基础必备技能 | 查看防火墙状态 | service iptables status || ———————- | —————————– || 开启防火墙 | service iptables start || 关闭防火墙 | service iptables stop || 关闭防火墙开机自启 | chkconfig iptables off || 设置防火墙开机自启 | chkconfig iptables on || 查看防火墙开机启动状态 | chkconfig iptables –list | 扩展知识 1234567891011121314151617181920212223242526# 1、列出iptables规则iptables -L -n# 列出iptables规则并显示规则编号iptables -L -n --line-numbers# 2、列出iptables nat表规则（默认是filter表）iptables -L -n -t nat# 3、清除默认规则（注意默认是filter表，如果对nat表操作要加-t nat）#清除所有规则iptables -F#重启iptables发现规则依然存在，因为没有保存service iptables restart#保存配置service iptables save# 4、禁止SSH登陆（如果服务器在机房，一定要小心）iptables -A INPUT -p tcp --dport 22 -j DROP# 5、删除规则iptables -D INPUT -p tcp --dport 22 -j DROP # 6、加入一条INPUT规则开放80端口 iptables -I INPUT -p tcp --dport 80 -j ACCEPT 3. Linux系统启动级别管理使用runlevel命令可以查看系统运行的级别 12runlevel [23:12:37]N 3 修改系统默认启动级别 1234567891011121314# 启动级别在这个路径查看vi /etc/inittab-------------------# Default runlevel. The runlevels used are:# 0 - halt (Do NOT set initdefault to this)# 1 - Single user mode# 2 - Multiuser, without NFS (The same as 3, if you do not have networking)# 3 - Full multiuser mode# 4 - unused# 5 - X11# 6 - reboot (Do NOT set initdefault to this)#id:3:initdefault: id:3:initdefault: ## 配置默认启动级别 ## 通常将默认启动级别设置为：3 4. 用户和组1. 用户和组的概念Linux是一个多任务多用户的操作系统，当我们在使用ls -l命令的时候我们看到如下信息： 123drwxrwxr-x. 2 root root 4096 5月 24 00:15 test-rw-r--r--. 1 root root 58 5月 25 11:00 test.sh-rwxr-xr-x. 1 root root 35 5月 23 23:55 test.txt test：表示文件或者目录，具体的文件类型是由该行最前面的那个符号表示 12345- 当为[ *d* ]则是目录- 当为[ *-* ]则是文件；- 若是[ *l* ]则表示为链接文档(link file)；- 若是[ *b* ]则表示为装置文件里面的可供储存的接口设备(可随机存取装置)；- 若是[ *c* ]则表示为装置文件里面的串行端口设备，例如键盘、鼠标(一次性读取装置); drwxrwxr-x ：该文件的类型和权限信息 2 ：链接数，如果是文件则是1 ， 如果是文件夹则表示该文件夹下的子文件夹个数 第一个root ：文件或者目录的所属者** 第二个root ：所属用户组** 4096 ：文件或者目录的大小，是目录的话一般都是4096 5月 24 00:15：文件的最后编辑时间 通过以上信息得知，每个文件都设计到用户和组的权限问题 在Linux中，用户是能够获取系统资源的权限的集合，组是权限的容器 Linux 用户类型 用户类型 描述 管理员root 具有使用系统所有权限的用户,其UID 为0 系统用户 保障系统运行的用户,一般不提供密码登录系统,其UID为1-499之间 普通用户 即一般用户,其使用系统的权限受限,其UID为500-60000之间. 与Linux用户信息相关的文件有两个：分别是/etc/passwd和 /etc/shadow 123456789101112# 查看文件/etc/passwd文件的内容，选取第一行：root:x:0:0:root:/root:/bin/bashroot:用户名x:密码占位符，密码保存在shadow文件内0:用户id，UID0:组id，GIDroot:注释信息/root:用户家目录/bin/bash:用户默认使用shell登录名:加密口令:最后一次修改时间:最小时间间隔:最大时间间隔:警告时间:不活动时间:失效时间:标志 Linux用户组类型 用户组类型 描述 系统组 一般加入一些系统用户 普通用户组 可以加入多个用户 私有组/基本组 当创建用户时,如果没有为其指明所属组，则就为其定义一个私有的用户组，起名称与用户名同名，当把其他用户加入到该组中，则其就变成了普通组 与Linux用户组信息相关的文件有两个：分别是/etc/group和 /etc/gshadow 12345# 查看文件/etc/group文件内容，选取一个普通组行：hadoop:x:500:hadoop:组名x:组密码占位符500:组id 2. 用户操作Linux中的用户管理主要涉及到用户账号的添加、删除和修改。所有操作都影响/etc/passwd中的文件内容 选项12345678910111213-c&lt;备注&gt;：修改用户帐号的备注文字；-d&lt;登入目录&gt;：修改用户登入时的目录；-e&lt;有效期限&gt;：修改帐号的有效期限；-f&lt;缓冲天数&gt;：修改在密码过期后多少天即关闭该帐号；-g&lt;群组&gt;：修改用户所属的群组；-G&lt;群组&gt;；修改用户所属的附加群组；-l&lt;帐号名称&gt;：修改用户帐号名称；-L：锁定用户密码，使密码无效；-U:解除密码锁定。-s&lt;shell&gt;：修改用户登入后所使用的shell；-u&lt;uid&gt;：修改用户ID； 参数 登录名：指定要修改信息的用户登录名。 添加用户 useradd spark usermod -G bigdata spark ## 设置组, spark用户添加到bigdata组中 usermod -c “mylove spark” spark ## -c: 添加备注信息 一步完成：useradd -G bigdata -c “mylove” spark # spark用户添加到bigdata组中, 并设置备注为 mylover useradd -u 508 -g 514 -G 1001 user3 添加用户user3时, 指定用户uid 和 主组-g, 附属组-G, 组必须要先存在 useradd -u(添加的时候修改用户id) 508 -g 514(-g是添加组, 此组号码必须存在) 指定user4 的家目录 useradd -u 520 -g 1000 -d /home/user44 user4 1234567891011121314151617&gt; &gt; 不创建user4的家目录, user5刚创建的时候, 家就被毁了&gt; &gt; `useradd -u 508 -g 1000 -M user6&gt; &gt; &gt; &gt; 当user6没有家的时候, 这样显示&gt; &gt; `bash-4.1$ ....&gt; &gt; 此时应该这样操作&gt; &gt; cp -v /etc/skel/.b* /home/user6&gt; &gt; "/etc/skel/.bash_logout" -&gt; "/home/user6/.bash_logout"&gt; &gt; "/etc/skel/.bash_profile" -&gt; "/home/user6/.bash_profile"&gt; &gt; "/etc/skel/.bashrc" -&gt; "/home/user6/.bashrc"&gt; &gt; su user6&gt; &gt; [user6@cts1 shixuanji]$ &gt; &gt; &gt; &gt; --------&gt; &gt; 查看进程 ps -au | grep xxx&gt; &gt; 杀死进程 kill -9 xxx&gt; &gt; 123456&gt; &gt; # 不允许用户登录&gt; &gt; useradd -u 523 -g 1000 -s /sbin/nologin user7&gt; &gt; &gt; &gt; # 修改此用户登录权限为 /bin/bash&gt; &gt; usermod -s /bin/bash user7&gt; &gt; 设置密码 passwd spark 根据提示设置密码即可 创建密码工具 mkpasswd 下载： yum -y install expect mkpasswd -l 14 创建一个14位数的密码 mkpasswd -l 15 -s 0: -s是设置特殊符号的长度, 这里是不要特殊符号 123456789101112131415161718192021222324&gt; ### 修改密码&gt; # 此时只需要输入一遍密码&gt; passwd --stdin user4 &gt; &gt; # 通过管道可以直接更改&gt; echo "123" | passwd --stdin user4&gt; &gt; # 通过手动输入回车符 `\n` 来实现2遍确认 -e：激活转义字符。&gt; echo -e "123\n123\n" | passwd user4&gt; &gt; =======#额外知识点&gt; # 使用echo -e选项时，若字符串中出现以下字符，则特别加以处理，而不会将它当成一般文字输出：&gt; \a 发出警告声；&gt; \b 删除前一个字符；&gt; \c 最后不加上换行符号；&gt; \f 换行但光标仍旧停留在原来的位置；&gt; \n 换行且光标移至行首；&gt; \r 光标移至行首，但不换行；&gt; \t 插入tab；&gt; \v 与\f相同；&gt; \ 插入\字符；&gt; \nnn 插入nnn（八进制）所代表的ASCII字符；&gt; &gt; 修改用户 修改spark登录名为storm：usermod -l spark storm 将spark添加到bigdata和root组：usermod -G root,bigdata spark 查看spark的组信息：groups spark 1234567891011121314&gt; # usermod : usermodify&gt; # 使用的参数跟useradd 几乎一样&gt; &gt; chfn user4 : 修改用户的finger, 电话, 办公室等...&gt; # 修改后, 其实这里就是修改的备注选项&gt; $ ☞ grep shixuanji /etc/passwd&gt; shixuanji:x:500:500:airpoet,sh,110,110:/home/shixuanji:/bin/bash&gt; &gt; # 修改备注, 这里又把之前改chfn又改了&gt; $ ☞ usermod -c "哎呀我去" shixuanji&gt; $ ☞ grep shixuanji /etc/passwd &gt; shixuanji:x:500:500:哎呀我去:/home/shixuanji:/bin/bash&gt; &gt; 删除用户 userdel -r spark 加一个-r就表示把用户及用户的家目录, 信箱地址等都删除 查看用户的信息 id 用户名 root用户以其他用户的权限执行某条命令 123[~] su -c "touch /tmp/shixuanji.txt" - shixuanji 22:32:02[~] ls -l /tmp/shixuanji.txt 22:32:25-rw-rw-r--. 1 shixuanji shixuanji 0 5月 29 22:32 /tmp/shixuanji.txt 3. 组操作前面我们知道，组是权限的集合。在linux系统中，每个用户都有一个用户组，没有指定时都默认为私有组，私有组名同用户名一致，建立用户组的好处是系统能对一个用户组中的所有用户的操作权限进行集中管理。组管理涉及组的添加、删除和修改。组的增加、删除和修改实际上就对/etc/group文件的更新 添加一个叫bigdata的组 groupadd bigdata 查看系统当前有那些组 cat /etc/group 将spark用户添加到bigdata组中 usermod -G bigdata spark 或者 gpasswd -a spark bigdata 这两个命令的区分记忆技巧： 命令是什么，就证明对什么做操作，所以最后的参数就是命令的操作对象，中间的可选项表示要干嘛 将spark用户从bigdata组删除 gpasswd -d spark bigdata 将bigdata组名修改为bigspark groupmod -n bigspark bigdata 删除组 groupdel bigdata 4. 为用户配置sudoer权限普通情况下，使用普通用户进行一些简单的操作就OK，但是普通用户和root用户的区别就在于root用户能对系统做任何事，但是普通用户就不行。处处受限。那么假如在某些情况下，普通用户想拥有更大的权限做更多的事情，虽然有权限限制，但也不是不可以。部分操作还是可以赋予更高的权限让普通用户做一次。这就需要给普通用户配置root权限了。意思就是让普通用户使用root权限去做一些操作，这当然是需要配置的。 用root编辑 vi /etc/sudoers 直接用命令 visudo也可以直接编辑此文件 在文件的如下位置，为hadoop添加一行即可 root ALL=(ALL) ALL hadoop ALL=(ALL) ALL spark ALL=(ALL) ALL 然后，hadoop用户和spark用户就可以用sudo来执行系统级别的指令 [hadoop\@hadoop01 ~]\$ sudo useradd huangxiaoming 123456789&gt; ~ ➤ su shixuanji # 修改了配置文件之后, 用sudo可以访问root&gt; [shixuanji@cts1 root]$ sudo /bin/ls /root&gt; [sudo] password for shixuanji:&gt; anaconda-ks.cfg date.txt Documents git-2018-05-22 install.log.syslog Music Public Videos&gt; autojump Desktop Downloads install.log k-vim Pictures Templates wget-log&gt; [shixuanji@cts1 root]$ ls /root&gt; ls: 无法打开目录/root: 权限不够&gt; [shixuanji@cts1 root]$&gt; 让一个普通用户可以无密码登录到root用户, 还是修改配置文件 /etc/sudoers` 禁止远程登录 修改 vi /etc/ssh/sshd_config 此文件中的 #PermitRootLogin yes 为 no 这样就不能远程 ssh 登录了 查看系统日志 查看登录错误之类的日志 tail /var log/secure 有时候, 登录时, 要过几秒才让输入登录密码 修改 /etc/ssh/sshd_config中的UseDNS=no, 就不会去找DNS了 普通用户免密使用sudo &amp; 免密登录为root12345678vi /ect/sudoers# 设置不需要使用密码使用sudoap ALL=(ALL) NOPASSWD: ALL # 使用 sudo su - 免密登录到root账户User_Alias USER_SU=apCmnd_Alias SU=/bin/suUSER_SU ALL=(ALL) NOPASSWD: SU 5. 切换用户在linux的系统使用过程当中，免不了会有多个用户来回切换使用。 所以在此提供切换用户的使用操作：切换用户使用的命令是 su（switch user） 从普通用户切换到root用户 , 需要输密码 su root 或 su 从root用户切换到普通用户, 不需要输密码 su xxx 5. 文件权限1、linux文件权限的描述格式解读Linux用 户分为：拥有者、组群(Group)、其他（other），Linux系统中，预设的情況下，系统中所有的帐号与一般身份使用者，以及root的相关信 息， 都是记录在/etc/passwd文件中。每个人的密码则是记录在/etc/shadow文件下。 此外，所有的组群名称记录在/etc/group內 linux文件的用户权限的分析图 123456 -rw-r--r-- 1 user staff 651 Oct 12 12:53 .gitmodules# ↑╰┬╯╰┬╯╰┬╯# ┆ ┆ ┆ ╰┈ 0 其他人# ┆ ┆ ╰┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈ g 属组# ┆ ╰┈┈┈┈ u 属主# ╰┈┈ 第一个字母 `d` 代表目录，`-` 代表普通文件 d：标识节点类型（d：文件夹 -：文件 l：链接） r：可读 w：可写 x：可执行 文件 文件夹 r 可读取内容 可以ls w 可修改文件的内容 可以在其中创建或者删除子节点 x 能否运行这个文件 能否cd**进入这个目录** 2、 修改文件权限chmod命令 用来变更文件或目录的权限。在UNIX系统家族里，文件或目录权限的控制分别以读取、写入、执行3种一般权限来区分，另有3种特殊权限可供运用。用户可以使用chmod指令去变更文件与目录的权限，设置方式采用文字或数字代号皆可。符号连接的权限无法变更，如果用户对符号连接修改权限，其改变会作用在被连接的原始文件。 语法12345chmod(选项)(参数)r=读取属性 //值＝4 w=写入属性 //值＝2 x=执行属性 //值＝1 选项： u User，即文件或目录的拥有者；g Group，即文件或目录的所属群组；o Other，除了文件或目录拥有者或所属群组之外，其他用户皆属于这个范围；a All，即全部的用户，包含拥有者，所属群组以及其他用户；r 读取权限，数字代号为“4”;w 写入权限，数字代号为“2”；x 执行或切换权限，数字代号为“1”；- 不具任何权限，数字代号为“0”；s 特殊功能说明：变更文件或目录的权限。 参数: 权限模式：指定文件的权限模式；文件：要改变权限的文件。 示例变更 文件/目录 的权限 1234567891011# 直接修改chmod g-rw haha.dat ## 表示将haha.dat对所属组的rw权限取消chmod o-rw haha.dat ## 表示将haha.dat对其他人的rw权限取消chmod u+x haha.dat ## 表示将haha.dat对所属用户的权限增加xchmod a-x haha.dat ## 表示将haha.dat对所用户取消x权限# 也可以用数字的方式来修改权限(常用)chmod 664 haha.dat就会修改成 rw-rw-r--如果要将一个文件夹的所有内容权限统一修改，则可以-R参数chmod -R 770 aaa/ 变更 文件/目录 的拥有者 或 所属组 1234# &lt;只有root权限能执行&gt;chown angela aaa ## 改变所属用户chown :angela aaa ## 改变所属组chown angela:angela aaa/ ## 同时修改所属用户和所属组 6. 压缩打包gzipgzip是在Linux系统中经常使用的一个对文件进行压缩和解压缩的命令，既方便又好用。 gzip是个使用广泛的压缩程序，文件经它压缩过后，其名称后面会多处“.gz”扩展名。 语法 gzip (选项) (参数) 选项 123456789101112131415161718-a或——ascii：使用ASCII文字模式；-d或--decompress或----uncompress：解开压缩文件；-f或——force：强行压缩文件。不理会文件名称或硬连接是否存在以及该文件是否为符号连接；-h或——help：在线帮助；-l或——list：列出压缩文件的相关信息；-L或——license：显示版本与版权信息；-n或--no-name：压缩文件时，不保存原来的文件名称及时间戳记；-N或——name：压缩文件时，保存原来的文件名称及时间戳记；-q或——quiet：不显示警告信息；-r或——recursive：递归处理，将指定目录下的所有文件及子目录一并处理；-S或&lt;压缩字尾字符串&gt;或----suffix&lt;压缩字尾字符串&gt;：更改压缩字尾字符串；-t或——test：测试压缩文件是否正确无误；-v或——verbose：显示指令执行过程；-V或——version：显示版本信息；-&lt;压缩效率&gt;：压缩效率是一个介于1~9的数值，预设值为“6”，指定愈大的数值，压缩效率就会愈高；--best：此参数的效果和指定“-9”参数相同；--fast：此参数的效果和指定“-1”参数相同。-num 用指定的数字num调整压缩的速度，-1或--fast表示最快压缩方法（低压缩比），-9或--best表示最慢压缩方法（高压缩比）。系统缺省值为6。 参数 文件列表：指定要压缩的文件列表。 示例 12345678910111213# 把test6目录下的每个文件压缩成.gz文件gzip *# 把上例中每个压缩的文件解压，并列出详细的信息gzip -dv *# 详细显示例1中每个压缩的文件的信息，并不解压gzip -l *# 压缩一个tar备份文件，此时压缩文件的扩展名为.tar.gzgzip -r log.tar# 递归的压缩目录gzip -rv test6# 这样，所有test下面的文件都变成了.gz，目录依然存在只是目录里面的文件相应变成了.gz.这就是压缩，和打包不同。因为是对目录操作，所以需要加上-r选项，这样也可以对子目录进行递归了。# 递归地解压目录gzip -dr test6 bzipbzip2命令 用于创建和管理（包括解压缩）“.bz2”格式的压缩包。 语法 bzip2 (选项) (参数) 选项 123456789101112-c或——stdout：将压缩与解压缩的结果送到标准输出；-d或——decompress：执行解压缩；-f或-force：bzip2在压缩或解压缩时，若输出文件与现有文件同名，预设不会覆盖现有文件。若要覆盖。请使用此参数；-h或——help：在线帮助；-k或——keep：bzip2在压缩或解压缩后，会删除原始文件。若要保留原始文件，请使用此参数；-s或——small：降低程序执行时内存的使用量；-t或——test：测试.bz2压缩文件的完整性；-v或——verbose：压缩或解压缩文件时，显示详细的信息；-z或——compress：强制执行压缩；-V或——version：显示版本信息；--repetitive-best：若文件中有重复出现的资料时，可利用此参数提高压缩效果；--repetitive-fast：若文件中有重复出现的资料时，可利用此参数加快执行效果。 参数 文件：指定要压缩的文件。 示例 1234567891011## 压缩指定文件filename:bzip2 filename 或 bzip2 -z filename## 解压指定的文件filename.bz2:bzip2 -d filename.bz2 或 bunzip2 filename.bz2# 压缩解压的时候将结果也输出：bzip2 -v filename filename: 0.119:1, 67.200 bits/byte, -740.00% saved, 5 in, 42 out.# 压缩解压的时候，除了生成结果文件，将原来的文件也保存: bzip2 -k filename# 解压到标准输出, 输出文件的内容bzip2 -dc filename.bz2 tarLinux下的归档使用工具，用来打包和备份。 首先要弄清两个概念：打包和压缩。打包是指将一大堆文件或目录变成一个总的文件；压缩则是将一个大的文件通过一些压缩算法变成一个小文件。 为什么要区分这两个概念呢？这源于Linux中很多压缩程序只能针对一个文件进行压缩，这样当你想要压缩一大堆文件时，你得先将这一大堆文件先打成一个包（tar命令），然后再用压缩程序进行压缩（gzip bzip2命令）。 其实最简单的使用 tar 就只要记忆底下的方式即可： tar.gz 格式: 1234# 一次性打包并压缩、解压并解包打包并压缩： tar -zcvf [目标文件名].tar.gz [原文件名/目录名]解压并解包： tar -zxvf [原文件名].tar.gz -C [目标目录]注：z代表用gzip算法来压缩/解压。 tar.bz2格式: 1234# 一次性打包并压缩、解压并解包打包并压缩： tar -jcvf [目标文件名].tar.bz2 [原文件名/目录名]解压并解包： tar -jxvf [原文件名].tar.bz2注：小写j代表用bzip2算法来压缩/解压。 其实用不到的话就不用看了, 下面的. 语法 tar(选项)(参数) 选项 1234567891011121314151617181920212223-A或--catenate：新增文件到以存在的备份文件；-B：设置区块大小；-c或--create：建立新的备份文件；-C &lt;目录&gt;：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。-d：记录文件的差别；-x或--extract或--get：从备份文件中还原文件；-t或--list：列出备份文件的内容；-z或--gzip或--ungzip：通过gzip指令处理备份文件；-Z或--compress或--uncompress：通过compress指令处理备份文件；-f&lt;备份文件&gt;或--file=&lt;备份文件&gt;：指定备份文件；-v或--verbose：显示指令执行过程；-r：添加文件到已经压缩的文件；-u：添加改变了和现有的文件到已经存在的压缩文件；-j：支持bzip2解压文件；-v：显示操作过程；-l：文件系统边界设置；-k：保留原有文件不覆盖；-m：保留文件不被覆盖；-w：确认压缩文件的正确性；-p或--same-permissions：用原来的文件权限还原文件；-P或--absolute-names：文件名使用绝对名称，不移除文件名称前的“/”号；-N &lt;日期格式&gt; 或 --newer=&lt;日期时间&gt;：只将较指定日期更新的文件保存到备份文件里；--exclude=&lt;范本样式&gt;：排除符合范本样式的文件。 参数 文件或目录：指定要打包的文件或目录列表。 -f: 使用档案名字，切记，这个参数是最后一个参数，后面只能接档案名。 12345- z：有gzip属性的- j：有bz2属性的- Z：有compress属性的- v：显示所有过程- O：将文件解开到标准输出 实例 zip格式 123压缩： zip -r [目标文件名].zip [原文件/目录名]解压： unzip [原文件名].zip注： -r参数代表递归 tar格式（该格式仅仅打包，不压缩） 123打包：tar -cvf [目标文件名].tar [原文件名/目录名]解包：tar -xvf [原文件名].tar注：c参数代表create（创建），x参数代表extract（解包），v参数代表verbose（详细信息），f参数代表filename（文件名），所以f后必须接文件名。 tar.gz格式(方式二常用) 12345678# 方式一：利用前面已经打包好的tar文件，直接用压缩命令。压缩：gzip [原文件名].tar解压：gunzip [原文件名].tar.gz# 方式二：一次性打包并压缩、解压并解包打包并压缩： tar -zcvf [目标文件名].tar.gz [原文件名/目录名]解压并解包： tar -zxvf [原文件名].tar.gz注：z代表用gzip算法来压缩/解压。 tar.bz2格式 12345678方式一：利用已经打包好的tar文件，直接执行压缩命令：压缩：bzip2 [原文件名].tar解压：bunzip2 [原文件名].tar.bz2方式二：一次性打包并压缩、解压并解包打包并压缩： tar -jcvf [目标文件名].tar.bz2 [原文件名/目录名]解压并解包： tar -jxvf [原文件名].tar.bz2注：小写j代表用bzip2算法来压缩/解压。 jar格式 12345678910压缩：jar -cvf [目标文件名].jar [原文件名/目录名]解压：jar -xvf [原文件名].jar# 注：如果是打包的是Java类库，并且该类库中存在主类，那么需要写一个META-INF/MANIFEST.MF配置文件，内容如下：Manifest-Version: 1.0Created-By: 1.6.0_27 (Sun Microsystems Inc.)Main-class: the_name_of_the_main_class_should_be_put_here# 然后用如下命令打包：jar -cvfm [目标文件名].jar META-INF/MANIFEST.MF [原文件名/目录名] 这样以后就能用“java -jar [文件名].jar”命令直接运行主类中的public static void main方法了。 7z格式 12345压缩：7z a [目标文件名].7z [原文件名/目录名]解压：7z x [原文件名].7z注：这个7z解压命令支持rar格式，即：7z x [原文件名].rar 其它例子 参考网址 7. Linux开关机和重启 开机：开机键 关机：shutdown，halt，init 0，poweroff 重启：reboot，init 6 Shutdown命令详解： shutdown -h now ## 立刻关机 shutdown -h +10 ## 10分钟以后关机 shutdown -h 12:00:00 ##12点整的时候关机]]></content>
      <categories>
        <category>Linux</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习笔记-1]]></title>
    <url>%2F2018%2F05%2F26%2FLinux%2FLinux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1%2F</url>
    <content type="text"><![CDATA[1&gt; 初识Linux1. Linux介绍Linux系统是一套免费使用和自由传播的类UNIX操作系统（主要用在服务器上），是一个基于POSIX和UNIX的多用户、多任务、支持多线程和多CPU的操作系统。它能运行主要的UNIX工具软件、应用程序和网络协议。它支持32位和64位硬件。Linux继承了UNIX以网络为核心的设计思想，是一个性能稳定的多用户网络操作系统。 UNIX：操作系统，是美国AT&amp;T公司贝尔实验室于1969年完成的操作系统，最早由肯·汤普逊（Ken Thompson），丹尼斯·里奇（Dennis Ritchie）开发。在1971年首次对外发布，刚好在1971，丹尼斯·里奇（Dennis Ritchie）发明了C语言，后来在1973，Unix被他用C语言重新编写。Unix前身源自于MultiCS，叫UniCS，后来改名叫Unix。 POSIX：可移植操作系统接口（英语：Portable Operating System Interface of UNIX，缩写为POSIX），是IEEE（电气和电子工程师协会）为要在各种UNIX操作系统上运行软件，而定义API的一系列互相关联的标准的总称。 GNU：1983年，Richard Stallman（理查德·马修·斯托曼）创立GNU计划。一套完全自由的操作系统，其内容软件完全以GPL方式发布。这个操作系统是GNU计划的主要目标（发展出一套完整的开放源代码操作系统来取代Unix），名称来自GNU\’s Not Unix!的递归缩写。 GPL：一种GNU通用公共许可协议，为保证GNU软件可以自由的使用、复制、修改和发布，所有的GNU软件都有一份在禁止其他人添加任何限制的情况下授权所有权利给任何人的协议条款，是一个被广泛使用的自由软件许可协议条款，保证终端用户运行、学习、分享（复制）及编辑软件之自由，GPL是自由软件和开源软件的最流行许可证，特色表现： •取得软件与原始码：您可以根据自己的需求来执行这个自由软件 •复制：您可以自由的复制该软件 •修改：您可以将取得的原始码进行程序修改工作，使之适合您的工作 •再发行：您可以将您修改过的程序，再度的自由发行，而不会与原先的撰写者冲突 •回馈：您应该将您修改过的程序代码回馈于社群 ==不同许可证的区别== 1985年，Richard Stallman又创立了自由软件基金会（Free Software Foundation，FSF）来为GNU计划提供技术、法律以及财政支持。 1990年，GNU计划开发主要项目有Emacs（文本编辑器）、GCC（GUN Compiler Collection，GNU编译器集合）、Bash等，GCC是一套GNU开发的编程语言编译器。还有开发一些UNIX系统的程序库和工具。 Linux操作系统诞生于1991年10月5日（这是第一次正式向外公布时间），与UNIX兼容，并在GPL条款下发布。现在，Linux产生了许多不同的Linux发行版本，但它们都使用了Linux内核。Linux可安装在各种计算机硬件设备中，比如手机、平板电脑、路由器、视频游戏控制台、台式计算机、大型机和超级计算机。 1992年，Linux与其他GUN软件结合，完全自由的GNU/Linux操作系统正式诞生，简称Linux ==Linux的基本思想有两点== 第一，一切都是文件 第二，每个软件都有确定的用途 与Unix思想十分相近。 2. Linux特点1、分时的多用户、多任务操作系统 2、多数网络协议支持、方便的远程管理 3、强大的内存管理和文件系统管理 4、大量的可用的软件和免费的软件 5、优良的稳定性和安全性 6、良好的可移植性和灵活性 7、可供选择的厂商多 3. Linux操作系统架构 补充：linux内核必须加上一个”界面”软件，才能让用户去使用，”界面”分两类： a、命令行界面CLI SHELL（有很多种，最流行的一种是bash shell） b、图形界面GUI SHELL（也有很多种，目前最流行的有两种：gnome和kde） 4. Linux内核严格来讲，Linux不是一个操作系统，Linux只是一个操作系统中的内核。 内核建立了计算机软件与硬件之间通讯的平台，内核提供系统服务，比如文件管理、虚拟内存、设备I/O、进程管理等。 内核官网：[http://www.kernel.org/]{.underline}。目前最新的内核版本：4.13.5 要注意区分linux发型版本和linux内核版本。两者不是同一个事物 下面这位是Linux内核的作者： 这是Linux的logo 5. 常见发行版红帽企业系统（RedHat Enterprise Linux, RHEL） 全球最大的开源技术厂商，全世界内使用最广泛的Linux发布套件， 提供性能与稳定性极强的Linux套件系统并拥有完善的全球技术支持。 官网：[http://www.redhat.com]{.underline} 社区企业操作系统（CentOS） 最初是将红帽企业系统”重新编译/发布”给用户免费使用而广泛使用， 当前已正式加入红帽公司并继续保持免费（随RHEL更新而更新）。 官网：[http://www.centos.org/]{.underline} 红帽用户桌面版（Fedora [Linux]） 最初由红帽公司发起的桌面版系统套件（目前已经不限于桌面版）， 用户可免费体验到最新的技术或工具，而功能成熟后加入到RHEL中。 官网：[http://fedora.redhat.com]{.underline} 国际化组织的开源操作系统（Debian） 提供超过37500种不同的自由软件且拥有很高的认可度， 对于各类内核架构支持性良好，稳定性、安全性强更有免费的技术支持。 官网：[http://www.debian.org/]{.underline} 基于Debian的桌面版（Ubuntu） Ubuntu是一款基于Debian派生的产品，对新款硬件具有极强的兼容能力。 普遍认为Ubuntu与Fedora都是极其出色的LINUX桌面系统。 官网：[http://www.ubuntulinux.org/]{.underline} 当然还有国内的国防科技大学发行麒麟kylin和中科院发行红旗RedFlag 2&gt; Linux文件系统1. CentOS的目录结构 2. 根目录下每个目录的简单解释 /**：**根目录，一般根目录下只存放目录，不要存放文件，/etc、/bin、/dev、/lib、/sbin应该和根目录放置在一个分区中 /bin:/usr/bin: 可执行二进制文件的目录，如常用的命令ls、tar、mv、cat等 /boot**：**放置linux系统启动时用到的一些文件。/boot/vmlinuz为linux的内核文件，以及/boot/gurb。建议单独分区，分区大小100M即可 /dev**：**存放linux系统下的设备文件，访问该目录下某个文件，相当于访问某个设备，常用的是挂载光驱mount /dev/cdrom /mnt /etc**：**系统配置文件存放的目录，不建议在此目录下存放可执行文件，重要的配置文件有/etc/inittab、/etc/gateways、/etc/resolv.conf、/etc/fstab、/etc/init.d、/etc/X11、/etc/sysconfig、/etc/xinetd.d修改配置文件之前记得备份。注：/etc/X11存放与x windows有关的设置 /home**：**系统默认的用户家目录，新增用户账号时，用户的家目录都存放在此目录下，表示当前用户的家目录，test表示用户test的家目录。建议单独分区，并设置较大的磁盘空间，方便用户存放数据 /lib:/lib64:/usr/lib:/usr/local/lib**：**系统使用的函数库的目录，程序在执行过程中，需要调用一些额外的参数时需要函数库的协助，比较重要的目录为/lib/modules /lost+fount**：**系统异常产生错误时，会将一些遗失的片段放置于此目录下，通常这个目录会自动出现在装置目录下。如加载硬盘于/disk 中，此目录下就会自动产生目录/disk/lost+found /mnt:/media**：**光盘默认挂载点，通常光盘挂载于/mnt/cdrom下，也不一定，可以选择任意位置进行挂载 /opt**：**给主机额外安装软件所摆放的目录。如：FC4使用的Fedora 社群开发软件，如果想要自行安装新的KDE桌面软件，可以将该软件安装在该目录下。以前的Linux系统中，习惯放置在 /usr/local目录下option /proc**：*此目录的数据都在内存中，如系统核心，外部设备，网络状态，由于数据都存放于内存中，所以不占用磁盘空间，比较重要的目录有/proc/cpuinfo、/proc/interrupts、/proc/dma、/proc/ioports、/proc/net/等process /root**：**系统管理员root的家目录，系统第一个启动的分区为/，所以最好将/root和/放置在一个分区下 /sbin:/usr/sbin:/usr/local/sbin**：**放置系统管理员使用的可执行命令，如fdisk、shutdown、mount等。与/bin不同的是，这几个目录是给系统管理员root使用的命令，一般用户只能”查看”而不能设置和使用。 /selinux**：**selinux软件目录，用于保证系统安全 /srv**：**服务启动之后需要访问的数据目录，如www服务需要访问的网页数据存放在/srv/www内service /sys**：**类似于/proc的特殊文件系统，存放内核数据信息 /tmp**：**一般用户或正在执行的程序临时存放文件的目录,任何人都可以访问,重要数据不可放置在此目录下 /usr**：应用程序存放目录， /usr/bin 存放应用程序 /usr/share 存放共享数据 /usr/lib 存放不能直接运行的，却是许多程序运行所必需的一些函数库文件 /usr/local:存放软件升级包 /usr/share/doc: 系统说明文件存放目录 /usr/share/man: 程序说明文件存放目录，使用man ls时会查询/usr/share/man/man1/ls.1.gz的内容建议单独分区，设置较大的磁盘空间 usr**：user share resources/unix share resouces /var**：**放置系统执行过程中经常变化的文件，如： /var/log：随时更改的日志文件 /var/log/message：所有的登录文件存放目录 /var/spool/mail：邮件存放的目录 /var/run：程序或服务启动 使用建议： 用户应该将文件存储在自己的主目录及其子目录下 系统绝大多数设置都在/etc目录下 不要修改/或者/usr目录下的任何内容，除非你真的清楚你在做什么，也就是说/目录最好和安装好系统之初保持一致 大多数工具和应用软件程序都安装在/bin，/sbin，/usr/bin，/usr/sbin，/usr/local/bin 文件或者目录都有唯一的绝对路径，没有盘符的概念 3&gt; Linux命令终端1. Linux 的命令格式：命令选项 命令参数注意：三者之间要空格隔开，其中命令选项分为长格式和短格式。 短格式用’-‘表示，比如：-l， 长格式用”–”表示，比如：–help， 也可以使用组合格式，比如：-a -l 等价于-la或者-al 2. Linux的默认命令提示符：#：管理员用户 $：普通用户 PS: Linux以回车键表示命令结束，如果 linux命令需要折行输入，那么可以以 \表示每行结束 4&gt; 常用命令归纳分类基本命令 文件管理 mkdir, rmdir, mv, rm, cp, touch, cat, tac, echo, more, less, head, tail, file, find, rename, ln, pwd, scp, alias 磁盘管理 ls, cd, du, df, mount, unmounts, fdisk 文档处理 wc, sort, uniq, cut, sed, awk, grep, vi, diff 用户和组 useradd, usermod, passwd, userdel, groupadd, groupdel, chgrp, su 文件传输 get, put, wget 网络通信 telnet, nc, ifconfig, ping, netstat, ip, host 备份压缩 gzip, bzip2, bunzip2, tar, zip 系统管理 exit, kill, last, ps, top, free, pstree, reboot, halt, shutdown, sudo, who, w, whoami, whereis, which, last, whatis 系统设置 clear, set, unset, hwclock, time, date, 其他 history, hostname, nohup, service, init, rpm, ssh, cal, yum 网站速查http://man.linuxde.net/ http://www.jb51.net/linux/ https://jaywcjlove.github.io/linux-command ps: 直接在命令行中, 用 man xx, 也可以查看命令信息 5&gt; 常用文件系统命令详解磁盘管理 cd change directory 常使用方式： cd sourcedir 进入用户主目录 cd ~ 进入用户主目录 cd - 返回进入此目录之前所在的目录 cd .. 返回上级目录（若当前目录为”/“，则执行完后还在”/“；”..”为上级目录的意思） cd ../.. 返回上两级目录 pwd print working directory pwd 显示当前工作目录 ls list：显示目录内容列表 使用格式：ls 选项 目录或文件名 常用选项 -l：详细信息显示 -a：显示所有子目录和文件的信息，包括隐藏文件 -A：类似于“-a”，但不显示“.”和“..”目录的信息 -R：递归显示内容 -h：以友好方式显示文件大小 例子： ls -l ## 列出文件详细信息， 也可以写作 ll ls -lah ## 以友好方式显示包括隐藏文件的详细信息 du disk usage：显示每个文件和目录的磁盘使用空间 使用格式：du 选项 目录或文件名 常用选项： -a：统计时包括所有的文件，而不仅仅只统计目录 -h：以更易读的字节单位（K、M等）显示信息 -s：只统计每个参数所占用空间总的大小 例子： du -ah ## df disk free：显示磁盘相关信息 常用选项： -h：以更易读的字节单位（K、M等）显示信息 -T：显示分区格式 例子： df -h ## 显示磁盘信息，以友好方式 df -T -h ## 以友好格式显示磁盘信息，并且附加磁盘格式 文件管理 touch 创建空文件 或更新文件时间标记 使用格式： touch 文件名 file 查看文件类型 使用格式： file 文件名 根据文件内容格式判断文件类型。而不是根据后缀名 mkdir 创建文件夹 make directory 使用格式：mkdir 选项 参数 常用选项： -p：已级联的方式创建文件夹 例子： mkdir -p /root/ma/niu/zhu/dagou ## 上级目录不存在自动创建上一级目录，常用 cp 复制文件 使用格式： cp 选项 源文件或目录… 目标文件或目录 常用选项： -r：递归复制整个目录树 -p：保持源文件的属性不变 -i：需要覆盖文件或目录时进行提醒 rm 删除文件或目录 使用格式：rm [选项] 文件或目录 常用选项： -f：强行删除文件或目录，不进行提醒 -i：删除文件或目录时提醒用户确认 -r：递归删除整个目录树 例子： rm -rf /root/ma/ ## 不提醒递归删除整个目录，慎用慎用慎用 mv 移动文件 如果与源文件位置一样，则相当于重命名 使用格式： mv [选项]… 源文件或目录… 目标文件或目录 常用选项： -f：若目标文件或目录与现有的文件或目录重复，则直接覆盖现有的文件或目录 -u：当源文件比目标文件新或者目标文件不存在时，才执行移动操作 rmdir 删除空文件夹 常用选项： -p或–parents：删除指定目录后，若该目录的上层目录已变成空目录，则将其一并删除； rmdir -p /root/aa/bb/cc/dd/ee 删除文件夹ee，如果删除ee后，dd变为空，则删除dd，依次类推 cat 连接文件并打印到标准输出设备上 常用选项： -n或–number：由1开始对所有输出的行数编号 cat /home/hadoop/data.txt ## 查看文件内容 tac 倒序输出文件内容 tac /home/hadoop/data.txt echo 输出指定的字符串或者变量 常用选项： -e：若字符串中出现以下字符，则特别加以处理，而不会将它当成一般文字输出： \a 发出警告声； \b 删除前一个字符； \c 最后不加上换行符号； \f 换行但光标仍旧停留在原来的位置； \n 换行且光标移至行首； \r 光标移至行首，但不换行； \t 插入tab； \v 与\f相同； \ 插入\字符； \nnn 插入nnn（八进制）所代表的ASCII字符； 例子： echo ‘ma’ ## 输出ma echo -e ‘ma\n’ ## 打印ma之后换行 echo -ne ‘ma’ ‘zhonghua’ ## 打印完不换行 echo ‘ma’ &gt; ma.dat ## 覆盖 echo ‘ma’ &gt;&gt; ma.dat ## 追加 echo $PWD ## 输出变量内容 head 在屏幕上显示指定文件的开头若干行 默认显示10行 常用选项： -n&lt;数字&gt;：指定显示头部内容的行数； 例子： head -n 5 install.log ## 显示该文件前五行内容 tail 在屏幕上显示指定文件的末尾若干行 常用选项： -f：显示文件最新追加的内容 tail -f install.log ## 显示最新追加的内容 ## 显示文件file的最后10行 tail -1 file ## 显示文件file最后一行的内容 tail -c 10 file ## 显示文件file的最后10个字符 more 显示文件内容，每次显示一屏 使用方式： 按Space键：显示文本的下一屏内容。 按Enter键：只显示文本的下一行内容。 按h键：显示帮助屏，该屏上有相关的帮助信息。 按b键：显示上一屏内容。 按q键：退出more命令。 less 分屏上下翻页浏览文件内容 和more使用方式基本类似 按e键：向上滚动一行 按y键：向下滚动一行 G：跳到文件末尾 gg：跳到文件首行 ln 用来为文件创件连接 软链接 -s 和 硬链接 ln /mnt/cdrom1 /var/www/html/centos/ —&gt;硬链接 ln -s /mnt/cdrom2 /var/www/html/centos/ –&gt;软链接-符号链接 alias 别名 查看别名：alias 定义别名：alias la=&#39;ll -a 取消别名：unalias la 软/硬 链接相关软链接, ln -s 文件/文件夹 产生的链接 创建文件的软链接 1ln -s /tmp/yum.log /root/yuntest 创建文件夹的软链接 1234# 后面的链接不指定名字, 就默认用前面的源文件/文件夹名字ln -s /tmp /root ---lrwxrwxrwx. 1 root root 4 5月 29 20:53 tmp -&gt; /tmp 如果软链接是指向目录的话, 是可以直接cd进去的, cd进去的是真实的目录! 12pwd -P: 查看软链接的真实指向某目录(物理目录)pwd -L: 逻辑目录 硬链接, ln 不加s, 一般不会用 不能链接目录 不能跨分区做硬链接 其它有用的命令123456789101112131415161718man：显示命令帮助信息clear：清屏，或者按ctrl + l也行ctrl + c：退出当前进程ctrl + z：挂起当前前台进程whatis：命令是什么whereis：在标准路径下搜索与名称相关的文件，whereis将所有搜索到的文件都显示which：which在设定的搜索路径下进行目录搜索，只显示搜索到的第一个文件su：切换用户history：显示历史命令hostname：显示主机名set：查看系统变量get：下载文件put：上传文件sudo：以root用户权限执行一次命令exit：退出登录状态w：显示当前连接的用户who：显示当前会话信息uptime：查看系统运行时间 使用小技巧(重点)12345678910111213141516ctrl + u：清除光标前的命令，相当于剪切 # !ctrl + k：清除光标后的命令，相当于剪切 # !ctrl + y：粘贴 # !ctrl + t：把光标前面的那个字符往后挪动一位ctrl + l：清屏ctrl + a: 移到命令行首 # !ctrl + e: 移到命令行尾 # !ctrl + ← →: 光标移动一个单词 # !!!：执行上一次命令 # !!$：上个命令的最后一个单词ctrl + w：删除光标前一个单词cd data; cat sed.txt：表示先执行cd，然后执行cat，工作目录会切换(cd data; cat sed.txt)：跟上个命令相比，不切换工作目录|：管道符，表示把前面命令内容的输出当做后面命令的输入&gt;：表示内容覆盖&gt;&gt;：表示内容追加]]></content>
      <categories>
        <category>Linux</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习笔记-3]]></title>
    <url>%2F2018%2F05%2F25%2FLinux%2FLinux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3%2F</url>
    <content type="text"><![CDATA[1. 系统服务管理 检查本机httpd服务是否开启, 使用命令service httpd status 开启/关闭/重启 httpd服务 service httpd start/stop/restart 查看所有的服务状态 service --status-all 过滤出某个服务service –status-all | grep httpd` 防火墙服务 sevice iptables status/stop/start/restart 配置后台服务进程开机自启 12345# 开启开机自启http进程-&gt; chkconfig httpd on# 查看httpd的开机自启状态-&gt; chkconfig --list | grep httpdhttpd 0:关闭 1:关闭 2:启用 3:启用 4:启用 5:启用 6:关闭 缺省系统运行级别 0 为停机，机器关闭。 1 为单用户模式，就像Win9x下的安全模式类似。 2 为多用户模式，但是没有NFS支持。 3 为完整的多用户模式，是标准的运行级。 4 一般不用，在一些特殊情况下可以用它来做一些事情。例如在笔记本 电脑的电池用尽时，可以切换到这个模式来做一些设置。 5 就是X11，进到X Window系统了。 6 为重启，运行init 6机器就会重启。 Centos中时区 当前正在使用的时d区文件位于 /etc/localtime 其他时区文件则位于 /usr/share/zoneinfo 中国时区使用 /usr/share/zoneinfo/Asia/Shanghai 更改时区 cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 如果没有 Asia/Shanghai 时区文件，请使用tzselect命令去生成时区文件，生成好的时区文件就在 /usr/share/zoneinfo目录下 修改系统时间 date : 直接查看时间 date -s : 手动设置时间 ntpdate time.windows.www : 同步时间服务器时间 2. 简单磁盘管理 df：列出文件系统的整体磁盘使用量 -h 人性化的列出文件的大小等 123-&gt; # df -h /dev/sr0Filesystem Size Used Avail Use% Mounted on/dev/sr0 3.7G 3.7G 0 100% /media/cdrom du：检查磁盘空间使用量 fdisk：用于磁盘分区 3. 文件的基本属性前缀的含义 1234567# 查看目录的权限-&gt; # ll -d /var/www/html/localyum ls -ld 也是一样, 查看目录的权限drwxr-xr-x. 7 root root 4096 5月 25 07:50 autojump-rw-r--r--. 1 root root 50434 5月 23 10:11 install.loglrwxrwxrwx. 1 root root 13 5月 25 21:04 localyum -&gt; /media/cdrom/ 当为[ d ]则是目录 当为[ - ]则是文件； 若是[ l ]则表示为链接文档(link file)； 若是[ b ]则表示为装置文件里面的可供储存的接口设备(可随机存取装置)； 若是[ c ]则表示为装置文件里面的串行端口设备，例如键盘、鼠标(一次性读取装置); r、w、x 对于文件和目录的含义 权限 对文件的含义 对目录的含义 r 读权限 可以查看文件内容 可以列出目录中的内容 w 写权限 可以修改文件内容 可以再目录中创建、删除文件 x 执行权限 可以执行文件 可以进入目录 4. 软件安装 TODO1. 二进制发布包安装 TODO:软件已经针对具体平台编译打包发布，只要解压，修改配置即可 安装jdk , 安装tomcat也一样 TODO: 通过 ftp工具把 jdk传到 linux服务器 创建一个 /var/www/html/soft/jdk8 的软链接, 指向服务器中的安装包 —— 这一步失败了, 先搁置把, 先copy过去. 2. 源码编译安装 TODO:软件以源码工程的形式发布，需要获取到源码工程后用相应开发工具进行编译打包部署 安装/卸载 redis 卸载: 首先查看redis-server是否启动 ps aux | grep redis 有的话, 关闭这些进程 kill -9 进程pid 删除redis相应的文件夹就可以了。 find / -name redis 安装: 拷贝到/usr/local, 解压, 删掉原安装包 12tar -zxvf redis-3.0.0.tar.gz rm redis-3.0.0.tar.gz 检查运行环境 123# 检测一下是否可以安装makemake test 安装到指定目录 1make PREFIX=/root/apps/redis install 拷贝配置信息 1cp /usr/local/redis-3.0.0/redis.conf /usr/local/redis/bin 启动 前端模式 bin/redis-server 后端模式启动 修改redis.conf配置文件，daemonize yes` 以后端模式启动 : TODO 3. RPM发布包软件已经按照RedHat（Redhat Package Manager）的包管理工具规范RPM进行打包发布，需要获取到相应的软件RPM发布包，然后用rpm命令进行安装 Mysql安装 1234567891011121314151617181920# rpm安装命令1、安装包：rpm -ivh 包名参数：-i ：安装的意思-v ：可视化-h ：显示安装进度另外在安装一个rpm包时常用的附带参数有：--force 强制安装，即使覆盖属于其他包的文件也要安装--nodeps 当要安装的rpm包依赖其他包时，即使其他包没有安装，也要安装这个包2、升级包：rpm -Uvh filename-U 升级3、卸载包rpm -e filename （这里的filename是通过rpm的查询功能所查询到的）4、查询一个包是否安装：rpm -q 包名（这里的包名，是不带有平台信息以及后缀名的）5.查询当前安装的所有rpm包：rpm -qa查询当前安装的和sql相关的包：rpm -qa | grep 'sql'查询sqlite安装路径：rpm -ql sqlite 5. 设置本地/网络yum源 首先检查虚拟机的 CD/DVD驱动器是否有挂载iso镜像文件 找到挂载源的位置 在 /dev/sr0下, 将其挂载到/mnt下创建的cdrom文件夹下 mount -t iso9660 -o ro /dev/sr0 /media/cdrom 配置开机挂载 , vi /etc/fstab, 增加一行 /dev/cdrom /media/cdrom iso9660 defaults 0 0 创建软连接, 设置可以通过web访问 ln -s /mnt/cdrom/ /var/www/html/yumsorurce 修改etc/yum.repos.d中的文件entOS-Media.repo中的enabled=1, 开启从本地寻找, 因为文件中, 原本就配置了baseurl: file:///media/cdrom/, 而我们自己创建了media/cdrom这个目录, 所以就可以从这里面读取了 执行yum repolist, 发现已经能读取出media中的repo了, 大功告成 c6-media CentOS-6 - Media 6,575 tips: 个人觉得还是配置个网络源比较好, 比如163/阿里的. 12345678910111213141516171819# 操作步骤# 1.备份原来的Base源cd /etc/yum.repos.d/mv CentOS-Base.repo CentOS-Base.repo_bak# 2. 下载网易/阿里源 到/etc/yum.repos.d下, 替换掉原本的# 网易源wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.163.com/.help/CentOS6-Base-163.repo# 阿里源wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repoyum clean allyum makecache# yum使用yum update 􏻦􏻧􏷉􏷊升级系统yum install -y xxx 直接安装, 不用确认􏻈yum update ~ 􏻦􏻧􏸻􏹺􏷜 升级指定软件包yum remove ~ 􏺌􏷎􏸻􏹺􏷜􏷕 卸载指定软件 挂载, 配置源的其他说明 12345678910111213141516171819202122232425# 关于挂载# lrwxrwxrwx. 1 root root 3 5月 25 16:47 cdrom -&gt; sr0cdrom其实是sr0的软链接, 因此直接找sr0即可# 挂载的基本语法mont -t iso9660 -o ro(只读) /dev/sr0 /mnt/cdrom (挂载类型) (挂载方式) (挂载源) (挂载点)# 卸载的语法umount /mnt/cdrom 如果卸载时遇到 umount: /mnt/cdrom: device is busy.解决方式: 1. 查找哪个进程在用: fuser /mnt/cdrom 2. 查找进程: ps -ef | grep 进程号 3. 插死进程: kill -9 进程号, 如果是root用户, 可能会断开连接, 需要重连 4. 然后继续 umount, 如果不行, 就强制卸载 umount -f /mnt/cdrom# 关于etc/yum.repos.d 中文件的说明 CentOS-Base.repo：有网的环境下默认使用这个，这个是第一优先级。因为没网，所以修改文件名，设置成备份文件。这样系统就会使用第二优先级的文件。 CentOS-Media.repo：没网的环境下使用这个，在上图中会发现他默认配置了4个路径，第4个yumsource是我自己加的。意思是说，如果系统检测yum使用了离线安装，那么会从上到下从这4个路径中查找安装软件。所以我们只要把光盘挂载在这四个目录下的任意一个目录即可。同时，该配置文件默认是不启用的，如果想使用需要修改倒数第二行的enabled为1，否则该文件无效。 # /mnt &amp; /media 目录的区别media：挂载一些移动设备，例如光盘，U盘等。mnt： 挂载一些硬盘等设备。所以我们的光盘应该挂载在media目录下，从yum给的默认配置文件也能看出。 6. 进程相关1. ps命令ps命令用于报告当前系统的进程状态。可以搭配kill指令随时中断、删除不必要的程序。 常用选项包括： 123456ps -1、-a显示所有用户的进程2、-u显示用户名和启动时间3、-x显示所有进程，包括没有控制终端的进程4、-e显示所有进程，包括没有控制终端的进程，较x选项，信息更为简略5、-l显示进程详细信息，按长格式显示 常用组合 ps -au 显示所有用户进程，并给出用户名和启动时间等详细信息 ps -aux 显示所有用户进程，包括没有控制终端的进程，并给出用户和和启动埋单等详细信息 ps -el 按长格式显示进程详细信息 1234567891011121314151617181920212223242526272829303132# 上述命令可能出现的字段含义USER: 进程所有者PID: 进程号PPID: 进程的父进程ID%CPU: CPU占用率C: 进程的CPU占用率%MEM: 内存占用率VSZ: 表示如果一个程序完全驻留在内存的话需要占用多少内存空间;RSS: 指明了当前实际占用了多少内存;TTY: 终端的次要装置号码 (minor device number of tty)F：进程的标志S：进程的状态STAT: 该进程程的状态，有以下值D: 不可中断的静止R: 正在执行中S: 静止状态T: 暂停执行Z: 不存在但暂时无法消除W: 没有足够的记忆体分页可分配&lt;: 高优先序的进程N: 低优先序的进程L: 有记忆体分页分配并锁在记忆体内PRI：进程的优先权NI：进程的Nice值ADDR：进程的地址空间SZ：进程占用内存的大小WCHAN：进程当前是否在运行TTY：进程所属终端START: 进程开始时间TIME: 执行的运行时间COMMAND：所执行的指令CMD：进程的命令 2. kill / pidof / pkill 命令有时候某个进程可能会长期占用CPU资源或无法正常执行或超出运行时间等，此时可能希望人工干预直接将进程杀死，这时候kill命令可以派上用场 12341、kill pid 直接杀死进程，但不能保证一定能杀死2、kill -9 pid 强制杀死进程3、pidof命令用于查看某个进程的进程号（例如：pidof mysqld）4、pkill命令可以按照进程名杀死进程。pkill和killall应用方法差不多，也是直接杀死运行中的程序；如果您想杀掉单个进程，请用kill来杀掉 3. 进程切换前台进程指的是进程在执行时会将命令行阻塞，直到进程执行完毕；后台进程指的是进程在执行时不会阻塞当前命令行，而是在系统后台执行 123451、ctrl + c 终止进程2、ctrl + z 挂起进程3、fg命令将进程转换到前台执行4、bg命令将进程转换到后台执行5、jobs命令查看任务 4. top命令top 命令可以定期显示所有正在运行和实际运行并且更新到列表中，它显示出 CPU 的使用、内存的使用、交换内存、缓存大小、缓冲区大小、过程控制、用户和更多命令。它也会显示内存和 CPU 使用率过高的正在运行的进程。 按q键退出查看. 5. pstree命令将进程间的关系以树结构的形式展示，能清楚看各进程之间的父子关系 12pstree ：以树状形式显示进程pstree -p ： 以树状形式显示进程，并且显示进程号 6. JPS命令JPS命令是JDK提供的一个检查系统是否启动了JVM进程的一个进程。不是linux系统自带的。主要任务就是用来检查java进程的。 7. 计划任务概念计划任务在Linux的体现主要分为at和crontab，其中： at：通过at命令安排任务在某一时刻执行一次 crontab：通过crontab 命令，我们可以在固定的间隔时间执行指定的系统指令或 shell script脚本。时间间隔的单位可以是分钟、小时、日、月、周及以上的任意组合。这个命令非常适合周期性的日志分析或数据备份等工作。 命令服务管理crontab在CentOS系统上，crontab服务的名称叫做crond 安装 yum -y install crontabs 服务操作说明 1234567891011service crond start #启动服务service crond stop #关闭服务service crond restart #重启服务service crond reload #重新载入配置service crond status #服务状态#查看crontab服务是否已设置为开机启动，执行命令：chkconfig --list#加入开机自动启动：chkconfig --level 35 crond on crontab功能使用 命令格式 123456789101112131415161718192021222324252627282930313233crontab [-u user] filecrontab [-u user] [ -e | -l | -r ]# crontab 参数说明：-u user：用来设定某个用户的crontab服务，例如，”-u ixdba”表示设定ixdba用户的crontab服务，此参数一般有root用户来运行。file：file是命令文件的名字,表示将file做为crontab的任务列表文件并载入crontab。-e：编辑某个用户的crontab文件内容。如果不指定用户，则表示编辑当前用户的crontab文件。-l：显示某个用户的crontab文件内容，如果不指定用户，则表示显示当前用户的crontab文件内容。-r：删除定时任务配置，从/var/spool/cron目录中删除某个用户的crontab文件，如果不指定用户，则默认删除当前用户的crontab文件。-i：在删除用户的crontab文件时给确认提示。# 命令示例：crontab file [-u user] ## 用指定的文件替代目前的crontab。 # 必须掌握：crontab -l [-u user] ## 列出用户目前的crontab. crontab -e [-u user] ## 编辑用户目前的crontab.# 通过crontab添加的计划任务都会存储在/var/spool/cron/目录里# 查看当前服务状态service crond status# 操作服务/sbin/service crond start //启动服务/sbin/service crond stop //关闭服务/sbin/service crond restart //重启服务/sbin/service crond reload //重新载入配置# 查看开机启动服务ntsysv # 退出时, 按tab切换# 加入开机自动启动chkconfig –level 35 crond on 配置说明 123456789101112131415161718# 基本格式 : * * * * * command 分 时 日 月 周 命令 # 每个小时的第几分钟执行该任务. 其它的类似第1列表示分钟1～59 每分钟用*或者 */1表示 第2列表示小时0～23（0表示0点） 7-9表示：8点到10点之间第3列表示日期1～31 第4列表示月份1～12 第5列标识号星期0～6（0表示星期天） 第6列要运行的命令# 记住几个特殊符号的含义:“*”代表取值范围内的数字,“/”代表”每”,“-”代表从某个数字到某个数字,“,”分开几个离散的数字 配置示例 123456789101112131415161718192021222324252627282930313233# * * * * * command # 分 时 日 月 周 命令 */1 * * * * date &gt;&gt; /root/date.txt上面的例子表示每分钟执行一次date命令可用 tail -f查看30 21 * * * /usr/local/etc/rc.d/httpd restart上面的例子表示每晚的21:30重启apache45 4 1,10,22 * * /usr/local/etc/rc.d/httpd restart上面的例子表示每月1、10、22日的4 : 45重启apache10 1 * * 6,0 /usr/local/etc/rc.d/httpd restart 上面的例子表示每周六、周日的1 : 10重启apache0,30 18-23 * * * /usr/local/etc/rc.d/httpd restart上面的例子表示在每天18 : 00至23 : 00之间每隔30分钟重启apache0 23 * * 6 /usr/local/etc/rc.d/httpd restart上面的例子表示每星期六的11 : 00 pm重启apache* */1 * * * /usr/local/etc/rc.d/httpd restart上面的例子每一小时重启apache* 23-7/1 * * * /usr/local/etc/rc.d/httpd restart上面的例子晚上11点到早上7点之间，每隔一小时重启apache0 11 4 * mon-wed /usr/local/etc/rc.d/httpd restart上面的例子每月的4号与每周一到周三的11点重启apache0 4 1 jan * /usr/local/etc/rc.d/httpd restart 上面的例子一月一号的4点重启apache 更详细的见这里 8. Linux虚拟主机集群测试环境基本搭建 注意点: 首次使用 NAT 模式装好CentOS之后, 使用ifconfig查看ip, 虚拟机是没有ip的, 需要手动开启ip服务, 命令是 dhclient, 如果已开启就不能再次开启 1. 第一台虚拟主机的静态ip配置(针对于mac环境) 执行ifconfig命令, 如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667shixuanji@x:~|⇒ ifconfiglo0: flags=8049&lt;UP,LOOPBACK,RUNNING,MULTICAST&gt; mtu 16384 options=1203&lt;RXCSUM,TXCSUM,TXSTATUS,SW_TIMESTAMP&gt; inet 127.0.0.1 netmask 0xff000000 inet6 ::1 prefixlen 128 inet6 fe80::1%lo0 prefixlen 64 scopeid 0x1 nd6 options=201&lt;PERFORMNUD,DAD&gt;gif0: flags=8010&lt;POINTOPOINT,MULTICAST&gt; mtu 1280stf0: flags=0&lt;&gt; mtu 1280EHC29: flags=0&lt;&gt; mtu 0EHC26: flags=0&lt;&gt; mtu 0XHC20: flags=0&lt;&gt; mtu 0en0: flags=8823&lt;UP,BROADCAST,SMART,SIMPLEX,MULTICAST&gt; mtu 1500 ether 60:03:08:a1:ac:ee nd6 options=201&lt;PERFORMNUD,DAD&gt; media: autoselect (&lt;unknown type&gt;) status: inactivep2p0: flags=8802&lt;BROADCAST,SIMPLEX,MULTICAST&gt; mtu 2304 ether 02:03:08:a1:ac:ee media: autoselect status: inactiveawdl0: flags=8902&lt;BROADCAST,PROMISC,SIMPLEX,MULTICAST&gt; mtu 1484 ether 72:25:b2:c8:2a:03 nd6 options=201&lt;PERFORMNUD,DAD&gt; media: autoselect status: inactiveen1: flags=8963&lt;UP,BROADCAST,SMART,RUNNING,PROMISC,SIMPLEX,MULTICAST&gt; mtu 1500 options=60&lt;TSO4,TSO6&gt; ether 32:00:1a:0d:12:00 media: autoselect &lt;full-duplex&gt; status: inactiveen2: flags=8963&lt;UP,BROADCAST,SMART,RUNNING,PROMISC,SIMPLEX,MULTICAST&gt; mtu 1500 options=60&lt;TSO4,TSO6&gt; ether 32:00:1a:0d:12:01 media: autoselect &lt;full-duplex&gt; status: inactivebridge0: flags=8822&lt;BROADCAST,SMART,SIMPLEX,MULTICAST&gt; mtu 1500 options=63&lt;RXCSUM,TXCSUM,TSO4,TSO6&gt; ether 32:00:1a:0d:12:00 Configuration: id 0:0:0:0:0:0 priority 0 hellotime 0 fwddelay 0 maxage 0 holdcnt 0 proto stp maxaddr 100 timeout 1200 root id 0:0:0:0:0:0 priority 0 ifcost 0 port 0 ipfilter disabled flags 0x2 member: en1 flags=3&lt;LEARNING,DISCOVER&gt; ifmaxaddr 0 port 10 priority 0 path cost 0 member: en2 flags=3&lt;LEARNING,DISCOVER&gt; ifmaxaddr 0 port 11 priority 0 path cost 0 media: &lt;unknown type&gt; status: inactiveutun0: flags=8051&lt;UP,POINTOPOINT,RUNNING,MULTICAST&gt; mtu 2000 inet6 fe80::2ed8:c28:27b2:f5d2%utun0 prefixlen 64 scopeid 0xd nd6 options=201&lt;PERFORMNUD,DAD&gt;vmnet1: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500 ether 00:50:56:c0:00:01 inet 172.16.63.1 netmask 0xffffff00 broadcast 172.16.63.255vmnet8: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500 ether 00:50:56:c0:00:08 inet 192.168.170.1 netmask 0xffffff00 broadcast 192.168.170.255en4: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500 options=3&lt;RXCSUM,TXCSUM&gt; ether 00:0e:c6:cc:ae:d7 inet6 fe80::1c60:7fe2:3947:4ec0%en4 prefixlen 64 secured scopeid 0x11 inet 192.168.63.148 netmask 0xffffff00 broadcast 192.168.63.255 nd6 options=201&lt;PERFORMNUD,DAD&gt; media: autoselect (100baseTX &lt;full-duplex,flow-control&gt;) status: active 找到最后的vmnet8, 其中的 inet 就是虚拟主机的网段, 配置虚拟主机的静态ip的时候, 就配置此网段内的. 广播地址 broadcast也是虚拟主机NAT的广播地址, 配置时可以不填 123vmnet8: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500 ether 00:50:56:c0:00:08 inet 192.168.170.1 netmask 0xffffff00 broadcast 192.168.170.255 修改虚拟机网卡配置vi /etc/sysconfig/network-scripts/ifcfg-eth0, 做如下配置 123456789101112DEVICE=eth0HWADDR=00:0C:29:D6:C7:0ETYPE=EthernetUUID=bebc1b63-4f20-405a-860a-32d0d8211582ONBOOT=yesNM_CONTROLLED=yesBOOTPROTO=static # ip类型IPADDR=192.168.170.6 # ip地址, 与 vmnet8 在同一网段NETMASK=255.255.255.0 # 子网掩码GATEWAY=192.168.170.2 # 网关, 跟 ip在同一网段DNS1=192.168.170.2 # 与 ip 同一网段 DNS2=8.8.8.8 # google的 dns 重启网络服务(这里重启也没啥用，等后面hostname改完了，重启一下) service network restart 2. 复制原本的虚拟主机 复制虚拟主机1到2 按照原虚拟主机的root用户名&amp;密码登录2 3. 修改新机网卡 修改网卡vi /etc/udev/rules.d/70-persistent-net.rules, 删除eth0所在的整个段落, 把下面的eth1改为eth0, 保存退出 修改网卡配置vi /etc/sysconfig/network-scripts/ifcfg-eth0, 如果有UUID, HARDDR, 删掉, IPADDR改为与原虚拟主机不同的地址, 但要在同一网段, BOOTPROTO改为static4. 修改新机hostname ​ 修改hostname, vi /etc/sysconfig/network, 把HOSTNAME改为要修改的名字 5.到这里配置完了，重启一下reboot 6. SSH免密登录原理 注意点：如果遇到需要添加新机器之类的，最好是先把 .ssh文件删掉，然后所有的机器一起操作看，全部都重新生成ssh-keygen， 全部重新发送 ssh-copy-id ，这样最简单，不易出错 在mac端，传ssh-copy-id时候，要制定用户名 ssh-copy-id ap@cs1；类似这样 新机生成自己公钥 ssh-keygen, 注意: 如果原本主机中已经生成, 此处在提示verwrite (y/n)?的时候要选择y, 才会重新生成覆盖 把新主机公钥发送给其它机器 ssh-copy-id root@xxx(其它主机ip), 此命令相当于 下面2条命令的效果 12cat id_rsa.pub &gt; authorized_keysscp -r authorized_keys root@192.168.123.202:/root/.ssh/ 其他主机也把公钥发给新主机, 此时就可以实现主机间的免密登录了. 7. 功能增强(可选) 可以在 每台机器中设置host别名, vi /etc/hosts, 加上xxx.xxx.xx.xxx cts1/2/3..., 这样在访问其他主机时, 可以直接用别名替代域名 如果是用的zsh的shell, 可以在所有主机的 &lt;sub&gt;.zshrc中, 添加alias login1=&#39;ssh root@cts1&#39; &gt;&gt; &lt;/sub&gt;/.zshrc, 这样可以直接 用 login1登录到对应的主机. 9. 安装, 使用 zsh &amp; oh-my-zsh &amp;相关插件主骨架安装&amp;介绍 安装zsh套件 1yum install zsh -y 安装 oh-my-zsh套件 1sh -c "$(wget https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)" zsh 的一些基本配置操作, 常用插件安装见这里! 12345678910111213141516171819202122# 查看oh-my-zsh 的主题ls ~/.oh-my-zsh/themes------#这里都是 .zshrc中的配置------# 可以更改为random, 这样会随机显示, 很有乐趣, 直接输入zsh也会切换vi ~/.zshrcZSH_THEME="random"# 添加plugin, 按照对应方式安装plugins=(git ... ... )# 添加alias 到 ~/.zshrcalias vi='vim'alias zshconfig='vi ~/.zshrc'alias vimconfig='vi ~/.vimrc'-----------------------# 设置当前用户使用zsh为默认的shellchsh -s /bin/zsh# 卸载 oh-my-zshuninstall_oh_my_zsh zsh zsh 的一些骚气操作 12345671. 兼容 bash, 这个就不用说了2. 输入某条命令, 比如 cat, 然后用上下键, 可以翻阅所有执行过的命令3. 各种补全, 输入任何命令, 按 2下 tab键, 下面会出现所有可能的补全, 可以 tab, 或 上下左右切换.4. 比如要杀掉进程java, 原来是需要 ps aux | grep java, 查进程的 PID，然后 kill PID; 现在只需要 kill java, 然后按下 tab, java会被替换为 对应的 PID, 点回车, kill !5. 目录浏览和跳转, 输入 d, 可以列出在这个回话中访问过的目录列表, 再输入列表前的序号, 即可直接跳转.6. 在当前目录下输入 .. 或 ... , 或者直接输入目录名, 都可以直接跳转, 甚至都不需要使用 cd命令了.7. 通配符搜索：ls -l */.sh，可以递归显示当前目录下的 shell 文件，文件少时可以代替 find，文件太多就歇菜了. zsh主题介绍, zsh插件介绍 zsh常用插件安装安装 zsh-autosuggestions1234567891011121314151617181920方式1: # 下载到本地git clone git://github.com/zsh-users/zsh-autosuggestions ~/.zsh/zsh-autosuggestions# 添加到.zshrc, 这样就不用每次source了添加 source ~/.zsh/zsh-autosuggestions/zsh-autosuggestions.zsh 到 .zshrc 尾部plugins=() 中添加上 zsh-autosuggestions, 用空格隔开即可======================方式2: 官方建议# 少了添加source到 ~/.zshrc这一步, 猜想是会按照默认的路径加载? ==&gt; 是的, 是可行的, 建议这个# 猜想, oh-my-zsh会自动对安装到 &lt;sub&gt;/.oh-my-zsh/custom&#125;/plugins/ 此路径下的插件source, 就不需要手动在 &lt;/sub&gt;/.zshrc中添加source了, 其它的插件就先不折腾了, 以后有机会再试试# 1.Clone this repository into $ZSH_CUSTOM/plugins (by default ~/.oh-my-zsh/custom/plugins)git clone https://github.com/zsh-users/zsh-autosuggestions $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/zsh-autosuggestions# 2.Add the plugin to the list of plugins for Oh My Zsh to load:plugins=(zsh-autosuggestions)# 3.Start a new terminal session. 安装 autojump12345678910111213# 下载到本地git clone git://github.com/joelthelion/autojump.git# 执行安装脚本cd autojump./install.py# 安装完成在~/下面有.autojump目录, 在.zshrc中加一句[[ -s &lt;sub&gt;/.autojump/etc/profile.d/autojump.sh ]] &amp;&amp; . &lt;/sub&gt;/.autojump/etc/profile.d/autojump.sh# 在plugins=(git zsh-autosuggestions autojump) 加上autojump, 与前者用空格隔开# 接下来可以愉悦的使用 j 了 安装 zsh-syntax-highlighting 123456789101112131415# 到 ~/.zshrc 目录, 克隆仓库# 这里默认的是主目录, 当然可以下载到其它目录# ps: 在那个目录, git clone就会下载到哪个目录git clone https://github.com/zsh-users/zsh-syntax-highlighting.git# source the script 到 ~/.zshrc# 这个在哪个目录下echo的, 就会把当前目录拼到前面?? 结果好像是这样echo "source $&#123;(q-)PWD&#125;/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh" &gt;&gt; $&#123;ZDOTDIR:-$HOME&#125;/.zshrc# 在plugins=(git zsh-autosuggestions autojump zsh-syntax-highlighting) zsh-syntax-highlighting, 与前者用空格隔开# 其实这里不加好像也没事, 还是加上为好# 在当前 shell生效# 看这个文件在哪个目录, 在哪个目录就source 哪个目录, 立即生效source ./zsh-syntax-highlighting/zsh-syntax-highlighting.zsh]]></content>
      <categories>
        <category>Linux</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>原创</tag>
        <tag>技术</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
</search>
