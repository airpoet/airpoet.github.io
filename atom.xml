<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>A.P的文艺杂谈</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://airpoet.github.io/"/>
  <updated>2018-06-12T15:18:29.177Z</updated>
  <id>https://airpoet.github.io/</id>
  
  <author>
    <name>airpoet</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>王石松-hadoop测试-1</title>
    <link href="https://airpoet.github.io/2018/06/12/Hadoop/Test/%E7%8E%8B%E7%9F%B3%E6%9D%BEhadoop%E6%B5%8B%E8%AF%95/"/>
    <id>https://airpoet.github.io/2018/06/12/Hadoop/Test/王石松hadoop测试/</id>
    <published>2018-06-12T10:40:25.949Z</published>
    <updated>2018-06-12T15:18:29.177Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第一题：简答题-（30分）"><a href="#第一题：简答题-（30分）" class="headerlink" title="第一题：简答题 （30分）"></a>第一题：简答题 （30分）</h1><h3 id="1、简述hdfs的读写流程"><a href="#1、简述hdfs的读写流程" class="headerlink" title="1、简述hdfs的读写流程"></a>1、简述hdfs的读写流程</h3><h4 id="读流程："><a href="#读流程：" class="headerlink" title="读流程："></a><strong>读流程：</strong></h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-12-104221.png" alt="image-20180612184221033"></p><ol><li>使用 HDFS 提供的客户端 Client，向远程的 namenode 发起 RPC 请求; </li><li>namenode 会视情况返回文件的全部 block 列表，对于每个 block，namenode 都会返回有 该 block 拷贝的 datanode 地址; </li><li>客户端 Client 会选取离客户端最近的 datanode 来读取 block;如果客户端本身就是 datanode， 那么将从本地直接获取数据; </li><li>读取完当前 block 的数据后，关闭当前的 datanode 链接，并为读取下一个 block 寻找最 佳的 datanode; </li><li>当读完列表 block 后，且文件读取还没有结束，客户端会继续向 namenode 获取下一批的 block 列表; </li><li>读取完一个 block 都会进行 checksum 验证，如果读取 datanode 时出现错误，客户端会 通知 namenode，然后再从下一个拥有该 block 拷贝的 datanode 继续读。 </li></ol><h4 id="写流程："><a href="#写流程：" class="headerlink" title="写流程："></a>写流程：</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-12-104620.png" alt="image-20180612184619400"></p><ol><li><p>使用 HDFS 提供的客户端 Client，向远程的 namenode 发起 RPC 请求</p></li><li><p>namenode 会检查要创建的文件是否已经存在，创建者是否有权限进行操作，成功则会为文件创建一个记录，否则会让客户端抛出异常;</p></li><li><p>当客户端开始写入文件的时候，客户端会将文件切分成多个 packets，并在内部以数据队列“data queue(数据队列)”的形式管理这些 packets，并向 namenode 申请 blocks，获 取用来存储 replicas 的合适的 datanode 列表，列表的大小根据 namenode 中 replication 的设定而定; </p></li><li><p>开始以 pipeline(管道)的形式将 packet 写入所有的 replicas 中。客户端把 packet 以流的 方式写入第一个 datanode，该 datanode 把该 packet 存储之后，再将其传递给在此 pipeline 中的下一个 datanode，直到最后一个 datanode，这种写数据的方式呈流水线的形式 </p></li><li><p>最后一个 datanode 成功存储之后会返回一个 ack packet(确认队列)，在 pipeline 里传递 至客户端，在客户端的开发库内部维护着”ack queue”，成功收到 datanode 返回的 ack packet 后会从”data queue”移除相应的 packet。 </p></li><li><p>如果传输过程中，有某个 datanode 出现了故障，那么当前的 pipeline 会被关闭，出现故 障的 datanode 会从当前的 pipeline 中移除，剩余的 block 会继续剩下的 datanode 中继续 以 pipeline 的形式传输，同时 namenode 会分配一个新的 datanode，保持 replicas 设定的 数量。 </p></li><li><p>客户端完成数据的写入后，会对数据流调用 close()方法，关闭数据流; </p></li><li><p>只要写入了 dfs.replication.min(最小写入成功的副本数)的复本数(默认为 1)，写操作 就会成功，并且这个块可以在集群中异步复制，直到达到其目标复本数(dfs.replication 的默认值为 3)，因为 namenode 已经知道文件由哪些块组成，所以它在返回成功前只需</p><p> 要等待数据块进行最小量的复制。</p></li></ol><h3 id="2、详细写出mapreduce的shuffle过程-20分"><a href="#2、详细写出mapreduce的shuffle过程-20分" class="headerlink" title="2、详细写出mapreduce的shuffle过程   20分"></a>2、详细写出mapreduce的shuffle过程   20分</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-024417.png" alt="image-20180611104416430"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-12-014818.jpg" alt=""></p><p><strong>Map:</strong></p><ol><li>mapTask 收集我们的 map()方法输出的 kv 对，放到内存缓冲区 kvbuffer(环形缓冲区:内 存中的一种首尾相连的数据结构，kvbuffer 包含数据区和索引区)中 </li><li>从内存缓冲区中的数据区的数据不断溢出本地磁盘文件 file.out，可能会溢出多次，则会 有多个文件，相应的内存缓冲区中的索引区数据溢出为磁盘索引文件 file.out.index </li><li>多个溢出文件会被合并成大的溢出文件 </li><li>在溢出过程中，及合并的过程中，都要调用 partitoner 进行分区和针对 key 进行排序 </li></ol><p><strong>Reduce:</strong></p><ol><li>reduceTask 根据自己的分区号，去各个 mapTask 机器上取相应的结果分区数据 </li><li>reduceTask 会取到同一个分区的来自不同 mapTask 的结果文件，reduceTask 会将这些文 件再进行合并(归并排序) </li><li>合并成大文件后，shuffle 的过程也就结束了，后面进入 reduceTask 的逻辑运算过程(从 文件中取出一个一个的键值对 group，调用用户自定义的 reduce()方法) </li></ol><hr><p><br></p><h1 id="第二题：编程题-（70分）"><a href="#第二题：编程题-（70分）" class="headerlink" title="第二题：编程题  （70分）"></a>第二题：编程题  （70分）</h1><h3 id="第一题、HDFS题综合练习（10分）"><a href="#第一题、HDFS题综合练习（10分）" class="headerlink" title="第一题、HDFS题综合练习（10分）"></a>第一题、HDFS题综合练习（10分）</h3><blockquote><p> 编写程序统计出HDFS文件系统中的平均数据块数（数据块总数/文件总数）</p><p>比如：一个文件有5个块，一个文件有3个块，那么平均数据块数为4</p><p>如果还有一个文件，并且数据块就1个，那么整个HDFS的平均数据块数就是3</p></blockquote><p><strong>代码如下</strong>：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.test._01;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.LocatedFileStatus;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.RemoteIterator;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 编写程序统计出HDFS文件系统中的平均数据块数（数据块总数/文件总数）</span></span><br><span class="line"><span class="comment">比如：一个文件有5个块，一个文件有3个块，那么平均数据块数为4</span></span><br><span class="line"><span class="comment">如果还有一个文件，并且数据块就1个，那么整个HDFS的平均数据块数就是3</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BlockCount</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">configuration.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line">FileSystem fSystem = FileSystem.get(configuration);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 数据块总数</span></span><br><span class="line"><span class="keyword">int</span> sumBlockNum = <span class="number">0</span>;</span><br><span class="line"><span class="comment">// 副本总数</span></span><br><span class="line"><span class="keyword">int</span> sumReplicationNum = <span class="number">0</span>;</span><br><span class="line"><span class="comment">// 递归获取HDFS上所有文件</span></span><br><span class="line">RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fSystem.listFiles(<span class="keyword">new</span> Path(<span class="string">"/"</span>), <span class="keyword">true</span>);</span><br><span class="line"><span class="keyword">while</span> (listFiles.hasNext()) &#123;</span><br><span class="line">LocatedFileStatus next = listFiles.next();</span><br><span class="line"><span class="comment">// 所有的的block块数目</span></span><br><span class="line"><span class="keyword">int</span> currentBlockNum = next.getBlockLocations().length;</span><br><span class="line">sumBlockNum += currentBlockNum;</span><br><span class="line"><span class="comment">//计算所有的副本数 副本总数=当全前副本数*block块数</span></span><br><span class="line"> sumReplicationNum += next.getReplication() * currentBlockNum;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">float</span> avgReplicationNum = (<span class="keyword">float</span>)sumReplicationNum / sumBlockNum;</span><br><span class="line">System.out.println(<span class="string">"当前平均块数为: "</span>+avgReplicationNum);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">---运行结果------</span><br><span class="line"><span class="number">2018</span>-<span class="number">06</span>-<span class="number">12</span> <span class="number">19</span>:<span class="number">23</span>:<span class="number">10</span>,<span class="number">696</span> WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:&lt;clinit&gt;(<span class="number">62</span>)) - Unable to load <span class="keyword">native</span>-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes where applicable</span><br><span class="line">当前平均块数为: <span class="number">3.0</span></span><br></pre></td></tr></table></figure><p><br></p><h3 id="第二题、MapReduce题–基础复习（40分-20-分-20分）"><a href="#第二题、MapReduce题–基础复习（40分-20-分-20分）" class="headerlink" title="第二题、MapReduce题–基础复习（40分 = 20 分 + 20分）"></a>第二题、MapReduce题–基础复习（40分 = 20 分 + 20分）</h3><p><strong>下面是三种商品的销售数据</strong> </p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-12-113150.png" alt="image-20180612193149996"></p><p><strong>要求：根据以上数据，用MapReduce统计出如下数据：</strong></p><h4 id="1、每种商品的销售总金额，并降序排序"><a href="#1、每种商品的销售总金额，并降序排序" class="headerlink" title="1、每种商品的销售总金额，并降序排序"></a><strong>1、每种商品的销售总金额，并降序排序</strong></h4><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLWWeoayfJIvnIatCut98pKi1oW00"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.test._02;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.AllArgsConstructor;</span><br><span class="line"><span class="keyword">import</span> lombok.Getter;</span><br><span class="line"><span class="keyword">import</span> lombok.NoArgsConstructor;</span><br><span class="line"><span class="keyword">import</span> lombok.Setter;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@Setter</span></span><br><span class="line"><span class="meta">@Getter</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProductBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">ProductBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> String proId;</span><br><span class="line"><span class="keyword">private</span> Double sales;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">out.writeUTF(proId);</span><br><span class="line">out.writeDouble(sales);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.proId = in.readUTF();</span><br><span class="line"><span class="keyword">this</span>.sales = in.readDouble();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(ProductBean o)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> o.getSales().compareTo(<span class="keyword">this</span>.getSales());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span>  proId  + <span class="string">"\t"</span></span><br><span class="line">+ sales;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLWWiJIqkoSpFCuetv798pKi1oW00"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.test._02;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.DoubleWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Question2_1</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line">job.setJarByClass(Question2_1.class);</span><br><span class="line"></span><br><span class="line">job.setMapperClass(MyMapper.class);</span><br><span class="line">job.setReducerClass(MyReducer.class);</span><br><span class="line"></span><br><span class="line">job.setMapOutputKeyClass(Text.class);</span><br><span class="line">job.setMapOutputValueClass(DoubleWritable.class);</span><br><span class="line"></span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(DoubleWritable.class);</span><br><span class="line"></span><br><span class="line">String inP = <span class="string">"/in/productsales"</span>;</span><br><span class="line">String outP = <span class="string">"/out/Question2_1"</span>;</span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(inP));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(outP));</span><br><span class="line"></span><br><span class="line">Job job2 = Job.getInstance(conf);</span><br><span class="line">job2.setJarByClass(Question2_1.class);</span><br><span class="line"></span><br><span class="line">job2.setMapperClass(MyMapper2.class);</span><br><span class="line">job2.setReducerClass(MyReducer2.class);</span><br><span class="line"></span><br><span class="line">job2.setMapOutputKeyClass(ProductBean.class);</span><br><span class="line">job2.setMapOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">job.setOutputKeyClass(ProductBean.class);</span><br><span class="line">job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">String inP2 = <span class="string">"/out/Question2_1"</span>;</span><br><span class="line">String outP2 = <span class="string">"/out/Question2_1_2"</span>;</span><br><span class="line">FileInputFormat.setInputPaths(job2, <span class="keyword">new</span> Path(inP2));</span><br><span class="line">FileOutputFormat.setOutputPath(job2, <span class="keyword">new</span> Path(outP2));</span><br><span class="line"></span><br><span class="line">JobControl control = <span class="keyword">new</span> JobControl(<span class="string">"jc"</span>);</span><br><span class="line"></span><br><span class="line">ControlledJob conjob1 = <span class="keyword">new</span> ControlledJob(job.getConfiguration());</span><br><span class="line">ControlledJob conjob2 = <span class="keyword">new</span> ControlledJob(job2.getConfiguration());</span><br><span class="line"></span><br><span class="line">conjob2.addDependingJob(conjob1);</span><br><span class="line"></span><br><span class="line">control.addJob(conjob1);</span><br><span class="line">control.addJob(conjob2);</span><br><span class="line"></span><br><span class="line">Thread t = <span class="keyword">new</span> Thread(control);</span><br><span class="line">t.start();</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (!control.allFinished()) &#123;</span><br><span class="line">Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">&#125;</span><br><span class="line">System.exit(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span></span></span><br><span class="line"><span class="class"><span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">DoubleWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line">DoubleWritable v = <span class="keyword">new</span> DoubleWritable();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">String[] line = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line"><span class="comment">//根据商品ID作为Key这样在reduce阶段我们就能对每一种商品的销售额进行累加  </span></span><br><span class="line">k.set(line[<span class="number">1</span>]);</span><br><span class="line"><span class="comment">//根据数据格式，求出每条记录的销售总金额  </span></span><br><span class="line"><span class="keyword">double</span> sum = Double.parseDouble(line[<span class="number">2</span>])</span><br><span class="line">* Double.parseDouble(line[<span class="number">3</span>]);</span><br><span class="line">v.set(sum);</span><br><span class="line">context.write(k, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span></span></span><br><span class="line"><span class="class"><span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">DoubleWritable</span>, <span class="title">Text</span>, <span class="title">DoubleWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">DoubleWritable v = <span class="keyword">new</span> DoubleWritable();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;DoubleWritable&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">double</span> sum = <span class="number">0</span>D;</span><br><span class="line"><span class="comment">//相同的商品会在一个reduceTask中进行处理，  </span></span><br><span class="line"><span class="comment">//这里能够对每一种商品的销售额进行累加  </span></span><br><span class="line"><span class="keyword">for</span> (DoubleWritable num : values) &#123;</span><br><span class="line">sum += num.get();</span><br><span class="line">&#125;</span><br><span class="line">v.set(sum);</span><br><span class="line">context.write(key, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** </span></span><br><span class="line"><span class="comment">* 对商品进行排序的时候这里选择使用自定义数据类型的方式进行 </span></span><br><span class="line"><span class="comment">* 使用compareTo方法对数据进行排序 </span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper2</span></span></span><br><span class="line"><span class="class"><span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">ProductBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">String[] line = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">ProductBean p = <span class="keyword">new</span> ProductBean(line[<span class="number">0</span>],</span><br><span class="line">Double.parseDouble(line[<span class="number">1</span>]));</span><br><span class="line">context.write(p, NullWritable.get());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer2</span> <span class="keyword">extends</span></span></span><br><span class="line"><span class="class"><span class="title">Reducer</span>&lt;<span class="title">ProductBean</span>, <span class="title">NullWritable</span>, <span class="title">ProductBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(ProductBean key, Iterable&lt;NullWritable&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">context.write(key, NullWritable.get());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"> 执行结果 </span><br><span class="line">B0001<span class="number">1681.0</span></span><br><span class="line">A0003<span class="number">1217.5</span></span><br><span class="line">A0001<span class="number">753.8</span></span><br><span class="line">A0002<span class="number">738.5</span></span><br></pre></td></tr></table></figure><h4 id="2、每种商品销售额最多的三周"><a href="#2、每种商品销售额最多的三周" class="headerlink" title="2、每种商品销售额最多的三周"></a><strong>2、每种商品销售额最多的三周</strong></h4><p>思路：</p><ul><li>按商品名分组<ul><li>自定义<code>ProductBean</code>, 实现<code>WritableComparable</code>,   <code>compareTo()</code>方法中实现<ul><li>商品名相同的，按照销售额降序排序</li><li>商品名不同的，按照商品名降序排序</li></ul></li><li>自定义<code>MyGroup</code>，继承<code>WritableComparator</code>, 重写<code>compare()</code>中实现<ul><li>比价商品编号， 按照商品编号分组</li></ul></li><li>在job中，启用自定义分组类<ul><li><code>job.setGroupingComparatorClass(MyGroup.class);</code></li></ul></li></ul></li><li>Mapper<ul><li><code>Mapper&lt;LongWritable, Text, ProductBean, NullWritable&gt;</code> </li></ul></li><li>Reducer<ul><li><code>Reducer&lt;ProductBean, NullWritable, ProductBean, NullWritable&gt;</code> </li><li><code>Reducer</code>中，写出前三次接收到的<code>ProductBean</code></li></ul></li></ul><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLWWeoayfJIvnIatCut98pKi1oW00"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.test._02_2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.AllArgsConstructor;</span><br><span class="line"><span class="keyword">import</span> lombok.Getter;</span><br><span class="line"><span class="keyword">import</span> lombok.NoArgsConstructor;</span><br><span class="line"><span class="keyword">import</span> lombok.Setter;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@Setter</span></span><br><span class="line"><span class="meta">@Getter</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProductBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">ProductBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> String proId;</span><br><span class="line"><span class="keyword">private</span> Double sales;</span><br><span class="line"><span class="keyword">private</span> String week;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">out.writeUTF(proId);</span><br><span class="line">out.writeDouble(sales);</span><br><span class="line">out.writeUTF(week);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.proId = in.readUTF();</span><br><span class="line"><span class="keyword">this</span>.sales = in.readDouble();</span><br><span class="line"><span class="keyword">this</span>.week = in.readUTF();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(ProductBean o)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 如果商品编号相同</span></span><br><span class="line"><span class="keyword">if</span> (<span class="keyword">this</span>.getProId().compareTo(o.getProId()) == <span class="number">0</span>) &#123;</span><br><span class="line"><span class="comment">// 就按照价格降序排序</span></span><br><span class="line"><span class="keyword">return</span> o.getSales().compareTo( <span class="keyword">this</span>.getSales());</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 否则按照商品编号排序</span></span><br><span class="line"><span class="keyword">return</span> o.getProId().compareTo(<span class="keyword">this</span>.getProId());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span>  proId  + <span class="string">"\t"</span></span><br><span class="line">+ sales + <span class="string">"\t"</span> + week;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLl2jT2_ABotWSaZDIm6A0W00"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.test._02_2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparator;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyGroup</span> <span class="keyword">extends</span> <span class="title">WritableComparator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">MyGroup</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">super</span>(ProductBean.class,<span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">ProductBean aBean = (ProductBean)a;</span><br><span class="line">ProductBean bBean = (ProductBean)b;</span><br><span class="line"><span class="comment">// 根据商品编号分组</span></span><br><span class="line"><span class="keyword">return</span> aBean.getProId().compareTo(bBean.getProId());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLWWiJIqkoSpFCuetud98pKi1oW00"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.test._02_2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Question2_2</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line">job.setJarByClass(Question2_2.class);</span><br><span class="line"></span><br><span class="line">job.setMapperClass(MyMapper.class);</span><br><span class="line">job.setReducerClass(MyReducer.class);</span><br><span class="line"></span><br><span class="line">job.setMapOutputKeyClass(ProductBean.class);</span><br><span class="line">job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置分组组件</span></span><br><span class="line">job.setGroupingComparatorClass(MyGroup.class);</span><br><span class="line"></span><br><span class="line">job.setOutputKeyClass(ProductBean.class);</span><br><span class="line">job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">String inP = <span class="string">"/in/productsales"</span>;</span><br><span class="line">String outP = <span class="string">"/out/Question2_2"</span>;</span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(inP));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(outP));</span><br><span class="line"></span><br><span class="line">FileSystem fs = FileSystem.get(conf);</span><br><span class="line">Path out = <span class="keyword">new</span> Path(outP);</span><br><span class="line"><span class="keyword">if</span> (fs.exists(out)) &#123;</span><br><span class="line">fs.delete(out,<span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br><span class="line">System.exit(job.waitForCompletion(<span class="keyword">true</span>)?<span class="number">0</span>:-<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 商品名分组</span></span><br><span class="line"><span class="comment"> * 商品当周销售额降序排序</span></span><br><span class="line"><span class="comment"> * 取出对应的三周 </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">ProductBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value,</span></span></span><br><span class="line"><span class="function"><span class="params">Mapper&lt;LongWritable, Text, ProductBean, NullWritable&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">String[] datas = value.toString().trim().split(<span class="string">"\t"</span>);</span><br><span class="line">Double sales = Double.parseDouble(datas[<span class="number">2</span>])</span><br><span class="line">* Double.parseDouble(datas[<span class="number">3</span>]);</span><br><span class="line">ProductBean pBean = <span class="keyword">new</span> ProductBean(datas[<span class="number">1</span>], sales, datas[<span class="number">0</span>]);</span><br><span class="line">context.write(pBean, NullWritable.get());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">ProductBean</span>, <span class="title">NullWritable</span>, <span class="title">ProductBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(ProductBean key, Iterable&lt;NullWritable&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Reducer&lt;ProductBean, NullWritable, ProductBean, NullWritable&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (NullWritable nullWritable : values) &#123;</span><br><span class="line"><span class="keyword">if</span> (count &lt; <span class="number">3</span>) &#123;</span><br><span class="line">context.write(key, nullWritable);</span><br><span class="line">&#125;</span><br><span class="line">                count ++;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">------运行结果---</span><br><span class="line">    </span><br><span class="line">B0001<span class="number">550.0</span>week2</span><br><span class="line">B0001<span class="number">525.0</span>week1</span><br><span class="line">B0001<span class="number">522.0</span>week3</span><br><span class="line">A0003<span class="number">463.50000000000006</span>week4</span><br><span class="line">A0003<span class="number">294.0</span>week3</span><br><span class="line">A0003<span class="number">276.0</span>week1</span><br><span class="line">A0002<span class="number">264.0</span>week3</span><br><span class="line">A0002<span class="number">187.0</span>week4</span><br><span class="line">A0002<span class="number">160.0</span>week2</span><br><span class="line">A0001<span class="number">330.0</span>week2</span><br><span class="line">A0001<span class="number">200.0</span>week1</span><br><span class="line">A0001<span class="number">128.79999999999998</span>week4</span><br></pre></td></tr></table></figure><p><br></p><h1 id="第三题：MapReduce题–倒排索引（-20分）"><a href="#第三题：MapReduce题–倒排索引（-20分）" class="headerlink" title="第三题：MapReduce题–倒排索引（ 20分）"></a>第三题：MapReduce题–倒排索引（ 20分）</h1><p>:unamused:</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;第一题：简答题-（30分）&quot;&gt;&lt;a href=&quot;#第一题：简答题-（30分）&quot; class=&quot;headerlink&quot; title=&quot;第一题：简答题 （30分）&quot;&gt;&lt;/a&gt;第一题：简答题 （30分）&lt;/h1&gt;&lt;h3 id=&quot;1、简述hdfs的读写流程&quot;&gt;&lt;a hre
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="Test" scheme="https://airpoet.github.io/categories/Hadoop/Test/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="Test" scheme="https://airpoet.github.io/tags/Test/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce-简单总结</title>
    <link href="https://airpoet.github.io/2018/06/12/Hadoop/Study/2-MapReduce/MapReduce-%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93/"/>
    <id>https://airpoet.github.io/2018/06/12/Hadoop/Study/2-MapReduce/MapReduce-简单总结/</id>
    <published>2018-06-12T07:03:29.967Z</published>
    <updated>2018-06-12T07:05:09.710Z</updated>
    
    <content type="html"><![CDATA[<p>MapReduce在编程的时候，基本上一个固化的模式，没有太多可灵活改变的地方，<strong>除了以下几处</strong>：</p><p>1、输入数据接口：InputFormat   —&gt;     FileInputFormat(文件类型数据读取的通用抽象类)  DBInputFormat （数据库数据读取的通用抽象类）<br>   默认使用的实现类是： TextInputFormat     job.setInputFormatClass(TextInputFormat.class)<br>   TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回</p><p>2、逻辑处理接口： Mapper<br>   完全需要用户自己去实现其中  map()   setup()   clean()   </p><p>3、map输出的结果在shuffle阶段会被partition以及sort，此处有两个接口可自定义：<br>     Partitioner<br>        有默认实现 HashPartitioner，逻辑是  根据key和numReduces来返回一个分区号； key.hashCode()&amp;Integer.MAXVALUE % numReduces<br>    通常情况下，用默认的这个HashPartitioner就可以，如果业务上有特别的需求，可以自定义</p><pre><code>Comparable   当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，override其中的compareTo()方法</code></pre><p>4、reduce端的数据分组比较接口 ： Groupingcomparator<br>    reduceTask拿到输入数据（一个partition的所有数据）后，首先需要对数据进行分组，其分组的默认原则是key相同，然后对每一组kv数据调用一次reduce()方法，并且将这一组kv中的第一个kv的key作为参数传给reduce的key，将这一组数据的value的迭代器传给reduce()的values参数</p><pre><code>利用上述这个机制，我们可以实现一个高效的分组取最大值的逻辑：自定义一个bean对象用来封装我们的数据，然后改写其compareTo方法产生倒序排序的效果然后自定义一个Groupingcomparator，将bean对象的分组逻辑改成按照我们的业务分组id来分组（比如订单号）这样，我们要取的最大值就是reduce()方法中传进来key</code></pre><p>5、逻辑处理接口：Reducer<br>    完全需要用户自己去实现其中  reduce()   setup()   clean()   </p><p>6、输出数据接口： OutputFormat  —&gt; 有一系列子类  FileOutputformat  DBoutputFormat  …..<br>    默认实现类是TextOutputFormat，功能逻辑是：  将每一个KV对向目标文本文件中输出为一行</p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-12-070505.png" alt="image-20180612150505148">    </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;MapReduce在编程的时候，基本上一个固化的模式，没有太多可灵活改变的地方，&lt;strong&gt;除了以下几处&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;1、输入数据接口：InputFormat   —&amp;gt;     FileInputFormat(文件类型数据读取的通用抽象类) 
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/"/>
    
      <category term="学习笔记" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>Hive学习——1</title>
    <link href="https://airpoet.github.io/2018/06/12/Hadoop/Study/3-Hive/Hive%E2%80%94%E2%80%941/"/>
    <id>https://airpoet.github.io/2018/06/12/Hadoop/Study/3-Hive/Hive——1/</id>
    <published>2018-06-12T03:24:46.733Z</published>
    <updated>2018-06-13T13:19:34.399Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hive简介"><a href="#Hive简介" class="headerlink" title="Hive简介"></a>Hive简介</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-12-034055.png" alt="image-20180612114055129"></p><h3 id="Hive基本情况"><a href="#Hive基本情况" class="headerlink" title="Hive基本情况"></a>Hive基本情况</h3><p><strong>Hive</strong> 是建立在 <strong>Hadoop</strong> 上的数据仓库基础构架</p><ul><li>由facebook开源，最初用于解决海量结构化的日志数据统 计问题;<ul><li><strong>ETL</strong>(<strong>Extraction-Transformation-Loading</strong>)工具</li></ul></li><li>构建在Hadoop之上的数据仓库;<ul><li>数据计算使用<strong>MR</strong>，数据存储使用<strong>HDFS</strong></li><li>数据库&amp;数据仓库 的区别：<ul><li>概念上<ul><li><strong>数据库</strong>：用于管理精细化数据，一般情况下用于存储结果数据，分库分表进行存储</li><li><strong>数据仓库</strong>：<strong>存储、查询、分析大规模数据</strong> 。更像一个打包的过程，里面存储的数据没有细化区分，粒度较粗</li></ul></li><li>用途上：<ul><li>数据库：OLTP，on line Transation Processing 联机事务处理，增删改</li><li>数据仓库：OLAP，on line analysis Processing 联机事务分析处理，查询，hive不支持删除、修改。 支持插入。</li></ul></li><li>使用上：<ul><li>数据库：标准sql,  hbase: 非标准sql</li><li>数据仓库：方言版的sql， HQL</li></ul></li><li>模式上：<ul><li>数据库：写模式</li><li>数据仓库：读模式</li></ul></li></ul></li></ul></li><li>可以将结构化的数据映射成一张数据库表<ul><li>结构化数据映射成二维表</li><li><strong>将文本中每一行数据映射为数据库的每一条数据</strong></li><li><strong>将文本中每一列数据映射为hive的表字段</strong></li></ul></li><li>提供HQL 查询功能<ul><li>hive query language， 方言版sql</li></ul></li><li><strong>底层数据是存储在HDFS上</strong><ul><li>hive上建的表仅仅相当于<strong>对hdfs上的结构化数据进行映射管理</strong></li><li>hive仅仅是一个管理数据的作用，而<strong>不会存储数据</strong></li><li>hive想要管理hdfs上的数据，就要建立一个关联关系，关联hive上的表和hdfs上的数据路径</li><li>数据是依赖于一个元数据库</li><li>元数据库采用的是关系型数据库， 真实生产中一般使用mysql为hive的元数据库，hive内置默认的元数据库是 derby</li><li><strong>元数据：</strong> <strong>HCalalog</strong> <ul><li>hive中的表和hdfs的<strong>映射关系</strong>，以及<strong>hive表属性</strong>（内部表，外部表，视图）和<strong>字段信息</strong></li><li>元数据一旦修饰，hive的所有映射关系等都没了，就无法使用了</li><li>可与<strong>Pig</strong>、<strong>Presto</strong>等共享 </li></ul></li></ul></li><li>Hive的是<strong>构建在Hadoop之上的数据仓库</strong><ul><li>数据计算使用<strong>MR</strong>，数据存储使用<strong>HDFS</strong> </li></ul></li><li>通常用于进行离线数据处理(采用MapReduce) </li><li><strong>可认为是一个HQL—-&gt;MR的语言翻译器</strong> </li></ul><h3 id="Hive优缺点："><a href="#Hive优缺点：" class="headerlink" title="Hive优缺点："></a>Hive优缺点：</h3><ul><li><p>优点：</p><ul><li>简单，容易上手 <ul><li>提供了类<strong>SQL</strong>查询语言<strong>HQL</strong> </li></ul></li><li>为超大数据集设计的计算/扩展能力 <ul><li><strong>MR</strong>作为计算引擎，<strong>HDFS</strong>作为存储系统 </li></ul></li><li>统一的元数据管理(HCalalog) <ul><li>可与<strong>Pig</strong>、<strong>Presto</strong>等共享 </li></ul></li></ul></li><li><p><strong>缺点</strong></p><ul><li><p><strong>不支持 删除 &amp; 修改</strong>  delete&amp;update，<strong>不支持事务</strong></p><ul><li>因为是基于HDFS</li><li>hive做的最多的是查询</li></ul></li><li><p>Hive的HQL表达的能力有限 </p><ul><li>迭代式算法无法表达 </li><li>有些复杂运算用<strong>HQL</strong>不易表达 </li></ul></li><li><p><strong>Hive效率较低，查询延时高</strong></p><ul><li><strong>Hive</strong>自动生成<strong>MapReduce</strong>作业，通常不够智能 </li><li><strong>HQL</strong>调优困难，粒度较粗 </li><li>可控性差 </li></ul></li></ul></li></ul><h3 id="Hive与传统关系型数据库-RDBMS）对比"><a href="#Hive与传统关系型数据库-RDBMS）对比" class="headerlink" title="Hive与传统关系型数据库(RDBMS）对比"></a>Hive与传统关系型数据库(RDBMS）对比</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-12-082659.png" alt="image-20180612162659151"><br></p><h1 id="Hive的架构"><a href="#Hive的架构" class="headerlink" title="Hive的架构"></a>Hive的架构</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-12-061321.png" alt="image-20180612141321130"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-12-061413.png" alt="image-20180612141413285"></p><ul><li><p><strong>1) 用户接口</strong></p><ul><li><p>CLI：Command Line Interface，即Shell终端命令行，使用 Hive 命令行与 Hive 进行交互，最常用（学习，调试，生产），包括两种运行方式：</p><ul><li><p>hive命令方式：前提必须在hive安装节点上执行</p></li><li><p>hiveserver2方式：hive安装节点将hive启动为一个后台进程，客户机进行连接（类似于启动了一个hive服务端）</p></li></ul></li></ul></li></ul><pre><code>真实生产中常用！- 1) 修改配置文件，允许远程连接;  第一：修改 hdfs-site.xml，加入一条配置信息，启用 webhdfs；  第二：修改 core-site.xml，加入两条配置信息，设置 hadoop的代理用户。- 2) 启动服务进程  - 前台启动：hiveserver2  - 后台启动    - 记录日志：nohup hiveserver2 1&gt;/home/sigeon/hiveserver.log 2&gt;/home/sigeon/hiveserver.err &amp;      0：标准日志输入   1：标准日志输出   2：错误日志输出      如果我没有配置日志的输出路径，日志会生成在当前工作目录，默认的日志名称叫做：      nohup.xxx    - 不记录日志：nohup hiveserver2 1&gt;/dev/null 2&gt;/dev/null &amp;     - [补充：]      - nohup命令：no hang up的缩写，即不挂起，可以在你退出帐户/关闭终端之后继续运行相应的进程。      - 语法：nohup &lt;command&gt; &amp;- 3) 开启beenline客户端并连接：  - 方法一：     - beenline，开启beenline客户端;    - !connect jdbc:&lt;hive2://master:10000&gt;，回车。然后输入用户名和密码，这个用户名是安装 hadoop 集群的用户名  - 方法二：    - beeline -u jdbc:&lt;hive2://master:10000&gt; -n sigeon</code></pre><ul><li><p>JDBC/ODBC：是 Hive 的基于 JDBC 操作提供的客户端，用户（开发员，运维人员）通过 这连接至 Hive server 服务 </p></li><li><p>Web UI：通过浏览器访问 Hive，基本不会使用</p></li></ul><ul><li><p>2) 元数据库：保存元数据，一般会选用关系型数据库（如mysql，Hive 和 MySQL 之间通过 MetaStore 服务交互）</p></li><li><p>3) Thrift服务：Facebook 开发的一个软件框架，可以用来进行可扩展且跨语言的服务的开发， Hive 集成了该服务，能让不同的编程语言调用 Hive 的接口</p></li><li><p>4) 驱动Driver</p><ul><li>a. 解释器：解释器的作用是将 HiveSQL 语句转换为抽象语法树（AST） </li><li>b. 编译器：编译器是将语法树编译为逻辑执行计划 </li><li>c. 优化器：优化器是对逻辑执行计划进行优化 </li><li>d. 执行器：执行器是调用底层的运行框架执行逻辑执行计划 </li></ul></li></ul><p><br></p><h1 id="Hive的数据组织格式"><a href="#Hive的数据组织格式" class="headerlink" title="Hive的数据组织格式"></a>Hive的数据组织格式</h1><ul><li><p>1)库：database</p></li><li><p>2) 表</p><ul><li><p>a. 内部表（管理表：managed_table）</p></li><li><p>b. 外部表（external_table）</p></li><li><p>内部表和外部表区别：</p><ul><li>内部表和外部表是两个相对的概念，不可能有一个表同时是内部表又是外部表；</li><li>内部表删除表的时候会删除原始数据和元数据，而外部表删除表的时候只会删除元数据不会删除原始数据</li><li>一般情况下存储公共数据的表存放为外部表</li><li>大多数情况，他们的区别不明显。如果数据的所有处理都在 Hive 中进行，那么倾向于 选择内部表；但是如果 Hive 和其他工具要针对相同的数据集进行处理，外部表更合适。 </li><li>使用外部表访问存储在 HDFS 上的初始数据，然后通过 Hive 转换数据并存到内部表中，使用外部表的场景是针对一个数据集有多个不同的 Schema。 </li></ul></li><li><p>c. 分区表</p><ul><li>不同于hadoop中的分区，分区表是人为划分的</li><li>hive最终存储海量数据，海量数据查询一定注意避免全表扫描</li><li>查询的时候为了提升我们的查询性能，出现了分区表</li><li>将数据按照用户的业务存储到不同的目录下，在进行数据查询时只会对指定分区下的数据进行扫描<br>一般情况下生产中用日期作为分区字段</li></ul></li><li><p>d. 分桶表</p><ul><li><p>类似于hadoop中的分区，是由程序决定的，只能指定桶的个数（分区的个数）</p></li><li><p>根据hash算法将余数不同的输出到不同的文件中</p></li><li><p>作用：</p><ul><li>1）提升join的性能<br>思考这个问题：select <a href="http://a.id" target="_blank" rel="noopener">a.id</a>,<a href="http://a.name" target="_blank" rel="noopener">a.name</a>,b.addr from a join b on <a href="http://a.id" target="_blank" rel="noopener">a.id</a> = <a href="http://b.id" target="_blank" rel="noopener">b.id</a>;如果 a 表和 b 表已经是 分桶表，而且分桶的字段是 id 字段做这个 join 操作时，还需要全表做笛卡尔积</li><li>2）提升数据样本的抽取效率，直接拿一个桶中的数据作为样本数据</li></ul></li><li><p>分区表和分桶表的区别： </p><p><img src="/var/folders/6l/blvdbwd53hqglz0f09n3sd5c0000gn/T/abnerworks.Typora/image-20180612143606401.png" alt="image-20180612143606401"></p><ul><li>Hive 数据表可以根据某些字段进行分区操作，细化数据管理，可以让部分查询更快。</li><li>同时表和分区也可以进一步被划分为 Buckets，分桶表的原理和 MapReduce 编程中的 HashPartitioner 的原理类似 </li><li>分区和分桶都是细化数据管理，但是分区表是手动添加区分，由于 Hive 是读模式，所以对添加进分区的数据不做模式校验</li><li>分桶表中的数据是按照某些分桶字段进行 hash 散列 形成的多个文件，所以数据的准确性也高很多</li></ul></li></ul></li></ul></li></ul><p><br></p><ul><li><p><strong>3)视图：</strong></p><ul><li>hive中的视图仅仅相当于一个sql语句的别名</li><li><strong>在hive中仅仅存在逻辑视图，不存在物理视图</strong><ul><li>物理视图：讲sql语句的执行结果存在视图中</li><li>逻辑视图： <strong>仅仅是对查询结果的引用</strong></li></ul></li></ul></li><li><p><strong>4)数据存储：</strong></p><ul><li>原始数据中存在HDFS</li><li>元数据存在mysql</li></ul></li></ul><p><br></p><h1 id="Hive安装"><a href="#Hive安装" class="headerlink" title="Hive安装"></a>Hive安装</h1><p><strong>装hive其实不难，主要是安装mysql，解决mysql的权限问题</strong></p><h2 id="MySql安装"><a href="#MySql安装" class="headerlink" title="MySql安装"></a>MySql安装</h2><h3 id="RPM-安装MySQl："><a href="#RPM-安装MySQl：" class="headerlink" title="RPM 安装MySQl："></a>RPM 安装MySQl：</h3><ol><li><p>检查以前是否装过 MySQL </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -qa|grep -i mysql</span><br></pre></td></tr></table></figure></li><li><p>发现有的话就都卸载 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rpm -e --nodeps mysql-libs-5.1.73-5.el6_6.x86_64</span><br><span class="line">rpm -e --nodeps ....</span><br></pre></td></tr></table></figure></li><li><p>删除老版本 mysql 的开发头文件和库 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rm -rf /usr/lib64/mysql</span><br><span class="line"><span class="comment"># 在搜索 my.cnf 文件，有的话就删掉</span></span><br></pre></td></tr></table></figure></li><li><p>上传mysql 安装包到 Linux中，解压</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tar -xvf mysql-5.6.26-1.linux_glibc2.5.x86_64.rpm-bundle.tar</span><br><span class="line"><span class="comment"># 解压出来有这些文件</span></span><br><span class="line">MySQL-devel-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br><span class="line">MySQL-embedded-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br><span class="line">MySQL-server-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br><span class="line">MySQL-shared-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br><span class="line">MySQL-shared-compat-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br><span class="line">MySQL-test-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br></pre></td></tr></table></figure></li><li><p>安装 server &amp; client </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># server</span></span><br><span class="line">rpm -ivh MySQL-server-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br><span class="line"><span class="comment"># client</span></span><br><span class="line">rpm -ivh MySQL-client-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br></pre></td></tr></table></figure></li><li><p>启动Mysql</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service mysql start</span><br></pre></td></tr></table></figure></li><li><p>登录Mysql并改密码，等</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"># 初始密码在这个文件中</span><br><span class="line">cat /root/.mysql_sercert </span><br><span class="line"></span><br><span class="line"># 登录</span><br><span class="line">mysql -uroot -pxxxxx</span><br><span class="line"></span><br><span class="line"># 删除除了`%`之外的其他所有host</span><br><span class="line">use mysql;</span><br><span class="line">select host,user,password from user;</span><br><span class="line">delete from user where host in (&apos;localhost&apos;, &apos;127.0.0.1&apos;,&apos;::1&apos;, ...)</span><br><span class="line"></span><br><span class="line"># 修改密码 </span><br><span class="line">UPDATE user SET Password = PASSWORD(&apos;psd&apos;) WHERE user = &apos;root&apos;;</span><br><span class="line"></span><br><span class="line"># 为`%` 和 `*` 添加远程登录权限 </span><br><span class="line">#注意： 前面的 mysql 登录用户名， 123 是登录密码</span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;psd&apos; WITH GRANT OPTION;</span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;*&apos; IDENTIFIED BY &apos;psd&apos; WITH GRANT OPTION;</span><br><span class="line">FLUSH PRIVILEGES;</span><br><span class="line"></span><br><span class="line"># 退出登录</span><br><span class="line">exit;</span><br><span class="line"></span><br><span class="line"># 此时可以再登录试试</span><br><span class="line">mysql -uroot -ppsd</span><br><span class="line"></span><br><span class="line">======================================</span><br><span class="line"></span><br><span class="line"># 修改字符集为 utf-8</span><br><span class="line"># 新建一个文件</span><br><span class="line">vi /etc/my.cnf  </span><br><span class="line"># 添加以下内容</span><br><span class="line">[client]</span><br><span class="line">default-character-set=utf8</span><br><span class="line"></span><br><span class="line">[mysql]</span><br><span class="line">default-character-set=utf8</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line">character-set-server=utf8</span><br><span class="line"></span><br><span class="line"># 重启mysql</span><br><span class="line">sudo service mysql restart</span><br><span class="line"></span><br><span class="line">======================================</span><br><span class="line"></span><br><span class="line"># 忘记密码，修改密码的方法</span><br><span class="line"># 停止mysql服务的运行</span><br><span class="line">service mysql stop</span><br><span class="line"></span><br><span class="line"># 跳过授权表访问</span><br><span class="line">mysqld_safe --user=mysql --skip-grant-tables --skip-networking &amp; </span><br><span class="line"></span><br><span class="line"># 登录mysql</span><br><span class="line">mysql -u root mysql </span><br><span class="line"></span><br><span class="line"># 接下来可以修改密码了</span><br><span class="line">  ##在mysql5.7以下的版本如下：</span><br><span class="line">mysql&gt; UPDATE user SET Password=PASSWORD(&apos;newpassword&apos;) where USER=&apos;root’；</span><br><span class="line">  ##在mysql5.7版本如下：</span><br><span class="line">update mysql.user set authentication_string=password(&apos;newpassword&apos;) ;</span><br><span class="line"></span><br><span class="line"># 修改完了重启</span><br><span class="line">service mysql restart</span><br><span class="line"></span><br><span class="line">======================================</span><br></pre></td></tr></table></figure></li></ol><h4 id="MySql的其它错误"><a href="#MySql的其它错误" class="headerlink" title="MySql的其它错误"></a>MySql的其它错误</h4><p>在运行<code>schematool -dbType mysql -initSchema</code>手动初始化元数据库的时候</p><ul><li><p>报了一个log4j重复加载的问题</p><ul><li>解决：可以不用理睬</li></ul></li><li><p>链接mysql 密码过期问题 <code>Your password has expired.</code></p><ul><li><p>解决： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">mysql -uroot -p123</span><br><span class="line"></span><br><span class="line">mysql&gt; use mysql</span><br><span class="line">Reading table information for completion of table and column names</span><br><span class="line">You can turn off this feature to get a quicker startup with -A</span><br><span class="line"></span><br><span class="line">Database changed</span><br><span class="line">mysql&gt; select host,user,password_expired from user;</span><br><span class="line">+-----------+------+------------------+</span><br><span class="line">| host      | user | password_expired |</span><br><span class="line">+-----------+------+------------------+</span><br><span class="line">| %         | root | N                |</span><br><span class="line">| cs1       | root | Y                |</span><br><span class="line">| 127.0.0.1 | root | Y                |</span><br><span class="line">| ::1       | root | Y                |</span><br><span class="line">+-----------+------+------------------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br><span class="line">-------------</span><br><span class="line">mysql&gt; update user set password_expired=&apos;N&apos; where host=&apos;::1&apos;;</span><br><span class="line">Query OK, 1 row affected (0.01 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class="line"></span><br><span class="line">mysql&gt; update user set password_expired=&apos;N&apos; where host=&apos;cs1&apos;;</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class="line"></span><br><span class="line">mysql&gt; update user set password_expired=&apos;N&apos; where host=&apos;127.0.0.1&apos;;</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class="line"></span><br><span class="line">mysql&gt; FLUSH PRIVILEGES;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select host,user,password_expired from user;</span><br><span class="line">+-----------+------+------------------+</span><br><span class="line">| host      | user | password_expired |</span><br><span class="line">+-----------+------+------------------+</span><br><span class="line">| %         | root | N                |</span><br><span class="line">| cs1       | root | N                |</span><br><span class="line">| 127.0.0.1 | root | N                |</span><br><span class="line">| ::1       | root | N                |</span><br><span class="line">+-----------+------+------------------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mysql&gt; exit</span><br><span class="line">Bye</span><br><span class="line">[ap@cs1]~/apps/hive% sudo service mysql restart</span><br><span class="line"></span><br><span class="line"># 再重新初始化就好了</span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="Yum安装-Mysql"><a href="#Yum安装-Mysql" class="headerlink" title="Yum安装 Mysql"></a>Yum安装 Mysql</h3><blockquote><p><strong>因为笔者没装过，所以这部分暂且不表</strong></p></blockquote><hr><h2 id="step2-安装Hive"><a href="#step2-安装Hive" class="headerlink" title="step2-安装Hive"></a>step2-安装Hive</h2><p>MySql安装好之后， 安装Hive就很简答了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.上传到Linux</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.解压安装包到安装目录</span></span><br><span class="line">tar -zxvf apache-hive-2.3.2-bin.tar.gz -C ~/apps/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.把MySQL驱动包(mysql-connector-java-5.1.40-bin.jar)放置在hive 的根路径下的 lib 目录，此处是 ~/apps/apache-hive-2.3.2-bin/lib</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.修改配置文件</span></span><br><span class="line"><span class="built_in">cd</span> ~/apps/apache-hive-2.3.2-bin/conf</span><br><span class="line"><span class="comment"># 新建一个 hive-site.xml</span></span><br><span class="line">touch hive-site.xml</span><br><span class="line">vi hive-site.xml</span><br><span class="line"></span><br><span class="line">+++++++++++++++++++++++++++++++++++++++++++添加如下内容++++++++++++</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">&lt;value&gt;jdbc:mysql://localhost:3306/hivedb?createDatabaseIfNotExist=<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">&lt;description&gt;JDBC connect string <span class="keyword">for</span> a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;!-- 注意:如果mysql和hive 不在同一个服务器节点,需要使用mysql节点的 hostname 或 ip --&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">&lt;description&gt;Driver class name <span class="keyword">for</span> a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">&lt;value&gt;root&lt;/value&gt;</span><br><span class="line">&lt;description&gt;username to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">&lt;value&gt;123&lt;/value&gt;</span><br><span class="line">&lt;description&gt;password to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line">+++++++++++++++++++++++++++++++++++++++++++添加如下内容+++++++++++</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">&lt;description&gt;hive default warehouse, <span class="keyword">if</span> nessecory, change it</span><br><span class="line"></span><br><span class="line">&lt;!-- 这个是配置hive在HDFS上 db 的路存储径的，不配默认默认就是上述路径 --&gt;</span><br><span class="line">&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">++++++++++++++++++++++++++</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.配置环境变量 &amp; source</span></span><br><span class="line"><span class="comment">## 注意：ap是我的用户家目录，换上自己的用户家目录</span></span><br><span class="line"><span class="built_in">export</span> HIVE_HOME=/home/ap/apps/apache-hive-2.3.2-bin </span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HIVE_HOME</span>/bin</span><br><span class="line"><span class="comment"># 用bash的找 .bash_profile， 配置所有环境变量</span></span><br><span class="line"><span class="built_in">source</span> ~/.zshrc  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 6.此时基本安装完成了，验证Hive安装</span></span><br><span class="line">hive --helo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7.重点来了！ 初始化元数据库</span></span><br><span class="line">chematool -dbType mysql -initSchema</span><br><span class="line">&gt; 这里可能会遇到很多错误！！ </span><br><span class="line">&gt; 但是如果前面按照我的方法装的, 应该就问题不大了</span><br><span class="line">&gt; 主要是 mysql 连接权限的问题！！</span><br><span class="line"><span class="comment"># 一定会出现的2个</span></span><br><span class="line">1&gt; 找不到 hive命令的一长串环境变量 (不用理会)</span><br><span class="line">2&gt; 两个log4j，jar包重复的问题 (不用理会)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 8.初始化完成后，可以看下数据库中有没有 hivedb 这个库，成功的话是会有的</span></span><br><span class="line"><span class="comment">## 启动hive</span></span><br><span class="line">hive --service cli  </span><br><span class="line">&gt;hive: show databases;</span><br><span class="line"><span class="comment"># 如果能显示数据库，就没啥问题了</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ❤️9.Hive的使用方式之  HiveServer2/beeline</span></span><br><span class="line">此处需要修改 hadoop 的配置文件</span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.1.首先关闭hdfs &amp; yarn服务 &amp; RunJar(hive服务)，修改hadoop配置文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.2.修改 hadoop 集群的 hdfs-site.xml 配置文件:加入一条配置信息，表示启用 webhdfs</span></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;</span><br><span class="line">&lt;value&gt;<span class="literal">true</span>&lt;/value&gt; </span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.3.修改 hadoop 集群的 core-site.xml 配置文件:加入两条配置信息:表示设置 hadoop 的代理用户</span></span><br><span class="line"><span class="comment">## 注意： 此处的ap是配置 hadoop 的用户名</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.proxyuser.ap.hosts&lt;/name&gt;</span><br><span class="line">&lt;value&gt;*&lt;/value&gt; </span><br><span class="line">&lt;!-- 表示任意节点使用 hadoop 集群的代理用户 hadoop 都能访问 hdfs 集群 --&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt; </span><br><span class="line">&lt;name&gt;hadoop.proxyuser.ap.groups&lt;/name&gt; </span><br><span class="line">&lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;!-- 表示代理用户的组所属 --&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"> 注意 </span><br><span class="line">修改完成后， 发给hadoop集群其他主机</span><br><span class="line">scp xxx.xx  xxx.ss  cs2:<span class="variable">$PWD</span></span><br><span class="line">scp xxx.xx  xxx.ss  cs3:<span class="variable">$PWD</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.4.重启hdfs &amp; yarn服务</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.5.启动 hiveserver2 服务</span></span><br><span class="line"><span class="comment">## 后台启动：</span></span><br><span class="line">nohup hiveserver2 1&gt;/home/hadoop/hiveserver.log 2&gt;/home/hadoop/hiveserver.err &amp; </span><br><span class="line"><span class="comment"># 或者:</span></span><br><span class="line">nohup hiveserver2 1&gt;/dev/null 2&gt;/dev/null &amp;</span><br><span class="line"><span class="comment"># 或者:</span></span><br><span class="line">nohup hiveserver2 &gt;/dev/null 2&gt;&amp;1 &amp;</span><br><span class="line"><span class="comment"># 以上 3 个命令是等价的，第一个表示记录日志，第二个和第三个表示不记录日志</span></span><br><span class="line"></span><br><span class="line">注意： nohup 可以在你退出帐户/关闭终端之后继续运行相应的进程。 nohup 就是不挂起的意思(no hang up)。</span><br><span class="line">该命令的一般形式为:nohup <span class="built_in">command</span> &amp;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.6 启动 beeline 客户端去连接</span></span><br><span class="line">方式1：执行命令:</span><br><span class="line">beeline -u jdbc:hive2://cs2:10000 -n ap</span><br><span class="line">-u : 指定元数据库的链接信息 -n : 指定用户名和密码</span><br><span class="line"></span><br><span class="line">方式2：</span><br><span class="line">先执行 </span><br><span class="line">beeline</span><br><span class="line">然后按图所示输入:</span><br><span class="line"><span class="comment"># 此处的cs2是只安装hive的hostname</span></span><br><span class="line">!connect jdbc:hive2://cs2:10000 按回车，然后输入用户名，密码，这个 用户名就是安装 hadoop 集群的用户名和密码</span><br></pre></td></tr></table></figure><h4 id="PS-Linux环境变量失效"><a href="#PS-Linux环境变量失效" class="headerlink" title="PS: Linux环境变量失效"></a>PS: Linux环境变量失效</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 就是直接把环境变量设置为/bin:/usr/bin，因为常用的命令都在/bin这个文件夹中。</span></span><br><span class="line">PATH=/bin:/usr/bin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 接下来修改 .bashr_profile 或则 .zshrc中的内容即可， 修改完了重新source</span></span><br></pre></td></tr></table></figure><h1 id="Hive-–-DDL"><a href="#Hive-–-DDL" class="headerlink" title="Hive – DDL"></a>Hive – DDL</h1><h3 id="库的操作"><a href="#库的操作" class="headerlink" title="库的操作"></a>库的操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">  库的操作 </span><br><span class="line">============================================================================</span><br><span class="line"></span><br><span class="line">#  建库 </span><br><span class="line">CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment]</span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[WITH DBPROPERTIES (property_name=property_value, ...)];</span><br><span class="line"></span><br><span class="line">1、创建普通库</span><br><span class="line">create database dbname;</span><br><span class="line"></span><br><span class="line">2、创建库的时候检查存与否</span><br><span class="line">create databse if not exists dbname;</span><br><span class="line"></span><br><span class="line">3、创建库的时候带注释</span><br><span class="line">create database if not exists dbname comment &apos;create my db named dbname&apos;;</span><br><span class="line"></span><br><span class="line">4、查看创建库的详细语句</span><br><span class="line">show create database mydb;</span><br><span class="line"></span><br><span class="line">#  查看库 </span><br><span class="line">1、查看有哪些数据库 </span><br><span class="line">show databases;</span><br><span class="line"></span><br><span class="line">2、显示数据库的详细属性信息</span><br><span class="line">语法:desc database [extended] dbname; </span><br><span class="line">示例:desc database extended myhive;</span><br><span class="line"></span><br><span class="line">3、查看正在使用哪个库 </span><br><span class="line">select current_database();</span><br><span class="line"></span><br><span class="line">4、查看创建库的详细语句 </span><br><span class="line">show create database mydb;</span><br><span class="line"></span><br><span class="line">5、查看以xx开头的库</span><br><span class="line">show databases like &apos;s*&apos;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#  删除库 </span><br><span class="line">删除库操作:</span><br><span class="line">drop database dbname;</span><br><span class="line">drop database if exists dbname;</span><br><span class="line"></span><br><span class="line">默认情况下，hive 不允许删除包含表的数据库，有两种解决办法:</span><br><span class="line">1、 手动删除库下所有表，然后删除库</span><br><span class="line">2、 使用 cascade 关键字</span><br><span class="line">drop database if exists dbname cascade;</span><br><span class="line"></span><br><span class="line">    默认情况下就是 restrict（严格模式）, 后2行效果一样</span><br><span class="line">drop database if exists myhive </span><br><span class="line">drop database if exists myhive restrict</span><br><span class="line"></span><br><span class="line">#  切换库 </span><br><span class="line">切换库操作:</span><br><span class="line">- 语法:</span><br><span class="line">use database_name </span><br><span class="line">- 实例:</span><br><span class="line">use myhive;</span><br></pre></td></tr></table></figure><h3 id="表的操作"><a href="#表的操作" class="headerlink" title="表的操作"></a>表的操作</h3><h4 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h4><blockquote><p>语法结构</p></blockquote><p><img src="/var/folders/6l/blvdbwd53hqglz0f09n3sd5c0000gn/T/abnerworks.Typora/image-20180613201729283.png" alt="image-20180613201729283"></p><h5 id="建表语法"><a href="#建表语法" class="headerlink" title="建表语法"></a><strong>建表语法</strong></h5><ul><li><code>CREATE [EXTERNAL] TABLE [IF NOT EXISTS] &lt;table_name&gt;</code><br>创建内部表；添加EXTERNAL参数会创建外部表</li><li><code>(col_name data_type [COMMENT col_comment], ...)</code><br>添加字段和字段描述</li><li><code>[COMMENT table_comment]</code><br>添加表描述</li><li><code>[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]</code><br>指定分区字段和字段描述，分区字段不能为建表字段！</li><li><code>[CLUSTERED BY (col_name, col_name, ...)]  SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]</code><ul><li>指定分桶字段，分桶字段必须为建表字段！</li><li>指定排序字段，此字段也必须为建表字段，指定的是分桶内的排序</li><li>指定分桶个数（hash后再取模）</li></ul></li><li><code>[ROW FORMAT row_format]</code><br>指定分隔符，row_format 格式：<pre><code>列分隔符：`delimited fields terminated by ‘x&apos;`行分隔符：`line terminated by ‘x&apos;`</code></pre></li><li><code>[STORED AS file_format]</code><br>指定存储格式<pre><code>textfile：文本格式，默认rcfile：行列结合格式parquet：压缩格式</code></pre></li><li><code>[LOCATION hdfs_path]</code><br>指定表在hsfs上的存储路径，不指定的话就按配置的路径存储，如果也没指定就在hive默认的路经 <code>/user/hive/warehouse</code></li></ul><h5 id="建表代码"><a href="#建表代码" class="headerlink" title="建表代码"></a>建表代码</h5><ul><li>a. 创建内部表<ul><li><code>create table mytable (id int, name string) row format delimited fields terminated by &#39;,&#39; stored as textfile;</code></li></ul></li><li>b. 创建外部表<ul><li><code>create  external table mytable2 (id int, name string) row format delimited  fields terminated by &#39;,&#39; location &#39;/user/hive/warehouse/mytable2&#39;;</code></li></ul></li><li>c. 创建分区表<ul><li><code>create  table table3(id int, name string) partitioned by(sex string) row format  delimited fields terminated by &#39;,&#39; stored as textfile;</code></li><li>插入分区数据：<code>load data local inpath &#39;/root/hivedata/mingxing.txt&#39; overwrite into table mytable3 partition(sex=&#39;girl’);</code></li><li>查询表分区： <code>show partitions mytable3</code></li></ul></li><li>d. 创建分桶表<ul><li><code>create  table stu_buck(Sno int,Sname string,Sex string,Sage int,Sdept string)  clustered by(Sno) sorted by(Sno DESC) into 4 buckets row format  delimited fields terminated by &#39;,’;</code></li></ul></li><li>e. 复制表<ul><li><code>create [external] table [if not exists] new_table like table_name;</code></li></ul></li><li>f. 查询表<ul><li><code>create table table_a as select * from  teble_b;</code></li></ul></li></ul><h4 id="查看表"><a href="#查看表" class="headerlink" title="查看表"></a>查看表</h4><ul><li><code>desc &lt;table_name&gt;</code>：显示表的字段信息</li><li><code>desc formatted &lt;table_name&gt;</code>：格式化显示表的详细信息</li><li><code>desc extended &lt;table_name&gt;</code>：显示表的详细信息</li></ul><h4 id="修改表"><a href="#修改表" class="headerlink" title="修改表"></a>修改表</h4><ul><li>重命名<ul><li><code>ALTER TABLE old_name RENAME TO new_name</code></li></ul></li><li>修改属性<ul><li><code>ALTER TABLE table_name SET TBLPROPERTIES (&#39;comment&#39; = &#39;my new students table’);</code></li><li>不支持修改表名，和表的数据存储目录</li></ul></li><li>增加/修改/替换字段<ul><li><code>ALTER TABLE table_name ADD COLUMNS (col_spec [, col_spec ...])</code><br>新增的字段位置在所有列后面 ( partition 列前 )</li><li><code>ALTER TABLE table_name CHANGE c_name new_c_name new_c_type [FIRST | AFTER c_name]</code><br>注意修改字段时，类型只能由小类型转为大类型，不让回报错；（在hive1.2.2中并没有此限制）</li><li><code>ALTER TABLE table_name REPLACE COLUMNS (col_spec [, col_spec ...])</code><br>REPLACE 表示替换表中所有字段</li></ul></li><li>添加/删除分区<ul><li>添加分区：<code>ALTER TABLE table_name ADD [IF NOT EXISTS]  PARTITION (partition_col = col_value1 [ ... ] ) [LOCATION &#39;location1’]</code></li><li>删除分区：<code>ALTER TABLE table_name DROP [IF EXISTS] PARTITION (partition_col = col_value1 [ ... ] )</code></li><li>修改分区路径：<code>ALTER TABLE student_p PARTITION (part=&#39;bb&#39;) SET LOCATION &#39;/myhive_bbbbb’;</code></li><li><strong>[补充：]</strong><ul><li>1、 防止分区被删除：alter table student_p partition (part=’aa’) enable no_drop;</li><li>2、 防止分区被查询：alter table student_p partition (part=’aa’) enable offline;<br>enable 和 disable 是反向操作 </li></ul></li></ul></li></ul><h4 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h4><ul><li><code>drop table if exists &lt;table_name&gt;;</code></li></ul><h4 id="清空表"><a href="#清空表" class="headerlink" title="清空表"></a>清空表</h4><p>会保留表结构</p><ul><li><code>truncate table table_name;</code></li><li><code>truncate table table_name partition(city=&#39;beijing’);</code></li></ul><h4 id="其他辅助命令"><a href="#其他辅助命令" class="headerlink" title="其他辅助命令"></a>其他辅助命令</h4><ul><li>a. 查看数据库列表<ul><li><code>show databases;</code></li><li><code>show databases like &#39;my*&#39;;</code></li></ul></li><li>b. 查看数据表<ul><li><code>show tables;</code></li><li><code>show tables in db_name;</code></li></ul></li><li>c. 查看数据表的建表语句<ul><li><code>show create table table_name;</code></li></ul></li><li>d. 查看 hive 函数列表<ul><li><code>show functions;</code></li></ul></li><li>e. 查看 hive 表的分区<ul><li><code>show partitions table_name;</code></li><li><code>show partitions table_name partition(city=&#39;beijing&#39;)</code></li></ul></li><li>f. 查看表的详细信息（元数据信息） <ul><li><code>desc table_name;</code></li><li><code>desc extended table_name;</code></li></ul></li><li>g. 查看数据库的详细属性信息<ul><li><code>desc formatted table_name;</code></li><li><code>desc database db_name; desc database extended db_name;</code></li></ul></li><li>h. 清空数据表<ul><li><code>truncate table table_name;</code></li></ul></li></ul><p><br></p><h1 id="Hive-–-DML"><a href="#Hive-–-DML" class="headerlink" title="Hive – DML"></a>Hive – DML</h1><h3 id="装载数据"><a href="#装载数据" class="headerlink" title="装载数据"></a>装载数据</h3><ul><li><code>LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE table_name [PARTITION (partcol1=val1, partcol2=val2 ...)]</code></li><li>注意：<ul><li>LOAD 操作只是单纯的 复制（本地文件）或者 移动（hdfs文件，一般是公共数据，需要建立外部表）操作，将数据文件移动到 Hive 表对应的位置</li><li>如果指定了 LOCAL 就去本地文件系统中查找，否则按 inpath 中的 uri 在 hdfs 上查找</li><li>inpath 子句中的文件路径下，不能再有文件夹</li><li>如果使用了 OVERWRITE 关键字，则目标表（或者分区）中的内容会被删除，然后再将 filepath 指向的文件/目录中的内容添加到表/分区中。 </li><li>如果目标表（分区）已经有一个文件，并且文件名和 filepath 中的文件名冲突，那么现有的文件会被新文件所替代。???不是自动重命名为xxx_copy_1</li></ul></li></ul><h3 id="插入数据"><a href="#插入数据" class="headerlink" title="插入数据"></a>插入数据</h3><ul><li><p>a. 单条插入</p><ul><li><code>INSERT INTO TABLE table_name VALUES(value1, value2, ...);</code></li></ul></li><li><p>b. 单重插入</p><ul><li><code>INSERT INTO TABLE table_name [PARTITION (partcol1=val1, ...)] &lt;select_statement1 FROM from_statement&gt;</code></li></ul></li><li><p>c. 多重插入</p><ul><li>FROM from_statement<br>从基表中按不同的字段查询得到的结果分别插入不同的 hive 表<br>只会扫描一次基表，提高查询性能</li><li><code>INSERT INTO TABLE table_name1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 [WHERE where_statement]</code></li><li><code>INSERT INTO TABLE table_name2 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement2 [WHERE where_statement]</code></li><li>[ … ];</li></ul></li><li><p>d. 分区插入</p><ul><li><p>分区插入有两种：一种是静态分区，另一种是动态分区。</p></li><li><p>如果混合使用静态分区和动态分区， 则静态分区必须出现在动态分区之前。</p></li><li><p>静态分区</p><ul><li>A)、创建静态分区表 </li><li>B)、从查询结果中导入数据（单重插入）<br>记得加载数据前要添加分区，然后指定要加载到那个分区</li><li>C)、查看插入结果 </li></ul></li><li><p>动态分区</p><ul><li><p>静态分区添加数据前需要指定分区，当分区个数不确定的时候就很不方便了，这个时候可以使用动态分区</p></li><li><p>重要且常用，尤其是按照日期分区时！</p></li><li><p>A)、创建分区表 </p></li><li><p>B)、参数设置</p></li></ul></li></ul></li></ul><pre><code>  hive-1.2版本  -  set hive.exec.dynamic.partition=true;   //动态分区开启状态，默认开启  -  set hive.exec.dynamic.partition.mode=nonstrict;   //动态分区执行模式，默认&quot;strict&quot;，在这种模式 下要求至少有一列分区字段是静态的，这有助于阻止因设计错误导致查询产生大量的分区  - \# 可选设置项    如果这些参数被更改了又想还原，则执行一次 reset 命令即可    - set hive.exec.max.dynamic.partitions.pernode=100;   //每个节点生成动态分区最大个数     - set hive.exec.max.dynamic.partitions=1000;   //生成动态分区最大个数，如果自动分区数大于这个参数，将会报错     - set hive.exec.max.created.files=100000;   //一个任务最多可以创建的文件数目     - set dfs.datanode.max.xcievers=4096;   //限定一次最多打开的文件数 set     - hive.error.on.empty.partition=false;   //表示当有空分区产生时，是否抛出异常- C)、动态数据插入   - 单个分区字段    - insert into table test2 partition (age) select name,address,school,age from students;   - 多个分区字段    多重分区中目录结构是按照分区字段顺序进行划分的    - insert into table student_ptn2 partition(department, age) select id, name, sex, department,age from students;      分区字段都是动态的    - insert into table student_ptn2 partition(city=&apos;sa&apos;, zipcode) select id, name, sex, age, department, department as zipcode from students;      第一个分区字段时静态的，第二字department字段动态的，重命名为zipcode？  - [注意：]    - 查询语句 select 查询出来的动态分区 age 和 zipcode 必须放在 最后，和分区字段对应，不然结果会出错- D)、查看插入结果  - select * from student_ptn2 where city=&apos;sa&apos; and zipcode=&apos;MA&apos;;</code></pre><ul><li><p>e. 分桶插入</p><ul><li>A)、创建分桶表</li><li>B)、从查询结果中导入数据<br>只能使用insert方式</li><li>C)、查看插入结果 </li><li># 几个命令<ul><li>set hive.exec.reducers.bytes.per.reducer=<number>  // 设置每个reducer的吞吐量，单位byte，默认256M</number></li><li>set hive.exec.reducers.max=<number>  //reduceTask最多执行个数，默认1009</number></li><li>set mapreduce.job.reduces=<number>   //设置reducetask实际运行数，默认-1，代表没有设置，即reducetask默认数为1</number></li><li>set hive.exec.mode.local.auto=true //设置hive本地模式</li></ul></li></ul></li></ul><h3 id="导出数据（了解）"><a href="#导出数据（了解）" class="headerlink" title="导出数据（了解）"></a>导出数据（了解）</h3><ul><li>单模式导出<ul><li><code>INSERT OVERWRITE [LOCAL] DIRECTORY directory1 &lt;select_statement&gt;;</code></li></ul></li><li>多模式导出<ul><li><code>FROM from_statement</code></li><li><code>INSERT OVERWRITE [LOCAL] DIRECTORY directory1 &lt;select_statement1&gt;</code></li><li><code>[INSERT OVERWRITE [LOCAL] DIRECTORY directory2 &lt;select_statement2&gt;] ...</code></li></ul></li></ul><h3 id="查询数据"><a href="#查询数据" class="headerlink" title="查询数据"></a>查询数据</h3><ul><li>Hive 中的 SELECT 基础语法和标准 SQL 语法基本一致，支持 WHERE、DISTINCT、GROUP BY、 ORDER BY、HAVING、LIMIT、子查询等；<ul><li>1、<code>select * from db.table1</code><br>虽然可以，但是要尽量避免 select * 这样的全表扫描操作，效率太低又费时</li><li>2、<code>select count(distinct uid) from db.table1</code></li><li>3、支持 select、union all、join（left、right、full join）、like、where、having、各种聚合函数、 支持 json 解析 </li><li>4、UDF/ UDAF/UDTF <ul><li>UDF：User Defined Function，自定义函数，一对一</li><li>UDAF：User Defined Aggregate Function，自定义聚合函数，多对一，如sum()，count()</li><li>UDTF ：User Defined Table Function，自定义表函数，一对多，如explode()</li></ul></li><li>5、不支持 update 和 delete </li><li>6、hive 虽然支持 in/exists（老版本是不支持的），但是 hive 推荐使用 semi join 的方式来代替 实现，而且效率更高。 <ul><li>半连接<ul><li>左半连接：left semi join，以左表为基表，右表有的，只显示左表相应记录（即一半）</li><li>右半连接：right semi join，与左半连接相反</li></ul></li><li>内连接：inner join，两表中都有的才会连接</li><li>外连接<ul><li>左外连接：left outer join，坐标为基表，右表有的会关联，右表没有的以null表示；左表没有右表有的不会关联</li><li>右外连接：right outer join，与左外连接相反</li><li>全外连接：full outer join，两表合并</li></ul></li></ul></li><li>7、支持 case … when …</li></ul></li><li><strong>语法结构</strong><ul><li><code>SELECT [ALL | DISTINCT] select_ condition, select_ condition, ...</code></li><li>FROM table_name a </li><li>[JOIN table_other b ON <a href="http://a.id" target="_blank" rel="noopener">a.id</a> = <a href="http://b.id" target="_blank" rel="noopener">b.id</a>]<br>表连接</li><li>[WHERE where_condition]<br>过滤条件</li><li>[GROUP BY col_list [HAVING condition]]<br>分组条件</li><li>[CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY col_list | ORDER BY col_list] [DESC]]<br>排序条件：<br>\1. order by：全局排序，默认升序，DESC表示降序。只有一个 reduce task 的结果，比如文<br>件名是 000000_0，会导致当输入规模较大时，需要较长的计算时间。<br>\2.  sort by：局部排序，其在数据进入 reducer 前完成排序。因此，如果用 sort by 进行排序，并且设置  mapred.reduce.tasks &gt; 1，则 sort by 只保证每个 reducer 的输出有序，不保证全局有序。<br>\3. distribute by：类似于分桶，根据指定的字段将数据分到不同的 reducer，且分发算法是 hash 散列<br>\4. cluster by：除了具有 Distribute by 的功能外，还会对该字段进行排序。<br>注意：如果 distribute 和 sort 字段是同一个时，cluster by = distribute by + sort by；<br>如果分桶字段和排序字段不一样，那么就不能使用 clustered by </li><li>[LIMIT number]<br>显示结果的前几个记录</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hive简介&quot;&gt;&lt;a href=&quot;#Hive简介&quot; class=&quot;headerlink&quot; title=&quot;Hive简介&quot;&gt;&lt;/a&gt;Hive简介&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/study/2018-
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/categories/Hadoop/Hive/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>_数据分析系统——Hive</title>
    <link href="https://airpoet.github.io/2018/06/12/Hadoop/Study/3-Hive/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9FHive/"/>
    <id>https://airpoet.github.io/2018/06/12/Hadoop/Study/3-Hive/数据分析系统Hive/</id>
    <published>2018-06-11T16:42:14.342Z</published>
    <updated>2018-06-11T23:26:01.191Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hive背景及应用场景"><a href="#Hive背景及应用场景" class="headerlink" title="Hive背景及应用场景"></a>Hive背景及应用场景</h1><h2 id="Hive是什么？"><a href="#Hive是什么？" class="headerlink" title="Hive是什么？"></a>Hive是什么？</h2><p><strong>看一下MR的 wordcount 和 Hive 的 wordcount</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173251.png" alt="image-20180612013251049"></p><p><br></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173355.png" alt="image-20180612013349514"></p><h2 id="Hive典型应用场景"><a href="#Hive典型应用场景" class="headerlink" title="Hive典型应用场景"></a>Hive典型应用场景</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173443.png" alt="image-20180612013442768"></p><p><br></p><h2 id="为什么使用Hive？"><a href="#为什么使用Hive？" class="headerlink" title="为什么使用Hive？"></a>为什么使用Hive？</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173532.png" alt="image-20180612013531999"></p><p><br></p><h2 id="有了Hive，还需要自己写MR程序吗？"><a href="#有了Hive，还需要自己写MR程序吗？" class="headerlink" title="有了Hive，还需要自己写MR程序吗？"></a>有了Hive，还需要自己写MR程序吗？</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173601.png" alt="image-20180612013600542"></p><p><br></p><p><br></p><h1 id="Hive基本架构"><a href="#Hive基本架构" class="headerlink" title="Hive基本架构"></a>Hive基本架构</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173645.png" alt="image-20180612013645384"></p><h2 id="Hive各模块组成"><a href="#Hive各模块组成" class="headerlink" title="Hive各模块组成"></a>Hive各模块组成</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173742.png" alt="image-20180612013742424"></p><p><br></p><h2 id="Hive部署架构-实验环境"><a href="#Hive部署架构-实验环境" class="headerlink" title="Hive部署架构-实验环境"></a>Hive部署架构-实验环境</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173814.png" alt="image-20180612013813506"></p><p><br></p><h2 id="Hive部署架构-生产环境"><a href="#Hive部署架构-生产环境" class="headerlink" title="Hive部署架构-生产环境"></a>Hive部署架构-生产环境</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173838.png" alt="image-20180612013837922"></p><p><br></p><h2 id="Hive部署架构-metastore服务"><a href="#Hive部署架构-metastore服务" class="headerlink" title="Hive部署架构-metastore服务"></a>Hive部署架构-metastore服务</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173904.png" alt="image-20180612013903730"></p><p><br></p><p><br></p><h1 id="Hive使用方式"><a href="#Hive使用方式" class="headerlink" title="Hive使用方式"></a>Hive使用方式</h1><h2 id="CLI（Command-Line-Interface）"><a href="#CLI（Command-Line-Interface）" class="headerlink" title="CLI（Command Line Interface）"></a>CLI（Command Line Interface）</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173952.png" alt="image-20180612013951537"></p><p><br></p><h2 id="CLI—hive外部资源"><a href="#CLI—hive外部资源" class="headerlink" title="CLI—hive外部资源"></a>CLI—hive外部资源</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174023.png" alt="image-20180612014022323"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174103.png" alt="image-20180612014102686"></p><p><br></p><h2 id="Hive-Web-UI"><a href="#Hive-Web-UI" class="headerlink" title="Hive Web UI"></a>Hive Web UI</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174201.png" alt="image-20180612014200724"></p><p><br></p><h2 id="Hive客户端程序"><a href="#Hive客户端程序" class="headerlink" title="Hive客户端程序"></a>Hive客户端程序</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174222.png" alt="image-20180612014221547"></p><p><br></p><p><br></p><h1 id="HQL查询语句"><a href="#HQL查询语句" class="headerlink" title="HQL查询语句"></a>HQL查询语句</h1><h2 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174257.png" alt="image-20180612014257010"></p><p><br></p><h2 id="数据类型（不断增加中……）"><a href="#数据类型（不断增加中……）" class="headerlink" title="数据类型（不断增加中……）"></a>数据类型（不断增加中……）</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174340.png" alt="image-20180612014340830"></p><p><br></p><ul><li><strong>自有的特殊类型</strong></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174412.png" alt="image-20180612014411860"></p><p><br></p><h2 id="数据定义语句（DDL）"><a href="#数据定义语句（DDL）" class="headerlink" title="数据定义语句（DDL）"></a>数据定义语句（DDL）</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174515.png" alt="image-20180612014514763"></p><p><br></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174533.png" alt="image-20180612014533008"></p><p><br></p><p><strong>分隔符开发中这样写</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174607.png" alt="image-20180612014606665"></p><p><br></p><h2 id="Hive-Partition与Bucket"><a href="#Hive-Partition与Bucket" class="headerlink" title="Hive Partition与Bucket"></a>Hive Partition与Bucket</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174853.png" alt="image-20180612014852827"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174912.png" alt="image-20180612014912295"></p><p><br></p><p><br></p><h2 id="Hive数据格式"><a href="#Hive数据格式" class="headerlink" title="Hive数据格式"></a>Hive数据格式</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174952.png" alt="image-20180612014951697"></p><h3 id="应用举例：日志清理"><a href="#应用举例：日志清理" class="headerlink" title="应用举例：日志清理"></a>应用举例：日志清理</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175053.png" alt="image-20180612015053452"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175104.png" alt="image-20180612015104125"></p><p><br></p><p><br></p><h2 id="数据操作语句（DML）"><a href="#数据操作语句（DML）" class="headerlink" title="数据操作语句（DML）"></a>数据操作语句（DML）</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175153.png" alt="image-20180612015153237"></p><h3 id="数据加载与插入语句"><a href="#数据加载与插入语句" class="headerlink" title="数据加载与插入语句"></a>数据加载与插入语句</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175226.png" alt="image-20180612015225942"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175246.png" alt="image-20180612015245860"></p><h3 id="几个实例"><a href="#几个实例" class="headerlink" title="几个实例"></a>几个实例</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175318.png" alt="image-20180612015317619"></p><h2 id="数据查询语句SELECT"><a href="#数据查询语句SELECT" class="headerlink" title="数据查询语句SELECT"></a>数据查询语句SELECT</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175356.png" alt="image-20180612015355128"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175419.png" alt="image-20180612015419432"></p><p><br></p><h2 id="两种分布式Join算法"><a href="#两种分布式Join算法" class="headerlink" title="两种分布式Join算法"></a>两种分布式Join算法</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175511.png" alt="image-20180612015511444"></p><p><br></p><h2 id="HQL中两种特殊Join"><a href="#HQL中两种特殊Join" class="headerlink" title="HQL中两种特殊Join"></a>HQL中两种特殊Join</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175606.png" alt="image-20180612015606096"></p><p><br></p><h2 id="Order-By与Sort-By"><a href="#Order-By与Sort-By" class="headerlink" title="Order By与Sort By"></a>Order By与Sort By</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-231641.png" alt="image-20180612071641233"></p><p><br></p><h2 id="Distribute-by与Cluster-by"><a href="#Distribute-by与Cluster-by" class="headerlink" title="Distribute by与Cluster by"></a>Distribute by与Cluster by</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-231718.png" alt="image-20180612071718342"></p><p><br></p><h2 id="Transform语法（Streaming）"><a href="#Transform语法（Streaming）" class="headerlink" title="Transform语法（Streaming）"></a>Transform语法（Streaming）</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-231750.png" alt="image-20180612071749235"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-231813.png" alt="image-20180612071813456"></p><p><br></p><h2 id="用户自定义函数（UDF）"><a href="#用户自定义函数（UDF）" class="headerlink" title="用户自定义函数（UDF）"></a>用户自定义函数（UDF）</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-231839.png" alt="image-20180612071839711"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-231921.png" alt="image-20180612071920788"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-231940.png" alt="image-20180612071940199"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232009.png" alt="image-20180612072009048"></p><p>UDAF</p><ul><li><a href="http://svn.apache.org/repos/asf/hive/branches/branch0.13/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFAverage.java" target="_blank" rel="noopener">http://svn.apache.org/repos/asf/hive/branches/branch0.13/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFAverage.java</a></li></ul><p>UDTF</p><ul><li><a href="http://svn.apache.org/repos/asf/hive/branches/branch0.13/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFExplode.java" target="_blank" rel="noopener">http://svn.apache.org/repos/asf/hive/branches/branch0.13/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFExplode.java</a></li></ul><p><br></p><h2 id="其它问题"><a href="#其它问题" class="headerlink" title="其它问题"></a>其它问题</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232136.png" alt="image-20180612072136290"></p><p><br></p><h2 id="Hive-On-HBase"><a href="#Hive-On-HBase" class="headerlink" title="Hive On HBase"></a>Hive On HBase</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232225.png" alt="image-20180612072225509"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232245.png" alt="image-20180612072244391"></p><h1 id="Hive总结及其类似开源系统"><a href="#Hive总结及其类似开源系统" class="headerlink" title="Hive总结及其类似开源系统"></a>Hive总结及其类似开源系统</h1><h2 id="Stinger"><a href="#Stinger" class="headerlink" title="Stinger"></a>Stinger</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232321.png" alt="image-20180612072320862"></p><p><br></p><h2 id="Hive-MR-vs-Hive-Tez"><a href="#Hive-MR-vs-Hive-Tez" class="headerlink" title="Hive-MR vs Hive-Tez"></a>Hive-MR vs Hive-Tez</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232350.png" alt="image-20180612072350053"></p><p><br></p><h2 id="Shark"><a href="#Shark" class="headerlink" title="Shark"></a>Shark</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232415.png" alt="image-20180612072415680"></p><p><br></p><h2 id="Impala"><a href="#Impala" class="headerlink" title="Impala"></a>Impala</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232439.png" alt="image-20180612072438860"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232454.png" alt="image-20180612072454317"></p><p><br></p><h2 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a>性能比较</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232528.png" alt="image-20180612072528668"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232548.png" alt="image-20180612072547727"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hive背景及应用场景&quot;&gt;&lt;a href=&quot;#Hive背景及应用场景&quot; class=&quot;headerlink&quot; title=&quot;Hive背景及应用场景&quot;&gt;&lt;/a&gt;Hive背景及应用场景&lt;/h1&gt;&lt;h2 id=&quot;Hive是什么？&quot;&gt;&lt;a href=&quot;#Hive是什么？&quot;
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/categories/Hadoop/Hive/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>偶然发现的好歌</title>
    <link href="https://airpoet.github.io/2018/06/11/Songs/%E5%81%B6%E7%84%B6%E5%8F%91%E7%8E%B0%E7%9A%84%E5%A5%BD%E6%AD%8C/"/>
    <id>https://airpoet.github.io/2018/06/11/Songs/偶然发现的好歌/</id>
    <published>2018-06-11T11:04:05.646Z</published>
    <updated>2018-06-11T11:32:43.834Z</updated>
    
    <content type="html"><![CDATA[ <blockquote class="blockquote-center"><h1 id="Dealbreaker"><a href="#Dealbreaker" class="headerlink" title="Dealbreaker"></a>Dealbreaker</h1><ul><li>专辑：<a href="orpheus://orpheus/pub/app.html#/m/album/?id=1715512" target="_blank" rel="noopener">Chesapeake</a></li><li>歌手：<a href="orpheus://orpheus/pub/app.html#/m/artist/?id=72720" target="_blank" rel="noopener">Rachael Yamagata</a></li><li>链接：<a href="http://music.163.com/#/m/song?id=18733198" target="_blank" rel="noopener">http://music.163.com/#/m/song?id=18733198</a></li></ul><p><br></p><h1 id="You-Won’t-Let-Me"><a href="#You-Won’t-Let-Me" class="headerlink" title="You Won’t Let Me"></a>You Won’t Let Me</h1><ul><li>专辑：<a href="orpheus://orpheus/pub/app.html#/m/album/?id=1715512" target="_blank" rel="noopener">Chesapeake</a></li><li>歌手：<a href="orpheus://orpheus/pub/app.html#/m/artist/?id=72720" target="_blank" rel="noopener">Rachael Yamagata</a></li><li>链接：<a href="http://music.163.com/#/song?id=18733192" target="_blank" rel="noopener">http://music.163.com/#/song?id=18733192</a></li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      
      
         &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h1 id=&quot;Dealbreaker&quot;&gt;&lt;a href=&quot;#Dealbreaker&quot; class=&quot;headerlink&quot; title=&quot;Dealbreaker&quot;&gt;&lt;/a&gt;Dealbreaker&lt;/h
      
    
    </summary>
    
      <category term="文艺" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/"/>
    
      <category term="Songs" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/Songs/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="文艺" scheme="https://airpoet.github.io/tags/%E6%96%87%E8%89%BA/"/>
    
      <category term="Songs" scheme="https://airpoet.github.io/tags/Songs/"/>
    
  </entry>
  
  <entry>
    <title>_HDFS应用场景&amp;原理&amp;基本架构及使用方法</title>
    <link href="https://airpoet.github.io/2018/06/11/Hadoop/Study/1-HDFS/HDFS%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF&amp;%E5%8E%9F%E7%90%86&amp;%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84%E5%8F%8A%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <id>https://airpoet.github.io/2018/06/11/Hadoop/Study/1-HDFS/HDFS应用场景&amp;原理&amp;基本架构及使用方法/</id>
    <published>2018-06-11T00:01:43.146Z</published>
    <updated>2018-06-11T16:27:43.347Z</updated>
    
    <content type="html"><![CDATA[<h1 id="HDFS基本架构和原理"><a href="#HDFS基本架构和原理" class="headerlink" title="HDFS基本架构和原理"></a>HDFS基本架构和原理</h1><h2 id="HDFS设计思想"><a href="#HDFS设计思想" class="headerlink" title="HDFS设计思想"></a>HDFS设计思想</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-142401.png" alt="image-20180611222401493"></p><h2 id="HDFS架构"><a href="#HDFS架构" class="headerlink" title="HDFS架构"></a>HDFS架构</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-142442.png" alt="image-20180611222442066"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-142505.png" alt="image-20180611222505133"></p><h2 id="HDFS数据块（block）"><a href="#HDFS数据块（block）" class="headerlink" title="HDFS数据块（block）"></a>HDFS数据块（block）</h2><ul><li><strong>注意： Hadoop2.x，block默认大小是128MB</strong></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-142646.png" alt="image-20180611222645888"></p><h2 id="HDFS写流程"><a href="#HDFS写流程" class="headerlink" title="HDFS写流程"></a>HDFS写流程</h2><ul><li>创建Distributed FileSystem类 </li><li>询问 NameNode 要写的文件对否存在</li><li>不存在就写入到 FSDataOutputStream 流中</li><li>流写出去到一个 DataNode</li><li>…</li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-143228.png" alt="image-20180611223228053"></p><h2 id="HDFS读流程"><a href="#HDFS读流程" class="headerlink" title="HDFS读流程"></a>HDFS读流程</h2><ul><li>客户端向 NameNode 询问 block 的位置</li><li>按照客户端按照拿到的位置，向不同的DataNode 请求数据</li><li>……</li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-143510.png" alt="image-20180611223510239"></p><h2 id="HDFS典型物理拓扑"><a href="#HDFS典型物理拓扑" class="headerlink" title="HDFS典型物理拓扑"></a>HDFS典型物理拓扑</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-143914.png" alt="image-20180611223914168"></p><h2 id="HDFS副本放置策略"><a href="#HDFS副本放置策略" class="headerlink" title="HDFS副本放置策略"></a>HDFS副本放置策略</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-143940.png" alt="image-20180611223939952"></p><h2 id="HDFS可靠性策略"><a href="#HDFS可靠性策略" class="headerlink" title="HDFS可靠性策略"></a>HDFS可靠性策略</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-144153.png" alt="image-20180611224152799"></p><h2 id="HDFS不适合存储小文件"><a href="#HDFS不适合存储小文件" class="headerlink" title="HDFS不适合存储小文件"></a>HDFS不适合存储小文件</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-144344.png" alt="image-20180611224344481"></p><p><br></p><h1 id="HDFS程序设计"><a href="#HDFS程序设计" class="headerlink" title="HDFS程序设计"></a>HDFS程序设计</h1><h2 id="HDFS访问方式"><a href="#HDFS访问方式" class="headerlink" title="HDFS访问方式"></a>HDFS访问方式</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-144935.png" alt="image-20180611224934785"></p><h2 id="HDFS-Shell命令"><a href="#HDFS-Shell命令" class="headerlink" title="HDFS Shell命令"></a>HDFS Shell命令</h2><h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145132.png" alt="image-20180611225131966"></p><h3 id="HDFS-Shell命令—文件操作命令"><a href="#HDFS-Shell命令—文件操作命令" class="headerlink" title="HDFS Shell命令—文件操作命令"></a>HDFS Shell命令—文件操作命令</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145219.png" alt="image-20180611225219211"></p><h3 id="HDFS-Shell命令—文件操作命令-1"><a href="#HDFS-Shell命令—文件操作命令-1" class="headerlink" title="HDFS Shell命令—文件操作命令"></a>HDFS Shell命令—文件操作命令</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145604.png" alt="image-20180611225604218"></p><h3 id="HDFS-Shell命令—管理命令"><a href="#HDFS-Shell命令—管理命令" class="headerlink" title="HDFS Shell命令—管理命令"></a>HDFS Shell命令—管理命令</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145728.png" alt="image-20180611225727910"></p><h3 id="HDFS-Shell命令—管理脚本"><a href="#HDFS-Shell命令—管理脚本" class="headerlink" title="HDFS Shell命令—管理脚本"></a>HDFS Shell命令—管理脚本</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145803.png" alt="image-20180611225802894"></p><h3 id="HDFS-Shell命令—文件管理命令fsck"><a href="#HDFS-Shell命令—文件管理命令fsck" class="headerlink" title="HDFS Shell命令—文件管理命令fsck"></a>HDFS Shell命令—文件管理命令fsck</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145932.png" alt="image-20180611225931880"></p><ul><li><strong>查看帮助</strong></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145915.png" alt="image-20180611225914482"></p><ul><li><strong>用法示例</strong></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-150253.png" alt="image-20180611230252655"></p><h3 id="HDFS-Shell命令—数据均衡器balancer"><a href="#HDFS-Shell命令—数据均衡器balancer" class="headerlink" title="HDFS Shell命令—数据均衡器balancer"></a>HDFS Shell命令—数据均衡器balancer</h3><ul><li><strong>一般设置10% —— 15% 就差不多了</strong></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-150614.png" alt="image-20180611230613854"></p><h3 id="HDFS-Shell命令—设置目录份额"><a href="#HDFS-Shell命令—设置目录份额" class="headerlink" title="HDFS Shell命令—设置目录份额"></a>HDFS Shell命令—设置目录份额</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-150704.png" alt="image-20180611230703142"></p><h3 id="※-HDFS-Shell命令—增加-移除节点-※"><a href="#※-HDFS-Shell命令—增加-移除节点-※" class="headerlink" title="※ HDFS Shell命令—增加/移除节点 ※"></a>※ HDFS Shell命令—增加/移除节点 ※</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-150741.png" alt="image-20180611230741126"></p><h2 id="HDFS-Java"><a href="#HDFS-Java" class="headerlink" title="HDFS Java"></a>HDFS Java</h2><h3 id="API介绍"><a href="#API介绍" class="headerlink" title="API介绍"></a>API介绍</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-150953.png" alt="image-20180611230953359"></p><h3 id="HDFS-Java程序举例"><a href="#HDFS-Java程序举例" class="headerlink" title="HDFS Java程序举例"></a>HDFS Java程序举例</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-151048.png" alt="image-20180611231047354"></p><h2 id="HDFS-多语言API—借助thrift"><a href="#HDFS-多语言API—借助thrift" class="headerlink" title="HDFS 多语言API—借助thrift"></a>HDFS 多语言API—借助thrift</h2><p><a href="http://thrift.apache.org/" target="_blank" rel="noopener">也是Apache的顶级项目</a></p><h3 id="thrift执行流程"><a href="#thrift执行流程" class="headerlink" title="thrift执行流程"></a>thrift执行流程</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-151149.png" alt="image-20180611231148955"></p><h3 id="hadoopfs-thrift接口定义"><a href="#hadoopfs-thrift接口定义" class="headerlink" title="hadoopfs.thrift接口定义"></a>hadoopfs.thrift接口定义</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-151634.png" alt="image-20180611231634046"></p><h3 id="PHP语言访问HDFS"><a href="#PHP语言访问HDFS" class="headerlink" title="PHP语言访问HDFS"></a>PHP语言访问HDFS</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-151654.png" alt="image-20180611231653750"></p><h3 id="Python语言访问HDFS"><a href="#Python语言访问HDFS" class="headerlink" title="Python语言访问HDFS"></a>Python语言访问HDFS</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-152052.png" alt="image-20180611232052633"></p><hr><h1 id="Hadoop-2-0新特性"><a href="#Hadoop-2-0新特性" class="headerlink" title="Hadoop 2.0新特性"></a>Hadoop 2.0新特性</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-152948.png" alt="image-20180611232948514"></p><h2 id="HA-高可用-与Federation-联邦"><a href="#HA-高可用-与Federation-联邦" class="headerlink" title="HA(高可用)与Federation(联邦)"></a>HA(高可用)与Federation(联邦)</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153058.png" alt="image-20180611233058385"></p><h2 id="异构层级存储结构"><a href="#异构层级存储结构" class="headerlink" title="异构层级存储结构"></a>异构层级存储结构</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153117.png" alt="image-20180611233117295"></p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153206.png" alt="image-20180611233206158"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153228.png" alt="image-20180611233227447"></p><h2 id="HDFS-ACL"><a href="#HDFS-ACL" class="headerlink" title="HDFS ACL"></a>HDFS ACL</h2><h3 id="背景：现有权限管理的局限性"><a href="#背景：现有权限管理的局限性" class="headerlink" title="背景：现有权限管理的局限性"></a>背景：现有权限管理的局限性</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153255.png" alt="image-20180611233255196"></p><h3 id="基于POSIX-ACL的实现"><a href="#基于POSIX-ACL的实现" class="headerlink" title="基于POSIX ACL的实现"></a>基于POSIX ACL的实现</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153310.png" alt="image-20180611233310082"></p><h2 id="HDFS快照"><a href="#HDFS快照" class="headerlink" title="HDFS快照"></a>HDFS快照</h2><h3 id="背景-1"><a href="#背景-1" class="headerlink" title="背景"></a>背景</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153338.png" alt="image-20180611233337709"></p><h3 id="基本使用方法"><a href="#基本使用方法" class="headerlink" title="基本使用方法"></a>基本使用方法</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153358.png" alt="image-20180611233357356"></p><h2 id="HDFS缓存"><a href="#HDFS缓存" class="headerlink" title="HDFS缓存"></a>HDFS缓存</h2><h3 id="背景-2"><a href="#背景-2" class="headerlink" title="背景"></a>背景</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153423.png" alt="image-20180611233423131"></p><h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153446.png" alt="image-20180611233446072"></p><h3 id="实现情况"><a href="#实现情况" class="headerlink" title="实现情况"></a>实现情况</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153502.png" alt="image-20180611233502379"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153521.png" alt="image-20180611233521502"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;HDFS基本架构和原理&quot;&gt;&lt;a href=&quot;#HDFS基本架构和原理&quot; class=&quot;headerlink&quot; title=&quot;HDFS基本架构和原理&quot;&gt;&lt;/a&gt;HDFS基本架构和原理&lt;/h1&gt;&lt;h2 id=&quot;HDFS设计思想&quot;&gt;&lt;a href=&quot;#HDFS设计思想&quot;
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="HDFS" scheme="https://airpoet.github.io/categories/Hadoop/HDFS/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="HDFS" scheme="https://airpoet.github.io/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>关于hexo的时序图插件 hexo-filter-sequence 的巨坑</title>
    <link href="https://airpoet.github.io/2018/06/10/Tools/Hexo/%E5%85%B3%E4%BA%8Ehexo%E7%9A%84%E6%97%B6%E5%BA%8F%E5%9B%BE%E6%8F%92%E4%BB%B6-hexo-filter-sequence-%E7%9A%84%E5%B7%A8%E5%9D%91/"/>
    <id>https://airpoet.github.io/2018/06/10/Tools/Hexo/关于hexo的时序图插件-hexo-filter-sequence-的巨坑/</id>
    <published>2018-06-10T13:13:02.149Z</published>
    <updated>2018-06-10T13:19:32.909Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在写代码的过程中，发现时序图还是挺管用的，简洁明了，于是想用一用。</p><p>结果发到站上，不显示。</p><p>在网上查了下，发现是hexo所使用的Markdown渲染语法不支持。</p><p>这里吐槽下，这里渲染的确实烂，作者为啥不改改..</p><p><br></p><p>于是开始找解决方案，发现大多数都推荐了一个叫<code>hexo-filter-sequence</code>的插件，故安装之。</p><p>结果死活还是不行。</p><p>装了其它的几个flow图，却可以显示。</p><p><strong>当flow图和sequence图同时存在的时候，sequence可以显示，但是sequence单独存在时，死活不显示。</strong></p><p>难不成有啥依赖？必须先使用flow才能使用sequence？逻辑上不应该啊！</p><p>但是事实却是这样！</p><p><br></p><p>网上有一篇大神说要改源码才行，开始没注意，觉得应该不会吧，这么多人使用，源码还会有问题？？</p><p><strong>仔细一看源码，妈的！！心里千万匹草泥马呼啸而过！</strong></p><p><strong>把初始化 sequence，写成了初始化 flow！！！</strong></p><p><strong>把 flow 改成 sequence， 再把 js CDN源换成国内的！</strong></p><p><strong>可以了！！</strong></p><p>再仔细一看，发现最后一次更新是在1年前！</p><p>坑爹的作者，浪费了我至少3-5个小时！！</p><p><br></p><hr><h2 id="下面为部分摘抄"><a href="#下面为部分摘抄" class="headerlink" title="下面为部分摘抄"></a>下面为部分摘抄</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p><a href="https://github.com/bubkoo/hexo-filter-sequence" target="_blank" rel="noopener">hexo-filter-sequence</a> 插件:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install --save hexo-filter-sequence</span><br></pre></td></tr></table></figure><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>站点配置文件 <code>_config.yml</code> 中增加如下配置:</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sequence:</span><br><span class="line">  webfont: https:<span class="comment">//cdn.bootcss.com/webfont/1.6.28/webfontloader.js</span></span><br><span class="line">  raphael: https:<span class="comment">//cdn.bootcss.com/raphael/2.2.7/raphael.min.js</span></span><br><span class="line">  underscore: https:<span class="comment">//cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js</span></span><br><span class="line">  sequence: https:<span class="comment">//cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js</span></span><br><span class="line">  css: # optional, the url for css, such as hand drawn theme </span><br><span class="line">  options: </span><br><span class="line">    theme: simple</span><br><span class="line">    css_class:</span><br></pre></td></tr></table></figure><h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><p>源码修改后才能正常使用，进入插件目录作如下修改：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// index.js</span></span><br><span class="line"><span class="keyword">var</span> assign = <span class="built_in">require</span>(<span class="string">'deep-assign'</span>);</span><br><span class="line"><span class="keyword">var</span> renderer = <span class="built_in">require</span>(<span class="string">'./lib/renderer'</span>);</span><br><span class="line"></span><br><span class="line">hexo.config.sequence = assign(&#123;</span><br><span class="line">  webfont: <span class="string">'https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js'</span>,</span><br><span class="line">  raphael: <span class="string">'https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js'</span>,</span><br><span class="line">  underscore: <span class="string">'https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js'</span>,</span><br><span class="line">  sequence: <span class="string">'https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js'</span>,</span><br><span class="line">  css: <span class="string">''</span>,</span><br><span class="line">  options: &#123;</span><br><span class="line">    theme: <span class="string">'simple'</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;, hexo.config.sequence);</span><br><span class="line"></span><br><span class="line">hexo.extend.filter.register(<span class="string">'before_post_render'</span>, renderer.render, <span class="number">9</span>);</span><br></pre></td></tr></table></figure><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// lib/renderer.js, 25 行</span></span><br><span class="line"><span class="keyword">if</span> (sequences.length) &#123;</span><br><span class="line">      <span class="keyword">var</span> config = <span class="keyword">this</span>.config.sequence;</span><br><span class="line">      <span class="comment">// resources</span></span><br><span class="line">      data.content += <span class="string">'&lt;script src="'</span> + config.webfont + <span class="string">'"&gt;&lt;/script&gt;'</span>;</span><br><span class="line">      data.content += <span class="string">'&lt;script src="'</span> + config.raphael + <span class="string">'"&gt;&lt;/script&gt;'</span>;</span><br><span class="line">      data.content += <span class="string">'&lt;script src="'</span> + config.underscore + <span class="string">'"&gt;&lt;/script&gt;'</span>;</span><br><span class="line">      data.content += <span class="string">'&lt;script src="'</span> + config.sequence + <span class="string">'"&gt;&lt;/script&gt;'</span>;</span><br><span class="line">      ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>新建代码块，增加如下内容：</p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-10-131910.png" alt="image-20180610211909603"></p><div id="sequence-0"></div><p><a href="http://wewelove.github.io/fcoder/2017/09/06/markdown-sequence/index.html" target="_blank" rel="noopener">详情参考</a></p><p><script src="https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js"></script><script src="https://cdn.bootcss.com/snap.svg/0.5.1/snap.svg-min.js"></script><script src="https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js"></script><script src="https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js"></script><script src="https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js"></script><textarea id="sequence-0-code" style="display: none">Alice->Bob: Hello Bob, how are you?Note right of Bob: Bob thinksBob-->Alice: I am good thanks!</textarea><textarea id="sequence-0-options" style="display: none">{"theme":"simple"}</textarea><script>  var code = document.getElementById("sequence-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value));  var diagram = Diagram.parse(code);  diagram.drawSVG("sequence-0", options);</script></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在写代码的过程中，发现时序图还是挺管用的，简洁明了，于是想用一用。&lt;/p&gt;
&lt;p&gt;结果发到站上，不显示。&lt;/p&gt;
&lt;p&gt;在网上查了下，发现是
      
    
    </summary>
    
      <category term="工具" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="hexo" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/hexo/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="工具" scheme="https://airpoet.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="hexo" scheme="https://airpoet.github.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce-分组浅探</title>
    <link href="https://airpoet.github.io/2018/06/10/Hadoop/Study/2-MapReduce/MapReduce-%E5%88%86%E7%BB%84%E6%B5%85%E6%8E%A2/"/>
    <id>https://airpoet.github.io/2018/06/10/Hadoop/Study/2-MapReduce/MapReduce-分组浅探/</id>
    <published>2018-06-10T08:49:24.337Z</published>
    <updated>2018-06-11T11:46:23.533Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>最近学分组的时候，一直弄不明白，总感觉一头雾水，通过近3个小时的断点调试和log输出。 大概了解了一些表象，先记录。</p><p>案例是这个</p><blockquote><p>求出每门课程参考学生成绩最高平均分的学生的信息：</p><p>课程，姓名和平均分，详细见<a href="https://airpoet.github.io/2018/06/09/Hadoop/Study/MapReduce%E7%AC%94%E8%AE%B0-%E7%BB%83%E4%B9%A0/"><sup>MapReduce笔记-练习第二题第3小题</sup></a></p><p>数据格式是这样的：</p><blockquote><p>第一个是课程名称，总共四个课程，computer，math，english，algorithm，</p><p>第二个是学生姓名，后面是每次考试的分数</p><p><em>math,huangxiaoming,85,75,85,99,66,88,75,91</em></p><p><em>english,huanglei,85,75,85,99,66,88,75,91</em></p><p>… </p></blockquote></blockquote><p><br></p><p><br></p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><div id="sequence-0"></div><h4 id="执行流程结论"><a href="#执行流程结论" class="headerlink" title="执行流程结论"></a><strong>执行流程结论</strong></h4><ul><li>map每读一行就 write 到 context 一次，按照指定的<code>key</code>进行分发</li><li><p>map 把所有的数据都读完了之后，大概执行到<code>67%</code>的时候，开始进入 <code>CustomBean</code>，执行<code>CustomBean</code>的<code>compareTo()</code>方法，会按照自己写的规则一条一条数据比较</p></li><li><p>上述都比较完毕之后，map阶段就结束了，此时来到了 reduce阶段，但是是到了<code>67%</code>了</p></li><li>到了reduce阶段，直接进入了<strong>MyGroup</strong>中自定义的<strong>compare</strong>方法。</li><li>MyGroup的<code>compare()</code>方法，如果返回<strong>非0</strong>， 就会进入 reduce 方法<strong>写出到context</strong></li></ul><h4 id="MyGroup进入Reduce的条件是"><a href="#MyGroup进入Reduce的条件是" class="headerlink" title="MyGroup进入Reduce的条件是"></a><strong>MyGroup进入Reduce的条件是</strong></h4><ul><li>MyReduce中，如果compare的结果不等于0，也就是比较的2者不相同， 此时就进入Reduce， 写出到上下文</li><li>如果相同，会一直往下读，直到读到不同的， 此时写出读到上下文。</li><li>因为MyGroup会在Reduce阶段执行，而<code>CustomBean</code>中的<code>compareTo()</code>是在map阶段执行，所以需要在<code>CustomBean</code>中就把组排好序，此时<strong>分组功能</strong>才能正常运作</li></ul><h4 id="指定分组类MyGroup和不指定的区别"><a href="#指定分组类MyGroup和不指定的区别" class="headerlink" title="指定分组类MyGroup和不指定的区别"></a>指定分组类MyGroup和不指定的区别</h4><p><em>指定与不指定是指：在Driver类中，是否加上<code>job.setGroupingComparatorClass(MyGrouper.class);</code>这一句。</em></p><ul><li><strong>指定分组类</strong>：<ul><li>会按照分组类中，自定义的<code>compare()</code>方法比较，相同的为一组，分完一组就进入一次reduce方法</li></ul></li><li><strong>不指定分组类：（目前存疑）</strong><ul><li>是否是按照key进行分组</li><li>如果是自定义类为key，是否是按照此key中值相同的分为一组</li><li>如果是hadoop内置类，是否是按照此类的值分组（Text-String的值，IntWritable-int值等..）</li><li><strong>依然是走得以上这套分组逻辑，一组的数据读完才进入到Reduce阶段做归并</strong></li></ul></li></ul><p><br></p><p><br></p><h3 id="Log信息"><a href="#Log信息" class="headerlink" title="Log信息"></a>Log信息</h3><h5 id="CustomBean中没有进行分组-组内排序的log"><a href="#CustomBean中没有进行分组-组内排序的log" class="headerlink" title="CustomBean中没有进行分组, 组内排序的log"></a><code>CustomBean</code>中没有进行<code>分组</code>, <code>组内排序</code>的log</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ==================MyGroup中compare()方法=======================</span></span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// ======================reduce中的执行log==========================</span></span><br><span class="line"></span><br><span class="line">==================第<span class="number">1</span>次进入reduce</span><br><span class="line">computerhuangjiaju<span class="number">83.2</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerliutao<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">2</span>次进入reduce</span><br><span class="line">mathhuangxiaoming<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">3</span>次进入reduce</span><br><span class="line">englishhuanglei<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">4</span>次进入reduce</span><br><span class="line">mathhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">5</span>次进入reduce</span><br><span class="line">algorithmhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmliutao<span class="number">82.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">6</span>次进入reduce</span><br><span class="line">computerhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">7</span>次进入reduce</span><br><span class="line">englishliuyifei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">8</span>次进入reduce</span><br><span class="line">algorithmhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">9</span>次进入reduce</span><br><span class="line">mathhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">10</span>次进入reduce</span><br><span class="line">algorithmhuangzitao<span class="number">72.75</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">11</span>次进入reduce</span><br><span class="line">mathliujialing<span class="number">72.75</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">12</span>次进入reduce</span><br><span class="line">computerhuangzitao<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">13</span>次进入reduce</span><br><span class="line">englishhuangxiaoming<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">14</span>次进入reduce</span><br><span class="line">mathwangbaoqiang<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">15</span>次进入reduce</span><br><span class="line">computerhuangxiaoming<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">16</span>次进入reduce</span><br><span class="line">mathxuzheng<span class="number">69.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">17</span>次进入reduce</span><br><span class="line">englishzhaobenshan<span class="number">69.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">18</span>次进入reduce</span><br><span class="line">computerhuangbo<span class="number">65.25</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerxuzheng<span class="number">65.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">19</span>次进入reduce</span><br><span class="line">englishzhouqi<span class="number">64.18181818181819</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">20</span>次进入reduce</span><br><span class="line">computerliujialing<span class="number">64.11111111111111</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">21</span>次进入reduce</span><br><span class="line">algorithmliuyifei<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">22</span>次进入reduce</span><br><span class="line">englishliujialing<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">23</span>次进入reduce</span><br><span class="line">computerliuyifei<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">24</span>次进入reduce</span><br><span class="line">englishliuyifei<span class="number">59.57142857142857</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">25</span>次进入reduce</span><br><span class="line">mathliutao<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">26</span>次进入reduce</span><br><span class="line">algorithmhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">27</span>次进入reduce</span><br><span class="line">computerhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">28</span>次进入reduce</span><br><span class="line">englishhuangbo<span class="number">55.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br></pre></td></tr></table></figure><h5 id="CustomBean中做了分组-amp-组内排序-的"><a href="#CustomBean中做了分组-amp-组内排序-的" class="headerlink" title="CustomBean中做了分组&amp;组内排序 的"></a><code>CustomBean</code>中做了<code>分组&amp;组内排序</code> 的</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// **Bean中相同的组进行分数降序， 组进行字典排序，此时MyGroup中执行的**</span></span><br><span class="line"></span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line"></span><br><span class="line"><span class="comment">// ======================reduce中执行==========================</span></span><br><span class="line"></span><br><span class="line">==================第<span class="number">1</span>次进入reduce</span><br><span class="line">algorithmhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmliutao<span class="number">82.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmhuangzitao<span class="number">72.75</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmliuyifei<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">===================离开reduce</span><br><span class="line">==================第<span class="number">2</span>次进入reduce</span><br><span class="line">computerhuangjiaju<span class="number">83.2</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerliutao<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerhuangzitao<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerhuangxiaoming<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerhuangbo<span class="number">65.25</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerxuzheng<span class="number">65.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerliujialing<span class="number">64.11111111111111</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerliuyifei<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">===================离开reduce</span><br><span class="line">==================第<span class="number">3</span>次进入reduce</span><br><span class="line">englishhuanglei<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishliuyifei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishhuangxiaoming<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishzhaobenshan<span class="number">69.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishzhouqi<span class="number">64.18181818181819</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishliujialing<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishliuyifei<span class="number">59.57142857142857</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishhuangbo<span class="number">55.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">===================离开reduce</span><br><span class="line">==================第<span class="number">4</span>次进入reduce</span><br><span class="line">mathhuangxiaoming<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathliujialing<span class="number">72.75</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathwangbaoqiang<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathxuzheng<span class="number">69.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathliutao<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">===================离开reduce</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//  如果只取一个每次values的第一个的话 </span></span><br><span class="line"></span><br><span class="line">algorithmhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">==================第<span class="number">1</span>次进入reduce</span><br><span class="line">computerhuangjiaju<span class="number">83.2</span></span><br><span class="line">==================第<span class="number">2</span>次进入reduce</span><br><span class="line">englishhuanglei<span class="number">83.0</span></span><br><span class="line">==================第<span class="number">3</span>次进入reduce</span><br><span class="line">mathhuangxiaoming<span class="number">83.0</span></span><br><span class="line">==================第<span class="number">4</span>次进入reduce</span><br></pre></td></tr></table></figure><p><br></p><p><br></p><h3 id="其它疑点"><a href="#其它疑点" class="headerlink" title="其它疑点"></a>其它疑点</h3><ul><li>通过log可以看出来， MyGroup是在读到了不同的数据，才会进入到 reduce 类中写出；</li><li>但是 通过 <strong>断点调试</strong>时， 现象是，第一次读到了2个相同的，就去reduce去写出了；</li><li>后面再读的就不做写出操作，直到下一次读到2个相同的，再在reduce中写出。</li></ul><p><script src="https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js"></script><script src="https://cdn.bootcss.com/snap.svg/0.5.1/snap.svg-min.js"></script><script src="https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js"></script><script src="https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js"></script><script src="https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js"></script><textarea id="sequence-0-code" style="display: none">title: 执行流程时序图 Mapper(map)->ScoreBean: k:ScoreBean(courseName,avgScore)Mapper(map)->ScoreBean: v:Text(stuName)Mapper(ScoreBean)->Reducer(MyGroup): course按字典升序Mapper(ScoreBean)->Reducer(MyGroup): course内成绩降序Reducer(MyGroup)->Reducer(reduce): 根据自定义的分组规则按组输出Reducer(MyGroup)->Reducer(reduce): 一组只调用reduce一次Reducer(reduce)-->Reducer(reduce):</textarea><textarea id="sequence-0-options" style="display: none">{"theme":"simple"}</textarea><script>  var code = document.getElementById("sequence-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value));  var diagram = Diagram.parse(code);  diagram.drawSVG("sequence-0", options);</script></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;最近学分组的时候，一直弄不明白，总感觉一头雾水，通过近3个小时的断点调试和log输出。 大概了解了一些表象，先记录。&lt;/p&gt;
&lt;p&gt;案例是这
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>▍开始</title>
    <link href="https://airpoet.github.io/2018/06/10/Poetry/%E5%BC%80%E5%A7%8B/"/>
    <id>https://airpoet.github.io/2018/06/10/Poetry/开始/</id>
    <published>2018-06-09T17:03:10.001Z</published>
    <updated>2018-06-09T17:19:40.887Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-09-601528563678_.pic.jpg" alt=""></p><p>月亮落下一两片羽毛在田野上。</p><p>黑暗中的麦子聆听着。</p><p>快静下来。</p><p>快。</p><p>就在那儿，月亮的孩子们正试着</p><p>挥动翅膀。</p><p>在两棵树之间，身材修长的女子抬起面庞，</p><p>美丽的剪影。接着，她步入空中，接着，</p><p>她完全消失在空中。</p><p>我独自站在一棵接骨木旁，不敢呼吸，</p><p>也不敢动。</p><p>我聆听着。</p><p>麦子向后靠着自己的黑暗，</p><p>而我靠着我的。</p><p><br></p><p><strong>作者 / [美国] 詹姆斯·赖特</strong></p><p>翻译 / 张文武</p><hr><h3 id="▍Beginning"><a href="#▍Beginning" class="headerlink" title="▍Beginning"></a><strong>▍Beginning</strong></h3><p><br></p><p>The moon drops one or two feathers into the fields.</p><p>The dark wheat listens.</p><p>Be still.</p><p>Now.</p><p>There they are, the moon’s young, trying</p><p>Their wings.</p><p>Between trees, a slender woman lifts up the lovely shadow</p><p>Of her face, and now she steps into the air, now she is gone</p><p>Wholly, into the air.</p><p>I stand alone by an elder tree, I do not dare breathe</p><p>Or move.</p><p>I listen.</p><p>The wheat leans back toward its own darkness,</p><p>And I lean toward mine.</p><p><br></p><p><strong>Author / James Wright</strong></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-09-601528563678_.pic.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;月
      
    
    </summary>
    
      <category term="文艺" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/"/>
    
      <category term="诗歌" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/%E8%AF%97%E6%AD%8C/"/>
    
    
      <category term="诗歌" scheme="https://airpoet.github.io/tags/%E8%AF%97%E6%AD%8C/"/>
    
  </entry>
  
  <entry>
    <title>Markdown高阶语法</title>
    <link href="https://airpoet.github.io/2018/06/10/Tools/Markdown/Markdown%E9%AB%98%E9%98%B6%E8%AF%AD%E6%B3%95/"/>
    <id>https://airpoet.github.io/2018/06/10/Tools/Markdown/Markdown高阶语法/</id>
    <published>2018-06-09T16:49:50.407Z</published>
    <updated>2018-06-10T13:21:24.745Z</updated>
    
    <content type="html"><![CDATA[<h3 id="时序图的写法"><a href="#时序图的写法" class="headerlink" title="时序图的写法"></a>时序图的写法</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-09-165900.png" alt="image-20180610005859980"></p><div id="sequence-0"></div><h3 id="流程图的写法"><a href="#流程图的写法" class="headerlink" title="流程图的写法"></a>流程图的写法</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-09-165930.png" alt="image-20180610005930019"></p><div id="flowchart-0" class="flow-chart"></div><h3 id="类图的写法"><a href="#类图的写法" class="headerlink" title="类图的写法"></a>类图的写法</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-09-165948.png" alt="image-20180610005948130"></p><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLl2jz4qiA4Wjud98pKi12WC0"></p><p><script src="https://cdnjs.cloudflare.com/ajax/libs/raphael/2.2.7/raphael.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/flowchart/1.6.5/flowchart.min.js"></script><textarea id="flowchart-0-code" style="display: none">st=>start: Start|past:>http://www.google.com[blank]e=>end: End:>http://www.google.comop1=>operation: My Operation|pastop2=>operation: Stuff|currentsub1=>subroutine: My Subroutine|invalidcond=>condition: Yesor No?|approved:>http://www.google.comc2=>condition: Good idea|rejectedio=>inputoutput: catch something...|requestst->op1(right)->condcond(yes, right)->c2cond(no)->sub1(left)->op1c2(yes)->io->ec2(no)->op2->e</textarea><textarea id="flowchart-0-options" style="display: none">{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("flowchart-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value));  var diagram = flowchart.parse(code);  diagram.drawSVG("flowchart-0", options);</script><script src="https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js"></script><script src="https://cdn.bootcss.com/snap.svg/0.5.1/snap.svg-min.js"></script><script src="https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js"></script><script src="https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js"></script><script src="https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js"></script><textarea id="sequence-0-code" style="display: none">mapper->Bob: Hello Bob, how are you?Note right of Bob: Bob thinksBob-->reducer: I am good thanks!ad u?reducer->out: I'm fine tooout->me: ok, you winme-->Bob: nono, not</textarea><textarea id="sequence-0-options" style="display: none">{"theme":"simple"}</textarea><script>  var code = document.getElementById("sequence-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value));  var diagram = Diagram.parse(code);  diagram.drawSVG("sequence-0", options);</script></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;时序图的写法&quot;&gt;&lt;a href=&quot;#时序图的写法&quot; class=&quot;headerlink&quot; title=&quot;时序图的写法&quot;&gt;&lt;/a&gt;时序图的写法&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/study/2018-
      
    
    </summary>
    
      <category term="工具" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="Markdown" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/Markdown/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="工具" scheme="https://airpoet.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="Markdown" scheme="https://airpoet.github.io/tags/Markdown/"/>
    
  </entry>
  
  <entry>
    <title>About Sublime Text3</title>
    <link href="https://airpoet.github.io/2018/06/09/Tools/Sublime/%E5%AE%89%E8%A3%85%E4%B8%BB%E9%A2%98/"/>
    <id>https://airpoet.github.io/2018/06/09/Tools/Sublime/安装主题/</id>
    <published>2018-06-09T04:43:17.189Z</published>
    <updated>2018-06-10T01:09:27.890Z</updated>
    
    <content type="html"><![CDATA[<h3 id="主题"><a href="#主题" class="headerlink" title="主题"></a>主题</h3><p><a href="https://scotch.io/bar-talk/best-sublime-text-3-themes-of-2015-and-2016" target="_blank" rel="noopener">详情参见这个网站</a></p><h3 id="详细操作"><a href="#详细操作" class="headerlink" title="详细操作"></a>详细操作</h3><p><a href="http://zh.lucida.me/blog/sublime-text-complete-guide/" target="_blank" rel="noopener">见此站</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;主题&quot;&gt;&lt;a href=&quot;#主题&quot; class=&quot;headerlink&quot; title=&quot;主题&quot;&gt;&lt;/a&gt;主题&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://scotch.io/bar-talk/best-sublime-text-3-themes-of-2015
      
    
    </summary>
    
      <category term="工具" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="Sublime" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/Sublime/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="工具" scheme="https://airpoet.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="Sublime" scheme="https://airpoet.github.io/tags/Sublime/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce笔记-Bug汇总</title>
    <link href="https://airpoet.github.io/2018/06/09/Hadoop/Study/2-MapReduce/MapReduce-Bug%E6%B1%87%E6%80%BB/"/>
    <id>https://airpoet.github.io/2018/06/09/Hadoop/Study/2-MapReduce/MapReduce-Bug汇总/</id>
    <published>2018-06-09T03:11:48.470Z</published>
    <updated>2018-06-11T11:46:16.724Z</updated>
    
    <content type="html"><![CDATA[<h4 id="1、reduce-输出路径必须是新创建的。不能已经存在"><a href="#1、reduce-输出路径必须是新创建的。不能已经存在" class="headerlink" title="1、reduce 输出路径必须是新创建的。不能已经存在"></a>1、reduce 输出路径必须是新创建的。不能已经存在</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread <span class="string">"main"</span> org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs:<span class="comment">//cs1:9000/flowout01 already exists</span></span><br></pre></td></tr></table></figure><h4 id="2、在初始化-job-的时候，-没有传-conf-，-导致后面一直找不到文件，-因为不知道到哪里去找"><a href="#2、在初始化-job-的时候，-没有传-conf-，-导致后面一直找不到文件，-因为不知道到哪里去找" class="headerlink" title="2、在初始化 job 的时候， 没有传 conf ， 导致后面一直找不到文件， 因为不知道到哪里去找"></a>2、在初始化 job 的时候， 没有传 <code>conf</code> ， 导致后面一直找不到文件， 因为不知道到哪里去找</h4><h4 id="3、Text导包倒错-导的时候要注意"><a href="#3、Text导包倒错-导的时候要注意" class="headerlink" title="3、Text导包倒错, 导的时候要注意"></a>3、Text导包倒错, 导的时候要注意</h4><p><strong>应该是这个 <code>import org.apache.hadoop.io.Text;</code></strong></p><h4 id="4、进行字符串拼接的时候，把-StringBuilder-写到了-reduce-方法外，-这样导致-sb-会越来越多，当然，也可以每次拼接完了清空"><a href="#4、进行字符串拼接的时候，把-StringBuilder-写到了-reduce-方法外，-这样导致-sb-会越来越多，当然，也可以每次拼接完了清空" class="headerlink" title="4、进行字符串拼接的时候，把 StringBuilder 写到了 reduce 方法外， 这样导致 sb 会越来越多，当然，也可以每次拼接完了清空"></a>4、进行字符串拼接的时候，把 <strong>StringBuilder 写到了 reduce 方法外</strong>， 这样导致 sb 会越来越多，当然，也可以每次拼接完了清空</h4><p>类似于这样</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">AF,I,O,K,G,D,C,H,B</span><br><span class="line">BF,I,O,K,G,D,C,H,B,E,J,F,A</span><br><span class="line">CF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F</span><br><span class="line">DF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L</span><br><span class="line">EF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H</span><br><span class="line">FF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G</span><br><span class="line">GF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M</span><br><span class="line">HF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O</span><br><span class="line">IF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C</span><br><span class="line">JF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O</span><br><span class="line">KF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,B</span><br><span class="line">LF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,B,D,E</span><br><span class="line">MF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,B,D,E,E,F</span><br><span class="line">OF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,B,D,E,E,F,A,H,I,J,F</span><br></pre></td></tr></table></figure><h4 id="4、mapreduce执行错误Mapper-错误"><a href="#4、mapreduce执行错误Mapper-错误" class="headerlink" title="4、mapreduce执行错误Mapper.\错误"></a>4、mapreduce执行错误Mapper.\<init>错误</init></h4><ul><li>Mapper &amp; Reducer 写成内部类的时候，有没有加上 <code>static</code></li><li>Bean类有没有无参构造</li></ul><h4 id="5、排序过程中，自定义了排序类，bean类的-compareTo-方法，只写了按照分数大小排序。"><a href="#5、排序过程中，自定义了排序类，bean类的-compareTo-方法，只写了按照分数大小排序。" class="headerlink" title="5、排序过程中，自定义了排序类，bean类的 compareTo()方法，只写了按照分数大小排序。"></a>5、排序过程中，自定义了排序类，bean类的 <code>compareTo()</code>方法，只写了按照分数大小排序。</h4><p><u>会出现如下错误： 课程并没有分组</u></p><p>没有在相同的一组课程中比较分数， 而是比较的所有的分数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">computerhuangjiaju<span class="number">83.2</span></span><br><span class="line">mathhuangxiaoming<span class="number">83.0</span></span><br><span class="line">englishhuanglei<span class="number">83.0</span></span><br><span class="line">mathhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">algorithmhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">computerhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">englishliuyifei<span class="number">74.42857142857143</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><p><strong>此时应该在Bean对象中做如下事情</strong></p><ul><li>相同课程的按照分数降序排序</li><li>课程名按照自然（升序）排序</li><li><strong>换言之，就是CustomBean 对象要输出的数据是 <code>组名升序排序，组内按成绩降序排序</code></strong></li><li><a href="https://airpoet.github.io/2018/06/10/Hadoop/Study/MapReduce-%E5%88%86%E7%BB%84%E6%B5%85%E6%8E%A2/#more">具体分析参阅</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;1、reduce-输出路径必须是新创建的。不能已经存在&quot;&gt;&lt;a href=&quot;#1、reduce-输出路径必须是新创建的。不能已经存在&quot; class=&quot;headerlink&quot; title=&quot;1、reduce 输出路径必须是新创建的。不能已经存在&quot;&gt;&lt;/a&gt;1、red
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce笔记-练习</title>
    <link href="https://airpoet.github.io/2018/06/09/Hadoop/Study/2-MapReduce/MapReduce-%E7%BB%83%E4%B9%A0/"/>
    <id>https://airpoet.github.io/2018/06/09/Hadoop/Study/2-MapReduce/MapReduce-练习/</id>
    <published>2018-06-09T03:11:16.234Z</published>
    <updated>2018-06-11T14:15:19.807Z</updated>
    
    <content type="html"><![CDATA[<h2 id="求微博共同粉丝"><a href="#求微博共同粉丝" class="headerlink" title="求微博共同粉丝"></a>求微博共同粉丝</h2><h3 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h3><p><strong>涉及知识点： 多 Job 串联</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">A:B,C,D,F,E,O</span><br><span class="line">B:A,C,E,K</span><br><span class="line">C:F,A,D,I</span><br><span class="line">D:A,E,F,L</span><br><span class="line">E:B,C,D,M,L</span><br><span class="line">F:A,B,C,D,E,O,M</span><br><span class="line">G:A,C,D,E,F</span><br><span class="line">H:A,C,D,E,O</span><br><span class="line">I:A,O</span><br><span class="line">J:B,O</span><br><span class="line">K:A,C,D</span><br><span class="line">L:D,E,F</span><br><span class="line">M:E,F,G</span><br><span class="line">O:A,H,I,J,K</span><br></pre></td></tr></table></figure><blockquote><p>以上是数据：<br>A:B,C,D,F,E,O<br>表示：A用户 关注B,C,D,E,F,O</p><blockquote><p>求所有两两用户之间的共同关注对象</p></blockquote></blockquote><h3 id="答案："><a href="#答案：" class="headerlink" title="答案："></a>答案：</h3><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLd3EpytDptDBp2jsIStDvt98pKi1IW80"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._01_多Job串联;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CommonFansDemo</span> </span>&#123;</span><br><span class="line"><span class="meta">@SuppressWarnings</span>(<span class="string">"deprecation"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// Job 逻辑</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 HDFS 相关的参数</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// 新建一个 job1</span></span><br><span class="line">Job job1 = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置 Jar 包所在路径</span></span><br><span class="line">job1.setJarByClass(CommonFansDemo.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 mapper 类和 reducer 类</span></span><br><span class="line">job1.setMapperClass(MyMapper_Step1.class);</span><br><span class="line">job1.setReducerClass(MyReducer_Step1.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 maptask 的输出类型</span></span><br><span class="line">job1.setMapOutputKeyClass(Text.class);</span><br><span class="line">job1.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定最终的输出类型(reduce存在时,就是指 ReduceTask 的输出类型)</span></span><br><span class="line">job1.setOutputKeyClass(Text.class);</span><br><span class="line">job1.setOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定该 MapReduce 程序数据的输入输出路径</span></span><br><span class="line">FileInputFormat.setInputPaths(job1, <span class="keyword">new</span> Path(<span class="string">"/in/commonfriend"</span>));</span><br><span class="line">FileOutputFormat.setOutputPath(job1, <span class="keyword">new</span> Path(<span class="string">"/out/job1"</span>));</span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// 新建一个 job2</span></span><br><span class="line">Job job2 = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置 Jar 包所在路径</span></span><br><span class="line">job2.setJarByClass(CommonFansDemo.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 mapper 类和 reducer 类</span></span><br><span class="line">job2.setMapperClass(MyMapper_Step2.class);</span><br><span class="line">job2.setReducerClass(MyReducer_Step2.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 maptask 的输出类型</span></span><br><span class="line">job2.setMapOutputKeyClass(Text.class);</span><br><span class="line">job2.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定最终的输出类型(reduce存在时,就是指 ReduceTask 的输出类型)</span></span><br><span class="line">job2.setOutputKeyClass(Text.class);</span><br><span class="line">job2.setOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定该 MapReduce 程序数据的输入输出路径</span></span><br><span class="line">FileInputFormat.setInputPaths(job2, <span class="keyword">new</span> Path(<span class="string">"/out/job1"</span>));</span><br><span class="line">FileOutputFormat.setOutputPath(job2, <span class="keyword">new</span> Path(<span class="string">"/out/job2"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将多个 job 当做一个组中的 job 提交, 参数名是组名</span></span><br><span class="line"><span class="comment"> * 注意: JobControl 是实现了 Runnable 接口的 </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">JobControl jControl = <span class="keyword">new</span> JobControl(<span class="string">"common_friend"</span>);</span><br><span class="line"><span class="comment">// 将原生的 job携带配置 转换为可控的 job</span></span><br><span class="line">ControlledJob aJob = <span class="keyword">new</span> ControlledJob(job1.getConfiguration());</span><br><span class="line">ControlledJob bJob = <span class="keyword">new</span> ControlledJob(job2.getConfiguration());</span><br><span class="line"><span class="comment">// 添加依赖关系</span></span><br><span class="line">bJob.addDependingJob(aJob);</span><br><span class="line"><span class="comment">// 添加 job 到组中</span></span><br><span class="line">jControl.addJob(aJob);</span><br><span class="line">jControl.addJob(bJob);</span><br><span class="line"><span class="comment">// 启动一个线程</span></span><br><span class="line">Thread jobThread = <span class="keyword">new</span> Thread(jControl);</span><br><span class="line">jobThread.start();</span><br><span class="line"><span class="keyword">while</span> (!jControl.allFinished()) &#123;</span><br><span class="line">Thread.sleep(<span class="number">500</span>);</span><br><span class="line">&#125;</span><br><span class="line">jobThread.stop();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper_Step1</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">String[] user_attentions;</span><br><span class="line">String[] attentions;</span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value,</span></span></span><br><span class="line"><span class="function"><span class="params">Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">user_attentions = value.toString().split(<span class="string">":"</span>);</span><br><span class="line">attentions = user_attentions[<span class="number">1</span>].trim().split(<span class="string">","</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (String att : attentions) &#123;</span><br><span class="line">k.set(att);</span><br><span class="line">v.set(user_attentions[<span class="number">0</span>].trim());</span><br><span class="line">context.write(k, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 将两两粉丝(普通用户)拼接起来, 格式a-f:c =&gt; a,b 都共同关注了 c</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> *  AF,I,O,K,G,D,C,H,B</span></span><br><span class="line"><span class="comment">BE,J,F,A</span></span><br><span class="line"><span class="comment">CB,E,K,A,H,G,F</span></span><br><span class="line"><span class="comment">DH,C,G,F,E,A,K,L</span></span><br><span class="line"><span class="comment">EA,B,L,G,M,F,D,H</span></span><br><span class="line"><span class="comment">FC,M,L,A,D,G</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper_Step2</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">String[] attenion_users;</span><br><span class="line">String[] users;</span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value,</span></span></span><br><span class="line"><span class="function"><span class="params">Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">attenion_users = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">users = attenion_users[<span class="number">1</span>].trim().split(<span class="string">","</span>);</span><br><span class="line"><span class="keyword">for</span> (String u1 : users) &#123;</span><br><span class="line"><span class="keyword">for</span> (String u2 : users) &#123;</span><br><span class="line"><span class="keyword">if</span> (u1.compareTo(u2) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">String users = u1 + <span class="string">"-"</span> + u2;</span><br><span class="line">k.set(users);</span><br><span class="line">v.set(attenion_users[<span class="number">0</span>].trim());</span><br><span class="line">context.write(k, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> *需要统计的是, 某人拥有的全部粉丝</span></span><br><span class="line"><span class="comment"> *  key: 传过来的 key</span></span><br><span class="line"><span class="comment"> *  value:  用,分割 </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer_Step1</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Reducer&lt;Text, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注意: 这里 sb 不能写在外面,会不断的拼接</span></span><br><span class="line">StringBuilder sb = <span class="keyword">new</span> StringBuilder();</span><br><span class="line"><span class="keyword">for</span> (Text v : values) &#123;</span><br><span class="line">sb.append(v.toString()).append(<span class="string">","</span>);</span><br><span class="line">&#125;</span><br><span class="line">k.set(key);</span><br><span class="line">v.set(sb.substring(<span class="number">0</span>, sb.length() - <span class="number">1</span>));</span><br><span class="line">context.write(k, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 拿到的数据: a-b c</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer_Step2</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Reducer&lt;Text, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">StringBuilder sb = <span class="keyword">new</span> StringBuilder();</span><br><span class="line"><span class="keyword">for</span> (Text attention : values) &#123;</span><br><span class="line">sb.append(attention.toString()).append(<span class="string">","</span>);</span><br><span class="line">&#125;</span><br><span class="line">k.set(key);</span><br><span class="line">v.set(sb.substring(<span class="number">0</span>, sb.length() - <span class="number">1</span>));</span><br><span class="line">context.write(k, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// job1的输出</span></span><br><span class="line">AF,I,O,K,G,D,C,H,B</span><br><span class="line">BE,J,F,A</span><br><span class="line">CB,E,K,A,H,G,F</span><br><span class="line">DH,C,G,F,E,A,K,L</span><br><span class="line">EA,B,L,G,M,F,D,H</span><br><span class="line">FC,M,L,A,D,G</span><br><span class="line">GM</span><br><span class="line">HO</span><br><span class="line">IO,C</span><br><span class="line">JO</span><br><span class="line">KO,B</span><br><span class="line">LD,E</span><br><span class="line">ME,F</span><br><span class="line">OA,H,I,J,F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// job2的输出</span></span><br><span class="line">A-BE,C</span><br><span class="line">A-CD,F</span><br><span class="line">A-DF,E</span><br><span class="line">A-EC,D,B</span><br><span class="line">A-FO,B,E,D,C</span><br><span class="line">A-GE,F,D,C</span><br><span class="line">A-HO,E,D,C</span><br><span class="line">A-IO</span><br><span class="line">A-JB,O</span><br><span class="line">A-KD,C</span><br><span class="line">A-LD,F,E</span><br><span class="line">A-ME,F</span><br><span class="line">B-CA</span><br><span class="line">B-DE,A</span><br><span class="line">B-EC</span><br><span class="line">B-FA,E,C</span><br><span class="line">B-GC,A,E</span><br><span class="line">B-HA,E,C</span><br><span class="line">B-IA</span><br><span class="line">B-KC,A</span><br><span class="line">B-LE</span><br><span class="line">B-ME</span><br><span class="line">B-OA,K</span><br><span class="line">C-DA,F</span><br><span class="line">C-ED</span><br><span class="line">C-FD,A</span><br><span class="line">C-GF,A,D</span><br><span class="line">C-HD,A</span><br><span class="line">C-IA</span><br><span class="line">C-KA,D</span><br><span class="line">C-LF,D</span><br><span class="line">C-MF</span><br><span class="line">C-OI,A</span><br><span class="line">D-EL</span><br><span class="line">D-FE,A</span><br><span class="line">D-GA,F,E</span><br><span class="line">D-HE,A</span><br><span class="line">D-IA</span><br><span class="line">D-KA</span><br><span class="line">D-LF,E</span><br><span class="line">D-MF,E</span><br><span class="line">D-OA</span><br><span class="line">E-FC,B,M,D</span><br><span class="line">E-GC,D</span><br><span class="line">E-HC,D</span><br><span class="line">E-JB</span><br><span class="line">E-KD,C</span><br><span class="line">E-LD</span><br><span class="line">F-GA,D,C,E</span><br><span class="line">F-HA,E,C,D,O</span><br><span class="line">F-IO,A</span><br><span class="line">F-JO,B</span><br><span class="line">F-KC,A,D</span><br><span class="line">F-LE,D</span><br><span class="line">F-ME</span><br><span class="line">F-OA</span><br><span class="line">G-HA,C,D,E</span><br><span class="line">G-IA</span><br><span class="line">G-KC,A,D</span><br><span class="line">G-LD,E,F</span><br><span class="line">G-MF,E</span><br><span class="line">G-OA</span><br><span class="line">H-IO,A</span><br><span class="line">H-JO</span><br><span class="line">H-KA,D,C</span><br><span class="line">H-LE,D</span><br><span class="line">H-ME</span><br><span class="line">H-OA</span><br><span class="line">I-JO</span><br><span class="line">I-KA</span><br><span class="line">I-OA</span><br><span class="line">K-LD</span><br><span class="line">K-OA</span><br><span class="line">L-MF,E</span><br></pre></td></tr></table></figure><h2 id="求学生成绩"><a href="#求学生成绩" class="headerlink" title="求学生成绩"></a>求学生成绩</h2><h3 id="题目-1"><a href="#题目-1" class="headerlink" title="题目"></a>题目</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">computer,huangxiaoming,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span></span><br><span class="line">computer,xuzheng,<span class="number">54</span>,<span class="number">52</span>,<span class="number">86</span>,<span class="number">91</span>,<span class="number">42</span></span><br><span class="line">computer,huangbo,<span class="number">85</span>,<span class="number">42</span>,<span class="number">96</span>,<span class="number">38</span></span><br><span class="line">english,zhaobenshan,<span class="number">54</span>,<span class="number">52</span>,<span class="number">86</span>,<span class="number">91</span>,<span class="number">42</span>,<span class="number">85</span>,<span class="number">75</span></span><br><span class="line">english,liuyifei,<span class="number">85</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">21</span>,<span class="number">85</span>,<span class="number">96</span>,<span class="number">14</span></span><br><span class="line">algorithm,liuyifei,<span class="number">75</span>,<span class="number">85</span>,<span class="number">62</span>,<span class="number">48</span>,<span class="number">54</span>,<span class="number">96</span>,<span class="number">15</span></span><br><span class="line">computer,huangjiaju,<span class="number">85</span>,<span class="number">75</span>,<span class="number">86</span>,<span class="number">85</span>,<span class="number">85</span></span><br><span class="line">english,liuyifei,<span class="number">76</span>,<span class="number">95</span>,<span class="number">86</span>,<span class="number">74</span>,<span class="number">68</span>,<span class="number">74</span>,<span class="number">48</span></span><br><span class="line">english,huangdatou,<span class="number">48</span>,<span class="number">58</span>,<span class="number">67</span>,<span class="number">86</span>,<span class="number">15</span>,<span class="number">33</span>,<span class="number">85</span></span><br><span class="line">algorithm,huanglei,<span class="number">76</span>,<span class="number">95</span>,<span class="number">86</span>,<span class="number">74</span>,<span class="number">68</span>,<span class="number">74</span>,<span class="number">48</span></span><br><span class="line">algorithm,huangjiaju,<span class="number">85</span>,<span class="number">75</span>,<span class="number">86</span>,<span class="number">85</span>,<span class="number">85</span>,<span class="number">74</span>,<span class="number">86</span></span><br><span class="line">computer,huangdatou,<span class="number">48</span>,<span class="number">58</span>,<span class="number">67</span>,<span class="number">86</span>,<span class="number">15</span>,<span class="number">33</span>,<span class="number">85</span></span><br><span class="line">english,zhouqi,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span>,<span class="number">75</span>,<span class="number">55</span>,<span class="number">47</span>,<span class="number">22</span></span><br><span class="line">english,huangbo,<span class="number">85</span>,<span class="number">42</span>,<span class="number">96</span>,<span class="number">38</span>,<span class="number">55</span>,<span class="number">47</span>,<span class="number">22</span></span><br><span class="line">algorithm,liutao,<span class="number">85</span>,<span class="number">75</span>,<span class="number">85</span>,<span class="number">99</span>,<span class="number">66</span></span><br><span class="line">computer,huangzitao,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span></span><br><span class="line">math,wangbaoqiang,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span></span><br><span class="line">computer,liujialing,<span class="number">85</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">21</span>,<span class="number">85</span>,<span class="number">96</span>,<span class="number">14</span>,<span class="number">74</span>,<span class="number">86</span></span><br><span class="line">computer,liuyifei,<span class="number">75</span>,<span class="number">85</span>,<span class="number">62</span>,<span class="number">48</span>,<span class="number">54</span>,<span class="number">96</span>,<span class="number">15</span></span><br><span class="line">computer,liutao,<span class="number">85</span>,<span class="number">75</span>,<span class="number">85</span>,<span class="number">99</span>,<span class="number">66</span>,<span class="number">88</span>,<span class="number">75</span>,<span class="number">91</span></span><br><span class="line">computer,huanglei,<span class="number">76</span>,<span class="number">95</span>,<span class="number">86</span>,<span class="number">74</span>,<span class="number">68</span>,<span class="number">74</span>,<span class="number">48</span></span><br><span class="line">english,liujialing,<span class="number">75</span>,<span class="number">85</span>,<span class="number">62</span>,<span class="number">48</span>,<span class="number">54</span>,<span class="number">96</span>,<span class="number">15</span></span><br><span class="line">math,huanglei,<span class="number">76</span>,<span class="number">95</span>,<span class="number">86</span>,<span class="number">74</span>,<span class="number">68</span>,<span class="number">74</span>,<span class="number">48</span></span><br><span class="line">math,huangjiaju,<span class="number">85</span>,<span class="number">75</span>,<span class="number">86</span>,<span class="number">85</span>,<span class="number">85</span>,<span class="number">74</span>,<span class="number">86</span></span><br><span class="line">math,liutao,<span class="number">48</span>,<span class="number">58</span>,<span class="number">67</span>,<span class="number">86</span>,<span class="number">15</span>,<span class="number">33</span>,<span class="number">85</span></span><br><span class="line">english,huanglei,<span class="number">85</span>,<span class="number">75</span>,<span class="number">85</span>,<span class="number">99</span>,<span class="number">66</span>,<span class="number">88</span>,<span class="number">75</span>,<span class="number">91</span></span><br><span class="line">math,xuzheng,<span class="number">54</span>,<span class="number">52</span>,<span class="number">86</span>,<span class="number">91</span>,<span class="number">42</span>,<span class="number">85</span>,<span class="number">75</span></span><br><span class="line">math,huangxiaoming,<span class="number">85</span>,<span class="number">75</span>,<span class="number">85</span>,<span class="number">99</span>,<span class="number">66</span>,<span class="number">88</span>,<span class="number">75</span>,<span class="number">91</span></span><br><span class="line">math,liujialing,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span>,<span class="number">75</span></span><br><span class="line">english,huangxiaoming,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span></span><br><span class="line">algorithm,huangdatou,<span class="number">48</span>,<span class="number">58</span>,<span class="number">67</span>,<span class="number">86</span>,<span class="number">15</span>,<span class="number">33</span>,<span class="number">85</span></span><br><span class="line">algorithm,huangzitao,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span>,<span class="number">75</span></span><br></pre></td></tr></table></figure><blockquote><p>一、数据解释</p><p>数据字段个数不固定：<br>第一个是课程名称，总共四个课程，computer，math，english，algorithm，<br>第二个是学生姓名，后面是每次考试的分数</p><p>二、统计需求：<br>1、统计每门课程的参加考试人数和课程平均分</p><p>2、统计每门课程参考学生的平均分，并且按课程存入不同的结果文件，要求一门课程一个结果文件</p><p>3、求出每门课程参考学生成绩最高平均分的学生的信息：课程，姓名和平均分</p></blockquote><h3 id="答案"><a href="#答案" class="headerlink" title="答案"></a>答案</h3><h4 id="第1小题"><a href="#第1小题" class="headerlink" title="第1小题"></a>第1小题</h4><p><strong>统计每门课程的参考人数和课程平均分</strong></p><p><strong>涉及知识点: 去重， 自定义类</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//  ScoreBean </span></span><br><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._02_分组组件;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.AllArgsConstructor;</span><br><span class="line"><span class="keyword">import</span> lombok.Getter;</span><br><span class="line"><span class="keyword">import</span> lombok.NoArgsConstructor;</span><br><span class="line"><span class="keyword">import</span> lombok.Setter;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Getter</span></span><br><span class="line"><span class="meta">@Setter</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ScoreBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">ScoreBean</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> String courseName;</span><br><span class="line"><span class="keyword">private</span> String stuName; </span><br><span class="line"><span class="keyword">private</span> Double score;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">out.writeUTF(courseName);</span><br><span class="line">out.writeUTF(stuName);</span><br><span class="line">out.writeDouble(score);</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.courseName = in.readUTF();</span><br><span class="line"><span class="keyword">this</span>.stuName = in.readUTF();</span><br><span class="line"><span class="keyword">this</span>.score = in.readDouble();</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 如果是相同课程, 按照分数降序排列的</span></span><br><span class="line"><span class="comment"> * 如果是不同课程, 按照课程名称升序排列</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(ScoreBean o)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 测试一下只写按分数降序排序</span></span><br><span class="line"><span class="comment">//return o.getScore().compareTo(this.getScore());</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*// 首先分组(只在相同的组内进行比较)</span></span><br><span class="line"><span class="comment">int nameRes = this.getCourseName().compareTo(o.getCourseName());</span></span><br><span class="line"><span class="comment">if (nameRes == 0) &#123;</span></span><br><span class="line"><span class="comment">// 课程相同的时候才进行降序排序</span></span><br><span class="line"><span class="comment">int scoreRes = </span></span><br><span class="line"><span class="comment">return scoreRes;</span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment">return nameRes;*/</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString1</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> stuName + <span class="string">"\t"</span> + score;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> courseName + <span class="string">"\t"</span> + stuName</span><br><span class="line">+ <span class="string">"\t"</span> + score;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ScoreBean</span><span class="params">(String stuName, Double score)</span> </span>&#123;</span><br><span class="line"><span class="keyword">super</span>();</span><br><span class="line"><span class="keyword">this</span>.stuName = stuName;</span><br><span class="line"><span class="keyword">this</span>.score = score;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//  ScorePlusDemo1 </span></span><br><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._02_分组组件;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.HashSet;</span><br><span class="line"><span class="keyword">import</span> java.util.Set;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ScorePlusDemo1</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line">job.setJarByClass(ScorePlusDemo1.class);</span><br><span class="line"></span><br><span class="line">job.setMapperClass(MyMapper.class);</span><br><span class="line">job.setReducerClass(MyReducer.class);</span><br><span class="line"></span><br><span class="line">job.setMapOutputKeyClass(Text.class);</span><br><span class="line">job.setMapOutputValueClass(ScoreBean.class);</span><br><span class="line"></span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">String inP = <span class="string">"/in/newScoreIn"</span>;</span><br><span class="line">String outP = <span class="string">"/out/ans1"</span>;</span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(inP));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(outP));</span><br><span class="line"></span><br><span class="line">        Path mypath = <span class="keyword">new</span> Path(outP);</span><br><span class="line">        FileSystem hdfs = mypath.getFileSystem(conf);</span><br><span class="line">        <span class="keyword">if</span> (hdfs.isDirectory(mypath)) &#123;</span><br><span class="line">            hdfs.delete(mypath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">Boolean waitForComp = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">System.exit(waitForComp?<span class="number">0</span>:<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">ScoreBean</span>&gt;  </span>&#123;</span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">// 1.截取</span></span><br><span class="line">String[] datas = value.toString().trim().split(<span class="string">","</span>);</span><br><span class="line">String courseName = datas[<span class="number">0</span>].trim();</span><br><span class="line">String stuName = datas[<span class="number">1</span>].trim();</span><br><span class="line"><span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;datas.length; i++) &#123;</span><br><span class="line">sum += Integer.parseInt(datas[i]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">double</span> avgScore = sum/(datas.length-<span class="number">2</span>);</span><br><span class="line">ScoreBean sb = <span class="keyword">new</span> ScoreBean(courseName, stuName, avgScore);</span><br><span class="line">k.set(courseName);</span><br><span class="line">context.write(k, sb);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">ScoreBean</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;ScoreBean&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Reducer&lt;Text, ScoreBean, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">Set&lt;String&gt; stuNames = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line"><span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (ScoreBean sb : values) &#123;</span><br><span class="line">stuNames.add(sb.getStuName());</span><br><span class="line">count ++;</span><br><span class="line">sum += sb.getScore();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">int</span> size = stuNames.size();</span><br><span class="line">String val = size + <span class="string">"\t"</span> + (<span class="keyword">double</span>)sum/count;</span><br><span class="line">v.set(val);</span><br><span class="line">context.write(key, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行结果 </span></span><br><span class="line">algorithm<span class="number">6</span><span class="number">71.33333333333333</span></span><br><span class="line">computer<span class="number">10</span><span class="number">69.6</span></span><br><span class="line">english<span class="number">8</span><span class="number">66.0</span></span><br><span class="line">math<span class="number">7</span><span class="number">72.57142857142857</span></span><br></pre></td></tr></table></figure><h4 id="第2小题"><a href="#第2小题" class="headerlink" title="第2小题"></a>第2小题</h4><p><strong>统计每门课程参考学生的平均分，并且按课程存入不同的结果文件，要求一门课程一个结果文件</strong></p><p><strong>涉及知识点： 分区, 字符串组合key， Partitioner</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._02_分组组件;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.DoubleWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 注意: 此题因为数据中有2条 course 和 stuName相同的数据(english liuyifei), 所以必须再在reduce中继续去重一下, 再计算一下平均分</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 否则, 可以不用写reduce, 因为Mapper中已经把逻辑处理完了,可以直接输出</span></span><br><span class="line"><span class="comment"> </span></span><br><span class="line"><span class="comment"> * 最终输出: </span></span><br><span class="line"><span class="comment"> * computer liuyifei 43</span></span><br><span class="line"><span class="comment"> * computer huanglei 63</span></span><br><span class="line"><span class="comment"> * math liutao   64</span></span><br><span class="line"><span class="comment"> * ...</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ScorePlusDemo2</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定HDFS相关参数</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//  创建/配置 Job</span></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Jar包类型</span></span><br><span class="line">job.setJarByClass(ScorePlusDemo2.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Map Reduce执行类</span></span><br><span class="line">job.setMapperClass(MyMapper.class);</span><br><span class="line">job.setReducerClass(MyReducer.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Map输出类</span></span><br><span class="line">job.setMapOutputKeyClass(Text.class);</span><br><span class="line">job.setMapOutputValueClass(DoubleWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Reduce输出类</span></span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(DoubleWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//  设置分区 </span></span><br><span class="line">job.setPartitionerClass(MyPartition.class);</span><br><span class="line">job.setNumReduceTasks(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置输入 输出路径</span></span><br><span class="line">String inP = <span class="string">"/in/newScoreIn"</span>;</span><br><span class="line">String outP = <span class="string">"/out/scorePlus2"</span>;</span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(inP));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(outP));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置如果存在路径就删除</span></span><br><span class="line">        Path mypath = <span class="keyword">new</span> Path(outP);</span><br><span class="line">        FileSystem hdfs = mypath.getFileSystem(conf);</span><br><span class="line">        <span class="keyword">if</span> (hdfs.isDirectory(mypath)) &#123;</span><br><span class="line">            hdfs.delete(mypath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//  执行job</span></span><br><span class="line">        <span class="keyword">boolean</span> waitForCompletion = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(waitForCompletion?<span class="number">0</span>:-<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line">    </span><br><span class="line">===============================================================</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">DoubleWritable</span>&gt; </span>&#123;</span><br><span class="line"><span class="comment">// 把 课程+学生 作为 key</span></span><br><span class="line"></span><br><span class="line">Text k = <span class="keyword">new</span> Text();  <span class="comment">//只有输出String类型的, 才需要在这里设置Text</span></span><br><span class="line">DoubleWritable v = <span class="keyword">new</span> DoubleWritable();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">String[] datas = value.toString().trim().split(<span class="string">","</span>);</span><br><span class="line">String kStr = datas[<span class="number">0</span>].trim() + <span class="string">"\t"</span> + datas[<span class="number">1</span>].trim();</span><br><span class="line"><span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>; i &lt; datas.length; i++) &#123;</span><br><span class="line">sum += Integer.parseInt(datas[i]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">double</span> avg = sum / (datas.length - <span class="number">2</span>);</span><br><span class="line">k.set(kStr);</span><br><span class="line">v.set(avg);</span><br><span class="line">context.write(k, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">===============================================================</span><br><span class="line">    </span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 注意: 此题因为数据中有2条 course 和 stuName相同的数据, 所以必须再在reduce中</span></span><br><span class="line"><span class="comment"> * 继续去重一下, 再计算一下平均分</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 否则, 可以不用写reduce, 因为Mapper中已经把逻辑处理完了,可以直接输出</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">DoubleWritable</span>, <span class="title">Text</span>, <span class="title">DoubleWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">DoubleWritable v = <span class="keyword">new</span> DoubleWritable();</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;DoubleWritable&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Reducer&lt;Text, DoubleWritable, Text, DoubleWritable&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 考虑到有 课程, 学生名相同, 后面的数据不同的情况, 这里再做一个平均求和</span></span><br><span class="line"><span class="comment"> * 可以验证打印下</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">Double sum = <span class="number">0.0</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (DoubleWritable avg : values) &#123;</span><br><span class="line"><span class="keyword">if</span> (count &gt; <span class="number">0</span>) &#123;</span><br><span class="line"><span class="comment">// 有key完全相同的情况才会进到这里</span></span><br><span class="line">System.out.println(<span class="string">"这是第"</span> +count +<span class="string">"次, 说明课程和姓名有相同的两条数据\n课程姓名是: "</span>+key.toString());</span><br><span class="line">&#125;</span><br><span class="line">sum += avg.get();</span><br><span class="line">count ++;</span><br><span class="line">&#125;</span><br><span class="line">Double finAvg = sum/count;</span><br><span class="line">v.set(finAvg);</span><br><span class="line">context.write(key, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">===============================================================</span><br><span class="line">===============================================================</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 继承 Partitioner, 实现自定义分区</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyPartition</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">DoubleWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> HashMap&lt;String, Integer&gt; courseMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line">courseMap.put(<span class="string">"algorithm"</span>, <span class="number">0</span>);</span><br><span class="line">courseMap.put(<span class="string">"computer"</span>, <span class="number">1</span>);</span><br><span class="line">courseMap.put(<span class="string">"english"</span>, <span class="number">2</span>);</span><br><span class="line">courseMap.put(<span class="string">"math"</span>, <span class="number">3</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text key, DoubleWritable value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 取出Map输出的key中的前半部分--courseName</span></span><br><span class="line">Integer code = courseMap.get(key.toString().trim().split(<span class="string">"\t"</span>)[<span class="number">0</span>]);</span><br><span class="line"><span class="keyword">if</span> (code != <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="keyword">return</span> code;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="number">5</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">===============================================================</span><br><span class="line"> ===============================================================</span><br><span class="line">    </span><br><span class="line"> 执行结果 </span><br><span class="line">algorithmhuangdatou<span class="number">56.0</span></span><br><span class="line">algorithmhuangjiaju<span class="number">82.0</span></span><br><span class="line">algorithmhuanglei<span class="number">74.0</span></span><br><span class="line">algorithmhuangzitao<span class="number">72.0</span></span><br><span class="line">algorithmliutao<span class="number">82.0</span></span><br><span class="line">algorithmliuyifei<span class="number">62.0</span></span><br><span class="line">----------</span><br><span class="line">computerhuangbo<span class="number">65.0</span></span><br><span class="line">computerhuangdatou<span class="number">56.0</span></span><br><span class="line">computerhuangjiaju<span class="number">83.0</span></span><br><span class="line">computerhuanglei<span class="number">74.0</span></span><br><span class="line">computerhuangxiaoming<span class="number">72.0</span></span><br><span class="line">computerhuangzitao<span class="number">72.0</span></span><br><span class="line">computerliujialing<span class="number">64.0</span></span><br><span class="line">computerliutao<span class="number">83.0</span></span><br><span class="line">computerliuyifei<span class="number">62.0</span></span><br><span class="line">computerxuzheng<span class="number">65.0</span></span><br><span class="line">---------</span><br><span class="line">englishhuangbo<span class="number">55.0</span></span><br><span class="line">englishhuangdatou<span class="number">56.0</span></span><br><span class="line">englishhuanglei<span class="number">83.0</span></span><br><span class="line">englishhuangxiaoming<span class="number">72.0</span></span><br><span class="line">englishliujialing<span class="number">62.0</span></span><br><span class="line">englishliuyifei<span class="number">66.5</span></span><br><span class="line">englishzhaobenshan<span class="number">69.0</span></span><br><span class="line">englishzhouqi<span class="number">64.0</span></span><br><span class="line">------------</span><br><span class="line">mathhuangjiaju<span class="number">82.0</span></span><br><span class="line">mathhuanglei<span class="number">74.0</span></span><br><span class="line">mathhuangxiaoming<span class="number">83.0</span></span><br><span class="line">mathliujialing<span class="number">72.0</span></span><br><span class="line">mathliutao<span class="number">56.0</span></span><br><span class="line">mathwangbaoqiang<span class="number">72.0</span></span><br><span class="line">mathxuzheng<span class="number">69.0</span></span><br></pre></td></tr></table></figure><h4 id="第3小题"><a href="#第3小题" class="headerlink" title="第3小题"></a>第3小题</h4><p>求出 <strong><em>每门课程</em></strong><sup>①</sup>参与考试的学生成绩 <strong><em>最高平局分<sup>②</sup></em></strong>   的学生的信息：<u>课程，姓名和平均分</u></p><p><br></p><p><strong>解题思路：</strong> </p><ul><li>通过题意得出<strong>2个结论</strong><ul><li><strong>课程要分组</strong></li><li><strong>平均分要排序</strong></li></ul></li><li>排序的话，交给<strong>key</strong>来做无疑是最好的，因为<strong>MapReduce</strong>会<strong>自动</strong>对<strong>key</strong>进行<strong>分组&amp;排序</strong></li><li>因此可以把 <code>课程&amp;平均分</code> 作为一个<strong>联合key</strong></li><li>为了操作方便，可以<strong>封装到一个对象</strong>中去： <strong>ScoreBean</strong></li><li><strong>分组和排序</strong>需要在 <strong>ScoreBean</strong>重写的<strong><code>compareTo()</code>方法中完成</strong></li><li>因为最后结果是求<strong>每门课程</strong>的<strong>最高平均分</strong>，因此需要对课程进行分组。</li><li>此时原本的默认分组（以Bean对象整体分组）就不管用了，需要<strong>自定义分组</strong></li><li>自定义分组要<strong>继承<code>WritableComparator</code></strong>，重写<code>compare()</code>方法，指定分组的规则。</li><li><strong>ScoreBean</strong>先按照组别进行排序，到<strong>reduce</strong>中时，已经是按照组，排好的数据，<strong>MyGroup</strong> 会把相同的比较结果放到同一个组中，分发到<strong>reduce</strong>.</li><li><strong>reduce</strong>中，只需要取出每组的第一个元素输出到上下文即可</li></ul><p><br></p><p><strong>图示</strong></p><div id="sequence-0"></div><p><br></p><p><strong>涉及知识点： mr中key的作用，自定义对象的用法，自定义分组，mr的执行流程</strong></p><ul><li>利用“班级和平均分”作为 key，可以将 map 阶段读取到的所有学生成绩数据按照班级 和成绩排倒序，发送到 reduce</li><li>在 reduce 端利用 GroupingComparator 将班级相同的 kv 聚合成组，然后取第一个即是最 大值</li></ul><p><br></p><h5 id="先贴个结论："><a href="#先贴个结论：" class="headerlink" title="先贴个结论："></a><strong>先贴个结论：</strong></h5><p><strong>执行流程结论</strong></p><ul><li>map每读一行就 write 到 context 一次，按照指定的<code>key</code>进行分发</li><li><p>map 把所有的数据都读完了之后，大概执行到<code>67%</code>的时候，开始进入 <code>CustomBean</code>，执行<code>CustomBean</code>的<code>compareTo()</code>方法，会按照自己写的规则一条一条数据比较</p></li><li><p>上述都比较完毕之后，map阶段就结束了，此时来到了 reduce阶段，但是是到了<code>67%</code>了</p></li><li>到了reduce阶段，直接进入了<strong>MyGroup</strong>中自定义的<strong>compare</strong>方法。</li><li>MyGroup的<code>compare()</code>方法，如果返回<strong>非0</strong>， 就会进入 reduce 方法<strong>写出到context</strong></li></ul><p><strong>MyGroup进入Reduce的条件是</strong></p><ul><li>MyReduce中，如果compare的结果不等于0，也就是比较的2者不相同， 此时就进入Reduce， 写出到上下文</li><li>如果相同，会一直往下读，直到读到不同的， 此时写出读到上下文。</li><li>因为MyGroup会在Reduce阶段执行，而<code>CustomBean</code>中的<code>compareTo()</code>是在map阶段执行，所以需要在<code>CustomBean</code>中就把组排好序，此时<strong>分组功能</strong>才能<strong>正常运作</strong></li></ul><p><strong>指定分组类MyGroup和不指定的区别</strong></p><p><u>指定与不指定是指：在Driver类中，是否加上<code>job.setGroupingComparatorClass(MyGrouper.class);</code>这一句。</u></p><ul><li><strong>指定分组类</strong>：<ul><li>会按照分组类中，自定义的<code>compare()</code>方法比较，相同的为一组，分完一组就进入一次reduce方法</li></ul></li><li><strong>不指定分组类：（目前存疑）</strong><ul><li>是否是按照key进行分组</li><li>如果是自定义类为key，是否是按照此key中值相同的分为一组</li><li>如果是hadoop内置类，是否是按照此类的值分组（Text-String的值，IntWritable-int值等..）</li><li><strong>依然是走得以上这套分组逻辑，一组的数据读完才进入到Reduce阶段做归并</strong></li></ul></li></ul><p><br></p><h5 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ScoreBean2 </span></span><br><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._02_分组组件;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.Getter;</span><br><span class="line"><span class="keyword">import</span> lombok.Setter;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Getter</span></span><br><span class="line"><span class="meta">@Setter</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ScoreBean2</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">ScoreBean2</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> String courseName;</span><br><span class="line"><span class="keyword">private</span> Double score;</span><br><span class="line">    </span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">out.writeUTF(courseName);</span><br><span class="line">out.writeDouble(score);</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.courseName = in.readUTF();</span><br><span class="line"><span class="keyword">this</span>.score = in.readDouble();</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 如果是相同课程, 按照分数降序排列的</span></span><br><span class="line"><span class="comment"> * 如果是不同课程, 按照课程名称升序排列</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(ScoreBean2 o)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 测试一下只写按分数降序排序</span></span><br><span class="line"><span class="comment">//return o.getScore().compareTo(this.getScore());</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 首先分组(只在相同的组内进行比较)</span></span><br><span class="line"><span class="keyword">int</span> nameRes = <span class="keyword">this</span>.getCourseName().compareTo(o.getCourseName());</span><br><span class="line"><span class="keyword">if</span> (nameRes == <span class="number">0</span>) &#123;</span><br><span class="line"><span class="comment">// 课程相同的时候才进行降序排序</span></span><br><span class="line"><span class="keyword">int</span> scoreRes = o.getScore().compareTo(<span class="keyword">this</span>.getScore());</span><br><span class="line"><span class="keyword">return</span> scoreRes;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> nameRes;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 实际上ScoreBean中是包含所有的参数的, 这里的输出可以自己设置</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> courseName + <span class="string">"\t"</span> + score;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ScoreBean2</span><span class="params">(String courseName, Double score)</span> </span>&#123;</span><br><span class="line"><span class="keyword">super</span>();</span><br><span class="line"><span class="keyword">this</span>.courseName = courseName;</span><br><span class="line"><span class="keyword">this</span>.score = score;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ScoreBean2</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">super</span>();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// ScorePlusDemo3 </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._02_分组组件;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparator;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ScorePlusDemo3</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line"> main     </span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line">job.setJarByClass(ScorePlusDemo3.class);</span><br><span class="line"></span><br><span class="line">job.setMapperClass(MyMapper.class);</span><br><span class="line">job.setReducerClass(MyReducer.class);</span><br><span class="line"></span><br><span class="line">job.setMapOutputKeyClass(ScoreBean2.class);</span><br><span class="line">job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">job.setGroupingComparatorClass(MyGrouper.class);</span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">String outP = <span class="string">"/out/scorePlus3"</span>;</span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"/in/newScoreIn"</span>));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(outP));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果输出目录存在,就先删除</span></span><br><span class="line">Path myPath = <span class="keyword">new</span> Path(outP);</span><br><span class="line">FileSystem fs = myPath.getFileSystem(conf);</span><br><span class="line"><span class="keyword">if</span> (fs.isDirectory(myPath)) &#123;</span><br><span class="line">fs.delete(myPath, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">boolean</span> waitForCompletion = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">System.exit(waitForCompletion ? <span class="number">0</span> : -<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"> Mapper </span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 输出: key: course</span></span><br><span class="line"><span class="comment"> *     value: score ...</span></span><br><span class="line"><span class="comment"> * 思路:</span></span><br><span class="line"><span class="comment"> * 1.不同课程要分开展示, 以 课程+分数 作为key, 在mapper中完成排序 </span></span><br><span class="line"><span class="comment"> * 2.在reduce中按照 MyGrouper 完成分组</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span></span></span><br><span class="line"><span class="class"><span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">ScoreBean2</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> String[] datas;</span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">datas = value.toString().trim().split(<span class="string">","</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>; i &lt; datas.length; i++) &#123;</span><br><span class="line">sum += Integer.parseInt(datas[i]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">double</span> avg = (<span class="keyword">double</span>) sum / (datas.length - <span class="number">2</span>);</span><br><span class="line">ScoreBean2 sb = <span class="keyword">new</span> ScoreBean2(datas[<span class="number">0</span>].trim(), avg);</span><br><span class="line"></span><br><span class="line">v.set(datas[<span class="number">1</span>].trim());</span><br><span class="line">context.write(sb, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"> Redecer     </span><br><span class="line"><span class="keyword">static</span> <span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span></span></span><br><span class="line"><span class="class"><span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">ScoreBean2</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line"><span class="keyword">int</span> count = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(ScoreBean2 key, Iterable&lt;Text&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 如果没有其它问题</span></span><br><span class="line"><span class="comment"> * 此时是按照课程分好组了, 同一个课程的所有学生都过来了, 并且学生成绩是排好的,</span></span><br><span class="line"><span class="comment"> * 如果此时求最大值, 只需要取出第一个即可 </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">// 进来一次只取第一个</span></span><br><span class="line">Text name = values.iterator().next();</span><br><span class="line">k.set(key.getCourseName() + <span class="string">"\t"</span> + name.toString() + <span class="string">"\t"</span></span><br><span class="line">+ key.getScore());</span><br><span class="line">context.write(k, NullWritable.get());</span><br><span class="line">context.write(<span class="keyword">new</span> Text(<span class="string">"==================第"</span>+count+<span class="string">"次进入reduce"</span>), NullWritable.get());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/*context.write(new Text("==================第"+count+"次进入reduce"), NullWritable.get());</span></span><br><span class="line"><span class="comment">for (Text name : values) &#123;</span></span><br><span class="line"><span class="comment">k.set(key.getCourseName() + "\t" + name.toString() + "\t"</span></span><br><span class="line"><span class="comment">+ key.getScore());</span></span><br><span class="line"><span class="comment">context.write(k, NullWritable.get());</span></span><br><span class="line"><span class="comment">context.write(new Text("---------in for write------"), NullWritable.get());</span></span><br><span class="line"><span class="comment">&#125;*/</span></span><br><span class="line">count++;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"> MyGrouper </span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 自定义分组  需要继承一个类WritableComparator</span></span><br><span class="line"><span class="comment"> * 重写compare方法</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyGrouper</span> <span class="keyword">extends</span> <span class="title">WritableComparator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// WritableComparator 此方法的默认无参构造是不会创建对象的, 需要自己重写</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">MyGrouper</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="comment">// 中间省去的参数是 Configuration, 如果为空, 会创建一个新的</span></span><br><span class="line"><span class="keyword">super</span>(ScoreBean2.class, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 此处比较的是2个  WritableComparable 对象, 需要强转一下具体的类对象</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@SuppressWarnings</span>(<span class="string">"rawtypes"</span>)</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> </span>&#123;</span><br><span class="line">ScoreBean2 aBean = (ScoreBean2) a;</span><br><span class="line">ScoreBean2 bBean = (ScoreBean2) b;</span><br><span class="line"><span class="comment">// 返回分组规则</span></span><br><span class="line">System.out.println(aBean.getCourseName()+<span class="string">"---MyGroup中比较---"</span>+(bBean.getCourseName()));</span><br><span class="line"><span class="keyword">return</span> aBean.getCourseName().compareTo(bBean.getCourseName());</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">================================================================================</span><br><span class="line"> 执行结果 </span><br><span class="line">================================================================================</span><br><span class="line"></span><br><span class="line">algorithmhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">==================第<span class="number">1</span>次进入reduce</span><br><span class="line">computerhuangjiaju<span class="number">83.2</span></span><br><span class="line">==================第<span class="number">2</span>次进入reduce</span><br><span class="line">englishhuanglei<span class="number">83.0</span></span><br><span class="line">==================第<span class="number">3</span>次进入reduce</span><br><span class="line">mathhuangxiaoming<span class="number">83.0</span></span><br><span class="line">==================第<span class="number">4</span>次进入reduce</span><br></pre></td></tr></table></figure><h2 id="MR实现两个表的数据关联Join"><a href="#MR实现两个表的数据关联Join" class="headerlink" title="MR实现两个表的数据关联Join"></a>MR实现两个表的数据关联<code>Join</code></h2><h3 id="题目-2"><a href="#题目-2" class="headerlink" title="题目"></a>题目</h3><blockquote><p>订单数据表t_order：  flag=0<br>id    date    pid    amount<br>1001    20150710    P0001    2<br>1002    20150710    P0001    3<br>1003    20150710    P0002    3<br>Id:数据记录id<br>Date   日期<br>Pid     商品id<br>Amount    库存数量</p><p>6.商品信息表t_product   flag=1<br>pid    name    category_id    price<br>P0001    小米5    C01    2000<br>P0002    锤子T1    C01    3500</p><p>mr实现两个表的数据关联<br>id   pid    date    amount    name    category_id     price</p></blockquote><p><br></p><h3 id="答案1-Reducer-端-实现-Join"><a href="#答案1-Reducer-端-实现-Join" class="headerlink" title="答案1 : Reducer 端 实现 Join"></a>答案1 : Reducer 端 实现 <code>Join</code></h3><h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><ul><li><p>map端</p><ul><li><p>读取到当前路径下，所有文件的切片信息， 根据文件名判断是那张表</p><ul><li><p>在setup中，从文件切片中获取到文件名</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取读取到的切片相关信息,一个切片对应一个 maptask</span></span><br><span class="line">InputSplit inputSplit = context.getInputSplit();</span><br><span class="line"><span class="comment">// 转换为文件切片</span></span><br><span class="line">FileSplit fs = (FileSplit)inputSplit;</span><br><span class="line"><span class="comment">// 获取文件名</span></span><br><span class="line">filename = fs.getPath().getName();</span><br></pre></td></tr></table></figure></li><li><p>这里总共会获得2个文件名（指定目录存了2个指定文件），一个文件名对应一个切片</p></li></ul></li><li><p>关联字段作为key， 其它的作为value，在value前面加上当前文件的名称标记</p></li></ul></li><li><p>reduce端</p><ul><li>通过标记区分两张表，把读取到的信息，分别存入2个list中</li><li>遍历大的表，与小表进行拼接（小表的相同pid记录只会有一条）</li><li>拼接完成后即可写出</li></ul></li></ul><h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._03_join2表的数据关联;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReduceJoinDemo</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 指定HDFS相关参数</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//  创建/配置 Job</span></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Jar包类型</span></span><br><span class="line">job.setJarByClass(ReduceJoinDemo.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Map Reduce执行类</span></span><br><span class="line">job.setMapperClass(MyMapper.class);</span><br><span class="line">job.setReducerClass(MyReducer.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Map输出类</span></span><br><span class="line">job.setMapOutputKeyClass(Text.class);</span><br><span class="line">job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Reduce输出类</span></span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置输入 输出路径</span></span><br><span class="line">String inP = <span class="string">"/in/joindemo"</span>;</span><br><span class="line">String outP = <span class="string">"/out/joinout1"</span>;</span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(inP));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(outP));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置如果存在路径就删除</span></span><br><span class="line">        Path mypath = <span class="keyword">new</span> Path(outP);</span><br><span class="line">        FileSystem hdfs = mypath.getFileSystem(conf);</span><br><span class="line">        <span class="keyword">if</span> (hdfs.isDirectory(mypath)) &#123;</span><br><span class="line">            hdfs.delete(mypath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//  执行job</span></span><br><span class="line">        <span class="keyword">boolean</span> waitForCompletion = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(waitForCompletion?<span class="number">0</span>:-<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 思路: 读取2个表中的数据,进行标记发送</span></span><br><span class="line"><span class="comment"> * key: 两表需要关联的字段</span></span><br><span class="line"><span class="comment"> * value: 其它值, 需要标记， 标记数据的来源</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * **核心： 关联条件**</span></span><br><span class="line"><span class="comment">- 想要在 reduce 端完成 join， 要在 reduce 端可以同时接收到两个表中的数据</span></span><br><span class="line"><span class="comment">- 要保证在 Map 端进行读文件的时候， 读到2个表的数据， 并且需要对2个表的数据进行区分</span></span><br><span class="line"><span class="comment">- 将2个表放在同一个目录下</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">解决: </span></span><br><span class="line"><span class="comment">mapper 开始执行时, 在setup方法中, 从上下文中取到文件名, 根据文件名打标记</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">String filename = <span class="string">""</span>;</span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">// 获取读取到的切片相关信息,一个切片对应一个 maptask</span></span><br><span class="line">InputSplit inputSplit = context.getInputSplit();</span><br><span class="line"><span class="comment">// 转换为文件切片</span></span><br><span class="line">FileSplit fs = (FileSplit)inputSplit;</span><br><span class="line"><span class="comment">// 获取文件名</span></span><br><span class="line">filename = fs.getPath().getName();</span><br><span class="line">System.out.println(<span class="string">"本次获取到的文件名为-----"</span>+filename);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value,</span></span></span><br><span class="line"><span class="function"><span class="params">Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">// 解析出来每一行内容, 打标记, 发送</span></span><br><span class="line">String[] infos = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (filename.equals(<span class="string">"order"</span>)) &#123;</span><br><span class="line">k.set(infos[<span class="number">2</span>]);</span><br><span class="line"><span class="comment">// 设置标记前缀为 OR</span></span><br><span class="line">v.set(<span class="string">"OR"</span>+infos[<span class="number">0</span>]+<span class="string">"\t"</span>+infos[<span class="number">1</span>]+<span class="string">"\t"</span>+infos[<span class="number">3</span>]);</span><br><span class="line">&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">k.set(infos[<span class="number">0</span>]);</span><br><span class="line"><span class="comment">// 设置标记前缀为 PR</span></span><br><span class="line">v.set(<span class="string">"PR"</span>+infos[<span class="number">1</span>]+<span class="string">"\t"</span>+infos[<span class="number">2</span>]+<span class="string">"\t"</span>+infos[<span class="number">3</span>]);</span><br><span class="line">&#125;</span><br><span class="line">context.write(k, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Reducer&lt;Text, Text, Text, NullWritable&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 首先明确 product 和 order 是 一对多的关系</span></span><br><span class="line"><span class="comment"> * 根据前缀不同,取到2个不同的表存进2个容器中</span></span><br><span class="line"><span class="comment"> * 遍历多的表, 与一进行拼接</span></span><br><span class="line"><span class="comment"> * 最后写出到上下文</span></span><br><span class="line"><span class="comment"> * 最终的输出格式 id   pid    date    amount    name    category_id     price</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">// 因为每次遍历到不同的pid, 都会走进来一次, list也会有新的输出,所以必须定义在里面,每次进来都要初始化</span></span><br><span class="line">List&lt;String&gt; productList =<span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">List&lt;String&gt; orderList =<span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (Text v : values) &#123;</span><br><span class="line">String vStr = v.toString();</span><br><span class="line"><span class="keyword">if</span> (vStr.startsWith(<span class="string">"OR"</span>)) &#123;</span><br><span class="line">orderList.add(vStr.substring(<span class="number">2</span>));</span><br><span class="line">&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">productList.add(vStr.substring(<span class="number">2</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 此时2个list添加完了本次 相同的 key(pid) 的所有商品</span></span><br><span class="line"><span class="comment">// 遍历多的进行拼接</span></span><br><span class="line"><span class="keyword">for</span> (String or : orderList) &#123;</span><br><span class="line"><span class="comment">// 相同的 pid的 product 只有一个, productList中的数量是1</span></span><br><span class="line"><span class="comment">// 但是相同pid 的 订单 可能有多个</span></span><br><span class="line">String res =  key.toString() + <span class="string">"\t"</span> + or + productList.get(<span class="number">0</span>);</span><br><span class="line">k.set(res);</span><br><span class="line">context.write(k, NullWritable.get());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><br></p><h3 id="※-答案2-：-Mapper-端实现-Join-※"><a href="#※-答案2-：-Mapper-端实现-Join-※" class="headerlink" title="※ 答案2 ： Mapper 端实现 Join  ※"></a>※ 答案2 ： Mapper 端实现 <code>Join</code>  ※</h3><h4 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h4><ul><li>创建job的时候,把小表加入缓存 在map的setup中, </li><li>读取缓存中的数据, 存入一个成员变量 map中<ul><li>map方法中,只需要读一个表, 然后根据关联条件(关联key: pid)消除笛卡尔集,进行拼接</li><li>map直接输出, 甚至都不需要reduce</li></ul></li></ul><h4 id="注意点"><a href="#注意点" class="headerlink" title="注意点:"></a>注意点:</h4><ul><li><p>需要达成jar包运行, 直接用Eclipse会找不到缓存</p></li><li><p><strong>jar包执行方法</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果代码内部指定了输入输出路径，后面的/in，/out参数可以不加</span></span><br><span class="line">hadoop jar xxxx.jar com.rox.xxx.xxxx(主方法)  /<span class="keyword">in</span>/xx   /out/xx</span><br></pre></td></tr></table></figure></li><li><p>如果没有Reduce方法</p><ul><li><p>main方法中，设置map的写出key，value,应该用 <code>setOutputKeyClass</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//// 设置Map输出类 (因为这里没有Reduce, 所以这里是最终输出,一定要注意!!!)//////////</span></span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(NullWritable.class);</span><br></pre></td></tr></table></figure></li><li><p>要设置reduce task 的个数为0 </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setNumReduceTasks(<span class="number">0</span>);</span><br></pre></td></tr></table></figure></li><li><p>把小文件加载到缓存中的方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">////////////// 将小文件加载到缓存  </span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"/in/joindemo/product"</span>));</span><br></pre></td></tr></table></figure></li></ul></li></ul><p>  ​    </p><h4 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._03_join;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.FileReader;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MapJoinDemo</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 指定HDFS相关参数</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//  创建/配置 Job</span></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Jar包类型:这里千万别写错了</span></span><br><span class="line">job.setJarByClass(MapJoinDemo.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Map Reduce执行类</span></span><br><span class="line">job.setMapperClass(MyMapper.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">///////////// 设置Map输出类 (因为这里没有Reduce, 所以这里是最终输出,一定要注意!!!)//////////</span></span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">////////////// 设置reduce执行个数为0</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">////////////// 将小文件加载到缓存  </span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"/in/joindemo/product"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置输入 输出路径</span></span><br><span class="line">String inP = <span class="string">"/in/joindemo/order"</span>;</span><br><span class="line">String outP = <span class="string">"/out/joinout2"</span>;</span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(inP));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(outP));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置如果存在路径就删除</span></span><br><span class="line">        Path mypath = <span class="keyword">new</span> Path(outP);</span><br><span class="line">        FileSystem hdfs = mypath.getFileSystem(conf);</span><br><span class="line">        <span class="keyword">if</span> (hdfs.isDirectory(mypath)) &#123;</span><br><span class="line">            hdfs.delete(mypath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//  执行job</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="keyword">true</span>)?<span class="number">0</span>:-<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 思路: </span></span><br><span class="line"><span class="comment"> * 创建job的时候,把小表加入缓存</span></span><br><span class="line"><span class="comment"> * 在map的setup中, 读取缓存中的数据, 存入一个成员变量 map中</span></span><br><span class="line"><span class="comment"> * map方法中,只需要读一个表, 然后根据关联条件(关联key: pid)消除笛卡尔集,进行拼接</span></span><br><span class="line"><span class="comment"> * 直接输出, 甚至都不需要reduce</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 注意点: </span></span><br><span class="line"><span class="comment"> * 需要达成jar包运行, 直接用Eclipse会找不到缓存</span></span><br><span class="line"><span class="comment"> * 格式: hadoop jar包本地路径 jar包主方法全限定名 hadoop输入  hadoop输出</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建装载小表的map, key存储 关联键, value存其它</span></span><br><span class="line">Map&lt;String, String&gt; proMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">// 获取缓存中存储的小表 (一般是 一对多中的 一), 因为只存了1个,所以直接取第0个</span></span><br><span class="line">Path path = context.getLocalCacheFiles()[<span class="number">0</span>];</span><br><span class="line">String pString = path.toString();</span><br><span class="line"><span class="comment">// 开启in流, BufferedReader 逐行读取文件</span></span><br><span class="line">BufferedReader br = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> FileReader(pString));</span><br><span class="line">String line = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">while</span> ((line = br.readLine()) != <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="comment">// 成功读取一行</span></span><br><span class="line">String[] infos = line.split(<span class="string">"\t"</span>);</span><br><span class="line"><span class="comment">// 存进proMap</span></span><br><span class="line">proMap.put(infos[<span class="number">0</span>],</span><br><span class="line">infos[<span class="number">1</span>] + <span class="string">"\t"</span> + infos[<span class="number">2</span>] + <span class="string">"\t"</span> + infos[<span class="number">3</span>]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//br.close();</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 直接从路径读取大文件</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">String[] infos = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">String pid = infos[<span class="number">2</span>];</span><br><span class="line"><span class="comment">//进行关联   pid到map中匹配   如果包含  证明匹配上了</span></span><br><span class="line"><span class="comment">// 艹, 这里pid之前加了 "", 妈的,当然找不到啦!!!</span></span><br><span class="line"><span class="keyword">if</span> (proMap.containsKey(pid)) &#123;</span><br><span class="line">String res = value.toString() + <span class="string">"\t"</span> + proMap.get(pid);</span><br><span class="line">k.set(res);</span><br><span class="line">context.write(k, NullWritable.get());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><script src="https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js"></script><script src="https://cdn.bootcss.com/snap.svg/0.5.1/snap.svg-min.js"></script><script src="https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js"></script><script src="https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js"></script><script src="https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js"></script><textarea id="sequence-0-code" style="display: none">title: 执行流程时序图 Mapper(map)->ScoreBean: k:ScoreBean(courseName,avgScore)Mapper(map)->ScoreBean: v:Text(stuName)Mapper(ScoreBean)->Reducer(MyGroup): course按字典升序Mapper(ScoreBean)->Reducer(MyGroup): course内成绩降序Reducer(MyGroup)->Reducer(reduce): 根据自定义的分组规则按组输出Reducer(MyGroup)->Reducer(reduce): 一组只调用reduce一次</textarea><textarea id="sequence-0-options" style="display: none">{"theme":"simple"}</textarea><script>  var code = document.getElementById("sequence-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value));  var diagram = Diagram.parse(code);  diagram.drawSVG("sequence-0", options);</script></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;求微博共同粉丝&quot;&gt;&lt;a href=&quot;#求微博共同粉丝&quot; class=&quot;headerlink&quot; title=&quot;求微博共同粉丝&quot;&gt;&lt;/a&gt;求微博共同粉丝&lt;/h2&gt;&lt;h3 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce笔记-3</title>
    <link href="https://airpoet.github.io/2018/06/08/Hadoop/Study/2-MapReduce/MapReduce%E7%AC%94%E8%AE%B0-3/"/>
    <id>https://airpoet.github.io/2018/06/08/Hadoop/Study/2-MapReduce/MapReduce笔记-3/</id>
    <published>2018-06-08T01:23:27.815Z</published>
    <updated>2018-06-13T13:32:37.816Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-多-Job-串联"><a href="#1-多-Job-串联" class="headerlink" title="1.多 Job 串联"></a>1.多 Job 串联</h2><h3 id="1-概念"><a href="#1-概念" class="headerlink" title="1.概念"></a>1.概念</h3><p>当程序中有多个 Job， 并且多个 job 之间相互依赖， a ， job 需要依赖另一个b，job 的执行结果时候， 此时需要使用多 job 串联</p><h3 id="2-涉及到昨天的微博求共同粉丝题目"><a href="#2-涉及到昨天的微博求共同粉丝题目" class="headerlink" title="2. 涉及到昨天的微博求共同粉丝题目"></a>2. 涉及到昨天的微博求共同粉丝题目</h3><blockquote><p>A:B,C,D,F,E,O<br>B:A,C,E,K<br>C:F,A,D,I<br>D:A,E,F,L<br>E:B,C,D,M,L<br>F:A,B,C,D,E,O,M<br>G:A,C,D,E,F<br>H:A,C,D,E,O<br>I:A,O<br>J:B,O<br>K:A,C,D<br>L:D,E,F<br>M:E,F,G<br>O:A,H,I,J,K</p><p>以上是数据：<br>A:B,C,D,F,E,O<br>表示：A用户 关注B,C,D,E,F,O</p><p>求所有两两用户之间的共同关注对象</p></blockquote><h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><ul><li>要写2个MapReduce， 开启2个job</li><li>后一个job依赖于前一个的执行结果</li><li>后一个job的输入文件路径，就是前一个job的输出路径</li><li>2个job需要添加依赖</li></ul><h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><p><a href="https://airpoet.github.io/2018/06/09/Hadoop/Study/2-MapReduce/MapReduce-%E7%BB%83%E4%B9%A0/#more">参考练习-第一题-求微博共同好友</a></p><h4 id="多-Job-串联部分代码"><a href="#多-Job-串联部分代码" class="headerlink" title="多 Job 串联部分代码"></a><strong>多 Job 串联部分代码</strong></h4><ul><li><p>基本的写到一起， job1， job2</p></li><li><p>用<code>JobControl</code>对象管理多 job， 会将多个 job 当做一个组中的 job 提交， 参数指的是组名， 随意起</p></li><li><p>原生的 job 要转为可控制的 job</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建 JobControl 组</span></span><br><span class="line">JobControl jc = <span class="keyword">new</span> JobControl(<span class="string">"common_friend"</span>);</span><br><span class="line"><span class="comment">// job 拿好配置， 加入 ControlledJob 管理, 变成可控制的 job</span></span><br><span class="line">ControlledJob ajob = <span class="keyword">new</span> ControlledJob(job1.getConfiguration());</span><br><span class="line">ControlledJob bjob = <span class="keyword">new</span> ControlledJob(job2.getConfiguration());</span><br><span class="line"><span class="comment">// 添加依赖关系</span></span><br><span class="line">bjob.addDependingJob(ajob);  </span><br><span class="line"><span class="comment">//  添加 job进 JC</span></span><br><span class="line">jc.addJob(ajob);</span><br><span class="line">jc.addJob(bjob);</span><br><span class="line"><span class="comment">// 启动线程</span></span><br><span class="line">Thread jobControlTread = <span class="keyword">new</span> Thread(jc);</span><br><span class="line">jobControlTread.start();</span><br><span class="line"><span class="comment">// 在线程完成之后关闭</span></span><br><span class="line"><span class="keyword">while</span>(!jc.allFinished()) &#123;</span><br><span class="line">Thread.sleep(<span class="number">500</span>);</span><br><span class="line">&#125;</span><br><span class="line">jobControl.stop();</span><br></pre></td></tr></table></figure></li></ul><hr><h2 id="2-分组组件"><a href="#2-分组组件" class="headerlink" title="2. 分组组件"></a>2. 分组组件</h2><p><strong>map–分组–reduce</strong>  </p><p>reduce 接收到的数据是按照 map 输出的 key 进行分组的, 分组的时候按照 key 相同的时候为一组,  默认都实现了 <code>WritableComparable</code>接口， 其中的 compareTo（）方法返回为0的时候 默认为一组， 返回不为0， 则分到下一组</p><p><br></p><p><strong>自定义分组使用场景：</strong> 默认的数据分组不能满足需求</p><blockquote><p>一、数据解释</p><p>数据字段个数不固定：<br>第一个是课程名称，总共四个课程，computer，math，english，algorithm，<br>第二个是学生姓名，后面是每次考试的分数</p><p>二、统计需求：<br>1、统计每门课程的参考人数和课程平均分</p><p>2、统计每门课程参考学生的平均分，并且按课程存入不同的结果文件，要求一门课程一个结果文件</p><p>3、求出每门课程参考学生成绩最高平均分的学生的信息：课程，姓名和平均分</p></blockquote><p><strong>第三题： 要求就是分组求最大值， 两件事情： 分组， 排序（shuffle）</strong></p><p><strong>总结：</strong> </p><p>1、利用“班级和平均分”作为 key，可以将 map 阶段读取到的所有学生成绩数据按照班级 和成绩排倒序，发送到 reduce</p><p>2、在 reduce 端利用 GroupingComparator 将班级相同的 kv 聚合成组，然后取第一个即是最 大值</p><h4 id="具体参考："><a href="#具体参考：" class="headerlink" title="具体参考："></a>具体参考：</h4><p><a href="https://airpoet.github.io/2018/06/09/Hadoop/Study/2-MapReduce/MapReduce-%E7%BB%83%E4%B9%A0/#more">练习-求学生成绩-第三小题</a></p><hr><h2 id="3-Reduce-中的2个坑"><a href="#3-Reduce-中的2个坑" class="headerlink" title="3. Reduce 中的2个坑"></a>3. Reduce 中的2个坑</h2><h3 id="坑1"><a href="#坑1" class="headerlink" title="坑1"></a>坑1</h3><p><strong>Iterable\<text>只能循环遍历一次</text></strong></p><p>迭代器每次循环遍历完成， 指针都会移动到最后一个</p><p>系统类型，没事</p><p>自定义类型 ，有问题？</p><h3 id="坑2"><a href="#坑2" class="headerlink" title="坑2"></a>坑2</h3><p><strong>迭代器中所有对象公用同一个地址</strong></p><hr><h2 id="4-Reduce-端的-Join"><a href="#4-Reduce-端的-Join" class="headerlink" title="4. Reduce 端的 Join"></a>4. Reduce 端的 Join</h2><p><strong>牺牲效率换执行</strong></p><p>思路： </p><p><strong>核心： 关联条件</strong></p><ul><li>想要在 reduce 端完成 join， 要在 reduce 端可以同时接收到两个表中的数据</li><li>要保证在 Map 端进行读文件的时候， 读到2个表的数据， 并且需要对2个表的数据进行区分</li><li>将2个表放在同一个目录下</li></ul><h4 id="Map-端"><a href="#Map-端" class="headerlink" title="Map 端"></a>Map 端</h4><ul><li><strong>读取两个表中的数据， 进行切分、发送</strong></li><li>key ： 公共字段–关联字段–<code>pid</code></li><li>value： 剩下的字段， 标记数据的来源表</li></ul><h4 id="Reduce-端"><a href="#Reduce-端" class="headerlink" title="Reduce 端"></a>Reduce 端</h4><ul><li>通过编辑分离出2个表的数据</li><li>分别存到2个容器中（ArrayList）</li><li>遍历大表，拼接小表</li></ul><h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><p><a href="https://airpoet.github.io/2018/06/09/Hadoop/Study/2-MapReduce/MapReduce-%E7%BB%83%E4%B9%A0/#more">参考练习-第三题MR实现2个表之间的Join</a></p><h3 id="缺陷"><a href="#缺陷" class="headerlink" title="缺陷"></a>缺陷</h3><p><strong>1. ReduceTask 的并行度问题：</strong></p><ul><li>建议0.95*datanode 的个数</li><li>并行度不高， 性能不高</li></ul><p><strong>2. 容器性能</strong></p><ul><li>list 等， 不提倡， reduce 接收的数据， 可能会很大</li></ul><p><strong>3. ReduceTask 容易产生数据倾斜</strong></p><ul><li>假设我们设置多个 ReduceTask， 根据分区规则， 默认 hash</li><li>以 key关联条件分，  ReduceTask数据倾斜， 每个 ReduceTask 分工不均， 非常影响性能，没有合理的利用集群资源</li><li>在真实的生产中一定要尽量的避免数据倾斜</li><li>最好的做法：将分区设计的足够完美，难度比较大</li><li>因此，ReduceTask 一般不会完成 John工作</li><li><strong>放在 Map 端完成就不会有这个问题了</strong></li></ul><hr><h5 id="补充：Mapper-中的源码分析"><a href="#补充：Mapper-中的源码分析" class="headerlink" title="补充：Mapper 中的源码分析"></a>补充：Mapper 中的源码分析</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 在 maptask 执行之前调用一次， 一个 maptask 只会调用一次。setup 中通常会帮 map 中初始化一些变量和资源， 比如数据库的连接等。</span></span><br><span class="line">    <span class="comment">// 主要目的：减少资源的初始化次数而提升程序的性能</span></span><br><span class="line">    setup(context);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 获取文件是否还有下一行， 一行只调用一次</span></span><br><span class="line">      <span class="keyword">while</span> (context.nextKeyValue()) &#123;</span><br><span class="line">        map(context.getCurrentKey(), context.getCurrentValue(), context);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">       <span class="comment">// maptask 任务执行完成之后会调用一次，一个 maptask 只会调用一次</span></span><br><span class="line">       <span class="comment">// 帮 map 处理一些善后工作， 比如：资源的关闭</span></span><br><span class="line">      cleanup(context);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><hr><h2 id="5-Map-端的-Join"><a href="#5-Map-端的-Join" class="headerlink" title="5. Map 端的 Join"></a>5. Map 端的 Join</h2><p><strong>注意点：这种方式只能通过 Jar 包上传的方式，直接用 Eclipse 会找不到缓存</strong> </p><p>为了提升 Map 端 Join 性能， 我们的策略是， <strong>将小表的数据加载到每个运行的 MapTask 的内存中</strong>。</p><p>如果小表被加载到了内存中， 我们<strong>每次在 Map 端只需要读取大表，当读取到大表的每一行数据，可以直接和内存中的小表进行关联。</strong></p><p>这个时候，<strong>只需要 Map 就可以完成 Join 操作了</strong>。</p><p><br></p><h3 id="1-如何将小表加入到内存中？"><a href="#1-如何将小表加入到内存中？" class="headerlink" title="1. 如何将小表加入到内存中？"></a>1. 如何将小表加入到内存中？</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将指定路径文件加载到缓存中</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"/xxx"</span>));</span><br></pre></td></tr></table></figure><h3 id="2-Map-端怎样读取缓存中的数据"><a href="#2-Map-端怎样读取缓存中的数据" class="headerlink" title="2. Map 端怎样读取缓存中的数据"></a>2. Map 端怎样读取缓存中的数据</h3><p>想要在 Java 中使用缓存中的数据，缓存中的数据必须封装到 Java 的容器中</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取缓存文件</span></span><br><span class="line">context.getLocalCacheFiles()[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h3 id="3-代码"><a href="#3-代码" class="headerlink" title="3. 代码"></a>3. 代码</h3><p><a href="https://airpoet.github.io/2018/06/09/Hadoop/Study/2-MapReduce/MapReduce-%E7%BB%83%E4%B9%A0/#more">参考练习-第3题</a></p><p><strong>代码注意点：</strong> </p><p><strong>setup</strong>：从缓存读取一文件（多对一的一）到 HashMap</p><p><strong>main 方法中注意点</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 指定文件加入缓存</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"/xxx"</span>)); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果没有ReduceTask， 要设置为0</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">0</span>);</span><br></pre></td></tr></table></figure><hr><h2 id="6-对比"><a href="#6-对比" class="headerlink" title="6. 对比"></a>6. 对比</h2><p><strong>MapJoin 的方式： 大 &amp; 小表</strong></p><p>因为有一个表需要加载到内存中，注定加载到内存中的表不能过大（hive 中默认是256M）</p><p><strong>大表 &amp; 大表 如何设计</strong></p><ul><li>ReduceJoin ： 解决数据倾斜的问题，合理设计分区。 —很难做到</li><li>将其中一个<strong>大表进行切分</strong>，切分成小表， <strong>最终执行 大表 &amp; 小表</strong></li></ul><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul><li><p>并行度高，不存在数据倾斜的问题，运行效率高</p></li><li><p><strong>优先选择MapJoin</strong></p></li></ul><hr><h2 id="arrow-forward-7-排序算法-（待整理：TODO）"><a href="#arrow-forward-7-排序算法-（待整理：TODO）" class="headerlink" title=":arrow_forward: 7. 排序算法 （待整理：TODO）"></a>:arrow_forward: 7. 排序算法 （待整理：TODO）</h2><h3 id="1-快速排序"><a href="#1-快速排序" class="headerlink" title="1. 快速排序"></a>1. 快速排序</h3><p><strong>边界值始终是不变的。</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-08-083614.png" alt="image-20180608163614370"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-08-083842.png" alt="image-20180608163841417"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-08-084004.png" alt="image-20180608164004311"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-08-084128.png" alt="image-20180608164127167"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-08-084319.png" alt="image-20180608164318335"></p><h3 id="2-归并排序"><a href="#2-归并排序" class="headerlink" title="2. 归并排序"></a>2. 归并排序</h3><p>一般情况针对<strong>有序的</strong>，<strong>多个</strong>， <strong>小</strong>数据集</p><blockquote><p>应用场景：想到了多个Reduce 任务产生的多个文件的合并</p></blockquote><h4 id="1-归并排序前传：-合并多个数组"><a href="#1-归并排序前传：-合并多个数组" class="headerlink" title="1. 归并排序前传： 合并多个数组"></a>1. 归并排序前传： 合并多个数组</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-08-085031.png" alt="image-20180608165031396"></p><h4 id="2-归并排序-之-一个大数据集"><a href="#2-归并排序-之-一个大数据集" class="headerlink" title="2. 归并排序 之 一个大数据集"></a>2. 归并排序 之 一个大数据集</h4><h5 id="—归———"><a href="#—归———" class="headerlink" title="—归———-"></a>—归———-</h5><p><strong>切分成单个的数据集</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-08-085652.png" alt="image-20180608165651868"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-08-085727.png" alt="image-20180608165727484"></p><h5 id="—-并———"><a href="#—-并———" class="headerlink" title="—-并———"></a>—-并———</h5><ol><li>两两相并， 并成新的数组， 小的先放入数组， 再放大的</li><li>新的数组再不断执行 上述的 <strong>合并多个数组</strong></li></ol><hr><h2 id="8-※-Shuffle-过程-※"><a href="#8-※-Shuffle-过程-※" class="headerlink" title="8.  ※ Shuffle 过程 ※"></a>8.  ※ Shuffle 过程 ※</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-024417.png" alt="image-20180611104416430"></p><ul><li>mapper 阶段处理的数据如何传递给 reducer 阶段，是 MapReduce 框架中 最关键的一个流程，这个流程就叫 Shuffle</li><li>Shuffle<ul><li>即 数据混洗  ——  核心机制：数据分区，排序，局部聚合，缓存，拉取，再合并，排序； </li><li>环形缓冲区<ul><li>内存中的一种首尾相连的数据结构（底层是字节数组），kvbuffer 包含原始数据区和元数据区，一个mapTask任务对应一个环形缓冲区</li><li>默认大小 100M，默认阈(yu)值 0.8，即当达到 80M 后，会触发 spill溢写 操作，将数据写入磁盘，此时mapper输出会继续向剩余20M中写数据<br>缓冲区大小 mapred-site.xml：mapreduce.task.io.sort.mb<br>阈值 mapred-site.xml：mapreduce.map.sort.spill.percent<br>路径：mapred-site.xml：mapreduce.cluster.local.dir</li><li>如果此80M数据写入磁盘完成前，剩余20M缓冲区也写完，则会进入阻塞状态，直到是spill完成腾出缓冲区空间</li><li>赤道（equtor）：环形缓冲区中原始数据和元数据的边界<ul><li>原始数据：mapTask输出的数据</li><li>元数据<ul><li>记录原始数据的数据，包含4部分内容，占16*4字节；</li><li>每一条元数据占用空间是一样的，排序可以通过交换元数据实现</li><li>分类<ul><li>a. 原始数据中key的起始位置</li><li>b. 原始数据中value的起始位置</li><li>c. value的长度</li><li>d. 分区信息，即该条信息属于哪个分区</li></ul></li></ul></li></ul></li></ul></li><li>核心操作<ul><li>\1. 分区 partition（如果 reduceTask 只有一个或者没有，那么 partition 将不起作用） </li><li>\2. Sort 根据 key 排序（MapReduce 编程中的 sort 是一定会做的，并且 只能按照 key排序， 当然 如果没有 reducer 阶段，那么就不会对 key 排序） </li><li>\3. Combiner 进行局部 value 的合并（Combiner 是可选的组件，作用只是为了提高任务的执行效率）</li></ul></li><li>详细过程<ul><li>\1. 一个大文件需要处理，它在在 HDFS 上是以 block 块形式存放，每个 block 默认为 128M 存 3 份；运行时每个 map 任务会处理一个切块（split），如果 block 大和 split 相同，有多少个 block 就有多少个 map 任务；所以对整个文件处理时会有很多 map 任务进行并行计算。</li><li>\2. 每个 map 任务处理完输入的切块后会把结果写入到内存的一个 环形缓冲区，写入过程中会进行简单排序，当缓冲区的大小阀值，一个后台的线程就会启动把缓冲区中的数据溢写（spill）到本地磁盘中，同时Mapper继续时向环形缓冲区中写入数据。<ul><li>数据溢写入到磁盘之前，首先会根据 reducer 的数量划分成同数量的分区（partition），每个分区中的都数据会有后台线程根据 map 任务的输出结果 key 进行排序；</li><li>如果有 combiner，它会在 缓冲区溢写到磁盘之前 和 mapTask排好序的输出上 运行，使写到本地磁盘和传给 reducer 的数据更少；<br>Combiner即是把同一分区中的同一key的数据进行合并，整个shuffle过程会调用两个Combiner !</li><li>最后在本地生成分好区且排好序的小文件。</li><li>注意：如果 map 向环形缓冲区写入数据的速度大于向本地写入数据的速度，环形缓冲区会被写满，向环形缓冲区写入数据的线程会阻塞直至缓冲区中的内容全部溢写到磁盘后再次启动，到阀值后会向本地磁盘新建一个溢写文件；</li></ul></li><li>\3. map 任务完成之前，会把本地磁盘溢写的所有文件 不停地 合并（merge）成得到一个结果文件，合并得到的结果文件会根据小溢写文件的分区而分区，每个分区的数据会再次根据 key 进行 排序，得到的结果文件是分好区且排好序的（可以合并成一个文件的溢写文件数量默认为10）；<br>默认合并溢写文件数量 mapred-site.xml：mapreduce.task.io.sort.factor</li><li>\4.  reduce 任务启动，Reducer 中的一个线程定期向 MRAppMaster 询问 Mapper 输出结果文件位置，Mapper  结束后会向 MRAppMaster 汇报信息，从而 Reducer 会得知 Mapper 状态并得到 map 结果文件目录；<br>reduce任务数配置<br>a) mapred-site.xml：mapreduce.job.reduces<br>b) job.setNumReduceTasks(num)</li><li>\6. 当有一个 Mapper 结束时，reduce 任务进入复制阶段，reduce 任务通过 http 协议（hadoop 内置了netty容器）把所有 Mapper 结果文件的 对应的分区数据 拉取（fetch）过来，Reducer  可以并行复 制 Mapper 的 结果 ， 默认线程数为5； 所有 Reducer 复制完成 map 结果文件后，由于 Reducer  可能会失败，NodeManager 并不会在第一个 map 结果文件复制完成后就删除它，而是直到作业完成后 MRAppMaster 通知  NodeManager 进行删除； 另外如果 map 结果文件相当小，则会被直接复制到 reduce NodeManager  的内存；一旦缓冲区达到 reduce 的阈值大小 0.66 或 写入到 reduce NodeManager 内 存 中 文 件 个 数 达 到  map 输出阈值 1000，reduce 就会把 map 结果文件合并（merge）溢写到本地；<br>默认线程数 mapred-site.xml：mapreduce.reduce.shuffle.parallelcopies<br>缓冲区大小 mapred-site.xml:mapreduce.reduce.shuffle.input.buffer.percent，默认0.7<br>阈值：mapred-site.xml:mapreduce.reduce.shuffle.merge.percent，默认0.66<br>map输出阈值1000：mapred-site.xml:mapreduce.reduce.merge.inmem.threshold</li><li>\7. 复制阶段完成后，Reducer 进入 Merge 阶段，循环地合并 map 结果文件，维持其顺序排序，合并因子默认为 10，经过不断地 Merge 后得到一个”最终文件”，可能存储在磁盘也可能存在内存中； </li><li>\8. “最终文件”输入到 reduce 进行计算，计算结果输入到 HDFS。</li><li>[ 注意 ]<ul><li>溢写前会先按照分区进行排序，再按key进行排序，采用 快速排序<br>排序是按照原始数据排序，但是由于原始数据不好移动且原始数据包含了原始数据的位置信息，所以移动的其实是元数据；写入时读的是元数据，真正写入的时原始数据</li><li>最后的数据如果不够80M，也会被强制flush到磁盘</li><li>每个mapTask任务生成的磁盘小数据最后都会merge成一个大文件，采用 归并排序</li><li>Shuffle 中的缓冲区大小会影响到 mapreduce 程序的执行效率，原则上说，缓冲区越大，磁 盘io的次数越少，执行速度就越快。</li></ul></li></ul></li></ul></li></ul><p><br></p><h2 id="10、自定义输入-InputFormat"><a href="#10、自定义输入-InputFormat" class="headerlink" title="10、自定义输入 InputFormat"></a>10、自定义输入 InputFormat</h2><ul><li>默认的文件加载：TextInputFormat</li><li>默认的文件读取：LineRecordReader</li><li>源码追踪过程：context  –&gt; mappercontext –&gt; mapcontext –&gt; reader –&gt; input  –&gt; real –&gt;  inputFormat.createRecordReader（split，taskContext），然后查找 inputFormat  –&gt; createRecordReader（split，taskContext），inputFormat –&gt;  TextInputFormat实例对象</li><li>案例：多个小文件合并   word1.txt ~word10.txt   每次读取一个小文件<ul><li>自定义输入，需要创建两个类，并通过Job对象指定自定义输入</li><li>\1. 创建XxxInputFormat类，继承FileInputFormat&lt;&gt;，重写 createRecordReader() 方法</li><li>\2. 创建XxxRecordReader类，继承RecordReader&lt;&gt;，重写以下方法：<ul><li>initialize()：初始化方法，类似于setup()，对属性、链接或流进行初始化</li><li>getCurrentKey()：返回key</li><li>getCurrentValue()：返回value</li><li>getProgress()：返回文件执行进度</li><li>nextKeyValue()：返回文件是否读取结束</li><li>close()：进行一些资源的释放</li></ul></li><li>\3. 在mapreduce类的main()方法中指定自定义输入：job.setInputFormatClass(XxxInputFormat.class);</li></ul></li></ul><h4 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h4><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLWZFoCz9TSlCIVNCAoWjSSiloaqiuN98pKi1AW40"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr4._02_inputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.JobContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordReader;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WholeFileInputFormat</span> <span class="keyword">extends</span> <span class="title">FileInputFormat</span>&lt;<span class="title">NullWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 设置每个小文件不可分片,保证一个小文件生成一个key-v键值对</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">isSplitable</span><span class="params">(JobContext context, Path filename)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> RecordReader&lt;NullWritable, Text&gt; <span class="title">createRecordReader</span><span class="params">(InputSplit split,</span></span></span><br><span class="line"><span class="function"><span class="params">TaskAttemptContext context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">WholeFileRecordReader reader = <span class="keyword">new</span> WholeFileRecordReader();</span><br><span class="line">reader.initialize(split, context);</span><br><span class="line"><span class="keyword">return</span> reader;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLWZFoCz9TSlCIVNCAoWjSSiloaqiuN98pKi1AW40"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr4._02_inputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordReader;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WholeFileRecordReader</span> <span class="keyword">extends</span> <span class="title">RecordReader</span>&lt;<span class="title">NullWritable</span>, <span class="title">Text</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> FileSplit fileSplit;</span><br><span class="line"><span class="keyword">private</span> Configuration conf;</span><br><span class="line"><span class="keyword">private</span> Text value = <span class="keyword">new</span> Text(); </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">boolean</span> processed = <span class="keyword">false</span>;  <span class="comment">// 标识文件是否读取完成</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 初始化方法</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(InputSplit split, TaskAttemptContext context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.fileSplit=(FileSplit)split;</span><br><span class="line"><span class="keyword">this</span>.conf = context.getConfiguration();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">nextKeyValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (!processed) &#123;</span><br><span class="line"><span class="keyword">byte</span>[] contents = <span class="keyword">new</span> <span class="keyword">byte</span>[(<span class="keyword">int</span>)fileSplit.getLength()];</span><br><span class="line">Path file = fileSplit.getPath();</span><br><span class="line">FileSystem fs = file.getFileSystem(conf);</span><br><span class="line">FSDataInputStream in = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">in = fs.open(file);</span><br><span class="line"><span class="comment">// 把输入流上的数据全部读取到contents字节数组中</span></span><br><span class="line">IOUtils.readFully(in, contents, <span class="number">0</span>, contents.length);</span><br><span class="line"><span class="comment">// 把读取到的数据设置到value里</span></span><br><span class="line">value.set(contents,<span class="number">0</span>,contents.length);</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">IOUtils.closeStream(in);</span><br><span class="line">&#125;</span><br><span class="line">processed = <span class="keyword">true</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> NullWritable <span class="title">getCurrentKey</span><span class="params">()</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="keyword">return</span> NullWritable.get();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Text <span class="title">getCurrentValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="keyword">return</span> value;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">float</span> <span class="title">getProgress</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="keyword">return</span> processed ? <span class="number">1.0f</span> : <span class="number">0.0f</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLWZEJSp9SSlCIIrspiyhIoqg2Sbtoapt3U9oICrB0Ie30000"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr4._02_inputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SmallFilesConvertToBigMR</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">int</span> exitCode = ToolRunner.run(<span class="keyword">new</span> SmallFilesConvertToBigMR(), args);</span><br><span class="line">System.exit(exitCode);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line"></span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line">Job job = Job.getInstance(conf, <span class="string">"combine small files to bigfile"</span>);</span><br><span class="line"></span><br><span class="line">job.setJarByClass(SmallFilesConvertToBigMR.class);</span><br><span class="line"></span><br><span class="line">job.setMapOutputKeyClass(Text.class);</span><br><span class="line"></span><br><span class="line">job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">job.setMapperClass(SmallFilesConvertToBigMRMapper.class);</span><br><span class="line"></span><br><span class="line">job.setReducerClass(SmallFilesConvertToBigMRReducer.class);</span><br><span class="line"></span><br><span class="line">job.setOutputKeyClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">job.setOutputValueClass(Text.class);</span><br><span class="line"><span class="comment">////////</span></span><br><span class="line">job.setInputFormatClass(WholeFileInputFormat.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// job.setOutputFormatClass(SequenceFileOutputFormat.class);</span></span><br><span class="line"></span><br><span class="line">Path input = <span class="keyword">new</span> Path(<span class="string">"/in/joindemo"</span>);</span><br><span class="line"></span><br><span class="line">Path output = <span class="keyword">new</span> Path(<span class="string">"/out/bigfile"</span>);</span><br><span class="line"></span><br><span class="line">FileInputFormat.setInputPaths(job, input);</span><br><span class="line"></span><br><span class="line">FileSystem fs = FileSystem.get(conf);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (fs.exists(output)) &#123;</span><br><span class="line"></span><br><span class="line">fs.delete(output, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">FileOutputFormat.setOutputPath(job, output);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> status = job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> status;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SmallFilesConvertToBigMRMapper</span></span></span><br><span class="line"><span class="class"><span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">NullWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> Text filenameKey;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">Mapper&lt;NullWritable, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">InputSplit split = context.getInputSplit();</span><br><span class="line">Path path = ((FileSplit) split).getPath();</span><br><span class="line">filenameKey = <span class="keyword">new</span> Text(path.toString());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(NullWritable key, Text value,</span></span></span><br><span class="line"><span class="function"><span class="params">Mapper&lt;NullWritable, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">context.write(filenameKey, value);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SmallFilesConvertToBigMRReducer</span></span></span><br><span class="line"><span class="class"><span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text filename, Iterable&lt;Text&gt; bytes,</span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">context.write(NullWritable.get(), bytes.iterator().next());</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><br></p><h2 id="11、自定义输出-OutputFormat"><a href="#11、自定义输出-OutputFormat" class="headerlink" title="11、自定义输出  OutputFormat"></a>11、自定义输出  OutputFormat</h2><ul><li>默认的文件加载：TextOutputFormat</li><li>默认的文件读取：LineRecordWriter</li><li>源码追踪过程 略</li><li>案例：将考试成绩合格的输出到一个文件夹，不及格的输出到另一个文件夹（注意，不同于分区，分区只是量结果输出到同一文件夹下不同文件）<ul><li>自定义输出，需要创建两个类，并通过Job对象指定自定义输入</li><li>\1. 创建XxxOutputFormat类，继承FileOutputFormat&lt;&gt;，重写getRecordWriter()方法</li><li>\2. 创建XxxRecordWriter类，继承RecordWriter&lt;&gt;，重写以下方法：<ul><li>write()：真正向外写出的方法，需要将结果输出到几个不同文件夹，就需要创建几个输出流<ul><li>而输出流通过FileSystem对象获取，FileSystem对象获取需要配置文件</li><li>一般可以通过构造方法直接传入FileSystem对象</li></ul></li><li>close()：释放资源</li></ul></li><li>\3. 在mapreduce类的main()方法中指定自定义输入：job.setOutputFormatClass(XxxOutputFormat.class);</li></ul></li></ul><h4 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h4><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLl0jpId9BCX9zIyjAIWjyGtYSaZDIm5A0m00"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.outputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MultipleOutputMR</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 指定HDFS相关参数</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//  创建/配置 Job</span></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Jar包类型:这里千万别写错了</span></span><br><span class="line">job.setJarByClass(MultipleOutputMR.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Map Reduce执行类</span></span><br><span class="line">job.setMapperClass(MultipleOutputMRMapper.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Map输出类</span></span><br><span class="line">job.setMapOutputKeyClass(Text.class);</span><br><span class="line">job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">////////////// 设置reduce执行个数为0</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">///////////// 设置MapOutputFormatClass</span></span><br><span class="line">job.setOutputFormatClass(MyOutputFormat.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置输入 输出路径</span></span><br><span class="line">String inP = <span class="string">"/in/newScoreIn"</span>;</span><br><span class="line">String outP = <span class="string">"/out/myoutformat/mulWriteSuc"</span>;</span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(inP));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(outP));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置如果存在路径就删除</span></span><br><span class="line">        Path mypath = <span class="keyword">new</span> Path(outP);</span><br><span class="line">        FileSystem hdfs = mypath.getFileSystem(conf);</span><br><span class="line">        <span class="keyword">if</span> (hdfs.exists(mypath)) &#123;</span><br><span class="line">            hdfs.delete(mypath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//  执行job</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="keyword">true</span>)?<span class="number">0</span>:-<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MultipleOutputMRMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value,</span></span></span><br><span class="line"><span class="function"><span class="params">Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">// 参考次数&gt;7次 算合格</span></span><br><span class="line">String[] splits = value.toString().split(<span class="string">","</span>);</span><br><span class="line"><span class="keyword">if</span> (splits.length &gt; <span class="number">9</span>) &#123;</span><br><span class="line">context.write(<span class="keyword">new</span> Text(<span class="string">"1::"</span>+value.toString()), NullWritable.get());</span><br><span class="line">&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">context.write(<span class="keyword">new</span> Text(<span class="string">"2::"</span>+value.toString()), NullWritable.get());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLl2jz2yjAIWjSSiloaqiuN98pKi1IW80"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.outputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyOutputFormat</span> <span class="keyword">extends</span> <span class="title">FileOutputFormat</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> RecordWriter&lt;Text, NullWritable&gt; <span class="title">getRecordWriter</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">TaskAttemptContext job)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">Configuration configuration = job.getConfiguration();</span><br><span class="line">FileSystem fs = FileSystem.get(configuration);</span><br><span class="line"></span><br><span class="line">Path p1 = <span class="keyword">new</span> Path(<span class="string">"/out/myoutformat/out1"</span>);</span><br><span class="line">Path p2 = <span class="keyword">new</span> Path(<span class="string">"/out/myoutformat/out2"</span>);</span><br><span class="line"></span><br><span class="line">FSDataOutputStream out1 = fs.create(p1);</span><br><span class="line">FSDataOutputStream out2 = fs.create(p2);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> MyRecordWriter(out1,out2);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLl2j34fDpYzA2I_AB4ajud98pKi1IW80"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.outputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyRecordWriter</span> <span class="keyword">extends</span> <span class="title">RecordWriter</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">FSDataOutputStream fsout = <span class="keyword">null</span>;</span><br><span class="line">FSDataOutputStream fsout1 = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">MyRecordWriter</span><span class="params">(FSDataOutputStream fsout, FSDataOutputStream fsout1)</span> </span>&#123;</span><br><span class="line"><span class="keyword">super</span>();</span><br><span class="line"><span class="keyword">this</span>.fsout = fsout;</span><br><span class="line"><span class="keyword">this</span>.fsout1 = fsout1;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Text key, NullWritable value)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"> String[] strs = key.toString().split(<span class="string">"::"</span>);</span><br><span class="line"> <span class="keyword">if</span> (strs[<span class="number">0</span>].equals(<span class="string">"1"</span>)) &#123;</span><br><span class="line">fsout.write((strs[<span class="number">1</span>]+<span class="string">"\n"</span>).getBytes());</span><br><span class="line">&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">fsout1.write((strs[<span class="number">1</span>]+<span class="string">"\n"</span>).getBytes());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(TaskAttemptContext context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">IOUtils.closeStream(fsout);</span><br><span class="line">IOUtils.closeStream(fsout1);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="12、Yarn"><a href="#12、Yarn" class="headerlink" title="12、Yarn"></a>12、Yarn</h2><h3 id="1、Yarn图示简介"><a href="#1、Yarn图示简介" class="headerlink" title="1、Yarn图示简介"></a>1、Yarn图示简介</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-081949.png" alt="image-20180611161948431"></p><h4 id="在Hadoop1-x时，-只有两个主要组件：hdfs（文件存储），-MapReduce（计算）"><a href="#在Hadoop1-x时，-只有两个主要组件：hdfs（文件存储），-MapReduce（计算）" class="headerlink" title="在Hadoop1.x时， 只有两个主要组件：hdfs（文件存储）， MapReduce（计算）"></a><strong>在Hadoop1.x时， 只有两个主要组件：hdfs（文件存储）， MapReduce（计算）</strong></h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-082801.png" alt="image-20180611162800671"></p><p><strong>所有的计算相关的全部放在MapReduce上</strong></p><ul><li><strong>JobTracker</strong>: 整个计算程序的老大<ul><li>资源调度：随机调度</li><li>监控程序运行的状态，启动运行程序</li><li>存在单点故障问题</li></ul></li><li><strong>TaskTracker</strong>：负责计算程序的执行<ul><li>强行的将计算资源分成2部分<ul><li>MapSlot</li><li>ReduceSlot</li></ul></li><li>每一部分资源只能跑对应的任务</li></ul></li><li><strong>缺陷：</strong><ul><li>单点故障</li><li>资源调度随机，会造成资源浪费</li><li>JobTracker的运行压力过大</li></ul></li></ul><p><br></p><h4 id="Hadoop2-x：-分理出Yarn，专门负责集群的资源管理和调度"><a href="#Hadoop2-x：-分理出Yarn，专门负责集群的资源管理和调度" class="headerlink" title="Hadoop2.x： 分理出Yarn，专门负责集群的资源管理和调度"></a><strong>Hadoop2.x： 分理出Yarn，专门负责集群的资源管理和调度</strong></h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-082010.png" alt="image-20180611162010610"></p><h5 id="Yarn的进程："><a href="#Yarn的进程：" class="headerlink" title="Yarn的进程："></a>Yarn的进程：</h5><p><strong>ResourceManager:</strong></p><ul><li><p>整个资源调度的老大</p><ul><li>接受hadoop客户端的请求</li><li>接受NodeManager 的状态报告， NM的资源状态和存活状态</li><li>资源调度，整个计算程序的资源调度，调度的运行资源和节点</li></ul></li></ul><ul><li><p><strong>内部组件</strong>： </p><ul><li><strong>ASM</strong>——ApplicationsManager<ul><li>所有应用程序的管理者，负责调度应用程序</li></ul></li><li><strong>Scheduler</strong>——调度器概念<ul><li>调度的是什么时候执行哪个计算程序</li><li><strong>调度器：</strong><ul><li><strong>FIFO</strong>: first in first out<ul><li>先提交的先执行，后提交的后执行</li><li>内部维护一个队列</li></ul></li><li><strong>FAIR</strong>: 公平调度器<ul><li>大家平分资源运行</li><li>假设刚开始只有一个任务，占资源100%，此时又来了一个任务，这是进行资源平分，每人50%</li><li>内部也是维护一个队列</li></ul></li><li><strong>CAPICITPY</strong>: 可以按需进行配置，使用资源<ul><li>内部<strong>可维护多个队列</strong>，多个队列之间可以进行资源分配</li><li>例如：分配两个队列<ul><li>队列1：60%</li><li>队列2：40%</li><li>每个队列中都是执行FIFO的</li></ul></li></ul></li></ul></li></ul></li></ul></li></ul><p><strong>NodeManager：</strong></p><ul><li>负责真正的提供资源，运行计算程序<ul><li>接受ResourceManager的命令</li><li>提供资源运行计算程序</li></ul></li></ul><p><strong><u>MRAppMaster</u></strong>: </p><ul><li>单个计算程序的老大, 类似于项目经理<ul><li>负责帮助当前计算程序向ResourceManager申请资源</li><li>负责启动 MapTask 和 ReduceTask 任务</li></ul></li></ul><p><u><strong>Container:</strong></u></p><ul><li>抽象资源容器，封装这一定的cpu，io 和网络资源（逻辑概念）</li><li>是运行MapTask，ReduceTask等的<strong>运行资源单位</strong></li><li>1个split —— 1个MapTask (ReduceTask) —— 1个Container —— 显示为YarnChild，底层运行的资源单位就是<code>Container</code></li></ul><h3 id="2、Yarn运行过程"><a href="#2、Yarn运行过程" class="headerlink" title="2、Yarn运行过程"></a>2、Yarn运行过程</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-12-013414.png" alt="image-20180612093413719"></p><ul><li>MRAppMaster会在所有的MapTask执行到0.8的时候，开启ReduceTask任务</li></ul><p><strong>YARN 作业执行流程:</strong></p><ol><li>用户向 YARN 中提交应用程序，其中包括 MRAppMaster 程序，启动 MRAppMaster 的命令， 用户程序等。</li><li>ResourceManager 为该程序分配第一个 Container，并与对应的 NodeManager 通讯，要求 它在这个 Container 中启动应用程序 MRAppMaster。</li><li>MRAppMaster 首先向 ResourceManager 注册，这样用户可以直接通过 ResourceManager 查看应用程序的运行状态，然后将为各个任务申请资源，并监控它的运行状态，直到运行结 束，重复 4 到 7 的步骤。</li><li>MRAppMaster 采用轮询的方式通过 RPC 协议向 ResourceManager 申请和领取资源。 </li><li>一旦 MRAppMaster 申请到资源后，便与对应的 NodeManager 通讯，要求它启动任务。</li><li>NodeManager 为任务设置好运行环境(包括环境变量、JAR 包、二进制程序等)后，将 任务启动命令写到一个脚本中，并通过运行该脚本启动任务。</li><li>各个任务通过某个 RPC 协议向 MRAppMaster 汇报自己的状态和进度，以让 MRAppMaster 随时掌握各个任务的运行状态，从而可以在任务败的时候重新启动任务。 8、应用程序运行完成后，MRAppMaster 向 ResourceManager 注销并关闭自己。</li></ol><h3 id="3、Job的提交过程-待整理"><a href="#3、Job的提交过程-待整理" class="headerlink" title="3、Job的提交过程(待整理)"></a>3、Job的提交过程(待整理)</h3><ul><li>客户端向rm发送 提交job请求</li><li>rm向客户端发送 共享资源路径 和 applicationId</li><li>客户端将程序运行需要的共享资源放进共享资源路径<br>包括：程序jar包，xml配置文件，split切片信息</li><li>客户端向rm发送资源放置成功的报告，并真正 提交应用程序</li><li>rm接收到客户端的请求，会返回一个空闲的资源节点(比如：node01)</li><li>到资源节点(node01)上启动container</li><li>启动MRAppMaster</li><li>创建作业簿 记录 maptask 和 reducetask 的 运行状态和进度 等信息</li><li>mrappmaster去共享资源路径下 ,获取 切片 和 配置文件 等信息</li><li>mrappmaster 向 rm 申请maptask 和 reducetask的资源</li><li>rm 在处理 mrappmaster 请求时，会 优先处理有关maptask的请求</li><li>rm 向 mrappmaster 返回空闲节点（数据本地优先原则），运行maptask 或 reducetask<br>优先返回有数据的节点。</li><li>对象节点需要到hdfs 共享路径下下载程序jar包等 共享资源 到本地</li><li>mrappmaster 到 对应的节点上，启动container 和 maptask</li><li>maptask需要向 mrappmaster 汇报自身的 运行状态和进度</li><li>mrappmaster 监控到所有的maptask 运行进度到 80%，启动reducetask（启动前，也会下载共享资源路径下的响应文件，程序jar包，配置文件等）</li><li>reducetask 时刻和 mrappmaster 通信，汇报自身的 运行状态和进度</li><li>整个运行过程中，maptask运行完成 ， 都会向mrappmaster 申请注销 自己</li><li>当所有的maptask 和 reducetask 运行完成 ， mrappmaster 就会向rm 申请注销，进行资源回收</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-多-Job-串联&quot;&gt;&lt;a href=&quot;#1-多-Job-串联&quot; class=&quot;headerlink&quot; title=&quot;1.多 Job 串联&quot;&gt;&lt;/a&gt;1.多 Job 串联&lt;/h2&gt;&lt;h3 id=&quot;1-概念&quot;&gt;&lt;a href=&quot;#1-概念&quot; class=&quot;head
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/"/>
    
      <category term="学习笔记" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce笔记-2 三大组件-Partitioner分区,sort排序,Combiner局部分区</title>
    <link href="https://airpoet.github.io/2018/06/07/Hadoop/Study/2-MapReduce/MapReduce%E7%AC%94%E8%AE%B0-2/"/>
    <id>https://airpoet.github.io/2018/06/07/Hadoop/Study/2-MapReduce/MapReduce笔记-2/</id>
    <published>2018-06-07T03:27:55.684Z</published>
    <updated>2018-06-11T11:46:14.096Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Combiner-组件"><a href="#1-Combiner-组件" class="headerlink" title="1.  Combiner 组件"></a>1.  Combiner 组件</h2><h3 id="1-产生缘由："><a href="#1-产生缘由：" class="headerlink" title="1. 产生缘由："></a>1. 产生缘由：</h3><p>Combiner 是 MapReduce 程序中 Mapper 和 Reducer 之外的一种组件，它的作用是在 maptask 之后给 maptask 的结果进行局部汇总，以减轻 reducetask 的计算负载，减少网络传输</p><p><strong>Combiner 组件的作用：</strong></p><ul><li>减少 reduce 端的数据量</li><li>减少 shuffle 过程的数据量</li><li>在 map 端做了一次合并，提高分布式计算程序的整体性能</li></ul><p><strong>Combiner 组件帮 reduce 分担压力， 因此其业务逻辑和 reduce 中的业务逻辑相似</strong></p><h3 id="2-自定义-Combiner-组件："><a href="#2-自定义-Combiner-组件：" class="headerlink" title="2.自定义 Combiner 组件："></a>2.自定义 Combiner 组件：</h3><p>默认情况下没有 Combiner 组件，Combiner 作用时间点 — map–combiner–reduce</p><ol><li><p>继承 Reduce 类</p><ul><li>public class  MyCombiner extends Reducer&lt;前两个： map 的输出， 后两个： reduce 的输入&gt;{}</li><li>我们在写 MapReduce 程序的时候， map 的输出就是 reduce 的输入</li><li>也就是说， 这个 MyCombiner() 的前两个泛型和后两个泛型的类型一致</li></ul></li><li><p>重写 reduce 方法</p><ul><li><p><strong>Combiner 本质上相当于 在 map 端进行了一次 reduce 操作， 通常情况下直接使用 reducer 的类作为 Combiner 的类，不再单独写 Combiner 代码逻辑</strong></p></li><li><p><strong>在 Job 中加上<code>job.setCombinerClass(WorldcountReduce.class)</code>， 就会调用 Combiner</strong></p></li></ul></li></ol><ol start="3"><li><p><strong>Combiner 使用原则</strong></p><ul><li>有或没有都<strong>不能影响业务逻辑</strong>，都<strong>不能影响最终结果</strong>。比如累加，最大值等，求平均值就不能用。</li></ul></li></ol><hr><h2 id="2、MapReduce-中的序列化"><a href="#2、MapReduce-中的序列化" class="headerlink" title="2、MapReduce 中的序列化"></a>2、MapReduce 中的序列化</h2><h3 id="2-1、概述"><a href="#2-1、概述" class="headerlink" title="2.1、概述"></a>2.1、概述</h3><p><strong>Java</strong> 的<strong>序列化</strong>是一个<strong>重量级序列化框架（Serializable）</strong>，一个对象被序列化后，会附带很多额 外的信息（各种校验信息，header，继承体系等），<strong>不便于在网络中高效传输</strong>；</p><p><strong>Hadoop 自己开发了一套序列化机制</strong>（参与序列化的对象的类都要<strong>实现 Writable 接口</strong>），精简，高效</p><h4 id="Java-基本类型-amp-Hadoop-类型对照表"><a href="#Java-基本类型-amp-Hadoop-类型对照表" class="headerlink" title="Java 基本类型 &amp; Hadoop 类型对照表"></a>Java 基本类型 &amp; Hadoop 类型对照表</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Java &amp; Hadoop类型参照</span></span><br><span class="line">hadoop数据类型      &lt;------------&gt;  java数据类型:  </span><br><span class="line">布尔型：  </span><br><span class="line">BooleanWritable     &lt;------------&gt;  <span class="keyword">boolean</span>  </span><br><span class="line">整型：  </span><br><span class="line">ByteWritable        &lt;------------&gt;  <span class="keyword">byte</span>  </span><br><span class="line">ShortWritable       &lt;------------&gt;  <span class="keyword">short</span>  </span><br><span class="line">IntWritable         &lt;------------&gt;  <span class="keyword">int</span>  </span><br><span class="line">LongWritable        &lt;------------&gt;  <span class="keyword">long</span>  </span><br><span class="line">浮点型：  </span><br><span class="line">FloatWritable       &lt;------------&gt;  <span class="keyword">float</span>  </span><br><span class="line">DoubleWritable      &lt;------------&gt;  <span class="keyword">double</span>  </span><br><span class="line">字符串（文本）：  </span><br><span class="line">Text                &lt;------------&gt;  String  </span><br><span class="line">数组：  </span><br><span class="line">ArrayWritable       &lt;------------&gt;  Array  </span><br><span class="line">map集合：  </span><br><span class="line">MapWritable         &lt;------------&gt;  map</span><br></pre></td></tr></table></figure><h3 id="2-2、自定义对象实现-MapReduce-框架的序列化"><a href="#2-2、自定义对象实现-MapReduce-框架的序列化" class="headerlink" title="2.2、自定义对象实现 MapReduce 框架的序列化"></a>2.2、自定义对象实现 MapReduce 框架的序列化</h3><p><strong>要实现<code>WritableComparable</code>接口</strong>，因为 MapReduce 框架中的 shuffle 过程一定会对 key 进行排序</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//序列化方法</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    out.writeUTF(phone);</span><br><span class="line">    out.writeLong(upfFlow);</span><br><span class="line">    out.writeLong(downFlow);</span><br><span class="line">    out.writeLong(sumFlow);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//反序列化方法</span></span><br><span class="line"><span class="comment">//注意： 字段的反序列化顺序与序列化时的顺序保持一致,並且类型也一致</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.phone = in.readUTF();</span><br><span class="line"><span class="keyword">this</span>.upfFlow = in.readLong();</span><br><span class="line"><span class="keyword">this</span>.downFlow = in.readLong();</span><br><span class="line"><span class="keyword">this</span>.sumFlow = in.readLong();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h2 id="3-MapReduce中的Sort-–TODO。。"><a href="#3-MapReduce中的Sort-–TODO。。" class="headerlink" title="3. MapReduce中的Sort –TODO。。"></a>3. MapReduce中的Sort –TODO。。</h2><p>MapTask –&gt; ReduceTask 之间， 框架<strong>默认</strong>添加了排序</p><p>排序的规则是<strong>按照Map 端输出的 key 的字典顺序进行排序</strong></p><h5 id="1、-如果没有重写-WritableComparable-时"><a href="#1、-如果没有重写-WritableComparable-时" class="headerlink" title="1、 如果没有重写 WritableComparable 时"></a>1、 如果没有重写 WritableComparable 时</h5><p> 按单词统计中词频出现的此处进行排序， 按照出现的次数， 从低到高</p><p>如果想要对词频进行排序， 那么词频应该放在 map 输出 key 的位置</p><p><strong>代码实现</strong>：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> Map </span><br><span class="line"><span class="comment">//词频为 key， 其它为 value</span></span><br><span class="line"></span><br><span class="line"> Reduce </span><br><span class="line"><span class="comment">// 将 map 输入的结果反转(k,v 换位置), 输出最终结果</span></span><br><span class="line"><span class="comment">// 最后输出还是按照左边词, 右边次数</span></span><br><span class="line"><span class="comment">// ps： 如果倒序排的时候, map 的时候发的时候 加上-,  reduce 发的时候, 再加上-, 转成 IntWritable</span></span><br></pre></td></tr></table></figure><h5 id="2、自定义排序要实现WritableComparable接口"><a href="#2、自定义排序要实现WritableComparable接口" class="headerlink" title="2、自定义排序要实现WritableComparable接口"></a>2、<strong>自定义排序要实现<code>WritableComparable</code>接口</strong></h5><ul><li><strong>自定义的类必须放在 key 的位置</strong></li><li><strong>实现<code>WritableComparable</code>接口</strong>， 重写 <code>compareTo()</code>方法</li><li>待扩展…</li></ul><blockquote><p> 作业： <strong>增强需求： 按照总流量排序， 总流量相同时， 按照手机号码排序</strong></p></blockquote><hr><h2 id="4、MapReduce-中的数据分发组件-Partitioner（分区）"><a href="#4、MapReduce-中的数据分发组件-Partitioner（分区）" class="headerlink" title="4、MapReduce 中的数据分发组件 Partitioner（分区）"></a>4、MapReduce 中的数据分发组件 <code>Partitioner（分区）</code></h2><p><strong>需求：</strong> 根据归属地<strong>输出</strong>流量统计数据<strong>结果到不同文件</strong>，以便于在查询统计结果时可以定位到 省级范围进行</p><p><strong>思路</strong>：MapReduce 中会将 map 输出的 kv 对，按照相同 key 分组，然后分发给不同的 reducetask</p><p><strong>执行时机</strong>: <strong>在Map输出 kv 对之后, 所携带的 k,v 参数，跟 Map 输出相同</strong></p><p><br></p><h4 id="MapReduce-默认的分发规则为："><a href="#MapReduce-默认的分发规则为：" class="headerlink" title="MapReduce 默认的分发规则为："></a><strong>MapReduce 默认的分发规则为</strong>：</h4><p><strong>根据 <code>key</code> 的 <code>hashcode%reducetask</code> 数来分发</strong>，所以：<strong>如果要按照我们自 己的需求进行分组，则需要改写数据分发（分区）组件 Partitioner</strong></p><p><br></p><h4 id="Partition重点总结："><a href="#Partition重点总结：" class="headerlink" title="Partition重点总结："></a>Partition重点总结：</h4><ul><li><p><strong>Partition 的 key value, 就是Mapper输出的key value</strong></p><p><code>public abstract int getPartition(KEY key, VALUE value, int numPartitions);</code></p><p><strong>输入是Map的结果对&lt;key, value&gt;和Reducer的数目，输出则是分配的Reducer（整数编号）</strong>。<strong>就是指定Mappr输出的键值对到哪一个reducer上去</strong>。系统缺省的Partitioner是HashPartitioner，它以key的Hash值对Reducer的数目取模，得到对应的Reducer。<strong>这样保证如果有相同的key值，肯定被分配到同一个reducre上。如果有N个reducer，编号就为0,1,2,3……(N-1)</strong>。</p></li><li><p>MapReduce 中会将 map 输出的 kv 对，按照相同 key 分组，然后分发给不同的 reducetask 默认的分发规则为:根据 key 的 hashcode%reducetask 数来分发，所以:如果要按照我们自 己的需求进行分组，则需要改写数据分发(分组)组件 Partitioner, 自定义一个 CustomPartitioner 继承抽象类:Partitioner</p></li><li><strong>因此， Partitioner 的执行时机， 是在Map输出 kv 对之后</strong></li></ul><h5 id="Partitioner-实现过程"><a href="#Partitioner-实现过程" class="headerlink" title="Partitioner 实现过程"></a><strong>Partitioner 实现过程</strong></h5><ol><li>先分析一下具体的业务逻辑，确定大概有多少个分区</li><li>首先书写一个类，它要<strong>继承 <code>org.apache.hadoop.mapreduce.Partitioner</code>这个抽象类</strong></li><li><strong>重写<code>public int getPartition</code>这个方法，根据具体逻辑，读数据库或者配置返回相同的数字</strong></li><li><strong>在<code>main</code>方法中设置<code>Partioner</code>的类，<code>job.setPartitionerClass(DataPartitioner.class)</code>;</strong></li><li><strong>设置<code>Reducer</code>的数量，<code>job.setNumReduceTasks(6)</code>;</strong></li></ol><h4 id="典型的-Partitioner-代码实现"><a href="#典型的-Partitioner-代码实现" class="headerlink" title="典型的 Partitioner 代码实现"></a>典型的 Partitioner 代码实现</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProvincePartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> HashMap&lt;String, Integer&gt; provincMap = <span class="keyword">new</span> HashMap&lt;String, Integer&gt;();</span><br><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line">provincMap.put(<span class="string">"138"</span>, <span class="number">0</span>);</span><br><span class="line">provincMap.put(<span class="string">"139"</span>, <span class="number">1</span>);</span><br><span class="line">provincMap.put(<span class="string">"136"</span>, <span class="number">2</span>);</span><br><span class="line">provincMap.put(<span class="string">"137"</span>, <span class="number">3</span>);</span><br><span class="line">provincMap.put(<span class="string">"135"</span>, <span class="number">4</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text key, FlowBean value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">Integer code = provincMap.get(key.toString().substring(<span class="number">0</span>, <span class="number">3</span>));</span><br><span class="line"><span class="keyword">if</span> (code != <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="keyword">return</span> code;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="number">5</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h2 id="5、全局计数器"><a href="#5、全局计数器" class="headerlink" title="5、全局计数器"></a>5、全局计数器</h2><h3 id="1-框架内置计数器："><a href="#1-框架内置计数器：" class="headerlink" title="1.  框架内置计数器："></a>1.  框架内置计数器：</h3><ul><li>Hadoop内置的计数器，主要用来记录作业的执行情况</li><li>内置计数器包括 <strong>MapReduce框架计数器</strong>（Map-Reduce Framework）<ul><li><strong>文件系统计数器（FielSystemCounters）</strong></li><li><strong>作业计数器（Job Counters）</strong></li><li><strong>文件输入格式计数器（File Output Format Counters）</strong></li><li><strong>文件输出格式计数器（File Input Format Counters)</strong></li></ul></li><li>计数器由相关的task进行维护，定期传递给tasktracker，再由tasktracker传给jobtracker；</li><li>最终的作业计数器实际上是有jobtracker维护，所以计数器可以被全局汇总，同时也不必在整个网络中传递</li><li>只有当一个作业执行成功后，最终的计数器的值才是完整可靠的；</li></ul><h3 id="2-自定义的计数器"><a href="#2-自定义的计数器" class="headerlink" title="2. 自定义的计数器"></a>2. 自定义的计数器</h3><h5 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h5><ul><li>用来统计运行过程中的进度和状态， 类似于 job 运行的一个报告、日志</li><li>要将数据处理过程中遇到的不合规数据行进行全局计数，类似这 种需求可以借助 MapReduce 框架中提供的全局计数器来实现</li><li><strong>计数器的值可以在mapper或reducer中增加</strong></li></ul><p><strong>使用方式</strong></p><ol><li><p>定义枚举类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">enum</span> Temperature&#123;  </span><br><span class="line">MISSING,  </span><br><span class="line">TOTAL  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>在map或者reduce中使用计数器 </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.自定义计数器</span></span><br><span class="line">Counter counter = context.getCounter(Temperature.TOTAL);  </span><br><span class="line"><span class="comment">// 2.为计数器赋初始值</span></span><br><span class="line">counter.setValue(<span class="keyword">long</span> value);</span><br><span class="line"><span class="comment">// 3.计数器工作</span></span><br><span class="line">counter.increment(<span class="keyword">long</span> incr);</span><br></pre></td></tr></table></figure></li><li><p>获取计数器</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Counters counters=job.getCounters(); </span><br><span class="line">Counter counter=counters.findCounter(LOG_PROCESSOR_COUNTER.BAD_RECORDS_LONG);<span class="comment">// 查找枚举计数器，假如Enum的变量为BAD_RECORDS_LONG </span></span><br><span class="line"><span class="keyword">long</span> value=counter.getValue();<span class="comment">//获取计数值</span></span><br></pre></td></tr></table></figure></li></ol><h4 id="计数器使用完整代码"><a href="#计数器使用完整代码" class="headerlink" title="计数器使用完整代码"></a>计数器使用完整代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** </span></span><br><span class="line"><span class="comment">* <span class="doctag">@Description</span> 假如一个文件，规范的格式是3个字段，“\t”作为分隔符，其中有2条异常数据，一条数据是只有2个字段，一条数据是有4个字段</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyCounter</span> </span>&#123;</span><br><span class="line">    <span class="comment">// \t键</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String TAB_SEPARATOR = <span class="string">"\t"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyCounterMap</span> <span class="keyword">extends</span></span></span><br><span class="line"><span class="class">            <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">        <span class="comment">// 定义枚举对象</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">enum</span> LOG_PROCESSOR_COUNTER &#123;</span><br><span class="line">            BAD_RECORDS_LONG, BAD_RECORDS_SHORT</span><br><span class="line">        &#125;;</span><br><span class="line">        </span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            String arr_value[] = value.toString().split(TAB_SEPARATOR);</span><br><span class="line">            <span class="keyword">if</span> (arr_value.length &gt; <span class="number">3</span>) &#123;</span><br><span class="line">                <span class="comment">/* 自定义计数器 */</span></span><br><span class="line">                context.getCounter(<span class="string">"ErrorCounter"</span>, <span class="string">"toolong"</span>).increment(<span class="number">1</span>);</span><br><span class="line">                <span class="comment">/* 枚举计数器 */</span></span><br><span class="line">                context.getCounter(LOG_PROCESSOR_COUNTER.BAD_RECORDS_LONG).increment(<span class="number">1</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (arr_value.length &lt; <span class="number">3</span>) &#123;</span><br><span class="line">                <span class="comment">// 自定义计数器</span></span><br><span class="line">                context.getCounter(<span class="string">"ErrorCounter"</span>, <span class="string">"tooshort"</span>).increment(<span class="number">1</span>);</span><br><span class="line">                <span class="comment">// 枚举计数器</span></span><br><span class="line">                context.getCounter(LOG_PROCESSOR_COUNTER.BAD_RECORDS_SHORT).increment(<span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@SuppressWarnings</span>(<span class="string">"deprecation"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException </span>&#123;</span><br><span class="line">        String[] args0 = &#123; </span><br><span class="line">                <span class="string">"hdfs://hadoop2:9000/buaa/counter/counter.txt"</span>,</span><br><span class="line">                <span class="string">"hdfs://hadoop2:9000/buaa/counter/out/"</span> </span><br><span class="line">            &#125;;</span><br><span class="line">        <span class="comment">// 读取配置文件</span></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 如果输出目录存在，则删除</span></span><br><span class="line">        Path mypath = <span class="keyword">new</span> Path(args0[<span class="number">1</span>]);</span><br><span class="line">        FileSystem hdfs = mypath.getFileSystem(conf);</span><br><span class="line">        <span class="keyword">if</span> (hdfs.isDirectory(mypath)) &#123;</span><br><span class="line">            hdfs.delete(mypath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 新建一个任务</span></span><br><span class="line">        Job job = <span class="keyword">new</span> Job(conf, <span class="string">"MyCounter"</span>);</span><br><span class="line">        <span class="comment">// 主类</span></span><br><span class="line">        job.setJarByClass(MyCounter.class);</span><br><span class="line">        <span class="comment">// Mapper</span></span><br><span class="line">        job.setMapperClass(MyCounterMap.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 输入目录</span></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(args0[<span class="number">0</span>]));</span><br><span class="line">        <span class="comment">// 输出目录</span></span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args0[<span class="number">1</span>]));</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 提交任务，并退出</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>注意点：在没有 ReduceTask 的时候，  <code>job.setNumReduceTasks(0);</code></strong></p><p><a href="https://blog.csdn.net/qq_35732963/article/details/53358033" target="_blank" rel="noopener">关于计数器，详情可参考</a></p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-Combiner-组件&quot;&gt;&lt;a href=&quot;#1-Combiner-组件&quot; class=&quot;headerlink&quot; title=&quot;1.  Combiner 组件&quot;&gt;&lt;/a&gt;1.  Combiner 组件&lt;/h2&gt;&lt;h3 id=&quot;1-产生缘由：&quot;&gt;&lt;a href=
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce笔记-1  WordCount, MapReduce运行机制</title>
    <link href="https://airpoet.github.io/2018/06/06/Hadoop/Study/2-MapReduce/MapReduce%E7%AC%94%E8%AE%B0-1/"/>
    <id>https://airpoet.github.io/2018/06/06/Hadoop/Study/2-MapReduce/MapReduce笔记-1/</id>
    <published>2018-06-06T09:41:22.111Z</published>
    <updated>2018-06-11T11:46:15.452Z</updated>
    
    <content type="html"><![CDATA[<p>参考链接:</p><p><a href="https://mubu.com/doc/254d__SRSn" target="_blank" rel="noopener">hdfs 笔记</a> </p><p><a href="https://mubu.com/doc/1BuHQkjk0G" target="_blank" rel="noopener">mapreduce 笔记</a></p><h1 id="1、MapReduce-入门"><a href="#1、MapReduce-入门" class="headerlink" title="1、MapReduce 入门"></a>1、MapReduce 入门</h1><h3 id="1-1、MapReduce概念"><a href="#1-1、MapReduce概念" class="headerlink" title="1.1、MapReduce概念"></a>1.1、MapReduce概念</h3><p><strong>hadoop 的四大组件：</strong></p><ul><li><strong>HDFS</strong>：分布式存储系统</li><li><strong>MapReduce</strong>：分布式计算系统</li><li><strong>YARN</strong>：hadoop 的资源调度系统</li><li><strong>Common</strong>：以上三大组件的底层支撑组件，主要提供基础工具包和 RPC 框架等</li></ul><p>MapReduce 是一个分布式运算程序的编程框架，是用户开发“<strong>基于 Hadoop 的数据分析应用</strong>” 的核心框架</p><p>MapReduce <strong>核心功能</strong> ：<strong>将用户编写的业务逻辑代码</strong>和<strong>自带默认组件</strong>整合成一个完整的<strong>分布 式运算程序</strong>，<strong>并发运行</strong>在一个 Hadoop <strong>集群</strong>上</p><h3 id="1-2、为什么需要-MapReduce？"><a href="#1-2、为什么需要-MapReduce？" class="headerlink" title="1.2、为什么需要 MapReduce？"></a>1.2、为什么需要 MapReduce？</h3><p>引入 MapReduce 框架后，<strong>开发人员</strong>可以将绝大部分工作<strong>集中在业务逻辑</strong>的开发上，而将 <strong>分布式计算中的复杂性交由框架来处理</strong></p><p>Hadoop 当中的 <strong>MapReduce</strong> <strong>分布式程序运算框架</strong>的<strong>整体结构</strong>如下：</p><blockquote><p><strong>MRAppMaster</strong>：MapReduce Application Master，分配任务，协调任务的运行</p><p><strong>MapTask</strong>：阶段并发任，负责 mapper 阶段的任务处理</p><p>YARNChild</p><p><strong>ReduceTask</strong>：阶段汇总任务，负责 reducer 阶段的任务处理</p><p>YARNChild</p></blockquote><h3 id="1-3、MapReduce-的编写规范"><a href="#1-3、MapReduce-的编写规范" class="headerlink" title="1.3、MapReduce 的编写规范"></a>1.3、MapReduce 的编写规范</h3><p>MapReduce 程序编写规范：</p><ol><li>用户编写的程序分成<strong>三个部分</strong>：<strong>Mapper，Reducer，Driver</strong>(提交运行 MR 程序的客户端)</li><li>Mapper 的输入数据是 KV 对的形式（KV 的类型可自定义）</li><li>Mapper 的输出数据是 KV 对的形式（KV 的类型可自定义）</li><li>Mapper 中的业务逻辑写在 map()方法中</li><li>map()方法（maptask 进程）对每一个&lt;K,V&gt;调用一次</li><li>Reducer 的输入数据类型对应 Mapper 的输出数据类型，也是 KV 对的形式</li><li>Reducer 的业务逻辑写在 reduce()方法中</li><li>Reducetask 进程对每一组相同 k 的&lt;K,V&gt;组调用一次 reduce()方法</li><li>用户自定义的 Mapper 和 Reducer 都要继承各自的父类</li><li>整个程序需要一个 Drvier 来进行提交，提交的是一个描述了各种必要信息的 job 对象</li></ol><h3 id="1-4、WordCount-程序"><a href="#1-4、WordCount-程序" class="headerlink" title="1.4、WordCount 程序"></a>1.4、WordCount 程序</h3><h4 id="1、业务逻辑"><a href="#1、业务逻辑" class="headerlink" title="1、业务逻辑"></a>1、业务逻辑</h4><ol><li><p>maptask阶段处理每个数据分块的单词统计分析，思路是每遇到一个单词则把其转换成一个 key-value对，比如单词  hello，就转换成&lt;’hello’,1&gt;发送给 reducetask去汇总</p></li><li><p>reducetask阶段将接受  maptask的结果，来做汇总计数</p></li></ol><h4 id="2、具体代码实现"><a href="#2、具体代码实现" class="headerlink" title="2、具体代码实现"></a>2、具体代码实现</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"> Map </span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">// 计算任务代码：切割单词，输出每个单词计 1 的 key-value 对</span></span><br><span class="line">String[] words = value.toString().split(<span class="string">" "</span>);</span><br><span class="line"><span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">context.write(<span class="keyword">new</span> Text(word), <span class="keyword">new</span> IntWritable(<span class="number">1</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> Reduce </span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">// 汇总计算代码：对每个 key 相同的一组 key-value 做汇总统计</span></span><br><span class="line"><span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (IntWritable v : values) &#123;</span><br><span class="line">sum += v.get();</span><br><span class="line">&#125;</span><br><span class="line">context.write(key, <span class="keyword">new</span> IntWritable(sum));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> main </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 指定 hdfs 相关的参数</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://hadoop02:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"hadoop"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 新建一个 job 任务</span></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置 jar 包所在路径</span></span><br><span class="line">job.setJarByClass(WordCountMR.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 mapper 类和 reducer 类</span></span><br><span class="line">job.setMapperClass(WordCountMapper.class);</span><br><span class="line">job.setReducerClass(WordCountReducer.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 maptask 的输出类型</span></span><br><span class="line">job.setMapOutputKeyClass(Text.class);</span><br><span class="line">job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 reducetask 的输出类型</span></span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定该 mapreduce 程序数据的输入和输出路径</span></span><br><span class="line">Path inputPath = <span class="keyword">new</span> Path(<span class="string">"/wordcount/input"</span>);</span><br><span class="line">Path outputPath = <span class="keyword">new</span> Path(<span class="string">"/wordcount/output"</span>);</span><br><span class="line">FileInputFormat.setInputPaths(job, inputPath);</span><br><span class="line">FileOutputFormat.setOutputPath(job, outputPath);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 最后提交任务</span></span><br><span class="line"><span class="keyword">boolean</span> waitForCompletion = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">System.exit(waitForCompletion?<span class="number">0</span>:<span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="2、MapReduce-程序的核心运行机制"><a href="#2、MapReduce-程序的核心运行机制" class="headerlink" title="2、MapReduce 程序的核心运行机制"></a>2、MapReduce 程序的核心运行机制</h1><h3 id="2-1、概述"><a href="#2-1、概述" class="headerlink" title="2.1、概述"></a>2.1、概述</h3><p>一个完整的 MapReduce 程序在分布式运行时有两类实例进程：</p><ol><li>MRAppMaster：负责整个程序的过程调度及状态协调</li><li>Yarnchild：负责 map 阶段的整个数据处理流程</li><li>Yarnchild：负责 reduce 阶段的整个数据处理流程</li></ol><p>以上两个阶段 MapTask 和 ReduceTask 的进程都是 YarnChild ， 并不是说这 MapTask 和 ReduceTask 就跑在同一个 YarnChild 进行里</p><h3 id="2-2、MapReduce-程序的运行流程"><a href="#2-2、MapReduce-程序的运行流程" class="headerlink" title="2.2、MapReduce 程序的运行流程"></a>2.2、MapReduce 程序的运行流程</h3><ol><li>一个 mr 程序启动的时候，最先启动的是 MRAppMaster，MRAppMaster 启动后根据本次 job 的描述信息，计算出需要的 maptask 实例数量，然后向集群申请机器启动相应数量的 maptask 进程</li><li>maptask 进程启动之后，根据给定的数据切片(哪个文件的哪个偏移量范围)范围进行数 据处理，主体流程为：<ul><li>利用客户指定的 InputFormat 来获取 RecordReader 读取数据，形成输入 KV 对</li><li>将输入 KV 对传递给客户定义的 map()方法，做逻辑运算，并将 map()方法输出的 KV 对收 集到缓存</li><li>将缓存中的 KV 对按照 K 分区排序后不断溢写到磁盘文件</li></ul></li><li>MRAppMaster 监控到所有 maptask 进程任务完成之后（真实情况是，某些 maptask 进 程处理完成后，就会开始启动 reducetask 去已完成的 maptask 处 fetch 数据），会根据客户指 定的参数启动相应数量的 reducetask 进程，并告知 reducetask 进程要处理的数据范围（数据 分区）</li><li>Reducetask 进程启动之后，根据 MRAppMaster 告知的待处理数据所在位置，从若干台 maptask 运行所在机器上获取到若干个 maptask 输出结果文件，并在本地进行重新<strong>归并排序</strong>， 然后按照相同 key 的 KV 为一个组，调用客户定义的 reduce()方法进行逻辑运算，并收集运 算输出的结果 KV，然后调用客户指定的 OutputFormat 将结果数据输出到外部存储</li></ol><h3 id="2-3、MapTask-并行度决定机制"><a href="#2-3、MapTask-并行度决定机制" class="headerlink" title="2.3、MapTask 并行度决定机制"></a>2.3、MapTask 并行度决定机制</h3><p><strong>将待处理数据执行逻辑切片（即按照一个特定切片大小，将待处理数据划分成逻辑上的多 个 split），然后每一个 split 分配一个 mapTask 并行实例处理</strong>。</p><p>这段逻辑及形成的切片规划描述文件，是由 FileInputFormat 实现类的 getSplits()方法完成的。 该方法返回的是 List<inputsplit>，InputSplit 封装了每一个逻辑切片的信息，包括长度和位置 信息，而 getSplits()方法返回一组 InputSplit。</inputsplit></p><h3 id="2-4、切片机制"><a href="#2-4、切片机制" class="headerlink" title="2.4、切片机制"></a>2.4、切片机制</h3><h4 id="FileInputFormat-中默认的切片机制"><a href="#FileInputFormat-中默认的切片机制" class="headerlink" title="FileInputFormat 中默认的切片机制"></a><strong>FileInputFormat 中默认的切片机制</strong></h4><ol><li>简单地按照文件的内容长度进行切片</li><li>切片大小，默认等于 block 大小</li><li>切片时不考虑数据集整体，而是逐个针对每一个文件单独切片</li></ol><blockquote><p>比如待处理数据有两个文件：</p><p>File1.txt    200M </p><p>File2.txt    100M </p><p>经过 getSplits()方法处理之后，形成的切片信息是：</p><p>File1.txt-split1    0-128M</p><p>File1.txt-split2    129M-200M</p><p>File2.txt-split1    0-100M</p></blockquote><h4 id="FileInputFormat-中切片的大小的参数配置"><a href="#FileInputFormat-中切片的大小的参数配置" class="headerlink" title="FileInputFormat 中切片的大小的参数配置"></a>FileInputFormat 中切片的大小的参数配置</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 通过分析源码，在 FileInputFormat 中，计算切片大小的逻辑：</span></span><br><span class="line"><span class="keyword">long</span> splitSize = computeSplitSize(blockSize, minSize, maxSize)，翻译一下就是求这三个值的中 间值</span><br><span class="line"></span><br><span class="line"><span class="comment">// 切片主要由这几个值来运算决定：</span></span><br><span class="line">blocksize：默认是 <span class="number">128</span>M，可通过 dfs.blocksize 修改</span><br><span class="line">minSize：默认是 <span class="number">1</span>，可通过 mapreduce.input.fileinputformat.split.minsize 修改</span><br><span class="line">maxsize：默认是 Long.MaxValue，可通过 mapreduce.input.fileinputformat.split.maxsize 修改</span><br><span class="line"></span><br><span class="line"><span class="comment">//因此，如果 maxsize 调的比 blocksize 小，则切片会小于 blocksize;  如果 minsize 调的比 blocksize 大，则切片会大于 blocksize</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 但是，不论怎么调参数，都不能让多个小文件“划入”一个 split</span></span><br></pre></td></tr></table></figure><h3 id="2-5、MapTask-并行度经验之谈"><a href="#2-5、MapTask-并行度经验之谈" class="headerlink" title="2.5、MapTask 并行度经验之谈"></a>2.5、MapTask 并行度经验之谈</h3><p>如果硬件配置为 2*12core + 64G，恰当的 map 并行度是大约每个节点 20-100 个 map，最好 每个 map 的执行时间至少一分钟。</p><ol><li>如果 job 的每个 map 或者 reduce task 的运行时间都只有 30-40 秒钟，那么就减少该 job 的 map 或者 reduce 数，每一个 task(map|reduce)的 setup 和加入到调度器中进行调度，这个 中间的过程可能都要花费几秒钟，所以如果每个 task 都非常快就跑完了，就会在 task 的开 始和结束的时候浪费太多的时间。</li></ol><p><strong>配置 task 的 JVM 重用</strong>可以改善该问题：</p><ul><li><strong>mapred.job.reuse.jvm.num.tasks</strong>，默认是 1，表示一个 JVM 上最多可以顺序执行的 task 数目（属于同一个 Job）是 1。也就是说一个 task 启一个 JVM。</li><li>这个值可以在 <strong>mapred-site.xml</strong> 中进行更改，当设置成多个，就意味着这多个 task 运行在同一个 JVM 上，但不是同时执行， 是排队顺序执行</li></ul><ol start="2"><li>如果 input 的<strong>文件非常的大</strong>，比如 1TB，<strong>可以考虑</strong>将 hdfs 上的每个 <strong>blocksize 设大</strong>，比如 设成 256MB 或者 512MB</li></ol><h3 id="2-6、ReduceTask-并行度决定机制"><a href="#2-6、ReduceTask-并行度决定机制" class="headerlink" title="2.6、ReduceTask 并行度决定机制"></a>2.6、ReduceTask 并行度决定机制</h3><p><strong>reducetask</strong> 的并行度同样影响整个 job 的执行并发度和执行效率，但与 maptask 的并发数由 切片数决定不同，Reducetask 数量的决定是可以直接手动设置：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置 ReduceTask 的并行度</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">4</span>);</span><br></pre></td></tr></table></figure><p>默认值是 1，</p><p>手动设置为 4，表示运行 4 个 reduceTask，</p><p>设置为 0，表示不运行 reduceTask 任务，也就是没有 reducer 阶段，只有 mapper 阶段</p><p><br></p><p>如果<strong>数据分布不均匀</strong>，就有<strong>可能在 reduce 阶段产生数据倾斜</strong></p><p>注意：reducetask 数量并不是任意设置，还要<strong>考虑业务逻辑需求</strong>，有些情况下，需要计算<strong>全局汇总结果</strong>，就<strong>只能有 1 个 reducetask</strong></p><p>尽量不要运行太多的 reducetask。对大多数 job 来说，最好 rduce 的个数最多和集群中的 reduce 持平，或者比集群的 reduce slots 小。这个对于小集群而言，尤其重要。</p><p><strong>最好的ReduceTask 个数是：datanode 个数 *  0.75~0.95 左右</strong></p><hr><h1 id="3-昨日复习"><a href="#3-昨日复习" class="headerlink" title="3. 昨日复习"></a>3. 昨日复习</h1><p>1.MapReduce 的 wc 编程</p><ul><li>手写代码<ul><li>Mapper</li><li>Reducer</li><li>Driver</li></ul></li></ul><p>2.MapTask 的并行度</p><ul><li>在程序执行的时候运行的 maptask 的总个数</li></ul><p>3.ReduceTask的并行度问题</p><ul><li>ReduceTask 的并行度设置依赖于自己传入的参数</li><li>一般经验： ReduceTask 的个数应该 = datanode 的阶段数 * （0.75~0.95）</li><li>ReduceTask 在设置的时候的并行度有一定的瓶颈</li><li>分区： 决定 ReduceTask 中的数据怎么分配的<ul><li>默认分区方式</li><li>自定义分区</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;参考链接:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://mubu.com/doc/254d__SRSn&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;hdfs 笔记&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://mubu.com/doc/1
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>▍Do Not Go Gentle into That Good Night</title>
    <link href="https://airpoet.github.io/2018/06/04/Poetry/%E4%B8%8D%E8%A6%81%E6%B8%A9%E5%92%8C%E7%9A%84%E8%B5%B0%E8%BF%9B%E9%82%A3%E4%B8%AA%E8%89%AF%E5%A4%9C/"/>
    <id>https://airpoet.github.io/2018/06/04/Poetry/不要温和的走进那个良夜/</id>
    <published>2018-06-03T16:38:57.809Z</published>
    <updated>2018-06-09T17:19:23.133Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-03-521528043568_.pic.jpg" alt=""></p><p> <br></p><p>Do not go gentle into that good night,</p><p>Old age should burn and rave at close of the day;</p><p>Rage, rage against the dying of the light.</p><p><br></p><p>Though wise men at their end know dark is right,</p><p>Because their words had forked no lightning they</p><p>Do not go gentle into that good night.</p><p> <br></p><p>Good men, the last wave by, crying how bright</p><p>Their frail deeds might have danced in a green bay,</p><p>Rage, rage against the dying of the light.</p><p><br></p><p>Wild men, who caught and sang the sun in flight,</p><p>And learn, too late, they grieved it on its way,</p><p>Do not go gentle into that good night.</p><p><br></p><p>Grave men, near death, who see with blinding sight</p><p>Blind eyes could blaze like meteors and be gay,</p><p>Rage, rage against the dying of the light.</p><p><br></p><p>And you, my father, there on the sad height,</p><p>Curse, bless, me now with your fierce tears, I pray.</p><p>Do not go gentle into that good night.</p><p>Rage, rage against the dying of the light.</p><p><br></p><hr><p>  <br></p><p>《不要温和地走进那个良夜》 -巫宁坤译本</p><p>​    <br></p><p>不要温和地走进那个良夜，</p><p>老年应当在日暮时燃烧咆哮；</p><p>怒斥，怒斥光明的消逝。</p><p>  <br></p><p>虽然智慧的人临终时懂得黑暗有理，</p><p>因为他们的话没有进发出闪电，他们</p><p>也并不温和地走进那个良夜。</p><p><br>  </p><p>善良的人，当最后一浪过去，高呼他们脆弱的善行</p><p>可能曾会多么光辉地在绿色的海湾里舞蹈，</p><p>怒斥，怒斥光明的消逝。</p><p>狂暴的人抓住并歌唱过翱翔的太阳，</p><p>懂得，但为时太晚，他们使太阳在途中悲伤，</p><p>也并不温和地走进那个良夜。</p><p>  <br></p><p>严肃的人，接近死亡，用炫目的视觉看出</p><p>失明的跟睛可以像流星一样闪耀欢欣，</p><p>怒斥，恕斥光明的消逝。</p><p>  <br></p><p>您啊，我的父亲，在那悲哀的高处。</p><p>现在用您的热泪诅咒我，祝福我吧。我求您</p><p>不要温和地走进那个良夜。</p><p>怒斥，怒斥光明的消逝</p><p><br></p><hr><p>《不要温顺地走入那长夜》 -和菜头译本</p><p><br></p><p>白日将尽，暮年仍应燃烧咆哮</p><p>狂怒吧，狂怒吧！</p><p>对抗着光明渐逝</p><p><br></p><p>虽然智者深知</p><p>人之将死，黑暗自有其时</p><p>只因他们所言未曾裂天如电</p><p>他们不要温顺地走入那长夜</p><p><br></p><p>随着最后一浪，善人在哭喊</p><p>哭喊那脆弱的善行</p><p>它本应何其欢快</p><p>在绿色峡湾里起舞</p><p>狂怒吧，狂怒吧！</p><p>对抗着光明渐逝。</p><p><br></p><p>狂人曾抓住飞驰的太阳</p><p>放声歌唱</p><p>太晚，他们才感到其中的伤感</p><p>不要温顺地走进那长夜</p><p><br></p><p>严肃的人行将死去时</p><p>用那渐渐失神的目光去看</p><p>盲瞳却如流星璀璨，欢欣溢满</p><p>狂怒吧，狂怒吧！</p><p>对抗着光明渐逝</p><p><br></p><p>还有你啊，我的父亲，远在悲伤的高地</p><p>我恳请你现在</p><p>就让你诅咒，你的祝福</p><p>随着热泪落下</p><p>不要温顺地走进那长夜</p><p>狂怒吧，狂怒吧！</p><p>对抗这光明渐逝</p><p><br></p><hr><p>​       《 绝不向黑夜请安》  -高晓松译本</p><p>绝不向黑夜请安</p><p>老朽请于白日尽头涅槃</p><p>咆哮于光之消散</p><p><br></p><p>先哲虽败于幽暗</p><p>诗歌终不能将苍穹点燃</p><p>绝不向黑夜请安</p><p><br></p><p>贤者舞蹈于碧湾</p><p>为惊涛淹没的善行哭喊</p><p>咆哮于光之消散</p><p><br></p><p>狂者如夸父逐日</p><p>高歌中顿觉迟来的伤感</p><p>绝不向黑夜请安</p><p><br></p><p>逝者于临终迷幻</p><p>盲瞳怒放出流星的灿烂</p><p>咆哮于光之消散</p><p><br></p><p>那么您，我垂垂将死的父亲</p><p>请掬最后一捧热泪降临</p><p>请诅咒，请保佑</p><p>我祈愿，绝不向</p><p>黑夜请安，咆哮</p><p>于光之消散</p><p><br> </p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-03-521528043568_.pic.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt; 
      
    
    </summary>
    
      <category term="文艺" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/"/>
    
      <category term="诗歌" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/%E8%AF%97%E6%AD%8C/"/>
    
    
      <category term="诗歌" scheme="https://airpoet.github.io/tags/%E8%AF%97%E6%AD%8C/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop学习笔记-3 HDFS 原理剖析</title>
    <link href="https://airpoet.github.io/2018/06/03/Hadoop/Study/1-HDFS/HDFS-2-%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90/"/>
    <id>https://airpoet.github.io/2018/06/03/Hadoop/Study/1-HDFS/HDFS-2-原理剖析/</id>
    <published>2018-06-03T09:12:34.396Z</published>
    <updated>2018-06-11T11:45:23.066Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-HDFS体系结构"><a href="#1-HDFS体系结构" class="headerlink" title="1.  HDFS体系结构"></a>1.  HDFS体系结构</h2><p>主从。。。</p><p><br></p><h2 id="2-NameNode"><a href="#2-NameNode" class="headerlink" title="2.NameNode"></a>2.NameNode</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><ul><li><p>[x] 是整个文件系统的管理节点。它维护着整个文件系统的文件目录树，文件/目录的元信息和每个文件对应的数据块列表。接收用户的操作请求。 </p></li><li><p>[x] <strong>在<code>hdfs-site.xml</code>中的<code>dfs.namenode.name.dir</code>属性</strong> </p></li><li><p>[x] 文件包括： </p><ul><li><p>[x] 文件包括:</p><p><strong>①fsimage</strong>:元数据镜像文件。存储某一时段<code>NameNode</code>内存元数据信息。</p><p><strong>②edits</strong>:操作日志文件。</p><p><strong>③fstime</strong>:保存最近一次<code>checkpoint</code>的时间</p><p><strong>以上这些文件是保存在linux的文件系统中。</strong></p></li></ul></li></ul><h3 id="查看-fsimage-和-edits的内容"><a href="#查看-fsimage-和-edits的内容" class="headerlink" title="查看 fsimage 和 edits的内容"></a>查看 <code>fsimage</code> 和 <code>edits</code>的内容</h3><ol><li><p>查看 <code>NameNode中</code> <code>fsimage</code> 的内容 </p><ul><li><p>查看 <strong>fsimage镜像文件</strong>内容<code>Usage: bin/hdfs oiv [OPTIONS] -i INPUTFILE -o  OUTPUTFILE</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以知道数据存在那个哪个 fsimage 镜像中</span></span><br><span class="line">------------------------------------</span><br><span class="line"><span class="comment"># 使用离线的查看器 输出到网页查看</span></span><br><span class="line">oiv -i hadoopdata/namenode/current/fsimage_0000000000000000250 -o 0000000000000000250</span><br><span class="line"></span><br><span class="line"><span class="comment"># 出现这样的提示</span></span><br><span class="line">INFO offlineImageViewer.WebImageViewer: WebImageViewer started. Listening on /127.0.0.1:5978. Press Ctrl+C to stop the viewer.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 另起一个窗口查看</span></span><br><span class="line">hadoop fs -ls -R webhdfs://127.0.0.1:5978</span><br><span class="line">------------------------------------</span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以导出到 xml 文件</span></span><br><span class="line">bin/hdfs oiv -p XML -i  tmp/dfs/name/current/fsimage_0000000000000000055 -o fsimage.xml</span><br></pre></td></tr></table></figure></li></ul></li></ol><ol start="2"><li><p>查看<code>edits</code>文件， 也可以导出到 xml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看edtis内容</span></span><br><span class="line">bin/hdfs oev -i tmp/dfs/name/current/edits_0000000000000000057-0000000000000000186   -o edits.xml</span><br></pre></td></tr></table></figure></li></ol><p><br></p><h2 id="3-Datanode"><a href="#3-Datanode" class="headerlink" title="3.  Datanode"></a>3.  Datanode</h2><p><strong>提供真实文件数据的存储服务</strong></p><ul><li><p><strong>Datanode 节点的数据切块存储位置</strong></p><ul><li><p><code>~/hadoopdata/datanode/current/BP-1070660005-192.168.170.131-1527865615372/current/finalized/subdir0/subdir0</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[ap@cs2]~/hadoopdata/datanode/current/BP-1070660005-192.168.170.131-1527865615372/current/finalized/subdir0/subdir0% ll</span><br><span class="line">总用量 213340</span><br><span class="line">-rw-r--r-- 1 ap ap 134217728 6月   2 13:35 blk_1073741842</span><br><span class="line">-rw-r--r-- 1 ap ap   1048583 6月   2 13:35 blk_1073741842_1018.meta</span><br><span class="line">-rw-r--r-- 1 ap ap  82527955 6月   2 13:35 blk_1073741843</span><br><span class="line">-rw-r--r-- 1 ap ap    644759 6月   2 13:35 blk_1073741843_1019.meta</span><br><span class="line">-rw-r--r-- 1 ap ap        13 6月   3 02:12 blk_1073741850</span><br><span class="line">-rw-r--r-- 1 ap ap        11 6月   3 02:12 blk_1073741850_1028.meta</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p>文件块（block）：最基本的存储单位。对于文件内容而言，一个文件的长度大小是size，那么从文件的０偏移开始，按照固定的大小，顺序对文件进行划分并编号，划分好的每一个块称一个Block。HDFS默认Block大小是128MB，以一个256MB文件，共有256/128=2个Block.</p></li><li><p>不同于普通文件系统的是，<strong>HDFS中，如果一个文件小于一个数据块的大小，并不占用整个数据块存储空间, 按文件大小的实际容量存储</strong></p></li><li><p>Replication。多复本。默认是三个。</p><ul><li><code>hdfs-site.xml</code>的<code>dfs.replication</code>属性 </li><li>手动设置某个文件的副本数为3个<ul><li><code>bin/hdfs dfs -setrep 3 /a.txt</code></li></ul></li></ul></li></ul><h2 id="4-数据存储：-写文件解析"><a href="#4-数据存储：-写文件解析" class="headerlink" title="4.  数据存储： 写文件解析"></a>4.  数据存储： 写文件解析</h2><ul><li><p>[x] <strong>疑点</strong>： HDFS client上传数据到HDFS时，会<strong>首先在本地缓存数据</strong>，当<strong>数据达到一个block大小时，请求NameNode分配一个block。</strong>NameNode会把block所在的DataNode的地址告诉HDFS client。HDFS client会直接和DataNode通信，把数据写到DataNode节点一个block文件中。 </p><blockquote><p>问题： 如果一直写的数据都没有达到一个 block 大小， 那怎么存储？？</p></blockquote></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-03-125641.png" alt="image-20180603205640605"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-03-125412.png" alt="image-20180603205412264"></p><h3 id="写文件的过程："><a href="#写文件的过程：" class="headerlink" title="写文件的过程："></a>写文件的过程：</h3><ol><li>首先调用FileSystem对象的open方法，其实是一个DistributedFileSystem的实例</li><li>DistributedFileSystem通过rpc获得文件的第一批个block的locations，同一block按照重复数会返回多个locations，这些locations按照hadoop拓扑结构排序，距离客户端近的排在前面.</li><li>前两步会返回一个FSDataInputStream对象，该对象会被封装成DFSInputStream对象，DFSInputStream可以方便的管理datanode和namenode数据流。客户端调用read方法，DFSInputStream最会找出离客户端最近的datanode并连接。</li><li>数据从datanode源源不断的流向客户端。</li><li>如果第一块的数据读完了，就会关闭指向第一块的datanode连接，接着读取下一块。这些操作对客户端来说是透明的，客户端的角度看来只是读一个持续不断的流。</li><li>如果第一批block都读完了，DFSInputStream就会去namenode拿下一批blocks的location，然后继续读，如果所有的块都读完，这时就会关闭掉所有的流。</li></ol><blockquote><p> 如果在读数据的时候，DFSInputStream和datanode的通讯发生异常，就会尝试正在读的block的排第二近的datanode,并且会记录哪个datanode发生错误，剩余的blocks读的时候就会直接跳过该datanode。DFSInputStream也会检查block数据校验和，如果发现一个坏的block,就会先报告到namenode节点，然后DFSInputStream在其他的datanode上读该block的镜像</p></blockquote><blockquote><p>该设计的方向就是客户端直接连接datanode来检索数据并且namenode来负责为每一个block提供最优的datanode，namenode仅仅处理block location的请求，这些信息都加载在namenode的内存中，hdfs通过datanode集群可以承受大量客户端的并发访问。</p></blockquote><p><br></p><h2 id="5-数据存储：-读文件解析"><a href="#5-数据存储：-读文件解析" class="headerlink" title="5.  数据存储： 读文件解析"></a>5.  数据存储： 读文件解析</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-03-125556.png" alt="image-20180603205556077"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-03-125203.png" alt="image-20180603205203151"></p><h3 id="读文件的过程"><a href="#读文件的过程" class="headerlink" title="读文件的过程"></a>读文件的过程</h3><ol><li>首先调用FileSystem对象的open方法，其实是一个DistributedFileSystem的实例</li><li>DistributedFileSystem通过rpc获得文件的第一批个block的locations，同一block按照重复数会返回多个locations，这些locations按照hadoop拓扑结构排序，距离客户端近的排在前面.</li><li>前两步会返回一个FSDataInputStream对象，该对象会被封装成DFSInputStream对象，DFSInputStream可以方便的管理datanode和namenode数据流。客户端调用read方法，DFSInputStream最会找出离客户端最近的datanode并连接。</li><li>数据从datanode源源不断的流向客户端。</li><li>如果第一块的数据读完了，就会关闭指向第一块的datanode连接，接着读取下一块。这些操作对客户端来说是透明的，客户端的角度看来只是读一个持续不断的流。</li><li>如果第一批block都读完了，DFSInputStream就会去namenode拿下一批blocks的location，然后继续读，如果所有的块都读完，这时就会关闭掉所有的流。</li></ol><blockquote><p> 如果在读数据的时候，DFSInputStream和datanode的通讯发生异常，就会尝试正在读的block的排第二近的datanode,并且会记录哪个datanode发生错误，剩余的blocks读的时候就会直接跳过该datanode。DFSInputStream也会检查block数据校验和，如果发现一个坏的block,就会先报告到namenode节点，然后DFSInputStream在其他的datanode上读该block的镜像</p></blockquote><blockquote><p>该设计的方向就是客户端直接连接datanode来检索数据并且namenode来负责为每一个block提供最优的datanode，namenode仅仅处理block location的请求，这些信息都加载在namenode的内存中，hdfs通过datanode集群可以承受大量客户端的并发访问。</p></blockquote><p><br></p><h2 id="6-Hadoop-Archives-（HAR-files）"><a href="#6-Hadoop-Archives-（HAR-files）" class="headerlink" title="6.Hadoop Archives （HAR files）"></a>6.Hadoop Archives （HAR files）</h2><p><strong>Hadoop Archives (HAR files)</strong>是在0.18.0版本中引入的，它的出现就是为了<strong>缓解大量小文件消耗namenode内存</strong>的问题。HAR文件是<strong>通过在HDFS上构建一个层次化的文件系统</strong>来工作。一个HAR文件是<strong>通过hadoop的archive命令来创建</strong>，而这个命令<strong>实 际上也是运行了一个MapReduce任务来将小文件打包成HAR</strong>。对于client端来说，使用HAR文件没有任何影响。所有的原始文件都 （using har://URL）。但在HDFS端它内部的文件数减少了。</p><p>通过HAR来读取一个文件并不会比直接从HDFS中读取文件高效，而且实际上可能还会稍微低效一点，因为对每一个HAR文件的访问都需要完成两层 index文件的读取和文件本身数据的读取。并且尽管HAR文件可以被用来作为MapReduce job的input，但是并没有特殊的方法来使maps将HAR文件中打包的文件当作一个HDFS文件处理。</p><p><strong>打包出来的 har 文件在<code>xxx.har/part-0</code>  中， contentz-size 跟原来的文件总大小一样</strong></p><p><strong>创建文件</strong> <code>hadoop archive -archiveName xxx.har -p  /src  /dest</code><br><strong>查看内容</strong> <code>hadoop fs -lsr har:///dest/xxx.har</code> 可以原封不动的显示出来</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打包成 har</span></span><br><span class="line">hadoop archive -archiveName test.har -p /user/<span class="built_in">test</span> /</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看har 文件</span></span><br><span class="line">[ap@cs1]~% hadoop fs -count /test.har/part-0</span><br><span class="line">           0(目录数)            1(文件数)         72(文件大小)    /test.har/part-0 (文件名)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看打包前文件</span></span><br><span class="line">[ap@cs1]~% hadoop fs -count /user/<span class="built_in">test</span></span><br><span class="line">           1            2                 72 /user/<span class="built_in">test</span></span><br><span class="line">           </span><br><span class="line"><span class="comment"># 查看 har 文件， 把打包前的原本文件都显示出来了</span></span><br><span class="line">[ap@cs1]~% hadoop fs -ls -R har:///test.har</span><br><span class="line">-rw-r--r--   3 ap supergroup         50 2018-06-03 04:24 har:///test.har/a.txt</span><br><span class="line">-rw-r--r--   3 ap supergroup         22 2018-06-03 04:24 har:///test.har/b.txt</span><br></pre></td></tr></table></figure><p><strong>注意点：</strong></p><ul><li><strong>存储层面：</strong>为了解决小文件过多导致的 Namenode 压力过大问题， 把很多小文件打包成一个 har 文件。<ul><li><strong>使用层面：</strong> 但是实际处理的时候， 还是会还原出原本的小文件进行处理， 不会把 har 文件当成一个 HDFS 文件处理。 </li><li><strong>HDFS</strong> 上不支持 tar， <strong>只支持 har打包</strong></li></ul></li></ul><h2 id="7-HDFS-的-HA"><a href="#7-HDFS-的-HA" class="headerlink" title="7.HDFS 的 HA"></a>7.HDFS 的 HA</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-HDFS体系结构&quot;&gt;&lt;a href=&quot;#1-HDFS体系结构&quot; class=&quot;headerlink&quot; title=&quot;1.  HDFS体系结构&quot;&gt;&lt;/a&gt;1.  HDFS体系结构&lt;/h2&gt;&lt;p&gt;主从。。。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&quot;2-Na
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="HDFS" scheme="https://airpoet.github.io/categories/Hadoop/HDFS/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="HDFS" scheme="https://airpoet.github.io/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>▍回答</title>
    <link href="https://airpoet.github.io/2018/06/03/Poetry/%E5%9B%9E%E7%AD%94/"/>
    <id>https://airpoet.github.io/2018/06/03/Poetry/回答/</id>
    <published>2018-06-03T06:40:41.275Z</published>
    <updated>2018-06-09T17:12:20.524Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-03-511528043539_.pic.jpg" alt=""></p><p><br></p><p>卑鄙是卑鄙者的通行证，<br>高尚是高尚者的墓志铭，<br>看吧，在那镀金的天空中，<br>飘满了死者弯曲的倒影。</p><p><br></p><p>冰川纪过去了，<br>为什么到处都是冰凌？<br>好望角发现了，<br>为什么死海里千帆相竞？</p><p><br></p><p>我来到这个世界上，<br>只带着纸、绳索和身影，<br>为了在审判之前，<br>宣读那些被判决的声音。</p><p><br></p><p>告诉你吧，世界<br>我–不–相–信！<br>纵使你脚下有一千名挑战者，<br>那就把我算作第一千零一名。</p><p><br></p><p>我不相信天是蓝的，<br>我不相信雷的回声，<br>我不相信梦是假的，<br>我不相信死无报应。</p><p><br></p><p>如果海洋注定要决堤，<br>就让所有的苦水都注入我心中，<br>如果陆地注定要上升，<br>就让人类重新选择生存的峰顶。</p><p><br></p><p>新的转机和闪闪星斗，<br>正在缀满没有遮拦的天空。<br>那是五千年的象形文字，<br>那是未来人们凝视的眼睛。</p><p><br></p><p>作者 / 北岛</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-03-511528043539_.pic.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;
      
    
    </summary>
    
      <category term="文艺" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/"/>
    
      <category term="诗歌" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/%E8%AF%97%E6%AD%8C/"/>
    
    
      <category term="诗歌" scheme="https://airpoet.github.io/tags/%E8%AF%97%E6%AD%8C/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop学习笔记-2 HDFS基础入门</title>
    <link href="https://airpoet.github.io/2018/06/02/Hadoop/Study/1-HDFS/HDFS-1-%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8/"/>
    <id>https://airpoet.github.io/2018/06/02/Hadoop/Study/1-HDFS/HDFS-1-基础入门/</id>
    <published>2018-06-02T07:07:33.222Z</published>
    <updated>2018-06-11T11:45:18.181Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hadoop的核心组件-之-HDFS"><a href="#Hadoop的核心组件-之-HDFS" class="headerlink" title="Hadoop的核心组件 之 HDFS"></a>Hadoop的核心组件 之 HDFS</h1><h2 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h2><h3 id="1-HDFS是什么"><a href="#1-HDFS是什么" class="headerlink" title="1. HDFS是什么:"></a>1. HDFS是什么:</h3><ul><li>分布式文件系统</li></ul><h3 id="2-HDFS-设计思想"><a href="#2-HDFS-设计思想" class="headerlink" title="2. HDFS 设计思想"></a>2. HDFS 设计思想</h3><ul><li>分而治之,  切分存储, 当一个文件过大的时候, 一个节点存储不了, 采用切分存储</li><li><strong>分块存储</strong>: 每一个块叫做 block<ul><li>如果一个8T的数据, 这个怎么分合适???<ul><li>设置分块的时候要<strong>考虑</strong>一个事情 : <strong>负载均衡</strong></li><li>块的大小不能太大, 太大会造成负载不均衡</li><li><strong>hadoop2.x</strong> 中默认的切分的<strong>块的大小</strong>是: <strong>128M</strong>, 1.x中默认的是64M</li><li>如果一个文件<strong>不足128M, 也会单独存一个快</strong>, <strong>快的大小就是存储数据的实际大小</strong></li></ul></li><li>这个分块存储思想中, 如果一个块的存储节点宕机了, 这个时候, 数据的安全性得不到保证了</li></ul></li><li>HDFS中默认块的存储采用<strong>备份机制</strong><ul><li><strong>默认的备份个数是3个</strong>(总共存的, 存到datanode上的, namenode不存),  之前自己配的是2个, 所有备份相同地位是相同的.</li><li>相同的数据块的备份一定存储在不同的节点上</li><li>如果节点总共2个,  <code>dfs.replication=3</code> 副本个数是3个,  实际存储2个, 另一个进行<strong>记账</strong>,  当集群节点个数大于3个时, 会复制这个副本, 最终达到3个</li><li>假设集群中的节点4个, 副本3个, 有一个副本的机器宕机了, 这个时候发现副本的个数 小于 设定的个数,  就会进行<strong>复制, 达到3个副本</strong>,</li><li>如果 <em>这个时候</em>, 刚才宕机的节点又恢复了, 这个时候集群副本个数为4了,  集群会等待一段时间, 如果发现还是4个, 就会<strong>删除</strong>一个副本, <strong>达到3个</strong>(<u>设定值</u>)</li><li>备份越多越好吗?<ul><li>理论上副本数越多, 数据安全性越高</li><li>但是副本数越多, 会占用过多的存储资源, 会造成集群的维护变得越困难<ul><li>100 个节点, 50个副本,  在这50个副本中, 随时都有可能宕机, hdfs就需要维护副本</li></ul></li><li>一般情况下, 3个就可以了</li><li>hadoop是基于廉价的pc机设计的, 会造成机器随时可能宕机</li></ul></li></ul></li><li>HDFS的<strong>目录结构</strong><ul><li>hdfs的目录结构与linux 操作系统类似,  以 <code>/</code>为跟节点,  我们将这个目录🌲称为<code>抽象目录树</code></li><li>因为<strong>hdfs的目录结构</strong>代表的是所有数据节点的<strong>抽象出来的目录</strong>,  不代表任何一个节点<ul><li>hdfs:  /hadoop.zip   500M  被分成4个块存储</li><li>hdfs中存储的数据块 是有编号的, blk_1,  blk_2, blk_3,  blk_4</li><li>/spark.zip  300M  3个块, blk_5  blk_6  blk_7</li><li>底层存储的时候, 每一个block都有一个唯一的id</li><li>hdfs的数据底层存储的时候吗, 还是存在真正的物理节点上.</li></ul></li></ul></li></ul><h3 id="2-HDFS-的整体结构"><a href="#2-HDFS-的整体结构" class="headerlink" title="2. HDFS 的整体结构"></a>2. HDFS 的整体结构</h3><p><strong>主从结构:  一个主节点,  多个从节点</strong></p><h4 id="namenode"><a href="#namenode" class="headerlink" title="namenode:"></a>namenode:</h4><ul><li>用于存储元数据, 包括: <ul><li>抽象目录树</li><li>存储数据和block的对应关系</li><li>block存储的位置</li></ul></li><li>处理客户端的读写请求<ul><li>读: 下载</li><li>写: 上传</li></ul></li></ul><h4 id="datanode"><a href="#datanode" class="headerlink" title="datanode"></a>datanode</h4><ul><li>负责真正的数据存储, 存储数据的block</li><li>真正处理读写</li></ul><h4 id="secondarynamenode"><a href="#secondarynamenode" class="headerlink" title="secondarynamenode:"></a>secondarynamenode:</h4><ul><li>冷备份节点: 助理<ul><li>当namenode宕机的时候,  secondarynamenode不能主动切换为 namenode, 但是 secondarynamenode中存储的数据与namenode相同.</li></ul></li><li>主要作用: <ul><li>namenode宕机的时候, 帮助namenode恢复</li><li>帮助namenode做一些事情, 分担namenode的压力</li></ul></li></ul><h3 id="3-HDFS优缺点"><a href="#3-HDFS优缺点" class="headerlink" title="3. HDFS优缺点:"></a>3. HDFS优缺点:</h3><ul><li><p>优点: </p><ol><li><p>可构建在廉价机器上, 成本低,  通过多副本提高可靠性, 提供了容错和恢复机制</p></li><li><p>高容错性</p><ul><li>容错性: 数据访问上, 一个节点数据丢失, 不影响整体的数据访问</li><li>数据自动保存多个副本, 副本丢失后, 自动恢复, 最终恢复到用户配置的副本个数</li></ul></li><li><p>适合批处理, 适合离线数据处理</p><ul><li>移动计算而非数据, 数据位置暴露给计算框架</li></ul></li><li><p>适合大数据处理</p><ul><li>GB,  TB 甚至 PB 级数据, 百万规模以上的文件数量, 10k+ 节点规模</li></ul></li><li><p>流式文件访问, 不支持数据修改, hdfs用于数据存储</p><ul><li>一次性写入, 多次读取, 保证数据一致性</li></ul></li></ol></li></ul><ul><li><p><strong>缺点:</strong> </p><ol><li>不支持低延迟的数据访问, 不支持 <strong>实时/近实时</strong> 数据访问, 因为涉及到多轮<code>RPC</code>调用<ol><li>向 NameNode 寻址..</li><li>拿到地址后， 向 DataNode 请求数据..</li></ol></li><li><strong>不擅长存储大量的小文件</strong>–kb级别的<ul><li>寻址时间可能大于读取数据的时间, 不划算<ul><li>进行数据访问的时候先找元数据</li><li>元数据是和block对应的, 1个block块对应一条元数据</li><li>1000w个1kb的文件, 存了1000w个块 — 1000w元数据</li><li>在进行数据访问的时候可能花了 1s 的时间, 总体上不划算</li></ul></li><li>这样会造成元数据存储量过大, 增加namenode的压力<ul><li>在hdfs中一般情况下, 一条元数据大小 150byte 左右</li><li>1000w条元数据 — 1000w * 150,  1.5G左右</li></ul></li></ul></li><li><strong>不支持文件内容修改</strong>, 仅<strong>仅支持</strong>文件<strong>末尾追加</strong> <code>append</code>， 一个文件同时只能有一个写者，<strong>不支持并发操作</strong></li></ol></li></ul><h3 id="4-HDFS-的-常用命令"><a href="#4-HDFS-的-常用命令" class="headerlink" title="==4. HDFS 的 常用命令:=="></a>==4. HDFS 的 常用命令:==</h3><p><strong>HDFS归根结底就是一个文件系统,  类似于 linux,  需要用命令来操作</strong></p><h4 id="1-hapdoop-fs-命令"><a href="#1-hapdoop-fs-命令" class="headerlink" title="1. hapdoop fs 命令"></a>1. <code>hapdoop fs</code> 命令</h4><blockquote><p><code>hadoop fs</code> /  <code>hdfs dfs</code> 效果是一样的</p><p>在hadoop中查看, 只有绝对路径的访问方式</p></blockquote><ol><li><p>查看帮助</p><ul><li><code>hadoop fs -help</code> 查看所有 <code>hadoop fs</code>的帮助</li><li><code>hadoop fs -help ls</code> 查看 <code>fs</code>下的 <code>ls</code>的帮助</li></ul></li><li><p><strong>列出根目录</strong>: <code>hadoop fs -ls /</code></p><ul><li><code>hadoop fs -ls -R /</code> 递归展示</li><li><code>hadoop fs -ls -R -h /</code>友好展示， 展示文件大小单位</li><li><strong>如果不指定目录， 会默认找当前用户xx对应的<code>/user/xx</code>的目录</strong></li></ul></li><li><p><strong>递归创建 -mkdir -p</strong>:  </p><ul><li><code>hadoop fs -mkdir -p /aa/bb/cc/dd</code></li><li>不加 -p  为普通创建</li></ul></li><li><p><strong>创建空文件<code>-touchz</code></strong></p><ul><li>类似于 Linux 下的 touch</li></ul></li><li><p><strong>上传  put</strong>: <code>[-put [-f][-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]</code></p><ul><li>上传一个: <code>hadoop fs -put hadoop-2.7.6.tar.gz /ss</code></li><li>上传多个: <code>hadoop fs -put aa.txt bb.txt /ss</code></li></ul></li><li><p><strong>下载 get</strong></p><ul><li><code>hadoop fs -get hdfs路径 本地路径</code></li></ul></li><li><p><strong>合并下载 getmerge</strong></p><ul><li><code>hadoop fs -getmerge /ss/aa.txt /ss/bb.txt /home/ap/cc.txt</code></li><li>会将最后一个路径之前8的当做需要合并的文件, 最后一个路径中指定的文件就是合并生成的文件</li></ul></li><li><p><strong>查看文件内容 cat</strong></p><ul><li><code>-cat</code> 查看文件内容</li><li><code>-text</code>也是类似</li></ul></li><li><p><strong>删除文件  rm</strong></p><ul><li><sub></sub>rm -rf (错误的)<sub></sub></li><li>rm -r(递归)  -f(强制)</li><li>文件<code>hadoop fs -rm -f /ss/aa.txt</code></li><li>文件夹 <code>hadoop fs -rm -f -r /aa</code></li></ul></li><li><p><strong>mv 修改名字, 移动</strong></p><ul><li><p>移动的文件<strong>从 hdfs 到 hdfs</strong></p></li><li><p><code>hadoop fs -mv  ..  ..</code></p></li></ul></li><li><p><strong>cp 复制</strong></p><ul><li><code>hadoop fs -cp /hdfsfile /hdfsfile</code>: 从 hdfs 复制到 hdfs<ul><li>参数 <strong>-p</strong>  ： 复制后<strong>保持文件的原本属性</strong>, 时间戳， 权限等<ul><li><code>Passing -p preserves status [topax] (timestamps, ownership, permission, ACLs, XAttr).</code></li></ul></li><li>参数 <strong>-f</strong>   :   已有同名文件的话， 直接<strong>覆盖</strong></li></ul></li></ul></li><li><p><strong>在末尾追加</strong>: <code></code>-appendToFile 本地文件  hdfs文件` </p><ul><li>将本地文件bb.txt 追加到 htfd的 /aa/aa.txt 上<ul><li><code>hadoop fs -appendToFile aa.txt /ss/bb.txt</code></li></ul></li><li>从命令行追加 , 但是不知道怎么结束， 先存疑？？<ul><li><code></code>hadoop fs -appendToFile - /a.txt`</li></ul></li><li>这个追加是在原始块的末尾追加的. 会改变集群上的文件</li><li>如果超过128M才会进行切分,  但这个命令<em>一般不会使用</em></li></ul></li><li><p><strong>查看文件，文件夹数量  count</strong></p><ul><li><code>DIR_COUNT FILE_COUNT CONTENT_SIZE FILE_NAME</code></li><li><p><code>8            3         176.5 K     /tmp</code></p></li><li><p><code>hadoop fs -count -h /tmp</code>:  -h 是友好展示</p></li><li><code>hdfs dfs -count -h /tmp</code>:  与上面效果一样</li><li><code>hdfs dfs -count -q -h /tmp</code>:  查看文件配额， 具体看 help</li></ul></li><li><p><strong><code>du</code>：</strong> <strong>展示文件大小</strong>， 如果参数是文件夹， 则展示文件夹下文件的大小</p><ul><li><code>hadoop fs -du -h  /tmp</code></li><li><code>hadoop fs -du -s -h  /tmp</code>: s 应该是  sum 的意思， 展示所有文件大小的总和</li></ul></li><li><p><strong>展示文件最后1kb内容</strong><code>-tail</code></p><ul><li><p>Show the last 1KB of the file.</p></li><li><p><code></code> hadoop fs -tail /dd.txt`</p></li><li><code>-f  Shows appended data as the file grows.</code></li><li>应用场景： <strong>监控日志</strong></li></ul></li><li><p><strong>修改文件权限 chmod</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 直接使用十进制数字修改 </span></span><br><span class="line">[ap@cs2]~/<span class="built_in">test</span>% hadoop fs -ls /</span><br><span class="line">drwxr-xr-x   - ap supergroup          0 2018-06-01 08:55 /aa</span><br><span class="line"><span class="comment"># -R：  /aa 目录下所有的文件递归修改权限</span></span><br><span class="line">[ap@cs2]~/<span class="built_in">test</span>% hadoop fs -chmod -R 777 /aa</span><br><span class="line">[ap@cs2]~/<span class="built_in">test</span>% hadoop fs -ls /</span><br><span class="line">drwxrwxrwx   - ap supergroup          0 2018-06-01 08:55 /aa</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 针对用户组修改，注意，修改2个不同组权限， 用，隔开</span></span><br><span class="line">hadoop fs -chmod u+x,g+x /a.txt </span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 最常用的文件权限， 是  644(-rw-r--r--) 和 755(-rwxr-xr-x) </span></span><br><span class="line">文件创建默认就是644</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. u+x 与 +x 的区别</span></span><br><span class="line">前者指定加在哪组用户上，  后者是所有组都加</span><br></pre></td></tr></table></figure></li><li><p><strong>修改用户权限 chown</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -chown -R 用户名:组名  hdfs目录/文件</span><br></pre></td></tr></table></figure></li></ol><hr><h4 id="2-hdfs-dfsadmin命令"><a href="#2-hdfs-dfsadmin命令" class="headerlink" title="2. hdfs dfsadmin命令"></a>2. <strong><code>hdfs dfsadmin</code>命令</strong></h4><p> 管理员对当前节点的一些操作</p><ul><li><strong><code>hdfs dfsadmin -report</code>  报告当前的一些状态</strong><ul><li>-live  活跃的   </li><li>-dead  死的</li><li>-decommissioning  退役的</li></ul></li><li>**<code>hdfs dfsadmin -safemode</code> 安全模式<ul><li>系统刚启动的时候， 会有30秒的安全模式开启状态， 过了30秒就关了</li><li>enter 进入</li><li>leave 离开</li><li>get  查看</li></ul></li><li><strong><code>hdfs dfsadmin</code> 设置配额</strong><ul><li><code>-setQuota</code> ： 配额是限定的文件&amp;文件夹的数量<ul><li>A quota of 1 would force the directory to remain empty.</li><li>空文件本身算一个文件</li><li><code>bin/hdfs dfsadmin -setQuota 10 lisi</code></li></ul></li><li><code>-clrQuota</code></li><li><code>-setSpaceQuota</code>： 空间配额限定的是大小<ul><li><code>bin/hdfs dfsadmin -setSpaceQuota 4k /lisi/</code></li></ul></li><li><code>-clrSpaceQuota</code></li><li><code>hdfs dfs -count -q -h /user</code>:  加上 -q 是查看配额</li></ul></li></ul><hr><h4 id="3-httpFS访问"><a href="#3-httpFS访问" class="headerlink" title="3.  httpFS访问"></a>3.  httpFS访问</h4><p>使用 REST 的形式， 可以在浏览器上直接访问集群， 可以在非 Linux 平台访问</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 编辑文件httpfs-env.sh</span><br><span class="line"># 打开此句注释, 使用内嵌的 tomcat</span><br><span class="line">export HTTPFS_HTTP_PORT=14000</span><br><span class="line"></span><br><span class="line"># 编辑文件core-site.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">编辑文件hdfs-site.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.webhdfs.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">重新启动namenode，执行 sbin/httpfs.sh start</span><br><span class="line"># 执行命令</span><br><span class="line">curl -i "http://cs1:14000/webhdfs/v1?user.name=root&amp;op=LISTSTATUS"</span><br></pre></td></tr></table></figure><p><a href="http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/WebHDFS.html" target="_blank" rel="noopener">更多命令参考</a></p><hr><p><strong>相关知识点</strong></p><ul><li><p>这些命令<strong>在集群中的任何节点都可以做</strong>, hdfs文件系统中, 看到的目录结构只是一个抽象目录, 实际存储在集群中的节点上</p><ul><li>aa.txt ,  大小150M,   <code>hadoop fs -put aa.txt /</code></li><li>会在根目录下看到 /aa.txt,  但是 aa.txt 真实存储的时候, 会先进行分块, 分2块, 进行存储, 假设集群中5个存储节点,  这2个块存储在哪个节点,  由namenode进行分配</li><li>图形界面点进去, 可以看到存储的块</li></ul></li><li><p><strong>Linux的权限管理命令</strong>: </p><ol><li>修改 文件/文件夹 权限的 <code>chmod</code>: <ol><li>可读: r ,  =4        </li><li>可写: w,  =2</li><li>可执行: x,  =1<ol><li>最大权限是7</li><li>-rw-rw-r–</li><li>文件属性 d:目录  -:文件  l:链接</li><li>第一组: 本用户,  第二组: 本组用户,  第三组: 其它用户</li></ol></li></ol></li></ol><ul><li>chmod  711 </li><li>改一个文件夹下所有文件权限为711 <code>chmod -R 711 目录</code></li></ul><ol><li>修改文件所属用户和组 <code>chown</code><ul><li><code>chown -R root:root  ss/</code>把ss的文件夹全部改成root用户和root组</li></ul></li></ol></li></ul><hr><h3 id="5、Eclipse查看Hadoop文件信息"><a href="#5、Eclipse查看Hadoop文件信息" class="headerlink" title="5、Eclipse查看Hadoop文件信息"></a>5、Eclipse查看Hadoop文件信息</h3><p><a href="https://app.yinxiang.com/shard/s37/nl/7399077/9f4966ae-da19-48aa-960b-0a6081ab7ff2/" target="_blank" rel="noopener">详情可以查看</a></p><p><a href="https://app.yinxiang.com/shard/s37/nl/7399077/2668e8e5-f7a1-48e3-a5d5-a7fcf9076c1f/" target="_blank" rel="noopener">其中可能遇到的bug，参见</a></p><p>其中， Eclipse端无法直接删除文件的问题，似乎可以通过在<code>hdfs-site.xml</code> 中修改访问权限来实现， <strong>还未尝试</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><hr><h3 id="6-通过-Java-API的方式操作-HDFS"><a href="#6-通过-Java-API的方式操作-HDFS" class="headerlink" title="6. 通过 Java API的方式操作 HDFS"></a>6. 通过 Java API的方式操作 HDFS</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hadoop的核心组件-之-HDFS&quot;&gt;&lt;a href=&quot;#Hadoop的核心组件-之-HDFS&quot; class=&quot;headerlink&quot; title=&quot;Hadoop的核心组件 之 HDFS&quot;&gt;&lt;/a&gt;Hadoop的核心组件 之 HDFS&lt;/h1&gt;&lt;h2 id=&quot;H
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="HDFS" scheme="https://airpoet.github.io/categories/Hadoop/HDFS/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="HDFS" scheme="https://airpoet.github.io/tags/HDFS/"/>
    
  </entry>
  
</feed>
