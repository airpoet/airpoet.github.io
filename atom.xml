<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>A.P的文艺杂谈</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://airpoet.github.io/"/>
  <updated>2018-06-28T17:00:27.088Z</updated>
  <id>https://airpoet.github.io/</id>
  
  <author>
    <name>airpoet</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Sqoop</title>
    <link href="https://airpoet.github.io/2018/06/28/Hadoop/6-Sqoop/Sqoop-1/"/>
    <id>https://airpoet.github.io/2018/06/28/Hadoop/6-Sqoop/Sqoop-1/</id>
    <published>2018-06-28T01:45:13.084Z</published>
    <updated>2018-06-28T17:00:27.088Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-作用"><a href="#1-作用" class="headerlink" title="1. 作用"></a>1. 作用</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-28-123439.png" alt="image-20180628203438952"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-28-123419.png" alt="image-20180628203419288"></p><p>Sqoop 是 Apache 旗下一款“Hadoop 和关系数据库服务器之间传送数据”的工具。</p><p><strong>导入数据</strong>：<strong>MySQL，Oracle 导入数据到 Hadoop 的 HDFS、HIVE、HBASE 等数据存储系统</strong></p><p><strong>导出数据</strong>：从 Hadoop 的文件系统中导出数据到关系数据库 MySQL 等</p><p>Sqoop 的本质还是一个命令行工具，和 HDFS，Hive 相比，并没有什么高深的理论。</p><h2 id="2-工作机制"><a href="#2-工作机制" class="headerlink" title="2.工作机制"></a>2.工作机制</h2><p>将导入导出命令翻译成 MapReduce 程序来实现</p><p>在翻译出的 MapReduce 中主要是对 InputFormat 和 OutputFormat 进行定制</p><h2 id="3-安装"><a href="#3-安装" class="headerlink" title="3.安装"></a>3.安装</h2><p>注意: 目录下要有hive, 因为要拿到 hive 的 home, 执行 hive 操作</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">1、 准备安装包 sqoop-1.4.6.bin_hadoop-2.0.4-alpha.tar.gz</span><br><span class="line"></span><br><span class="line">2、 解压安装包到安装目录</span><br><span class="line">tar -zxvf sqoop-1.4.6.bin_hadoop-2.0.4-alpha.tar.gz -C apps/ cd apps</span><br><span class="line">mv sqoop-1.4.6.bin_hadoop-2.0.4-alpha/ sqoop-1.4.6</span><br><span class="line"></span><br><span class="line">3、 进入到 conf 文件夹，找到 sqoop-env-template.sh，修改其名称为 sqoop-env.sh cd conf</span><br><span class="line">mv sqoop-env-template.sh sqoop-env.sh</span><br><span class="line"></span><br><span class="line">4、 修改 sqoop-env.sh</span><br><span class="line"><span class="comment">-----</span></span><br><span class="line">export HADOOP_COMMON_HOME=/home/ap/apps/hadoop export HADOOP_MAPRED_HOME=/home/ap/apps/hadoop export HBASE_HOME=/home/ap/apps/hbase</span><br><span class="line">export HIVE_HOME=/home/ap/apps/hive</span><br><span class="line">export ZOOCFGDIR=/home/ap/apps/zookeeper/conf</span><br><span class="line"><span class="comment">-----</span></span><br><span class="line"></span><br><span class="line">5、 加入 mysql 驱动包到 sqoop-1.4.6/lib 目录下</span><br><span class="line">cp mysql-connector-java-5.1.40-bin.jar ~/apps/sqoop-1.4.6/lib/</span><br><span class="line"></span><br><span class="line">6、 配置系统环境变量 vi ~/.bashrc</span><br><span class="line">然后输入:</span><br><span class="line">export SQOOP_HOME=/home/hadoop/apps/sqoop-1.4.6 export PATH=$PATH:$SQOOP_HOME/bin</span><br><span class="line">然后保存退出</span><br><span class="line">source ~/.bashrc</span><br><span class="line"></span><br><span class="line">7、 验证安装是否成功</span><br><span class="line">sqoop-version 或者 sqoopversion</span><br><span class="line"></span><br><span class="line">ps : 吹出现警告, 不用管</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">[ap@cs2]~% sqoop version</span><br><span class="line">Warning: /home/ap/apps/sqoop/../hcatalog does not exist! HCatalog jobs will fail.</span><br><span class="line">Please <span class="keyword">set</span> $HCAT_HOME <span class="keyword">to</span> the root <span class="keyword">of</span> your HCatalog installation.</span><br><span class="line"><span class="keyword">Warning</span>: /home/ap/apps/sqoop/../accumulo does <span class="keyword">not</span> exist! Accumulo imports will fail.</span><br><span class="line">Please <span class="keyword">set</span> $ACCUMULO_HOME <span class="keyword">to</span> the root <span class="keyword">of</span> your Accumulo installation.</span><br><span class="line"><span class="keyword">Warning</span>: /home/ap/apps/sqoop/../zookeeper does <span class="keyword">not</span> exist! Accumulo imports will fail.</span><br><span class="line">Please <span class="keyword">set</span> $ZOOKEEPER_HOME <span class="keyword">to</span> the root <span class="keyword">of</span> your Zookeeper installation.</span><br><span class="line"><span class="number">18</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">20</span>:<span class="number">43</span>:<span class="number">52</span> INFO sqoop.Sqoop: Running Sqoop <span class="keyword">version</span>: <span class="number">1.4</span><span class="number">.6</span></span><br><span class="line">Sqoop <span class="number">1.4</span><span class="number">.6</span></span><br><span class="line">git <span class="keyword">commit</span> <span class="keyword">id</span> c0c5a81723759fa575844a0a1eae8f510fa32c25</span><br><span class="line"><span class="keyword">Compiled</span> <span class="keyword">by</span> root <span class="keyword">on</span> Mon Apr <span class="number">27</span> <span class="number">14</span>:<span class="number">38</span>:<span class="number">36</span> CST <span class="number">2015</span></span><br></pre></td></tr></table></figure><h2 id="4-基本使用"><a href="#4-基本使用" class="headerlink" title="4. 基本使用"></a>4. 基本使用</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1) sqoop <span class="keyword">help</span> : 查看帮助</span><br><span class="line"><span class="number">2</span>) sqoop <span class="keyword">help</span> <span class="keyword">import</span> : 进一步层级查看</span><br></pre></td></tr></table></figure><h2 id="5-Sqoop-数据导入"><a href="#5-Sqoop-数据导入" class="headerlink" title="5. Sqoop 数据导入"></a>5. Sqoop 数据导入</h2><h5 id="常用指令"><a href="#常用指令" class="headerlink" title="常用指令"></a>常用指令</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--connect指定数据库链接url</span></span><br><span class="line"><span class="comment">--username指定数据库的用户名</span></span><br><span class="line"><span class="comment">--password指定数据库的密码</span></span><br><span class="line"><span class="comment">--table指定要导出数据的mysql数据库表</span></span><br><span class="line">-m指定MapTask的个数</span><br><span class="line"><span class="comment">--target-dir指定导出数据在HDFS上的存储目录</span></span><br><span class="line"><span class="comment">--fields-terminated-by指定每条记录中字段之间的分隔符</span></span><br><span class="line"><span class="comment">--where指定查询SQL的where条件</span></span><br><span class="line"><span class="comment">--query指定查询SQL</span></span><br><span class="line"><span class="comment">--columns指定查询列</span></span><br></pre></td></tr></table></figure><h4 id="5-1-list-mysql-的数据库-amp-表-复制mysql-中表结构相同表-–-gt-hive"><a href="#5-1-list-mysql-的数据库-amp-表-复制mysql-中表结构相同表-–-gt-hive" class="headerlink" title="5.1  list mysql 的数据库 &amp; 表, 复制mysql 中表结构相同表 –&gt; hive"></a>5.1  list mysql 的数据库 &amp; 表, 复制mysql 中表结构相同表 –&gt; hive</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">列出MySQL数据有哪些数据库：</span><br><span class="line"></span><br><span class="line">sqoop list-databases \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/ \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password 123</span></span><br><span class="line"></span><br><span class="line">列出MySQL中的某个数据库有哪些数据表：</span><br><span class="line"></span><br><span class="line">sqoop list-tables \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/mysql \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password 123</span></span><br><span class="line"></span><br><span class="line">创建一张跟mysql中的help_keyword表一样的hive表hk：(没有数据貌似)</span><br><span class="line"></span><br><span class="line">sqoop <span class="keyword">create</span>-hive-<span class="keyword">table</span> \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/mysql \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password 123 \</span></span><br><span class="line"><span class="comment">--table help_keyword \</span></span><br><span class="line"><span class="comment">--hive-table hk</span></span><br></pre></td></tr></table></figure><h4 id="5-2-导入-mysql中表-到-HDFS"><a href="#5-2-导入-mysql中表-到-HDFS" class="headerlink" title="5.2 导入 mysql中表 到 HDFS"></a>5.2 导入 mysql中表 到 HDFS</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">导入MySQL表中数据到HDFS中：</span><br><span class="line"></span><br><span class="line">// 普通导入：导入mysql库中的help_keyword的数据到HDFS上的默认路径：/user/ap/help_keyword</span><br><span class="line">sqoop import   \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/mysql   \</span></span><br><span class="line"><span class="comment">--username root  \</span></span><br><span class="line"><span class="comment">--password 123   \</span></span><br><span class="line"><span class="comment">--table help_keyword   \</span></span><br><span class="line">-m 1</span><br><span class="line"></span><br><span class="line">最后的1,是指使用1个 mapTask </span><br><span class="line">导入到hdfs后, 可以使用 hdfs dfs -text /.... 查看数据</span><br><span class="line"><span class="comment">----</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 导入： 指定分隔符 &amp; 导入路径 &amp; mapTask 个数</span><br><span class="line"></span><br><span class="line">sqoop import   \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/mysql   \</span></span><br><span class="line"><span class="comment">--username root  \</span></span><br><span class="line"><span class="comment">--password 123   \</span></span><br><span class="line"><span class="comment">--table help_keyword   \</span></span><br><span class="line"><span class="comment">--target-dir /user/ap/my_help_keyword1  \</span></span><br><span class="line"><span class="comment">--fields-terminated-by '\t'  \</span></span><br><span class="line">-m 3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 导入数据：带where条件</span><br><span class="line"></span><br><span class="line">sqoop import   \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/mysql   \</span></span><br><span class="line"><span class="comment">--username root  \</span></span><br><span class="line"><span class="comment">--password 123   \</span></span><br><span class="line"><span class="comment">--where "name='STRING' " \</span></span><br><span class="line"><span class="comment">--table help_keyword   \</span></span><br><span class="line"><span class="comment">--target-dir /user/ap/my_help_keyword2  \</span></span><br><span class="line">-m 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 查询指定列</span><br><span class="line"><span class="comment">-------------发现一个结论, 如果启动3个 mapper, 但是最后只有一个结论, 只会生成一个结果文件</span></span><br><span class="line"></span><br><span class="line">sqoop import   \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/mysql   \</span></span><br><span class="line"><span class="comment">--username root  \</span></span><br><span class="line"><span class="comment">--password 123   \</span></span><br><span class="line"><span class="comment">--columns "name" \</span></span><br><span class="line"><span class="comment">--where "name='STRING' " \</span></span><br><span class="line"><span class="comment">--table help_keyword  \</span></span><br><span class="line"><span class="comment">--target-dir /user/ap/my_help_keyword3  \</span></span><br><span class="line">-m 3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 导入：指定自定义查询SQL</span><br><span class="line"><span class="comment">--------------疑点:  split-by 的作用, 和 -m 数量的关系</span></span><br><span class="line"></span><br><span class="line">sqoop import   \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/mysql   \</span></span><br><span class="line"><span class="comment">--username root  \</span></span><br><span class="line"><span class="comment">--password 123   \</span></span><br><span class="line"><span class="comment">--target-dir /user/ap/my_help_keyword4  \</span></span><br><span class="line"><span class="comment">--query 'select help_keyword_id,name from help_keyword WHERE $CONDITIONS and  name = "STRING"' \</span></span><br><span class="line"><span class="comment">--split-by  help_keyword_id \</span></span><br><span class="line"><span class="comment">--fields-terminated-by '\t'  \</span></span><br><span class="line">-m 1</span><br><span class="line"></span><br><span class="line"><span class="comment">----------------------------</span></span><br><span class="line">sqoop import  \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/mysql \</span></span><br><span class="line"><span class="comment">--username root  \</span></span><br><span class="line"><span class="comment">--password 123  \</span></span><br><span class="line"><span class="comment">--target-dir /user/hadoop/my_help_keyword5 \</span></span><br><span class="line"><span class="comment">--query "select help_keyword_id,name from help_keyword WHERE \$CONDITIONS"  \</span></span><br><span class="line"><span class="comment">--split-by  help_keyword_id \</span></span><br><span class="line"><span class="comment">--fields-terminated-by '\t' \</span></span><br><span class="line">-m 1</span><br><span class="line"></span><br><span class="line"># 在以上需要按照自定义SQL语句导出数据到HDFS的情况下：</span><br><span class="line">1、引号问题，要么外层使用单引号，内层使用双引号，$CONDITIONS的$符号不用转义， 要么外层使用双引号，那么内层使用单引号，然后$CONDITIONS的$符号需要转义</span><br><span class="line">2、自定义的SQL语句中必须带有WHERE \$CONDITIONS</span><br></pre></td></tr></table></figure><h4 id="5-3-导入MySQL数据库中的表数据到Hive中："><a href="#5-3-导入MySQL数据库中的表数据到Hive中：" class="headerlink" title="5.3 导入MySQL数据库中的表数据到Hive中："></a>5.3 导入MySQL数据库中的表数据到Hive中：</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">导入MySQL数据库中的表数据到Hive中：</span><br><span class="line"></span><br><span class="line">// 普通导入：数据存储在默认的default hive库中，表名就是对应的mysql的表名：</span><br><span class="line">// 通过对比发现, sqoop 是默认导入到 hdfs 的, 导入到hdfs时, 不用加额外的参数</span><br><span class="line"></span><br><span class="line">sqoop import   \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/mysql  \</span></span><br><span class="line"><span class="comment">--username root  \</span></span><br><span class="line"><span class="comment">--password 123  \</span></span><br><span class="line"><span class="comment">--table help_keyword  \</span></span><br><span class="line"><span class="comment">--hive-import \</span></span><br><span class="line">-m 1</span><br><span class="line"></span><br><span class="line">hadoop fs -cat /user/myhive/warehouse/help_keyword/part-m-00000     // 查看数据</span><br><span class="line">当然也可以在 hive 中查看数据</span><br><span class="line"></span><br><span class="line">// 指定行分隔符和列分隔符，指定hive-import，指定覆盖导入，指定自动创建hive表，指定表名，指定删除中间结果数据目录 </span><br><span class="line"></span><br><span class="line">Database does not exist: mydb_test ???</span><br><span class="line">注意: 要先创建 database mydb_test</span><br><span class="line">问题: 为什么导出后, 变成了4个块</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sqoop import  \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/mysql  \</span></span><br><span class="line"><span class="comment">--username root  \</span></span><br><span class="line"><span class="comment">--password 123  \</span></span><br><span class="line"><span class="comment">--table help_keyword  \</span></span><br><span class="line"><span class="comment">--fields-terminated-by "\t"  \</span></span><br><span class="line"><span class="comment">--lines-terminated-by "\n"  \</span></span><br><span class="line"><span class="comment">--hive-import  \</span></span><br><span class="line"><span class="comment">--hive-overwrite  \</span></span><br><span class="line"><span class="comment">--create-hive-table  \</span></span><br><span class="line"><span class="comment">--delete-target-dir \</span></span><br><span class="line"><span class="comment">--hive-database mydb_test \</span></span><br><span class="line"><span class="comment">--hive-table new_help_keyword</span></span><br><span class="line"></span><br><span class="line">另外一种写法：</span><br><span class="line">sqoop import  \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/mysql  \</span></span><br><span class="line"><span class="comment">--username root  \</span></span><br><span class="line"><span class="comment">--password 123  \</span></span><br><span class="line"><span class="comment">--table help_keyword  \</span></span><br><span class="line"><span class="comment">--fields-terminated-by "\t"  \</span></span><br><span class="line"><span class="comment">--lines-terminated-by "\n"  \</span></span><br><span class="line"><span class="comment">--hive-import  \</span></span><br><span class="line"><span class="comment">--hive-overwrite  \</span></span><br><span class="line"><span class="comment">--create-hive-table  \</span></span><br><span class="line"><span class="comment">--hive-table mydb_test.new_help_keyword1 \</span></span><br><span class="line"><span class="comment">--delete-target-dir</span></span><br></pre></td></tr></table></figure><h4 id="5-4-增量导入到-HDFS"><a href="#5-4-增量导入到-HDFS" class="headerlink" title="5.4 增量导入到 HDFS"></a>5.4 增量导入到 HDFS</h4><p>—-应该也是可以导入到 Hive</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">第三部分：增量导入</span><br><span class="line"></span><br><span class="line">Incremental import arguments:</span><br><span class="line">   <span class="comment">--check-column &lt;column&gt;        Source column to check for incremental</span></span><br><span class="line">                                  <span class="keyword">change</span>  原始的列作为增长改变的</span><br><span class="line">   <span class="comment">--incremental &lt;import-type&gt;    Define an incremental import of type</span></span><br><span class="line">                                  <span class="string">'append'</span> <span class="keyword">or</span> <span class="string">'lastmodified'</span></span><br><span class="line">   <span class="comment">--last-value &lt;value&gt;           Last imported value in the incremental</span></span><br><span class="line">                                  <span class="keyword">check</span> <span class="keyword">column</span></span><br><span class="line"></span><br><span class="line">比较使用于自增长主键!! </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 增量导入</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sqoop <span class="keyword">import</span>   \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/mysql   \</span></span><br><span class="line"><span class="comment">--username root  \</span></span><br><span class="line"><span class="comment">--password 123   \</span></span><br><span class="line"><span class="comment">--table help_keyword  \</span></span><br><span class="line"><span class="comment">--target-dir /user/ap/my_help_keyword_import1  \</span></span><br><span class="line"><span class="comment">--incremental  append  \</span></span><br><span class="line"><span class="comment">--check-column  help_keyword_id \</span></span><br><span class="line"><span class="comment">--last-value 500  \</span></span><br><span class="line">-m <span class="number">3</span></span><br></pre></td></tr></table></figure><h4 id="5-5-第四部分：-导入数据到HBase"><a href="#5-5-第四部分：-导入数据到HBase" class="headerlink" title="5.5 第四部分： 导入数据到HBase"></a>5.5 第四部分： 导入数据到HBase</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">第四部分： 导入数据到HBase</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">导入MySQL数据库中的表数据到HBase中：</span><br><span class="line"></span><br><span class="line">sqoop import  \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/mysql  \</span></span><br><span class="line"><span class="comment">--username root  \</span></span><br><span class="line"><span class="comment">--password 123  \</span></span><br><span class="line"><span class="comment">--table help_keyword  \</span></span><br><span class="line"><span class="comment">--hbase-table new_help_keyword  \</span></span><br><span class="line"><span class="comment">--column-family person  \</span></span><br><span class="line"><span class="comment">--hbase-row-key help_keyword_id</span></span><br></pre></td></tr></table></figure><h4 id="5-6-导出"><a href="#5-6-导出" class="headerlink" title="5.6 导出"></a>5.6 导出</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">第五部分：导出：</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：导出的RDBMS的表必须自己预先创建，不会自动创建</span><br><span class="line"><span class="comment">------mysql-----先在 mysql 中创建库</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> sqoopdb <span class="keyword">default</span> <span class="built_in">character</span> <span class="keyword">set</span> utf8 <span class="keyword">COLLATE</span> utf8_general_ci; </span><br><span class="line"><span class="keyword">use</span> sqoopdb;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sqoopstudent ( </span><br><span class="line">   <span class="keyword">id</span> <span class="built_in">INT</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> PRIMARY <span class="keyword">KEY</span>, </span><br><span class="line">   <span class="keyword">name</span> <span class="built_in">VARCHAR</span>(<span class="number">20</span>), </span><br><span class="line">   sex <span class="built_in">VARCHAR</span>(<span class="number">20</span>),</span><br><span class="line">   age <span class="built_in">INT</span>,</span><br><span class="line">department <span class="built_in">VARCHAR</span>(<span class="number">20</span>)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 导出HDFS数据到MySQL：</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sqoop export \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/sqoopdb  \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password 123 \</span></span><br><span class="line"><span class="comment">--table sqoopstudent \</span></span><br><span class="line"><span class="comment">--export-dir /sqoopdata \</span></span><br><span class="line"><span class="comment">--fields-terminated-by ','</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 导出hive数据到MySQL：</span><br><span class="line">sqoop export \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/sqoopdb \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password 123 \</span></span><br><span class="line"><span class="comment">--table uv_info \</span></span><br><span class="line"><span class="comment">--export-dir /user/hive/warehouse/uv/dt=2011-08-03 \</span></span><br><span class="line"><span class="comment">--input-fields-terminated-by '\t'</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-作用&quot;&gt;&lt;a href=&quot;#1-作用&quot; class=&quot;headerlink&quot; title=&quot;1. 作用&quot;&gt;&lt;/a&gt;1. 作用&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-28-
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="Sqoop" scheme="https://airpoet.github.io/categories/Hadoop/Sqoop/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="Sqoop" scheme="https://airpoet.github.io/tags/Sqoop/"/>
    
  </entry>
  
  <entry>
    <title>i-HBase-1</title>
    <link href="https://airpoet.github.io/2018/06/26/Hadoop/5-HBase/i-HBase-1/"/>
    <id>https://airpoet.github.io/2018/06/26/Hadoop/5-HBase/i-HBase-1/</id>
    <published>2018-06-26T01:47:42.704Z</published>
    <updated>2018-06-26T01:51:25.411Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Hbase概念"><a href="#Hbase概念" class="headerlink" title="Hbase概念"></a>Hbase概念</h2><ul><li><p>hbase &amp; hdfs 关系图</p><p><img src="https://img.mubu.com/document_image/40f0d2a9-20f1-4f73-84d9-e31b52685aad-584150.jpg" alt="img"></p></li><li><p>hadoop数据库，分布式可伸缩大型数据存储。</p></li><li><p>用户对随机、实时读写数据。</p></li><li><p>十亿行 x 百万列。    </p></li><li><p>版本化、非关系型数据库。</p></li></ul><h2 id="Feature"><a href="#Feature" class="headerlink" title="Feature"></a>Feature</h2><ul><li>Linear and modular scalability.<br> //线性模块化扩展方式。</li><li>Strictly consistent reads and writes.<br> //严格一致性读写</li><li>Automatic and configurable sharding of tables<br>//自动可配置表切割</li><li>Automatic failover support between RegionServers.<br>//区域服务器之间自动容在</li><li>Convenient base classes for backing Hadoop MapReduce jobs with Apache HBase tables.        </li><li>Easy to use Java API for client access.<br>//java API</li><li>Block cache and Bloom Filters for real-time queries<br>//块缓存和布隆过滤器用于实时查询</li><li>Query predicate push down via server side Filters<br>//通过服务器端过滤器实现查询预测</li><li>Thrift gateway and a REST-ful Web service that supports XML, Protobuf, and binary data encoding options    </li><li>Extensible jruby-based (JIRB) shell                    </li><li>Support for exporting metrics via the Hadoop metrics subsystem to files or Ganglia; or via JMX<br> //可视化</li><li>面向列数据库。</li></ul><h2 id="HBase要点"><a href="#HBase要点" class="headerlink" title="HBase要点"></a>HBase要点</h2><ul><li>1) 它介于 NoSQL 和 RDBMS 之间，仅能通过主键(rowkey)和主键的 range 来检索数据</li><li>2) HBase 查询数据功能很简单，不支持 join 等复杂操作</li><li>3) 不支持复杂的事务，只支持行级事务(可通过 hive 支持来实现多表 join 等复杂操作)。</li><li>4) HBase 中支持的数据类型:byte<a href="底层所有数据的存储都是字节数组"></a></li><li>5) 主要用来存储结构化和半结构化的松散数据。</li></ul><h2 id="HBase-表特点"><a href="#HBase-表特点" class="headerlink" title="HBase 表特点"></a>HBase 表特点</h2><ul><li>大:一个表可以有上十亿行，上百万列</li><li>面向列:面向列(族)的存储和权限控制，列(簇)独立检索。</li><li>稀疏:对于为空(null)的列，并不占用存储空间，因此，表可以设计的非常稀疏。</li><li>无模式:每行都有一个可排序的主键和任意多的列，列可以根据需要动态的增加，同一 张表中不同的行可以有截然不同的列</li></ul><h2 id="HBase-存储机制"><a href="#HBase-存储机制" class="headerlink" title="HBase 存储机制"></a>HBase 存储机制</h2><ul><li>面向列存储，table是按row排序。</li><li>底层是 跳表 &amp; 布隆过滤器</li></ul><h2 id="HBase-定位机制-三级坐标"><a href="#HBase-定位机制-三级坐标" class="headerlink" title="HBase 定位机制(三级坐标)"></a>HBase 定位机制(三级坐标)</h2><ul><li>行<br>rowkey</li><li>列族 &amp; 列<br>column family &amp; column</li><li>时间戳<br>timestamp版本</li></ul><h2 id="表-行-列-列族的关系"><a href="#表-行-列-列族的关系" class="headerlink" title="表 行 列 列族的关系"></a>表 行 列 列族的关系</h2><ul><li><p>官网的关系图如下</p><p><img src="https://img.mubu.com/document_image/d4572c94-9da5-463f-a721-0c4dc42b87db-584150.jpg" alt="img"></p></li><li><p>其它版本</p><p><img src="https://img.mubu.com/document_image/feb7a6a8-6dfb-463a-8d57-181502d3d0e9-584150.jpg" alt="img"></p></li><li><p>Table is a collection of rows.<br>表是行的集合</p></li><li><p>Row is a collection of column families<br>行是列族的集合</p></li><li><p>Column family is a collection of columns.<br>列族是列的集合</p></li><li><p>Column is a collection of key value pairs.<br>列是键值对的集合(列其实就是 key)</p></li></ul><h2 id="RowKey-Column-Family-TimeStamp-Cell-概念解释"><a href="#RowKey-Column-Family-TimeStamp-Cell-概念解释" class="headerlink" title="RowKey, Column Family, TimeStamp, Cell 概念解释"></a>RowKey, Column Family, TimeStamp, Cell 概念解释</h2><ul><li>RowKey 是用来检索记录的主键<ul><li>rowkey 行键可以是任意字符串(最大长度是 64KB，实际应用中长度一般为 10-100bytes)，最好是 16。<ul><li>每一个物理文件中都会存一个 rk:TODO ??</li></ul></li><li>在 HBase 内部，rowkey 保存为字节数组。HBase 会对表中的数据按照 rowkey 排序 (字典顺序)</li><li>设计 key 时，要充分排序存储这 个特性，将经常一起读取的行存储放到一起。(位置相关性)</li><li>注意：字典序对 int 排序的结果是1,10,100,11,12,13,14,15,16,17,18,19,2,20,21,…9,91,92,93,94,95,96,97,98,99。要保持整形的自然序，行键必须用 0 作左填充。</li></ul></li><li>Column Family 列族是表的 Schema 的一部分(而列不是)，必须在使用表之前定义好，而且定义好了之后就不能更改。<ul><li>HBase 表中的每个列，都归属与某个列簇。</li><li>列簇越多，在取一行数据时所要参与 IO、搜寻的文件就越多，所以，如果没有必要，不要 设置太多的列簇（最好就一个列簇）</li></ul></li><li>TimeStamp<ul><li>HBase 中通过 rowkey 和 columns 确定的为一个存储单元称为 cell。</li><li>每个 cell 都保存着同一份 数据的多个版本。</li><li>版本通过时间戳来索引。</li><li>每个 cell 中，不同版本的数据按照时间倒序排序，即最新的数据排在最前面。</li><li>hbase 提供了两种数据版 本回收方式:<ul><li>保存数据的最后 n 个版本</li><li>保存最近一段时间内的版本(设置数据的生命周期 TTL)。</li></ul></li></ul></li><li>Cell 单元格<ul><li>由{rowkey, column( =<family> + <column>), version} 唯一确定的单元。</column></family></li><li>Cell 中的数据是没有类型的，全部是字节码形式存贮。</li></ul></li></ul><h2 id="HBase-的HA搭建"><a href="#HBase-的HA搭建" class="headerlink" title="HBase 的HA搭建"></a>HBase 的HA搭建</h2><ul><li><p>选择安装的主机cs1~cs4, 安装jdk, 解压hbase 包, 配置环境变量</p></li><li><p>验证安装是否成功<br>$&gt;hbase version</p></li><li><p>配置 hbase 完全分布式<br>[hbase/conf/<a href="http://hbase-env.sh" target="_blank" rel="noopener">hbase-env.sh</a>]<br>export JAVA_HOME=/usr/local/jdk1.8.0_73<br>export HBASE_MANAGES_ZK=false</p><p>[hbse-site.xml]</p><!-- 使用完全分布式 --><property><br>    <name>hbase.cluster.distributed</name><br>    <value>true</value><br></property><br><!-- 指定hbase数据在hdfs上的存放路径,存储路径由 namenode 来统一管理 --><br><property><br>    <name>hbase.rootdir</name><br>    <value><a href="hdfs://mycluster/hbase" target="_blank" rel="noopener">hdfs://mycluster/hbase</a></value><br></property><br><!-- 配置zk地址 --><br><property><br>    <name>hbase.zookeeper.quorum</name><br>    <value>cs1:2181,cs2:2181,cs3:2181</value><br></property><br><!-- zk的本地目录 --><br><property><br>    <name>hbase.zookeeper.property.dataDir</name><br>    <value>/home/ap/zookeeper</value><br></property> </li><li><p>配置 regonservers<br>[hbase/conf/regionservers]<br>cs2<br>cs3<br>cs4<br>cs5</p></li><li><p>修改 backup-masters（自行创建），指定备用的主HMaster</p><h2 id="vi-backup-masters"><a href="#vi-backup-masters" class="headerlink" title="vi backup-masters"></a>vi backup-masters</h2><p>cs6</p></li><li><p>最重要一步:</p><ul><li>傻逼版:<ul><li>要把 hadoop 的 hdfs-site.xml 和 core-site.xml 放到 hbase/conf 下, 这样hbase 才能通过命名空间mycluster 找到当前可用 namenode, 再通过 namenode 分配指定 hbase 在 HDFS上的存储路径<br>cp <sub>/apps/hadoop/etc/hadoop/core-site.xml </sub>/apps/hbase/conf/<br>cp <sub>/apps/hadoop/etc/hadoop/hdfs-site.xml </sub>/apps/hbase/conf/</li></ul></li><li>高级版:<ul><li>在hbase-env.sh文件添加hadoop配置文件目录到HBASE_CLASSPATH环境变量并分发.<br>[/soft/hbase/conf/<a href="http://hbase-env.sh" target="_blank" rel="noopener">hbase-env.sh</a>]<br>export HBASE_CLASSPATH=$HBASE_CLASSPATH:/soft/hadoop/etc/hadoop </li><li>在hbase/conf/目录下创建到hadoop的hdfs-site.xml符号连接。<br>$&gt;ln -s /soft/hadoop/etc/hadoop/hdfs-site.xml /soft/hbase/conf/hdfs-site.xml</li></ul></li></ul></li><li><p>启动 hbase 集群<br>$&gt;<a href="http://start-hbase.sh" target="_blank" rel="noopener">start-hbase.sh</a></p></li><li><p>登录 hbase 的 webUI</p></li></ul><p>  <a href="http://cs1:16010/" target="_blank" rel="noopener">http://cs1:16010/</a></p><ul><li>注意, 页面上显示的版本, 是~/apps/hbase/lib中自己存储的 jar 包的版本</li></ul><h2 id="启动-停止-hbase-进程"><a href="#启动-停止-hbase-进程" class="headerlink" title="启动/停止 hbase 进程"></a>启动/停止 hbase 进程</h2><ul><li><a href="http://start-hbase.sh" target="_blank" rel="noopener">start-hbase.sh</a></li></ul><p>  等价于</p><ul><li>hbase-daemon.sh start master</li><li>hbase-daemons.sh start regionserver</li><li>注意: 在哪个节点启动 <a href="http://start-hbase.sh" target="_blank" rel="noopener">start-hbase.sh</a> , 就在哪里启动 HMaster 进程, 如果</li></ul><ul><li><a href="http://stop-hbase.sh" target="_blank" rel="noopener">stop-hbase.sh</a><br>停止 hbase</li></ul><h2 id="HBase-shell-操作"><a href="#HBase-shell-操作" class="headerlink" title="HBase shell 操作"></a>HBase shell 操作</h2><h4 id="登录shell终端"><a href="#登录shell终端" class="headerlink" title="登录shell终端."></a>登录shell终端.</h4><ul><li>$hbase&gt; hbase shell                           </li></ul><h4 id="help-相关"><a href="#help-相关" class="headerlink" title="help 相关"></a>help 相关</h4><ul><li>$hbase&gt; help<br>查看所有帮助</li><li>$hbase&gt; table_help<br>关于表操作的另外一种方式的帮助文档</li><li>$hbase&gt; help “dml”<br>获取一组命令的提示</li><li>$hbase&gt;help  ‘list_namespace’<br>查看特定的命令帮助</li></ul><h4 id="desc-查看描述信息"><a href="#desc-查看描述信息" class="headerlink" title="desc 查看描述信息"></a>desc 查看描述信息</h4><ul><li>desc ‘t1’ : 下面这些在建表的时候都可以指定<br>{NAME =&gt; ‘f1’, BLOOMFILTER =&gt; ‘ROW’, VERSIONS =&gt; ‘1’, IN_MEMORY =&gt; ‘false’, KEEP_DELETED_CELLS =&gt;<br> ‘FALSE’, DATA_BLOCK_ENCODING =&gt; ‘NONE’, TTL =&gt; ‘FOREVER’, COMPRESSION =&gt; ‘NONE’, MIN_VERSIONS =&gt;<br> ‘0’, BLOCKCACHE =&gt; ‘true’, BLOCKSIZE =&gt; ‘65536’, REPLICATION_SCOPE =&gt; ‘0’}<br>1 row(s) in 0.1780 seconds</li></ul><h4 id="list-相关"><a href="#list-相关" class="headerlink" title="list 相关"></a>list 相关</h4><ul><li>$hbase&gt; list<br>列出所有非系统表</li><li>$hbase&gt;list_namespace<br>列出名字空间(数据库)</li><li>$hbase&gt;list_namespace_tables ‘defalut’<br>列出名字空间的所以表</li></ul><h4 id="create-相关"><a href="#create-相关" class="headerlink" title="create 相关"></a>create 相关</h4><ul><li>$hbase&gt;create_namespace ‘ns1’<br>创建名字空间</li><li>$hbase&gt;create ‘ns1:t1’,’f1’<br>创建表,指定空间下,列族<br>注意: 如果 t1已经创建了, 就不能通过这个方式来添加 列族</li><li>create ‘t1’,’f1’</li><li>create ‘user_info’,{NAME=&gt;’base_info’,VERSIONS=&gt;3 },{NAME=&gt;’extra_info’,VERSIONS=&gt;1 }<br>同时创建多个 列族</li></ul><h4 id="exist-相关"><a href="#exist-相关" class="headerlink" title="exist 相关"></a>exist 相关</h4><ul><li>exists ‘t1’<br>查看表是否存在</li></ul><h4 id="put-相关"><a href="#put-相关" class="headerlink" title="put 相关"></a>put 相关</h4><ul><li>注意: 如果往 同表 &amp; 同rowkey &amp; 同样的列 中插入 value, 会默认展示最近的一个时间戳的信息, 如果 get 的回收指定 VERSIONS数量, 则会展示此数量的版本信息</li><li>向 user 表中插入信息，row key 为 rk0001，列簇 info 中添加 name 列标示符，值为 zhangsan<ul><li>put ‘user’, ‘rk0001’, ‘info:name’, ‘zhangsan’, 1482077777777<br>手动指定时间戳<br>时间戳是可以自己指定的，如若不指定，则会自动获取系统的当前时间的时间戳</li></ul></li><li>向 user 表中插入信息，row key 为 rk0001，列簇 info 中添加 gender 列标示符，值为 female<ul><li>put ‘user’, ‘rk0001’, ‘info:gender’, ‘female’</li></ul></li><li>向 user 表中插入信息，row key 为 rk0001，列簇 info 中添加 age 列标示符，值为 20<ul><li>put ‘user’, ‘rk0001’, ‘info:age’, 20</li></ul></li><li>向 user 表中插入信息，row key 为 rk0001，列簇 data 中添加 pic 列标示符，值为 picture<ul><li>put ‘user’, ‘rk0001’, ‘data:pic’, ‘picture’</li></ul></li></ul><h4 id="get-相关"><a href="#get-相关" class="headerlink" title="get 相关"></a>get 相关</h4><ul><li>获取 user 表中 row key 为 rk0001 的所有信息              </li></ul><p>  注意: ‘表名’, ‘ rowkey’</p><ul><li>get ‘user’, ‘rk0001’</li></ul><ul><li>获取 user 表中 row key 为 rk0001，info 列簇的所有信息</li></ul><p>  注意: ‘表名’, ‘ rowkey’, ‘列族名’</p><ul><li>get ‘user’, ‘rk0001’, ‘info’</li></ul><ul><li>获取 user 表中 row key 为 rk0001，info 列簇的 name、age 列标示符的信息</li></ul><p>  注意: ‘表名’, ‘rowkey’, ‘列族名’:’列名’, ‘列族名’:’列名’</p><ul><li>get ‘user’, ‘rk0001’, ‘info:name’, ‘info:age’</li></ul><ul><li>获取 user 表中 row key 为 rk0001，info、data 列簇的信息</li></ul><p>  注意: ‘表名’, ‘rowkey’, ‘列族名’, ‘列族名’</p><ul><li>get ‘user’, ‘rk0001’, ‘info’, ‘data’</li><li>get ‘user’, ‘rk0001’, {COLUMN =&gt; [‘info’, ‘data’]}</li><li>get ‘user’, ‘rk0001’, {COLUMN =&gt; [‘info:name’, ‘data:pic’]}</li></ul><ul><li>获取 user 表中 row key 为 rk0001，列簇为 info，版本号最新 5 个的信息</li></ul><p>  注意: ‘表名’, ‘rowkey’, {COLUMN =&gt;’列族名’, 存储版本数=&gt; 数量}</p><ul><li>get ‘user’, ‘rk0001’, {COLUMN =&gt; ‘info’, VERSIONS =&gt; 2}</li><li>get ‘user’, ‘rk0001’, {COLUMN =&gt; ‘info:name’, VERSIONS =&gt; 5}</li><li>get ‘user’, ‘rk0001’, {COLUMN =&gt; ‘info:name’, VERSIONS =&gt; 5, TIMERANGE =&gt; [1392368783980, 1392380169184]}</li></ul><ul><li><p>获取 user 表中 row key 为 rk0001，列标示符中含有 a 的信息</p><ul><li>get ‘people’, ‘rk0001’, {FILTER =&gt; “(QualifierFilter(=,’substring:a’))”}</li></ul></li><li><p>获取 国籍 为中国的用户信息</p></li></ul><p>  注意:   ‘表名’,    ‘rowkey’,      ‘列族名’:’列名’,  ‘value’</p><ul><li>put ‘user’, ‘rk0002’, ‘info:name’, ‘fanbingbing’</li><li>put ‘user’, ‘rk0002’, ‘info:gender’, ‘female’</li><li>put ‘user’, ‘rk0002’, ‘info:nationality’, ‘中国’</li><li>get ‘user’, ‘rk0002’, {FILTER =&gt; “ValueFilter(=, ‘binary:中国’)”}</li></ul><ul><li>获取2个不同列中的指定字段</li></ul><p>  注意:   ‘表名’,    ‘rowkey’, {COLUMN  =&gt; [‘列族名’:’列名’,  ‘列族名’:’列名’]}</p><ul><li>get ‘user’, ‘rk0002’, {COLUMN =&gt; [‘info:name’, ‘data:pic’]}</li></ul><h4 id="scan-相关"><a href="#scan-相关" class="headerlink" title="scan 相关"></a>scan 相关</h4><ul><li><p>扫描元数据</p><ul><li><p>scan ‘hbase : meta’<br> t9,,1529936935368.f1a900954c069f0319195f16043c8e1a column=info:regioninfo, timestamp=1529936935933, value={ENCODED =&gt; f1a900954c069f0319195f16043c8e1a, NAME =&gt; ‘t9,,1529936935368.f1a900954c069f031919<br> .                                                  5f16043c8e1a.’, STARTKEY =&gt; ‘’, ENDKEY =&gt; ‘’}<br> t9,,1529936935368.f1a900954c069f0319195f16043c8e1a column=info:seqnumDuringOpen, timestamp=1529936935933, value=\x00\x00\x00\x00\x00\x00\x00\x02<br> .<br> t9,,1529936935368.f1a900954c069f0319195f16043c8e1a column=info:server, timestamp=1529936935933, value=cs5:16020<br> .<br> t9,,1529936935368.f1a900954c069f0319195f16043c8e1a column=info:serverstartcode, timestamp=1529936935933, value=1529923622913</p></li><li><p>$hbase&gt;split ‘ns1:t1’<br>//切割表</p></li><li><p>$hbase&gt;split ‘’<br> //切割区域, 见下面</p></li><li><p>切割之前</p><p><img src="https://img.mubu.com/document_image/13cfae11-e0c1-46fb-be2c-799578d701c9-584150.jpg" alt="img"></p></li><li><p>切割之后</p><p><img src="https://img.mubu.com/document_image/da7a0e5d-7157-458c-abfc-e17a2c81796f-584150.jpg" alt="img"></p></li><li><p>UI界面 table regions</p><p><img src="https://img.mubu.com/document_image/9a336ce6-5915-44bc-a3a6-f8ad86de7611-584150.jpg" alt="img"></p></li><li><p>继续进行 region 切割</p><ul><li>scan ‘hbase:meta’</li><li>找到切割点 STARTKEY =&gt; ‘row000550’ 的 那行的 NAME =&gt; 后面的值</li><li>从这个值开始切割到指定值<br>split ‘t9,row000551,1529938745618.90ed2fb6b7744ea5f0a7ff613d05aada.’,’row000888’</li></ul></li></ul></li><li><p>扫描表中所有的信息</p><ul><li>$hbase&gt;scan ‘ns1:t1’</li></ul></li><li><p>查询 user 表中列簇为 info 的信息</p><ul><li>hbase&gt; scan ‘hbase:meta’<br>扫描权标</li><li>$hbase&gt; scan ‘hbase:meta’, {COLUMNS =&gt; ‘info’}<br>注意: COLUMNS要大写</li><li>$hbase&gt; scan ‘ns1:t1’, {COLUMNS =&gt; [‘c1’, ‘c2’], LIMIT =&gt; 10, STARTROW =&gt; ‘xyz’}</li><li>scan ‘user’, {COLUMNS =&gt; ‘info’, RAW =&gt; true, VERSIONS =&gt; 5}</li><li>scan ‘persion’, {COLUMNS =&gt; ‘info’, RAW =&gt; true, VERSIONS =&gt; 3}<br>Scan 时可以设置是否开启 Raw 模式，开启 Raw 模式会返回包括已添加删除标记但是未 实际删除的数据。</li></ul></li><li><p>查询 user 表中列簇为 info 和 data 的信息</p><ul><li>scan ‘user’, {COLUMNS =&gt; [‘info’, ‘data’]}</li><li>scan ‘user’, {COLUMNS =&gt; [‘info:name’, ‘data:pic’]}</li></ul></li><li><p>查询 user 表中列簇为 info、列标示符为 name 的信息</p><ul><li>scan ‘user’, {COLUMNS =&gt; ‘info:name’}</li></ul></li><li><p>查询 user 表中列簇为 info、列标示符为 name 的信息,并且版本最新的 5 个</p><ul><li>scan ‘user’, {COLUMNS =&gt; ‘info:name’, VERSIONS =&gt; 5}</li></ul></li><li><p>查询 user 表中列簇为 info 和 data 且列标示符中含有 a 字符的信息</p><ul><li>scan ‘user’, {COLUMNS =&gt; [‘info’, ‘data’], FILTER =&gt; “(QualifierFilter(=,’substring:a’))”}</li></ul></li><li><p>查询 user 表中列簇为 info，rk 范围是[rk0001, rk0003)的数据</p><ul><li>scan ‘people’, {COLUMNS =&gt; ‘info’, STARTROW =&gt; ‘rk0001’, ENDROW =&gt; ‘rk0003’}</li></ul></li><li><p>查询 user 表中 row key 以 rk 字符开头的</p><ul><li>scan ‘user’,{FILTER=&gt;”PrefixFilter(‘rk’)”}</li></ul></li><li><p>查询 user 表中指定范围的数据</p><ul><li>scan ‘user’, {TIMERANGE =&gt; [1392368783980, 1392380169184]}</li></ul></li></ul><h4 id="delete-truncate-相关-删除-清空数据"><a href="#delete-truncate-相关-删除-清空数据" class="headerlink" title="delete/truncate 相关 (删除,清空数据)"></a>delete/truncate 相关 (删除,清空数据)</h4><ul><li>注意: 删除列族, 在 alter 中</li><li>删除 user 表 row key 为 rk0001，列标示符为 info:name 的字段数据<ul><li>delete ‘people’, ‘rk0001’, ‘info:name’</li></ul></li><li>删除 user 表 row key 为 rk0001，列标示符为 info:name，timestamp 为 1392383705316 的数据<ul><li>delete ‘user’, ‘rk0001’, ‘info:name’, 1392383705316</li></ul></li><li>清空 user 表中的数据<ul><li>truncate ‘people’<br>目前版本会先 disable , 再 truncate, 最后再 enable</li></ul></li></ul><h4 id="alter-相关"><a href="#alter-相关" class="headerlink" title="alter 相关"></a>alter 相关</h4><ul><li>注意,在修改列族 时</li></ul><p>  hbase&gt; alter ‘t1’, NAME =&gt; ‘f1’, VERSIONS =&gt; 5</p><ul><li>VERSIONS 是指能存多少个版本, 如果不指定的话, 退出 hbase 的时候, 默认只会留最新的一个版本</li><li>如果存在, 就是修改</li><li>不存在, 就是增加</li></ul><ul><li><p>添加两个列簇 f1 和 f2</p><ul><li>简写: alter ‘t1’, ‘f2’</li><li>alter ‘people’, NAME =&gt; ‘f1’</li><li>alter ‘user’, NAME =&gt; ‘f2’</li></ul></li><li><p>停用/启用 表</p><ul><li>enable ‘t9’<br>启用</li><li>is_enabled ‘t9’<br>是否可用</li><li>disable ‘t9’<br>停用</li><li>is_disabled ‘t9’<br>是否不可用</li></ul></li><li><p>删除一个列簇:</p><ul><li>注意: 当表中只有一个列族时, 无法将其删除</li><li>disable ‘user’(新版本不用)</li><li>alter ‘user’, NAME =&gt; ‘f1’, METHOD =&gt; ‘delete’</li><li>或</li><li>alter ‘user’, ‘delete’ =&gt; ‘f1’</li><li>enable ‘user’</li></ul></li><li><p>添加列簇 f1 同时删除列簇 f2</p><ul><li>disable ‘user’(新版本不用)</li><li>alter ‘user’, {NAME =&gt; ‘f1’}, {NAME =&gt; ‘f2’, METHOD =&gt; ‘delete’}</li><li>enable ‘user’</li></ul></li><li><p>将 user 表的 f1 列簇版本号改为 5</p><ul><li>disable ‘user’(新版本不用)</li><li>alter ‘people’, NAME =&gt; ‘info’, VERSIONS =&gt; 5<br>people 为 rowkey</li><li>enable ‘user’</li></ul></li></ul><h4 id="drop-相关"><a href="#drop-相关" class="headerlink" title="drop 相关"></a>drop 相关</h4><ul><li>disable ‘user’</li><li>drop ‘user’</li></ul><h4 id="count-相关"><a href="#count-相关" class="headerlink" title="count 相关"></a>count 相关</h4><ul><li>$hbase&gt;count ‘ns1:t1’<br>统计函数, 1000行统计一次</li></ul><h4 id="flush-相关"><a href="#flush-相关" class="headerlink" title="flush 相关"></a>flush 相关</h4><ul><li>把文件刷到磁盘的过程</li><li>hbase&gt; flush ‘TABLENAME’</li><li>hbase&gt; flush ‘REGIONNAME’</li><li>hbase&gt; flush ‘ENCODED_REGIONNAME’</li></ul><h4 id="ValueFilter-过滤器"><a href="#ValueFilter-过滤器" class="headerlink" title="ValueFilter 过滤器"></a>ValueFilter 过滤器</h4><ul><li>get ‘person’, ‘rk0001’, {FILTER =&gt; “ValueFilter(=, ‘binary:中国’)”}</li><li>get ‘person’, ‘rk0001’, {FILTER =&gt; “(QualifierFilter(=,’substring:a’))”}</li></ul><h4 id="exit-退出"><a href="#exit-退出" class="headerlink" title="exit 退出"></a>exit 退出</h4><p>$hbase&gt; exit</p><h4 id="遇到问题总结"><a href="#遇到问题总结" class="headerlink" title="遇到问题总结"></a>遇到问题总结</h4><ul><li>插入前, 必须创建好<ul><li>namespace<br>相当于数据库的概念</li><li>table<br>表命</li><li>column family<br>列族名</li></ul></li><li>不需要提前创建<ul><li>行名</li><li>列名</li><li>value</li></ul></li><li>如果表已经创建, 要增加列族的话, 只能用 alter, 不能继续用 create</li></ul><h2 id="通过Java-API访问Hbase"><a href="#通过Java-API访问Hbase" class="headerlink" title="通过Java API访问Hbase"></a>通过Java API访问Hbase</h2><ul><li><p>创建hbase模块</p></li><li><p>添加依赖<br>&lt;?xml version=”1.0” encoding=”UTF-8”?&gt;</p><project xmlns="<http://maven.apache.org/POM/4.0.0>" xmlns:xsi="<http://www.w3.org/2001/XMLSchema-instance>" xsi:schemalocation="<http://maven.apache.org/POM/4.0.0http://maven.apache.org/xsd/maven-4.0.0.xsd>"><br>    <modelversion>4.0.0</modelversion><br>    <groupid>com.rox</groupid><br>    <artifactid>HbaseDemo</artifactid><br>    <version>1.0-SNAPSHOT</version><br>    <dependencies><br>        <dependency><br>            <groupid>org.apache.hbase</groupid><br>            <artifactid>hbase-client</artifactid><br>            <version>1.2.3</version><br>        </dependency><br>    </dependencies><br></project> </li><li><p>复制hbase集群的hbase-site.xml文件到模块的src/main/resources目录下。</p></li><li><p>-——————————————————————————————</p></li><li><p>Hbase API 类和数据模型之间的对应关系:</p><ul><li><p>\0. 总的对应关系</p><p><img src="https://img.mubu.com/document_image/12d391d8-836e-48de-bf08-53a95e01343c-584150.jpg" alt="img"></p></li><li><ol><li>HBaseAdmin</li></ol><ul><li></li></ul></li></ul></li><li><p>编程实现 代码见 <a href="https://github.com/airpoet/bigdata/blob/master/HBaseDemo/src/test/java/com/rox/text/TestCRUD.java" target="_blank" rel="noopener">https://github.com/airpoet/bigdata/blob/master/HBaseDemo/src/test/java/com/rox/text/TestCRUD.java</a><br>public class TestCRUD {<br>@Test<br>public void put() throws IOException {<br>// 创建 conf 对象<br>Configuration conf = HBaseConfiguration.create();/<em> 通过连接工厂创建连接对 </em>/<br>// 通过连接工厂创建连接对象<br>Connection conn = ConnectionFactory.createConnection(conf);<br>// 通过连接查询 TableName 对象<br>TableName tname = TableName.valueOf(“ns1:t1”);<br>// 获得 table对象<br>Table table = conn.getTable(tname);<br>// 通过bytes 工具类转化字符串为字节数组<br>byte[] bytes = Bytes.toBytes(“row3”);<br>// 创建 put 对象, 传入行号<br>Put put = new Put(bytes);<br>// 创建 列族, 列, value 的 byte 数据<br>byte[] f1 = Bytes.toBytes(“f1”);<br>byte[] id = Bytes.toBytes(“id”);<br>byte[] value = Bytes.toBytes(188);<br>put.addColumn(f1, id, value);<br>// table put 数据<br>table.put(put);<br>//============================<br>// get<br>byte[] rowid = Bytes.toBytes(“row3”);<br>Get get = new Get(rowid);<br>// 得到 res<br>Result res = table.get(get);<br> // 从 res 中取出 value<br>byte[] idvalue = res.getValue(Bytes.toBytes(“f1”),Bytes.toBytes(“id”));<br>System.out.println(Bytes.toInt(idvalue));<br>}<br>}</p></li></ul><h2 id="HBase-写入过程-amp-存储路径"><a href="#HBase-写入过程-amp-存储路径" class="headerlink" title="HBase 写入过程 &amp; 存储路径"></a>HBase 写入过程 &amp; 存储路径</h2><ul><li>WAL<br>write ahead log,写前日志。</li><li>HDFS 上存储路径详解<ul><li><a href="http://cs1:50070/explorer.html#/hbase/data/ns1/t1/97eada11d196f1e134c41e859d338e07/f1/fsfsfgwgfsgfrgfdg" target="_blank" rel="noopener">http://cs1:50070/explorer.html#/hbase/data/ns1/t1/97eada11d196f1e134c41e859d338e07/f1/fsfsfgwgfsgfrgfdg</a><ul><li>data<br>存储数据的目录</li><li>ns1<br>namespace 名称</li><li>t1<br>表名</li><li>97eada11d196f1e134c41e859d338e07<br>region 的编号</li><li>f1<br>列族</li><li>fsfsfgwgfsgfrgfdg<br>HFile: 列族存储的文件, 每一个 hfile 文件对应的是一个列族</li></ul></li><li>meta 数据路径 <a href="http://cs1:50070/explorer.html#/hbase/data/hbase/meta/1588230740/info" target="_blank" rel="noopener">http://cs1:50070/explorer.html#/hbase/data/hbase/meta/1588230740/info</a></li></ul></li></ul><h2 id="HBase-和-Hive-的比较"><a href="#HBase-和-Hive-的比较" class="headerlink" title="HBase 和 Hive 的比较"></a>HBase 和 Hive 的比较</h2><ul><li>相同点<ul><li>HBase 和 Hive 都是架构在 Hadoop 之上，用 HDFS 做底层的数据存储，用 MapReduce 做 数据计算</li></ul></li><li>不同点<ul><li>解决的问题不同<ul><li>Hive 是建立在 Hadoop 之上为了降低 MapReduce 编程复杂度的 ETL 工具。</li><li>HBase 是为了弥补 Hadoop 对实时操作的缺陷</li></ul></li><li>表的架构不同<ul><li>Hive 表是纯逻辑表，因为 Hive 的本身并不能做数据存储和计算，而是完全依赖 Hadoop</li><li>HBase 是物理表，提供了一张超大的内存 Hash 表来存储索引，方便查询</li></ul></li><li>定位 &amp; 访问机制不同<ul><li>Hive 是数据仓库工具，需要全表扫描，就用 Hive，因为 Hive 是文件存储</li><li>HBase 是数据库，需要索引访问，则用 HBase，因为 HBase 是面向列的 NoSQL 数据库</li></ul></li><li>存储模式不同<ul><li>Hive 表中存入数据(文件)时不做校验，属于读模式存储系统</li><li>HBase 表插入数据时，会和 RDBMS 一样做 Schema 校验，所以属于写模式存储系统</li></ul></li><li>是否实时处理(处理效率)不同<ul><li>Hive 不支持单行记录操作，数据处理依靠 MapReduce，操作延时高</li><li>HBase 支持单行记录的 CRUD，并且是实时处理，效率比 Hive 高得多</li></ul></li></ul></li></ul><h2 id="HBase在-HDFS-上的存储路径"><a href="#HBase在-HDFS-上的存储路径" class="headerlink" title="HBase在 HDFS 上的存储路径"></a>HBase在 HDFS 上的存储路径</h2><ul><li>相同列族的数据存放在一个文件中。</li><li>[表数据的存储目录结构构成]<ul><li><a href="hdfs://cs1:8020/hbase/data/${名字空间}/${表名}/${区域名称}/${列族名称}/${文件名}" target="_blank" rel="noopener">hdfs://cs1:8020/hbase/data/${名字空间}/${表名}/${区域名称}/${列族名称}/${文件名}</a></li></ul></li><li>[WAL目录结构构成]<ul><li><a href="hdfs://cs1:8020/hbase/WALs/${区域服务器名称,端口号,时间戳}/" target="_blank" rel="noopener">hdfs://cs1:8020/hbase/WALs/${区域服务器名称,端口号,时间戳}/</a></li></ul></li></ul><h2 id="Client-端-与-HBase-交互过程"><a href="#Client-端-与-HBase-交互过程" class="headerlink" title="Client 端 与 HBase 交互过程"></a>Client 端 与 HBase 交互过程</h2><ul><li><p>HBase 简单集群结构</p><p><img src="https://img.mubu.com/document_image/5c640ea0-d4ff-43ed-94bb-ebebfc35aa62-584150.jpg" alt="img"></p><ul><li>region:是 hbase 中对表进行切割的单元，由 regionserver 负责管理<ul><li>region 分裂是逻辑概念?? :TODO</li></ul></li><li>hamster:hbase 的主节点，负责整个集群的状态感知，负载分配、负责用户表的元数据(schema)管理(可以配置多个用来实现 HA),hmaster 负载压力相对于 hdfs 的 namenode 会小很多</li><li>regionserver:hbase 中真正负责管理 region 的服务器，也就是负责为客户端进行表数据读写 的服务器每一台 regionserver 会管理很多的 region，同一个 regionserver 上面管理的所有的 region 不属于同一张表</li><li>zookeeper:整个 hbase 中的主从节点协调，主节点之间的选举，集群节点之间的上下线感 知„„都是通过 zookeeper 来实现</li><li>HDFS:用来存储 hbase 的系统文件，或者表的 region</li></ul></li><li><p>Hbase 顶层结构图</p><p><img src="https://img.mubu.com/document_image/ebc78e6e-830c-49af-a05f-c79ec1bad8d6-584150.jpg" alt="img"></p></li><li><p>0.hbase集群启动时，master负责分配区域到指定区域服务器。</p></li><li><p>1.联系zk，找出meta表所在rs(regionserver)/hbase/meta-region-server</p></li><li><p>2.定位row key,找到对应region server</p></li><li><p>3.缓存信息在本地。</p></li><li><p>4.联系RegionServer</p></li><li><p>5.HRegionServer负责open HRegion对象，为每个列族创建Store对象，Store包含多个StoreFile实例，他们是对HFile的轻量级封装。每个Store还对应了一个MemStore，用于内存存储数据。</p></li></ul><h2 id="百万数据批量插入"><a href="#百万数据批量插入" class="headerlink" title="百万数据批量插入"></a>百万数据批量插入</h2><ul><li>代码<br>long start = System.currentTimeMillis() ;<br>Configuration conf = HBaseConfiguration.create();<br>Connection conn = ConnectionFactory.createConnection(conf);<br>TableName tname = TableName.valueOf(“ns1:t1”);<br>HTable table = (HTable)conn.getTable(tname);<br>//不要自动清理缓冲区<br>table.setAutoFlush(false);<br>for(int i = 4 ; i &lt; 1000000 ; i ++){<pre><code>Put put = new Put(Bytes.toBytes(&quot;row&quot; + i)) ;//关闭写前日志put.setWriteToWAL(false);put.addColumn(Bytes.toBytes(&quot;f1&quot;),Bytes.toBytes(&quot;id&quot;),Bytes.toBytes(i));put.addColumn(Bytes.toBytes(&quot;f1&quot;),Bytes.toBytes(&quot;name&quot;),Bytes.toBytes(&quot;tom&quot; + i));put.addColumn(Bytes.toBytes(&quot;f1&quot;),Bytes.toBytes(&quot;age&quot;),Bytes.toBytes(i % 100));table.put(put);if(i % 2000 == 0){    table.flushCommits();}</code></pre>}<br>//<br>table.flushCommits();<br>System.out.println(System.currentTimeMillis() - start ); </li></ul><h2 id="HBase-切割文件"><a href="#HBase-切割文件" class="headerlink" title="HBase 切割文件"></a>HBase 切割文件</h2><ul><li>默认10G 进行切割<property><br>        <name>hbase.hregion.max.filesize</name><br>        <value>10737418240</value><br>        <source>hbase-default.xml<br></property> </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Hbase概念&quot;&gt;&lt;a href=&quot;#Hbase概念&quot; class=&quot;headerlink&quot; title=&quot;Hbase概念&quot;&gt;&lt;/a&gt;Hbase概念&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;hbase &amp;amp; hdfs 关系图&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;htt
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="HBase" scheme="https://airpoet.github.io/categories/Hadoop/HBase/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="HBase" scheme="https://airpoet.github.io/tags/HBase/"/>
    
  </entry>
  
  <entry>
    <title>i-Zookeeper</title>
    <link href="https://airpoet.github.io/2018/06/23/Hadoop/4-Zookeeper/i-Zookeeper/"/>
    <id>https://airpoet.github.io/2018/06/23/Hadoop/4-Zookeeper/i-Zookeeper/</id>
    <published>2018-06-23T06:00:32.111Z</published>
    <updated>2018-06-23T09:59:32.068Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Zookeeper-的安装-amp-概述"><a href="#1-Zookeeper-的安装-amp-概述" class="headerlink" title="1.Zookeeper 的安装&amp;概述"></a>1.Zookeeper 的安装&amp;概述</h1><h3 id="1-1-Zookeeper-简介"><a href="#1-1-Zookeeper-简介" class="headerlink" title="1.1. Zookeeper 简介"></a>1.1. Zookeeper 简介</h3><p><strong>Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。</strong></p><p>zk提供的服务 </p><ul><li><p>Naming service                //按名称区分集群中的节点. </p></li><li><p>Configuration management    //对加入节点的最新化处理。 </p></li><li>Cluster management            //实时感知集群中节点的增减. </li><li>Leader election                //leader选举 </li><li>Locking and synchronization service    //修改时锁定数据，实现容灾. </li><li>Highly reliable data registry        //节点宕机数据也是可用的。 </li></ul><h3 id="1-2-ZK-的工作机制"><a href="#1-2-ZK-的工作机制" class="headerlink" title="1.2.  ZK 的工作机制"></a>1.2.  ZK 的工作机制</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-23-065529.png" alt="image-20180623145528682"></p><h3 id="1-4-ZK-架构"><a href="#1-4-ZK-架构" class="headerlink" title="1.4. ZK 架构"></a>1.4. ZK 架构</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-23-064232.png" alt="image-20180623144232085"></p><h4 id="1-4-1-名词解释"><a href="#1-4-1-名词解释" class="headerlink" title="1.4.1 名词解释"></a>1.4.1 名词解释</h4><p>   1.Client </p><p>​        从server获取信息，周期性发送数据给server，表示自己还活着。 </p><p>​        client连接时，server回传ack信息。 </p><p>​        如果client没有收到reponse，自动重定向到另一个server. </p><p>​    2.Server </p><p>​        zk集群中的一员，向client提供所有service，回传ack信息给client，表示自己还活着。 </p><p>​    3.ensemble </p><p>​        一组服务器。 </p><p>​        最小节点数是3. </p><p>​    4.Leader </p><p>​        如果连接的节点失败，自定恢复，zk服务启动时，完成leader选举。 </p><p>​    5.Follower </p><p>​        追寻leader指令的节点。 </p><h4 id="1-4-2-整体解释"><a href="#1-4-2-整体解释" class="headerlink" title="1.4.2 整体解释"></a>1.4.2 整体解释</h4><ul><li>1）Zookeeper：一个领导者（leader），多个跟随者（follower）组成的集群。</li><li>2）Leader负责进行投票的发起和决议，更新系统状态</li><li>3）Follower用于接收客户请求并向客户端返回结果，在选举Leader过程中参与投票</li><li>4）集群中只要有半数以上节点存活，Zookeeper集群就能正常服务。</li><li>5）全局数据一致：每个server保存一份相同的数据副本，client无论连接到哪个server，数据都是一致的。</li><li>6）更新请求顺序进行，来自同一个client的更新请求按其发送顺序依次执行。</li><li>7）数据更新原子性，一次数据更新要么成功，要么失败。</li><li>8）实时性，在一定时间范围内，client能读到最新数据。</li></ul><h3 id="1-5-znode"><a href="#1-5-znode" class="headerlink" title="1.5. znode"></a>1.5. znode</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-23-064202.png" alt="image-20180623144202281"></p><p>ZooKeeper数据模型的结构与Unix文件系统很类似，整体上可以看作是一棵树，每个节点称做一个ZNode。每一个ZNode默认能够存储1MB的数据，每个ZNode都可以通过其路径唯一标识 </p><p>zk中的节点，维护了stat，由Version number, Action control list (ACL), Timestamp,Data length.构成. </p><p>data version        //数据写入的过程变化 </p><p> ACL                    //action control list, </p><h3 id="1-6-节点类型"><a href="#1-6-节点类型" class="headerlink" title="1.6. 节点类型"></a>1.6. 节点类型</h3><p>​    1.持久节点 </p><p>​        client结束，还存在。 </p><p>​    2.临时节点 </p><p>​        在client活动时有效，断开自动删除。临时节点不能有子节点。 </p><p>​        leader推选时使用。 </p><p>​    3.序列节点 </p><p>​        在节点名之后附加10个数字，主要用于同步和锁. </p><h3 id="1-7-Session"><a href="#1-7-Session" class="headerlink" title="1.7. Session"></a>1.7. Session</h3><p>​    Session中的请求以FIFO执行，一旦client连接到server，session就建立了。sessionid分配client. </p><p>​    client以固定间隔向server发送心跳，表示session是valid的，zk集群如果在超时时候，没有收到心跳， </p><p>​    判定为client挂了，与此同时，临时节点被删除。 </p><h3 id="1-8-Watches"><a href="#1-8-Watches" class="headerlink" title="1.8. Watches"></a>1.8. Watches</h3><p>​    观察。 </p><p>​    client能够通过watch机制在数据发生变化时收到通知。 </p><p>​    client可以在read 节点时设置观察者。watch机制会发送通知给注册的客户端。 </p><p>​    观察模式只触发一次。 </p><p>​    session过期，watch机制删除了。 </p><h3 id="1-9-zk工作流程"><a href="#1-9-zk工作流程" class="headerlink" title="1.9. zk工作流程"></a>1.9. zk工作流程</h3><p>​    zk集群启动后，client连接到其中的一个节点，这个节点可以是leader，也可以是follower。 </p><p>​    连通后，node分配一个id给client，发送ack信息给client。 </p><p>​    如果客户端没有收到ack，连接到另一个节点。 </p><p>​    client周期性发送心跳信息给节点保证连接不会丢失。 </p><p>​    如果client读取数据，发送请求给node，node读取自己数据库，返回节点数据给client. </p><p>​    如果client想要在 zk 中存储数据，将路径和数据发送给server，server转发给leader。 </p><p>​    leader再补发请求给所有follower。只有大多数(超过半数)节点成功响应，则 </p><p>​    写操作成功。 </p><h3 id="1-10-zk-应用场景"><a href="#1-10-zk-应用场景" class="headerlink" title="1.10 zk 应用场景"></a>1.10 zk 应用场景</h3><h4 id="1-10-1-统一命名服务"><a href="#1-10-1-统一命名服务" class="headerlink" title="1.10.1 统一命名服务"></a>1.10.1 统一命名服务</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-23-073654.png" alt="image-20180623153654235"></p><h4 id="1-10-2-统一配管理"><a href="#1-10-2-统一配管理" class="headerlink" title="1.10.2 统一配管理"></a>1.10.2 统一配管理</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-23-073811.png" alt="image-20180623153811327"></p><h4 id="1-10-3-统一集群管理"><a href="#1-10-3-统一集群管理" class="headerlink" title="1.10.3 统一集群管理"></a>1.10.3 统一集群管理</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-23-073914.png" alt="image-20180623153914499"></p><h4 id="1-10-4-服务器动态上下线"><a href="#1-10-4-服务器动态上下线" class="headerlink" title="1.10.4 服务器动态上下线"></a>1.10.4 服务器动态上下线</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-23-074113.png" alt="image-20180623154113232"></p><h4 id="1-10-5-软负载均衡"><a href="#1-10-5-软负载均衡" class="headerlink" title="1.10.5 软负载均衡"></a>1.10.5 软负载均衡</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-23-074213.png" alt="image-20180623154212560"></p><h1 id="2-Zookeeper-完全分布式的安装-amp-基本使用"><a href="#2-Zookeeper-完全分布式的安装-amp-基本使用" class="headerlink" title="2.Zookeeper 完全分布式的安装&amp;基本使用"></a>2.Zookeeper 完全分布式的安装&amp;基本使用</h1><h3 id="2-1-安装"><a href="#2-1-安装" class="headerlink" title="2.1 安装"></a>2.1 安装</h3><h4 id="2-1-1-高可用安装"><a href="#2-1-1-高可用安装" class="headerlink" title="2.1.1 高可用安装"></a>2.1.1 高可用安装</h4><p><a href="https://airpoet.github.io/2018/06/22/Hadoop/0-Hadoop/Hadoop-HA%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/">PS:  高可用安装见这里</a></p><h4 id="2-2-2-完全分布式安装"><a href="#2-2-2-完全分布式安装" class="headerlink" title="2.2.2 完全分布式安装"></a>2.2.2 完全分布式安装</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">1.挑选3台主机</span><br><span class="line">cs1 ~ cs3</span><br><span class="line">2.每台机器都安装zk &amp; 配置环境变量</span><br><span class="line"></span><br><span class="line">3.配置zk配置文件</span><br><span class="line">cs1 ~ cs3</span><br><span class="line">[/home/ap/apps/zk/conf/zoo.cfg]</span><br><span class="line">----------</span><br><span class="line">tickTime=2000</span><br><span class="line">initLimit=10</span><br><span class="line">syncLimit=5</span><br><span class="line">dataDir=/home/ap/zookeeper</span><br><span class="line">clientPort=2181</span><br><span class="line"></span><br><span class="line">server.1=cs1:2888:3888</span><br><span class="line">server.2=cs2:2888:3888</span><br><span class="line">server.3=cs3:2888:3888</span><br><span class="line">---------</span><br><span class="line"></span><br><span class="line">4.在每台主机的/home/ap/zookeeper中添加myid,内容分别是1,2,3</span><br><span class="line">[cs1]</span><br><span class="line">$&gt;<span class="built_in">echo</span> 1 &gt; /home/ap/zookeeper/myid</span><br><span class="line">[cs2]</span><br><span class="line">$&gt;<span class="built_in">echo</span> 2 &gt; /home/ap/zookeeper/myid</span><br><span class="line">[cs3]</span><br><span class="line">$&gt;<span class="built_in">echo</span> 3 &gt; /home/ap/zookeeper/myid</span><br><span class="line"></span><br><span class="line">5.在cs1~cs3上,启动服务器集群 </span><br><span class="line">$&gt;zkServer.sh start</span><br><span class="line"></span><br><span class="line">6.查看每台服务器的状态</span><br><span class="line">$&gt;zkServer.sh status</span><br><span class="line">* 注意: 如果有3台机器, 只启动一台的话, 会显示如下, 因为根据配置, 有3台机器, 此时只有一台启动, 没有过半数, 整个集群是挂掉的</span><br><span class="line">* 只有启动的机器数量超过配置的半数, zk 集群才有效.</span><br></pre></td></tr></table></figure><h4 id="2-2-3-zoo-cfg-文件中配置参数的含义"><a href="#2-2-3-zoo-cfg-文件中配置参数的含义" class="headerlink" title="2.2.3 zoo.cfg 文件中配置参数的含义"></a>2.2.3 <strong>zoo.cfg 文件中配置参数的含义</strong></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">1）tickTime=2000：通信心跳数</span><br><span class="line">    tickTime：通信心跳数，Zookeeper服务器心跳时间，单位毫秒</span><br><span class="line">    Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。</span><br><span class="line">    它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime)</span><br><span class="line">2）initLimit=10：LF初始通信时限</span><br><span class="line">    initLimit：LF初始通信时限</span><br><span class="line">    集群中的follower跟随者服务器(F)与leader领导者服务器(L)之间初始连接时能容忍的最多心跳数（tickTime的数量），用它来限定集群中的Zookeeper服务器连接到Leader的时限。</span><br><span class="line">    投票选举新leader的初始化时间</span><br><span class="line">    Follower在启动过程中，会从Leader同步所有最新数据，然后确定自己能够对外服务的起始状态。</span><br><span class="line">    Leader允许F在initLimit时间内完成这个工作。</span><br><span class="line">3）syncLimit=5：LF同步通信时限</span><br><span class="line">    syncLimit：LF同步通信时限</span><br><span class="line">    集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，</span><br><span class="line">    Leader认为Follwer死掉，从服务器列表中删除Follwer。</span><br><span class="line">    在运行过程中，Leader负责与ZK集群中所有机器进行通信，例如通过一些心跳检测机制，来检测机器的存活状态。</span><br><span class="line">    如果L发出心跳包在syncLimit之后，还没有从F那收到响应，那么就认为这个F已经不在线了。</span><br><span class="line">4）dataDir：数据文件目录+数据持久化路径</span><br><span class="line">    dataDir：数据文件目录+数据持久化路径</span><br><span class="line">    保存内存数据库快照信息的位置，如果没有其他说明，更新的事务日志也保存到数据库。</span><br><span class="line">5）clientPort=2181：客户端连接端口</span><br><span class="line">    监听客户端连接的端口</span><br><span class="line">6) Server.A=B:C:D。</span><br><span class="line">    A是一个数字，表示这个是第几号服务器；</span><br><span class="line">    B是这个服务器的ip地址；</span><br><span class="line">    C是这个服务器与集群中的Leader服务器交换信息的端口；</span><br><span class="line">    D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。</span><br><span class="line">    <span class="comment"># 集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。</span></span><br></pre></td></tr></table></figure><h3 id="2-2-ZK客户端的连接"><a href="#2-2-ZK客户端的连接" class="headerlink" title="2.2 ZK客户端的连接"></a>2.2 ZK客户端的连接</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">$&gt;zkCli.sh -server cs1:2181    //进入zk命令行</span><br><span class="line"><span class="variable">$zk</span>]<span class="built_in">help</span>                        //查看帮助</span><br><span class="line"><span class="variable">$zk</span>]quit                        //退出</span><br><span class="line"><span class="variable">$zk</span>]create /a tom                 //创建节点</span><br><span class="line"><span class="variable">$zk</span>]get /a                        //查看数据</span><br><span class="line"><span class="variable">$zk</span>]ls /                          //列出节点</span><br><span class="line"><span class="variable">$zk</span>]ls2 /                         //查看当前节点数据并能看到更新次数等数据</span><br><span class="line"><span class="variable">$zk</span>]<span class="built_in">set</span> /a tom                    //设置数据</span><br><span class="line"><span class="variable">$zk</span>]delete /a                     //删除一个节点</span><br><span class="line"><span class="variable">$zk</span>]rmr /a                        //递归删除所有节点。</span><br><span class="line"><span class="variable">$zk</span>]<span class="built_in">stat</span> ///查看节点状态</span><br><span class="line"></span><br><span class="line">注意: </span><br><span class="line">* 创建节点不能递归创建, 目前只能一层一层创建</span><br><span class="line">* 每次创建都的写数据, 只创建目录的话会创建不成功</span><br><span class="line">* 如果 close 后, 并没有退出客户端, 只是访问不到数据, 此时如果想要连接, 可以用connect host:port</span><br><span class="line">================================================================================</span><br><span class="line"></span><br><span class="line">ZK 的帮助文档</span><br><span class="line">----------------</span><br><span class="line"><span class="built_in">help</span>:</span><br><span class="line">ZooKeeper -server host:port cmd args</span><br><span class="line">    <span class="built_in">stat</span> path [watch]</span><br><span class="line">    <span class="built_in">set</span> path data [version]</span><br><span class="line">    ls path [watch]</span><br><span class="line">    delquota [-n|-b] path</span><br><span class="line">    ls2 path [watch]</span><br><span class="line">    setAcl path acl</span><br><span class="line">    setquota -n|-b val path</span><br><span class="line">    <span class="built_in">history</span></span><br><span class="line">    redo cmdno</span><br><span class="line">    printwatches on|off</span><br><span class="line">    delete path [version]</span><br><span class="line">    sync path</span><br><span class="line">    listquota path</span><br><span class="line">    rmr path</span><br><span class="line">    get path [watch]</span><br><span class="line">    create [-s] [-e] path data acl</span><br><span class="line">    addauth scheme auth</span><br><span class="line">    quit</span><br><span class="line">    getAcl path</span><br><span class="line">    close</span><br><span class="line">    connect host:port</span><br></pre></td></tr></table></figure><h3 id="2-3-监听测试"><a href="#2-3-监听测试" class="headerlink" title="2.3 监听测试"></a>2.3 监听测试</h3><p><strong>注意点: 注册的监听只能使用一次, 监听完毕后需要重新注册.</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">=======节点值的变化的监听==========</span><br><span class="line">（1）在cs1主机上注册监听/app1节点数据变化</span><br><span class="line">[zk: localhost:2181(CONNECTED) 26] get /app1 watch</span><br><span class="line"></span><br><span class="line">（2）在cs2主机上修改/app1节点的数据</span><br><span class="line">[zk: localhost:2181(CONNECTED) 5] <span class="built_in">set</span> /app1  777</span><br><span class="line"></span><br><span class="line">（3）观察cs2主机收到数据变化的监听</span><br><span class="line">WATCHER::</span><br><span class="line">WatchedEvent state:SyncConnected <span class="built_in">type</span>:NodeDataChanged path:/app1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">=======节点的子节点变化的监听==========</span><br><span class="line">（1）在cs1主机上注册监听/app1节点的子节点变化</span><br><span class="line">[zk: localhost:2181(CONNECTED) 1] ls /app1 watch</span><br><span class="line">[aa0000000001, server101]</span><br><span class="line"></span><br><span class="line">（2）在cs2主机/app1节点上创建子节点</span><br><span class="line">[zk: localhost:2181(CONNECTED) 6] create /app1/bb 666</span><br><span class="line">Created /app1/bb</span><br><span class="line"></span><br><span class="line">（3）观察cs1主机收到子节点变化的监听</span><br><span class="line">WATCHER::</span><br><span class="line">WatchedEvent state:SyncConnected <span class="built_in">type</span>:NodeChildrenChanged path:/app1</span><br></pre></td></tr></table></figure><h1 id="3-ZK-内部机制"><a href="#3-ZK-内部机制" class="headerlink" title="3. ZK 内部机制"></a>3. ZK 内部机制</h1><h2 id="3-1-选举机制"><a href="#3-1-选举机制" class="headerlink" title="3.1  选举机制"></a>3.1  选举机制</h2><h4 id="3-1-1-zookeeper的选举机制（全新集群paxos）"><a href="#3-1-1-zookeeper的选举机制（全新集群paxos）" class="headerlink" title="3.1.1. zookeeper的选举机制（全新集群paxos）"></a>3.1.1. zookeeper的选举机制（全新集群paxos）</h4><p>以一个简单的例子来说明整个选举的过程.<br> 假设有五台服务器组成的zookeeper集群,它们的id从1-5,同时它们都是最新启动的,也就是没有历史数据,在存放数据量这一点上,都是一样的.假设这些服务器依序启动,来看看会发生什么.<br> 1) 服务器1启动,此时只有它一台服务器启动了,它发出去的报没有任何响应,所以它的选举状态一直是LOOKING状态<br> 2) 服务器2启动,它与最开始启动的服务器1进行通信,互相交换自己的选举结果,由于两者都没有历史数据,所以id值较大的服务器2胜出,但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3),所以服务器1,2还是继续保持LOOKING状态.<br> 3) 服务器3启动,根据前面的理论分析,服务器3成为服务器1,2,3中的老大,而与上面不同的是,此时有三台服务器选举了它,所以它成为了这次选举的leader.<br> 4) 服务器4启动,根据前面的分析,理论上服务器4应该是服务器1,2,3,4中最大的,但是由于前面已经有半数以上的服务器选举了服务器3,所以它只能接收当小弟的命了.<br> 5) 服务器5启动,同4一样,当小弟.</p><h4 id="3-1-2-非全新集群的选举机制-数据恢复"><a href="#3-1-2-非全新集群的选举机制-数据恢复" class="headerlink" title="3.1.2. 非全新集群的选举机制(数据恢复)"></a>3.1.2. 非全新集群的选举机制(数据恢复)</h4><p>那么，初始化的时候，是按照上述的说明进行选举的，但是当zookeeper运行了一段时间之后，有机器down掉，重新选举时，选举过程就相对复杂了。</p><p>需要加入数据id、leader id和逻辑时钟。</p><p>数据id：数据新的id就大，数据每次更新都会更新id。</p><p>Leader id：就是我们配置的myid中的值，每个机器一个。</p><p>逻辑时钟：这个值从0开始递增,每次选举对应一个值,也就是说:  如果在同一次选举中,那么这个值应该是一致的 ;  逻辑时钟值越大,说明这一次选举leader的进程更新.</p><p>选举的标准就变成：</p><p>​                     1、逻辑时钟小的选举结果被忽略，重新投票</p><p>​                     2、统一逻辑时钟后，数据id大的胜出</p><p>​                     3、数据id相同的情况下，leader id大的胜出</p><p>根据这个规则选出leader。</p><h2 id="3-2-stat-的结构"><a href="#3-2-stat-的结构" class="headerlink" title="3.2  stat 的结构"></a>3.2  stat 的结构</h2><ul><li>1）czxid- 引起这个znode创建的zxid，创建节点的事务的zxid<ul><li>每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。<br>事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于zxid2，那么zxid1在zxid2之前发生。</li></ul></li><li>2）ctime - znode被创建的毫秒数(从1970年开始)</li><li>3）mzxid - znode最后更新的zxid</li><li>4）mtime - znode最后修改的毫秒数(从1970年开始)</li><li>5）pZxid-znode最后更新的子节点zxid</li><li>6）cversion - znode子节点变化号，znode子节点修改次数</li><li>7）dataversion - znode数据变化号</li><li>8）aclVersion - znode访问控制列表的变化号</li><li>9）ephemeralOwner- 如果是临时节点，这个是znode拥有者的session id。如果不是临时节点则是0。</li><li>10）dataLength- znode的数据长度</li><li>11）numChildren - znode子节点数量</li></ul><h1 id="4-通过-JavaAPI-访问-ZK"><a href="#4-通过-JavaAPI-访问-ZK" class="headerlink" title="4. 通过 JavaAPI 访问 ZK"></a>4. 通过 JavaAPI 访问 ZK</h1><h3 id="4-1-添加Maven-依赖"><a href="#4-1-添加Maven-依赖" class="headerlink" title="4.1  添加Maven 依赖"></a>4.1  添加Maven 依赖</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">9.1[pom.xml]</span><br><span class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.it18zhang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>ZooKeeperDemo<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.zookeeper<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>zookeeper<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.4.9<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.11<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="4-2-Java-代码"><a href="#4-2-Java-代码" class="headerlink" title="4.2 Java 代码"></a>4.2 Java 代码</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line">* 对应关系:</span><br><span class="line">*  Linux   Java</span><br><span class="line">----------------------</span><br><span class="line">*  ls      getChildren</span><br><span class="line">*  get     getData</span><br><span class="line">*  set     setDAta</span><br><span class="line">*  create  create</span><br><span class="line">˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> com.rox.zktest;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.data.ACL;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.data.Stat;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 对应关系:</span></span><br><span class="line"><span class="comment"> *  Linux   Java</span></span><br><span class="line"><span class="comment"> *  ls      getChildren</span></span><br><span class="line"><span class="comment"> *  get     getData</span></span><br><span class="line"><span class="comment"> *  set     setDAta</span></span><br><span class="line"><span class="comment"> *  create  create</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestZK</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">ls</span><span class="params">()</span> <span class="keyword">throws</span> IOException, KeeperException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 放三个就不行, 只能放2个目前来看</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        ZooKeeper zk = <span class="keyword">new</span> ZooKeeper(<span class="string">"cs1:2181,cs2:2181,cs3:2181"</span>, <span class="number">5000</span>, <span class="keyword">null</span>);</span><br><span class="line">        List&lt;String&gt; list = zk.getChildren(<span class="string">"/"</span>, <span class="keyword">null</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (String s : list) &#123;</span><br><span class="line">            System.out.println(s);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">lsAll</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            ls(<span class="string">"/"</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 列出指定 path 下的children</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> path</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">ls</span><span class="params">(String path)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        System.out.println(path);</span><br><span class="line"></span><br><span class="line">        ZooKeeper zk = <span class="keyword">new</span> ZooKeeper(<span class="string">"cs1:2181,cs2:2181,cs3:2181"</span>, <span class="number">5000</span>, <span class="keyword">null</span>);</span><br><span class="line">        List&lt;String&gt; list = zk.getChildren(path, <span class="keyword">null</span>);</span><br><span class="line">        <span class="keyword">if</span> (list == <span class="keyword">null</span> || list.isEmpty()) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (String s : list) &#123;</span><br><span class="line">            <span class="comment">// 先输出 children</span></span><br><span class="line">            <span class="keyword">if</span> (path.equals(<span class="string">"/"</span> )) &#123;</span><br><span class="line">                ls(path + s);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                ls(path + <span class="string">"/"</span> + s);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 设置数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span>  <span class="keyword">void</span>  <span class="title">setData</span><span class="params">()</span> <span class="keyword">throws</span>  Exception </span>&#123;</span><br><span class="line">        ZooKeeper zk = <span class="keyword">new</span> ZooKeeper(<span class="string">"cs1:2181"</span>, <span class="number">5000</span>, <span class="keyword">null</span>);</span><br><span class="line">        zk.setData(<span class="string">"/a"</span>,<span class="string">"tomaslee"</span>.getBytes(),<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 创建临时节点</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span>  <span class="title">createEPHEMERAL</span><span class="params">()</span> <span class="keyword">throws</span>  Exception </span>&#123;</span><br><span class="line">        ZooKeeper zk = <span class="keyword">new</span> ZooKeeper(<span class="string">"cs1:2181"</span>, <span class="number">5000</span>, <span class="keyword">null</span>);</span><br><span class="line">        zk.create(<span class="string">"/c/c1"</span>, <span class="string">"tom"</span>.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE,</span><br><span class="line">                CreateMode.EPHEMERAL);</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"hello"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 创建观察者</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span>  <span class="title">testWatch</span><span class="params">()</span> <span class="keyword">throws</span>  Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> ZooKeeper zk = <span class="keyword">new</span> ZooKeeper(<span class="string">"cs1:2181,cs2:2181,cs3:2181"</span>, <span class="number">5000</span>, <span class="keyword">null</span>);</span><br><span class="line"></span><br><span class="line">        Stat st = <span class="keyword">new</span> Stat();</span><br><span class="line"></span><br><span class="line">        Watcher w = <span class="keyword">null</span>;</span><br><span class="line">        w = <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    System.out.println(<span class="string">"数据改了..."</span>);</span><br><span class="line">                    zk.getData(<span class="string">"/a"</span>,<span class="keyword">this</span>,<span class="keyword">null</span>);</span><br><span class="line">                &#125;<span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">byte</span>[] data = zk.getData(<span class="string">"/a"</span>,w,st);</span><br><span class="line">        System.out.println(<span class="keyword">new</span> String(data));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Zookeeper-的安装-amp-概述&quot;&gt;&lt;a href=&quot;#1-Zookeeper-的安装-amp-概述&quot; class=&quot;headerlink&quot; title=&quot;1.Zookeeper 的安装&amp;amp;概述&quot;&gt;&lt;/a&gt;1.Zookeeper 的安装&amp;amp;
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="Zookeeper" scheme="https://airpoet.github.io/categories/Hadoop/Zookeeper/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="Zookeeper" scheme="https://airpoet.github.io/tags/Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop-HA高可用集群QJM搭建</title>
    <link href="https://airpoet.github.io/2018/06/22/Hadoop/0-Hadoop/Hadoop-HA%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"/>
    <id>https://airpoet.github.io/2018/06/22/Hadoop/0-Hadoop/Hadoop-HA高可用集群搭建/</id>
    <published>2018-06-21T16:38:59.993Z</published>
    <updated>2018-06-23T07:21:01.387Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-此教程默认已经搭建好完全分布式"><a href="#1-此教程默认已经搭建好完全分布式" class="headerlink" title="1.此教程默认已经搭建好完全分布式"></a>1.此教程默认已经搭建好完全分布式</h3><h3 id="2-Zookeeper-集群搭建"><a href="#2-Zookeeper-集群搭建" class="headerlink" title="2. Zookeeper 集群搭建"></a>2. Zookeeper 集群搭建</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">配置完全分布式zk集群</span><br><span class="line">---------------------</span><br><span class="line">    1.挑选3台主机</span><br><span class="line">        cs1 ~ cs3</span><br><span class="line">    2.每台机器都安装zk</span><br><span class="line">        tar</span><br><span class="line">        环境变量</span><br><span class="line"></span><br><span class="line">    3.配置zk配置文件</span><br><span class="line">        cs1 ~ cs3</span><br><span class="line">        [/home/ap/apps/zk/conf/zoo.cfg]</span><br><span class="line">        ...</span><br><span class="line">        dataDir=/home/ap/zookeeper</span><br><span class="line"></span><br><span class="line">    4.在每台主机的/home/centos/zookeeper中添加myid,内容分别是1,2,3</span><br><span class="line">        [cs1]</span><br><span class="line">        $&gt;<span class="built_in">echo</span> 1 &gt; /home/ap/zookeeper/myid</span><br><span class="line">        [cs2]</span><br><span class="line">        $&gt;<span class="built_in">echo</span> 2 &gt; /home/ap/zookeeper/myid</span><br><span class="line">        [cs3]</span><br><span class="line">        $&gt;<span class="built_in">echo</span> 3 &gt; /home/ap/zookeeper/myid</span><br><span class="line"></span><br><span class="line">    5.启动服务器集群 </span><br><span class="line">        $&gt;zkServer.sh start</span><br><span class="line">            </span><br><span class="line">    6.查看每台服务器的状态</span><br><span class="line">        $&gt;zkServer.sh status</span><br><span class="line">* 注意: 如果有3台机器, 只启动一台的话, 会显示如下, 因为根据配置, 有3台机器, 此时只有一台启动, 没有过半数, 整个集群是挂掉的</span><br><span class="line">* 只有启动的机器数量超过配置的半数, zk 集群才有效.</span><br></pre></td></tr></table></figure><h3 id="3-HA-集群搭建"><a href="#3-HA-集群搭建" class="headerlink" title="3.HA 集群搭建"></a>3.HA 集群搭建</h3><h4 id="首先声明"><a href="#首先声明" class="headerlink" title="首先声明"></a>首先声明</h4><p><strong>笔者用的6台主机,  主机名 <code>cs1-cs6</code>,  用户名为<code>ap</code>, 可以对照改为自己的主机名&amp;用户名</strong></p><p>另外, 搭建 HA 不会影响原来的完全分布式, 具体操作会在下面告知.<br><strong>hadoop 安装目录层级结构:</strong></p><ul><li><code>/home/ap/apps/hadoop/etc/hadoop/hdfs-site.xml</code></li></ul><p><strong>data 目录层级结构:</strong></p><ul><li><code>cs1</code>: <code>/home/ap/hadoopdata/namenode/current/edits_00....</code> </li><li><code>cs2</code>: <code>/home/ap/hadoopdata/datanode/current/BP-15...</code></li></ul><p><strong>可以对照参考</strong></p><p><br></p><p><strong>集群结构如下</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-21-164723.png" alt="image-20180622004722436"></p><hr><h4 id="开始搭建"><a href="#开始搭建" class="headerlink" title="开始搭建"></a>开始搭建</h4><p><strong>首先保证 各节点直接的 ssh 免密登录没问题</strong></p><ul><li>如果非生产环境, 可以同时把 <code>.ssh</code>删掉后, 全部重新生成 <code>ssh-keygen</code>,  同时相互发送, 这样操作最简单, 效率最高. </li></ul><p><strong>其次上代码了</strong></p><ol><li>把原本<code>/home/ap/apps/hadoop/etc/hadoop</code>中的 <code>hadoop</code>目录改为<code>full</code>,意思是完全分布式.  </li><li><code>cp -r full ha</code>, 复制一份 full 为 ha, 在这份配置文件中配置 HA</li><li><code>ln -s /home/ap/apps/hadoop/etc/ha  /home/ap/apps/hadoop/etc/hadoop</code>, 用一个软链接hadoop 指向 ha</li><li>配置 <code>/home/ap/apps/hadoop/etc/ha/hdfs-site.xml</code></li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">[hdfs-site.xml]</span><br><span class="line">------------------------------------------------------------------</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定 2个 namenode 的命名空间 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- myucluster下的名称节点两个id --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 配置每个nn的rpc地址。 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cs1:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cs6:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 配置webui端口 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cs1:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cs6:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 名称节点共享编辑目录. --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://cs2:8485;cs3:8485;cs4:8485/mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- java类，client使用它判断哪个节点是激活态。 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 脚本列表或者java类，在容灾保护激活态的nn. --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">                sshfence</span><br><span class="line">                shell(/bin/true)</span><br><span class="line">        <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- ssh免密登陆 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/ap/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 配置 sshfence 隔离机制超时时间(可不配) --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.connect-timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>30000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 配置JN存放edit的本地路径。 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/ap/hadoopdata/journal<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><ol start="5"><li>配置 <code>core-site.xml</code>, 这里给出完整配置 (目前不包括 Hive 配置)</li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[core-site.xml]</span><br><span class="line">------------------------------------------------------------</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!-- 指定 hdfs 的 nameservice 为 mycluster --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定 hadoop 工作目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/ap/hadoopdata<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定zk集群访问地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cs1:2181,cs2:2181,cs3:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><ol start="6"><li>配置<code>mapred-site.xml</code></li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[mapred-site.xml]</span><br><span class="line">-------------------</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!-- 指定 mr 框架为 yarn 方式 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 设置 mapreduce 的历史服务器地址和端口号 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cs1:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- mapreduce 历史服务器的 web 访问地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cs1:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><ol start="6"><li>把<code>/home/ap/apps/hadoop/etc/*</code>发给其他所有节点 2-6<ul><li>注意: 软链接scp 的时候会有问题, 最终保证每个节点跟 cs1一样就可以了,可以每个节点单独修改, 也可以写脚本一起修改</li><li><code>ln -sfT /home/ap/apps/hadoop/etc/ha  /home/ap/apps/hadoop/etc/hadoop</code></li></ul></li><li><strong>部署细节</strong></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">0.在 zk 节点启动 zkServer</span><br><span class="line">cs1-cs3: $&gt;zkServer.sh start</span><br><span class="line"></span><br><span class="line">1.在jn节点分别启动jn进程</span><br><span class="line">$&gt;hadoop-daemon.sh start journalnode</span><br><span class="line"></span><br><span class="line">2.启动jn之后，在两个NN之间进行disk元数据同步</span><br><span class="line">    a)如果是全新集群，先format文件系统,只需要在一个nn上执行。</span><br><span class="line">    [cs1]</span><br><span class="line">    $&gt;hadoop namenode -format</span><br><span class="line"></span><br><span class="line">    b)如果将非HA集群转换成HA集群，复制原NN的metadata到另一个nn.</span><br><span class="line">        1.步骤一</span><br><span class="line">        [cs1]</span><br><span class="line">        $&gt;scp -r /home/centos/hadoop/dfs ap@cs6:/home/centos/hadoop/</span><br><span class="line"></span><br><span class="line">        2.步骤二</span><br><span class="line">        在新的nn(未格式化的nn)上运行一下命令，实现待命状态引导。</span><br><span class="line">        [cs6]</span><br><span class="line">        $&gt;hdfs namenode -bootstrapStandby        //需要cs1为启动状态,提示是否格式化,选择N.</span><br><span class="line"></span><br><span class="line">3.在一个NN上执行以下命令，完成edit日志到jn节点的传输。</span><br><span class="line">$&gt;hdfs namenode -initializeSharedEdits</span><br><span class="line"><span class="comment">#查看cs2,cs3,cs4 这几个 jn 节点是否有edit数据.</span></span><br><span class="line"></span><br><span class="line">4.启动所有节点.</span><br><span class="line">[cs1]</span><br><span class="line">$&gt;hadoop-daemon.sh start namenode        //启动名称节点</span><br><span class="line">$&gt;hadoop-daemons.sh start datanode        //启动所有数据节点</span><br><span class="line"></span><br><span class="line">[2,3,4]</span><br><span class="line">$&gt;hadoop-daemon.sh start journalnode</span><br><span class="line"></span><br><span class="line">[cs6]</span><br><span class="line">$&gt;hadoop-daemon.sh start namenode        //启动名称节点</span><br></pre></td></tr></table></figure><ol start="8"><li><strong>HA 管理</strong></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看web 界面, 是否是2个 standby 状态</span></span><br><span class="line">http://cs1:50070/</span><br><span class="line">http://cs6:50070/</span><br><span class="line"></span><br><span class="line">hdfs haadmin : 查看 ha 帮助</span><br><span class="line">-----------------</span><br><span class="line">$&gt;hdfs haadmin -transitionToActive nn1                //切成激活态</span><br><span class="line">$&gt;hdfs haadmin -transitionToStandby nn1                //切成待命态</span><br><span class="line">$&gt;hdfs haadmin -transitionToActive --forceactive nn2//强行激活</span><br><span class="line">$&gt;hdfs haadmin -failover nn1 nn2                    //模拟容灾演示,从nn1切换到nn2</span><br></pre></td></tr></table></figure><ol start="9"><li><strong>加入 Zookeeper 容灾服务 zkfc</strong></li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 1.部署容灾 --&gt;</span></span><br><span class="line">------------------------------</span><br><span class="line">a.停止所有进程</span><br><span class="line">$&gt;stop-all.sh</span><br><span class="line"></span><br><span class="line">b.配置hdfs-site.xml，启用自动容灾.</span><br><span class="line">[hdfs-site.xml]</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">c.配置core-site.xml，指定zk的连接地址.</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>cs1:2181,cs2:2181,cs3:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">d.分发以上两个文件到所有节点。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 2.登录其中的一台NN(s201),在ZK中初始化HA状态,</span></span><br><span class="line"><span class="comment"> 创建 namenode 的 命名空间节点 mycluster --&gt;</span></span><br><span class="line">------------------------------------</span><br><span class="line">$&gt;hdfs zkfc -formatZK</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 3.启动hdfs进程. --&gt;</span>    </span><br><span class="line">$&gt;start-dfs.sh</span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 4.查看 webUI, 是否有一台自动切换为 active 状态了 --&gt;</span></span><br><span class="line">http://cs1:50070/</span><br><span class="line">http://cs6:50070/</span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 5.测试自动容灾(如果cs6是活跃节点) --&gt;</span>    </span><br><span class="line">$&gt;kill -9  cs6的 namenode进程号</span><br><span class="line">观察 cs1:50070的状态变化</span><br></pre></td></tr></table></figure><ol start="10"><li><strong>配置RM的HA自动容灾</strong></li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line">1.配置yarn-site.xml</span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 是否允许高可用 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- Identifies the cluster. Used by the elector to ensure an RM doesn’t take over as Active for another cluster.  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cluster1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- RM 的 id --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 2台 RM 的宿主 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cs1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cs6<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 配置web界面 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cs1:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cs6:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- zookeeper 地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cs1:2181,cs2:2181,cs3:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- YARN 集群为 MapReduce 程序提供的 shuffle 服务(原本的)  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- ===========以下是可选的============= --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 开启 YARN 集群的日志聚合功能 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- YARN 集群的聚合日志最长保留时长 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>86400<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 启用自动恢复 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定 resourcemanager 的状态信息存储在 zookeeper 集群上--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.store.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2.使用管理命令</span><br><span class="line"><span class="comment">&lt;!-- 查看状态 --&gt;</span></span><br><span class="line">$&gt;yarn rmadmin -getServiceState rm1</span><br><span class="line"><span class="comment">&lt;!-- 切换状态到standby --&gt;</span></span><br><span class="line">$&gt;yarn rmadmin -transitionToStandby rm1</span><br><span class="line"></span><br><span class="line">3.启动yarn集群</span><br><span class="line">$&gt;start-yarn.sh</span><br><span class="line"></span><br><span class="line">4.hadoop没有启动两个resourcemanager,需要手动启动另外一个</span><br><span class="line">$&gt;yarn-daemon.sh start resourcemanager</span><br><span class="line"></span><br><span class="line">5.查看webUI, 点击 About, 查看 active 或 standby</span><br><span class="line">http://cs1:8088</span><br><span class="line">http://cs6:8088</span><br><span class="line"></span><br><span class="line">6.做容灾模拟.</span><br><span class="line">kill -9 活跃的 RM 端口号</span><br><span class="line"></span><br><span class="line">7.注意: 如果容灾失败, 检查下每台主机时间是否同步</span><br><span class="line">$&gt;sudo ntpdate ntp1.aliyun.com</span><br></pre></td></tr></table></figure><p><strong>至此, 大功告成</strong></p><hr><h3 id="4-HA-集群的启动-关闭"><a href="#4-HA-集群的启动-关闭" class="headerlink" title="4.HA 集群的启动/关闭"></a>4.HA 集群的启动/关闭</h3><h4 id="4-1-HA-的启动"><a href="#4-1-HA-的启动" class="headerlink" title="4.1 HA 的启动"></a>4.1 HA 的启动</h4><p><strong>一:单点启动</strong></p><ol start="0"><li><p>启动 zk 服务 QuorumPeerMain, 否则后面 RM 会起不来,连不上 ZK 服务<br>cs1-cs3: $&gt; <code>zkServer.sh start</code></p></li><li><p>启动 namenode/datanode<br> cs1,cs6: $&gt; <code>hadoop-daemon.sh start namenode</code><br> cs1/cs6: $&gt; <code>hadoop-daemons.sh start datanode</code><br> cs6:     $&gt; <code>hadoop-daemon.sh start namenode</code></p></li></ol><ol start="2"><li>启动 journalnode<br> cs2-cs4: $&gt; <code>hadoop-daemon.sh start journalnode</code></li></ol><ol start="3"><li>启动RM (RM 会自动选出一个 active)<br> cs1,cs6: $&gt; <code>yarn-daemon.sh start resourcemanager</code><br> cs1/cs6: $&gt; <code>yarn-daemons.sh start nodemanager</code></li></ol><ol start="4"><li><p>启动 zk 的 DFSZKFailoverController<br> cs1,cs6: $&gt; <code>hadoop-daemon.sh start zkfc</code></p></li><li><p>此外可以启动MapReduce 的历史任务服务器</p><p>[ap@cs1]$&gt; <code>mr-jobhistory-daemon.sh start historyserver</code></p><p>然后访问配置的 <code>http://cs1:19888/jobhistory</code></p></li></ol><p><strong>二:懒汉启动</strong></p><ol start="0"><li><p>启动 zk 服务 QuorumPeerMain, 否则后面 RM 会起不来,连不上 ZK 服务<br>cs1-cs3: $&gt; <code>zkServer.sh start</code></p></li><li><p>执行启动全部</p><p>cs1: $&gt;  <code>start-all.sh</code>(RM 节点)</p><p>或者, 使用新的启动方式</p><p>cs1: $&gt; <code>start-dfs.sh</code>(任意节点)</p><p>cs1: $&gt; <code>start-yarn.sh</code>(在 RM 节点)</p></li><li><p>另一个 RM 节点不会自己启动,要手动启动</p><p>cs6: $&gt;  <code>yarn-daemon.sh start resourcemanager</code></p></li></ol><p>三: 启动完成后</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">============= cs1 jps =============</span><br><span class="line">5696 QuorumPeerMain</span><br><span class="line">6641 DFSZKFailoverController</span><br><span class="line">6338 NameNode</span><br><span class="line">6756 ResourceManager</span><br><span class="line">6873 Jps</span><br><span class="line">============= cs2 jps =============</span><br><span class="line">10849 Jps</span><br><span class="line">10722 NodeManager</span><br><span class="line">10631 JournalNode</span><br><span class="line">10541 DataNode</span><br><span class="line">127934 QuorumPeerMain</span><br><span class="line">============= cs3 jps =============</span><br><span class="line">630 NodeManager</span><br><span class="line">758 Jps</span><br><span class="line">535 JournalNode</span><br><span class="line">443 DataNode</span><br><span class="line">117503 QuorumPeerMain</span><br><span class="line">============= cs4 jps =============</span><br><span class="line">79589 Jps</span><br><span class="line">79462 NodeManager</span><br><span class="line">79368 JournalNode</span><br><span class="line">79278 DataNode</span><br><span class="line">============= cs5 jps =============</span><br><span class="line">23655 Jps</span><br><span class="line">23529 NodeManager</span><br><span class="line">23423 DataNode</span><br><span class="line">============= cs6 jps =============</span><br><span class="line">35680 Jps</span><br><span class="line">35506 NameNode</span><br><span class="line">35608 DFSZKFailoverController</span><br><span class="line">21455 ResourceManager</span><br></pre></td></tr></table></figure><h4 id="4-2-HA-的关闭"><a href="#4-2-HA-的关闭" class="headerlink" title="4.2 HA 的关闭"></a>4.2 HA 的关闭</h4><p>在 一台NN 上<code>stop-all.sh</code>, 注意 zk 的 server- <code>QuorumPeerMain</code>不会停掉</p><h3 id="5-在集群实现时间同步-root-用户操作"><a href="#5-在集群实现时间同步-root-用户操作" class="headerlink" title="5. 在集群实现时间同步(root 用户操作)"></a>5. 在集群实现时间同步(root 用户操作)</h3><h4 id="思路"><a href="#思路" class="headerlink" title="思路:"></a>思路:</h4><p><strong>时间同步的方式：找一个机器，作为时间服务器，所有的机器与这台集群时间进行定时的同步，比如，每隔十分钟，同步一次时间。</strong> </p><h4 id="5-1-在cs1上修改ntp服务"><a href="#5-1-在cs1上修改ntp服务" class="headerlink" title="5.1 在cs1上修改ntp服务"></a>5.1 在<code>cs1</code>上修改<code>ntp</code>服务</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">1) 检查 ntp 是否安装</span><br><span class="line"><span class="comment">#root&gt; rpm -qa|grep ntp</span></span><br><span class="line">---</span><br><span class="line">ntp-4.2.6p5-10.el6.centos.x86_64</span><br><span class="line">fontpackages-filesystem-1.41-1.1.el6.noarch</span><br><span class="line">ntpdate-4.2.6p5-10.el6.centos.x86_64</span><br><span class="line"><span class="comment"># 出现上面的3个文件, 就是安装了; 如果没有的话, 用 yum 装一下</span></span><br><span class="line"><span class="comment"># root&gt; yum inatall -y ntp</span></span><br><span class="line"></span><br><span class="line">2) 修改 ntp 配置文件</span><br><span class="line"><span class="comment">#root&gt; vi /etc/ntp.conf</span></span><br><span class="line"></span><br><span class="line">修改内容如下</span><br><span class="line">    a）修改1（设置本地网络上的主机不受限制。）</span><br><span class="line">    <span class="comment">#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap</span></span><br><span class="line">    为</span><br><span class="line">    restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap</span><br><span class="line">    b）修改2（设置为不采用公共的服务器）</span><br><span class="line">    server 0.centos.pool.ntp.org iburst</span><br><span class="line">    server 1.centos.pool.ntp.org iburst</span><br><span class="line">    server 2.centos.pool.ntp.org iburst</span><br><span class="line">    server 3.centos.pool.ntp.org iburst</span><br><span class="line">    为</span><br><span class="line">    <span class="comment">#server 0.centos.pool.ntp.org iburst</span></span><br><span class="line">    <span class="comment">#server 1.centos.pool.ntp.org iburst</span></span><br><span class="line">    <span class="comment">#server 2.centos.pool.ntp.org iburst</span></span><br><span class="line">    <span class="comment">#server 3.centos.pool.ntp.org iburst</span></span><br><span class="line">    c）添加3（添加默认的一个内部时钟数据，使用它为局域网用户提供服务。）</span><br><span class="line">    server 127.127.1.0</span><br><span class="line">    fudge 127.127.1.0 stratum 10</span><br><span class="line"></span><br><span class="line">3）修改/etc/sysconfig/ntpd 文件</span><br><span class="line"><span class="comment">#root&gt; vim /etc/sysconfig/ntpd</span></span><br><span class="line">增加内容如下（让硬件时间与系统时间一起同步）</span><br><span class="line">SYNC_HWCLOCK=yes</span><br><span class="line"></span><br><span class="line">4）重新启动ntpd</span><br><span class="line"><span class="comment">#root&gt; service ntpd status</span></span><br><span class="line">ntpd 已停</span><br><span class="line"></span><br><span class="line"><span class="comment">#root&gt; service ntpd start</span></span><br><span class="line">正在启动 ntpd：    </span><br><span class="line"></span><br><span class="line">5) 执行开机启动 ntpd 服务</span><br><span class="line"><span class="comment">#root&gt; chkconfig ntpd on</span></span><br><span class="line">查看 ntpd 服务开机启动的状态</span><br><span class="line"><span class="comment">#root&gt; chkconfig --list ntpd</span></span><br></pre></td></tr></table></figure><h4 id="5-2-其它机器上"><a href="#5-2-其它机器上" class="headerlink" title="5.2 其它机器上"></a>5.2 其它机器上</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1）在其他所有机器配置10分钟与时间服务器同步一次</span><br><span class="line"><span class="comment">#编写定时任务脚本</span></span><br><span class="line"><span class="comment">#root&gt; crontab -e </span></span><br><span class="line">*/10 * * * * /usr/sbin/ntpdate hadoop102</span><br><span class="line"></span><br><span class="line">2) 修改任意机器时间</span><br><span class="line"><span class="comment">#root&gt; date -s "2017-9-11 11:11:11"</span></span><br><span class="line"></span><br><span class="line">3) 十分钟后查看机器时间是否与cs1 同步</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-此教程默认已经搭建好完全分布式&quot;&gt;&lt;a href=&quot;#1-此教程默认已经搭建好完全分布式&quot; class=&quot;headerlink&quot; title=&quot;1.此教程默认已经搭建好完全分布式&quot;&gt;&lt;/a&gt;1.此教程默认已经搭建好完全分布式&lt;/h3&gt;&lt;h3 id=&quot;2-Zoo
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="hadoop" scheme="https://airpoet.github.io/categories/Hadoop/hadoop/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>i-Hive-Practice-1 影评练习</title>
    <link href="https://airpoet.github.io/2018/06/19/Hadoop/3-Hive/i-Hive-Practice-1-%E5%BD%B1%E8%AF%84%E7%BB%83%E4%B9%A0/"/>
    <id>https://airpoet.github.io/2018/06/19/Hadoop/3-Hive/i-Hive-Practice-1-影评练习/</id>
    <published>2018-06-19T11:56:19.763Z</published>
    <updated>2018-06-19T15:33:56.958Z</updated>
    
    <content type="html"><![CDATA[<p><strong>现有如此三份数据：</strong><br><strong>1、users.dat    数据格式为：  2::M::56::16::70072</strong><br><strong>对应字段为：UserID BigInt, Gender String, Age Int, Occupation String, Zipcode String</strong><br><strong>对应字段中文解释：用户id，性别，年龄，职业，邮政编码</strong></p><p><strong>2、movies.dat        数据格式为： 2::Jumanji (1995)::Adventure|Children’s|Fantasy</strong><br><strong>对应字段为：MovieID BigInt, Title String, Genres String</strong><br><strong>对应字段中文解释：电影ID，电影名字，电影类型</strong></p><p><strong>3、ratings.dat        数据格式为：  1::1193::5::978300760</strong><br><strong>对应字段为：UserID BigInt, MovieID BigInt, Rating Double, Timestamped String</strong><br><strong>对应字段中文解释：用户ID，电影ID，评分，评分时间戳</strong></p><p>题目要求：</p><p>数据要求：<br>（1）写shell脚本清洗数据。（hive不支持解析多字节的分隔符，也就是说hive只能解析’:’, 不支持解析’::’，所以用普通方式建表来使用是行不通的，要求对数据做一次简单清洗）<br>（2）使用Hive能解析的方式进行</p><p>Hive要求：<br><strong>（1）正确建表，导入数据（三张表，三份数据），并验证是否正确</strong></p><p><strong>（2）求被评分次数最多的10部电影，并给出评分次数（电影名，评分次数）</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">思路:</span><br><span class="line">1. 分组</span><br><span class="line"><span class="keyword">select</span> movieid, <span class="keyword">count</span>(movieid) rateCount <span class="keyword">from</span> ratings </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> movieid <span class="keyword">limit</span> <span class="number">10</span>;</span><br><span class="line">2. 排序</span><br><span class="line"><span class="keyword">select</span> movieid, <span class="keyword">count</span>(movieid) rateCount <span class="keyword">from</span> ratings </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> movieid <span class="keyword">order</span> <span class="keyword">by</span> rateCount <span class="keyword">desc</span> <span class="keyword">limit</span> <span class="number">10</span>;</span><br><span class="line">3.join</span><br><span class="line"><span class="keyword">select</span> a.movieid mvid, b.Title mvtitle, a.rateCount mvratecount <span class="keyword">from</span> </span><br><span class="line">(<span class="keyword">select</span> movieid, <span class="keyword">count</span>(movieid) rateCount <span class="keyword">from</span> ratings </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> movieid <span class="keyword">order</span> <span class="keyword">by</span> rateCount <span class="keyword">desc</span> <span class="keyword">limit</span> <span class="number">10</span>) a</span><br><span class="line"><span class="keyword">join</span> movies b <span class="keyword">where</span> a.movieid=b.movieid;</span><br><span class="line">==============================</span><br><span class="line"></span><br><span class="line"> 完整的</span><br><span class="line"><span class="comment">------------</span></span><br><span class="line"><span class="keyword">select</span> a.movieid mvid, b.Title mvtitle, a.rateCount mvratecount <span class="keyword">from</span> </span><br><span class="line">(<span class="keyword">select</span> movieid, <span class="keyword">count</span>(movieid) rateCount <span class="keyword">from</span> ratings </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> movieid <span class="keyword">order</span> <span class="keyword">by</span> rateCount <span class="keyword">desc</span> <span class="keyword">limit</span> <span class="number">10</span>) a</span><br><span class="line"><span class="keyword">join</span> movies b <span class="keyword">where</span> a.movieid=b.movieid;</span><br><span class="line"></span><br><span class="line"><span class="comment">--结果---</span></span><br><span class="line">+<span class="comment">-------+----------------------------------------------------+--------------+</span></span><br><span class="line">| mvid  |                      mvtitle                       | mvratecount  |</span><br><span class="line">+<span class="comment">-------+----------------------------------------------------+--------------+</span></span><br><span class="line">| 2858  | American Beauty (1999)                             | 3428         |</span><br><span class="line">| 260   | Star Wars: Episode IV - A New Hope (1977)          | 2991         |</span><br><span class="line">| 1196  | Star Wars: Episode V - The Empire Strikes Back (1980) | 2990         |</span><br><span class="line">| 1210  | Star Wars: Episode VI - Return of the Jedi (1983)  | 2883         |</span><br><span class="line">| 480   | Jurassic Park (1993)                               | 2672         |</span><br><span class="line">| 2028  | Saving Private Ryan (1998)                         | 2653         |</span><br><span class="line">| 589   | Terminator 2: Judgment Day (1991)                  | 2649         |</span><br><span class="line">| 2571  | Matrix, The (1999)                                 | 2590         |</span><br><span class="line">| 1270  | Back to the Future (1985)                          | 2583         |</span><br><span class="line">| 593   | Silence of the Lambs, The (1991)                   | 2578         |</span><br><span class="line">+<span class="comment">-------+----------------------------------------------------+--------------+</span></span><br></pre></td></tr></table></figure><p><strong>（3）分别求男性，女性当中评分最高的10部电影（性别，电影名，影评分）</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># 注意,这里的 avg(r.rating) 是因为group by了 gender,title, 也就是</span><br><span class="line"># 说有 gender,title 分组, 此时可能对应此组的有多个值,</span><br><span class="line"># 这里就是 相同性别评价相同电影的, 评分有多个, 此时就要使用聚合函数,得出唯一值</span><br><span class="line"># 因此就使用了 avg(r.rating)</span><br><span class="line"><span class="keyword">select</span> u.gender, m.title, <span class="keyword">avg</span>(r.rating) rr</span><br><span class="line"><span class="keyword">from</span> ratings r </span><br><span class="line"><span class="keyword">join</span> <span class="keyword">users</span> u <span class="keyword">on</span> r.userid=u.userid </span><br><span class="line"><span class="keyword">join</span> movies m <span class="keyword">on</span> r.movieid=m.movieid</span><br><span class="line"><span class="keyword">where</span> u.gender = <span class="string">'M'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> gender,title</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> rr <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line">+<span class="comment">-----------+--------------------------------------------+------+</span></span><br><span class="line">| u.gender  |                  m.title                   |  rr  |</span><br><span class="line">+<span class="comment">-----------+--------------------------------------------+------+</span></span><br><span class="line">| M         | Schlafes Bruder (Brother of Sleep) (1995)  | 5.0  |</span><br><span class="line">| M         | Small Wonders (1996)                       | 5.0  |</span><br><span class="line">| M         | Lured (1947)                               | 5.0  |</span><br><span class="line">| M         | Bells, The (1926)                          | 5.0  |</span><br><span class="line">| M         | Dangerous Game (1993)                      | 5.0  |</span><br><span class="line">| M         | Baby, The (1973)                           | 5.0  |</span><br><span class="line">| M         | Gate of Heavenly Peace, The (1995)         | 5.0  |</span><br><span class="line">| M         | Follow the Bitch (1998)                    | 5.0  |</span><br><span class="line">| M         | Ulysses (Ulisse) (1954)                    | 5.0  |</span><br><span class="line">| M         | Angela (1995)                              | 5.0  |</span><br><span class="line">+<span class="comment">-----------+--------------------------------------------+------+</span></span><br></pre></td></tr></table></figure><p><strong>（4）求movieid = 2116这部电影各年龄段（因为年龄就只有7个，就按这个7个分就好了）的平均影评（年龄段，影评分）</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 注意: 没有歧义的字段, 可以不用指明是谁的属性</span><br><span class="line"><span class="keyword">select</span> age, <span class="keyword">avg</span>(r.rating) avgrating <span class="keyword">from</span> ratings r </span><br><span class="line"><span class="keyword">join</span> <span class="keyword">users</span> u <span class="keyword">on</span> r.userid=u.userid</span><br><span class="line"><span class="keyword">where</span> movieid=<span class="number">2116</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> age;</span><br></pre></td></tr></table></figure><p><strong>（5）求最喜欢看电影（影评次数最多）的那位女性评最高分的10部电影的平均影评分（观影者，电影名，影评分）</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"># 1.先拿到 影评次数最多的女性(id)</span><br><span class="line">0: jdbc:hive2://cs2:10000&gt; select UserID, count(UserID) count from ratings</span><br><span class="line">. . . . . . . . . . . . .&gt; group by UserID</span><br><span class="line">. . . . . . . . . . . . .&gt; order by count desc</span><br><span class="line">. . . . . . . . . . . . .&gt; limit 1;</span><br><span class="line">+<span class="comment">---------+--------+</span></span><br><span class="line">| userid  | count  |</span><br><span class="line">+<span class="comment">---------+--------+</span></span><br><span class="line">| 4169    | 2314   |</span><br><span class="line">+<span class="comment">---------+--------+</span></span><br><span class="line"></span><br><span class="line"># 2.拿到此人评分最高的10部电影, 此人 userid, 电影 id, 此人评分</span><br><span class="line">##  一定要去重</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">distinct</span>(MovieID) MovieID, Rating <span class="keyword">from</span> ratings <span class="keyword">where</span> UserID=<span class="number">4169</span> </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> Rating <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line">+<span class="comment">----------+---------+</span></span><br><span class="line">| movieid  | rating  |</span><br><span class="line">+<span class="comment">----------+---------+</span></span><br><span class="line">| 78       | 5.0     |</span><br><span class="line">| 73       | 5.0     |</span><br><span class="line">| 72       | 5.0     |</span><br><span class="line">| 58       | 5.0     |</span><br><span class="line">| 55       | 5.0     |</span><br><span class="line">| 50       | 5.0     |</span><br><span class="line">| 41       | 5.0     |</span><br><span class="line">| 36       | 5.0     |</span><br><span class="line">| 25       | 5.0     |</span><br><span class="line">| 17       | 5.0     |</span><br><span class="line">+<span class="comment">----------+---------+</span></span><br><span class="line"></span><br><span class="line">3.这10部电影的(观影者，电影名，平均影评分)</span><br><span class="line">观影者还没写</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> t.MovieID, m.Title, <span class="keyword">avg</span>(r.rating) avgrating <span class="keyword">from</span> topten t </span><br><span class="line"><span class="keyword">join</span> ratings r <span class="keyword">on</span> t.MovieID=r.MovieID</span><br><span class="line"><span class="keyword">join</span> movies m <span class="keyword">on</span> t.MovieID=m.MovieID</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> t.MovieID,m.Title;</span><br><span class="line"></span><br><span class="line">+<span class="comment">------------+-----------------------------------------------+---------------------+</span></span><br><span class="line">| t.movieid  |                    m.title                    |      avgrating      |</span><br><span class="line">+<span class="comment">------------+-----------------------------------------------+---------------------+</span></span><br><span class="line">| 3849       | Spiral Staircase, The (1946)                  | 4.046511627906977   |</span><br><span class="line">| 3870       | Our Town (1940)                               | 3.857142857142857   |</span><br><span class="line">| 3871       | Shane (1953)                                  | 3.839344262295082   |</span><br><span class="line">| 3893       | Nurse Betty (2000)                            | 3.5026833631484795  |</span><br><span class="line">| 3897       | Almost Famous (2000)                          | 4.22635814889336    |</span><br><span class="line">| 3910       | Dancer in the Dark (2000)                     | 3.82                |</span><br><span class="line">| 3927       | Fantastic Voyage (1966)                       | 3.5804597701149423  |</span><br><span class="line">| 3928       | Abbott and Costello Meet Frankenstein (1948)  | 3.441747572815534   |</span><br><span class="line">| 3929       | Bank Dick, The (1940)                         | 3.993197278911565   |</span><br><span class="line">| 3932       | Invisible Man, The (1933)                     | 3.75                |</span><br><span class="line">+<span class="comment">------------+-----------------------------------------------+---------------------+</span></span><br></pre></td></tr></table></figure><p>（6）求好片（评分&gt;=4.0）最多的那个年份的最好看的10部电影<br>（7）求1997年上映的电影中，评分最高的10部Comedy类电影<br>（8）该影评库中各种类型电影中评价最高的5部电影（类型，电影名，平均影评分）<br>（9）各年评分最高的电影类型（年份，类型，影评分）<br>（10）每个地区最高评分的电影名，把结果存入HDFS（地区，电影名，影评分）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;现有如此三份数据：&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;1、users.dat    数据格式为：  2::M::56::16::70072&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;对应字段为：UserID BigInt, Gender String, 
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/categories/Hadoop/Hive/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>IDEA的简单使用</title>
    <link href="https://airpoet.github.io/2018/06/18/Tools/IDEA/IDEA%E7%9A%84%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/"/>
    <id>https://airpoet.github.io/2018/06/18/Tools/IDEA/IDEA的简单使用/</id>
    <published>2018-06-18T11:14:22.768Z</published>
    <updated>2018-06-18T13:44:30.402Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="工具" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="IDEA" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/IDEA/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="工具" scheme="https://airpoet.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="IDEA" scheme="https://airpoet.github.io/tags/IDEA/"/>
    
  </entry>
  
  <entry>
    <title>Hive学习—2</title>
    <link href="https://airpoet.github.io/2018/06/14/Hadoop/3-Hive/Hive%E5%AD%A6%E4%B9%A0-2/"/>
    <id>https://airpoet.github.io/2018/06/14/Hadoop/3-Hive/Hive学习-2/</id>
    <published>2018-06-14T01:04:30.334Z</published>
    <updated>2018-06-14T15:21:20.295Z</updated>
    
    <content type="html"><![CDATA[<h1 id="hive数据类型"><a href="#hive数据类型" class="headerlink" title="hive数据类型"></a>hive数据类型</h1><h3 id="1-原子数据类型"><a href="#1-原子数据类型" class="headerlink" title="1) 原子数据类型"></a>1) 原子数据类型</h3><ul><li>TinyInt：1byte有符号整数</li><li>SmallInt：2byte有符号整数</li><li>Int：4byte有符号整数</li><li>BigInt：8byte有符号整数</li><li>Float：单精度浮点数</li><li>Double：双精度浮点数</li><li>Boolean：布尔类型</li><li>String：字符串</li><li>TimeStamp：整数</li></ul><h3 id="2-复杂数据类型"><a href="#2-复杂数据类型" class="headerlink" title="2) 复杂数据类型"></a>2) 复杂数据类型</h3><ul><li><p>Array\&lt;Type></p><ul><li>由一系列相同数据类型的元素组成</li><li>这些元素可以通过 下标 来访问</li><li>查询时如果查到返回响应值，没查到则返回null<br>建表：<br>create table person(name string,work_locations array\&lt;string>)<br>row format delimited fields terminated by ‘\t’<br>collection items terminated by ‘,’;<br>导入数据：<br>load data local inpath ‘/home/sigeon/person.txt’ into table person;<br>查询<br>Select work_locations[0] from person;</li></ul></li><li><p>Map&lt;KType, VType&gt;</p><ul><li>包含 key-value 键值对</li><li>可以通过 key 来访问元素<br>建表语句：<br>create table score(name string, scores map)<br>row format delimited fields terminated by ‘\t’<br>collection items terminated by ‘,’<br>map keys terminated by ‘:’;<br>导入数据：<br>load data local inpath ‘/home/sigeon/score.txt’ into table score;<br>查询语句：<br>Select scores[‘Chinese’] from score;</li></ul></li><li><p>Struct&lt;Param1:Type1, Param1:Type1, … &gt;</p><ul><li>可以包含不同数据类型的元素<br>类似于c语言中的结构体</li><li>这些元素可以通过 点语法 的方式来得到<br>建表语句：<br>create table course(id int,course struct&lt;name:string, score:int&gt;)<br>row format delimited fields terminated by ‘\t’<br>collection items terminated by ‘,’;<br>导入数据：<br>load data local inpath ‘/ home/sigoen/course.txt’ into table course;<br>查询语句：<br>Select c.course.score from course c;</li></ul></li><li><p>几个分隔符</p></li></ul><p>  指定分隔符要按从外向内的顺序，字段 -&gt; 集合元素 -&gt;map k-v</p><ul><li>ROW FORMAT：指定分隔符的关键字 </li><li>DELIMITED FIELDS TERMINATED BY：字段分隔符 </li><li>COLLECTION ITEMS TERMINATED BY：集合元素分隔符（Array 中的各元素、Struct 中的各元素、 Map 中的 key-value 对之间） </li><li>MAP KEYS TERMINATED BY：Map 中 key 与 value 的分隔符 </li><li>LINES TERMINATED BY：行之间的分隔符</li></ul><h1 id="hive视图"><a href="#hive视图" class="headerlink" title="hive视图"></a>hive视图</h1><ul><li><p>和关系型数据库一样，Hive也提供了视图的功能</p></li><li><p>Hive 的视图和关系型数据库的视图有很大的区别： </p><ul><li>1、只有逻辑视图，没有物化视图； </li><li>2、视图只能查询，不能增删改 (Load|Insert/Update/Delete) 数据； </li><li>3、视图在创建时候，只是保存了一份元数据 (存在TBLS中)，当查询视图的时候，才开始执行视图对应的那些子查询<br>视图元数据只存储了hql语句，而不是执行结果</li></ul></li><li><p>视图操作</p><ul><li><p>创建视图</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">view</span> my_view <span class="keyword">as</span> &lt;<span class="keyword">select</span> * <span class="keyword">from</span> mytable&gt; [<span class="keyword">limit</span> <span class="number">500</span>];</span><br></pre></td></tr></table></figure></li><li><p>查看视图</p><ul><li>show tables;   // 显示所有表和视图</li><li>show views;    //显示所有视图</li><li>desc [formatted] view_name;   // 查看某个具体视图的(详细)信息<br>视图类型：VIRTUAL_VIEW</li></ul></li><li><p>删除视图</p><ul><li>drop view [if exists] view_name</li></ul></li><li><p>使用视图 </p><ul><li>select count(distinct uid) from my_view;</li></ul></li></ul></li></ul><h1 id="hive函数"><a href="#hive函数" class="headerlink" title="hive函数"></a>hive函数</h1><ul><li><p>函数分类</p><ul><li>UDF（自定义函数 User-Defined Function）作用于单个数据行，产生一个数据行作为输出。（数学函数，字 符串函数） </li><li>UDAF（用户定义聚集函数 User-Defined Aggregation Funcation）：接收多个输入数据行，并产 生一个输出数据行。（count，max） </li><li>UDTF（表格生成函数 User-Defined Table Function）：接收一行输入，输出多行（explode）</li></ul></li><li><p>内置函数</p><ul><li><p>查看函数命令</p><ul><li>查看内置函数： show functions; </li><li>显示函数的详细信息： desc function [extended] fun_name;<br>extended：显示扩展信息</li></ul></li><li><p>分类</p><ul><li><p>关系运算</p><ul><li>分类<br>\1. 等值比较: =<br>\2. 等值比较:&lt;=&gt;<br>\3. 不等值比较: &lt;&gt;和!=<br>\4. 小于比较: &lt;<br>\5. 小于等于比较: &lt;=<br>\6. 大于比较: &gt;<br>\7. 大于等于比较: &gt;=<br>\8. 区间比较<br>\9. 空值判断: IS NULL<br>\10. 非空判断: IS NOT NULL<br>\10. LIKE 比较: LIKE<br>\11. JAVA 的 LIKE 操作: RLIKE<br>\12. REGEXP 操作: REGEXP</li></ul></li><li><p>数学运算</p><ul><li>分类<br>\1. 加法操作: +<br>\2. 减法操作: –<br>\3. 乘法操作: *<br>\4. 除法操作: /<br>\5. 取余操作: %<br>\6. 位与操作: &amp;<br>\7. 位或操作: |<br>\8. 位异或操作: ^<br>9．位取反操作: ~</li></ul></li><li><p>逻辑运算</p><ul><li>分类<br>\1. 逻辑与操作: AND 、&amp;&amp;<br>\2. 逻辑或操作: OR 、||<br>\3. 逻辑非操作: NOT、!</li></ul></li><li><p>复合类型构造函数 </p><ul><li>分类<ol><li>array 结构</li><li>map 结构<br>\3. struct 结构<br>\4. named_struct 结构<br>\5. create_union </li></ol></li></ul></li><li><p>复合类型操作符</p><ul><li>\1. 获取 array 中的元素 </li><li>\2. 获取 map 中的元素 </li><li>\3. 获取 struct 中的元素</li></ul></li><li><p>集合操作函数 </p><ul><li><p>\1. map 类型大小：size </p></li><li><p>\2. array 类型大小：size </p></li><li><p>\3. 判断元素数组是否包含元素：array_contains </p></li><li><p>\4. 获取 map 中所有 value 集合 </p></li><li><p>\5. 获取 map 中所有 key 集合 </p></li><li><p>\6. 数组排序</p></li><li><p>\7. 获取素组或map集合的单个元素（k-v对）：<strong>explode()</strong></p><p><strong>当同时查询炸裂字段和普通字段时，需要使用横向虚拟视图：lateral view</strong></p><ul><li>如：select name,addr.city from usr_addr lateral view(address) addr as city;<br>这里一般需要给 查询结果 和 结果字段 起别名，不然没办法获得每个字段，如果只是查询所有的话就不需要了</li></ul></li></ul></li><li><p>类型转换函数</p><ul><li>\1. 二进制转换：binary </li><li>\2. 基础类型之间强制转换：cast </li></ul></li><li><p>数值计算函数</p><ul><li>\1. 取整函数: round </li><li>\2. 指定精度取整函数: round </li><li>\3. 向下取整函数: floor </li><li>\4. 向上取整函数: ceil </li><li>\5. 向上取整函数: ceiling </li><li>\6. 取随机数函数: rand </li><li>\7. 自然指数函数: exp </li><li>\8. 以 10 为底对数函数: log10 </li><li>\9. 以 2 为底对数函数: log2 </li><li>\10. 对数函数: log </li><li>\11. 幂运算函数: pow </li><li>\12. 幂运算函数: power </li><li>\13. 开平方函数: sqrt </li><li>\14. 二进制函数: bin </li><li>\15. 十六进制函数: hex </li><li>\16. 反转十六进制函数: unhex </li><li>\17. 进制转换函数: conv </li><li>\18. 绝对值函数: abs </li><li>\19. 正取余函数: pmod </li><li>\20. 正弦函数: sin </li><li>\21. 反正弦函数: asin </li><li>\22. 余弦函数: cos </li><li>\23. 反余弦函数: acos </li><li>\24. positive 函数: positive </li><li>\25. negative 函数: negative </li></ul></li><li><p>字符串函数</p><ul><li>\1. 字符 ascii 码函数：ascii </li><li>\2. base64 字符串 </li><li>\3. 字符串连接函数：concat </li><li>\4. 带分隔符字符串连接函数：concat_ws </li><li>\5. 数组转换成字符串的函数：concat_ws </li><li>\6. 小数位格式化成字符串函数：format_number </li><li>\7. 字符串截取函数：substr, substring<br>序号从1开始，可以传负数，代表从右开始</li><li>\9. 字符串查找函数：instr<br>找到返回一个正整数，未找到返回0</li><li>\10. 字符串长度函数：length </li><li>\11. 字符串查找函数：locate </li><li>\12. 字符串格式化函数：printf </li><li>\13. 字符串转换成 map 函数：str_to_map </li><li>\14. base64 解码函数：unbase64(string str) </li><li>\15. 字符串转大写函数：upper,ucase </li><li>\16. 字符串转小写函数：lower,lcase </li><li>\17. 去空格函数：trim </li><li>\18. 左边去空格函数：ltrim </li><li>\19. 右边去空格函数：rtrim </li><li>\20. 正则表达式替换函数：regexp_replace </li><li>\21. 正则表达式解析函数：regexp_extract </li><li>\22. URL 解析函数：parse_url </li><li>\23. json 解析函数：get_json_object </li><li>\24. 空格字符串函数：space </li><li>\25. 重复字符串函数：repeat </li><li>\26. 左补足函数：lpad</li><li>\27. 右补足函数：rpad </li><li>\28. 分割字符串函数: split </li><li>\29. 集合查找函数: find_in_set </li><li>\30. 分词函数：sentences </li><li>\31. 分词后统计一起出现频次最高的 TOP-K </li><li>\32. 分词后统计与指定单词一起出现频次最高的 TOP-K </li></ul></li><li><p>日期函数</p><ul><li>\1. UNIX 时间戳转日期函数: from_unixtime </li><li>\2. 获取当前 UNIX 时间戳函数: unix_timestamp </li><li>\3. 日期转 UNIX 时间戳函数: unix_timestamp </li><li>\4. 指定格式日期转 UNIX 时间戳函数: unix_timestamp </li><li>\5. 日期时间转日期函数: to_date </li><li>\6. 日期转年函数: year </li><li>\7. 日期转月函数: month </li><li>\8. 日期转天函数: day </li><li>\9. 日期转小时函数: hour </li><li>\10. 日期转分钟函数: minute </li><li>\11. 日期转秒函数: second </li><li>\12. 日期转周函数: weekofyear </li><li>\13. 日期比较函数: datediff </li><li>\14. 日期增加函数: date_add </li><li>\15. 日期减少函数: date_sub </li></ul></li><li><p>条件函数</p><ul><li>\1. If 函数: if( 条件 ，true返回参数，false返回参数 ) </li><li>\2. 当param1不为null返回param1，否则返回param2：nvl(param1, param2)</li><li>\2. 非空查找函数: coalesce</li><li>\3. 条件判断函数：case</li></ul></li><li><p>混合函数 </p><ul><li>分类<br>\1. 调用 Java 函数：java_method<br>\2. 调用 Java 函数：reflect<br>\3. 字符串的 hash 值：hash</li></ul></li><li><p>XPath 解析 XML 函数 </p><ul><li>分类<br>\1. xpath<br>\2. xpath_string<br>\3. xpath_boolean<br>\4. xpath_short, xpath_int, xpath_long<br>\5. xpath_float, xpath_double, xpath_number </li></ul></li><li><p>汇总统计函数（UDAF）</p><ul><li>\1. 个数统计函数: count </li><li>\2. 总和统计函数: sum </li><li>\3. 平均值统计函数: avg </li><li>\4. 最小值统计函数: min </li><li>\5. 最大值统计函数: max</li><li>\6. 非空集合总体变量函数: var_pop </li><li>\7. 非空集合样本变量函数: var_samp </li><li>\8. 总体标准偏离函数: stddev_pop </li><li>\9. 样本标准偏离函数: stddev_samp </li><li>10．中位数函数: percentile </li><li>\11. 中位数函数: percentile </li><li>\12. 近似中位数函数: percentile_approx </li><li>\13. 近似中位数函数: percentile_approx </li><li>\14. 直方图: histogram_numeric </li><li>\15. 集合去重数：collect_set </li><li>\16. 集合不去重函数：collect_list </li></ul></li><li><p>表格生成函数 Table-Generating Functions (UDTF) </p><ul><li>分类<br>1．数组拆分成多行：explode(array)<br>2．Map 拆分成多行：explode(map) </li></ul></li></ul></li></ul></li><li><p>自定义函数</p><ul><li>\1. 需要继承 org.apache.hadoop.hive.ql.exec.UDF 类，实现一个或多个 evaluate() 方法</li><li>\2. 将hive的jar包放在hive的classpath路径下，进入hive客户端，执行命令：add jar \<jar path="">;</jar></li><li>\3. 检查jar包是否添加成功，执行命令：list jars;</li><li>\4. 给自定义函数 添加别名，并在hive中 注册 该函数，执行命令：create temporary function myfunc as ‘主类全路径名’;<br>myfunc：自定义函数别名<br>该方法创建的是临时函数，当前客户端关闭就没有了，下次重复第3，4步；<br>真实 生产中就是使用该方法！</li><li>\5. 查看hive函数库有没有成功添加，执行命令：show functions;</li><li>\6. 调用函数时通过函数名和参数列表确定调用的是具体哪一个方法</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;hive数据类型&quot;&gt;&lt;a href=&quot;#hive数据类型&quot; class=&quot;headerlink&quot; title=&quot;hive数据类型&quot;&gt;&lt;/a&gt;hive数据类型&lt;/h1&gt;&lt;h3 id=&quot;1-原子数据类型&quot;&gt;&lt;a href=&quot;#1-原子数据类型&quot; class=&quot;head
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/categories/Hadoop/Hive/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>i-Hive-1</title>
    <link href="https://airpoet.github.io/2018/06/13/Hadoop/3-Hive/i-Hive-1/"/>
    <id>https://airpoet.github.io/2018/06/13/Hadoop/3-Hive/i-Hive-1/</id>
    <published>2018-06-13T14:04:40.446Z</published>
    <updated>2018-06-22T13:29:40.732Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Hive-初探"><a href="#1-Hive-初探" class="headerlink" title="1.  Hive 初探"></a>1.  Hive 初探</h1><h3 id="1-1-Hive-的数据存储"><a href="#1-1-Hive-的数据存储" class="headerlink" title="1.1 Hive 的数据存储"></a>1.1 Hive 的数据存储</h3><ul><li>Hive的数据存储基于Hadoop HDFS</li><li>Hive没有专门的数据存储格式</li><li>存储结构主要包括：数据库、文件、表、视图、索引</li><li>Hive默认可以直接加载文本文件（TextFile），还支持SequenceFile、RCFile </li><li>创建表时，指定Hive数据的列分隔符与行分隔符，Hive即可解析数据</li></ul><h3 id="1-2-Hive的系统架构"><a href="#1-2-Hive的系统架构" class="headerlink" title="1.2 Hive的系统架构"></a>1.2 Hive的系统架构</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-13-152054.jpg" alt=""></p><ul><li>用户接口，包括 CLI，JDBC/ODBC，WebUI</li><li>元数据存储，通常是存储在关系数据库如 mysql, derby 中</li><li>解释器、编译器、优化器、执行器</li><li>Hadoop：用 HDFS 进行存储，利用 MapReduce 进行计算</li></ul><h3 id="1-3-Hive的系统架构"><a href="#1-3-Hive的系统架构" class="headerlink" title="1.3 Hive的系统架构"></a>1.3 Hive的系统架构</h3><ul><li>用户接口主要有三个：CLI，JDBC/ODBC和 WebUI<ul><li>CLI，即Shell命令行</li><li>JDBC/ODBC 是 Hive 的Java，与使用传统数据库JDBC的方式类似</li><li>WebGUI是通过浏览器访问 Hive</li></ul></li><li>Hive 将元数据存储在数据库中(metastore)，目前只支持 mysql、derby。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等</li><li>解释器、编译器、优化器完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划（plan）的生成。生成的查询计划存储在 HDFS 中，并在随后由 MapReduce 调用执行</li><li>Hive 的数据存储在 HDFS 中，大部分的查询由 MapReduce 完成（包含 <em> 的查询，比如 select </em> from table 不会生成 MapRedcue 任务</li></ul><h3 id="1-4-Hive的metastore"><a href="#1-4-Hive的metastore" class="headerlink" title="1.4 Hive的metastore"></a>1.4 Hive的metastore</h3><ul><li>metastore是hive元数据的集中存放地。</li><li>metastore默认使用内嵌的derby数据库作为存储引擎</li><li>Derby引擎的缺点：一次只能打开一个会话</li><li>使用Mysql作为外置存储引擎，多用户同时访问 </li></ul><h3 id="1-5-Hive-和-Hadoop-的调用关系"><a href="#1-5-Hive-和-Hadoop-的调用关系" class="headerlink" title="1.5 Hive 和 Hadoop 的调用关系"></a>1.5 Hive 和 Hadoop 的调用关系</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-13-140311.jpg" alt=""></p><blockquote><p>1、提交sql  交给驱动<br>2、驱动编译    解析相关的字段表信息<br>3、去metastore查询相关的信息    返回字段表信息<br>4、编译返回信息 发给驱动<br>5、驱动发送一个执行计划    交给执行引擎<br>6.1、DDLs <strong>对数据库表的操作的, 直接和metastore交互</strong>,<code>create table t1(name string);</code></p><p>6.2、完成job返回数据信息、找<strong>namenode</strong>查数据<br>6.3、<strong>namenode</strong>交互<code>select count(1) from t1;</code><br>7、返回结果信息集</p></blockquote><h3 id="1-6-Hive-参数配置使用"><a href="#1-6-Hive-参数配置使用" class="headerlink" title="1.6 Hive 参数配置使用"></a>1.6 Hive 参数配置使用</h3><table><thead><tr><th>命名空间</th><th>使用权限</th><th>描述</th></tr></thead><tbody><tr><td>hivevar</td><td>可读写</td><td>$   hive -d name=zhangsan;</td></tr><tr><td>hiveconf</td><td>可读写</td><td>\$   hive –hiveconf hive.cli.print.current.db=true;   $   hive –hiveconf hive.cli.print.header=true;</td></tr><tr><td>system</td><td>可读写</td><td>java定义的配置属性，如system:user.name</td></tr><tr><td>env</td><td>只读</td><td>shell环境变量，如env:USER</td></tr></tbody></table><ul><li><p>hivevar </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用场景: 起别名</span></span><br><span class="line">hive -d name=zhangsan  <span class="comment">#传参</span></span><br><span class="line">&gt; create table t2(name string,<span class="variable">$&#123;name&#125;</span> string); <span class="comment">#取参数</span></span><br><span class="line">&gt; desc t2;</span><br><span class="line">---</span><br><span class="line">name                string</span><br><span class="line">zhangsan            string</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>hiveconf :</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  显示当前数据库名称</span></span><br><span class="line">[ap@cs2]~% hive --hiveconf hive.cli.print.current.db=<span class="literal">true</span>;</span><br><span class="line">hive (default)&gt; create database mydb;</span><br><span class="line">hive (default)&gt; use mydb;</span><br><span class="line">hive (mydb)&gt; </span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示表头(字段名)</span></span><br><span class="line">hive --hiveconf hive.cli.print.header=<span class="literal">true</span>;</span><br><span class="line">select * from t2;</span><br><span class="line">t2.namet2.zhangsan</span><br></pre></td></tr></table></figure></li></ul><h3 id="1-7-Hive-的脚本执行"><a href="#1-7-Hive-的脚本执行" class="headerlink" title="1.7 Hive 的脚本执行"></a>1.7 Hive 的脚本执行</h3><ul><li><strong>Hive -e “xx ”</strong><ul><li>e 就是 edit, 在终端打印输出</li></ul></li><li><strong>Hive -e “show tables” &gt;&gt; a.txt</strong><ul><li>可以把执行结果重定向到文件中</li></ul></li><li><strong>Hive -S -e “show tables” &gt;&gt; a.txt</strong><ul><li>-S : silence 安静的执行</li></ul></li><li><strong>hive -f file</strong> <ul><li>hive -f hql ,  hql 是文件, 执行文件</li><li>执行完了之后,  就离开 hive 命令行</li></ul></li><li><strong>hive -i /home/ap/hive-init.sql</strong> <ul><li>执行完了,还在控制台, 可以继续操作</li></ul></li><li><strong>hive&gt;source file</strong><ul><li>source + 文件名  : 直接执行当前目录文件</li><li>source /home/ap/xx.sql;</li></ul></li></ul><h3 id="1-8-hive与依赖环境的交互"><a href="#1-8-hive与依赖环境的交互" class="headerlink" title="1.8 hive与依赖环境的交互"></a>1.8 hive与依赖环境的交互</h3><ul><li><strong>与linux交互命令 ！</strong><ul><li><code>!ls</code></li><li><code>!pwd</code></li></ul></li><li><strong>与hdfs交互命令</strong><ul><li><code>dfs -ls /</code></li><li><code>dfs -mkdir /hive</code></li><li><code>hive (default)&gt; dfs -rm -r /user/hive/warehouse/t5;</code></li></ul></li><li><strong>beeline 与 linux &amp; hdfs 交互</strong><ul><li>!help 查看帮助</li></ul></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-043653.png" alt="image-20180619123652339"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-043558.png" alt="image-20180619123557944"></p><h3 id="1-9-Hive-的-JDBC-模式"><a href="#1-9-Hive-的-JDBC-模式" class="headerlink" title="1.9 Hive 的 JDBC 模式"></a>1.9 Hive 的 JDBC 模式</h3><ul><li><p>JAVA API交互执行方式 </p></li><li><p>hive 远程服务 (端口号1000  0) 启动方式</p><ul><li><code>hive --service hiveserver2</code></li><li><code>org.apache.hive.jdbc.HiveDriver</code></li></ul></li><li><p>在java代码中调用hive的JDBC建立连接</p></li><li><p><strong>用 beeline 连接</strong></p><ul><li><p><strong>方式1: 直接登录</strong></p><ul><li>注意: 这里的<strong>cs2是指的数据库所在的服务器</strong>, 如果mysql 安装在 cs2上, 那么不管在哪台机器上登录beeline , 都这样输入就行了</li></ul><p><code>beeline -u jdbc:hive2://cs2:10000 -n ap</code></p></li><li><p><strong>方式2: 输入用户名密码登录</strong></p><p><code>!connect jdbc:hive2://cs2:10000</code></p></li></ul></li><li><p><strong>beeline注意点:</strong> </p><ul><li>使用 beeline 连接时, 貌似无法与 Linux 目录交互</li><li>当前目录在<code>/home/ap/apps/apache-hive-2.3.2-bin/bin/</code>下</li><li><strong>要传文件的话, 要使用全路径</strong></li></ul></li></ul><h3 id="1-10-SET命令使用"><a href="#1-10-SET命令使用" class="headerlink" title="1.10 SET命令使用"></a>1.10 SET命令使用</h3><ul><li>Hive 控制台set 命令<ul><li>set;    set -v;  显示所有的环境变量</li><li><code>set hive.cli.print.current.db=true;</code></li><li><code>set hive.cli.print.header=true;</code> </li><li><code>set hive.metastore.warehouse.dir=/hive;</code></li></ul></li><li><strong>hive参数初始化配置set命令:</strong><ul><li><strong>~/.hiverc</strong><ul><li>创建此文件, 在此文件中配置初始化命令</li></ul></li><li>补充：<br><strong>hive历史操作命令集</strong><br><strong>~/.hivehistory</strong></li></ul></li></ul><p><br></p><h1 id="2-Hive数据类型"><a href="#2-Hive数据类型" class="headerlink" title="2.  Hive数据类型"></a>2.  Hive数据类型</h1><h3 id="2-1-基本数据类型"><a href="#2-1-基本数据类型" class="headerlink" title="2.1 基本数据类型"></a>2.1 基本数据类型</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-13-163316.jpg" alt=""></p><h3 id="2-2-复合数据类型"><a href="#2-2-复合数据类型" class="headerlink" title="2.2 复合数据类型"></a>2.2 复合数据类型</h3><blockquote><p>创建学生表</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> student(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">INT</span>,</span><br><span class="line">    <span class="keyword">name</span> <span class="keyword">STRING</span>,</span><br><span class="line">    favors <span class="built_in">ARRAY</span>\&lt;<span class="keyword">STRING</span>&gt;,</span><br><span class="line">    scores <span class="keyword">MAP</span>&lt;<span class="keyword">STRING</span>, <span class="built_in">FLOAT</span>&gt;</span><br><span class="line">);</span><br></pre></td></tr></table></figure><table><thead><tr><th>默认分隔符</th><th>描述</th><th>语句</th></tr></thead><tbody><tr><td>\n</td><td>分隔行</td><td>LINES   TERMINATED BY ‘\t’</td></tr><tr><td>^A</td><td>分隔字段(列)，显示编码使用\001</td><td>FIELDS   TERMINATED BY ‘\001’</td></tr><tr><td>^B</td><td>分隔复合类型中的元素，显示编码使用\002</td><td>COLLECTION   ITEMS TERMINATED BY ‘\002’</td></tr><tr><td>^C</td><td>分隔map元素的key和value，显示编码使用\003</td><td>MAP   KEYS TERMINATED BY ‘\003’</td></tr></tbody></table><h4 id="2-2-1-Struct-使用"><a href="#2-2-1-Struct-使用" class="headerlink" title="2.2.1.  Struct 使用"></a>2.2.1.  Struct 使用</h4><p><strong>Structs内部的数据可以通过DOT（.）来存取</strong>，例如，表中一列c的类型为<code>STRUCT{a INT; b INT}</code>，我们可以通过c.a来访问域a</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># 数据</span><br><span class="line">1001,zhangsan:24</span><br><span class="line">1002,lisi:28</span><br><span class="line">1003,wangwu:25</span><br><span class="line"></span><br><span class="line"># 1.创建表</span><br><span class="line">hive&gt; create table student_test(id INT, info struct&lt;name:STRING, age:INT&gt;)ROW FORMAT DELIMITED FIELDS TERMINATED BY ','COLLECTION ITEMS TERMINATED BY ':';</span><br><span class="line"></span><br><span class="line"># 2.加载表</span><br><span class="line">hive&gt; load data local inpath "student_test" into table student_test;</span><br><span class="line"></span><br><span class="line"># 3.顺便设置 显示表头,和当前数据库</span><br><span class="line">hive&gt; set hive.cli.print.header=true;</span><br><span class="line">hive&gt; set hive.cli.print.current.db=true;</span><br><span class="line"></span><br><span class="line"># 4. 展示所有的</span><br><span class="line">hive (default)&gt; select * from student_test;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">student_test.idstudent_test.info</span><br><span class="line">1001&#123;"name":"zhangsan","age":24&#125;</span><br><span class="line">1002&#123;"name":"lisi","age":28&#125;</span><br><span class="line">1003&#123;"name":"wangwu","age":25&#125;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line"></span><br><span class="line"># Struct -结构体-使用 . </span><br><span class="line">hive (default)&gt; select id,info.name,info.age from student_test;</span><br><span class="line">idnameage</span><br><span class="line">1001zhangsan24</span><br><span class="line">1002lisi28</span><br><span class="line">1003wangwu25</span><br></pre></td></tr></table></figure><h4 id="2-2-2-Array-使用"><a href="#2-2-2-Array-使用" class="headerlink" title="2.2.2. Array 使用"></a>2.2.2. Array 使用</h4><p><strong>Array中的数据为相同类型</strong>，例如，假如array A中元素<code>[&#39;a&#39;,&#39;b&#39;,&#39;c’]</code>，则A[1]的值为’b’</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># 原始数据</span><br><span class="line">zhangsan,24:25:27:37</span><br><span class="line">lisi,28:39:23:43</span><br><span class="line">wangwu,25:23:02:54</span><br><span class="line"></span><br><span class="line"># 创建表</span><br><span class="line">hive (default)&gt; create table class_test(name string,student_id_list array&lt;int&gt;) row format delimited fields terminated by ',' collection items terminated by ':';</span><br><span class="line"></span><br><span class="line"># 加载表</span><br><span class="line">hive (default)&gt; load data local inpath "class_test" into table class_test;</span><br><span class="line"></span><br><span class="line"># 查看表</span><br><span class="line">hive (default)&gt; select * from class_test;</span><br><span class="line">OK</span><br><span class="line">class_test.nameclass_test.student_id_list</span><br><span class="line">zhangsan[24,25,27,37]</span><br><span class="line">lisi[28,39,23,43]</span><br><span class="line">wangwu[25,23,2,54]</span><br><span class="line"></span><br><span class="line"># 查看数据中某个元素</span><br><span class="line">hive (default)&gt; select name, student_id_list[0] from class_test where name='zhangsan';</span><br><span class="line">OK</span><br><span class="line">name_c1</span><br><span class="line">zhangsan24</span><br></pre></td></tr></table></figure><h4 id="2-2-3-Map-使用"><a href="#2-2-3-Map-使用" class="headerlink" title="2.2.3. Map 使用"></a>2.2.3. Map 使用</h4><p>访问指定域可以通过[“指定域名称”]进行，例如，一个Map M包含了一个group-&gt;gid的kv对，<strong>gid的值可以通过M[‘group’]来获取</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"># 原始数据</span><br><span class="line">1001job:80,team:60,person:70</span><br><span class="line">1002job:60,team:80,person:80</span><br><span class="line">1003job:90,team:70,person:100</span><br><span class="line"></span><br><span class="line"># 创建表</span><br><span class="line">hive (default)&gt; create table employee(id string,perf map&lt;string,int&gt;) row format delimited fields terminated by '\t' collection items terminated by ',' map keys terminated by ':';</span><br><span class="line"></span><br><span class="line"># 导入</span><br><span class="line">hive (default)&gt; load data local inpath "employee_data" into table employee;</span><br><span class="line"></span><br><span class="line"># 查看</span><br><span class="line">hive (default)&gt; select * from employee;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">employee.idemployee.perf</span><br><span class="line">1001&#123;"job":80,"team":60,"person":70&#125;</span><br><span class="line">1002&#123;"job":60,"team":80,"person":80&#125;</span><br><span class="line">1003&#123;"job":90,"team":70,"person":100&#125;</span><br><span class="line">Time taken: 0.228 seconds, Fetched: 3 row(s)</span><br><span class="line"></span><br><span class="line"># 查看单个</span><br><span class="line">hive (default)&gt; select id,perf['job'],perf['team'],perf['person'] from employee;</span><br><span class="line">OK</span><br><span class="line">id_c1_c2_c3</span><br><span class="line">1001806070</span><br><span class="line">1002608080</span><br><span class="line">10039070100</span><br><span class="line"></span><br><span class="line"># 显示别名</span><br><span class="line">hive (default)&gt; select id,perf['job'] as job,perf['team'] as team,perf['person'] as person from employee;</span><br><span class="line">OK</span><br><span class="line">idjobteamperson</span><br><span class="line">1001806070</span><br><span class="line">1002608080</span><br><span class="line">10039070100</span><br></pre></td></tr></table></figure><p><br></p><h1 id="3-DDL-DML"><a href="#3-DDL-DML" class="headerlink" title="3. DDL , DML"></a>3. DDL , DML</h1><h2 id="3-1-DDL"><a href="#3-1-DDL" class="headerlink" title="3.1 DDL"></a>3.1 DDL</h2><h3 id="3-1-1-数据库定义"><a href="#3-1-1-数据库定义" class="headerlink" title="3.1.1  数据库定义"></a>3.1.1  数据库定义</h3><ul><li><p>默认数据库”default”</p></li><li><p>使用某个数据库 <code>use &lt;数据库名&gt;</code></p></li><li><p>创建一个新库</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span>  </span><br><span class="line">[<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] mydb  </span><br><span class="line">[LOCATION] <span class="string">'/.......'</span>  </span><br><span class="line">[<span class="keyword">COMMENT</span>] <span class="string">'....’;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">hive&gt;SHOW DATABASES;</span></span><br><span class="line"><span class="string">hive&gt;DESCRIBE DATABASE [extended] mydb;</span></span><br><span class="line"><span class="string">hive&gt;DROP DATABASE [IF EXISTS] mydb [CASCADE];</span></span><br></pre></td></tr></table></figure></li><li><p>创建</p><ul><li><code>create database db1;</code></li></ul></li><li><p>删除</p><ul><li><code>drop database if exists db1;</code></li><li><strong>级联删除</strong><ul><li><code>drop database if exists db1 cascade;</code></li></ul></li></ul></li></ul><h3 id="3-1-2-表定义-修改"><a href="#3-1-2-表定义-修改" class="headerlink" title="3.1.2  表定义/修改"></a>3.1.2  表定义/修改</h3><ul><li><p><strong>创建表</strong></p><ul><li><p>hive&gt;CREATE TABLE IF NOT EXISTS t1(…) </p><p>[COMMENT ‘….’] </p><p>[LOCATION ‘…’] </p><ul><li><code>hive (default)&gt; create table t4(name string,age int) row format delimited fields terminated by &quot;\t”;</code></li></ul></li><li><p>hive&gt; SHOW TABLES in mydb;</p><ul><li><code>show tables in mydb ‘’class*“</code> : 查看以 mydb 库中, 以 class 开头的表</li></ul></li><li><p><strong>hive&gt;CREATE TABLE t2 LIKE t1;</strong>   复制表</p><ul><li><strong>只会复制表结构</strong></li><li><code>hive (default)&gt; create table t2 like t1;</code></li><li><code>hive (mydb)&gt; create table t3 like default.employee;</code><ul><li>复制其它库的表</li></ul></li></ul></li><li><p>hive&gt;DESCRIBE t2;</p><ul><li>desc t2;  # 效果一样的</li><li><code>desc extended t1;</code>    # 查看更详细的表信息</li><li><strong><code>hive (default)&gt; desc formatted t1;</code>  # 格式化查看表的详细信息</strong></li></ul></li><li><p>drop  table xxx;</p><ul><li>删除表</li></ul></li><li><p><strong>查看建表语句</strong></p><ul><li><strong>show create table t_table;</strong></li></ul></li></ul></li><li><p><strong>修改表</strong></p><ul><li><strong>重命名表</strong><ul><li><code>ALTER TABLE table_name RENAME TO new_table_name</code></li></ul></li><li><strong>增加/删除 分区</strong><ul><li><code>alter table student_p add partition(part=&#39;a&#39;) partition(part=&#39;b&#39;);</code><ul><li>两个 partition中没有’,’</li></ul></li><li>alter table student drop partition(stat_data=‘ffff’), partition(part=‘a’),partiton(part=‘b’);<ul><li>两个 partition中有’,’</li></ul></li></ul></li><li><strong>增加/更新 列</strong><ul><li><code>ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)</code><ul><li><em>注：ADD<strong>是代表新增一字段，字段位置在所有列后面(partition</strong>列前)<strong>，REPLACE</strong>则是表示替换表中所有字段。</em> </li><li><code>alter table student add  columns (name1 string);</code></li></ul></li><li><code>ALTER TABLE table_name CHANGE c_name new_c_name new_c_type [FIRST | AFTER c_name]</code></li></ul></li></ul></li></ul><h3 id="3-1-3-列定义"><a href="#3-1-3-列定义" class="headerlink" title="3.1.3 列定义"></a>3.1.3 列定义</h3><ul><li><p>修改列的名称、类型、位置、注释</p><ul><li><p><code>ALTER TABLE t3 CHANGE COLUMN old_name new_name String COMMENT &#39;...&#39; AFTER column2;</code></p></li><li><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 修改列名</span><br><span class="line">hive (default)&gt; alter table t1 change column name username string comment 'new name';</span><br><span class="line"># 查看表</span><br><span class="line">hive (default)&gt; desc t1;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">col_namedata_type<span class="keyword">comment</span></span><br><span class="line">username            <span class="keyword">string</span>              <span class="keyword">new</span> <span class="keyword">name</span></span><br><span class="line">age                 <span class="built_in">int</span></span><br><span class="line"><span class="comment">---</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>增加列</p><ul><li><p><code>hive&gt; ALTER TABLE t3 ADD COLUMNS(gender int);</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 查看表结构</span><br><span class="line">hive (default)&gt; desc t3;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">col_namedata_type<span class="keyword">comment</span></span><br><span class="line"><span class="keyword">name</span>                <span class="keyword">string</span></span><br><span class="line"><span class="comment">---</span></span><br><span class="line"># 添加列</span><br><span class="line">hive (<span class="keyword">default</span>)&gt; <span class="keyword">alter</span> <span class="keyword">table</span> t3 <span class="keyword">add</span> <span class="keyword">columns</span>(gender <span class="built_in">int</span>);</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">hive (default)&gt; desc t3;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">col_namedata_type<span class="keyword">comment</span></span><br><span class="line"><span class="keyword">name</span>                <span class="keyword">string</span></span><br><span class="line">gender              <span class="built_in">int</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>删除列  replace </p><ul><li><strong>非常不建议使用</strong>, 会造成数据错乱, 一般采取重新创建一张表的方式.</li></ul></li></ul><h3 id="3-1-4-显示命令"><a href="#3-1-4-显示命令" class="headerlink" title="3.1.4 显示命令"></a>3.1.4 显示命令</h3><blockquote><p>show tables</p><p>show databases</p><p>show partitions</p><p>show functions</p><p>desc extended t_name;</p><p>desc formatted table_name;</p></blockquote><h2 id="3-2-DML"><a href="#3-2-DML" class="headerlink" title="3.2 DML"></a>3.2 DML</h2><h3 id="3-2-1-Load"><a href="#3-2-1-Load" class="headerlink" title="3.2.1 Load"></a>3.2.1 Load</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-17-032955.gif" alt="*">  <strong>语法结构</strong></p><p><code>LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]</code></p><p><strong>说明：</strong></p><ol><li>Load 操作只是单纯的复制/移动操作，将数据文件移动到 Hive 表对应的位置。</li><li>filepath：<ul><li>相对路径，例如：<code>project/data1</code></li><li>绝对路径，例如：<code>/user/hive/project/data1</code></li><li>包含模式的完整 URI，例如：<ul><li><code>hdfs://namenode:9000/user/hive/project/data1</code></li></ul></li></ul></li><li><p>LOCAL关键字</p><ul><li>如果指定了 LOCAL， load 命令会去查找本地文件系统中的 filepath。</li><li>如果没有指定 LOCAL 关键字，则根据inpath中的uri查找文件</li></ul></li><li><p>OVERWRITE 关键字</p><ul><li>如果使用了 OVERWRITE 关键字，则目标表（或者分区）中的内容会被删除，然后再将 filepath 指向的文件/目录中的内容添加到表/分区中。 </li><li>如果目标表（分区）已经有一个文件，并且文件名和 filepath 中的文件名冲突，那么现有的文件会被新文件所替代。 </li></ul></li></ol><h3 id="3-2-2-Insert"><a href="#3-2-2-Insert" class="headerlink" title="3.2.2 Insert"></a>3.2.2 Insert</h3><h4 id="语法结构"><a href="#语法结构" class="headerlink" title="语法结构"></a><strong>语法结构</strong></h4><ul><li><p>普通插入</p><ul><li>INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 …)] select_statement1 FROM from_statement </li></ul></li><li><p>Multiple inserts:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">FROM from_statement </span><br><span class="line">[<span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename1 [<span class="keyword">PARTITION</span> (partcol1=val1, partcol2=val2 ...)] select_statement1 ]</span><br><span class="line">[<span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename2 [<span class="keyword">PARTITION</span> ...] select_statement2] ...</span><br><span class="line"></span><br><span class="line"># 多重插入举例</span><br><span class="line"><span class="keyword">from</span> student</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> student_p <span class="keyword">partition</span>(part=<span class="string">'a'</span>)</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">where</span> <span class="keyword">id</span>&lt;<span class="number">95011</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> student_p <span class="keyword">partition</span>(part=<span class="string">'b'</span>)</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">where</span> <span class="keyword">id</span>&gt;<span class="number">95011</span>;</span><br></pre></td></tr></table></figure></li><li><p>Dynamic partition inserts:</p><ul><li>不指定分区字段, 按照 from 表的分区字段插入</li><li><code>INSERT OVERWRITE TABLE tablename PARTITION (partcol1, partcol2 ...) select_statement FROM from_statement</code></li></ul></li></ul><h4 id="导出表数据"><a href="#导出表数据" class="headerlink" title="导出表数据"></a>导出表数据</h4><p><strong>语法结构</strong></p><ul><li><p><code>INSERT OVERWRITE [LOCAL] DIRECTORY directory1 SELECT ... FROM ...</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 导出到本地</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/home/ap/test/stucent1'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student1;</span><br><span class="line"><span class="comment">--------------------</span></span><br><span class="line">例子:</span><br><span class="line">'查询学生信息，按性别分区，在分区内按年龄有序'</span><br><span class="line">0: jdbc:hive2://cs2:10000&gt; set mapred.reduce.tasks=2;</span><br><span class="line">No rows affected (0.015 seconds)</span><br><span class="line">0: jdbc:hive2://cs2:10000&gt; insert overwrite local directory '/home/ap/ihiveout'</span><br><span class="line">. . . . . . . . . . . . .&gt; select * from student distribute by Sex sort by Sage;</span><br><span class="line"><span class="comment">--------------------</span></span><br><span class="line"></span><br><span class="line"># 导出到 HDFS (仅仅是少了一个 local)</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">directory</span> <span class="string">'/test/stucent1'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student1;</span><br></pre></td></tr></table></figure></li><li><p><strong>multiple inserts:</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">FROM from_statement</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE [<span class="keyword">LOCAL</span>] <span class="keyword">DIRECTORY</span> directory1 select_statement1</span><br><span class="line">[<span class="keyword">INSERT</span> OVERWRITE [<span class="keyword">LOCAL</span>] <span class="keyword">DIRECTORY</span> directory2 select_statement2] ...</span><br></pre></td></tr></table></figure></li></ul><h1 id="4-Hive的数据模型"><a href="#4-Hive的数据模型" class="headerlink" title="4. Hive的数据模型"></a>4. Hive的数据模型</h1><h3 id="4-1-管理表-又称为内部表-受控表"><a href="#4-1-管理表-又称为内部表-受控表" class="headerlink" title="4.1 管理表 - 又称为内部表, 受控表"></a>4.1 管理表 - 又称为内部表, 受控表</h3><h4 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a><strong>基本操作</strong></h4><ul><li>创建数据文件inner_table.dat</li><li>创建表<ul><li><code>hive&gt;create table inner_table (key string);</code></li></ul></li><li><strong>加载数据</strong><ul><li><strong>加载本地数据</strong><ul><li><code>hive&gt;load data local inpath &#39;/root/inner_table.dat&#39; into table inner_table;</code></li></ul></li><li><strong>加载HDFS 上数据</strong><ul><li><code>hive&gt;load data inpath ‘xxx’ into table xxx;</code></li></ul></li><li><strong>区别</strong><ul><li>加载 hdfs 上的数据没有 <strong>local</strong></li><li><strong>加载本地数据是 copy 一份, 加载 hdfs 上的数据是直接移动数据到加载的表目录下– mv</strong></li></ul></li></ul></li><li>查看数据<ul><li><code>select * from inner_table</code></li><li><code>select count(*) from inner_table</code></li></ul></li><li><strong>删除表 <code>drop table inner_table</code></strong></li><li><strong>清空表 <code>truncate table table_name;</code></strong> </li><li><strong>注意: </strong><ul><li><strong>如果创建表的时候, 只指定了目录, 没有指定表名, 删除表的时候, 会把该目录下的所有表全部删掉</strong></li><li><code>hive (mydb)&gt; create table t2(id int)location &#39;/home/t2&#39;;</code></li></ul></li></ul><h4 id="内部表解释"><a href="#内部表解释" class="headerlink" title="内部表解释"></a><strong>内部表解释</strong></h4><ul><li>管理表，也称作<strong>内部表</strong>,受控表<ul><li>所有的 Table 数据（不包括 External Table）<strong>都保存在warehouse这个目录中。</strong></li><li><strong>删除表时，元数据与数据都会被删除</strong></li><li>创建过程和数据加载过程（这两个过程可以在同一个语句中完成），<strong>在加载数据的过程中，实际数据会被移动到数据仓库目录中</strong>；之后<strong>对数据对访问</strong>将会<strong>直接在数据仓库目录中</strong>完成。删除表时，表中的数据和元数据将会被同时删除</li></ul></li></ul><h4 id="内部表转为外部表-外部表转为内部表"><a href="#内部表转为外部表-外部表转为内部表" class="headerlink" title="内部表转为外部表,  外部表转为内部表"></a><strong>内部表转为外部表,  外部表转为内部表</strong></h4><ul><li><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">hive (mydb)&gt; create table t1(id int);</span><br><span class="line"></span><br><span class="line"># manage_table 转换为 外部表 external_table </span><br><span class="line">## 注意: 修改为外部表时, 后面2个都要大写</span><br><span class="line">hive (mydb)&gt; alter table t1 set tblproperties('EXTERNAL'='TRUE');</span><br><span class="line">## 修改为内部表</span><br><span class="line">hive (mydb)&gt; alter table t1 set tblproperties('EXTERNAL'='FALSE');</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 查看t1详细信息</span><br><span class="line">desc formatted t1;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">Location:           hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1</span><br><span class="line">Table Type:         EXTERNAL_TABLE</span><br><span class="line"><span class="comment">-- </span></span><br><span class="line"></span><br><span class="line"># 删除t1</span><br><span class="line">hive (mydb)&gt; drop table t1;</span><br><span class="line"># 此时再查看, 已经没了</span><br><span class="line">hive (mydb)&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">tab_name</span><br><span class="line"></span><br><span class="line"># 但是查看hdfs 路径会发现还在, 因为此表现在已经是外部表, 删除不会删除数据</span><br><span class="line">dfs -ls /user/hive/warehouse/mydb.db/t1</span><br><span class="line"></span><br><span class="line"># 如果此时再创建一个新表 t1, 表结构一样, 则数据会自动加载</span><br></pre></td></tr></table></figure></li></ul><h3 id="4-2-※-外部表"><a href="#4-2-※-外部表" class="headerlink" title="4.2 ※ 外部表"></a>4.2 ※ 外部表</h3><h4 id="4-2-1基本操作"><a href="#4-2-1基本操作" class="headerlink" title="4.2.1基本操作"></a><strong>4.2.1基本操作</strong></h4><ul><li>创建数据文件external_table.dat</li><li>创建表<ul><li><code>hive&gt;create external table external_table1 (key string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39; location &#39;/home/external’;</code></li></ul></li><li>在HDFS创建目录/home/external<ul><li><code>#hadoop fs -put /home/external_table.dat /home/external</code></li><li>在工作中, 一般都这样使用, 把数据上传到 hdfs 中</li></ul></li><li>加载数据<ul><li><code>LOAD DATA  &#39;/home/external_table1.dat&#39; INTO TABLE external_table1;</code></li></ul></li><li>查看数据<ul><li><code>select * from external_table</code></li><li><code>select count(*) from external_table</code></li></ul></li><li>删除表 <ul><li><code>drop table external_table</code></li></ul></li></ul><h4 id="4-2-2-外部表解释"><a href="#4-2-2-外部表解释" class="headerlink" title="4.2.2 外部表解释"></a><strong>4.2.2 外部表解释</strong></h4><ul><li>包含External 的表叫外部表<ul><li>删除外部表只删除metastore的元数据，不删除hdfs中的表数据</li><li>外部表 只有一个过程，加载数据和创建表同时完成，并不会移动到数据仓库目录中，只是与外部数据建立一个链接。当删除一个 外部表 时，仅删除该链接</li><li>指向已经在 HDFS 中存在的数据，可以创建 Partition</li><li>它和 内部表 在元数据的组织上是相同的，而实际数据的存储则有较大的差异</li></ul></li></ul><h4 id="4-2-3-外部表语法"><a href="#4-2-3-外部表语法" class="headerlink" title="4.2.3 外部表语法"></a>4.2.3 外部表语法</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> page_view</span><br><span class="line">( viewTime <span class="built_in">INT</span>, </span><br><span class="line">  userid <span class="built_in">BIGINT</span>,</span><br><span class="line">  page_url <span class="keyword">STRING</span>, </span><br><span class="line"> referrer_url <span class="keyword">STRING</span>, </span><br><span class="line">  ip <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'IP Address of the User'</span>,</span><br><span class="line">  country <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'country of origination‘</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">    COMMENT '</span>This <span class="keyword">is</span> the staging page <span class="keyword">view</span> <span class="keyword">table</span><span class="string">'</span></span><br><span class="line"><span class="string">    ROW FORMAT DELIMITED FIELDS TERMINATED BY '</span>\t<span class="string">' LINES TERMINATED BY '</span>\n<span class="string">'</span></span><br><span class="line"><span class="string">    STORED AS TEXTFILE</span></span><br><span class="line"><span class="string">    LOCATION '</span>hdfs://centos:<span class="number">9000</span>/<span class="keyword">user</span>/<span class="keyword">data</span>/staging/page_view<span class="string">';</span></span><br></pre></td></tr></table></figure><h4 id="4-2-4外部表注意点"><a href="#4-2-4外部表注意点" class="headerlink" title="4.2.4外部表注意点:"></a>4.2.4外部表注意点:</h4><ul><li><p>先创建外部表/内部表, 表名为<code>t3</code>, 再往<code>t3</code>传对应字段的数据, 就可以直接 select 数据了</p></li><li><p><u>删除外部表之后, 原本数据不会删除</u>, 此时<strong>在相同的父路径</strong>创建与被删除表<strong>字段相同&amp;名称相同</strong>的<strong>内部/外部表</strong>, 数据<strong>也会直接加载</strong></p></li><li><p><strong>再看一个操作</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 在 mydb.db 库下创建一个外部表 t5, 指定路径为 '/external/t5'</span><br><span class="line"># 此时在 mydb.db 库的路径下是不存在 t5表的, 而是存在 '/external/t5' 中</span><br><span class="line"># 但是使用 'show tables' 是存在 t5 的</span><br><span class="line">hive (mydb)&gt; create external table t5(id int) location '/external/t5';</span><br><span class="line"># 往此目录传数据, 注意: 此时传过去, intdata 数据存在 t5目录下</span><br><span class="line">[ap@cs2]~% hadoop fs -put intdata /external/t5</span><br><span class="line">[ap@cs2]~% hadoop fs -put intdata /external/t5/i2</span><br><span class="line"># 继续传数据, 查询的时候, 就是传的全部数据'相当于所有的数据都在 t5表中'</span><br><span class="line">[ap@cs2]~% hadoop fs -put intdata /external/t5</span><br><span class="line"># 注意: 如果传到 t5 目录下, 没有指定数据文件名的话, 会默认采用数据的名称文件.</span><br></pre></td></tr></table></figure></li></ul><h3 id="4-3-※-分区表"><a href="#4-3-※-分区表" class="headerlink" title="4.3  ※ 分区表"></a>4.3  ※ 分区表</h3><h4 id="4-3-1-基本概念和操作"><a href="#4-3-1-基本概念和操作" class="headerlink" title="4.3.1 基本概念和操作"></a>4.3.1 基本概念和操作</h4><ul><li>分区可以理解为分类，通过分类把不同类型的数据放到不同的目录下。</li><li>分类的标准就是分区字段，可以一个，也可以多个。</li><li>分区表的意义在于优化查询。查询时尽量利用分区字段。如果不使用分区字段，就会全部扫描。</li></ul><blockquote><p><strong>创建分区表, 指定分区字段</strong></p><p><code>hive&gt;CREATE TABLE t3(...) PARTITIONED BY (province string);</code></p><ul><li>创建表的时候, 指定分区字段 key<code>province</code></li></ul><p><strong>为分区字段添加一个值</strong></p><p><code>hive&gt;ALTER TABLE t3 ADD [IF NOT EXISTS] PARTITION(...) LOCATION &#39;...’;</code></p><ul><li><code>alter table t3 add if not exists partition(province=&#39;hubei&#39;) partition(province=&#39;shanghai&#39;);</code></li><li><code>alter table t3 add if not exists partition(province=&#39;jiangsu&#39;);</code></li><li>可为此分区字段添加多个值,  为 province 添加 hubei, hunan….</li></ul><p><strong>查看表的分区字段&amp;值</strong></p><p><code>hive&gt;SHOW PARTITIONS t3 [partition (province=&#39;beijing&#39;)];</code></p><p><strong>删除分区</strong></p><p><code>hive&gt;ALTER TABLE t3 DROP PARTITION(province=‘beijing’.);</code></p><ul><li>这里是删除北京的分区 (如果是内部表, 会连数据一起删除)</li></ul><p><strong>设置表不能被删除/查询</strong>  ——– 这里报语法错误, :TODO</p><ul><li>防止分区被删除:<code>alter table student_p partition (part=&#39;aa&#39;) enable no_drop;</code></li><li>防止分区被查询:<code>alter table student_p partition (part=&#39;aa&#39;) enable offline;</code></li><li>enable 和 disable 是反向操作</li></ul><p><strong>其它一些相关命令</strong></p><p>SHOW TABLES; # 查看所有的表</p><p>SHOW TABLES ‘<em>TMP</em>‘; #支持模糊查询</p><p><code>SHOW PARTITIONS TMP_TABLE;</code> #查看表有哪些分区</p><p>DESC TMP_TABLE; #查看表结构</p></blockquote><h4 id="4-3-2-创建分区表完整语法"><a href="#4-3-2-创建分区表完整语法" class="headerlink" title="4.3.2 创建分区表完整语法"></a>4.3.2 创建分区表完整语法</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> tmp_table #表名</span><br><span class="line">(</span><br><span class="line">title   <span class="keyword">string</span>, # 字段名称 字段类型</span><br><span class="line">minimum_bid     <span class="keyword">double</span>,</span><br><span class="line">quantity        <span class="built_in">bigint</span>,</span><br><span class="line">have_invoice    <span class="built_in">bigint</span></span><br><span class="line">)<span class="keyword">COMMENT</span> <span class="string">'注释：XXX'</span> #表注释</span><br><span class="line"> PARTITIONED <span class="keyword">BY</span>(pt <span class="keyword">STRING</span>) #分区表字段（如果你文件非常之大的话，采用分区表可以快过滤出按分区字段划分的数据）</span><br><span class="line"> <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> </span><br><span class="line">   <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\001'</span>   # 字段是用什么分割开的</span><br><span class="line">STORED AS SEQUENCEFILE; #用哪种方式存储数据，SEQUENCEFILE是hadoop自带的文件压缩格式</span><br></pre></td></tr></table></figure><h4 id="4-3-3-分区表注意点-错误点"><a href="#4-3-3-分区表注意点-错误点" class="headerlink" title="4.3.3 分区表注意点(错误点)"></a>4.3.3 分区表注意点(错误点)</h4><ul><li><p><strong>1) 分区表在 load 数据的时候, 得指定分区, 否则会报错</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 错误1: 分区表在 load 数据的时候, 得指定分区</span><br><span class="line">hive (mydb)&gt; load data local inpath '~/ihivedata/intdata' into table t6;</span><br><span class="line">FAILED: SemanticException [Error 10062]: Need to specify partition columns because the destination table is partitioned</span><br><span class="line"></span><br><span class="line"># 错误2: 导入本地数据的时候, 'path'是从当前所在路径开始的</span><br><span class="line">hive (mydb)&gt; load data local inpath '~/ihivedata/intdata' into table t6 partition(class='job1');</span><br><span class="line">FAILED: SemanticException Line 1:23 Invalid path ''&lt;sub&gt;/ihivedata/intdata'': No files matching path file:/home/ap/&lt;/sub&gt;/ihivedata/intdata</span><br><span class="line"></span><br><span class="line"># 这里就正确了</span><br><span class="line">hive (mydb)&gt; load data local inpath 'ihivedata/intdata' into table t6 partition(class='job1');</span><br><span class="line">Loading data to table mydb.t6 partition (class=job1)</span><br><span class="line">OK</span><br></pre></td></tr></table></figure><ul><li><p><strong>本质原因:</strong> </p><ul><li><strong>分区表的分区, 就是在 hdfs 上, 原表的文件夹下面创建了一个子文件夹, 文件夹名就是分区名.</strong></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-14-113618.png" alt="image-20180614193618027"></p><ul><li><strong>从本地 load 数据:</strong> <code>hive (mydb)&gt; load data local inpath &#39;ihivedata/intdata&#39; into table t6 partition(class=&#39;job1&#39;);</code></li><li><strong>load 数据指定分区之后, 会直接 load 到数据文件夹里面</strong></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-14-113550.png" alt="image-20180614193549709"></p></li><li><p><strong>2) 没有添加分区时, 直接往不存在的分区导入数据, 分区会自动创建</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 直接往不存在的分区load数据, 分区会自动创建</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'ihivedata/intdata'</span> <span class="keyword">into</span> <span class="keyword">table</span> t6 <span class="keyword">partition</span>(<span class="keyword">class</span>=<span class="string">'job110'</span>);</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p><strong>3) 手动在表中创建分区(文件夹), 并直接向此文件夹中导入数据</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"># 直接创建目录</span><br><span class="line">hive (mydb)&gt; dfs -mkdir /user/hive/warehouse/mydb.db/t6/class=job120;</span><br><span class="line"></span><br><span class="line"># 直接从 hadoop 端传数据</span><br><span class="line">hadoop fs -put ihivedata/intdata /user/hive/warehouse/mydb.db/t6/class=job120</span><br><span class="line"></span><br><span class="line"># 此时再 show partitions t6; 会发现并没有此分区</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">partition</span><br><span class="line">class=job1</span><br><span class="line">class=job110</span><br><span class="line">class=job2</span><br><span class="line">class=job3</span><br><span class="line">class=job4</span><br><span class="line"></span><br><span class="line"># 此时就需要手动'激活'此分区, 加入了就有了</span><br><span class="line">hive (mydb)&gt; alter table t6 add partition(class='job120');</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">hive (mydb)&gt; show partitions t6;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">partition</span><br><span class="line">class=job1</span><br><span class="line">class=job110</span><br><span class="line">class=job120</span><br><span class="line">class=job2</span><br><span class="line">class=job3</span><br><span class="line">class=job4</span><br><span class="line"></span><br><span class="line"># 查看分区信息</span><br><span class="line">hive (mydb)&gt; select * from t6 where class='job110';</span><br><span class="line">OK</span><br><span class="line">t6.idt6.class</span><br><span class="line">1job110</span><br><span class="line">2job110</span><br><span class="line">3job110</span><br><span class="line">4job110</span><br><span class="line">5job110</span><br></pre></td></tr></table></figure></li></ul><h4 id="4-3-4-复合分区"><a href="#4-3-4-复合分区" class="headerlink" title="4.3.4 复合分区"></a>4.3.4 复合分区</h4><h5 id="基本操作-1"><a href="#基本操作-1" class="headerlink" title="基本操作"></a>基本操作</h5><ul><li>创建数据文件<code>partition_table.dat</code></li><li>创建表<ul><li><code>create table t7(name string,age int)partitioned by(class string,city string)row format delimited fields terminated by &#39;\t&#39; stored as TEXTFILE;</code></li></ul></li><li><strong>在 Hive 下加载数据到分区</strong><ul><li><code>load data local inpath &#39;ihivedata/partidata&#39; into table t7 partition(class=&#39;job1&#39;,city=&#39;beijing&#39;);</code></li><li><code>load data local inpath &#39;ihivedata/partidata&#39; into table t7 partition(class=&#39;job1&#39;,city=&#39;shanghai&#39;);</code></li><li><code>load data local inpath &#39;ihivedata/partidata&#39; into table t7 partition(class=&#39;job2&#39;,city=&#39;ss&#39;);</code></li><li><strong>注意: 多级分区其实就是多级目录</strong><ul><li>越靠近左边, 目录层级越高; </li><li>越靠近右边, 目录层级越低; </li></ul></li><li><strong>load 数据到多级分区, load层级必须和整个层级数量相同</strong><ul><li>也就是说, 如果<strong>分区有2层</strong>, <strong>传数据</strong>的时候, 也<strong>必须传2层分区</strong>, 并且<strong>层级顺序必须一致</strong></li></ul></li></ul></li><li><strong>从Linux 本地直接导数据到分区</strong><ul><li><strong>可以直接在 hadoop UI 页面, 查看路径, 然后直接传到此路径中</strong></li><li><code>hadoop fs -put ihivedata/partidata /user/hive/warehouse/mydb.db/t7/class=job1/city=beijing/p2</code></li></ul></li><li>查看数据<ul><li><code>select * from partition_table</code></li><li><code>select count(*) from partition_table</code></li></ul></li><li>删除表<ul><li><code>drop table partition_table</code></li></ul></li><li>工作中 用的最多的是 <strong>外部表 + 分区表</strong> </li></ul><h3 id="4-4-桶表-主要用于抽样查询"><a href="#4-4-桶表-主要用于抽样查询" class="headerlink" title="4.4 桶表 - 主要用于抽样查询"></a>4.4 桶表 - 主要用于抽样查询</h3><h4 id="桶表的基本操作"><a href="#桶表的基本操作" class="headerlink" title="桶表的基本操作"></a>桶表的基本操作</h4><ul><li><strong>创建桶表完整过程</strong></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#创建分桶表</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> stu_buck;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu_buck(Sno <span class="built_in">int</span>,Sname <span class="keyword">string</span>,Sex <span class="keyword">string</span>,Sage <span class="built_in">int</span>,Sdept <span class="keyword">string</span>)</span><br><span class="line">clustered <span class="keyword">by</span>(Sno) </span><br><span class="line">sorted <span class="keyword">by</span>(Sno <span class="keyword">DESC</span>)</span><br><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#设置变量,设置分桶为true, 设置reduce数量是分桶的数量个数</span><br><span class="line"><span class="keyword">set</span> hive.enforce.bucketing = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">4</span>;</span><br><span class="line"></span><br><span class="line">#开始往创建的分通表插入数据(插入数据需要是已分桶, 且排序的)</span><br><span class="line">#可以使用distribute by(sno) sort by(sno asc)   或是排序和分桶的字段相同的时候使用Cluster by(字段)</span><br><span class="line">#注意使用cluster by  就等同于分桶+排序(sort)</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_buck</span><br><span class="line"><span class="keyword">select</span> Sno,Sname,Sex,Sage,Sdept <span class="keyword">from</span> student <span class="keyword">distribute</span> <span class="keyword">by</span>(Sno) <span class="keyword">sort</span> <span class="keyword">by</span>(Sno <span class="keyword">asc</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> stu_buck</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student <span class="keyword">distribute</span> <span class="keyword">by</span>(Sno) <span class="keyword">sort</span> <span class="keyword">by</span>(Sno <span class="keyword">asc</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> stu_buck</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student cluster <span class="keyword">by</span>(Sno);</span><br><span class="line"><span class="comment">-----</span></span><br><span class="line">以上3者效果一样的</span><br></pre></td></tr></table></figure><ul><li><p><strong>保存select查询结果的几种方式：</strong></p><ul><li><p>将查询结果保存到一张新的hive表中</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_tmp</span><br><span class="line"><span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t_p;</span><br></pre></td></tr></table></figure></li></ul></li></ul><ol start="2"><li><p>将查询结果保存到一张已经存在的hive表中</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span>  <span class="keyword">table</span> t_tmp</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t_p;</span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>将查询结果保存到指定的文件目录（可以是本地，也可以是hdfs）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/home/hadoop/test'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t_p;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">directory</span> <span class="string">'/aaa/test'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t_p;</span><br></pre></td></tr></table></figure></li></ol><ul><li><p><strong>数据加载到桶表时，会对字段取hash值，然后与桶的数量取模。把数据放到对应的文件中。</strong></p><ul><li>所以<strong>顺序是打乱的</strong>, 不是原始 t1的数据顺序</li></ul></li><li><p><strong>查看数据</strong></p><ul><li><p>可以直接select * 查看全部</p></li><li><p>也可以直接单独查看每个桶的数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hive (mydb)&gt; dfs -cat /user/hive/warehouse/mydb.db/bucket_table/000000_0;</span><br><span class="line">4</span><br><span class="line">hive (mydb)&gt; dfs -cat /user/hive/warehouse/mydb.db/bucket_table/000001_0;</span><br><span class="line">5</span><br><span class="line">1</span><br><span class="line">hive (mydb)&gt; dfs -cat /user/hive/warehouse/mydb.db/bucket_table/000002_0;</span><br><span class="line">2</span><br><span class="line">hive (mydb)&gt; dfs -cat /user/hive/warehouse/mydb.db/bucket_table/000003_0;</span><br><span class="line">3</span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>修改桶的个数</strong></p><ul><li><code>alter table bucket_table clustered by (id) sorted by(id) into 10 buckets;</code><ul><li>但是这样修改之后, 生成的是原来的 copy, 并且里面的数据也很奇怪, 不知道是按照什么来执行的? <strong>:TODO</strong></li></ul></li></ul></li><li><p><strong>注意：</strong></p><ul><li><strong>物理上，每个桶就是表(或分区）目录里的一个文件</strong></li><li>一个作业产生的<strong>桶(输出文件)和reduce任务个数相同</strong></li></ul></li><li><p><strong>桶表工作中容易遇到的错误</strong></p><ul><li><p><strong>向桶表中插入其它表查出的数据的时候,  必须指定字段名</strong>, 否则会报字段不匹配.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">FAILED: SemanticException [Error 10044]: Line 1:12 Cannot <span class="keyword">insert</span> <span class="keyword">into</span> target <span class="keyword">table</span> because <span class="keyword">column</span> <span class="built_in">number</span>/types <span class="keyword">are</span> different <span class="string">'bucket_table'</span>: <span class="keyword">Table</span> insclause<span class="number">-0</span> has <span class="number">1</span> <span class="keyword">columns</span>, but <span class="keyword">query</span> has <span class="number">2</span> columns.</span><br><span class="line"></span><br><span class="line"># 应该是这样</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> bucket_table <span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> t6;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h4 id="※-桶表的抽样查询"><a href="#※-桶表的抽样查询" class="headerlink" title="※ 桶表的抽样查询"></a>※ 桶表的抽样查询</h4><ul><li>桶表的抽样查询<ul><li>select * from bucket_table tablesample<strong>(bucket 1 out of 4 on id);</strong></li></ul></li><li>tablesample是抽样语句<ul><li>语法解析：<strong>TABLESAMPLE(BUCKET x OUT OF y)</strong></li><li><strong>y必须是table总bucket数的<u>倍数</u>或者<u>因子</u>。</strong></li><li>hive根据y的大小，决定抽样的比例。</li><li><strong>例如</strong>，table总共分了64份，当y=32时，抽取(64/32=)2个bucket的数据，当y=128时，抽取(64/128=)1/2个bucket的数据。x表示从哪个bucket开始抽取。</li><li><strong>例如</strong>，table总bucket数为32，tablesample(bucket 3 out of 16)，表示总共抽取（32/16=）2个bucket的数据，分别为第3个bucket和第（3+16=）19个bucket的数据。</li></ul></li></ul><h1 id="5-Hive-视图的操作"><a href="#5-Hive-视图的操作" class="headerlink" title="5. Hive 视图的操作"></a>5. Hive 视图的操作</h1><ul><li>使用视图可以<strong>降低查询的复杂度</strong></li><li><strong>视图的创建</strong><ul><li><strong>create view</strong> v1 <strong>AS</strong> <u>select  t1.name from t1</u>;</li></ul></li><li><strong>视图的删除</strong><ul><li><strong>drop view</strong> if exists v1;</li></ul></li></ul><h1 id="6-Hive-索引的操作"><a href="#6-Hive-索引的操作" class="headerlink" title="6. Hive 索引的操作"></a>6. Hive 索引的操作</h1><ul><li><p><strong>创建索引</strong></p><ul><li><code>create index t1_index on table t1(id) as &#39;org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler&#39; with deferred rebuild in table t1_index_table;</code></li><li><code>t1_index</code>: <strong>索引名称</strong></li><li>as: 指定索引器</li><li><code>t1_index_table</code>: <strong>要创建的索引表</strong></li></ul></li><li><p><strong>显示索引</strong></p><ul><li><code>show formatted index on t1;</code></li></ul></li><li><p><strong>重建索引</strong></p><ul><li><p><strong>alter index</strong> t1_index <strong>on</strong> t1 <strong>rebuild</strong>;</p></li><li><p>重建完索引之后, 查看 t1_index_table 这张表, 就存了t1表文件具体的位置, 最后一列<code>t1_index_table._offsets</code>是 <strong>索引的偏移量, 类似于指针,  偏移量是索引的精髓</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (mydb)&gt; select * from t1_index_table;</span><br><span class="line">OK</span><br><span class="line">t1_index_table.idt1_index_table._bucketnamet1_index_table._offsets</span><br><span class="line">1hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata[0]</span><br><span class="line">2hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata[2]</span><br><span class="line">3hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata[4]</span><br><span class="line">4hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata[6]</span><br><span class="line">5hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata[8]</span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>分区字段本质上其实就是索引</strong></p></li></ul><h1 id="7-装载数据"><a href="#7-装载数据" class="headerlink" title="7. 装载数据"></a>7. 装载数据</h1><h3 id="7-1-普通装载数据"><a href="#7-1-普通装载数据" class="headerlink" title="7.1 普通装载数据:"></a><strong>7.1 普通装载数据:</strong></h3><ul><li><strong>从本地</strong>  put</li><li><p><strong>从 hive</strong> cp</p></li><li><p><strong>从文件中装载数据</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;LOAD DATA [LOCAL] INPATH '...' [OVERWRITE] INTO TABLE t2 [PARTITION (province='beijing')];</span><br></pre></td></tr></table></figure></li><li><p><strong>通过查询表装载数据</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 方式1</span><br><span class="line">hive&gt;INSERT OVERWRITE TABLE t2 PARTITION (province='beijing') SELECT * FROM xxx WHERE xxx;</span><br><span class="line"></span><br><span class="line"># 方式2</span><br><span class="line">hive&gt;FROM t4 </span><br><span class="line"> <span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> t3 <span class="keyword">PARTITION</span> (...) <span class="keyword">SELECT</span> ...WHERE... </span><br><span class="line"> <span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> t3 <span class="keyword">PARTITION</span> (...) <span class="keyword">SELECT</span> ...WHERE... </span><br><span class="line"> <span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> t3 <span class="keyword">PARTITION</span> (...) <span class="keyword">SELECT</span> ...WHERE...;</span><br><span class="line"> </span><br><span class="line"> # 方式3</span><br><span class="line"> 直接插入数据, 也会转化为文件的形式, 存在表的目录下</span><br><span class="line">- <span class="keyword">insert</span> <span class="keyword">into</span> table_name  <span class="keyword">values</span>(xxx); </span><br><span class="line"></span><br><span class="line"># 方式4</span><br><span class="line"> 直接传文件</span><br><span class="line"> - <span class="keyword">load</span> <span class="keyword">data</span> (<span class="keyword">local</span>) inpath ‘ xxx’ <span class="keyword">into</span> <span class="keyword">table</span> t_1;</span><br></pre></td></tr></table></figure></li></ul><h3 id="7-2动态装载数据"><a href="#7-2动态装载数据" class="headerlink" title="7.2动态装载数据"></a>7.2动态装载数据</h3><ul><li><p>不开启动态装载时</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;INSERT OVERWRITE TABLE t3 PARTITION(province='bj', city='bj') </span><br><span class="line"><span class="keyword">SELECT</span> t.province, t.city <span class="keyword">FROM</span> temp t <span class="keyword">WHERE</span> t.province=<span class="string">'bj'</span>;</span><br></pre></td></tr></table></figure></li><li><p><strong>开启动态分区支持</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;set hive.exec.dynamic.partition=true;</span><br><span class="line">hive&gt;set hive.exec.dynamic.partition.mode=nostrict;</span><br><span class="line">hive&gt;set hive.exec.max.dynamic.partitions.pernode=1000;</span><br></pre></td></tr></table></figure></li><li><p><strong>把 t6 表的所有的字段 (包括分区字段) 加载进 t9 对应的分区</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (mydb)&gt; insert overwrite table t9 partition(class) select id,class from t6;</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>单语句建表并同时装载数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;CREATE TABLE t4 AS SELECT ....</span><br></pre></td></tr></table></figure></li></ul><h1 id="8-导出数据"><a href="#8-导出数据" class="headerlink" title="8. 导出数据"></a>8. 导出数据</h1><ul><li><strong>在hdfs之间复制文件(夹)</strong><ul><li><code>hadoop fs -cp source destination</code></li><li><code>hive&gt; dfs -cp source destination</code></li><li>案例: <code>hive&gt;  dfs -get  /user/hive/warehouse/mydb.db/t9  /root/t9;</code><ul><li>从 hdfs 复制到本地</li></ul></li></ul></li><li>使用DIRECTORY<ul><li><code>hive&gt;INSERT OVERWRITE 【LOCAL】 DIRECTORY &#39;...&#39; SELECT ...FROM...WHERE ...;</code></li><li>案例:通过查询导出到 t9, 走的 MapReduce<ul><li>导到到 hdfs:  <code>insert overwrite directory &quot;/home/t9&quot; select * from t9;</code></li><li>导出到本地: <code>insert overwrite local directory &quot;/home/ap/t9&quot; select * from t9;</code></li></ul></li></ul></li></ul><h1 id="9-读模式-amp-写模式"><a href="#9-读模式-amp-写模式" class="headerlink" title="9. 读模式&amp;写模式"></a>9. 读模式&amp;写模式</h1><ul><li>RDBMS是写模式</li><li>Hive是读模式</li></ul><h1 id="10-完整建表语句语法"><a href="#10-完整建表语句语法" class="headerlink" title="10. 完整建表语句语法"></a>10. 完整建表语句语法</h1><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db_name.]table_name</span><br><span class="line">  [(col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)]</span><br><span class="line">    [<span class="keyword">COMMENT</span> table_comment]</span><br><span class="line">    [PARTITIONED <span class="keyword">BY</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)]</span><br><span class="line">    [CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) </span><br><span class="line">    [SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span>|<span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS]</span><br><span class="line">    [SKEWED <span class="keyword">BY</span> (col_name, col_name, ...) <span class="keyword">ON</span> ([(col_value, col_value, ...), ...|col_value, col_value, ...]) </span><br><span class="line">    [<span class="keyword">STORED</span> <span class="keyword">AS</span> DIRECTORIES]   ]</span><br><span class="line">  [ [<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format] </span><br><span class="line">    [<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format] | <span class="keyword">STORED</span> <span class="keyword">BY</span> <span class="string">'storage.handler.class.name'</span> [<span class="keyword">WITH</span> SERDEPROPERTIES (...)]   ]</span><br><span class="line">  [LOCATION hdfs_path]</span><br><span class="line">  [TBLPROPERTIES (property_name=property_value, ...)]     </span><br><span class="line">  [<span class="keyword">AS</span> select_statement]  (Note: <span class="keyword">not</span> supported <span class="keyword">when</span> creating <span class="keyword">external</span> tables.)</span><br></pre></td></tr></table></figure><h1 id="11-文件格式"><a href="#11-文件格式" class="headerlink" title="11. 文件格式"></a>11. 文件格式</h1><ul><li>TextFile</li><li>SequenceFile</li><li>RCFile</li><li>ORC</li></ul><h3 id="11-1-默认存储格式就是-TextFile"><a href="#11-1-默认存储格式就是-TextFile" class="headerlink" title="11.1 默认存储格式就是 TextFile"></a>11.1 默认存储格式就是 TextFile</h3><ul><li>存储空间消耗比较大，</li><li>并且压缩的text 无法分割和合并</li><li>查询的效率最低,可以直接存储，</li><li>加载数据的速度最高</li></ul><h3 id="11-2-使用SequenceFile存储"><a href="#11-2-使用SequenceFile存储" class="headerlink" title="11.2 使用SequenceFile存储"></a>11.2 使用SequenceFile存储</h3><ul><li>存储空间消耗大</li><li>压缩的文件可以分割和合并 </li><li>查询效率高</li><li>需要通过text文件转化来加载</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test2(<span class="keyword">str</span> <span class="keyword">STRING</span>) <span class="keyword">STORED</span> <span class="keyword">AS</span> SEQUENCEFILE;  </span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> hive.exec.compress.output=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> mapred.output.compress=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> mapred.output.compression.codec=com.hadoop.compression.lzo.LzoCodec;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> io.seqfile.compression.type=<span class="keyword">BLOCK</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> io.compression.codecs=com.hadoop.compression.lzo.LzoCodec;</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> test2 <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> test1;</span><br></pre></td></tr></table></figure><p><strong>注意点: SequenceFile 类型的表, 不能直接导入数据文件,  只能通过从他表查询</strong></p><ul><li><p><code>insert overwrite table t2 select * from t1;</code></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 查看此 'SequenceFile' 表</span><br><span class="line">hive (db2)&gt; dfs -cat /user/hive/warehouse/db2.db/t2/000000_0 ;</span><br><span class="line">SEQ"org.apache.hadoop.io.BytesWritableorg.apache.hadoop.io.Text*org.apache.hadoop.io.compress.DefaultCodec���/*&lt;bb�m�?x�c453x�c464x�c475x�c486x�c497hive (db2)&gt;</span><br></pre></td></tr></table></figure></li></ul><h3 id="11-3-使用RCFile存储"><a href="#11-3-使用RCFile存储" class="headerlink" title="11.3 使用RCFile存储"></a>11.3 使用RCFile存储</h3><p><strong>RCFILE是一种行列存储相结合的存储方式。首先，其将数据按行分块，保证同一个record在一个块上，避免读一个记录需要读取多个block。其次，块数据列式存储，有利于数据压缩和快速的列存取。</strong></p><ul><li>rcfile 存储空间最小</li><li>查询的效率最高</li><li>需要通过text文件转化来加载</li><li>加载的速度最低</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test3(<span class="keyword">str</span> <span class="keyword">STRING</span>)  <span class="keyword">STORED</span> <span class="keyword">AS</span> RCFILE;  </span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> hive.exec.compress.output=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> mapred.output.compress=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> mapred.output.compression.codec=com.hadoop.compression.lzo.LzoCodec;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> io.compression.codecs=com.hadoop.compression.lzo.LzoCodec;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> test3 <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> test1;</span><br></pre></td></tr></table></figure><p><strong>注意点:  RCFile 也只能从其它表导入数据</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (db2)&gt; dfs -cat /user/hive/warehouse/db2.db/t3/000000_0;</span><br><span class="line">RCF*org.apache.hadoop.io.compress.DefaultCodechive.io.rcfile.column.number1</span><br></pre></td></tr></table></figure><h3 id="11-4-使用ORC存储-最好的一种格式"><a href="#11-4-使用ORC存储-最好的一种格式" class="headerlink" title="11.4 使用ORC存储(最好的一种格式)"></a>11.4 使用ORC存储(最好的一种格式)</h3><p><strong>是一种针对 RCFile 优化的格式</strong></p><p>主要特点: <strong>压缩, 索引, 单文件输出</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-15-070816.jpg" alt=""></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t1_orc(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span> <span class="keyword">stored</span> <span class="keyword">as</span> orc </span><br><span class="line"></span><br><span class="line">tblproperties(<span class="string">"orc.compress"</span>=<span class="string">"ZLIB"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> ... [<span class="keyword">PARTITION</span> partition_spec] <span class="keyword">SET</span> FILEFORMAT ORC;</span><br><span class="line"></span><br><span class="line"># 也可以改为其它的, 修改的语法就是这样</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> t1 <span class="keyword">set</span> fileformat textfile;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SET</span> hive.default.fileformat=Orc;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> t1_orc <span class="keyword">select</span> * <span class="keyword">from</span> t1;</span><br></pre></td></tr></table></figure><p><strong>注意点:  </strong></p><ul><li><strong>ORC 也只能从其它表导入数据</strong></li><li><strong>占用空间大, 一个 block 有256M, 之前2种都是128M</strong></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive (db2)&gt; dfs -cat /user/hive/warehouse/db2.db/t4/000000_0;</span><br><span class="line">ORC</span><br><span class="line">P+</span><br><span class="line">P�6�b�``���ь@�H�</span><br><span class="line">                       1q01-</span><br><span class="line">P</span><br><span class="line">PK</span><br><span class="line"># ("</span><br><span class="line">       id0P:</span><br><span class="line">P@�;��"</span><br><span class="line">       (0��ORChive</span><br></pre></td></tr></table></figure><h1 id="12-序列化-amp-反序列化-Hive-SerDe"><a href="#12-序列化-amp-反序列化-Hive-SerDe" class="headerlink" title="12. 序列化 &amp; 反序列化 (Hive SerDe)"></a>12. 序列化 &amp; 反序列化 (Hive SerDe)</h1><h3 id="12-1-SerDe"><a href="#12-1-SerDe" class="headerlink" title="12.1 SerDe"></a>12.1 SerDe</h3><ul><li><strong>What is a SerDe?</strong><ul><li>SerDe 是 “Serializer and Deserializer.”的缩写</li><li>Hive 使用 SerDe和FileFormat进行行内容的读写.</li></ul></li><li><strong>Hive序列化流程</strong><ul><li><strong>从 HDFS 上读入文件 (反序列化)</strong><ul><li><code>HDFS文件 --&gt; InputFileFormat --&gt; &lt;key, value&gt; --&gt; Deserializer --&gt; 行对象</code></li></ul></li><li><strong>写出到 HDFS (序列化)</strong><ul><li><code>行对象 --&gt; Serializer --&gt; &lt;key, value&gt; --&gt; OutputFileFormat --&gt; HDFS文件</code></li></ul></li><li>注意: <strong>数据全部存在在value中，key内容无意义</strong></li></ul></li></ul><ul><li><p><strong>Hive 使用如下FileFormat 类读写 HDFS files:</strong></p><ul><li>TextInputFormat/HiveIgnoreKeyTextOutputFormat: 读写普通HDFS文本文件.</li><li>SequenceFileInputFormat/SequenceFileOutputFormat: 读写SequenceFile格式的HDFS文件</li><li>….</li></ul></li></ul><ul><li><p><strong>Hive 使用如下SerDe 类(反)序列化数据:</strong></p><ul><li>MetadataTypedColumnsetSerDe: 读写csv、tsv文件和默认格式文件</li><li>ThriftSerDe: 读写Thrift 序列化后的对象.</li><li>DynamicSerDe: 读写Thrift序列化后的对象, 不过不需要解读schema中的ddl.</li></ul></li></ul><h3 id="12-2-使用CSV-Serde"><a href="#12-2-使用CSV-Serde" class="headerlink" title="12.2 使用CSV Serde"></a>12.2 使用CSV Serde</h3><p>CSV格式的文件也称为逗号分隔值（Comma-Separated Values，CSV，有时也称为字符分隔值，因为分隔字符也可以不是逗号。在本文中的CSV格式的数据就不是简单的逗号分割的），其文件以纯文本形式存储表格数据（数字和文本）。CSV文件由任意数目的记录组成，记录间以某种换行符分隔；每条记录由字段组成，字段间的分隔符是其它字符或字符串，最常见的是逗号或制表符。通常，所有记录都有完全相同的字段序列。<br>默认的分隔符是</p><blockquote><p>DEFAULT_ESCAPE_CHARACTER \<br>DEFAULT_QUOTE_CHARACTER  “     —如果没有，则不需要指定<br>DEFAULT_SEPARATOR  , </p></blockquote><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> csv_table(a <span class="keyword">string</span>, b <span class="keyword">string</span>) <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> SERDE <span class="string">'org.apache.hadoop.hive.serde2.OpenCSVSerde'</span> <span class="keyword">WITH</span> SERDEPROPERTIES (<span class="string">"separatorChar"</span>=<span class="string">"\t"</span>, <span class="string">"quoteChar"</span>=<span class="string">"'"</span>, <span class="string">"escapeChar"</span>=<span class="string">"\\"</span>)  <span class="keyword">STORED</span> <span class="keyword">AS</span> TEXTFILE;</span><br><span class="line"></span><br><span class="line"># separatorChar：分隔符</span><br><span class="line"># quoteChar：引号符</span><br><span class="line"># escapeChar：转义符</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt;&gt;  :TODO 创建表没成功, 用到时再说</span><br></pre></td></tr></table></figure><h1 id="13-Lateral-View-语法"><a href="#13-Lateral-View-语法" class="headerlink" title="13. Lateral View 语法"></a>13. Lateral View 语法</h1><p>lateral view用于和split, explode等UDTF一起使用，它能够<strong>将一行数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合</strong>。lateral view首先为原始表的每行调用UDTF，UTDF会把一行拆分成一或者多行，lateral view再把结果组合，产生一个<strong>支持别名表的虚拟表</strong>。</p><p><strong>lateral: 侧面, 横切面</strong></p><p><strong>Lateral View: 切面表</strong></p><ul><li><p>创建表</p><ul><li><strong>create table t8(name string,nums array\&lt;int>)row format delimited fields terminated by “\t” COLLECTION ITEMS TERMINATED BY ‘:’;</strong></li></ul></li><li><p>数据切割</p><ul><li><strong>SELECT name,new_num FROM t8 LATERAL VIEW explode(nums) num AS new_num;</strong></li><li><code>select name,id from class_test lateral view explode(student_id_list) list as id;</code></li><li><strong>注意: as 前面的 <code>list</code> 貌似是可以随表起名的</strong></li></ul></li><li><p>效果演示</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from class_test;</span><br><span class="line">OK</span><br><span class="line">class_test.nameclass_test.student_id_list</span><br><span class="line">zhangsan[24,25,27,37]</span><br><span class="line">lisi[28,39,23,43]</span><br><span class="line">wangwu[25,23,2,54]</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">hive (default)&gt; select name,id from class_test lateral view explode(student_id_list) list as id;</span><br><span class="line">OK</span><br><span class="line">nameid</span><br><span class="line">zhangsan24</span><br><span class="line">zhangsan25</span><br><span class="line">zhangsan27</span><br><span class="line">zhangsan37</span><br><span class="line">lisi28</span><br><span class="line">lisi39</span><br><span class="line">lisi23</span><br><span class="line">lisi43</span><br><span class="line">wangwu25</span><br><span class="line">wangwu23</span><br><span class="line">wangwu2</span><br><span class="line">wangwu54</span><br></pre></td></tr></table></figure></li></ul><h1 id="14-Hive的高级函数"><a href="#14-Hive的高级函数" class="headerlink" title="14. Hive的高级函数"></a>14. Hive的高级函数</h1><h3 id="14-1-简介"><a href="#14-1-简介" class="headerlink" title="14.1 简介"></a>14.1 简介</h3><ul><li>简单查询<ul><li>select … from…where…</li></ul></li><li><strong>使用各种函数</strong><ul><li><strong>hive&gt;show functions;</strong><ul><li>展示所有函数</li></ul></li><li><strong>hive&gt;describe function  xxx;</strong><ul><li>详细描述函数用法</li></ul></li></ul></li><li>LIMIT语句</li><li>列别名</li><li>嵌套select语句</li></ul><h3 id="14-2-高级函数分类"><a href="#14-2-高级函数分类" class="headerlink" title="14.2 高级函数分类"></a>14.2 高级函数分类</h3><ul><li><p>标准函数</p><ul><li>reverse()</li><li>upper()</li></ul></li><li><p>聚合函数</p><ul><li><p>avg()</p></li><li><p>sum()</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">avg</span>(<span class="keyword">id</span>) <span class="keyword">from</span> t3;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">sum</span>(<span class="keyword">id</span>) <span class="keyword">from</span> t3;</span><br><span class="line"></span><br><span class="line"># 最简答的嵌套</span><br><span class="line">hive (mydb)&gt; select t.id from (select * from t1 where id &gt; 3)t;</span><br><span class="line"></span><br><span class="line"># 最简单的 group by</span><br><span class="line"></span><br><span class="line"># if else 的效果</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, </span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> </span><br><span class="line"><span class="keyword">when</span> <span class="keyword">id</span> &lt;= <span class="number">2</span> </span><br><span class="line"><span class="keyword">then</span> <span class="string">'low'</span></span><br><span class="line"><span class="keyword">when</span> <span class="keyword">id</span>&gt;=<span class="number">3</span> <span class="keyword">and</span> <span class="keyword">id</span> &lt;<span class="number">4</span> </span><br><span class="line"><span class="keyword">then</span> <span class="string">'middle'</span></span><br><span class="line"><span class="keyword">when</span> <span class="keyword">id</span>&gt;=<span class="number">4</span> <span class="keyword">and</span> <span class="keyword">id</span> &lt;<span class="number">5</span> </span><br><span class="line"><span class="keyword">then</span> <span class="string">'high'</span></span><br><span class="line"><span class="keyword">else</span> </span><br><span class="line"><span class="string">'very high'</span></span><br><span class="line"><span class="keyword">end</span> </span><br><span class="line"></span><br><span class="line"><span class="keyword">as</span> </span><br><span class="line">id_highly </span><br><span class="line"><span class="keyword">from</span> t1;</span><br><span class="line"></span><br><span class="line"><span class="comment">----执行结果----</span></span><br><span class="line">idid_highly</span><br><span class="line">1low</span><br><span class="line">2low</span><br><span class="line">3middle</span><br><span class="line">4high</span><br><span class="line">5very high</span><br><span class="line"></span><br><span class="line"><span class="comment">--------------------</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">sid</span>,</span><br><span class="line"><span class="keyword">case</span> course</span><br><span class="line"><span class="keyword">when</span> <span class="string">'yuwen'</span> <span class="keyword">then</span> score</span><br><span class="line"><span class="keyword">else</span> <span class="string">'0'</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">(别名) yuwen</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> course</span><br><span class="line"><span class="keyword">when</span> <span class="string">'shuxue'</span> <span class="keyword">then</span> score</span><br><span class="line"><span class="keyword">else</span> <span class="string">'0'</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">(别名) shuxue</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># <span class="keyword">cast</span> 转换函数, 大概是这么用</span><br><span class="line">hive (mydb)&gt; <span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> t1 <span class="keyword">where</span> <span class="keyword">cast</span>(<span class="keyword">id</span> <span class="keyword">AS</span> <span class="built_in">FLOAT</span>) &lt;<span class="number">3.0</span>;</span><br><span class="line">OK</span><br><span class="line">id</span><br><span class="line">1</span><br><span class="line">2</span><br></pre></td></tr></table></figure><pre><code>- ![image-20180619150824031](http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-070824.png)</code></pre></li></ul></li></ul><p><strong>首先当前不存在的列补0, 然后按照学号分组求和</strong></p><p>​    </p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-070624.png" alt="image-20180619150623836"></p><ul><li>然后按照 sid 分组求和/求最大值, 就可以了</li><li>同一列不同的放在不同的列上, 常用的方法</li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-071819.png" alt="image-20180619151819409"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-072342.png" alt="image-20180619152341523"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-072352.png" alt="image-20180619152351866"></p><hr><p>面试题4</p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-075926.png" alt="image-20180619155925871"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-080134.png" alt="image-20180619160134008"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-080605.png" alt="image-20180619160604750"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-080942.png" alt="image-20180619160941777"></p><p>array_contains()</p><p>desc function array_contains()</p><ul><li><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-081706.jpg?ff=fh3" alt=""></li></ul><ul><li><p>自定义函数</p><ul><li>UDF</li></ul></li></ul><hr><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-082423.png" alt="image-20180619162423281"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-084428.png" alt="image-20180619164428063"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-084655.png" alt="image-20180619164654514"></p><h1 id="15-Hive-性能调优"><a href="#15-Hive-性能调优" class="headerlink" title="15. Hive 性能调优"></a>15. Hive 性能调优</h1><h3 id="15-1-什么时候可以避免执行MapReduce？"><a href="#15-1-什么时候可以避免执行MapReduce？" class="headerlink" title="15.1 什么时候可以避免执行MapReduce？"></a>15.1 什么时候可以避免执行MapReduce？</h3><ul><li><p><code>select *  or  select field1,field2</code></p></li><li><p><code>limite 10</code></p></li><li><p>where语句中只有分区字段</p></li><li><p>使用本地<code>set hive.exec.mode.local.auto=true;</code></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t3;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,age <span class="keyword">from</span> t3;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,age <span class="keyword">from</span> t3 <span class="keyword">limit</span> <span class="number">2</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,age <span class="keyword">from</span> t3 <span class="keyword">where</span> age=<span class="number">25</span>;</span><br><span class="line"># 当 where 是本地字段(列中的字段), 是不走 MR的</span><br></pre></td></tr></table></figure></li><li><p>group by语句：</p><ul><li>通常和聚合函数一起使用，按照一个或者多个列对结果进行分组，然后对每组执行聚合操作</li></ul></li><li>having语句：<ul><li>限制结果的输出</li></ul></li><li>hive将查询转化为MapReduce执行，hive的优化可以转化为mapreduce的优化！</li></ul><h3 id="15-2-hive是如何将查询转化为MapReduce的？-EXPLAIN的使用"><a href="#15-2-hive是如何将查询转化为MapReduce的？-EXPLAIN的使用" class="headerlink" title="15.2 hive是如何将查询转化为MapReduce的？-EXPLAIN的使用"></a><strong>15.2 hive是如何将查询转化为MapReduce的？-EXPLAIN的使用</strong></h3><ul><li>hive对sql的查询计划信息解析</li><li><p>EXPLAIN SELECT COUNT(1) FROM T1;</p></li><li><p><strong>EXPLAIN EXTENDED</strong> </p><ul><li><p><strong>显示详细扩展查询计划信息</strong></p></li><li><p><strong><code>EXPLAIN EXTENDED SELECT COUNT(1) FROM T1;</code></strong></p></li><li><p>为啥我的 explain extended 只有固定的几行?</p><ul><li><strong>因为这个 count 没有调动 MR, 用 sum 就会启用 MR, 会出现长长的 log</strong></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">---不会启用 MR</span></span><br><span class="line">hive (mydb)&gt; explain EXTENDED select count(1) from t9;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line"><span class="keyword">Explain</span></span><br><span class="line">STAGE DEPENDENCIES:</span><br><span class="line">  Stage<span class="number">-0</span> <span class="keyword">is</span> a root stage</span><br><span class="line">  ...</span><br><span class="line"><span class="comment">-----------------------------</span></span><br><span class="line"><span class="comment">---这里会启用 MR</span></span><br><span class="line">hive (mydb)&gt; <span class="keyword">explain</span> <span class="keyword">EXTENDED</span> <span class="keyword">select</span> <span class="keyword">sum</span>(<span class="keyword">id</span>) <span class="keyword">from</span> t9;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line"><span class="keyword">Explain</span></span><br><span class="line">STAGE DEPENDENCIES:</span><br><span class="line">  Stage<span class="number">-1</span> <span class="keyword">is</span> a root stage</span><br><span class="line">  Stage<span class="number">-0</span> depends <span class="keyword">on</span> stages: Stage<span class="number">-1</span></span><br><span class="line"></span><br><span class="line">STAGE PLANS:</span><br><span class="line">  Stage: Stage<span class="number">-1</span></span><br><span class="line">  ....</span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="15-3-性能调优"><a href="#15-3-性能调优" class="headerlink" title="15.3 性能调优"></a>15.3 性能调优</h3><h4 id="15-3-1-本地mr"><a href="#15-3-1-本地mr" class="headerlink" title="15.3.1 本地mr"></a>15.3.1 本地mr</h4><ul><li><p><strong>本地模式设置方式：</strong></p><ul><li><code>set mapred.job.tracker=local;</code><ul><li>mapper 的本地模式, <strong>只有在开发中才会使用</strong></li></ul></li><li><strong><code>set hive.exec.mode.local.auto=true;</code></strong><ul><li><strong>Hive 的执行模式</strong></li><li><strong>可以用在生产中</strong>, 因为是自动模式, 可根据参数变化</li><li><strong>设置这里才会转成 local  hadoop</strong></li><li>按照这里设定的规则<code>hive.exec.mode.local.auto.input.files.max</code></li></ul></li><li>测试 select 1 from wlan limit 5;</li></ul></li><li><p><strong>下面两个参数是local mr中常用的控制参数:</strong></p><ul><li><p><code>hive.exec.mode.local.auto.inputbytes.max</code>默认134217728</p><ul><li><strong>设置local mr的最大输入数据量</strong>,当输入数据量小于这个值的时候会采用local  mr的方式</li></ul></li><li><p><strong><code>hive.exec.mode.local.auto.input.files.max</code>默认是4</strong></p><ul><li><p>设置local mr的最大输入文件个数,当输入文件个数小于这个值的时候会采用local mr的方式</p></li><li><p>大于此数时, 就不会转化为 local hadoop</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Cannot run job locally: Number of Input Files (= 6) is larger than hive.exec.mode.local.auto.input.files.max(= 4)</span><br></pre></td></tr></table></figure></li><li><p><strong>可以这样修改local mr的最大输入文件个数值, 主要在调试阶段使用</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (mydb)&gt; set hive.exec.mode.local.auto.input.files.max=8;</span><br><span class="line"></span><br><span class="line"># 这样设置了之后, 只要文件数&lt;=8, 就会在本地运行</span><br><span class="line">Job running in-process (local Hadoop)</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul><h4 id="15-3-2-开启并行计算"><a href="#15-3-2-开启并行计算" class="headerlink" title="15.3.2 开启并行计算"></a>15.3.2 开启并行计算</h4><ul><li>开启并行计算,增加集群的利用率<ul><li><code>set hive.exec.parallel=true;</code></li></ul></li></ul><h4 id="15-3-3-设置严格模式"><a href="#15-3-3-设置严格模式" class="headerlink" title="15.3.3 设置严格模式"></a>15.3.3 设置严格模式</h4><ul><li><strong>设置严格模式</strong><ul><li><code>set hive.mapred.mode=strict;</code></li></ul></li><li>设置非严格模式<ul><li><code>set hive.mapred.mode=nonstrict;</code></li></ul></li><li><strong>strict可以禁止三种类型的查询：</strong><ul><li>强制分区表的where条件过滤</li><li>Order by语句必须使用limit<ul><li><code>hive (mydb)&gt; select id from t9 where class=&#39;job110&#39; order by id limit 3;</code></li></ul></li><li>限制笛卡尔积查询</li></ul></li></ul><h4 id="15-3-4-调整mapper和reducer的数量"><a href="#15-3-4-调整mapper和reducer的数量" class="headerlink" title="15.3.4 调整mapper和reducer的数量"></a>15.3.4 调整mapper和reducer的数量</h4><ul><li>调整mapper和reducer的数量<ul><li>太多map导致启动产生过多开销</li><li><code>marpred.min.split.size</code></li><li>按照输入数据量大小确定reducer数目,<ul><li>`set mapred.reduce.tasks=  默认3</li><li>dfs -count  /分区目录/* </li><li>hive.exec.reducers.max设置阻止资源过度消耗</li></ul></li></ul></li><li>JVM重用<ul><li>小文件多或task多的业务场景</li><li>set mapred.job.reuse.jvm.num.task=10</li><li>会一直占用task槽</li></ul></li></ul><h4 id="15-3-5-排序方面的优化"><a href="#15-3-5-排序方面的优化" class="headerlink" title="15.3.5 排序方面的优化"></a>15.3.5 排序方面的优化</h4><ul><li><p><strong>order by</strong> 语句：是<strong>全局</strong>排序, 用的比较多</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 加个 desc 就是倒序排序</span><br><span class="line">hive (mydb)&gt; select id from bucket_table order by id desc limit 10;</span><br><span class="line">Automatically selecting local only mode for query</span><br></pre></td></tr></table></figure><p> <strong>sort by</strong> 语句：      是<strong>单reduce</strong>排序</p><ul><li>一般与 <code>distribute by</code>结合使用</li></ul></li><li><p><strong>distribute by</strong>语句：类似于分桶，<strong>根据指定的字段</strong>将<strong>数据分到不同的 reducer</strong>，且分发算法是 <strong>hash 散列</strong></p><ul><li><p><strong>与 sort by 结合用的比较多</strong>, <strong>在每个分区内有序</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 注意此处 distribute 的作用</span><br><span class="line">hive (mydb)&gt; select id from bucket_table distribute by id sort by id desc limit 10;</span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>cluster by</strong>语句：</p><ul><li><code>select * from t9 cluster by id;</code></li><li>可以确保类似的数据的分发到同一个reduce task中，并且保证数据有序, 防止所有的数据分发到同一个reduce上，导致整体的job时间延长</li></ul></li><li><p><strong>cluster by语句的等价语句：</strong></p><ul><li><em>如果分桶和sort字段是同一个时，此时，cluster by = distribute by + sort by</em> </li><li><code>distribute by Word sort by Word ASC</code></li></ul></li></ul><h4 id="15-3-6-Map-side聚合"><a href="#15-3-6-Map-side聚合" class="headerlink" title="15.3.6 Map-side聚合"></a>15.3.6 Map-side聚合</h4><p><strong>可以直接在 <code>.hiverc</code>中配置</strong></p><ul><li><code>set hive.map.aggr=true;</code></li><li>这个设置可以将顶层的聚合操作放在Map阶段执行，从而减轻清洗阶段数据传输和Reduce阶段的执行时间，提升总体性能。:TODO 不太懂</li><li>缺点：该设置会消耗更多的内存。</li><li>执行select count(1) from wlan;</li></ul><h4 id="15-3-6-Join-优化"><a href="#15-3-6-Join-优化" class="headerlink" title="15.3.6 Join 优化"></a>15.3.6 Join 优化</h4><ul><li>优先过滤后再进行 Join 操作，最大限度的减少参与 join 的数据量</li><li>小表 join 大表，最好启动 mapjoin<ul><li>在使用写有 Join 操作的查询语句时有一条原则:应该将条目少的表/子查询放在 Join 操作 符的左边。 </li></ul></li><li>Join on 的条件相同的话，最好放入同一个 job，并且 join 表的排列顺序从小到大</li></ul><blockquote><p>在编写 Join 查询语句时，如果确定是由于 join 出现的数据倾斜，那么请做如下设置:</p><p>set hive.skewjoin.key=100000; // 这个是 join 的键对应的记录条数超过这个值则会进行</p><p>分拆，值根据具体数据量设置</p><p>set hive.optimize.skewjoin=true; // 如果是 join 过程出现倾斜应该设置为 true</p></blockquote><h1 id="16-表连接-只支持等值连接"><a href="#16-表连接-只支持等值连接" class="headerlink" title="16. 表连接  (只支持等值连接)"></a>16. 表连接  (只支持等值连接)</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-16-071623.jpg" alt=""></p><h3 id="16-1-简介"><a href="#16-1-简介" class="headerlink" title="16.1 简介"></a>16.1 简介</h3><ul><li><p>INNER JOIN</p><ul><li>两张表中都有，且两表符合连接条件</li><li>select t1.name,t1.age,t9.age from t9 join t1 on t1.name=t9.name;</li></ul></li><li><p><strong>LEFT OUTER JOIN</strong></p><ul><li>左表中符合where条件出现，右表可以为空</li><li><strong>从左表返回所有的行(字段), 右表没有匹配where 条件的话返回null</strong></li></ul></li><li><p>RIGHT OUTER JOIN</p><ul><li>右表中符合where条件出现，左表可以为空</li></ul></li><li><p>FULL OUTER JOIN</p><ul><li>返回所有表符合where条件的所有记录，没有NULL替代</li></ul></li><li><p><strong>LEFT SEMI-JOIN</strong></p><ul><li><p>左表中符合右表on条件出现，右表不出现</p></li><li><p>Hive 当前<strong>没有实现 IN/EXISTS 子查询，可以用 LEFT SEMI JOIN 重写子查询语句</strong>。LEFT SEMI JOIN 的限制是， JOIN 子句中右边的表只能在 ON 子句中设置过滤条件，在 WHERE 子句、SELECT 子句或其他地方过滤都不行。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> a.key, a.value <span class="keyword">FROM</span> a</span><br><span class="line"><span class="keyword">WHERE</span> a.key <span class="keyword">in</span> (<span class="keyword">SELECT</span> b.key <span class="keyword">FROM</span> B);</span><br><span class="line"></span><br><span class="line"># 可以被重写为：</span><br><span class="line"><span class="keyword">SELECT</span> a.key, a.val <span class="keyword">FROM</span> a <span class="keyword">LEFT</span> SEMI <span class="keyword">JOIN</span> b <span class="keyword">on</span> (a.key = b.key)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>笛卡尔积</p><ul><li>是m x n的结果</li></ul></li><li><p><strong>map-side JOIN</strong></p><ul><li>只有一张小表，在mapper的时候将小表完全放在内存中</li><li>select /<em>+ mapjoin(t9) </em>/t1.name,t1.age from t9 JOIN t1on t1.name=t9.name;</li></ul></li></ul><h3 id="16-2-代码测试"><a href="#16-2-代码测试" class="headerlink" title="16.2  代码测试"></a>16.2  代码测试</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line">关于hive中的各种join</span><br><span class="line"></span><br><span class="line">准备数据</span><br><span class="line">1,a</span><br><span class="line">2,b</span><br><span class="line">3,c</span><br><span class="line">4,d</span><br><span class="line">7,y</span><br><span class="line">8,u</span><br><span class="line"></span><br><span class="line">2,bb</span><br><span class="line">3,cc</span><br><span class="line">7,yy</span><br><span class="line">9,pp</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">建表：</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> a(<span class="keyword">id</span> <span class="built_in">int</span>,<span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> b(<span class="keyword">id</span> <span class="built_in">int</span>,<span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br><span class="line"></span><br><span class="line">导入数据：</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/a.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> a;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/b.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> b;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">实验：</span><br><span class="line">** inner join</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> a <span class="keyword">inner</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.id=b.id;</span><br><span class="line">+<span class="comment">-------+---------+-------+---------+--+</span></span><br><span class="line">| a.id  | a.name  | b.id  | b.name  |</span><br><span class="line">+<span class="comment">-------+---------+-------+---------+--+</span></span><br><span class="line">| 2     | b       | 2     | bb      |</span><br><span class="line">| 3     | c       | 3     | cc      |</span><br><span class="line">| 7     | y       | 7     | yy      |</span><br><span class="line">+<span class="comment">-------+---------+-------+---------+--+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**left join</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> a <span class="keyword">left</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.id=b.id;</span><br><span class="line">+<span class="comment">-------+---------+-------+---------+--+</span></span><br><span class="line">| a.id  | a.name  | b.id  | b.name  |</span><br><span class="line">+<span class="comment">-------+---------+-------+---------+--+</span></span><br><span class="line">| 1     | a       | NULL  | NULL    |</span><br><span class="line">| 2     | b       | 2     | bb      |</span><br><span class="line">| 3     | c       | 3     | cc      |</span><br><span class="line">| 4     | d       | NULL  | NULL    |</span><br><span class="line">| 7     | y       | 7     | yy      |</span><br><span class="line">| 8     | u       | NULL  | NULL    |</span><br><span class="line">+<span class="comment">-------+---------+-------+---------+--+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**right join</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> a <span class="keyword">right</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.id=b.id;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> a <span class="keyword">full</span> <span class="keyword">outer</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.id=b.id;</span><br><span class="line">+<span class="comment">-------+---------+-------+---------+--+</span></span><br><span class="line">| a.id  | a.name  | b.id  | b.name  |</span><br><span class="line">+<span class="comment">-------+---------+-------+---------+--+</span></span><br><span class="line">| 1     | a       | NULL  | NULL    |</span><br><span class="line">| 2     | b       | 2     | bb      |</span><br><span class="line">| 3     | c       | 3     | cc      |</span><br><span class="line">| 4     | d       | NULL  | NULL    |</span><br><span class="line">| 7     | y       | 7     | yy      |</span><br><span class="line">| 8     | u       | NULL  | NULL    |</span><br><span class="line">| NULL  | NULL    | 9     | pp      |</span><br><span class="line">+<span class="comment">-------+---------+-------+---------+--+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> a <span class="keyword">left</span> semi <span class="keyword">join</span> b <span class="keyword">on</span> a.id = b.id;</span><br><span class="line">+<span class="comment">-------+---------+--+</span></span><br><span class="line">| a.id  | a.name  |</span><br><span class="line">+<span class="comment">-------+---------+--+</span></span><br><span class="line">| 2     | b       |</span><br><span class="line">| 3     | c       |</span><br><span class="line">| 7     | y       |</span><br><span class="line">+<span class="comment">-------+---------+--+</span></span><br></pre></td></tr></table></figure><p><br></p><h1 id="17-Hive自定义函数-amp-Transform"><a href="#17-Hive自定义函数-amp-Transform" class="headerlink" title="17.  Hive自定义函数 &amp; Transform"></a>17.  Hive自定义函数 &amp; Transform</h1><h3 id="17-1-自定义函数类别"><a href="#17-1-自定义函数类别" class="headerlink" title="17.1 自定义函数类别"></a>17.1 自定义函数类别</h3><ul><li><strong>UDF</strong>  <ul><li>作用于单个数据行，产生一个数据行作为输出。（数学函数，字符串函数）</li></ul></li><li><strong>UDAF</strong><ul><li>(用户定义聚集函数)：接收多个输入数据行，并产生一个输出数据行。（count，max）</li></ul></li></ul><h3 id="17-2-UDF-开发实例"><a href="#17-2-UDF-开发实例" class="headerlink" title="17.2 UDF 开发实例"></a>17.2 UDF 开发实例</h3><h4 id="17-2-1-简单入门"><a href="#17-2-1-简单入门" class="headerlink" title="17.2.1 简单入门"></a>17.2.1 简单入门</h4><ul><li><p>先开发一个java类，继承UDF，并重载evaluate方法 </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.hive.udf;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ToLowerCase</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 必须是 public</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">evaluate</span><span class="params">(String field)</span> </span>&#123;</span><br><span class="line">String res = field.toLowerCase();</span><br><span class="line"><span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>打成jar包上传到服务器 </p></li><li><p>将jar包添加到hive的classpath </p><ul><li><code>hive&gt;add JAR /home/ap/udf.jar;</code></li></ul></li><li><p>创建临时函数与开发好的java class关联 </p><ul><li><code>Hive&gt;create temporary function tolowercase as &#39;com.rox.hive.udf.ToLowerCase&#39;;</code></li></ul></li><li><p>即可在hql中使用自定义的函数strip  </p><ul><li><code>select tolowercase(name) from t_1..</code></li></ul></li></ul><h4 id="17-2-2-稍稍复杂"><a href="#17-2-2-稍稍复杂" class="headerlink" title="17.2.2 稍稍复杂"></a>17.2.2 稍稍复杂</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"># 需求: 通过一些手机号判断手机区域</span><br><span class="line">1364535532,10</span><br><span class="line">1374535532,42</span><br><span class="line">1384535532,34</span><br><span class="line">1364535532,45</span><br><span class="line">1384535532,22</span><br><span class="line">136-beijing</span><br><span class="line">137-shanghai</span><br><span class="line">138-guangzhou</span><br><span class="line"><span class="comment">-----</span></span><br><span class="line"></span><br><span class="line">## 1.编写 UDF</span><br><span class="line">public static HashMap&lt;String, String&gt; provinceMap = new HashMap&lt;String, String&gt;();</span><br><span class="line">static &#123;</span><br><span class="line">provinceMap.put("136", "beijing");</span><br><span class="line">provinceMap.put("136", "shanghai");</span><br><span class="line">provinceMap.put("136", "guangzhou");</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public String evaluate(String phoneNum) &#123;</span><br><span class="line">return provinceMap.get(phoneNum.substring(0, 3)) == null ? "huoxing" : provinceMap.get(phoneNum.substring(0, 3));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">## 2.打包上传, 添加到 classpath, 创建临时函数</span><br><span class="line"></span><br><span class="line">## 3.创建表,加载数据</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> flow_t(pnum <span class="keyword">string</span>,flow <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/ap/ihivedata/flow.tmp'</span> <span class="keyword">into</span> <span class="keyword">table</span> flow_t;</span><br><span class="line"></span><br><span class="line">## 4.使用</span><br><span class="line">0: jdbc:hive2://cs2:10000&gt; select pnum,tolow(pnum),flow from flow_t;</span><br><span class="line">+<span class="comment">-------------+------------+-------+</span></span><br><span class="line">|    pnum     |    _c1     | flow  |</span><br><span class="line">+<span class="comment">-------------+------------+-------+</span></span><br><span class="line">| 1364535532  | guangzhou  | 10    |</span><br><span class="line">| 1374535532  | huoxing    | 42    |</span><br><span class="line">| 1384535532  | huoxing    | 34    |</span><br><span class="line">| 1364535532  | guangzhou  | 45    |</span><br><span class="line">| 1384535532  | huoxing    | 22    |</span><br><span class="line">+<span class="comment">-------------+------------+-------+</span></span><br></pre></td></tr></table></figure><h4 id="17-2-3-有些复杂"><a href="#17-2-3-有些复杂" class="headerlink" title="17.2.3  有些复杂"></a>17.2.3  有些复杂</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"># 解析 json 数据表 rating.json</span><br><span class="line">## 1. 写 udf</span><br><span class="line"></span><br><span class="line"><span class="comment">// com.rox.json.MovieRateBean </span></span><br><span class="line"><span class="keyword">package</span> com.rox.json;</span><br><span class="line"><span class="keyword">import</span> lombok.Getter;</span><br><span class="line"><span class="keyword">import</span> lombok.Setter;</span><br><span class="line"><span class="meta">@Getter</span></span><br><span class="line"><span class="meta">@Setter</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MovieRateBean</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> String movie;</span><br><span class="line"><span class="keyword">private</span> String rate;</span><br><span class="line"><span class="keyword">private</span> String timeStamp;</span><br><span class="line"><span class="keyword">private</span> String uid;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> movie + <span class="string">"\t"</span> + rate + <span class="string">"\t"</span></span><br><span class="line">+ timeStamp + <span class="string">"\t"</span> + uid;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// com.rox.json.JsonParser </span></span><br><span class="line"><span class="keyword">package</span> com.rox.json;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"><span class="keyword">import</span> com.fasterxml.jackson.databind.ObjectMapper;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JsonParser</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">evaluate</span><span class="params">(String jsonLine)</span> </span>&#123;</span><br><span class="line">ObjectMapper objectMapper = <span class="keyword">new</span> ObjectMapper();</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">MovieRateBean bean = objectMapper.readValue(jsonLine, MovieRateBean.class);</span><br><span class="line"><span class="keyword">return</span> bean.toString();</span><br><span class="line">&#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="string">""</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">====================================================</span><br><span class="line">## 2.打包上传, 添加到 classpath, 创建临时函数, 检查是否成功</span><br><span class="line">show functions;</span><br><span class="line"></span><br><span class="line">## 3.创建表,加载数据</span><br><span class="line"><span class="function">create table <span class="title">t_json</span><span class="params">(line string)</span> row format delimited</span>;</span><br><span class="line"></span><br><span class="line">load data local inpath <span class="string">'/home/ap/ihivedata/flow.log'</span> into table t_json;</span><br><span class="line"></span><br><span class="line">## 4.检查数据</span><br><span class="line">select * from t_json limit <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line">## 5.调用函数</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="comment">//cs2:10000&gt; select jsonparser(line)parsedline  from t_json limit 10;</span></span><br><span class="line">+---------------------+</span><br><span class="line">|     parsedline      |</span><br><span class="line">+---------------------+</span><br><span class="line">| <span class="number">1193</span><span class="number">5</span><span class="number">978300760</span><span class="number">1</span>  |</span><br><span class="line">| <span class="number">661</span><span class="number">3</span><span class="number">978302109</span><span class="number">1</span>   |</span><br><span class="line">| <span class="number">914</span><span class="number">3</span><span class="number">978301968</span><span class="number">1</span>   |</span><br><span class="line">| <span class="number">3408</span><span class="number">4</span><span class="number">978300275</span><span class="number">1</span>  |</span><br><span class="line">| <span class="number">2355</span><span class="number">5</span><span class="number">978824291</span><span class="number">1</span>  |</span><br><span class="line">| <span class="number">1197</span><span class="number">3</span><span class="number">978302268</span><span class="number">1</span>  |</span><br><span class="line">| <span class="number">1287</span><span class="number">5</span><span class="number">978302039</span><span class="number">1</span>  |</span><br><span class="line">| <span class="number">2804</span><span class="number">5</span><span class="number">978300719</span><span class="number">1</span>  |</span><br><span class="line">| <span class="number">594</span><span class="number">4</span><span class="number">978302268</span><span class="number">1</span>   |</span><br><span class="line">| <span class="number">919</span><span class="number">4</span><span class="number">978301368</span><span class="number">1</span>   |</span><br><span class="line">+---------------------+</span><br><span class="line"><span class="comment">// 但是这样只是把每一行解析出来了,</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">## 6.删除原来的表    </span><br><span class="line">drop table <span class="keyword">if</span> exists t_rating;</span><br><span class="line"></span><br><span class="line">## 7.重新创建一个表</span><br><span class="line"><span class="function">create table <span class="title">t_rating</span><span class="params">(movieid string,rate <span class="keyword">int</span>,timestring string,uid string)</span></span></span><br><span class="line"><span class="function">row format delimited fields terminated by '\t'</span>;</span><br><span class="line"></span><br><span class="line">## 8.根据查出来的每一行, 按照 '\t'分割, 然后再插入到表中</span><br><span class="line">create table  t_rating as</span><br><span class="line">select </span><br><span class="line">split(jsonparser(line),<span class="string">'\t'</span>)[<span class="number">0</span>]as movieid, split(jsonparser(line),<span class="string">'\t'</span>)[<span class="number">1</span>] as rate, split(jsonparser(line),<span class="string">'\t'</span>)[<span class="number">2</span>] as timestring, split(jsonparser(line),<span class="string">'\t'</span>)[<span class="number">3</span>] as uid   </span><br><span class="line">from t_json limit <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 但是执行结果会报错,不知道为啥,难道是 java 代码的问题? :TODO</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">-------</span><br><span class="line">## 9.内置json函数</span><br><span class="line"><span class="function">select <span class="title">get_json_object</span><span class="params">(line,<span class="string">'$.movie'</span>)</span> as moive,<span class="title">get_json_object</span><span class="params">(line,<span class="string">'$.rate'</span>)</span> as rate  from rat_json limit 10</span>;</span><br></pre></td></tr></table></figure><h4 id="17-3-Transform实现"><a href="#17-3-Transform实现" class="headerlink" title="17.3 Transform实现"></a>17.3 Transform实现</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"># 1、先加载rating.json文件到hive的一个原始表 t_json</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> rat_json(line <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/ap/rating.json'</span> <span class="keyword">into</span> <span class="keyword">table</span> t_json;</span><br><span class="line"></span><br><span class="line">2、需要解析json数据成四个字段，插入一张新的表 t_rating</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> t_rating;</span><br><span class="line"># 创建表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_rating(movieid <span class="keyword">string</span>,rate <span class="built_in">int</span>,timestring <span class="keyword">string</span>,uid <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"># 插入, 也可以直接创建  create table xx as + select...</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> t_rating</span><br><span class="line"><span class="keyword">select</span> get_json_object(line,<span class="string">'$.movie'</span>) <span class="keyword">as</span> moiveid, get_json_object(line,<span class="string">'$.rate'</span>) <span class="keyword">as</span> rate, get_json_object(line,<span class="string">'$.timeStamp'</span>) <span class="keyword">as</span> timestring,</span><br><span class="line">get_json_object(line,<span class="string">'$.uid'</span>) <span class="keyword">as</span> uid </span><br><span class="line"><span class="keyword">from</span> t_json;</span><br><span class="line"></span><br><span class="line">3. 写一个 python 脚本</span><br><span class="line">vi weekday_mapper.py</span><br><span class="line"></span><br><span class="line">#!/bin/python</span><br><span class="line">import sys</span><br><span class="line">import datetime</span><br><span class="line"></span><br><span class="line">for line in sys.stdin:</span><br><span class="line">  line = line.strip()</span><br><span class="line">  movieid, rating, timestring,userid = line.split('\t')</span><br><span class="line">  weekday = datetime.datetime.fromtimestamp(float(timestring)).isoweekday()</span><br><span class="line">  print '\t'.join([movieid, rating, str(weekday),userid])</span><br><span class="line">  </span><br><span class="line">4. 保存文件, 然后将文件加入 hive 的 classpath</span><br><span class="line">hive&gt;add FILE /home/hadoop/weekday_mapper.py;</span><br><span class="line"></span><br><span class="line">5. 此时可以直接创建新表</span><br><span class="line">hive&gt;create TABLE u_data_new as</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  TRANSFORM (movieid, rate, timestring,uid)</span><br><span class="line">  <span class="keyword">USING</span> <span class="string">'python weekday_mapper.py'</span></span><br><span class="line">  <span class="keyword">AS</span> (movieid, rate, <span class="keyword">weekday</span>,uid)</span><br><span class="line"><span class="keyword">FROM</span> t_rating;</span><br><span class="line"></span><br><span class="line">6. 查询结果</span><br><span class="line">## distinct看看有多少个不重复的数字</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">distinct</span>(<span class="keyword">weekday</span>) <span class="keyword">from</span> u_data_new <span class="keyword">limit</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure><h1 id="18-案例"><a href="#18-案例" class="headerlink" title="18. 案例"></a>18. 案例</h1><h3 id="18-1-广告推送-用户画像的介绍"><a href="#18-1-广告推送-用户画像的介绍" class="headerlink" title="18.1 广告推送-用户画像的介绍"></a>18.1 广告推送-用户画像的介绍</h3><p><strong>一个广告推送平台的项目结构示意图</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-17-071714.png" alt="image-20180617151714445"></p><h3 id="18-2-累计报表实现套路-面试"><a href="#18-2-累计报表实现套路-面试" class="headerlink" title="18.2 累计报表实现套路(面试)"></a>18.2 累计报表实现套路(面试)</h3><p><strong>题:  求每个月的累计访问次数</strong></p><p><strong>此处的访问次数可以换成工资,等等..</strong></p><p><strong>有如下访客访问次数统计表 t_access_times</strong></p><table><thead><tr><th>访客</th><th>月份</th><th>访问次数</th></tr></thead><tbody><tr><td>A</td><td>2015-01</td><td>5</td></tr><tr><td>A</td><td>2015-01</td><td>15</td></tr><tr><td>B</td><td>2015-01</td><td>5</td></tr><tr><td>A</td><td>2015-01</td><td>8</td></tr><tr><td>B</td><td>2015-01</td><td>25</td></tr><tr><td>A</td><td>2015-01</td><td>5</td></tr><tr><td>A</td><td>2015-02</td><td>4</td></tr><tr><td>A</td><td>2015-02</td><td>6</td></tr><tr><td>B</td><td>2015-02</td><td>10</td></tr><tr><td>B</td><td>2015-02</td><td>5</td></tr><tr><td>……</td><td>……</td><td>……</td></tr></tbody></table><p><strong>需要输出报表：t_access_times_accumulate</strong></p><table><thead><tr><th>访客</th><th>月份</th><th>月访问总计</th><th>累计访问总计</th></tr></thead><tbody><tr><td>A</td><td>2015-01</td><td>33</td><td>33</td></tr><tr><td>A</td><td>2015-02</td><td>10</td><td>43</td></tr><tr><td>…….</td><td>…….</td><td>…….</td><td>…….</td></tr><tr><td>B</td><td>2015-01</td><td>30</td><td>30</td></tr><tr><td>B</td><td>2015-02</td><td>15</td><td>45</td></tr><tr><td>…….</td><td>…….</td><td>…….</td><td>…….</td></tr></tbody></table><h5 id="解题代码"><a href="#解题代码" class="headerlink" title="解题代码"></a>解题代码</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 准备数据</span><br><span class="line">vi t_access_times</span><br><span class="line"></span><br><span class="line">A,2015-01,5</span><br><span class="line">A,2015-01,15</span><br><span class="line">B,2015-01,5</span><br><span class="line">A,2015-01,8</span><br><span class="line">B,2015-01,254</span><br><span class="line">A,2015-01,5</span><br><span class="line">A,2015-02,4</span><br><span class="line">A,2015-02,6</span><br><span class="line">B,2015-02,10</span><br><span class="line">B,2015-02,5</span><br><span class="line"><span class="comment">----</span></span><br><span class="line"></span><br><span class="line"># 建表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_access_times(username <span class="keyword">string</span>,<span class="keyword">month</span> <span class="keyword">string</span>,salary <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br><span class="line"></span><br><span class="line"># 加载数据</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/ap/t_access_times'</span> <span class="keyword">into</span> <span class="keyword">table</span> t_access_times;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">1、第一步，先求个用户的月总金额</span><br><span class="line"><span class="keyword">select</span> username,<span class="keyword">month</span>,<span class="keyword">sum</span>(salary) <span class="keyword">as</span> salary <span class="keyword">from</span> t_access_times <span class="keyword">group</span> <span class="keyword">by</span> username,<span class="keyword">month</span>;</span><br><span class="line"></span><br><span class="line">+<span class="comment">-----------+----------+---------+--+</span></span><br><span class="line">| username  |  month   | salary  |</span><br><span class="line">+<span class="comment">-----------+----------+---------+--+</span></span><br><span class="line">| A         | 2015-01  | 33      |</span><br><span class="line">| A         | 2015-02  | 10      |</span><br><span class="line">| B         | 2015-01  | 30      |</span><br><span class="line">| B         | 2015-02  | 15      |</span><br><span class="line">+<span class="comment">-----------+----------+---------+--+</span></span><br><span class="line"></span><br><span class="line">2、第二步，将月总金额表 自己连接 自己连接</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> aa <span class="keyword">as</span></span><br><span class="line">(<span class="keyword">select</span> username,<span class="keyword">month</span>,<span class="keyword">sum</span>(salary) <span class="keyword">as</span> salary <span class="keyword">from</span> t_access_times <span class="keyword">group</span> <span class="keyword">by</span> username,<span class="keyword">month</span>) A </span><br><span class="line"><span class="keyword">inner</span> <span class="keyword">join</span> </span><br><span class="line">(<span class="keyword">select</span> username,<span class="keyword">month</span>,<span class="keyword">sum</span>(salary) <span class="keyword">as</span> salary <span class="keyword">from</span> t_access_times <span class="keyword">group</span> <span class="keyword">by</span> username,<span class="keyword">month</span>) B</span><br><span class="line">A.username=B.username</span><br><span class="line"></span><br><span class="line">+<span class="comment">-------------+----------+-----------+-------------+----------+-----------+--+</span></span><br><span class="line">| a.username  | a.month  | a.salary  | b.username  | b.month  | b.salary  |</span><br><span class="line">+<span class="comment">-------------+----------+-----------+-------------+----------+-----------+--+</span></span><br><span class="line">| A           | <span class="number">2015</span><span class="number">-01</span>  | <span class="number">33</span>        | A           | <span class="number">2015</span><span class="number">-01</span>  | <span class="number">33</span>        |</span><br><span class="line">| A           | <span class="number">2015</span><span class="number">-01</span>  | <span class="number">33</span>        | A           | <span class="number">2015</span><span class="number">-02</span>  | <span class="number">10</span>        |</span><br><span class="line">| A           | <span class="number">2015</span><span class="number">-02</span>  | <span class="number">10</span>        | A           | <span class="number">2015</span><span class="number">-01</span>  | <span class="number">33</span>        |</span><br><span class="line">| A           | <span class="number">2015</span><span class="number">-02</span>  | <span class="number">10</span>        | A           | <span class="number">2015</span><span class="number">-02</span>  | <span class="number">10</span>        |</span><br><span class="line">| B           | <span class="number">2015</span><span class="number">-01</span>  | <span class="number">30</span>        | B           | <span class="number">2015</span><span class="number">-01</span>  | <span class="number">30</span>        |</span><br><span class="line">| B           | <span class="number">2015</span><span class="number">-01</span>  | <span class="number">30</span>        | B           | <span class="number">2015</span><span class="number">-02</span>  | <span class="number">15</span>        |</span><br><span class="line">| B           | <span class="number">2015</span><span class="number">-02</span>  | <span class="number">15</span>        | B           | <span class="number">2015</span><span class="number">-01</span>  | <span class="number">30</span>        |</span><br><span class="line">| B           | <span class="number">2015</span><span class="number">-02</span>  | <span class="number">15</span>        | B           | <span class="number">2015</span><span class="number">-02</span>  | <span class="number">15</span>        |</span><br><span class="line">+<span class="comment">-------------+----------+-----------+-------------+----------+-----------+--+</span></span><br><span class="line"></span><br><span class="line"><span class="number">3</span>、第三步，从上一步的结果中</span><br><span class="line">进行分组查询，分组的字段是a.username a.month</span><br><span class="line">求月累计值：  将b.month &lt;= a.month的所有b.salary求和即可</span><br><span class="line"><span class="keyword">select</span> A.username,A.month,<span class="keyword">max</span>(A.salary) <span class="keyword">as</span> salary,<span class="keyword">sum</span>(B.salary) <span class="keyword">as</span> accumulate</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">(<span class="keyword">select</span> username,<span class="keyword">month</span>,<span class="keyword">sum</span>(salary) <span class="keyword">as</span> salary <span class="keyword">from</span> t_access_times <span class="keyword">group</span> <span class="keyword">by</span> username,<span class="keyword">month</span>) A </span><br><span class="line"><span class="keyword">inner</span> <span class="keyword">join</span> </span><br><span class="line">(<span class="keyword">select</span> username,<span class="keyword">month</span>,<span class="keyword">sum</span>(salary) <span class="keyword">as</span> salary <span class="keyword">from</span> t_access_times <span class="keyword">group</span> <span class="keyword">by</span> username,<span class="keyword">month</span>) B</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">A.username=B.username</span><br><span class="line"><span class="keyword">where</span> B.month &lt;= A.month</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> A.username,A.month</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> A.username,A.month;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">+<span class="comment">-------------+----------+---------+-------------+</span></span><br><span class="line">| a.username  | a.month  | salary  | accumulate  |</span><br><span class="line">+<span class="comment">-------------+----------+---------+-------------+</span></span><br><span class="line">| A           | 2015-01  | 33      | 33          |</span><br><span class="line">| A           | 2015-02  | 10      | 43          |</span><br><span class="line">| B           | 2015-01  | 259     | 259         |</span><br><span class="line">| B           | 2015-02  | 15      | 274         |</span><br><span class="line">+<span class="comment">-------------+----------+---------+-------------+</span></span><br></pre></td></tr></table></figure><h3 id="18-3-待做项目"><a href="#18-3-待做项目" class="headerlink" title="18.3 待做项目"></a>18.3 待做项目</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-17-082606.png" alt="image-20180617162606289"></p><h1 id="19-注意点"><a href="#19-注意点" class="headerlink" title="19.注意点"></a>19.注意点</h1><ol><li><strong>使用聚合函数, 后面一定要分组(group by xxx),  group by 会自动去重</strong><ul><li>如果 sql 语句中有 group by, 那么 select 后面必须有 group by 的字段, 或聚合函数</li></ul></li><li><strong>使用order by 排序某个字段时, 必须在 select中出现此字段, 否则会找不到 :TODO</strong><ul><li>待验证</li></ul></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Hive-初探&quot;&gt;&lt;a href=&quot;#1-Hive-初探&quot; class=&quot;headerlink&quot; title=&quot;1.  Hive 初探&quot;&gt;&lt;/a&gt;1.  Hive 初探&lt;/h1&gt;&lt;h3 id=&quot;1-1-Hive-的数据存储&quot;&gt;&lt;a href=&quot;#1-1-Hiv
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/categories/Hadoop/Hive/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce-简单总结</title>
    <link href="https://airpoet.github.io/2018/06/12/Hadoop/2-MapReduce/MapReduce-%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93/"/>
    <id>https://airpoet.github.io/2018/06/12/Hadoop/2-MapReduce/MapReduce-简单总结/</id>
    <published>2018-06-12T07:03:29.967Z</published>
    <updated>2018-06-17T09:27:08.821Z</updated>
    
    <content type="html"><![CDATA[<p>MapReduce在编程的时候，基本上一个固化的模式，没有太多可灵活改变的地方，<strong>除了以下几处</strong>：</p><ul><li><h4 id="输入数据接口：InputFormat"><a href="#输入数据接口：InputFormat" class="headerlink" title="输入数据接口：InputFormat"></a><strong>输入数据接口：InputFormat</strong></h4><ul><li>FileInputFormat  (文件类型数据读取的通用抽象类)  </li><li>DBInputFormat （数据库数据读取的通用抽象类）</li><li><strong>默认使用的实现类是： TextInputFormat</strong>    <ul><li>job.setInputFormatClass(TextInputFormat.class)</li><li>TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回</li></ul></li></ul></li><li><h4 id="逻辑处理接口：-Mapper"><a href="#逻辑处理接口：-Mapper" class="headerlink" title="逻辑处理接口： Mapper"></a><strong>逻辑处理接口： Mapper</strong></h4><ul><li>完全需要用户自己去实现其中  <code>map()   setup()   clean()</code></li></ul></li></ul><ul><li><h4 id="map输出的结果在shuffle阶段会被partition以及sort，此处有两个接口可自定义："><a href="#map输出的结果在shuffle阶段会被partition以及sort，此处有两个接口可自定义：" class="headerlink" title="map输出的结果在shuffle阶段会被partition以及sort，此处有两个接口可自定义："></a><strong>map输出的结果在shuffle阶段会被partition以及sort</strong>，此处有两个接口可自定义：</h4><ul><li><strong>Partitioner</strong><ul><li>有默认实现 <code>HashPartitioner</code>，逻辑是根据<strong>key</strong>和<strong>numReduces</strong>来返回一个分区号:<code>key.hashCode()&amp;Integer.MAXVALUE % numReduces</code></li><li>通常情况下，用默认的这个HashPartitioner就可以，如果业务上有特别的需求，可以自定义</li><li>继承 <code>Partitioner</code>, 重写<code>getPartition</code>方法, 具体见<a href="https://airpoet.github.io/2018/06/07/Hadoop/Study/2-MapReduce/MapReduce%E7%AC%94%E8%AE%B0-2/#%E5%85%B8%E5%9E%8B%E7%9A%84-Partitioner-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0">这里</a></li></ul></li><li><strong>Comparable</strong><ul><li>当我们用自定义的对象作为key来输出时，就必须要实现<code>WritableComparable</code>接口，<strong>override</strong>其中的<code>compareTo()</code>方法, 具体见<a href="https://airpoet.github.io/2018/06/07/Hadoop/Study/2-MapReduce/MapReduce%E7%AC%94%E8%AE%B0-2/#%E5%85%B8%E5%9E%8B%E7%9A%84-Partitioner-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0">这里</a></li></ul></li></ul></li></ul><ul><li><h4 id="reduce端的数据分组比较接口-：-Groupingcomparator"><a href="#reduce端的数据分组比较接口-：-Groupingcomparator" class="headerlink" title="reduce端的数据分组比较接口 ： Groupingcomparator"></a><strong>reduce端的数据分组比较接口 ： Groupingcomparator</strong></h4><ul><li>reduceTask拿到输入数据（一个partition的所有数据）后，首先需要对数据进行分组，其分组的默认原则是key相同，然后对每一组kv数据调用一次reduce()方法，并且将这一组kv中的第一个kv的key作为参数传给reduce的key，将这一组数据的value的迭代器传给reduce()的values参数</li><li>利用上述这个机制，我们可以<strong>实现一个高效的分组取最大值的逻辑：</strong><ul><li>自定义一个bean对象用来封装我们的数据，然后改写其compareTo方法产生倒序排序的效果</li><li>然后自定义一个Groupingcomparator，将bean对象的分组逻辑改成按照我们的业务分组id来分组（比如订单号）</li><li>这样，我们要取的最大值就是reduce()方法中传进来key</li></ul></li></ul></li></ul><hr><ul><li><h4 id="逻辑处理接口：Reducer"><a href="#逻辑处理接口：Reducer" class="headerlink" title="逻辑处理接口：Reducer"></a><strong>逻辑处理接口：Reducer</strong></h4><ul><li>完全需要用户自己去实现其中  <code>reduce()   setup()   clean()</code></li></ul></li><li><h4 id="输出数据接口：-OutputFormat"><a href="#输出数据接口：-OutputFormat" class="headerlink" title="输出数据接口： OutputFormat"></a><strong>输出数据接口： OutputFormat</strong></h4><ul><li>有一系列子类  FileOutputformat  DBoutputFormat  …..</li><li>默认实现类是TextOutputFormat</li><li>功能逻辑是：  <strong>将每一个KV对向目标文本文件中输出为一行</strong></li></ul><p>​    </p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;MapReduce在编程的时候，基本上一个固化的模式，没有太多可灵活改变的地方，&lt;strong&gt;除了以下几处&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;h4 id=&quot;输入数据接口：InputFormat&quot;&gt;&lt;a href=&quot;#输入数据接口：InputFormat&quot; c
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>Hive学习-1-安装</title>
    <link href="https://airpoet.github.io/2018/06/12/Hadoop/3-Hive/Hive%E5%AD%A6%E4%B9%A0-1/"/>
    <id>https://airpoet.github.io/2018/06/12/Hadoop/3-Hive/Hive学习-1/</id>
    <published>2018-06-12T03:24:46.733Z</published>
    <updated>2018-06-28T13:55:35.306Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hive简介"><a href="#Hive简介" class="headerlink" title="Hive简介"></a>Hive简介</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-12-034055.png" alt="image-20180612114055129"></p><h3 id="Hive基本情况"><a href="#Hive基本情况" class="headerlink" title="Hive基本情况"></a>Hive基本情况</h3><p><strong>Hive</strong> 是建立在 <strong>Hadoop</strong> 上的数据仓库基础构架</p><ul><li>由facebook开源，最初用于解决海量结构化的日志数据统 计问题;<ul><li><strong>ETL</strong>(<strong>Extraction-Transformation-Loading</strong>)工具</li></ul></li><li>构建在Hadoop之上的数据仓库;<ul><li>数据计算使用<strong>MR</strong>，数据存储使用<strong>HDFS</strong></li><li>数据库&amp;数据仓库 的区别：<ul><li>概念上<ul><li><strong>数据库</strong>：用于管理精细化数据，一般情况下用于存储结果数据，分库分表进行存储</li><li><strong>数据仓库</strong>：<strong>存储、查询、分析大规模数据</strong> 。更像一个打包的过程，里面存储的数据没有细化区分，粒度较粗</li></ul></li><li>用途上：<ul><li>数据库：OLTP，on line Transation Processing 联机事务处理，增删改</li><li>数据仓库：OLAP，on line analysis Processing 联机事务分析处理，查询，hive不支持删除、修改。 支持插入。</li></ul></li><li>使用上：<ul><li>数据库：标准sql,  hbase: 非标准sql</li><li>数据仓库：方言版的sql， HQL</li></ul></li><li>模式上：<ul><li>数据库：写模式</li><li>数据仓库：读模式</li></ul></li></ul></li></ul></li><li>可以将结构化的数据映射成一张数据库表<ul><li>结构化数据映射成二维表</li><li><strong>将文本中每一行数据映射为数据库的每一条数据</strong></li><li><strong>将文本中每一列数据映射为hive的表字段</strong></li></ul></li><li>提供HQL 查询功能<ul><li>hive query language， 方言版sql</li></ul></li><li><strong>底层数据是存储在HDFS上</strong><ul><li>hive上建的表仅仅相当于<strong>对hdfs上的结构化数据进行映射管理</strong></li><li>hive仅仅是一个管理数据的作用，而<strong>不会存储数据</strong></li><li>hive想要管理hdfs上的数据，就要建立一个关联关系，关联hive上的表和hdfs上的数据路径</li><li>数据是依赖于一个元数据库</li><li>元数据库采用的是关系型数据库， 真实生产中一般使用mysql为hive的元数据库，hive内置默认的元数据库是 derby</li><li><strong>元数据：</strong> <strong>HCalalog</strong> <ul><li>hive中的表和hdfs的<strong>映射关系</strong>，以及<strong>hive表属性</strong>（内部表，外部表，视图）和<strong>字段信息</strong></li><li>元数据一旦修饰，hive的所有映射关系等都没了，就无法使用了</li><li>可与<strong>Pig</strong>、<strong>Presto</strong>等共享 </li></ul></li></ul></li><li>Hive的是<strong>构建在Hadoop之上的数据仓库</strong><ul><li>数据计算使用<strong>MR</strong>，数据存储使用<strong>HDFS</strong> </li></ul></li><li>通常用于进行离线数据处理(采用MapReduce) </li><li><strong>可认为是一个HQL—-&gt;MR的语言翻译器</strong> </li></ul><h3 id="Hive优缺点："><a href="#Hive优缺点：" class="headerlink" title="Hive优缺点："></a>Hive优缺点：</h3><ul><li><p>优点：</p><ul><li>简单，容易上手 <ul><li>提供了类<strong>SQL</strong>查询语言<strong>HQL</strong> </li></ul></li><li>为超大数据集设计的计算/扩展能力 <ul><li><strong>MR</strong>作为计算引擎，<strong>HDFS</strong>作为存储系统 </li></ul></li><li>统一的元数据管理(HCalalog) <ul><li>可与<strong>Pig</strong>、<strong>Presto</strong>等共享 </li></ul></li></ul></li><li><p><strong>缺点</strong></p><ul><li><p><strong>不支持 删除 &amp; 修改</strong>  delete&amp;update，<strong>不支持事务</strong></p><ul><li>因为是基于HDFS</li><li>hive做的最多的是查询</li></ul></li><li><p>Hive的HQL表达的能力有限 </p><ul><li>迭代式算法无法表达 </li><li>有些复杂运算用<strong>HQL</strong>不易表达 </li></ul></li><li><p><strong>Hive效率较低，查询延时高</strong></p><ul><li><strong>Hive</strong>自动生成<strong>MapReduce</strong>作业，通常不够智能 </li><li><strong>HQL</strong>调优困难，粒度较粗 </li><li>可控性差 </li></ul></li></ul></li></ul><h3 id="Hive与传统关系型数据库-RDBMS）对比"><a href="#Hive与传统关系型数据库-RDBMS）对比" class="headerlink" title="Hive与传统关系型数据库(RDBMS）对比"></a>Hive与传统关系型数据库(RDBMS）对比</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-12-082659.png" alt="image-20180612162659151"><br></p><h1 id="Hive的架构"><a href="#Hive的架构" class="headerlink" title="Hive的架构"></a>Hive的架构</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-12-061321.png" alt="image-20180612141321130"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-12-061413.png" alt="image-20180612141413285"></p><ul><li><p><strong>1) 用户接口</strong></p><ul><li><p>CLI：Command Line Interface，即Shell终端命令行，使用 Hive 命令行与 Hive 进行交互，最常用（学习，调试，生产），包括两种运行方式：</p><ul><li><p>hive命令方式：前提必须在hive安装节点上执行</p></li><li><p>hiveserver2方式：hive安装节点将hive启动为一个后台进程，客户机进行连接（类似于启动了一个hive服务端）</p></li></ul></li></ul></li></ul><pre><code>真实生产中常用！- 1) 修改配置文件，允许远程连接;  第一：修改 hdfs-site.xml，加入一条配置信息，启用 webhdfs；  第二：修改 core-site.xml，加入两条配置信息，设置 hadoop的代理用户。- 2) 启动服务进程  - 前台启动：hiveserver2  - 后台启动    - 记录日志：nohup hiveserver2 1&gt;/home/sigeon/hiveserver.log 2&gt;/home/sigeon/hiveserver.err &amp;      0：标准日志输入   1：标准日志输出   2：错误日志输出      如果我没有配置日志的输出路径，日志会生成在当前工作目录，默认的日志名称叫做：      nohup.xxx    - 不记录日志：nohup hiveserver2 1&gt;/dev/null 2&gt;/dev/null &amp;     - [补充：]      - nohup命令：no hang up的缩写，即不挂起，可以在你退出帐户/关闭终端之后继续运行相应的进程。      - 语法：nohup &lt;command&gt; &amp;- 3) 开启beenline客户端并连接：  - 方法一：     - beenline，开启beenline客户端;    - !connect jdbc:&lt;hive2://master:10000&gt;，回车。然后输入用户名和密码，这个用户名是安装 hadoop 集群的用户名  - 方法二：    - beeline -u jdbc:&lt;hive2://master:10000&gt; -n sigeon</code></pre><ul><li><p>JDBC/ODBC：是 Hive 的基于 JDBC 操作提供的客户端，用户（开发员，运维人员）通过 这连接至 Hive server 服务 </p></li><li><p>Web UI：通过浏览器访问 Hive，基本不会使用</p></li></ul><ul><li><p>2) 元数据库：保存元数据，一般会选用关系型数据库（如mysql，Hive 和 MySQL 之间通过 MetaStore 服务交互）</p></li><li><p>3) Thrift服务：Facebook 开发的一个软件框架，可以用来进行可扩展且跨语言的服务的开发， Hive 集成了该服务，能让不同的编程语言调用 Hive 的接口</p></li><li><p>4) 驱动Driver</p><ul><li>a. 解释器：解释器的作用是将 HiveSQL 语句转换为抽象语法树（AST） </li><li>b. 编译器：编译器是将语法树编译为逻辑执行计划 </li><li>c. 优化器：优化器是对逻辑执行计划进行优化 </li><li>d. 执行器：执行器是调用底层的运行框架执行逻辑执行计划 </li></ul></li></ul><p><br></p><h1 id="Hive的数据组织格式"><a href="#Hive的数据组织格式" class="headerlink" title="Hive的数据组织格式"></a>Hive的数据组织格式</h1><ul><li><p>1)库：database</p></li><li><p>2) 表</p><ul><li><p>a. 内部表（管理表：managed_table）</p></li><li><p>b. 外部表（external_table）</p></li><li><p>内部表和外部表区别：</p><ul><li>内部表和外部表是两个相对的概念，不可能有一个表同时是内部表又是外部表；</li><li>内部表删除表的时候会删除原始数据和元数据，而外部表删除表的时候只会删除元数据不会删除原始数据</li><li>一般情况下存储公共数据的表存放为外部表</li><li>大多数情况，他们的区别不明显。如果数据的所有处理都在 Hive 中进行，那么倾向于 选择内部表；但是如果 Hive 和其他工具要针对相同的数据集进行处理，外部表更合适。 </li><li>使用外部表访问存储在 HDFS 上的初始数据，然后通过 Hive 转换数据并存到内部表中，使用外部表的场景是针对一个数据集有多个不同的 Schema。 </li></ul></li><li><p>c. 分区表</p><ul><li>不同于hadoop中的分区，分区表是人为划分的</li><li>hive最终存储海量数据，海量数据查询一定注意避免全表扫描</li><li>查询的时候为了提升我们的查询性能，出现了分区表</li><li>将数据按照用户的业务存储到不同的目录下，在进行数据查询时只会对指定分区下的数据进行扫描<br>一般情况下生产中用日期作为分区字段</li></ul></li><li><p>d. 分桶表</p><ul><li><p>类似于hadoop中的分区，是由程序决定的，只能指定桶的个数（分区的个数）</p></li><li><p>根据hash算法将余数不同的输出到不同的文件中</p></li><li><p>作用：</p><ul><li>1）提升join的性能<br>思考这个问题：select <a href="http://a.id" target="_blank" rel="noopener">a.id</a>,<a href="http://a.name" target="_blank" rel="noopener">a.name</a>,b.addr from a join b on <a href="http://a.id" target="_blank" rel="noopener">a.id</a> = <a href="http://b.id" target="_blank" rel="noopener">b.id</a>;如果 a 表和 b 表已经是 分桶表，而且分桶的字段是 id 字段做这个 join 操作时，还需要全表做笛卡尔积</li><li>2）提升数据样本的抽取效率，直接拿一个桶中的数据作为样本数据</li></ul></li><li><p>分区表和分桶表的区别： </p><p><img src="/var/folders/6l/blvdbwd53hqglz0f09n3sd5c0000gn/T/abnerworks.Typora/image-20180612143606401.png" alt="image-20180612143606401"></p><ul><li>Hive 数据表可以根据某些字段进行分区操作，细化数据管理，可以让部分查询更快。</li><li>同时表和分区也可以进一步被划分为 Buckets，分桶表的原理和 MapReduce 编程中的 HashPartitioner 的原理类似 </li><li>分区和分桶都是细化数据管理，但是分区表是手动添加区分，由于 Hive 是读模式，所以对添加进分区的数据不做模式校验</li><li>分桶表中的数据是按照某些分桶字段进行 hash 散列 形成的多个文件，所以数据的准确性也高很多</li></ul></li></ul></li></ul></li></ul><p><br></p><ul><li><p><strong>3)视图：</strong></p><ul><li>hive中的视图仅仅相当于一个sql语句的别名</li><li><strong>在hive中仅仅存在逻辑视图，不存在物理视图</strong><ul><li>物理视图：讲sql语句的执行结果存在视图中</li><li>逻辑视图： <strong>仅仅是对查询结果的引用</strong></li></ul></li></ul></li><li><p><strong>4)数据存储：</strong></p><ul><li>原始数据中存在HDFS</li><li>元数据存在mysql</li></ul></li></ul><p><br></p><h1 id="Hive安装"><a href="#Hive安装" class="headerlink" title="Hive安装"></a>Hive安装</h1><p><strong>装hive其实不难，主要是安装mysql，解决mysql的权限问题</strong></p><h2 id="MySql安装"><a href="#MySql安装" class="headerlink" title="MySql安装"></a>MySql安装</h2><h3 id="RPM-安装MySQl："><a href="#RPM-安装MySQl：" class="headerlink" title="RPM 安装MySQl："></a>RPM 安装MySQl：</h3><ol><li><p>检查以前是否装过 MySQL </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -qa|grep -i mysql</span><br></pre></td></tr></table></figure></li><li><p>发现有的话就都卸载 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rpm -e --nodeps mysql-libs-5.1.73-5.el6_6.x86_64</span><br><span class="line">rpm -e --nodeps ....</span><br></pre></td></tr></table></figure></li><li><p>删除老版本 mysql 的开发头文件和库 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rm -rf /usr/lib64/mysql</span><br><span class="line"><span class="comment"># 在搜索 my.cnf 文件，有的话就删掉</span></span><br></pre></td></tr></table></figure></li><li><p>上传mysql 安装包到 Linux中，解压</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tar -xvf mysql-5.6.26-1.linux_glibc2.5.x86_64.rpm-bundle.tar</span><br><span class="line"><span class="comment"># 解压出来有这些文件</span></span><br><span class="line">MySQL-devel-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br><span class="line">MySQL-embedded-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br><span class="line">MySQL-server-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br><span class="line">MySQL-shared-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br><span class="line">MySQL-shared-compat-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br><span class="line">MySQL-test-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br></pre></td></tr></table></figure></li><li><p>安装 server &amp; client </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># server</span></span><br><span class="line">rpm -ivh MySQL-server-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br><span class="line"><span class="comment"># client</span></span><br><span class="line">rpm -ivh MySQL-client-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br></pre></td></tr></table></figure></li><li><p>启动Mysql</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service mysql start</span><br></pre></td></tr></table></figure></li><li><p>登录Mysql并改密码，等</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"># 初始密码在这个文件中</span><br><span class="line">cat /root/.mysql_sercert </span><br><span class="line"></span><br><span class="line"># 登录</span><br><span class="line">mysql -uroot -pxxxxx</span><br><span class="line"></span><br><span class="line"># 删除除了`%`之外的其他所有host</span><br><span class="line">use mysql;</span><br><span class="line">select host,user,password from user;</span><br><span class="line">delete from user where host in (&apos;localhost&apos;, &apos;127.0.0.1&apos;,&apos;::1&apos;, ...)</span><br><span class="line"></span><br><span class="line"># 修改密码 </span><br><span class="line">UPDATE user SET Password = PASSWORD(&apos;psd&apos;) WHERE user = &apos;root&apos;;</span><br><span class="line"></span><br><span class="line"># 为`%` 和 `*` 添加远程登录权限 </span><br><span class="line">#注意： 前面的 mysql 登录用户名， 123 是登录密码</span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;psd&apos; WITH GRANT OPTION;</span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;*&apos; IDENTIFIED BY &apos;psd&apos; WITH GRANT OPTION;</span><br><span class="line">FLUSH PRIVILEGES;</span><br><span class="line"></span><br><span class="line"># 退出登录</span><br><span class="line">exit;</span><br><span class="line"></span><br><span class="line"># 此时可以再登录试试</span><br><span class="line">mysql -uroot -ppsd</span><br><span class="line"></span><br><span class="line">======================================</span><br><span class="line"></span><br><span class="line"># 修改字符集为 utf-8</span><br><span class="line"># 新建一个文件</span><br><span class="line">vi /etc/my.cnf  </span><br><span class="line"># 添加以下内容</span><br><span class="line">[client]</span><br><span class="line">default-character-set=utf8</span><br><span class="line"></span><br><span class="line">[mysql]</span><br><span class="line">default-character-set=utf8</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line">character-set-server=utf8</span><br><span class="line"></span><br><span class="line"># 重启mysql</span><br><span class="line">sudo service mysql restart</span><br><span class="line"></span><br><span class="line">======================================</span><br><span class="line"></span><br><span class="line"># 忘记密码，修改密码的方法</span><br><span class="line"># 停止mysql服务的运行</span><br><span class="line">service mysql stop</span><br><span class="line"></span><br><span class="line"># 跳过授权表访问</span><br><span class="line">mysqld_safe --user=mysql --skip-grant-tables --skip-networking &amp; </span><br><span class="line"></span><br><span class="line"># 登录mysql</span><br><span class="line">mysql -u root mysql </span><br><span class="line"></span><br><span class="line"># 接下来可以修改密码了</span><br><span class="line">  ##在mysql5.7以下的版本如下：</span><br><span class="line">mysql&gt; UPDATE user SET Password=PASSWORD(&apos;newpassword&apos;) where USER=&apos;root’；</span><br><span class="line">  ##在mysql5.7版本如下：</span><br><span class="line">update mysql.user set authentication_string=password(&apos;newpassword&apos;) ;</span><br><span class="line"></span><br><span class="line"># 修改完了重启</span><br><span class="line">service mysql restart</span><br><span class="line"></span><br><span class="line">======================================</span><br></pre></td></tr></table></figure></li></ol><h4 id="MySql的其它错误"><a href="#MySql的其它错误" class="headerlink" title="MySql的其它错误"></a>MySql的其它错误</h4><p>在运行<code>schematool -dbType mysql -initSchema</code>手动初始化元数据库的时候</p><ul><li><p>报了一个log4j重复加载的问题</p><ul><li>解决：可以不用理睬</li></ul></li><li><p>链接mysql 密码过期问题 <code>Your password has expired.</code></p><ul><li><p>解决： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">mysql -uroot -p123</span><br><span class="line"></span><br><span class="line">mysql&gt; use mysql</span><br><span class="line">Reading table information for completion of table and column names</span><br><span class="line">You can turn off this feature to get a quicker startup with -A</span><br><span class="line"></span><br><span class="line">Database changed</span><br><span class="line">mysql&gt; select host,user,password_expired from user;</span><br><span class="line">+-----------+------+------------------+</span><br><span class="line">| host      | user | password_expired |</span><br><span class="line">+-----------+------+------------------+</span><br><span class="line">| %         | root | N                |</span><br><span class="line">| cs1       | root | Y                |</span><br><span class="line">| 127.0.0.1 | root | Y                |</span><br><span class="line">| ::1       | root | Y                |</span><br><span class="line">+-----------+------+------------------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br><span class="line">-------------</span><br><span class="line">mysql&gt; update user set password_expired=&apos;N&apos; where host=&apos;::1&apos;;</span><br><span class="line">Query OK, 1 row affected (0.01 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class="line"></span><br><span class="line">mysql&gt; update user set password_expired=&apos;N&apos; where host=&apos;cs1&apos;;</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class="line"></span><br><span class="line">mysql&gt; update user set password_expired=&apos;N&apos; where host=&apos;127.0.0.1&apos;;</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class="line"></span><br><span class="line">mysql&gt; FLUSH PRIVILEGES;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select host,user,password_expired from user;</span><br><span class="line">+-----------+------+------------------+</span><br><span class="line">| host      | user | password_expired |</span><br><span class="line">+-----------+------+------------------+</span><br><span class="line">| %         | root | N                |</span><br><span class="line">| cs1       | root | N                |</span><br><span class="line">| 127.0.0.1 | root | N                |</span><br><span class="line">| ::1       | root | N                |</span><br><span class="line">+-----------+------+------------------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mysql&gt; exit</span><br><span class="line">Bye</span><br><span class="line">[ap@cs1]~/apps/hive% sudo service mysql restart</span><br><span class="line"></span><br><span class="line"># 再重新初始化就好了</span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="Yum安装-Mysql"><a href="#Yum安装-Mysql" class="headerlink" title="Yum安装 Mysql"></a>Yum安装 Mysql</h3><blockquote><p><strong>因为笔者没装过，所以这部分暂且不表</strong></p></blockquote><hr><h2 id="step2-安装Hive"><a href="#step2-安装Hive" class="headerlink" title="step2-安装Hive"></a>step2-安装Hive</h2><p>MySql安装好之后， 安装Hive就很简答了</p><h4 id="注意-Hive-是操作-Mysql-的数据库-一定要记得把-mysql-驱动文件放到-hive-下的-lib-中"><a href="#注意-Hive-是操作-Mysql-的数据库-一定要记得把-mysql-驱动文件放到-hive-下的-lib-中" class="headerlink" title="注意: Hive 是操作 Mysql 的数据库, 一定要记得把 mysql 驱动文件放到 hive 下的 lib 中!!!"></a>注意: Hive 是操作 Mysql 的数据库, 一定要记得把 mysql 驱动文件放到 hive 下的 lib 中!!!</h4><h4 id="启动beeline-前-要把-hiveserver2开起来-当然-hdfs-mysql-都要启动起来"><a href="#启动beeline-前-要把-hiveserver2开起来-当然-hdfs-mysql-都要启动起来" class="headerlink" title="启动beeline 前, 要把 hiveserver2开起来, 当然, hdfs, mysql 都要启动起来"></a>启动beeline 前, 要把 hiveserver2开起来, 当然, hdfs, mysql 都要启动起来</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.上传到Linux</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.解压安装包到安装目录</span></span><br><span class="line">tar -zxvf apache-hive-2.3.2-bin.tar.gz -C ~/apps/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.把MySQL驱动包(mysql-connector-java-5.1.40-bin.jar)放置在hive 的根路径下的 lib 目录，此处是 ~/apps/apache-hive-2.3.2-bin/lib</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.修改配置文件</span></span><br><span class="line"><span class="built_in">cd</span> ~/apps/apache-hive-2.3.2-bin/conf</span><br><span class="line"><span class="comment"># 新建一个 hive-site.xml</span></span><br><span class="line">touch hive-site.xml</span><br><span class="line">vi hive-site.xml</span><br><span class="line"></span><br><span class="line">+++++++++++++++++++++++++++++++++++++++++++添加如下内容++++++++++++</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">&lt;value&gt;jdbc:mysql://localhost:3306/hivedb?createDatabaseIfNotExist=<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">&lt;description&gt;JDBC connect string <span class="keyword">for</span> a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;!-- 注意:如果mysql和hive 不在同一个服务器节点,需要使用mysql节点的 hostname 或 ip --&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">&lt;description&gt;Driver class name <span class="keyword">for</span> a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">&lt;value&gt;root&lt;/value&gt;</span><br><span class="line">&lt;description&gt;username to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">&lt;value&gt;123&lt;/value&gt;</span><br><span class="line">&lt;description&gt;password to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line">+++++++++++++++++++++++++++++++++++++++++++添加如下内容+++++++++++</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">&lt;description&gt;hive default warehouse, <span class="keyword">if</span> nessecory, change it</span><br><span class="line"></span><br><span class="line">&lt;!-- 这个是配置hive在HDFS上 db 的路存储径的，不配默认默认就是上述路径 --&gt;</span><br><span class="line">&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">++++++++++++++++++++++++++</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.配置环境变量 &amp; source</span></span><br><span class="line"><span class="comment">## 注意：ap是我的用户家目录，换上自己的用户家目录</span></span><br><span class="line"><span class="built_in">export</span> HIVE_HOME=/home/ap/apps/apache-hive-2.3.2-bin </span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HIVE_HOME</span>/bin</span><br><span class="line"><span class="comment"># 用bash的找 .bash_profile， 配置所有环境变量</span></span><br><span class="line"><span class="built_in">source</span> ~/.zshrc  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 6.此时基本安装完成了，验证Hive安装</span></span><br><span class="line">hive --helo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7.重点来了！ 初始化元数据库</span></span><br><span class="line">schematool -dbType mysql -initSchema</span><br><span class="line">&gt; 这里可能会遇到很多错误！！ </span><br><span class="line">&gt; 但是如果前面按照我的方法装的, 应该就问题不大了</span><br><span class="line">&gt; 主要是 mysql 连接权限的问题！！</span><br><span class="line"><span class="comment"># 一定会出现的2个</span></span><br><span class="line">1&gt; 找不到 hive命令的一长串环境变量 (不用理会)</span><br><span class="line">2&gt; 两个log4j，jar包重复的问题 (不用理会)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 8.初始化完成后，可以看下数据库中有没有 hivedb 这个库，成功的话是会有的</span></span><br><span class="line"><span class="comment">## 启动hive</span></span><br><span class="line">hive --service cli  </span><br><span class="line">&gt;hive: show databases;</span><br><span class="line"><span class="comment"># 如果能显示数据库，就没啥问题了</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ❤️9.Hive的使用方式之  HiveServer2/beeline</span></span><br><span class="line">此处需要修改 hadoop 的配置文件</span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.1.首先关闭hdfs &amp; yarn服务 &amp; RunJar(hive服务)，修改hadoop配置文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.2.修改 hadoop 集群的 hdfs-site.xml 配置文件:加入一条配置信息，表示启用 webhdfs</span></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;</span><br><span class="line">&lt;value&gt;<span class="literal">true</span>&lt;/value&gt; </span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.3.修改 hadoop 集群的 core-site.xml 配置文件:加入两条配置信息:表示设置 hadoop 的代理用户</span></span><br><span class="line"><span class="comment">## 注意： 此处的ap是配置 hadoop 的用户名</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.proxyuser.ap.hosts&lt;/name&gt;</span><br><span class="line">&lt;value&gt;*&lt;/value&gt; </span><br><span class="line">&lt;!-- 表示任意节点使用 hadoop 集群的代理用户 hadoop 都能访问 hdfs 集群 --&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt; </span><br><span class="line">&lt;name&gt;hadoop.proxyuser.ap.groups&lt;/name&gt; </span><br><span class="line">&lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;!-- 表示代理用户的组所属 --&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"> 注意 </span><br><span class="line">修改完成后， 发给hadoop集群其他主机</span><br><span class="line">scp xxx.xx  xxx.ss  cs2:<span class="variable">$PWD</span></span><br><span class="line">scp xxx.xx  xxx.ss  cs3:<span class="variable">$PWD</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.4.重启hdfs &amp; yarn服务</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.5.启动 hiveserver2 服务</span></span><br><span class="line"><span class="comment">## 后台启动：</span></span><br><span class="line">nohup hiveserver2 1&gt;/home/ap/hiveserver.log 2&gt;/home/ap/hiveserver.err &amp; </span><br><span class="line"><span class="comment"># 或者:</span></span><br><span class="line">nohup hiveserver2 1&gt;/dev/null 2&gt;/dev/null &amp;</span><br><span class="line"><span class="comment"># 或者:</span></span><br><span class="line">nohup hiveserver2 &gt;/dev/null 2&gt;&amp;1 &amp;</span><br><span class="line"><span class="comment"># 以上 3 个命令是等价的，第一个表示记录日志，第二个和第三个表示不记录日志</span></span><br><span class="line"></span><br><span class="line">注意： nohup 可以在你退出帐户/关闭终端之后继续运行相应的进程。 nohup 就是不挂起的意思(no hang up)。</span><br><span class="line">该命令的一般形式为:nohup <span class="built_in">command</span> &amp;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.6 启动 beeline 客户端去连接</span></span><br><span class="line">方式1：执行命令:</span><br><span class="line">beeline -u jdbc:hive2://cs2:10000 -n ap</span><br><span class="line">-u : 指定元数据库的链接信息 -n : 指定用户名和密码</span><br><span class="line"></span><br><span class="line">方式2：</span><br><span class="line">先执行 </span><br><span class="line">beeline</span><br><span class="line">然后按图所示输入:</span><br><span class="line"><span class="comment"># 此处的cs2是只安装hive的hostname</span></span><br><span class="line">!connect jdbc:hive2://cs2:10000 按回车，然后输入用户名，密码，这个 用户名就是安装 hadoop 集群的用户名和密码</span><br></pre></td></tr></table></figure><h4 id="登录beeline-bee"><a href="#登录beeline-bee" class="headerlink" title="登录beeline  :bee:"></a>登录beeline  :bee:</h4><p><strong>方式1: 直接登录</strong></p><p>登录前要开启 RunJar 进程, 也就是 开启hiveserver2</p><p><code>nohup hiveserver2 1&gt;/home/ap/hiveserver.log 2&gt;/home/ap/hiveserver.err &amp;</code></p><p><code>beeline -u jdbc:hive2://cs2:10000 -n ap</code></p><p><strong>方式2: 输入用户名密码登录</strong></p><p><code>!connect jdbc:hive2://cs2:10000</code></p><h4 id="登录-hive"><a href="#登录-hive" class="headerlink" title="登录 hive"></a>登录 hive</h4><p><code>hive</code></p><h4 id="PS-Linux环境变量失效"><a href="#PS-Linux环境变量失效" class="headerlink" title="PS: Linux环境变量失效"></a>PS: Linux环境变量失效</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 就是直接把环境变量设置为/bin:/usr/bin，因为常用的命令都在/bin这个文件夹中。</span></span><br><span class="line">PATH=/bin:/usr/bin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 接下来修改 .bashr_profile 或则 .zshrc中的内容即可， 修改完了重新source</span></span><br></pre></td></tr></table></figure><h1 id="Hive-–-DDL"><a href="#Hive-–-DDL" class="headerlink" title="Hive – DDL"></a>Hive – DDL</h1><h3 id="库的操作"><a href="#库的操作" class="headerlink" title="库的操作"></a>库的操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">  库的操作 </span><br><span class="line">============================================================================</span><br><span class="line"></span><br><span class="line">#  建库 </span><br><span class="line">CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment]</span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[WITH DBPROPERTIES (property_name=property_value, ...)];</span><br><span class="line"></span><br><span class="line">1、创建普通库</span><br><span class="line">create database dbname;</span><br><span class="line"></span><br><span class="line">2、创建库的时候检查存与否</span><br><span class="line">create databse if not exists dbname;</span><br><span class="line"></span><br><span class="line">3、创建库的时候带注释</span><br><span class="line">create database if not exists dbname comment &apos;create my db named dbname&apos;;</span><br><span class="line"></span><br><span class="line">4、查看创建库的详细语句</span><br><span class="line">show create database mydb;</span><br><span class="line"></span><br><span class="line">#  查看库 </span><br><span class="line">1、查看有哪些数据库 </span><br><span class="line">show databases;</span><br><span class="line"></span><br><span class="line">2、显示数据库的详细属性信息</span><br><span class="line">语法:desc database [extended] dbname; </span><br><span class="line">示例:desc database extended myhive;</span><br><span class="line"></span><br><span class="line">3、查看正在使用哪个库 </span><br><span class="line">select current_database();</span><br><span class="line"></span><br><span class="line">4、查看创建库的详细语句 </span><br><span class="line">show create database mydb;</span><br><span class="line"></span><br><span class="line">5、查看以xx开头的库</span><br><span class="line">show databases like &apos;s*&apos;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#  删除库 </span><br><span class="line">删除库操作:</span><br><span class="line">drop database dbname;</span><br><span class="line">drop database if exists dbname;</span><br><span class="line"></span><br><span class="line">默认情况下，hive 不允许删除包含表的数据库，有两种解决办法:</span><br><span class="line">1、 手动删除库下所有表，然后删除库</span><br><span class="line">2、 使用 cascade 关键字</span><br><span class="line">drop database if exists dbname cascade;</span><br><span class="line"></span><br><span class="line">    默认情况下就是 restrict（严格模式）, 后2行效果一样</span><br><span class="line">drop database if exists myhive </span><br><span class="line">drop database if exists myhive restrict</span><br><span class="line"></span><br><span class="line">#  切换库 </span><br><span class="line">切换库操作:</span><br><span class="line">- 语法:</span><br><span class="line">use database_name </span><br><span class="line">- 实例:</span><br><span class="line">use myhive;</span><br></pre></td></tr></table></figure><h3 id="表的操作"><a href="#表的操作" class="headerlink" title="表的操作"></a>表的操作</h3><h4 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h4><h5 id="建表语法"><a href="#建表语法" class="headerlink" title="建表语法"></a><strong>建表语法</strong></h5><ul><li><code>CREATE [EXTERNAL] TABLE [IF NOT EXISTS] &lt;table_name&gt;</code><br>创建内部表；添加EXTERNAL参数会创建外部表</li><li><code>(col_name data_type [COMMENT col_comment], ...)</code><br>添加字段和字段描述</li><li><code>[COMMENT table_comment]</code><br>添加表描述</li><li><code>[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]</code><br>指定分区字段和字段描述，分区字段不能为建表字段！</li><li><code>[CLUSTERED BY (col_name, col_name, ...)]  SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]</code><ul><li>指定分桶字段，分桶字段必须为建表字段！</li><li>指定排序字段，此字段也必须为建表字段，指定的是分桶内的排序</li><li>指定分桶个数（hash后再取模）</li></ul></li><li><code>[ROW FORMAT row_format]</code><br>指定分隔符，row_format 格式：<pre><code>列分隔符：`delimited fields terminated by ‘x&apos;`行分隔符：`line terminated by ‘x&apos;`</code></pre></li><li><code>[STORED AS file_format]</code><br>指定存储格式<pre><code>textfile：文本格式，默认rcfile：行列结合格式parquet：压缩格式</code></pre></li><li><code>[LOCATION hdfs_path]</code><br>指定表在hsfs上的存储路径，不指定的话就按配置的路径存储，如果也没指定就在hive默认的路经 <code>/user/hive/warehouse</code></li></ul><h5 id="建表代码"><a href="#建表代码" class="headerlink" title="建表代码"></a>建表代码</h5><ul><li>a. 创建内部表<ul><li><code>create table mytable (id int, name string) row format delimited fields terminated by &#39;,&#39; stored as textfile;</code></li></ul></li><li>b. 创建外部表<ul><li><code>create  external table mytable2 (id int, name string) row format delimited  fields terminated by &#39;,&#39; location &#39;/user/hive/warehouse/mytable2&#39;;</code></li></ul></li><li>c. 创建分区表<ul><li><code>create  table table3(id int, name string) partitioned by(sex string) row format  delimited fields terminated by &#39;,&#39; stored as textfile;</code></li><li>插入分区数据：<code>load data local inpath &#39;/root/hivedata/mingxing.txt&#39; overwrite into table mytable3 partition(sex=&#39;girl’);</code></li><li>查询表分区： <code>show partitions mytable3</code></li></ul></li><li>d. 创建分桶表<ul><li><code>create  table stu_buck(Sno int,Sname string,Sex string,Sage int,Sdept string)  clustered by(Sno) sorted by(Sno DESC) into 4 buckets row format  delimited fields terminated by &#39;,’;</code></li></ul></li><li>e. 复制表<ul><li><code>create [external] table [if not exists] new_table like table_name;</code></li></ul></li><li>f. 查询表<ul><li><code>create table table_a as select * from  teble_b;</code></li></ul></li></ul><h4 id="查看表"><a href="#查看表" class="headerlink" title="查看表"></a>查看表</h4><ul><li><code>desc &lt;table_name&gt;</code>：显示表的字段信息</li><li><code>desc formatted &lt;table_name&gt;</code>：格式化显示表的详细信息</li><li><code>desc extended &lt;table_name&gt;</code>：显示表的详细信息</li></ul><h4 id="修改表"><a href="#修改表" class="headerlink" title="修改表"></a>修改表</h4><ul><li>重命名<ul><li><code>ALTER TABLE old_name RENAME TO new_name</code></li></ul></li><li>修改属性<ul><li><code>ALTER TABLE table_name SET TBLPROPERTIES (&#39;comment&#39; = &#39;my new students table’);</code></li><li>不支持修改表名，和表的数据存储目录</li></ul></li><li>增加/修改/替换字段<ul><li><code>ALTER TABLE table_name ADD COLUMNS (col_spec [, col_spec ...])</code><br>新增的字段位置在所有列后面 ( partition 列前 )</li><li><code>ALTER TABLE table_name CHANGE c_name new_c_name new_c_type [FIRST | AFTER c_name]</code><br>注意修改字段时，类型只能由小类型转为大类型，不让回报错；（在hive1.2.2中并没有此限制）</li><li><code>ALTER TABLE table_name REPLACE COLUMNS (col_spec [, col_spec ...])</code><br>REPLACE 表示替换表中所有字段</li></ul></li><li>添加/删除分区<ul><li>添加分区：<code>ALTER TABLE table_name ADD [IF NOT EXISTS]  PARTITION (partition_col = col_value1 [ ... ] ) [LOCATION &#39;location1’]</code></li><li>删除分区：<code>ALTER TABLE table_name DROP [IF EXISTS] PARTITION (partition_col = col_value1 [ ... ] )</code></li><li>修改分区路径：<code>ALTER TABLE student_p PARTITION (part=&#39;bb&#39;) SET LOCATION &#39;/myhive_bbbbb’;</code></li><li><strong>[补充：]</strong><ul><li>1、 防止分区被删除：alter table student_p partition (part=’aa’) enable no_drop;</li><li>2、 防止分区被查询：alter table student_p partition (part=’aa’) enable offline;<br>enable 和 disable 是反向操作 </li></ul></li></ul></li></ul><h4 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h4><ul><li><code>drop table if exists &lt;table_name&gt;;</code></li></ul><h4 id="清空表"><a href="#清空表" class="headerlink" title="清空表"></a>清空表</h4><p>会保留表结构</p><ul><li><code>truncate table table_name;</code></li><li><code>truncate table table_name partition(city=&#39;beijing’);</code></li></ul><h4 id="其他辅助命令"><a href="#其他辅助命令" class="headerlink" title="其他辅助命令"></a>其他辅助命令</h4><ul><li>a. 查看数据库列表<ul><li><code>show databases;</code></li><li><code>show databases like &#39;my*&#39;;</code></li></ul></li><li>b. 查看数据表<ul><li><code>show tables;</code></li><li><code>show tables in db_name;</code></li></ul></li><li>c. 查看数据表的建表语句<ul><li><code>show create table table_name;</code></li></ul></li><li>d. 查看 hive 函数列表<ul><li><code>show functions;</code></li></ul></li><li>e. 查看 hive 表的分区<ul><li><code>show partitions table_name;</code></li><li><code>show partitions table_name partition(city=&#39;beijing&#39;)</code></li></ul></li><li>f. 查看表的详细信息（元数据信息） <ul><li><code>desc table_name;</code></li><li><code>desc extended table_name;</code></li></ul></li><li>g. 查看数据库的详细属性信息<ul><li><code>desc formatted table_name;</code></li><li><code>desc database db_name; desc database extended db_name;</code></li></ul></li><li>h. 清空数据表<ul><li><code>truncate table table_name;</code></li></ul></li></ul><p><br></p><h1 id="Hive-–-DML"><a href="#Hive-–-DML" class="headerlink" title="Hive – DML"></a>Hive – DML</h1><h3 id="装载数据"><a href="#装载数据" class="headerlink" title="装载数据"></a>装载数据</h3><ul><li><code>LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE table_name [PARTITION (partcol1=val1, partcol2=val2 ...)]</code></li><li>注意：<ul><li>LOAD 操作只是单纯的 复制（本地文件）或者 移动（hdfs文件，一般是公共数据，需要建立外部表）操作，将数据文件移动到 Hive 表对应的位置</li><li>如果指定了 LOCAL 就去本地文件系统中查找，否则按 inpath 中的 uri 在 hdfs 上查找</li><li>inpath 子句中的文件路径下，不能再有文件夹</li><li>如果使用了 OVERWRITE 关键字，则目标表（或者分区）中的内容会被删除，然后再将 filepath 指向的文件/目录中的内容添加到表/分区中。 </li><li>如果目标表（分区）已经有一个文件，并且文件名和 filepath 中的文件名冲突，那么现有的文件会被新文件所替代。???不是自动重命名为xxx_copy_1</li></ul></li></ul><h3 id="插入数据"><a href="#插入数据" class="headerlink" title="插入数据"></a>插入数据</h3><ul><li><p>a. 单条插入</p><ul><li><code>INSERT INTO TABLE table_name VALUES(value1, value2, ...);</code></li></ul></li><li><p>b. 单重插入</p><ul><li><code>INSERT INTO TABLE table_name [PARTITION (partcol1=val1, ...)] &lt;select_statement1 FROM from_statement&gt;</code></li></ul></li><li><p>c. 多重插入</p><ul><li>FROM from_statement<br>从基表中按不同的字段查询得到的结果分别插入不同的 hive 表<br>只会扫描一次基表，提高查询性能</li><li><code>INSERT INTO TABLE table_name1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 [WHERE where_statement]</code></li><li><code>INSERT INTO TABLE table_name2 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement2 [WHERE where_statement]</code></li><li>[ … ];</li></ul></li><li><p>d. 分区插入</p><ul><li><p>分区插入有两种：一种是静态分区，另一种是动态分区。</p></li><li><p>如果混合使用静态分区和动态分区， 则静态分区必须出现在动态分区之前。</p></li><li><p>静态分区</p><ul><li>A)、创建静态分区表 </li><li>B)、从查询结果中导入数据（单重插入）<br>记得加载数据前要添加分区，然后指定要加载到那个分区</li><li>C)、查看插入结果 </li></ul></li><li><p>动态分区</p><ul><li><p>静态分区添加数据前需要指定分区，当分区个数不确定的时候就很不方便了，这个时候可以使用动态分区</p></li><li><p>重要且常用，尤其是按照日期分区时！</p></li><li><p>A)、创建分区表 </p></li><li><p>B)、参数设置</p></li></ul></li></ul></li></ul><pre><code>  hive-1.2版本  -  set hive.exec.dynamic.partition=true;   //动态分区开启状态，默认开启  -  set hive.exec.dynamic.partition.mode=nonstrict;   //动态分区执行模式，默认&quot;strict&quot;，在这种模式 下要求至少有一列分区字段是静态的，这有助于阻止因设计错误导致查询产生大量的分区  - \# 可选设置项    如果这些参数被更改了又想还原，则执行一次 reset 命令即可    - set hive.exec.max.dynamic.partitions.pernode=100;   //每个节点生成动态分区最大个数     - set hive.exec.max.dynamic.partitions=1000;   //生成动态分区最大个数，如果自动分区数大于这个参数，将会报错     - set hive.exec.max.created.files=100000;   //一个任务最多可以创建的文件数目     - set dfs.datanode.max.xcievers=4096;   //限定一次最多打开的文件数 set     - hive.error.on.empty.partition=false;   //表示当有空分区产生时，是否抛出异常- C)、动态数据插入   - 单个分区字段    - insert into table test2 partition (age) select name,address,school,age from students;   - 多个分区字段    多重分区中目录结构是按照分区字段顺序进行划分的    - insert into table student_ptn2 partition(department, age) select id, name, sex, department,age from students;      分区字段都是动态的    - insert into table student_ptn2 partition(city=&apos;sa&apos;, zipcode) select id, name, sex, age, department, department as zipcode from students;      第一个分区字段时静态的，第二字department字段动态的，重命名为zipcode？  - [注意：]    - 查询语句 select 查询出来的动态分区 age 和 zipcode 必须放在 最后，和分区字段对应，不然结果会出错- D)、查看插入结果  - select * from student_ptn2 where city=&apos;sa&apos; and zipcode=&apos;MA&apos;;</code></pre><ul><li><p>e. 分桶插入</p><ul><li>A)、创建分桶表</li><li>B)、从查询结果中导入数据<br>只能使用insert方式</li><li>C)、查看插入结果 </li><li># 几个命令<ul><li>set hive.exec.reducers.bytes.per.reducer=<number>  // 设置每个reducer的吞吐量，单位byte，默认256M</number></li><li>set hive.exec.reducers.max=<number>  //reduceTask最多执行个数，默认1009</number></li><li>set mapreduce.job.reduces=<number>   //设置reducetask实际运行数，默认-1，代表没有设置，即reducetask默认数为1</number></li><li>set hive.exec.mode.local.auto=true //设置hive本地模式</li></ul></li></ul></li></ul><h3 id="导出数据（了解）"><a href="#导出数据（了解）" class="headerlink" title="导出数据（了解）"></a>导出数据（了解）</h3><ul><li>单模式导出<ul><li><code>INSERT OVERWRITE [LOCAL] DIRECTORY directory1 &lt;select_statement&gt;;</code></li></ul></li><li>多模式导出<ul><li><code>FROM from_statement</code></li><li><code>INSERT OVERWRITE [LOCAL] DIRECTORY directory1 &lt;select_statement1&gt;</code></li><li><code>[INSERT OVERWRITE [LOCAL] DIRECTORY directory2 &lt;select_statement2&gt;] ...</code></li></ul></li></ul><h3 id="查询数据"><a href="#查询数据" class="headerlink" title="查询数据"></a>查询数据</h3><ul><li>Hive 中的 SELECT 基础语法和标准 SQL 语法基本一致，支持 WHERE、DISTINCT、GROUP BY、 ORDER BY、HAVING、LIMIT、子查询等；<ul><li>1、<code>select * from db.table1</code><br>虽然可以，但是要尽量避免 select * 这样的全表扫描操作，效率太低又费时</li><li>2、<code>select count(distinct uid) from db.table1</code></li><li>3、支持 select、union all、join（left、right、full join）、like、where、having、各种聚合函数、 支持 json 解析 </li><li>4、UDF/ UDAF/UDTF <ul><li>UDF：User Defined Function，自定义函数，一对一</li><li>UDAF：User Defined Aggregate Function，自定义聚合函数，多对一，如sum()，count()</li><li>UDTF ：User Defined Table Function，自定义表函数，一对多，如explode()</li></ul></li><li>5、不支持 update 和 delete </li><li>6、hive 虽然支持 in/exists（老版本是不支持的），但是 hive 推荐使用 semi join 的方式来代替 实现，而且效率更高。 <ul><li>半连接<ul><li>左半连接：left semi join，以左表为基表，右表有的，只显示左表相应记录（即一半）</li><li>右半连接：right semi join，与左半连接相反</li></ul></li><li>内连接：inner join，两表中都有的才会连接</li><li>外连接<ul><li>左外连接：left outer join，坐标为基表，右表有的会关联，右表没有的以null表示；左表没有右表有的不会关联</li><li>右外连接：right outer join，与左外连接相反</li><li>全外连接：full outer join，两表合并</li></ul></li></ul></li><li>7、支持 case … when …</li></ul></li><li><strong>语法结构</strong><ul><li><code>SELECT [ALL | DISTINCT] select_ condition, select_ condition, ...</code></li><li>FROM table_name a </li><li>[JOIN table_other b ON <a href="http://a.id" target="_blank" rel="noopener">a.id</a> = <a href="http://b.id" target="_blank" rel="noopener">b.id</a>]<br>表连接</li><li>[WHERE where_condition]<br>过滤条件</li><li>[GROUP BY col_list [HAVING condition]]<br>分组条件</li><li>[CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY col_list | ORDER BY col_list] [DESC]]<br>排序条件：<br>\1. order by：全局排序，默认升序，DESC表示降序。只有一个 reduce task 的结果，比如文<br>件名是 000000_0，会导致当输入规模较大时，需要较长的计算时间。<br>\2.  sort by：局部排序，其在数据进入 reducer 前完成排序。因此，如果用 sort by 进行排序，并且设置  mapred.reduce.tasks &gt; 1，则 sort by 只保证每个 reducer 的输出有序，不保证全局有序。<br>\3. distribute by：类似于分桶，根据指定的字段将数据分到不同的 reducer，且分发算法是 hash 散列<br>\4. cluster by：除了具有 Distribute by 的功能外，还会对该字段进行排序。<br>注意：如果 distribute 和 sort 字段是同一个时，cluster by = distribute by + sort by；<br>如果分桶字段和排序字段不一样，那么就不能使用 clustered by </li><li>[LIMIT number]<br>显示结果的前几个记录</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hive简介&quot;&gt;&lt;a href=&quot;#Hive简介&quot; class=&quot;headerlink&quot; title=&quot;Hive简介&quot;&gt;&lt;/a&gt;Hive简介&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/study/2018-
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/categories/Hadoop/Hive/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>i-数据分析系统Hive</title>
    <link href="https://airpoet.github.io/2018/06/12/Hadoop/3-Hive/i-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9FHive/"/>
    <id>https://airpoet.github.io/2018/06/12/Hadoop/3-Hive/i-数据分析系统Hive/</id>
    <published>2018-06-11T16:42:14.342Z</published>
    <updated>2018-06-14T01:19:27.806Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hive背景及应用场景"><a href="#Hive背景及应用场景" class="headerlink" title="Hive背景及应用场景"></a>Hive背景及应用场景</h1><h2 id="Hive是什么？"><a href="#Hive是什么？" class="headerlink" title="Hive是什么？"></a>Hive是什么？</h2><p><strong>看一下MR的 wordcount 和 Hive 的 wordcount</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173251.png" alt="image-20180612013251049"></p><p><br></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173355.png" alt="image-20180612013349514"></p><h2 id="Hive典型应用场景"><a href="#Hive典型应用场景" class="headerlink" title="Hive典型应用场景"></a>Hive典型应用场景</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173443.png" alt="image-20180612013442768"></p><p><br></p><h2 id="为什么使用Hive？"><a href="#为什么使用Hive？" class="headerlink" title="为什么使用Hive？"></a>为什么使用Hive？</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173532.png" alt="image-20180612013531999"></p><p><br></p><h2 id="有了Hive，还需要自己写MR程序吗？"><a href="#有了Hive，还需要自己写MR程序吗？" class="headerlink" title="有了Hive，还需要自己写MR程序吗？"></a>有了Hive，还需要自己写MR程序吗？</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173601.png" alt="image-20180612013600542"></p><p><br></p><p><br></p><h1 id="Hive基本架构"><a href="#Hive基本架构" class="headerlink" title="Hive基本架构"></a>Hive基本架构</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173645.png" alt="image-20180612013645384"></p><h2 id="Hive各模块组成"><a href="#Hive各模块组成" class="headerlink" title="Hive各模块组成"></a>Hive各模块组成</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173742.png" alt="image-20180612013742424"></p><p><br></p><h2 id="Hive部署架构-实验环境"><a href="#Hive部署架构-实验环境" class="headerlink" title="Hive部署架构-实验环境"></a>Hive部署架构-实验环境</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173814.png" alt="image-20180612013813506"></p><p><br></p><h2 id="Hive部署架构-生产环境"><a href="#Hive部署架构-生产环境" class="headerlink" title="Hive部署架构-生产环境"></a>Hive部署架构-生产环境</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173838.png" alt="image-20180612013837922"></p><p><br></p><h2 id="Hive部署架构-metastore服务"><a href="#Hive部署架构-metastore服务" class="headerlink" title="Hive部署架构-metastore服务"></a>Hive部署架构-metastore服务</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173904.png" alt="image-20180612013903730"></p><p><br></p><p><br></p><h1 id="Hive使用方式"><a href="#Hive使用方式" class="headerlink" title="Hive使用方式"></a>Hive使用方式</h1><h2 id="CLI（Command-Line-Interface）"><a href="#CLI（Command-Line-Interface）" class="headerlink" title="CLI（Command Line Interface）"></a>CLI（Command Line Interface）</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173952.png" alt="image-20180612013951537"></p><p><br></p><h2 id="CLI—hive外部资源"><a href="#CLI—hive外部资源" class="headerlink" title="CLI—hive外部资源"></a>CLI—hive外部资源</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174023.png" alt="image-20180612014022323"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174103.png" alt="image-20180612014102686"></p><p><br></p><h2 id="Hive-Web-UI"><a href="#Hive-Web-UI" class="headerlink" title="Hive Web UI"></a>Hive Web UI</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174201.png" alt="image-20180612014200724"></p><p><br></p><h2 id="Hive客户端程序"><a href="#Hive客户端程序" class="headerlink" title="Hive客户端程序"></a>Hive客户端程序</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174222.png" alt="image-20180612014221547"></p><p><br></p><p><br></p><h1 id="HQL查询语句"><a href="#HQL查询语句" class="headerlink" title="HQL查询语句"></a>HQL查询语句</h1><h2 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174257.png" alt="image-20180612014257010"></p><p><br></p><h2 id="数据类型（不断增加中……）"><a href="#数据类型（不断增加中……）" class="headerlink" title="数据类型（不断增加中……）"></a>数据类型（不断增加中……）</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174340.png" alt="image-20180612014340830"></p><p><br></p><ul><li><strong>自有的特殊类型</strong></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174412.png" alt="image-20180612014411860"></p><p><br></p><h2 id="数据定义语句（DDL）"><a href="#数据定义语句（DDL）" class="headerlink" title="数据定义语句（DDL）"></a>数据定义语句（DDL）</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174515.png" alt="image-20180612014514763"></p><p><br></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174533.png" alt="image-20180612014533008"></p><p><br></p><p><strong>分隔符开发中这样写</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174607.png" alt="image-20180612014606665"></p><p><br></p><h2 id="Hive-Partition与Bucket"><a href="#Hive-Partition与Bucket" class="headerlink" title="Hive Partition与Bucket"></a>Hive Partition与Bucket</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174853.png" alt="image-20180612014852827"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174912.png" alt="image-20180612014912295"></p><p><br></p><p><br></p><h2 id="Hive数据格式"><a href="#Hive数据格式" class="headerlink" title="Hive数据格式"></a>Hive数据格式</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174952.png" alt="image-20180612014951697"></p><h3 id="应用举例：日志清理"><a href="#应用举例：日志清理" class="headerlink" title="应用举例：日志清理"></a>应用举例：日志清理</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175053.png" alt="image-20180612015053452"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175104.png" alt="image-20180612015104125"></p><p><br></p><p><br></p><h2 id="数据操作语句（DML）"><a href="#数据操作语句（DML）" class="headerlink" title="数据操作语句（DML）"></a>数据操作语句（DML）</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175153.png" alt="image-20180612015153237"></p><h3 id="数据加载与插入语句"><a href="#数据加载与插入语句" class="headerlink" title="数据加载与插入语句"></a>数据加载与插入语句</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175226.png" alt="image-20180612015225942"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175246.png" alt="image-20180612015245860"></p><h3 id="几个实例"><a href="#几个实例" class="headerlink" title="几个实例"></a>几个实例</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175318.png" alt="image-20180612015317619"></p><h2 id="数据查询语句SELECT"><a href="#数据查询语句SELECT" class="headerlink" title="数据查询语句SELECT"></a>数据查询语句SELECT</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175356.png" alt="image-20180612015355128"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175419.png" alt="image-20180612015419432"></p><p><br></p><h2 id="两种分布式Join算法"><a href="#两种分布式Join算法" class="headerlink" title="两种分布式Join算法"></a>两种分布式Join算法</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175511.png" alt="image-20180612015511444"></p><p><br></p><h2 id="HQL中两种特殊Join"><a href="#HQL中两种特殊Join" class="headerlink" title="HQL中两种特殊Join"></a>HQL中两种特殊Join</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175606.png" alt="image-20180612015606096"></p><p><br></p><h2 id="Order-By与Sort-By"><a href="#Order-By与Sort-By" class="headerlink" title="Order By与Sort By"></a>Order By与Sort By</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-231641.png" alt="image-20180612071641233"></p><p><br></p><h2 id="Distribute-by与Cluster-by"><a href="#Distribute-by与Cluster-by" class="headerlink" title="Distribute by与Cluster by"></a>Distribute by与Cluster by</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-231718.png" alt="image-20180612071718342"></p><p><br></p><h2 id="Transform语法（Streaming）"><a href="#Transform语法（Streaming）" class="headerlink" title="Transform语法（Streaming）"></a>Transform语法（Streaming）</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-231750.png" alt="image-20180612071749235"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-231813.png" alt="image-20180612071813456"></p><p><br></p><h2 id="用户自定义函数（UDF）"><a href="#用户自定义函数（UDF）" class="headerlink" title="用户自定义函数（UDF）"></a>用户自定义函数（UDF）</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-231839.png" alt="image-20180612071839711"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-231921.png" alt="image-20180612071920788"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-231940.png" alt="image-20180612071940199"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232009.png" alt="image-20180612072009048"></p><p>UDAF</p><ul><li><a href="http://svn.apache.org/repos/asf/hive/branches/branch0.13/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFAverage.java" target="_blank" rel="noopener">http://svn.apache.org/repos/asf/hive/branches/branch0.13/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFAverage.java</a></li></ul><p>UDTF</p><ul><li><a href="http://svn.apache.org/repos/asf/hive/branches/branch0.13/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFExplode.java" target="_blank" rel="noopener">http://svn.apache.org/repos/asf/hive/branches/branch0.13/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFExplode.java</a></li></ul><p><br></p><h2 id="其它问题"><a href="#其它问题" class="headerlink" title="其它问题"></a>其它问题</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232136.png" alt="image-20180612072136290"></p><p><br></p><h2 id="Hive-On-HBase"><a href="#Hive-On-HBase" class="headerlink" title="Hive On HBase"></a>Hive On HBase</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232225.png" alt="image-20180612072225509"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232245.png" alt="image-20180612072244391"></p><h1 id="Hive总结及其类似开源系统"><a href="#Hive总结及其类似开源系统" class="headerlink" title="Hive总结及其类似开源系统"></a>Hive总结及其类似开源系统</h1><h2 id="Stinger"><a href="#Stinger" class="headerlink" title="Stinger"></a>Stinger</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232321.png" alt="image-20180612072320862"></p><p><br></p><h2 id="Hive-MR-vs-Hive-Tez"><a href="#Hive-MR-vs-Hive-Tez" class="headerlink" title="Hive-MR vs Hive-Tez"></a>Hive-MR vs Hive-Tez</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232350.png" alt="image-20180612072350053"></p><p><br></p><h2 id="Shark"><a href="#Shark" class="headerlink" title="Shark"></a>Shark</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232415.png" alt="image-20180612072415680"></p><p><br></p><h2 id="Impala"><a href="#Impala" class="headerlink" title="Impala"></a>Impala</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232439.png" alt="image-20180612072438860"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232454.png" alt="image-20180612072454317"></p><p><br></p><h2 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a>性能比较</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232528.png" alt="image-20180612072528668"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232548.png" alt="image-20180612072547727"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hive背景及应用场景&quot;&gt;&lt;a href=&quot;#Hive背景及应用场景&quot; class=&quot;headerlink&quot; title=&quot;Hive背景及应用场景&quot;&gt;&lt;/a&gt;Hive背景及应用场景&lt;/h1&gt;&lt;h2 id=&quot;Hive是什么？&quot;&gt;&lt;a href=&quot;#Hive是什么？&quot;
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/categories/Hadoop/Hive/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>偶然发现的好歌</title>
    <link href="https://airpoet.github.io/2018/06/11/Songs/%E5%81%B6%E7%84%B6%E5%8F%91%E7%8E%B0%E7%9A%84%E5%A5%BD%E6%AD%8C/"/>
    <id>https://airpoet.github.io/2018/06/11/Songs/偶然发现的好歌/</id>
    <published>2018-06-11T11:04:05.646Z</published>
    <updated>2018-06-11T11:32:43.834Z</updated>
    
    <content type="html"><![CDATA[ <blockquote class="blockquote-center"><h1 id="Dealbreaker"><a href="#Dealbreaker" class="headerlink" title="Dealbreaker"></a>Dealbreaker</h1><ul><li>专辑：<a href="orpheus://orpheus/pub/app.html#/m/album/?id=1715512" target="_blank" rel="noopener">Chesapeake</a></li><li>歌手：<a href="orpheus://orpheus/pub/app.html#/m/artist/?id=72720" target="_blank" rel="noopener">Rachael Yamagata</a></li><li>链接：<a href="http://music.163.com/#/m/song?id=18733198" target="_blank" rel="noopener">http://music.163.com/#/m/song?id=18733198</a></li></ul><p><br></p><h1 id="You-Won’t-Let-Me"><a href="#You-Won’t-Let-Me" class="headerlink" title="You Won’t Let Me"></a>You Won’t Let Me</h1><ul><li>专辑：<a href="orpheus://orpheus/pub/app.html#/m/album/?id=1715512" target="_blank" rel="noopener">Chesapeake</a></li><li>歌手：<a href="orpheus://orpheus/pub/app.html#/m/artist/?id=72720" target="_blank" rel="noopener">Rachael Yamagata</a></li><li>链接：<a href="http://music.163.com/#/song?id=18733192" target="_blank" rel="noopener">http://music.163.com/#/song?id=18733192</a></li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      
      
         &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h1 id=&quot;Dealbreaker&quot;&gt;&lt;a href=&quot;#Dealbreaker&quot; class=&quot;headerlink&quot; title=&quot;Dealbreaker&quot;&gt;&lt;/a&gt;Dealbreaker&lt;/h
      
    
    </summary>
    
      <category term="文艺" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/"/>
    
      <category term="Songs" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/Songs/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="文艺" scheme="https://airpoet.github.io/tags/%E6%96%87%E8%89%BA/"/>
    
      <category term="Songs" scheme="https://airpoet.github.io/tags/Songs/"/>
    
  </entry>
  
  <entry>
    <title>_HDFS应用场景&amp;原理&amp;基本架构及使用方法</title>
    <link href="https://airpoet.github.io/2018/06/11/Hadoop/1-HDFS/HDFS%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF&amp;%E5%8E%9F%E7%90%86&amp;%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84%E5%8F%8A%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <id>https://airpoet.github.io/2018/06/11/Hadoop/1-HDFS/HDFS应用场景&amp;原理&amp;基本架构及使用方法/</id>
    <published>2018-06-11T00:01:43.146Z</published>
    <updated>2018-06-11T16:27:43.347Z</updated>
    
    <content type="html"><![CDATA[<h1 id="HDFS基本架构和原理"><a href="#HDFS基本架构和原理" class="headerlink" title="HDFS基本架构和原理"></a>HDFS基本架构和原理</h1><h2 id="HDFS设计思想"><a href="#HDFS设计思想" class="headerlink" title="HDFS设计思想"></a>HDFS设计思想</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-142401.png" alt="image-20180611222401493"></p><h2 id="HDFS架构"><a href="#HDFS架构" class="headerlink" title="HDFS架构"></a>HDFS架构</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-142442.png" alt="image-20180611222442066"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-142505.png" alt="image-20180611222505133"></p><h2 id="HDFS数据块（block）"><a href="#HDFS数据块（block）" class="headerlink" title="HDFS数据块（block）"></a>HDFS数据块（block）</h2><ul><li><strong>注意： Hadoop2.x，block默认大小是128MB</strong></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-142646.png" alt="image-20180611222645888"></p><h2 id="HDFS写流程"><a href="#HDFS写流程" class="headerlink" title="HDFS写流程"></a>HDFS写流程</h2><ul><li>创建Distributed FileSystem类 </li><li>询问 NameNode 要写的文件对否存在</li><li>不存在就写入到 FSDataOutputStream 流中</li><li>流写出去到一个 DataNode</li><li>…</li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-143228.png" alt="image-20180611223228053"></p><h2 id="HDFS读流程"><a href="#HDFS读流程" class="headerlink" title="HDFS读流程"></a>HDFS读流程</h2><ul><li>客户端向 NameNode 询问 block 的位置</li><li>按照客户端按照拿到的位置，向不同的DataNode 请求数据</li><li>……</li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-143510.png" alt="image-20180611223510239"></p><h2 id="HDFS典型物理拓扑"><a href="#HDFS典型物理拓扑" class="headerlink" title="HDFS典型物理拓扑"></a>HDFS典型物理拓扑</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-143914.png" alt="image-20180611223914168"></p><h2 id="HDFS副本放置策略"><a href="#HDFS副本放置策略" class="headerlink" title="HDFS副本放置策略"></a>HDFS副本放置策略</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-143940.png" alt="image-20180611223939952"></p><h2 id="HDFS可靠性策略"><a href="#HDFS可靠性策略" class="headerlink" title="HDFS可靠性策略"></a>HDFS可靠性策略</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-144153.png" alt="image-20180611224152799"></p><h2 id="HDFS不适合存储小文件"><a href="#HDFS不适合存储小文件" class="headerlink" title="HDFS不适合存储小文件"></a>HDFS不适合存储小文件</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-144344.png" alt="image-20180611224344481"></p><p><br></p><h1 id="HDFS程序设计"><a href="#HDFS程序设计" class="headerlink" title="HDFS程序设计"></a>HDFS程序设计</h1><h2 id="HDFS访问方式"><a href="#HDFS访问方式" class="headerlink" title="HDFS访问方式"></a>HDFS访问方式</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-144935.png" alt="image-20180611224934785"></p><h2 id="HDFS-Shell命令"><a href="#HDFS-Shell命令" class="headerlink" title="HDFS Shell命令"></a>HDFS Shell命令</h2><h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145132.png" alt="image-20180611225131966"></p><h3 id="HDFS-Shell命令—文件操作命令"><a href="#HDFS-Shell命令—文件操作命令" class="headerlink" title="HDFS Shell命令—文件操作命令"></a>HDFS Shell命令—文件操作命令</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145219.png" alt="image-20180611225219211"></p><h3 id="HDFS-Shell命令—文件操作命令-1"><a href="#HDFS-Shell命令—文件操作命令-1" class="headerlink" title="HDFS Shell命令—文件操作命令"></a>HDFS Shell命令—文件操作命令</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145604.png" alt="image-20180611225604218"></p><h3 id="HDFS-Shell命令—管理命令"><a href="#HDFS-Shell命令—管理命令" class="headerlink" title="HDFS Shell命令—管理命令"></a>HDFS Shell命令—管理命令</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145728.png" alt="image-20180611225727910"></p><h3 id="HDFS-Shell命令—管理脚本"><a href="#HDFS-Shell命令—管理脚本" class="headerlink" title="HDFS Shell命令—管理脚本"></a>HDFS Shell命令—管理脚本</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145803.png" alt="image-20180611225802894"></p><h3 id="HDFS-Shell命令—文件管理命令fsck"><a href="#HDFS-Shell命令—文件管理命令fsck" class="headerlink" title="HDFS Shell命令—文件管理命令fsck"></a>HDFS Shell命令—文件管理命令fsck</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145932.png" alt="image-20180611225931880"></p><ul><li><strong>查看帮助</strong></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145915.png" alt="image-20180611225914482"></p><ul><li><strong>用法示例</strong></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-150253.png" alt="image-20180611230252655"></p><h3 id="HDFS-Shell命令—数据均衡器balancer"><a href="#HDFS-Shell命令—数据均衡器balancer" class="headerlink" title="HDFS Shell命令—数据均衡器balancer"></a>HDFS Shell命令—数据均衡器balancer</h3><ul><li><strong>一般设置10% —— 15% 就差不多了</strong></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-150614.png" alt="image-20180611230613854"></p><h3 id="HDFS-Shell命令—设置目录份额"><a href="#HDFS-Shell命令—设置目录份额" class="headerlink" title="HDFS Shell命令—设置目录份额"></a>HDFS Shell命令—设置目录份额</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-150704.png" alt="image-20180611230703142"></p><h3 id="※-HDFS-Shell命令—增加-移除节点-※"><a href="#※-HDFS-Shell命令—增加-移除节点-※" class="headerlink" title="※ HDFS Shell命令—增加/移除节点 ※"></a>※ HDFS Shell命令—增加/移除节点 ※</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-150741.png" alt="image-20180611230741126"></p><h2 id="HDFS-Java"><a href="#HDFS-Java" class="headerlink" title="HDFS Java"></a>HDFS Java</h2><h3 id="API介绍"><a href="#API介绍" class="headerlink" title="API介绍"></a>API介绍</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-150953.png" alt="image-20180611230953359"></p><h3 id="HDFS-Java程序举例"><a href="#HDFS-Java程序举例" class="headerlink" title="HDFS Java程序举例"></a>HDFS Java程序举例</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-151048.png" alt="image-20180611231047354"></p><h2 id="HDFS-多语言API—借助thrift"><a href="#HDFS-多语言API—借助thrift" class="headerlink" title="HDFS 多语言API—借助thrift"></a>HDFS 多语言API—借助thrift</h2><p><a href="http://thrift.apache.org/" target="_blank" rel="noopener">也是Apache的顶级项目</a></p><h3 id="thrift执行流程"><a href="#thrift执行流程" class="headerlink" title="thrift执行流程"></a>thrift执行流程</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-151149.png" alt="image-20180611231148955"></p><h3 id="hadoopfs-thrift接口定义"><a href="#hadoopfs-thrift接口定义" class="headerlink" title="hadoopfs.thrift接口定义"></a>hadoopfs.thrift接口定义</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-151634.png" alt="image-20180611231634046"></p><h3 id="PHP语言访问HDFS"><a href="#PHP语言访问HDFS" class="headerlink" title="PHP语言访问HDFS"></a>PHP语言访问HDFS</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-151654.png" alt="image-20180611231653750"></p><h3 id="Python语言访问HDFS"><a href="#Python语言访问HDFS" class="headerlink" title="Python语言访问HDFS"></a>Python语言访问HDFS</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-152052.png" alt="image-20180611232052633"></p><hr><h1 id="Hadoop-2-0新特性"><a href="#Hadoop-2-0新特性" class="headerlink" title="Hadoop 2.0新特性"></a>Hadoop 2.0新特性</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-152948.png" alt="image-20180611232948514"></p><h2 id="HA-高可用-与Federation-联邦"><a href="#HA-高可用-与Federation-联邦" class="headerlink" title="HA(高可用)与Federation(联邦)"></a>HA(高可用)与Federation(联邦)</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153058.png" alt="image-20180611233058385"></p><h2 id="异构层级存储结构"><a href="#异构层级存储结构" class="headerlink" title="异构层级存储结构"></a>异构层级存储结构</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153117.png" alt="image-20180611233117295"></p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153206.png" alt="image-20180611233206158"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153228.png" alt="image-20180611233227447"></p><h2 id="HDFS-ACL"><a href="#HDFS-ACL" class="headerlink" title="HDFS ACL"></a>HDFS ACL</h2><h3 id="背景：现有权限管理的局限性"><a href="#背景：现有权限管理的局限性" class="headerlink" title="背景：现有权限管理的局限性"></a>背景：现有权限管理的局限性</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153255.png" alt="image-20180611233255196"></p><h3 id="基于POSIX-ACL的实现"><a href="#基于POSIX-ACL的实现" class="headerlink" title="基于POSIX ACL的实现"></a>基于POSIX ACL的实现</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153310.png" alt="image-20180611233310082"></p><h2 id="HDFS快照"><a href="#HDFS快照" class="headerlink" title="HDFS快照"></a>HDFS快照</h2><h3 id="背景-1"><a href="#背景-1" class="headerlink" title="背景"></a>背景</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153338.png" alt="image-20180611233337709"></p><h3 id="基本使用方法"><a href="#基本使用方法" class="headerlink" title="基本使用方法"></a>基本使用方法</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153358.png" alt="image-20180611233357356"></p><h2 id="HDFS缓存"><a href="#HDFS缓存" class="headerlink" title="HDFS缓存"></a>HDFS缓存</h2><h3 id="背景-2"><a href="#背景-2" class="headerlink" title="背景"></a>背景</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153423.png" alt="image-20180611233423131"></p><h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153446.png" alt="image-20180611233446072"></p><h3 id="实现情况"><a href="#实现情况" class="headerlink" title="实现情况"></a>实现情况</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153502.png" alt="image-20180611233502379"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153521.png" alt="image-20180611233521502"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;HDFS基本架构和原理&quot;&gt;&lt;a href=&quot;#HDFS基本架构和原理&quot; class=&quot;headerlink&quot; title=&quot;HDFS基本架构和原理&quot;&gt;&lt;/a&gt;HDFS基本架构和原理&lt;/h1&gt;&lt;h2 id=&quot;HDFS设计思想&quot;&gt;&lt;a href=&quot;#HDFS设计思想&quot;
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="HDFS" scheme="https://airpoet.github.io/categories/Hadoop/HDFS/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="HDFS" scheme="https://airpoet.github.io/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>关于hexo的时序图插件 hexo-filter-sequence 的巨坑</title>
    <link href="https://airpoet.github.io/2018/06/10/Tools/Hexo/%E5%85%B3%E4%BA%8Ehexo%E7%9A%84%E6%97%B6%E5%BA%8F%E5%9B%BE%E6%8F%92%E4%BB%B6-hexo-filter-sequence-%E7%9A%84%E5%B7%A8%E5%9D%91/"/>
    <id>https://airpoet.github.io/2018/06/10/Tools/Hexo/关于hexo的时序图插件-hexo-filter-sequence-的巨坑/</id>
    <published>2018-06-10T13:13:02.149Z</published>
    <updated>2018-06-10T13:19:32.909Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在写代码的过程中，发现时序图还是挺管用的，简洁明了，于是想用一用。</p><p>结果发到站上，不显示。</p><p>在网上查了下，发现是hexo所使用的Markdown渲染语法不支持。</p><p>这里吐槽下，这里渲染的确实烂，作者为啥不改改..</p><p><br></p><p>于是开始找解决方案，发现大多数都推荐了一个叫<code>hexo-filter-sequence</code>的插件，故安装之。</p><p>结果死活还是不行。</p><p>装了其它的几个flow图，却可以显示。</p><p><strong>当flow图和sequence图同时存在的时候，sequence可以显示，但是sequence单独存在时，死活不显示。</strong></p><p>难不成有啥依赖？必须先使用flow才能使用sequence？逻辑上不应该啊！</p><p>但是事实却是这样！</p><p><br></p><p>网上有一篇大神说要改源码才行，开始没注意，觉得应该不会吧，这么多人使用，源码还会有问题？？</p><p><strong>仔细一看源码，妈的！！心里千万匹草泥马呼啸而过！</strong></p><p><strong>把初始化 sequence，写成了初始化 flow！！！</strong></p><p><strong>把 flow 改成 sequence， 再把 js CDN源换成国内的！</strong></p><p><strong>可以了！！</strong></p><p>再仔细一看，发现最后一次更新是在1年前！</p><p>坑爹的作者，浪费了我至少3-5个小时！！</p><p><br></p><hr><h2 id="下面为部分摘抄"><a href="#下面为部分摘抄" class="headerlink" title="下面为部分摘抄"></a>下面为部分摘抄</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p><a href="https://github.com/bubkoo/hexo-filter-sequence" target="_blank" rel="noopener">hexo-filter-sequence</a> 插件:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install --save hexo-filter-sequence</span><br></pre></td></tr></table></figure><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>站点配置文件 <code>_config.yml</code> 中增加如下配置:</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sequence:</span><br><span class="line">  webfont: https:<span class="comment">//cdn.bootcss.com/webfont/1.6.28/webfontloader.js</span></span><br><span class="line">  raphael: https:<span class="comment">//cdn.bootcss.com/raphael/2.2.7/raphael.min.js</span></span><br><span class="line">  underscore: https:<span class="comment">//cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js</span></span><br><span class="line">  sequence: https:<span class="comment">//cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js</span></span><br><span class="line">  css: # optional, the url for css, such as hand drawn theme </span><br><span class="line">  options: </span><br><span class="line">    theme: simple</span><br><span class="line">    css_class:</span><br></pre></td></tr></table></figure><h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><p>源码修改后才能正常使用，进入插件目录作如下修改：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// index.js</span></span><br><span class="line"><span class="keyword">var</span> assign = <span class="built_in">require</span>(<span class="string">'deep-assign'</span>);</span><br><span class="line"><span class="keyword">var</span> renderer = <span class="built_in">require</span>(<span class="string">'./lib/renderer'</span>);</span><br><span class="line"></span><br><span class="line">hexo.config.sequence = assign(&#123;</span><br><span class="line">  webfont: <span class="string">'https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js'</span>,</span><br><span class="line">  raphael: <span class="string">'https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js'</span>,</span><br><span class="line">  underscore: <span class="string">'https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js'</span>,</span><br><span class="line">  sequence: <span class="string">'https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js'</span>,</span><br><span class="line">  css: <span class="string">''</span>,</span><br><span class="line">  options: &#123;</span><br><span class="line">    theme: <span class="string">'simple'</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;, hexo.config.sequence);</span><br><span class="line"></span><br><span class="line">hexo.extend.filter.register(<span class="string">'before_post_render'</span>, renderer.render, <span class="number">9</span>);</span><br></pre></td></tr></table></figure><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// lib/renderer.js, 25 行</span></span><br><span class="line"><span class="keyword">if</span> (sequences.length) &#123;</span><br><span class="line">      <span class="keyword">var</span> config = <span class="keyword">this</span>.config.sequence;</span><br><span class="line">      <span class="comment">// resources</span></span><br><span class="line">      data.content += <span class="string">'&lt;script src="'</span> + config.webfont + <span class="string">'"&gt;&lt;/script&gt;'</span>;</span><br><span class="line">      data.content += <span class="string">'&lt;script src="'</span> + config.raphael + <span class="string">'"&gt;&lt;/script&gt;'</span>;</span><br><span class="line">      data.content += <span class="string">'&lt;script src="'</span> + config.underscore + <span class="string">'"&gt;&lt;/script&gt;'</span>;</span><br><span class="line">      data.content += <span class="string">'&lt;script src="'</span> + config.sequence + <span class="string">'"&gt;&lt;/script&gt;'</span>;</span><br><span class="line">      ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>新建代码块，增加如下内容：</p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-10-131910.png" alt="image-20180610211909603"></p><div id="sequence-0"></div><p><a href="http://wewelove.github.io/fcoder/2017/09/06/markdown-sequence/index.html" target="_blank" rel="noopener">详情参考</a></p><p><script src="https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js"></script><script src="https://cdn.bootcss.com/snap.svg/0.5.1/snap.svg-min.js"></script><script src="https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js"></script><script src="https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js"></script><script src="https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js"></script><textarea id="sequence-0-code" style="display: none">Alice->Bob: Hello Bob, how are you?Note right of Bob: Bob thinksBob-->Alice: I am good thanks!</textarea><textarea id="sequence-0-options" style="display: none">{"theme":"simple"}</textarea><script>  var code = document.getElementById("sequence-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value));  var diagram = Diagram.parse(code);  diagram.drawSVG("sequence-0", options);</script></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在写代码的过程中，发现时序图还是挺管用的，简洁明了，于是想用一用。&lt;/p&gt;
&lt;p&gt;结果发到站上，不显示。&lt;/p&gt;
&lt;p&gt;在网上查了下，发现是
      
    
    </summary>
    
      <category term="工具" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="hexo" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/hexo/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="工具" scheme="https://airpoet.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="hexo" scheme="https://airpoet.github.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce-分组浅探</title>
    <link href="https://airpoet.github.io/2018/06/10/Hadoop/2-MapReduce/MapReduce-%E5%88%86%E7%BB%84%E6%B5%85%E6%8E%A2/"/>
    <id>https://airpoet.github.io/2018/06/10/Hadoop/2-MapReduce/MapReduce-分组浅探/</id>
    <published>2018-06-10T08:49:24.337Z</published>
    <updated>2018-06-11T11:46:23.533Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>最近学分组的时候，一直弄不明白，总感觉一头雾水，通过近3个小时的断点调试和log输出。 大概了解了一些表象，先记录。</p><p>案例是这个</p><blockquote><p>求出每门课程参考学生成绩最高平均分的学生的信息：</p><p>课程，姓名和平均分，详细见<a href="https://airpoet.github.io/2018/06/09/Hadoop/Study/MapReduce%E7%AC%94%E8%AE%B0-%E7%BB%83%E4%B9%A0/"><sup>MapReduce笔记-练习第二题第3小题</sup></a></p><p>数据格式是这样的：</p><blockquote><p>第一个是课程名称，总共四个课程，computer，math，english，algorithm，</p><p>第二个是学生姓名，后面是每次考试的分数</p><p><em>math,huangxiaoming,85,75,85,99,66,88,75,91</em></p><p><em>english,huanglei,85,75,85,99,66,88,75,91</em></p><p>… </p></blockquote></blockquote><p><br></p><p><br></p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><div id="sequence-0"></div><h4 id="执行流程结论"><a href="#执行流程结论" class="headerlink" title="执行流程结论"></a><strong>执行流程结论</strong></h4><ul><li>map每读一行就 write 到 context 一次，按照指定的<code>key</code>进行分发</li><li><p>map 把所有的数据都读完了之后，大概执行到<code>67%</code>的时候，开始进入 <code>CustomBean</code>，执行<code>CustomBean</code>的<code>compareTo()</code>方法，会按照自己写的规则一条一条数据比较</p></li><li><p>上述都比较完毕之后，map阶段就结束了，此时来到了 reduce阶段，但是是到了<code>67%</code>了</p></li><li>到了reduce阶段，直接进入了<strong>MyGroup</strong>中自定义的<strong>compare</strong>方法。</li><li>MyGroup的<code>compare()</code>方法，如果返回<strong>非0</strong>， 就会进入 reduce 方法<strong>写出到context</strong></li></ul><h4 id="MyGroup进入Reduce的条件是"><a href="#MyGroup进入Reduce的条件是" class="headerlink" title="MyGroup进入Reduce的条件是"></a><strong>MyGroup进入Reduce的条件是</strong></h4><ul><li>MyReduce中，如果compare的结果不等于0，也就是比较的2者不相同， 此时就进入Reduce， 写出到上下文</li><li>如果相同，会一直往下读，直到读到不同的， 此时写出读到上下文。</li><li>因为MyGroup会在Reduce阶段执行，而<code>CustomBean</code>中的<code>compareTo()</code>是在map阶段执行，所以需要在<code>CustomBean</code>中就把组排好序，此时<strong>分组功能</strong>才能正常运作</li></ul><h4 id="指定分组类MyGroup和不指定的区别"><a href="#指定分组类MyGroup和不指定的区别" class="headerlink" title="指定分组类MyGroup和不指定的区别"></a>指定分组类MyGroup和不指定的区别</h4><p><em>指定与不指定是指：在Driver类中，是否加上<code>job.setGroupingComparatorClass(MyGrouper.class);</code>这一句。</em></p><ul><li><strong>指定分组类</strong>：<ul><li>会按照分组类中，自定义的<code>compare()</code>方法比较，相同的为一组，分完一组就进入一次reduce方法</li></ul></li><li><strong>不指定分组类：（目前存疑）</strong><ul><li>是否是按照key进行分组</li><li>如果是自定义类为key，是否是按照此key中值相同的分为一组</li><li>如果是hadoop内置类，是否是按照此类的值分组（Text-String的值，IntWritable-int值等..）</li><li><strong>依然是走得以上这套分组逻辑，一组的数据读完才进入到Reduce阶段做归并</strong></li></ul></li></ul><p><br></p><p><br></p><h3 id="Log信息"><a href="#Log信息" class="headerlink" title="Log信息"></a>Log信息</h3><h5 id="CustomBean中没有进行分组-组内排序的log"><a href="#CustomBean中没有进行分组-组内排序的log" class="headerlink" title="CustomBean中没有进行分组, 组内排序的log"></a><code>CustomBean</code>中没有进行<code>分组</code>, <code>组内排序</code>的log</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ==================MyGroup中compare()方法=======================</span></span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// ======================reduce中的执行log==========================</span></span><br><span class="line"></span><br><span class="line">==================第<span class="number">1</span>次进入reduce</span><br><span class="line">computerhuangjiaju<span class="number">83.2</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerliutao<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">2</span>次进入reduce</span><br><span class="line">mathhuangxiaoming<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">3</span>次进入reduce</span><br><span class="line">englishhuanglei<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">4</span>次进入reduce</span><br><span class="line">mathhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">5</span>次进入reduce</span><br><span class="line">algorithmhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmliutao<span class="number">82.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">6</span>次进入reduce</span><br><span class="line">computerhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">7</span>次进入reduce</span><br><span class="line">englishliuyifei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">8</span>次进入reduce</span><br><span class="line">algorithmhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">9</span>次进入reduce</span><br><span class="line">mathhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">10</span>次进入reduce</span><br><span class="line">algorithmhuangzitao<span class="number">72.75</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">11</span>次进入reduce</span><br><span class="line">mathliujialing<span class="number">72.75</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">12</span>次进入reduce</span><br><span class="line">computerhuangzitao<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">13</span>次进入reduce</span><br><span class="line">englishhuangxiaoming<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">14</span>次进入reduce</span><br><span class="line">mathwangbaoqiang<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">15</span>次进入reduce</span><br><span class="line">computerhuangxiaoming<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">16</span>次进入reduce</span><br><span class="line">mathxuzheng<span class="number">69.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">17</span>次进入reduce</span><br><span class="line">englishzhaobenshan<span class="number">69.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">18</span>次进入reduce</span><br><span class="line">computerhuangbo<span class="number">65.25</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerxuzheng<span class="number">65.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">19</span>次进入reduce</span><br><span class="line">englishzhouqi<span class="number">64.18181818181819</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">20</span>次进入reduce</span><br><span class="line">computerliujialing<span class="number">64.11111111111111</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">21</span>次进入reduce</span><br><span class="line">algorithmliuyifei<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">22</span>次进入reduce</span><br><span class="line">englishliujialing<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">23</span>次进入reduce</span><br><span class="line">computerliuyifei<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">24</span>次进入reduce</span><br><span class="line">englishliuyifei<span class="number">59.57142857142857</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">25</span>次进入reduce</span><br><span class="line">mathliutao<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">26</span>次进入reduce</span><br><span class="line">algorithmhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">27</span>次进入reduce</span><br><span class="line">computerhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">28</span>次进入reduce</span><br><span class="line">englishhuangbo<span class="number">55.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br></pre></td></tr></table></figure><h5 id="CustomBean中做了分组-amp-组内排序-的"><a href="#CustomBean中做了分组-amp-组内排序-的" class="headerlink" title="CustomBean中做了分组&amp;组内排序 的"></a><code>CustomBean</code>中做了<code>分组&amp;组内排序</code> 的</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// **Bean中相同的组进行分数降序， 组进行字典排序，此时MyGroup中执行的**</span></span><br><span class="line"></span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line"></span><br><span class="line"><span class="comment">// ======================reduce中执行==========================</span></span><br><span class="line"></span><br><span class="line">==================第<span class="number">1</span>次进入reduce</span><br><span class="line">algorithmhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmliutao<span class="number">82.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmhuangzitao<span class="number">72.75</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmliuyifei<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">===================离开reduce</span><br><span class="line">==================第<span class="number">2</span>次进入reduce</span><br><span class="line">computerhuangjiaju<span class="number">83.2</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerliutao<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerhuangzitao<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerhuangxiaoming<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerhuangbo<span class="number">65.25</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerxuzheng<span class="number">65.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerliujialing<span class="number">64.11111111111111</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerliuyifei<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">===================离开reduce</span><br><span class="line">==================第<span class="number">3</span>次进入reduce</span><br><span class="line">englishhuanglei<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishliuyifei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishhuangxiaoming<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishzhaobenshan<span class="number">69.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishzhouqi<span class="number">64.18181818181819</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishliujialing<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishliuyifei<span class="number">59.57142857142857</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishhuangbo<span class="number">55.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">===================离开reduce</span><br><span class="line">==================第<span class="number">4</span>次进入reduce</span><br><span class="line">mathhuangxiaoming<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathliujialing<span class="number">72.75</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathwangbaoqiang<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathxuzheng<span class="number">69.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathliutao<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">===================离开reduce</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//  如果只取一个每次values的第一个的话 </span></span><br><span class="line"></span><br><span class="line">algorithmhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">==================第<span class="number">1</span>次进入reduce</span><br><span class="line">computerhuangjiaju<span class="number">83.2</span></span><br><span class="line">==================第<span class="number">2</span>次进入reduce</span><br><span class="line">englishhuanglei<span class="number">83.0</span></span><br><span class="line">==================第<span class="number">3</span>次进入reduce</span><br><span class="line">mathhuangxiaoming<span class="number">83.0</span></span><br><span class="line">==================第<span class="number">4</span>次进入reduce</span><br></pre></td></tr></table></figure><p><br></p><p><br></p><h3 id="其它疑点"><a href="#其它疑点" class="headerlink" title="其它疑点"></a>其它疑点</h3><ul><li>通过log可以看出来， MyGroup是在读到了不同的数据，才会进入到 reduce 类中写出；</li><li>但是 通过 <strong>断点调试</strong>时， 现象是，第一次读到了2个相同的，就去reduce去写出了；</li><li>后面再读的就不做写出操作，直到下一次读到2个相同的，再在reduce中写出。</li></ul><p><script src="https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js"></script><script src="https://cdn.bootcss.com/snap.svg/0.5.1/snap.svg-min.js"></script><script src="https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js"></script><script src="https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js"></script><script src="https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js"></script><textarea id="sequence-0-code" style="display: none">title: 执行流程时序图 Mapper(map)->ScoreBean: k:ScoreBean(courseName,avgScore)Mapper(map)->ScoreBean: v:Text(stuName)Mapper(ScoreBean)->Reducer(MyGroup): course按字典升序Mapper(ScoreBean)->Reducer(MyGroup): course内成绩降序Reducer(MyGroup)->Reducer(reduce): 根据自定义的分组规则按组输出Reducer(MyGroup)->Reducer(reduce): 一组只调用reduce一次Reducer(reduce)-->Reducer(reduce):</textarea><textarea id="sequence-0-options" style="display: none">{"theme":"simple"}</textarea><script>  var code = document.getElementById("sequence-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value));  var diagram = Diagram.parse(code);  diagram.drawSVG("sequence-0", options);</script></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;最近学分组的时候，一直弄不明白，总感觉一头雾水，通过近3个小时的断点调试和log输出。 大概了解了一些表象，先记录。&lt;/p&gt;
&lt;p&gt;案例是这
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>▍开始</title>
    <link href="https://airpoet.github.io/2018/06/10/Poetry/%E5%BC%80%E5%A7%8B/"/>
    <id>https://airpoet.github.io/2018/06/10/Poetry/开始/</id>
    <published>2018-06-09T17:03:10.001Z</published>
    <updated>2018-06-09T17:19:40.887Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-09-601528563678_.pic.jpg" alt=""></p><p>月亮落下一两片羽毛在田野上。</p><p>黑暗中的麦子聆听着。</p><p>快静下来。</p><p>快。</p><p>就在那儿，月亮的孩子们正试着</p><p>挥动翅膀。</p><p>在两棵树之间，身材修长的女子抬起面庞，</p><p>美丽的剪影。接着，她步入空中，接着，</p><p>她完全消失在空中。</p><p>我独自站在一棵接骨木旁，不敢呼吸，</p><p>也不敢动。</p><p>我聆听着。</p><p>麦子向后靠着自己的黑暗，</p><p>而我靠着我的。</p><p><br></p><p><strong>作者 / [美国] 詹姆斯·赖特</strong></p><p>翻译 / 张文武</p><hr><h3 id="▍Beginning"><a href="#▍Beginning" class="headerlink" title="▍Beginning"></a><strong>▍Beginning</strong></h3><p><br></p><p>The moon drops one or two feathers into the fields.</p><p>The dark wheat listens.</p><p>Be still.</p><p>Now.</p><p>There they are, the moon’s young, trying</p><p>Their wings.</p><p>Between trees, a slender woman lifts up the lovely shadow</p><p>Of her face, and now she steps into the air, now she is gone</p><p>Wholly, into the air.</p><p>I stand alone by an elder tree, I do not dare breathe</p><p>Or move.</p><p>I listen.</p><p>The wheat leans back toward its own darkness,</p><p>And I lean toward mine.</p><p><br></p><p><strong>Author / James Wright</strong></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-09-601528563678_.pic.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;月
      
    
    </summary>
    
      <category term="文艺" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/"/>
    
      <category term="诗歌" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/%E8%AF%97%E6%AD%8C/"/>
    
    
      <category term="诗歌" scheme="https://airpoet.github.io/tags/%E8%AF%97%E6%AD%8C/"/>
    
  </entry>
  
  <entry>
    <title>Markdown高阶语法</title>
    <link href="https://airpoet.github.io/2018/06/10/Tools/Markdown/Markdown%E9%AB%98%E9%98%B6%E8%AF%AD%E6%B3%95/"/>
    <id>https://airpoet.github.io/2018/06/10/Tools/Markdown/Markdown高阶语法/</id>
    <published>2018-06-09T16:49:50.407Z</published>
    <updated>2018-06-10T13:21:24.745Z</updated>
    
    <content type="html"><![CDATA[<h3 id="时序图的写法"><a href="#时序图的写法" class="headerlink" title="时序图的写法"></a>时序图的写法</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-09-165900.png" alt="image-20180610005859980"></p><div id="sequence-0"></div><h3 id="流程图的写法"><a href="#流程图的写法" class="headerlink" title="流程图的写法"></a>流程图的写法</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-09-165930.png" alt="image-20180610005930019"></p><div id="flowchart-0" class="flow-chart"></div><h3 id="类图的写法"><a href="#类图的写法" class="headerlink" title="类图的写法"></a>类图的写法</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-09-165948.png" alt="image-20180610005948130"></p><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLl2jz4qiA4Wjud98pKi12WC0"></p><p><script src="https://cdnjs.cloudflare.com/ajax/libs/raphael/2.2.7/raphael.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/flowchart/1.6.5/flowchart.min.js"></script><textarea id="flowchart-0-code" style="display: none">st=>start: Start|past:>http://www.google.com[blank]e=>end: End:>http://www.google.comop1=>operation: My Operation|pastop2=>operation: Stuff|currentsub1=>subroutine: My Subroutine|invalidcond=>condition: Yesor No?|approved:>http://www.google.comc2=>condition: Good idea|rejectedio=>inputoutput: catch something...|requestst->op1(right)->condcond(yes, right)->c2cond(no)->sub1(left)->op1c2(yes)->io->ec2(no)->op2->e</textarea><textarea id="flowchart-0-options" style="display: none">{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("flowchart-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value));  var diagram = flowchart.parse(code);  diagram.drawSVG("flowchart-0", options);</script><script src="https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js"></script><script src="https://cdn.bootcss.com/snap.svg/0.5.1/snap.svg-min.js"></script><script src="https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js"></script><script src="https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js"></script><script src="https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js"></script><textarea id="sequence-0-code" style="display: none">mapper->Bob: Hello Bob, how are you?Note right of Bob: Bob thinksBob-->reducer: I am good thanks!ad u?reducer->out: I'm fine tooout->me: ok, you winme-->Bob: nono, not</textarea><textarea id="sequence-0-options" style="display: none">{"theme":"simple"}</textarea><script>  var code = document.getElementById("sequence-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value));  var diagram = Diagram.parse(code);  diagram.drawSVG("sequence-0", options);</script></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;时序图的写法&quot;&gt;&lt;a href=&quot;#时序图的写法&quot; class=&quot;headerlink&quot; title=&quot;时序图的写法&quot;&gt;&lt;/a&gt;时序图的写法&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/study/2018-
      
    
    </summary>
    
      <category term="工具" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="Markdown" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/Markdown/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="工具" scheme="https://airpoet.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="Markdown" scheme="https://airpoet.github.io/tags/Markdown/"/>
    
  </entry>
  
  <entry>
    <title>About Sublime Text3</title>
    <link href="https://airpoet.github.io/2018/06/09/Tools/Sublime/%E5%AE%89%E8%A3%85%E4%B8%BB%E9%A2%98/"/>
    <id>https://airpoet.github.io/2018/06/09/Tools/Sublime/安装主题/</id>
    <published>2018-06-09T04:43:17.189Z</published>
    <updated>2018-06-10T01:09:27.890Z</updated>
    
    <content type="html"><![CDATA[<h3 id="主题"><a href="#主题" class="headerlink" title="主题"></a>主题</h3><p><a href="https://scotch.io/bar-talk/best-sublime-text-3-themes-of-2015-and-2016" target="_blank" rel="noopener">详情参见这个网站</a></p><h3 id="详细操作"><a href="#详细操作" class="headerlink" title="详细操作"></a>详细操作</h3><p><a href="http://zh.lucida.me/blog/sublime-text-complete-guide/" target="_blank" rel="noopener">见此站</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;主题&quot;&gt;&lt;a href=&quot;#主题&quot; class=&quot;headerlink&quot; title=&quot;主题&quot;&gt;&lt;/a&gt;主题&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://scotch.io/bar-talk/best-sublime-text-3-themes-of-2015
      
    
    </summary>
    
      <category term="工具" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="Sublime" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/Sublime/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="工具" scheme="https://airpoet.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="Sublime" scheme="https://airpoet.github.io/tags/Sublime/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce笔记-Bug汇总</title>
    <link href="https://airpoet.github.io/2018/06/09/Hadoop/2-MapReduce/MapReduce-Bug%E6%B1%87%E6%80%BB/"/>
    <id>https://airpoet.github.io/2018/06/09/Hadoop/2-MapReduce/MapReduce-Bug汇总/</id>
    <published>2018-06-09T03:11:48.470Z</published>
    <updated>2018-06-11T11:46:16.724Z</updated>
    
    <content type="html"><![CDATA[<h4 id="1、reduce-输出路径必须是新创建的。不能已经存在"><a href="#1、reduce-输出路径必须是新创建的。不能已经存在" class="headerlink" title="1、reduce 输出路径必须是新创建的。不能已经存在"></a>1、reduce 输出路径必须是新创建的。不能已经存在</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread <span class="string">"main"</span> org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs:<span class="comment">//cs1:9000/flowout01 already exists</span></span><br></pre></td></tr></table></figure><h4 id="2、在初始化-job-的时候，-没有传-conf-，-导致后面一直找不到文件，-因为不知道到哪里去找"><a href="#2、在初始化-job-的时候，-没有传-conf-，-导致后面一直找不到文件，-因为不知道到哪里去找" class="headerlink" title="2、在初始化 job 的时候， 没有传 conf ， 导致后面一直找不到文件， 因为不知道到哪里去找"></a>2、在初始化 job 的时候， 没有传 <code>conf</code> ， 导致后面一直找不到文件， 因为不知道到哪里去找</h4><h4 id="3、Text导包倒错-导的时候要注意"><a href="#3、Text导包倒错-导的时候要注意" class="headerlink" title="3、Text导包倒错, 导的时候要注意"></a>3、Text导包倒错, 导的时候要注意</h4><p><strong>应该是这个 <code>import org.apache.hadoop.io.Text;</code></strong></p><h4 id="4、进行字符串拼接的时候，把-StringBuilder-写到了-reduce-方法外，-这样导致-sb-会越来越多，当然，也可以每次拼接完了清空"><a href="#4、进行字符串拼接的时候，把-StringBuilder-写到了-reduce-方法外，-这样导致-sb-会越来越多，当然，也可以每次拼接完了清空" class="headerlink" title="4、进行字符串拼接的时候，把 StringBuilder 写到了 reduce 方法外， 这样导致 sb 会越来越多，当然，也可以每次拼接完了清空"></a>4、进行字符串拼接的时候，把 <strong>StringBuilder 写到了 reduce 方法外</strong>， 这样导致 sb 会越来越多，当然，也可以每次拼接完了清空</h4><p>类似于这样</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">AF,I,O,K,G,D,C,H,B</span><br><span class="line">BF,I,O,K,G,D,C,H,B,E,J,F,A</span><br><span class="line">CF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F</span><br><span class="line">DF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L</span><br><span class="line">EF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H</span><br><span class="line">FF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G</span><br><span class="line">GF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M</span><br><span class="line">HF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O</span><br><span class="line">IF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C</span><br><span class="line">JF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O</span><br><span class="line">KF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,B</span><br><span class="line">LF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,B,D,E</span><br><span class="line">MF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,B,D,E,E,F</span><br><span class="line">OF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,B,D,E,E,F,A,H,I,J,F</span><br></pre></td></tr></table></figure><h4 id="4、mapreduce执行错误Mapper-错误"><a href="#4、mapreduce执行错误Mapper-错误" class="headerlink" title="4、mapreduce执行错误Mapper.\错误"></a>4、mapreduce执行错误Mapper.\<init>错误</init></h4><ul><li>Mapper &amp; Reducer 写成内部类的时候，有没有加上 <code>static</code></li><li>Bean类有没有无参构造</li></ul><h4 id="5、排序过程中，自定义了排序类，bean类的-compareTo-方法，只写了按照分数大小排序。"><a href="#5、排序过程中，自定义了排序类，bean类的-compareTo-方法，只写了按照分数大小排序。" class="headerlink" title="5、排序过程中，自定义了排序类，bean类的 compareTo()方法，只写了按照分数大小排序。"></a>5、排序过程中，自定义了排序类，bean类的 <code>compareTo()</code>方法，只写了按照分数大小排序。</h4><p><u>会出现如下错误： 课程并没有分组</u></p><p>没有在相同的一组课程中比较分数， 而是比较的所有的分数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">computerhuangjiaju<span class="number">83.2</span></span><br><span class="line">mathhuangxiaoming<span class="number">83.0</span></span><br><span class="line">englishhuanglei<span class="number">83.0</span></span><br><span class="line">mathhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">algorithmhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">computerhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">englishliuyifei<span class="number">74.42857142857143</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><p><strong>此时应该在Bean对象中做如下事情</strong></p><ul><li>相同课程的按照分数降序排序</li><li>课程名按照自然（升序）排序</li><li><strong>换言之，就是CustomBean 对象要输出的数据是 <code>组名升序排序，组内按成绩降序排序</code></strong></li><li><a href="https://airpoet.github.io/2018/06/10/Hadoop/Study/MapReduce-%E5%88%86%E7%BB%84%E6%B5%85%E6%8E%A2/#more">具体分析参阅</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;1、reduce-输出路径必须是新创建的。不能已经存在&quot;&gt;&lt;a href=&quot;#1、reduce-输出路径必须是新创建的。不能已经存在&quot; class=&quot;headerlink&quot; title=&quot;1、reduce 输出路径必须是新创建的。不能已经存在&quot;&gt;&lt;/a&gt;1、red
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce笔记-练习</title>
    <link href="https://airpoet.github.io/2018/06/09/Hadoop/2-MapReduce/MapReduce-%E7%BB%83%E4%B9%A0/"/>
    <id>https://airpoet.github.io/2018/06/09/Hadoop/2-MapReduce/MapReduce-练习/</id>
    <published>2018-06-09T03:11:16.234Z</published>
    <updated>2018-06-11T14:15:19.807Z</updated>
    
    <content type="html"><![CDATA[<h2 id="求微博共同粉丝"><a href="#求微博共同粉丝" class="headerlink" title="求微博共同粉丝"></a>求微博共同粉丝</h2><h3 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h3><p><strong>涉及知识点： 多 Job 串联</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">A:B,C,D,F,E,O</span><br><span class="line">B:A,C,E,K</span><br><span class="line">C:F,A,D,I</span><br><span class="line">D:A,E,F,L</span><br><span class="line">E:B,C,D,M,L</span><br><span class="line">F:A,B,C,D,E,O,M</span><br><span class="line">G:A,C,D,E,F</span><br><span class="line">H:A,C,D,E,O</span><br><span class="line">I:A,O</span><br><span class="line">J:B,O</span><br><span class="line">K:A,C,D</span><br><span class="line">L:D,E,F</span><br><span class="line">M:E,F,G</span><br><span class="line">O:A,H,I,J,K</span><br></pre></td></tr></table></figure><blockquote><p>以上是数据：<br>A:B,C,D,F,E,O<br>表示：A用户 关注B,C,D,E,F,O</p><blockquote><p>求所有两两用户之间的共同关注对象</p></blockquote></blockquote><h3 id="答案："><a href="#答案：" class="headerlink" title="答案："></a>答案：</h3><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLd3EpytDptDBp2jsIStDvt98pKi1IW80"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._01_多Job串联;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CommonFansDemo</span> </span>&#123;</span><br><span class="line"><span class="meta">@SuppressWarnings</span>(<span class="string">"deprecation"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// Job 逻辑</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 HDFS 相关的参数</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// 新建一个 job1</span></span><br><span class="line">Job job1 = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置 Jar 包所在路径</span></span><br><span class="line">job1.setJarByClass(CommonFansDemo.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 mapper 类和 reducer 类</span></span><br><span class="line">job1.setMapperClass(MyMapper_Step1.class);</span><br><span class="line">job1.setReducerClass(MyReducer_Step1.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 maptask 的输出类型</span></span><br><span class="line">job1.setMapOutputKeyClass(Text.class);</span><br><span class="line">job1.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定最终的输出类型(reduce存在时,就是指 ReduceTask 的输出类型)</span></span><br><span class="line">job1.setOutputKeyClass(Text.class);</span><br><span class="line">job1.setOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定该 MapReduce 程序数据的输入输出路径</span></span><br><span class="line">FileInputFormat.setInputPaths(job1, <span class="keyword">new</span> Path(<span class="string">"/in/commonfriend"</span>));</span><br><span class="line">FileOutputFormat.setOutputPath(job1, <span class="keyword">new</span> Path(<span class="string">"/out/job1"</span>));</span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// 新建一个 job2</span></span><br><span class="line">Job job2 = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置 Jar 包所在路径</span></span><br><span class="line">job2.setJarByClass(CommonFansDemo.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 mapper 类和 reducer 类</span></span><br><span class="line">job2.setMapperClass(MyMapper_Step2.class);</span><br><span class="line">job2.setReducerClass(MyReducer_Step2.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 maptask 的输出类型</span></span><br><span class="line">job2.setMapOutputKeyClass(Text.class);</span><br><span class="line">job2.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定最终的输出类型(reduce存在时,就是指 ReduceTask 的输出类型)</span></span><br><span class="line">job2.setOutputKeyClass(Text.class);</span><br><span class="line">job2.setOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定该 MapReduce 程序数据的输入输出路径</span></span><br><span class="line">FileInputFormat.setInputPaths(job2, <span class="keyword">new</span> Path(<span class="string">"/out/job1"</span>));</span><br><span class="line">FileOutputFormat.setOutputPath(job2, <span class="keyword">new</span> Path(<span class="string">"/out/job2"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将多个 job 当做一个组中的 job 提交, 参数名是组名</span></span><br><span class="line"><span class="comment"> * 注意: JobControl 是实现了 Runnable 接口的 </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">JobControl jControl = <span class="keyword">new</span> JobControl(<span class="string">"common_friend"</span>);</span><br><span class="line"><span class="comment">// 将原生的 job携带配置 转换为可控的 job</span></span><br><span class="line">ControlledJob aJob = <span class="keyword">new</span> ControlledJob(job1.getConfiguration());</span><br><span class="line">ControlledJob bJob = <span class="keyword">new</span> ControlledJob(job2.getConfiguration());</span><br><span class="line"><span class="comment">// 添加依赖关系</span></span><br><span class="line">bJob.addDependingJob(aJob);</span><br><span class="line"><span class="comment">// 添加 job 到组中</span></span><br><span class="line">jControl.addJob(aJob);</span><br><span class="line">jControl.addJob(bJob);</span><br><span class="line"><span class="comment">// 启动一个线程</span></span><br><span class="line">Thread jobThread = <span class="keyword">new</span> Thread(jControl);</span><br><span class="line">jobThread.start();</span><br><span class="line"><span class="keyword">while</span> (!jControl.allFinished()) &#123;</span><br><span class="line">Thread.sleep(<span class="number">500</span>);</span><br><span class="line">&#125;</span><br><span class="line">jobThread.stop();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper_Step1</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">String[] user_attentions;</span><br><span class="line">String[] attentions;</span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value,</span></span></span><br><span class="line"><span class="function"><span class="params">Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">user_attentions = value.toString().split(<span class="string">":"</span>);</span><br><span class="line">attentions = user_attentions[<span class="number">1</span>].trim().split(<span class="string">","</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (String att : attentions) &#123;</span><br><span class="line">k.set(att);</span><br><span class="line">v.set(user_attentions[<span class="number">0</span>].trim());</span><br><span class="line">context.write(k, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 将两两粉丝(普通用户)拼接起来, 格式a-f:c =&gt; a,b 都共同关注了 c</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> *  AF,I,O,K,G,D,C,H,B</span></span><br><span class="line"><span class="comment">BE,J,F,A</span></span><br><span class="line"><span class="comment">CB,E,K,A,H,G,F</span></span><br><span class="line"><span class="comment">DH,C,G,F,E,A,K,L</span></span><br><span class="line"><span class="comment">EA,B,L,G,M,F,D,H</span></span><br><span class="line"><span class="comment">FC,M,L,A,D,G</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper_Step2</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">String[] attenion_users;</span><br><span class="line">String[] users;</span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value,</span></span></span><br><span class="line"><span class="function"><span class="params">Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">attenion_users = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">users = attenion_users[<span class="number">1</span>].trim().split(<span class="string">","</span>);</span><br><span class="line"><span class="keyword">for</span> (String u1 : users) &#123;</span><br><span class="line"><span class="keyword">for</span> (String u2 : users) &#123;</span><br><span class="line"><span class="keyword">if</span> (u1.compareTo(u2) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">String users = u1 + <span class="string">"-"</span> + u2;</span><br><span class="line">k.set(users);</span><br><span class="line">v.set(attenion_users[<span class="number">0</span>].trim());</span><br><span class="line">context.write(k, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> *需要统计的是, 某人拥有的全部粉丝</span></span><br><span class="line"><span class="comment"> *  key: 传过来的 key</span></span><br><span class="line"><span class="comment"> *  value:  用,分割 </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer_Step1</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Reducer&lt;Text, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注意: 这里 sb 不能写在外面,会不断的拼接</span></span><br><span class="line">StringBuilder sb = <span class="keyword">new</span> StringBuilder();</span><br><span class="line"><span class="keyword">for</span> (Text v : values) &#123;</span><br><span class="line">sb.append(v.toString()).append(<span class="string">","</span>);</span><br><span class="line">&#125;</span><br><span class="line">k.set(key);</span><br><span class="line">v.set(sb.substring(<span class="number">0</span>, sb.length() - <span class="number">1</span>));</span><br><span class="line">context.write(k, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 拿到的数据: a-b c</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer_Step2</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Reducer&lt;Text, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">StringBuilder sb = <span class="keyword">new</span> StringBuilder();</span><br><span class="line"><span class="keyword">for</span> (Text attention : values) &#123;</span><br><span class="line">sb.append(attention.toString()).append(<span class="string">","</span>);</span><br><span class="line">&#125;</span><br><span class="line">k.set(key);</span><br><span class="line">v.set(sb.substring(<span class="number">0</span>, sb.length() - <span class="number">1</span>));</span><br><span class="line">context.write(k, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// job1的输出</span></span><br><span class="line">AF,I,O,K,G,D,C,H,B</span><br><span class="line">BE,J,F,A</span><br><span class="line">CB,E,K,A,H,G,F</span><br><span class="line">DH,C,G,F,E,A,K,L</span><br><span class="line">EA,B,L,G,M,F,D,H</span><br><span class="line">FC,M,L,A,D,G</span><br><span class="line">GM</span><br><span class="line">HO</span><br><span class="line">IO,C</span><br><span class="line">JO</span><br><span class="line">KO,B</span><br><span class="line">LD,E</span><br><span class="line">ME,F</span><br><span class="line">OA,H,I,J,F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// job2的输出</span></span><br><span class="line">A-BE,C</span><br><span class="line">A-CD,F</span><br><span class="line">A-DF,E</span><br><span class="line">A-EC,D,B</span><br><span class="line">A-FO,B,E,D,C</span><br><span class="line">A-GE,F,D,C</span><br><span class="line">A-HO,E,D,C</span><br><span class="line">A-IO</span><br><span class="line">A-JB,O</span><br><span class="line">A-KD,C</span><br><span class="line">A-LD,F,E</span><br><span class="line">A-ME,F</span><br><span class="line">B-CA</span><br><span class="line">B-DE,A</span><br><span class="line">B-EC</span><br><span class="line">B-FA,E,C</span><br><span class="line">B-GC,A,E</span><br><span class="line">B-HA,E,C</span><br><span class="line">B-IA</span><br><span class="line">B-KC,A</span><br><span class="line">B-LE</span><br><span class="line">B-ME</span><br><span class="line">B-OA,K</span><br><span class="line">C-DA,F</span><br><span class="line">C-ED</span><br><span class="line">C-FD,A</span><br><span class="line">C-GF,A,D</span><br><span class="line">C-HD,A</span><br><span class="line">C-IA</span><br><span class="line">C-KA,D</span><br><span class="line">C-LF,D</span><br><span class="line">C-MF</span><br><span class="line">C-OI,A</span><br><span class="line">D-EL</span><br><span class="line">D-FE,A</span><br><span class="line">D-GA,F,E</span><br><span class="line">D-HE,A</span><br><span class="line">D-IA</span><br><span class="line">D-KA</span><br><span class="line">D-LF,E</span><br><span class="line">D-MF,E</span><br><span class="line">D-OA</span><br><span class="line">E-FC,B,M,D</span><br><span class="line">E-GC,D</span><br><span class="line">E-HC,D</span><br><span class="line">E-JB</span><br><span class="line">E-KD,C</span><br><span class="line">E-LD</span><br><span class="line">F-GA,D,C,E</span><br><span class="line">F-HA,E,C,D,O</span><br><span class="line">F-IO,A</span><br><span class="line">F-JO,B</span><br><span class="line">F-KC,A,D</span><br><span class="line">F-LE,D</span><br><span class="line">F-ME</span><br><span class="line">F-OA</span><br><span class="line">G-HA,C,D,E</span><br><span class="line">G-IA</span><br><span class="line">G-KC,A,D</span><br><span class="line">G-LD,E,F</span><br><span class="line">G-MF,E</span><br><span class="line">G-OA</span><br><span class="line">H-IO,A</span><br><span class="line">H-JO</span><br><span class="line">H-KA,D,C</span><br><span class="line">H-LE,D</span><br><span class="line">H-ME</span><br><span class="line">H-OA</span><br><span class="line">I-JO</span><br><span class="line">I-KA</span><br><span class="line">I-OA</span><br><span class="line">K-LD</span><br><span class="line">K-OA</span><br><span class="line">L-MF,E</span><br></pre></td></tr></table></figure><h2 id="求学生成绩"><a href="#求学生成绩" class="headerlink" title="求学生成绩"></a>求学生成绩</h2><h3 id="题目-1"><a href="#题目-1" class="headerlink" title="题目"></a>题目</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">computer,huangxiaoming,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span></span><br><span class="line">computer,xuzheng,<span class="number">54</span>,<span class="number">52</span>,<span class="number">86</span>,<span class="number">91</span>,<span class="number">42</span></span><br><span class="line">computer,huangbo,<span class="number">85</span>,<span class="number">42</span>,<span class="number">96</span>,<span class="number">38</span></span><br><span class="line">english,zhaobenshan,<span class="number">54</span>,<span class="number">52</span>,<span class="number">86</span>,<span class="number">91</span>,<span class="number">42</span>,<span class="number">85</span>,<span class="number">75</span></span><br><span class="line">english,liuyifei,<span class="number">85</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">21</span>,<span class="number">85</span>,<span class="number">96</span>,<span class="number">14</span></span><br><span class="line">algorithm,liuyifei,<span class="number">75</span>,<span class="number">85</span>,<span class="number">62</span>,<span class="number">48</span>,<span class="number">54</span>,<span class="number">96</span>,<span class="number">15</span></span><br><span class="line">computer,huangjiaju,<span class="number">85</span>,<span class="number">75</span>,<span class="number">86</span>,<span class="number">85</span>,<span class="number">85</span></span><br><span class="line">english,liuyifei,<span class="number">76</span>,<span class="number">95</span>,<span class="number">86</span>,<span class="number">74</span>,<span class="number">68</span>,<span class="number">74</span>,<span class="number">48</span></span><br><span class="line">english,huangdatou,<span class="number">48</span>,<span class="number">58</span>,<span class="number">67</span>,<span class="number">86</span>,<span class="number">15</span>,<span class="number">33</span>,<span class="number">85</span></span><br><span class="line">algorithm,huanglei,<span class="number">76</span>,<span class="number">95</span>,<span class="number">86</span>,<span class="number">74</span>,<span class="number">68</span>,<span class="number">74</span>,<span class="number">48</span></span><br><span class="line">algorithm,huangjiaju,<span class="number">85</span>,<span class="number">75</span>,<span class="number">86</span>,<span class="number">85</span>,<span class="number">85</span>,<span class="number">74</span>,<span class="number">86</span></span><br><span class="line">computer,huangdatou,<span class="number">48</span>,<span class="number">58</span>,<span class="number">67</span>,<span class="number">86</span>,<span class="number">15</span>,<span class="number">33</span>,<span class="number">85</span></span><br><span class="line">english,zhouqi,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span>,<span class="number">75</span>,<span class="number">55</span>,<span class="number">47</span>,<span class="number">22</span></span><br><span class="line">english,huangbo,<span class="number">85</span>,<span class="number">42</span>,<span class="number">96</span>,<span class="number">38</span>,<span class="number">55</span>,<span class="number">47</span>,<span class="number">22</span></span><br><span class="line">algorithm,liutao,<span class="number">85</span>,<span class="number">75</span>,<span class="number">85</span>,<span class="number">99</span>,<span class="number">66</span></span><br><span class="line">computer,huangzitao,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span></span><br><span class="line">math,wangbaoqiang,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span></span><br><span class="line">computer,liujialing,<span class="number">85</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">21</span>,<span class="number">85</span>,<span class="number">96</span>,<span class="number">14</span>,<span class="number">74</span>,<span class="number">86</span></span><br><span class="line">computer,liuyifei,<span class="number">75</span>,<span class="number">85</span>,<span class="number">62</span>,<span class="number">48</span>,<span class="number">54</span>,<span class="number">96</span>,<span class="number">15</span></span><br><span class="line">computer,liutao,<span class="number">85</span>,<span class="number">75</span>,<span class="number">85</span>,<span class="number">99</span>,<span class="number">66</span>,<span class="number">88</span>,<span class="number">75</span>,<span class="number">91</span></span><br><span class="line">computer,huanglei,<span class="number">76</span>,<span class="number">95</span>,<span class="number">86</span>,<span class="number">74</span>,<span class="number">68</span>,<span class="number">74</span>,<span class="number">48</span></span><br><span class="line">english,liujialing,<span class="number">75</span>,<span class="number">85</span>,<span class="number">62</span>,<span class="number">48</span>,<span class="number">54</span>,<span class="number">96</span>,<span class="number">15</span></span><br><span class="line">math,huanglei,<span class="number">76</span>,<span class="number">95</span>,<span class="number">86</span>,<span class="number">74</span>,<span class="number">68</span>,<span class="number">74</span>,<span class="number">48</span></span><br><span class="line">math,huangjiaju,<span class="number">85</span>,<span class="number">75</span>,<span class="number">86</span>,<span class="number">85</span>,<span class="number">85</span>,<span class="number">74</span>,<span class="number">86</span></span><br><span class="line">math,liutao,<span class="number">48</span>,<span class="number">58</span>,<span class="number">67</span>,<span class="number">86</span>,<span class="number">15</span>,<span class="number">33</span>,<span class="number">85</span></span><br><span class="line">english,huanglei,<span class="number">85</span>,<span class="number">75</span>,<span class="number">85</span>,<span class="number">99</span>,<span class="number">66</span>,<span class="number">88</span>,<span class="number">75</span>,<span class="number">91</span></span><br><span class="line">math,xuzheng,<span class="number">54</span>,<span class="number">52</span>,<span class="number">86</span>,<span class="number">91</span>,<span class="number">42</span>,<span class="number">85</span>,<span class="number">75</span></span><br><span class="line">math,huangxiaoming,<span class="number">85</span>,<span class="number">75</span>,<span class="number">85</span>,<span class="number">99</span>,<span class="number">66</span>,<span class="number">88</span>,<span class="number">75</span>,<span class="number">91</span></span><br><span class="line">math,liujialing,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span>,<span class="number">75</span></span><br><span class="line">english,huangxiaoming,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span></span><br><span class="line">algorithm,huangdatou,<span class="number">48</span>,<span class="number">58</span>,<span class="number">67</span>,<span class="number">86</span>,<span class="number">15</span>,<span class="number">33</span>,<span class="number">85</span></span><br><span class="line">algorithm,huangzitao,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span>,<span class="number">75</span></span><br></pre></td></tr></table></figure><blockquote><p>一、数据解释</p><p>数据字段个数不固定：<br>第一个是课程名称，总共四个课程，computer，math，english，algorithm，<br>第二个是学生姓名，后面是每次考试的分数</p><p>二、统计需求：<br>1、统计每门课程的参加考试人数和课程平均分</p><p>2、统计每门课程参考学生的平均分，并且按课程存入不同的结果文件，要求一门课程一个结果文件</p><p>3、求出每门课程参考学生成绩最高平均分的学生的信息：课程，姓名和平均分</p></blockquote><h3 id="答案"><a href="#答案" class="headerlink" title="答案"></a>答案</h3><h4 id="第1小题"><a href="#第1小题" class="headerlink" title="第1小题"></a>第1小题</h4><p><strong>统计每门课程的参考人数和课程平均分</strong></p><p><strong>涉及知识点: 去重， 自定义类</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//  ScoreBean </span></span><br><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._02_分组组件;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.AllArgsConstructor;</span><br><span class="line"><span class="keyword">import</span> lombok.Getter;</span><br><span class="line"><span class="keyword">import</span> lombok.NoArgsConstructor;</span><br><span class="line"><span class="keyword">import</span> lombok.Setter;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Getter</span></span><br><span class="line"><span class="meta">@Setter</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ScoreBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">ScoreBean</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> String courseName;</span><br><span class="line"><span class="keyword">private</span> String stuName; </span><br><span class="line"><span class="keyword">private</span> Double score;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">out.writeUTF(courseName);</span><br><span class="line">out.writeUTF(stuName);</span><br><span class="line">out.writeDouble(score);</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.courseName = in.readUTF();</span><br><span class="line"><span class="keyword">this</span>.stuName = in.readUTF();</span><br><span class="line"><span class="keyword">this</span>.score = in.readDouble();</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 如果是相同课程, 按照分数降序排列的</span></span><br><span class="line"><span class="comment"> * 如果是不同课程, 按照课程名称升序排列</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(ScoreBean o)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 测试一下只写按分数降序排序</span></span><br><span class="line"><span class="comment">//return o.getScore().compareTo(this.getScore());</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*// 首先分组(只在相同的组内进行比较)</span></span><br><span class="line"><span class="comment">int nameRes = this.getCourseName().compareTo(o.getCourseName());</span></span><br><span class="line"><span class="comment">if (nameRes == 0) &#123;</span></span><br><span class="line"><span class="comment">// 课程相同的时候才进行降序排序</span></span><br><span class="line"><span class="comment">int scoreRes = </span></span><br><span class="line"><span class="comment">return scoreRes;</span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment">return nameRes;*/</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString1</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> stuName + <span class="string">"\t"</span> + score;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> courseName + <span class="string">"\t"</span> + stuName</span><br><span class="line">+ <span class="string">"\t"</span> + score;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ScoreBean</span><span class="params">(String stuName, Double score)</span> </span>&#123;</span><br><span class="line"><span class="keyword">super</span>();</span><br><span class="line"><span class="keyword">this</span>.stuName = stuName;</span><br><span class="line"><span class="keyword">this</span>.score = score;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//  ScorePlusDemo1 </span></span><br><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._02_分组组件;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.HashSet;</span><br><span class="line"><span class="keyword">import</span> java.util.Set;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ScorePlusDemo1</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line">job.setJarByClass(ScorePlusDemo1.class);</span><br><span class="line"></span><br><span class="line">job.setMapperClass(MyMapper.class);</span><br><span class="line">job.setReducerClass(MyReducer.class);</span><br><span class="line"></span><br><span class="line">job.setMapOutputKeyClass(Text.class);</span><br><span class="line">job.setMapOutputValueClass(ScoreBean.class);</span><br><span class="line"></span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">String inP = <span class="string">"/in/newScoreIn"</span>;</span><br><span class="line">String outP = <span class="string">"/out/ans1"</span>;</span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(inP));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(outP));</span><br><span class="line"></span><br><span class="line">        Path mypath = <span class="keyword">new</span> Path(outP);</span><br><span class="line">        FileSystem hdfs = mypath.getFileSystem(conf);</span><br><span class="line">        <span class="keyword">if</span> (hdfs.isDirectory(mypath)) &#123;</span><br><span class="line">            hdfs.delete(mypath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">Boolean waitForComp = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">System.exit(waitForComp?<span class="number">0</span>:<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">ScoreBean</span>&gt;  </span>&#123;</span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">// 1.截取</span></span><br><span class="line">String[] datas = value.toString().trim().split(<span class="string">","</span>);</span><br><span class="line">String courseName = datas[<span class="number">0</span>].trim();</span><br><span class="line">String stuName = datas[<span class="number">1</span>].trim();</span><br><span class="line"><span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;datas.length; i++) &#123;</span><br><span class="line">sum += Integer.parseInt(datas[i]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">double</span> avgScore = sum/(datas.length-<span class="number">2</span>);</span><br><span class="line">ScoreBean sb = <span class="keyword">new</span> ScoreBean(courseName, stuName, avgScore);</span><br><span class="line">k.set(courseName);</span><br><span class="line">context.write(k, sb);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">ScoreBean</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;ScoreBean&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Reducer&lt;Text, ScoreBean, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">Set&lt;String&gt; stuNames = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line"><span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (ScoreBean sb : values) &#123;</span><br><span class="line">stuNames.add(sb.getStuName());</span><br><span class="line">count ++;</span><br><span class="line">sum += sb.getScore();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">int</span> size = stuNames.size();</span><br><span class="line">String val = size + <span class="string">"\t"</span> + (<span class="keyword">double</span>)sum/count;</span><br><span class="line">v.set(val);</span><br><span class="line">context.write(key, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行结果 </span></span><br><span class="line">algorithm<span class="number">6</span><span class="number">71.33333333333333</span></span><br><span class="line">computer<span class="number">10</span><span class="number">69.6</span></span><br><span class="line">english<span class="number">8</span><span class="number">66.0</span></span><br><span class="line">math<span class="number">7</span><span class="number">72.57142857142857</span></span><br></pre></td></tr></table></figure><h4 id="第2小题"><a href="#第2小题" class="headerlink" title="第2小题"></a>第2小题</h4><p><strong>统计每门课程参考学生的平均分，并且按课程存入不同的结果文件，要求一门课程一个结果文件</strong></p><p><strong>涉及知识点： 分区, 字符串组合key， Partitioner</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._02_分组组件;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.DoubleWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 注意: 此题因为数据中有2条 course 和 stuName相同的数据(english liuyifei), 所以必须再在reduce中继续去重一下, 再计算一下平均分</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 否则, 可以不用写reduce, 因为Mapper中已经把逻辑处理完了,可以直接输出</span></span><br><span class="line"><span class="comment"> </span></span><br><span class="line"><span class="comment"> * 最终输出: </span></span><br><span class="line"><span class="comment"> * computer liuyifei 43</span></span><br><span class="line"><span class="comment"> * computer huanglei 63</span></span><br><span class="line"><span class="comment"> * math liutao   64</span></span><br><span class="line"><span class="comment"> * ...</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ScorePlusDemo2</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定HDFS相关参数</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//  创建/配置 Job</span></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Jar包类型</span></span><br><span class="line">job.setJarByClass(ScorePlusDemo2.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Map Reduce执行类</span></span><br><span class="line">job.setMapperClass(MyMapper.class);</span><br><span class="line">job.setReducerClass(MyReducer.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Map输出类</span></span><br><span class="line">job.setMapOutputKeyClass(Text.class);</span><br><span class="line">job.setMapOutputValueClass(DoubleWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Reduce输出类</span></span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(DoubleWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//  设置分区 </span></span><br><span class="line">job.setPartitionerClass(MyPartition.class);</span><br><span class="line">job.setNumReduceTasks(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置输入 输出路径</span></span><br><span class="line">String inP = <span class="string">"/in/newScoreIn"</span>;</span><br><span class="line">String outP = <span class="string">"/out/scorePlus2"</span>;</span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(inP));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(outP));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置如果存在路径就删除</span></span><br><span class="line">        Path mypath = <span class="keyword">new</span> Path(outP);</span><br><span class="line">        FileSystem hdfs = mypath.getFileSystem(conf);</span><br><span class="line">        <span class="keyword">if</span> (hdfs.isDirectory(mypath)) &#123;</span><br><span class="line">            hdfs.delete(mypath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//  执行job</span></span><br><span class="line">        <span class="keyword">boolean</span> waitForCompletion = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(waitForCompletion?<span class="number">0</span>:-<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line">    </span><br><span class="line">===============================================================</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">DoubleWritable</span>&gt; </span>&#123;</span><br><span class="line"><span class="comment">// 把 课程+学生 作为 key</span></span><br><span class="line"></span><br><span class="line">Text k = <span class="keyword">new</span> Text();  <span class="comment">//只有输出String类型的, 才需要在这里设置Text</span></span><br><span class="line">DoubleWritable v = <span class="keyword">new</span> DoubleWritable();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">String[] datas = value.toString().trim().split(<span class="string">","</span>);</span><br><span class="line">String kStr = datas[<span class="number">0</span>].trim() + <span class="string">"\t"</span> + datas[<span class="number">1</span>].trim();</span><br><span class="line"><span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>; i &lt; datas.length; i++) &#123;</span><br><span class="line">sum += Integer.parseInt(datas[i]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">double</span> avg = sum / (datas.length - <span class="number">2</span>);</span><br><span class="line">k.set(kStr);</span><br><span class="line">v.set(avg);</span><br><span class="line">context.write(k, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">===============================================================</span><br><span class="line">    </span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 注意: 此题因为数据中有2条 course 和 stuName相同的数据, 所以必须再在reduce中</span></span><br><span class="line"><span class="comment"> * 继续去重一下, 再计算一下平均分</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 否则, 可以不用写reduce, 因为Mapper中已经把逻辑处理完了,可以直接输出</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">DoubleWritable</span>, <span class="title">Text</span>, <span class="title">DoubleWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">DoubleWritable v = <span class="keyword">new</span> DoubleWritable();</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;DoubleWritable&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Reducer&lt;Text, DoubleWritable, Text, DoubleWritable&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 考虑到有 课程, 学生名相同, 后面的数据不同的情况, 这里再做一个平均求和</span></span><br><span class="line"><span class="comment"> * 可以验证打印下</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">Double sum = <span class="number">0.0</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (DoubleWritable avg : values) &#123;</span><br><span class="line"><span class="keyword">if</span> (count &gt; <span class="number">0</span>) &#123;</span><br><span class="line"><span class="comment">// 有key完全相同的情况才会进到这里</span></span><br><span class="line">System.out.println(<span class="string">"这是第"</span> +count +<span class="string">"次, 说明课程和姓名有相同的两条数据\n课程姓名是: "</span>+key.toString());</span><br><span class="line">&#125;</span><br><span class="line">sum += avg.get();</span><br><span class="line">count ++;</span><br><span class="line">&#125;</span><br><span class="line">Double finAvg = sum/count;</span><br><span class="line">v.set(finAvg);</span><br><span class="line">context.write(key, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">===============================================================</span><br><span class="line">===============================================================</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 继承 Partitioner, 实现自定义分区</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyPartition</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">DoubleWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> HashMap&lt;String, Integer&gt; courseMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line">courseMap.put(<span class="string">"algorithm"</span>, <span class="number">0</span>);</span><br><span class="line">courseMap.put(<span class="string">"computer"</span>, <span class="number">1</span>);</span><br><span class="line">courseMap.put(<span class="string">"english"</span>, <span class="number">2</span>);</span><br><span class="line">courseMap.put(<span class="string">"math"</span>, <span class="number">3</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text key, DoubleWritable value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 取出Map输出的key中的前半部分--courseName</span></span><br><span class="line">Integer code = courseMap.get(key.toString().trim().split(<span class="string">"\t"</span>)[<span class="number">0</span>]);</span><br><span class="line"><span class="keyword">if</span> (code != <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="keyword">return</span> code;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="number">5</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">===============================================================</span><br><span class="line"> ===============================================================</span><br><span class="line">    </span><br><span class="line"> 执行结果 </span><br><span class="line">algorithmhuangdatou<span class="number">56.0</span></span><br><span class="line">algorithmhuangjiaju<span class="number">82.0</span></span><br><span class="line">algorithmhuanglei<span class="number">74.0</span></span><br><span class="line">algorithmhuangzitao<span class="number">72.0</span></span><br><span class="line">algorithmliutao<span class="number">82.0</span></span><br><span class="line">algorithmliuyifei<span class="number">62.0</span></span><br><span class="line">----------</span><br><span class="line">computerhuangbo<span class="number">65.0</span></span><br><span class="line">computerhuangdatou<span class="number">56.0</span></span><br><span class="line">computerhuangjiaju<span class="number">83.0</span></span><br><span class="line">computerhuanglei<span class="number">74.0</span></span><br><span class="line">computerhuangxiaoming<span class="number">72.0</span></span><br><span class="line">computerhuangzitao<span class="number">72.0</span></span><br><span class="line">computerliujialing<span class="number">64.0</span></span><br><span class="line">computerliutao<span class="number">83.0</span></span><br><span class="line">computerliuyifei<span class="number">62.0</span></span><br><span class="line">computerxuzheng<span class="number">65.0</span></span><br><span class="line">---------</span><br><span class="line">englishhuangbo<span class="number">55.0</span></span><br><span class="line">englishhuangdatou<span class="number">56.0</span></span><br><span class="line">englishhuanglei<span class="number">83.0</span></span><br><span class="line">englishhuangxiaoming<span class="number">72.0</span></span><br><span class="line">englishliujialing<span class="number">62.0</span></span><br><span class="line">englishliuyifei<span class="number">66.5</span></span><br><span class="line">englishzhaobenshan<span class="number">69.0</span></span><br><span class="line">englishzhouqi<span class="number">64.0</span></span><br><span class="line">------------</span><br><span class="line">mathhuangjiaju<span class="number">82.0</span></span><br><span class="line">mathhuanglei<span class="number">74.0</span></span><br><span class="line">mathhuangxiaoming<span class="number">83.0</span></span><br><span class="line">mathliujialing<span class="number">72.0</span></span><br><span class="line">mathliutao<span class="number">56.0</span></span><br><span class="line">mathwangbaoqiang<span class="number">72.0</span></span><br><span class="line">mathxuzheng<span class="number">69.0</span></span><br></pre></td></tr></table></figure><h4 id="第3小题"><a href="#第3小题" class="headerlink" title="第3小题"></a>第3小题</h4><p>求出 <strong><em>每门课程</em></strong><sup>①</sup>参与考试的学生成绩 <strong><em>最高平局分<sup>②</sup></em></strong>   的学生的信息：<u>课程，姓名和平均分</u></p><p><br></p><p><strong>解题思路：</strong> </p><ul><li>通过题意得出<strong>2个结论</strong><ul><li><strong>课程要分组</strong></li><li><strong>平均分要排序</strong></li></ul></li><li>排序的话，交给<strong>key</strong>来做无疑是最好的，因为<strong>MapReduce</strong>会<strong>自动</strong>对<strong>key</strong>进行<strong>分组&amp;排序</strong></li><li>因此可以把 <code>课程&amp;平均分</code> 作为一个<strong>联合key</strong></li><li>为了操作方便，可以<strong>封装到一个对象</strong>中去： <strong>ScoreBean</strong></li><li><strong>分组和排序</strong>需要在 <strong>ScoreBean</strong>重写的<strong><code>compareTo()</code>方法中完成</strong></li><li>因为最后结果是求<strong>每门课程</strong>的<strong>最高平均分</strong>，因此需要对课程进行分组。</li><li>此时原本的默认分组（以Bean对象整体分组）就不管用了，需要<strong>自定义分组</strong></li><li>自定义分组要<strong>继承<code>WritableComparator</code></strong>，重写<code>compare()</code>方法，指定分组的规则。</li><li><strong>ScoreBean</strong>先按照组别进行排序，到<strong>reduce</strong>中时，已经是按照组，排好的数据，<strong>MyGroup</strong> 会把相同的比较结果放到同一个组中，分发到<strong>reduce</strong>.</li><li><strong>reduce</strong>中，只需要取出每组的第一个元素输出到上下文即可</li></ul><p><br></p><p><strong>图示</strong></p><div id="sequence-0"></div><p><br></p><p><strong>涉及知识点： mr中key的作用，自定义对象的用法，自定义分组，mr的执行流程</strong></p><ul><li>利用“班级和平均分”作为 key，可以将 map 阶段读取到的所有学生成绩数据按照班级 和成绩排倒序，发送到 reduce</li><li>在 reduce 端利用 GroupingComparator 将班级相同的 kv 聚合成组，然后取第一个即是最 大值</li></ul><p><br></p><h5 id="先贴个结论："><a href="#先贴个结论：" class="headerlink" title="先贴个结论："></a><strong>先贴个结论：</strong></h5><p><strong>执行流程结论</strong></p><ul><li>map每读一行就 write 到 context 一次，按照指定的<code>key</code>进行分发</li><li><p>map 把所有的数据都读完了之后，大概执行到<code>67%</code>的时候，开始进入 <code>CustomBean</code>，执行<code>CustomBean</code>的<code>compareTo()</code>方法，会按照自己写的规则一条一条数据比较</p></li><li><p>上述都比较完毕之后，map阶段就结束了，此时来到了 reduce阶段，但是是到了<code>67%</code>了</p></li><li>到了reduce阶段，直接进入了<strong>MyGroup</strong>中自定义的<strong>compare</strong>方法。</li><li>MyGroup的<code>compare()</code>方法，如果返回<strong>非0</strong>， 就会进入 reduce 方法<strong>写出到context</strong></li></ul><p><strong>MyGroup进入Reduce的条件是</strong></p><ul><li>MyReduce中，如果compare的结果不等于0，也就是比较的2者不相同， 此时就进入Reduce， 写出到上下文</li><li>如果相同，会一直往下读，直到读到不同的， 此时写出读到上下文。</li><li>因为MyGroup会在Reduce阶段执行，而<code>CustomBean</code>中的<code>compareTo()</code>是在map阶段执行，所以需要在<code>CustomBean</code>中就把组排好序，此时<strong>分组功能</strong>才能<strong>正常运作</strong></li></ul><p><strong>指定分组类MyGroup和不指定的区别</strong></p><p><u>指定与不指定是指：在Driver类中，是否加上<code>job.setGroupingComparatorClass(MyGrouper.class);</code>这一句。</u></p><ul><li><strong>指定分组类</strong>：<ul><li>会按照分组类中，自定义的<code>compare()</code>方法比较，相同的为一组，分完一组就进入一次reduce方法</li></ul></li><li><strong>不指定分组类：（目前存疑）</strong><ul><li>是否是按照key进行分组</li><li>如果是自定义类为key，是否是按照此key中值相同的分为一组</li><li>如果是hadoop内置类，是否是按照此类的值分组（Text-String的值，IntWritable-int值等..）</li><li><strong>依然是走得以上这套分组逻辑，一组的数据读完才进入到Reduce阶段做归并</strong></li></ul></li></ul><p><br></p><h5 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ScoreBean2 </span></span><br><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._02_分组组件;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.Getter;</span><br><span class="line"><span class="keyword">import</span> lombok.Setter;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Getter</span></span><br><span class="line"><span class="meta">@Setter</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ScoreBean2</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">ScoreBean2</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> String courseName;</span><br><span class="line"><span class="keyword">private</span> Double score;</span><br><span class="line">    </span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">out.writeUTF(courseName);</span><br><span class="line">out.writeDouble(score);</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.courseName = in.readUTF();</span><br><span class="line"><span class="keyword">this</span>.score = in.readDouble();</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 如果是相同课程, 按照分数降序排列的</span></span><br><span class="line"><span class="comment"> * 如果是不同课程, 按照课程名称升序排列</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(ScoreBean2 o)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 测试一下只写按分数降序排序</span></span><br><span class="line"><span class="comment">//return o.getScore().compareTo(this.getScore());</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 首先分组(只在相同的组内进行比较)</span></span><br><span class="line"><span class="keyword">int</span> nameRes = <span class="keyword">this</span>.getCourseName().compareTo(o.getCourseName());</span><br><span class="line"><span class="keyword">if</span> (nameRes == <span class="number">0</span>) &#123;</span><br><span class="line"><span class="comment">// 课程相同的时候才进行降序排序</span></span><br><span class="line"><span class="keyword">int</span> scoreRes = o.getScore().compareTo(<span class="keyword">this</span>.getScore());</span><br><span class="line"><span class="keyword">return</span> scoreRes;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> nameRes;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 实际上ScoreBean中是包含所有的参数的, 这里的输出可以自己设置</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> courseName + <span class="string">"\t"</span> + score;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ScoreBean2</span><span class="params">(String courseName, Double score)</span> </span>&#123;</span><br><span class="line"><span class="keyword">super</span>();</span><br><span class="line"><span class="keyword">this</span>.courseName = courseName;</span><br><span class="line"><span class="keyword">this</span>.score = score;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ScoreBean2</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">super</span>();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// ScorePlusDemo3 </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._02_分组组件;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparator;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ScorePlusDemo3</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line"> main     </span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line">job.setJarByClass(ScorePlusDemo3.class);</span><br><span class="line"></span><br><span class="line">job.setMapperClass(MyMapper.class);</span><br><span class="line">job.setReducerClass(MyReducer.class);</span><br><span class="line"></span><br><span class="line">job.setMapOutputKeyClass(ScoreBean2.class);</span><br><span class="line">job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">job.setGroupingComparatorClass(MyGrouper.class);</span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">String outP = <span class="string">"/out/scorePlus3"</span>;</span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"/in/newScoreIn"</span>));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(outP));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果输出目录存在,就先删除</span></span><br><span class="line">Path myPath = <span class="keyword">new</span> Path(outP);</span><br><span class="line">FileSystem fs = myPath.getFileSystem(conf);</span><br><span class="line"><span class="keyword">if</span> (fs.isDirectory(myPath)) &#123;</span><br><span class="line">fs.delete(myPath, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">boolean</span> waitForCompletion = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">System.exit(waitForCompletion ? <span class="number">0</span> : -<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"> Mapper </span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 输出: key: course</span></span><br><span class="line"><span class="comment"> *     value: score ...</span></span><br><span class="line"><span class="comment"> * 思路:</span></span><br><span class="line"><span class="comment"> * 1.不同课程要分开展示, 以 课程+分数 作为key, 在mapper中完成排序 </span></span><br><span class="line"><span class="comment"> * 2.在reduce中按照 MyGrouper 完成分组</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span></span></span><br><span class="line"><span class="class"><span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">ScoreBean2</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> String[] datas;</span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">datas = value.toString().trim().split(<span class="string">","</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>; i &lt; datas.length; i++) &#123;</span><br><span class="line">sum += Integer.parseInt(datas[i]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">double</span> avg = (<span class="keyword">double</span>) sum / (datas.length - <span class="number">2</span>);</span><br><span class="line">ScoreBean2 sb = <span class="keyword">new</span> ScoreBean2(datas[<span class="number">0</span>].trim(), avg);</span><br><span class="line"></span><br><span class="line">v.set(datas[<span class="number">1</span>].trim());</span><br><span class="line">context.write(sb, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"> Redecer     </span><br><span class="line"><span class="keyword">static</span> <span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span></span></span><br><span class="line"><span class="class"><span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">ScoreBean2</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line"><span class="keyword">int</span> count = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(ScoreBean2 key, Iterable&lt;Text&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 如果没有其它问题</span></span><br><span class="line"><span class="comment"> * 此时是按照课程分好组了, 同一个课程的所有学生都过来了, 并且学生成绩是排好的,</span></span><br><span class="line"><span class="comment"> * 如果此时求最大值, 只需要取出第一个即可 </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">// 进来一次只取第一个</span></span><br><span class="line">Text name = values.iterator().next();</span><br><span class="line">k.set(key.getCourseName() + <span class="string">"\t"</span> + name.toString() + <span class="string">"\t"</span></span><br><span class="line">+ key.getScore());</span><br><span class="line">context.write(k, NullWritable.get());</span><br><span class="line">context.write(<span class="keyword">new</span> Text(<span class="string">"==================第"</span>+count+<span class="string">"次进入reduce"</span>), NullWritable.get());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/*context.write(new Text("==================第"+count+"次进入reduce"), NullWritable.get());</span></span><br><span class="line"><span class="comment">for (Text name : values) &#123;</span></span><br><span class="line"><span class="comment">k.set(key.getCourseName() + "\t" + name.toString() + "\t"</span></span><br><span class="line"><span class="comment">+ key.getScore());</span></span><br><span class="line"><span class="comment">context.write(k, NullWritable.get());</span></span><br><span class="line"><span class="comment">context.write(new Text("---------in for write------"), NullWritable.get());</span></span><br><span class="line"><span class="comment">&#125;*/</span></span><br><span class="line">count++;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"> MyGrouper </span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 自定义分组  需要继承一个类WritableComparator</span></span><br><span class="line"><span class="comment"> * 重写compare方法</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyGrouper</span> <span class="keyword">extends</span> <span class="title">WritableComparator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// WritableComparator 此方法的默认无参构造是不会创建对象的, 需要自己重写</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">MyGrouper</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="comment">// 中间省去的参数是 Configuration, 如果为空, 会创建一个新的</span></span><br><span class="line"><span class="keyword">super</span>(ScoreBean2.class, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 此处比较的是2个  WritableComparable 对象, 需要强转一下具体的类对象</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@SuppressWarnings</span>(<span class="string">"rawtypes"</span>)</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> </span>&#123;</span><br><span class="line">ScoreBean2 aBean = (ScoreBean2) a;</span><br><span class="line">ScoreBean2 bBean = (ScoreBean2) b;</span><br><span class="line"><span class="comment">// 返回分组规则</span></span><br><span class="line">System.out.println(aBean.getCourseName()+<span class="string">"---MyGroup中比较---"</span>+(bBean.getCourseName()));</span><br><span class="line"><span class="keyword">return</span> aBean.getCourseName().compareTo(bBean.getCourseName());</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">================================================================================</span><br><span class="line"> 执行结果 </span><br><span class="line">================================================================================</span><br><span class="line"></span><br><span class="line">algorithmhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">==================第<span class="number">1</span>次进入reduce</span><br><span class="line">computerhuangjiaju<span class="number">83.2</span></span><br><span class="line">==================第<span class="number">2</span>次进入reduce</span><br><span class="line">englishhuanglei<span class="number">83.0</span></span><br><span class="line">==================第<span class="number">3</span>次进入reduce</span><br><span class="line">mathhuangxiaoming<span class="number">83.0</span></span><br><span class="line">==================第<span class="number">4</span>次进入reduce</span><br></pre></td></tr></table></figure><h2 id="MR实现两个表的数据关联Join"><a href="#MR实现两个表的数据关联Join" class="headerlink" title="MR实现两个表的数据关联Join"></a>MR实现两个表的数据关联<code>Join</code></h2><h3 id="题目-2"><a href="#题目-2" class="headerlink" title="题目"></a>题目</h3><blockquote><p>订单数据表t_order：  flag=0<br>id    date    pid    amount<br>1001    20150710    P0001    2<br>1002    20150710    P0001    3<br>1003    20150710    P0002    3<br>Id:数据记录id<br>Date   日期<br>Pid     商品id<br>Amount    库存数量</p><p>6.商品信息表t_product   flag=1<br>pid    name    category_id    price<br>P0001    小米5    C01    2000<br>P0002    锤子T1    C01    3500</p><p>mr实现两个表的数据关联<br>id   pid    date    amount    name    category_id     price</p></blockquote><p><br></p><h3 id="答案1-Reducer-端-实现-Join"><a href="#答案1-Reducer-端-实现-Join" class="headerlink" title="答案1 : Reducer 端 实现 Join"></a>答案1 : Reducer 端 实现 <code>Join</code></h3><h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><ul><li><p>map端</p><ul><li><p>读取到当前路径下，所有文件的切片信息， 根据文件名判断是那张表</p><ul><li><p>在setup中，从文件切片中获取到文件名</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取读取到的切片相关信息,一个切片对应一个 maptask</span></span><br><span class="line">InputSplit inputSplit = context.getInputSplit();</span><br><span class="line"><span class="comment">// 转换为文件切片</span></span><br><span class="line">FileSplit fs = (FileSplit)inputSplit;</span><br><span class="line"><span class="comment">// 获取文件名</span></span><br><span class="line">filename = fs.getPath().getName();</span><br></pre></td></tr></table></figure></li><li><p>这里总共会获得2个文件名（指定目录存了2个指定文件），一个文件名对应一个切片</p></li></ul></li><li><p>关联字段作为key， 其它的作为value，在value前面加上当前文件的名称标记</p></li></ul></li><li><p>reduce端</p><ul><li>通过标记区分两张表，把读取到的信息，分别存入2个list中</li><li>遍历大的表，与小表进行拼接（小表的相同pid记录只会有一条）</li><li>拼接完成后即可写出</li></ul></li></ul><h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._03_join2表的数据关联;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReduceJoinDemo</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 指定HDFS相关参数</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//  创建/配置 Job</span></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Jar包类型</span></span><br><span class="line">job.setJarByClass(ReduceJoinDemo.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Map Reduce执行类</span></span><br><span class="line">job.setMapperClass(MyMapper.class);</span><br><span class="line">job.setReducerClass(MyReducer.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Map输出类</span></span><br><span class="line">job.setMapOutputKeyClass(Text.class);</span><br><span class="line">job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Reduce输出类</span></span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置输入 输出路径</span></span><br><span class="line">String inP = <span class="string">"/in/joindemo"</span>;</span><br><span class="line">String outP = <span class="string">"/out/joinout1"</span>;</span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(inP));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(outP));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置如果存在路径就删除</span></span><br><span class="line">        Path mypath = <span class="keyword">new</span> Path(outP);</span><br><span class="line">        FileSystem hdfs = mypath.getFileSystem(conf);</span><br><span class="line">        <span class="keyword">if</span> (hdfs.isDirectory(mypath)) &#123;</span><br><span class="line">            hdfs.delete(mypath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//  执行job</span></span><br><span class="line">        <span class="keyword">boolean</span> waitForCompletion = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(waitForCompletion?<span class="number">0</span>:-<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 思路: 读取2个表中的数据,进行标记发送</span></span><br><span class="line"><span class="comment"> * key: 两表需要关联的字段</span></span><br><span class="line"><span class="comment"> * value: 其它值, 需要标记， 标记数据的来源</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * **核心： 关联条件**</span></span><br><span class="line"><span class="comment">- 想要在 reduce 端完成 join， 要在 reduce 端可以同时接收到两个表中的数据</span></span><br><span class="line"><span class="comment">- 要保证在 Map 端进行读文件的时候， 读到2个表的数据， 并且需要对2个表的数据进行区分</span></span><br><span class="line"><span class="comment">- 将2个表放在同一个目录下</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">解决: </span></span><br><span class="line"><span class="comment">mapper 开始执行时, 在setup方法中, 从上下文中取到文件名, 根据文件名打标记</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">String filename = <span class="string">""</span>;</span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">// 获取读取到的切片相关信息,一个切片对应一个 maptask</span></span><br><span class="line">InputSplit inputSplit = context.getInputSplit();</span><br><span class="line"><span class="comment">// 转换为文件切片</span></span><br><span class="line">FileSplit fs = (FileSplit)inputSplit;</span><br><span class="line"><span class="comment">// 获取文件名</span></span><br><span class="line">filename = fs.getPath().getName();</span><br><span class="line">System.out.println(<span class="string">"本次获取到的文件名为-----"</span>+filename);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value,</span></span></span><br><span class="line"><span class="function"><span class="params">Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">// 解析出来每一行内容, 打标记, 发送</span></span><br><span class="line">String[] infos = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (filename.equals(<span class="string">"order"</span>)) &#123;</span><br><span class="line">k.set(infos[<span class="number">2</span>]);</span><br><span class="line"><span class="comment">// 设置标记前缀为 OR</span></span><br><span class="line">v.set(<span class="string">"OR"</span>+infos[<span class="number">0</span>]+<span class="string">"\t"</span>+infos[<span class="number">1</span>]+<span class="string">"\t"</span>+infos[<span class="number">3</span>]);</span><br><span class="line">&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">k.set(infos[<span class="number">0</span>]);</span><br><span class="line"><span class="comment">// 设置标记前缀为 PR</span></span><br><span class="line">v.set(<span class="string">"PR"</span>+infos[<span class="number">1</span>]+<span class="string">"\t"</span>+infos[<span class="number">2</span>]+<span class="string">"\t"</span>+infos[<span class="number">3</span>]);</span><br><span class="line">&#125;</span><br><span class="line">context.write(k, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Reducer&lt;Text, Text, Text, NullWritable&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 首先明确 product 和 order 是 一对多的关系</span></span><br><span class="line"><span class="comment"> * 根据前缀不同,取到2个不同的表存进2个容器中</span></span><br><span class="line"><span class="comment"> * 遍历多的表, 与一进行拼接</span></span><br><span class="line"><span class="comment"> * 最后写出到上下文</span></span><br><span class="line"><span class="comment"> * 最终的输出格式 id   pid    date    amount    name    category_id     price</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">// 因为每次遍历到不同的pid, 都会走进来一次, list也会有新的输出,所以必须定义在里面,每次进来都要初始化</span></span><br><span class="line">List&lt;String&gt; productList =<span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">List&lt;String&gt; orderList =<span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (Text v : values) &#123;</span><br><span class="line">String vStr = v.toString();</span><br><span class="line"><span class="keyword">if</span> (vStr.startsWith(<span class="string">"OR"</span>)) &#123;</span><br><span class="line">orderList.add(vStr.substring(<span class="number">2</span>));</span><br><span class="line">&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">productList.add(vStr.substring(<span class="number">2</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 此时2个list添加完了本次 相同的 key(pid) 的所有商品</span></span><br><span class="line"><span class="comment">// 遍历多的进行拼接</span></span><br><span class="line"><span class="keyword">for</span> (String or : orderList) &#123;</span><br><span class="line"><span class="comment">// 相同的 pid的 product 只有一个, productList中的数量是1</span></span><br><span class="line"><span class="comment">// 但是相同pid 的 订单 可能有多个</span></span><br><span class="line">String res =  key.toString() + <span class="string">"\t"</span> + or + productList.get(<span class="number">0</span>);</span><br><span class="line">k.set(res);</span><br><span class="line">context.write(k, NullWritable.get());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><br></p><h3 id="※-答案2-：-Mapper-端实现-Join-※"><a href="#※-答案2-：-Mapper-端实现-Join-※" class="headerlink" title="※ 答案2 ： Mapper 端实现 Join  ※"></a>※ 答案2 ： Mapper 端实现 <code>Join</code>  ※</h3><h4 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h4><ul><li>创建job的时候,把小表加入缓存 在map的setup中, </li><li>读取缓存中的数据, 存入一个成员变量 map中<ul><li>map方法中,只需要读一个表, 然后根据关联条件(关联key: pid)消除笛卡尔集,进行拼接</li><li>map直接输出, 甚至都不需要reduce</li></ul></li></ul><h4 id="注意点"><a href="#注意点" class="headerlink" title="注意点:"></a>注意点:</h4><ul><li><p>需要达成jar包运行, 直接用Eclipse会找不到缓存</p></li><li><p><strong>jar包执行方法</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果代码内部指定了输入输出路径，后面的/in，/out参数可以不加</span></span><br><span class="line">hadoop jar xxxx.jar com.rox.xxx.xxxx(主方法)  /<span class="keyword">in</span>/xx   /out/xx</span><br></pre></td></tr></table></figure></li><li><p>如果没有Reduce方法</p><ul><li><p>main方法中，设置map的写出key，value,应该用 <code>setOutputKeyClass</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//// 设置Map输出类 (因为这里没有Reduce, 所以这里是最终输出,一定要注意!!!)//////////</span></span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(NullWritable.class);</span><br></pre></td></tr></table></figure></li><li><p>要设置reduce task 的个数为0 </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setNumReduceTasks(<span class="number">0</span>);</span><br></pre></td></tr></table></figure></li><li><p>把小文件加载到缓存中的方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">////////////// 将小文件加载到缓存  </span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"/in/joindemo/product"</span>));</span><br></pre></td></tr></table></figure></li></ul></li></ul><p>  ​    </p><h4 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._03_join;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.FileReader;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MapJoinDemo</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 指定HDFS相关参数</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//  创建/配置 Job</span></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Jar包类型:这里千万别写错了</span></span><br><span class="line">job.setJarByClass(MapJoinDemo.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Map Reduce执行类</span></span><br><span class="line">job.setMapperClass(MyMapper.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">///////////// 设置Map输出类 (因为这里没有Reduce, 所以这里是最终输出,一定要注意!!!)//////////</span></span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">////////////// 设置reduce执行个数为0</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">////////////// 将小文件加载到缓存  </span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"/in/joindemo/product"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置输入 输出路径</span></span><br><span class="line">String inP = <span class="string">"/in/joindemo/order"</span>;</span><br><span class="line">String outP = <span class="string">"/out/joinout2"</span>;</span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(inP));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(outP));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置如果存在路径就删除</span></span><br><span class="line">        Path mypath = <span class="keyword">new</span> Path(outP);</span><br><span class="line">        FileSystem hdfs = mypath.getFileSystem(conf);</span><br><span class="line">        <span class="keyword">if</span> (hdfs.isDirectory(mypath)) &#123;</span><br><span class="line">            hdfs.delete(mypath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//  执行job</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="keyword">true</span>)?<span class="number">0</span>:-<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 思路: </span></span><br><span class="line"><span class="comment"> * 创建job的时候,把小表加入缓存</span></span><br><span class="line"><span class="comment"> * 在map的setup中, 读取缓存中的数据, 存入一个成员变量 map中</span></span><br><span class="line"><span class="comment"> * map方法中,只需要读一个表, 然后根据关联条件(关联key: pid)消除笛卡尔集,进行拼接</span></span><br><span class="line"><span class="comment"> * 直接输出, 甚至都不需要reduce</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 注意点: </span></span><br><span class="line"><span class="comment"> * 需要达成jar包运行, 直接用Eclipse会找不到缓存</span></span><br><span class="line"><span class="comment"> * 格式: hadoop jar包本地路径 jar包主方法全限定名 hadoop输入  hadoop输出</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建装载小表的map, key存储 关联键, value存其它</span></span><br><span class="line">Map&lt;String, String&gt; proMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">// 获取缓存中存储的小表 (一般是 一对多中的 一), 因为只存了1个,所以直接取第0个</span></span><br><span class="line">Path path = context.getLocalCacheFiles()[<span class="number">0</span>];</span><br><span class="line">String pString = path.toString();</span><br><span class="line"><span class="comment">// 开启in流, BufferedReader 逐行读取文件</span></span><br><span class="line">BufferedReader br = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> FileReader(pString));</span><br><span class="line">String line = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">while</span> ((line = br.readLine()) != <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="comment">// 成功读取一行</span></span><br><span class="line">String[] infos = line.split(<span class="string">"\t"</span>);</span><br><span class="line"><span class="comment">// 存进proMap</span></span><br><span class="line">proMap.put(infos[<span class="number">0</span>],</span><br><span class="line">infos[<span class="number">1</span>] + <span class="string">"\t"</span> + infos[<span class="number">2</span>] + <span class="string">"\t"</span> + infos[<span class="number">3</span>]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//br.close();</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 直接从路径读取大文件</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">String[] infos = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">String pid = infos[<span class="number">2</span>];</span><br><span class="line"><span class="comment">//进行关联   pid到map中匹配   如果包含  证明匹配上了</span></span><br><span class="line"><span class="comment">// 艹, 这里pid之前加了 "", 妈的,当然找不到啦!!!</span></span><br><span class="line"><span class="keyword">if</span> (proMap.containsKey(pid)) &#123;</span><br><span class="line">String res = value.toString() + <span class="string">"\t"</span> + proMap.get(pid);</span><br><span class="line">k.set(res);</span><br><span class="line">context.write(k, NullWritable.get());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><script src="https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js"></script><script src="https://cdn.bootcss.com/snap.svg/0.5.1/snap.svg-min.js"></script><script src="https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js"></script><script src="https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js"></script><script src="https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js"></script><textarea id="sequence-0-code" style="display: none">title: 执行流程时序图 Mapper(map)->ScoreBean: k:ScoreBean(courseName,avgScore)Mapper(map)->ScoreBean: v:Text(stuName)Mapper(ScoreBean)->Reducer(MyGroup): course按字典升序Mapper(ScoreBean)->Reducer(MyGroup): course内成绩降序Reducer(MyGroup)->Reducer(reduce): 根据自定义的分组规则按组输出Reducer(MyGroup)->Reducer(reduce): 一组只调用reduce一次</textarea><textarea id="sequence-0-options" style="display: none">{"theme":"simple"}</textarea><script>  var code = document.getElementById("sequence-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value));  var diagram = Diagram.parse(code);  diagram.drawSVG("sequence-0", options);</script></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;求微博共同粉丝&quot;&gt;&lt;a href=&quot;#求微博共同粉丝&quot; class=&quot;headerlink&quot; title=&quot;求微博共同粉丝&quot;&gt;&lt;/a&gt;求微博共同粉丝&lt;/h2&gt;&lt;h3 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/tags/MapReduce/"/>
    
  </entry>
  
</feed>
