<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>A.P的文艺杂谈</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://airpoet.github.io/"/>
  <updated>2018-06-14T15:21:20.295Z</updated>
  <id>https://airpoet.github.io/</id>
  
  <author>
    <name>airpoet</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hive学习—2</title>
    <link href="https://airpoet.github.io/2018/06/14/Hadoop/Study/3-Hive/Hive%E5%AD%A6%E4%B9%A0-2/"/>
    <id>https://airpoet.github.io/2018/06/14/Hadoop/Study/3-Hive/Hive学习-2/</id>
    <published>2018-06-14T01:04:30.334Z</published>
    <updated>2018-06-14T15:21:20.295Z</updated>
    
    <content type="html"><![CDATA[<h1 id="hive数据类型"><a href="#hive数据类型" class="headerlink" title="hive数据类型"></a>hive数据类型</h1><h3 id="1-原子数据类型"><a href="#1-原子数据类型" class="headerlink" title="1) 原子数据类型"></a>1) 原子数据类型</h3><ul><li>TinyInt：1byte有符号整数</li><li>SmallInt：2byte有符号整数</li><li>Int：4byte有符号整数</li><li>BigInt：8byte有符号整数</li><li>Float：单精度浮点数</li><li>Double：双精度浮点数</li><li>Boolean：布尔类型</li><li>String：字符串</li><li>TimeStamp：整数</li></ul><h3 id="2-复杂数据类型"><a href="#2-复杂数据类型" class="headerlink" title="2) 复杂数据类型"></a>2) 复杂数据类型</h3><ul><li><p>Array\&lt;Type></p><ul><li>由一系列相同数据类型的元素组成</li><li>这些元素可以通过 下标 来访问</li><li>查询时如果查到返回响应值，没查到则返回null<br>建表：<br>create table person(name string,work_locations array\&lt;string>)<br>row format delimited fields terminated by ‘\t’<br>collection items terminated by ‘,’;<br>导入数据：<br>load data local inpath ‘/home/sigeon/person.txt’ into table person;<br>查询<br>Select work_locations[0] from person;</li></ul></li><li><p>Map&lt;KType, VType&gt;</p><ul><li>包含 key-value 键值对</li><li>可以通过 key 来访问元素<br>建表语句：<br>create table score(name string, scores map)<br>row format delimited fields terminated by ‘\t’<br>collection items terminated by ‘,’<br>map keys terminated by ‘:’;<br>导入数据：<br>load data local inpath ‘/home/sigeon/score.txt’ into table score;<br>查询语句：<br>Select scores[‘Chinese’] from score;</li></ul></li><li><p>Struct&lt;Param1:Type1, Param1:Type1, … &gt;</p><ul><li>可以包含不同数据类型的元素<br>类似于c语言中的结构体</li><li>这些元素可以通过 点语法 的方式来得到<br>建表语句：<br>create table course(id int,course struct&lt;name:string, score:int&gt;)<br>row format delimited fields terminated by ‘\t’<br>collection items terminated by ‘,’;<br>导入数据：<br>load data local inpath ‘/ home/sigoen/course.txt’ into table course;<br>查询语句：<br>Select c.course.score from course c;</li></ul></li><li><p>几个分隔符</p></li></ul><p>  指定分隔符要按从外向内的顺序，字段 -&gt; 集合元素 -&gt;map k-v</p><ul><li>ROW FORMAT：指定分隔符的关键字 </li><li>DELIMITED FIELDS TERMINATED BY：字段分隔符 </li><li>COLLECTION ITEMS TERMINATED BY：集合元素分隔符（Array 中的各元素、Struct 中的各元素、 Map 中的 key-value 对之间） </li><li>MAP KEYS TERMINATED BY：Map 中 key 与 value 的分隔符 </li><li>LINES TERMINATED BY：行之间的分隔符</li></ul><h1 id="hive视图"><a href="#hive视图" class="headerlink" title="hive视图"></a>hive视图</h1><ul><li><p>和关系型数据库一样，Hive也提供了视图的功能</p></li><li><p>Hive 的视图和关系型数据库的视图有很大的区别： </p><ul><li>1、只有逻辑视图，没有物化视图； </li><li>2、视图只能查询，不能增删改 (Load|Insert/Update/Delete) 数据； </li><li>3、视图在创建时候，只是保存了一份元数据 (存在TBLS中)，当查询视图的时候，才开始执行视图对应的那些子查询<br>视图元数据只存储了hql语句，而不是执行结果</li></ul></li><li><p>视图操作</p><ul><li><p>创建视图</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">view</span> my_view <span class="keyword">as</span> &lt;<span class="keyword">select</span> * <span class="keyword">from</span> mytable&gt; [<span class="keyword">limit</span> <span class="number">500</span>];</span><br></pre></td></tr></table></figure></li><li><p>查看视图</p><ul><li>show tables;   // 显示所有表和视图</li><li>show views;    //显示所有视图</li><li>desc [formatted] view_name;   // 查看某个具体视图的(详细)信息<br>视图类型：VIRTUAL_VIEW</li></ul></li><li><p>删除视图</p><ul><li>drop view [if exists] view_name</li></ul></li><li><p>使用视图 </p><ul><li>select count(distinct uid) from my_view;</li></ul></li></ul></li></ul><h1 id="hive函数"><a href="#hive函数" class="headerlink" title="hive函数"></a>hive函数</h1><ul><li><p>函数分类</p><ul><li>UDF（自定义函数 User-Defined Function）作用于单个数据行，产生一个数据行作为输出。（数学函数，字 符串函数） </li><li>UDAF（用户定义聚集函数 User-Defined Aggregation Funcation）：接收多个输入数据行，并产 生一个输出数据行。（count，max） </li><li>UDTF（表格生成函数 User-Defined Table Function）：接收一行输入，输出多行（explode）</li></ul></li><li><p>内置函数</p><ul><li><p>查看函数命令</p><ul><li>查看内置函数： show functions; </li><li>显示函数的详细信息： desc function [extended] fun_name;<br>extended：显示扩展信息</li></ul></li><li><p>分类</p><ul><li><p>关系运算</p><ul><li>分类<br>\1. 等值比较: =<br>\2. 等值比较:&lt;=&gt;<br>\3. 不等值比较: &lt;&gt;和!=<br>\4. 小于比较: &lt;<br>\5. 小于等于比较: &lt;=<br>\6. 大于比较: &gt;<br>\7. 大于等于比较: &gt;=<br>\8. 区间比较<br>\9. 空值判断: IS NULL<br>\10. 非空判断: IS NOT NULL<br>\10. LIKE 比较: LIKE<br>\11. JAVA 的 LIKE 操作: RLIKE<br>\12. REGEXP 操作: REGEXP</li></ul></li><li><p>数学运算</p><ul><li>分类<br>\1. 加法操作: +<br>\2. 减法操作: –<br>\3. 乘法操作: *<br>\4. 除法操作: /<br>\5. 取余操作: %<br>\6. 位与操作: &amp;<br>\7. 位或操作: |<br>\8. 位异或操作: ^<br>9．位取反操作: ~</li></ul></li><li><p>逻辑运算</p><ul><li>分类<br>\1. 逻辑与操作: AND 、&amp;&amp;<br>\2. 逻辑或操作: OR 、||<br>\3. 逻辑非操作: NOT、!</li></ul></li><li><p>复合类型构造函数 </p><ul><li>分类<ol><li>array 结构</li><li>map 结构<br>\3. struct 结构<br>\4. named_struct 结构<br>\5. create_union </li></ol></li></ul></li><li><p>复合类型操作符</p><ul><li>\1. 获取 array 中的元素 </li><li>\2. 获取 map 中的元素 </li><li>\3. 获取 struct 中的元素</li></ul></li><li><p>集合操作函数 </p><ul><li><p>\1. map 类型大小：size </p></li><li><p>\2. array 类型大小：size </p></li><li><p>\3. 判断元素数组是否包含元素：array_contains </p></li><li><p>\4. 获取 map 中所有 value 集合 </p></li><li><p>\5. 获取 map 中所有 key 集合 </p></li><li><p>\6. 数组排序</p></li><li><p>\7. 获取素组或map集合的单个元素（k-v对）：<strong>explode()</strong></p><p><strong>当同时查询炸裂字段和普通字段时，需要使用横向虚拟视图：lateral view</strong></p><ul><li>如：select name,addr.city from usr_addr lateral view(address) addr as city;<br>这里一般需要给 查询结果 和 结果字段 起别名，不然没办法获得每个字段，如果只是查询所有的话就不需要了</li></ul></li></ul></li><li><p>类型转换函数</p><ul><li>\1. 二进制转换：binary </li><li>\2. 基础类型之间强制转换：cast </li></ul></li><li><p>数值计算函数</p><ul><li>\1. 取整函数: round </li><li>\2. 指定精度取整函数: round </li><li>\3. 向下取整函数: floor </li><li>\4. 向上取整函数: ceil </li><li>\5. 向上取整函数: ceiling </li><li>\6. 取随机数函数: rand </li><li>\7. 自然指数函数: exp </li><li>\8. 以 10 为底对数函数: log10 </li><li>\9. 以 2 为底对数函数: log2 </li><li>\10. 对数函数: log </li><li>\11. 幂运算函数: pow </li><li>\12. 幂运算函数: power </li><li>\13. 开平方函数: sqrt </li><li>\14. 二进制函数: bin </li><li>\15. 十六进制函数: hex </li><li>\16. 反转十六进制函数: unhex </li><li>\17. 进制转换函数: conv </li><li>\18. 绝对值函数: abs </li><li>\19. 正取余函数: pmod </li><li>\20. 正弦函数: sin </li><li>\21. 反正弦函数: asin </li><li>\22. 余弦函数: cos </li><li>\23. 反余弦函数: acos </li><li>\24. positive 函数: positive </li><li>\25. negative 函数: negative </li></ul></li><li><p>字符串函数</p><ul><li>\1. 字符 ascii 码函数：ascii </li><li>\2. base64 字符串 </li><li>\3. 字符串连接函数：concat </li><li>\4. 带分隔符字符串连接函数：concat_ws </li><li>\5. 数组转换成字符串的函数：concat_ws </li><li>\6. 小数位格式化成字符串函数：format_number </li><li>\7. 字符串截取函数：substr, substring<br>序号从1开始，可以传负数，代表从右开始</li><li>\9. 字符串查找函数：instr<br>找到返回一个正整数，未找到返回0</li><li>\10. 字符串长度函数：length </li><li>\11. 字符串查找函数：locate </li><li>\12. 字符串格式化函数：printf </li><li>\13. 字符串转换成 map 函数：str_to_map </li><li>\14. base64 解码函数：unbase64(string str) </li><li>\15. 字符串转大写函数：upper,ucase </li><li>\16. 字符串转小写函数：lower,lcase </li><li>\17. 去空格函数：trim </li><li>\18. 左边去空格函数：ltrim </li><li>\19. 右边去空格函数：rtrim </li><li>\20. 正则表达式替换函数：regexp_replace </li><li>\21. 正则表达式解析函数：regexp_extract </li><li>\22. URL 解析函数：parse_url </li><li>\23. json 解析函数：get_json_object </li><li>\24. 空格字符串函数：space </li><li>\25. 重复字符串函数：repeat </li><li>\26. 左补足函数：lpad</li><li>\27. 右补足函数：rpad </li><li>\28. 分割字符串函数: split </li><li>\29. 集合查找函数: find_in_set </li><li>\30. 分词函数：sentences </li><li>\31. 分词后统计一起出现频次最高的 TOP-K </li><li>\32. 分词后统计与指定单词一起出现频次最高的 TOP-K </li></ul></li><li><p>日期函数</p><ul><li>\1. UNIX 时间戳转日期函数: from_unixtime </li><li>\2. 获取当前 UNIX 时间戳函数: unix_timestamp </li><li>\3. 日期转 UNIX 时间戳函数: unix_timestamp </li><li>\4. 指定格式日期转 UNIX 时间戳函数: unix_timestamp </li><li>\5. 日期时间转日期函数: to_date </li><li>\6. 日期转年函数: year </li><li>\7. 日期转月函数: month </li><li>\8. 日期转天函数: day </li><li>\9. 日期转小时函数: hour </li><li>\10. 日期转分钟函数: minute </li><li>\11. 日期转秒函数: second </li><li>\12. 日期转周函数: weekofyear </li><li>\13. 日期比较函数: datediff </li><li>\14. 日期增加函数: date_add </li><li>\15. 日期减少函数: date_sub </li></ul></li><li><p>条件函数</p><ul><li>\1. If 函数: if( 条件 ，true返回参数，false返回参数 ) </li><li>\2. 当param1不为null返回param1，否则返回param2：nvl(param1, param2)</li><li>\2. 非空查找函数: coalesce</li><li>\3. 条件判断函数：case</li></ul></li><li><p>混合函数 </p><ul><li>分类<br>\1. 调用 Java 函数：java_method<br>\2. 调用 Java 函数：reflect<br>\3. 字符串的 hash 值：hash</li></ul></li><li><p>XPath 解析 XML 函数 </p><ul><li>分类<br>\1. xpath<br>\2. xpath_string<br>\3. xpath_boolean<br>\4. xpath_short, xpath_int, xpath_long<br>\5. xpath_float, xpath_double, xpath_number </li></ul></li><li><p>汇总统计函数（UDAF）</p><ul><li>\1. 个数统计函数: count </li><li>\2. 总和统计函数: sum </li><li>\3. 平均值统计函数: avg </li><li>\4. 最小值统计函数: min </li><li>\5. 最大值统计函数: max</li><li>\6. 非空集合总体变量函数: var_pop </li><li>\7. 非空集合样本变量函数: var_samp </li><li>\8. 总体标准偏离函数: stddev_pop </li><li>\9. 样本标准偏离函数: stddev_samp </li><li>10．中位数函数: percentile </li><li>\11. 中位数函数: percentile </li><li>\12. 近似中位数函数: percentile_approx </li><li>\13. 近似中位数函数: percentile_approx </li><li>\14. 直方图: histogram_numeric </li><li>\15. 集合去重数：collect_set </li><li>\16. 集合不去重函数：collect_list </li></ul></li><li><p>表格生成函数 Table-Generating Functions (UDTF) </p><ul><li>分类<br>1．数组拆分成多行：explode(array)<br>2．Map 拆分成多行：explode(map) </li></ul></li></ul></li></ul></li><li><p>自定义函数</p><ul><li>\1. 需要继承 org.apache.hadoop.hive.ql.exec.UDF 类，实现一个或多个 evaluate() 方法</li><li>\2. 将hive的jar包放在hive的classpath路径下，进入hive客户端，执行命令：add jar \<jar path="">;</jar></li><li>\3. 检查jar包是否添加成功，执行命令：list jars;</li><li>\4. 给自定义函数 添加别名，并在hive中 注册 该函数，执行命令：create temporary function myfunc as ‘主类全路径名’;<br>myfunc：自定义函数别名<br>该方法创建的是临时函数，当前客户端关闭就没有了，下次重复第3，4步；<br>真实 生产中就是使用该方法！</li><li>\5. 查看hive函数库有没有成功添加，执行命令：show functions;</li><li>\6. 调用函数时通过函数名和参数列表确定调用的是具体哪一个方法</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;hive数据类型&quot;&gt;&lt;a href=&quot;#hive数据类型&quot; class=&quot;headerlink&quot; title=&quot;hive数据类型&quot;&gt;&lt;/a&gt;hive数据类型&lt;/h1&gt;&lt;h3 id=&quot;1-原子数据类型&quot;&gt;&lt;a href=&quot;#1-原子数据类型&quot; class=&quot;head
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/categories/Hadoop/Hive/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>i-Hive-1</title>
    <link href="https://airpoet.github.io/2018/06/13/Hadoop/Study/3-Hive/i-Hive-1/"/>
    <id>https://airpoet.github.io/2018/06/13/Hadoop/Study/3-Hive/i-Hive-1/</id>
    <published>2018-06-13T14:04:40.446Z</published>
    <updated>2018-06-15T16:38:40.138Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Hive-初探"><a href="#1-Hive-初探" class="headerlink" title="1)  Hive 初探"></a>1)  Hive 初探</h1><h3 id="Hive-的数据存储"><a href="#Hive-的数据存储" class="headerlink" title="Hive 的数据存储"></a>Hive 的数据存储</h3><ul><li>Hive的数据存储基于Hadoop HDFS</li><li>Hive没有专门的数据存储格式</li><li>存储结构主要包括：数据库、文件、表、视图、索引</li><li>Hive默认可以直接加载文本文件（TextFile），还支持SequenceFile、RCFile </li><li>创建表时，指定Hive数据的列分隔符与行分隔符，Hive即可解析数据</li></ul><h3 id="Hive的系统架构"><a href="#Hive的系统架构" class="headerlink" title="Hive的系统架构"></a>Hive的系统架构</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-13-152054.jpg" alt=""></p><ul><li>用户接口，包括 CLI，JDBC/ODBC，WebUI</li><li>元数据存储，通常是存储在关系数据库如 mysql, derby 中</li><li>解释器、编译器、优化器、执行器</li><li>Hadoop：用 HDFS 进行存储，利用 MapReduce 进行计算</li></ul><h3 id="Hive的系统架构-1"><a href="#Hive的系统架构-1" class="headerlink" title="Hive的系统架构"></a>Hive的系统架构</h3><ul><li>用户接口主要有三个：CLI，JDBC/ODBC和 WebUI<ul><li>CLI，即Shell命令行</li><li>JDBC/ODBC 是 Hive 的Java，与使用传统数据库JDBC的方式类似</li><li>WebGUI是通过浏览器访问 Hive</li></ul></li><li>Hive 将元数据存储在数据库中(metastore)，目前只支持 mysql、derby。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等</li><li>解释器、编译器、优化器完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划（plan）的生成。生成的查询计划存储在 HDFS 中，并在随后由 MapReduce 调用执行</li><li>Hive 的数据存储在 HDFS 中，大部分的查询由 MapReduce 完成（包含 <em> 的查询，比如 select </em> from table 不会生成 MapRedcue 任务</li></ul><h3 id="Hive的metastore"><a href="#Hive的metastore" class="headerlink" title="Hive的metastore"></a>Hive的metastore</h3><ul><li>metastore是hive元数据的集中存放地。</li><li>metastore默认使用内嵌的derby数据库作为存储引擎</li><li>Derby引擎的缺点：一次只能打开一个会话</li><li>使用Mysql作为外置存储引擎，多用户同时访问 </li></ul><h3 id="Hive-和-Hadoop-的调用关系"><a href="#Hive-和-Hadoop-的调用关系" class="headerlink" title="Hive 和 Hadoop 的调用关系"></a>Hive 和 Hadoop 的调用关系</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-13-140311.jpg" alt=""></p><blockquote><p>1、提交sql  交给驱动<br>2、驱动编译    解析相关的字段表信息<br>3、去metastore查询相关的信息    返回字段表信息<br>4、编译返回信息 发给驱动<br>5、驱动发送一个执行计划    交给执行引擎<br>6.1、DDLs <strong>对数据库表的操作的, 直接和metastore交互</strong>,<code>create table t1(name string);</code></p><p>6.2、完成job返回数据信息、找<strong>namenode</strong>查数据<br>6.3、<strong>namenode</strong>交互<code>select count(1) from t1;</code><br>7、返回结果信息集</p></blockquote><h3 id="Hive-参数配置使用"><a href="#Hive-参数配置使用" class="headerlink" title="Hive 参数配置使用"></a>Hive 参数配置使用</h3><table><thead><tr><th>命名空间</th><th>使用权限</th><th>描述</th></tr></thead><tbody><tr><td>hivevar</td><td>可读写</td><td>$   hive -d name=zhangsan;</td></tr><tr><td>hiveconf</td><td>可读写</td><td>\$   hive –hiveconf hive.cli.print.current.db=true;   $   hive –hiveconf hive.cli.print.header=true;</td></tr><tr><td>system</td><td>可读写</td><td>java定义的配置属性，如system:user.name</td></tr><tr><td>env</td><td>只读</td><td>shell环境变量，如env:USER</td></tr></tbody></table><ul><li><p>hivevar </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用场景: 起别名</span></span><br><span class="line">hive -d name=zhangsan  <span class="comment">#传参</span></span><br><span class="line">&gt; create table t2(name string,<span class="variable">$&#123;name&#125;</span> string); <span class="comment">#取参数</span></span><br><span class="line">&gt; desc t2;</span><br><span class="line">---</span><br><span class="line">name                string</span><br><span class="line">zhangsan            string</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>hiveconf :</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  显示当前数据库名称</span></span><br><span class="line">[ap@cs2]~% hive --hiveconf hive.cli.print.current.db=<span class="literal">true</span>;</span><br><span class="line">hive (default)&gt; create database mydb;</span><br><span class="line">hive (default)&gt; use mydb;</span><br><span class="line">hive (mydb)&gt; </span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示表头(字段名)</span></span><br><span class="line">hive --hiveconf hive.cli.print.header=<span class="literal">true</span>;</span><br><span class="line">select * from t2;</span><br><span class="line">t2.namet2.zhangsan</span><br></pre></td></tr></table></figure></li></ul><h3 id="Hive-的脚本执行"><a href="#Hive-的脚本执行" class="headerlink" title="Hive 的脚本执行"></a>Hive 的脚本执行</h3><ul><li><strong>Hive -e “xx ”</strong><ul><li>e 就是 edit, 在终端打印输出</li></ul></li><li><strong>Hive -e “show tables” &gt;&gt; a.txt</strong><ul><li>可以把执行结果重定向到文件中</li></ul></li><li><strong>Hive -S -e “show tables” &gt;&gt; a.txt</strong><ul><li>-S : silence 安静的执行</li></ul></li><li><strong>hive -f file</strong> <ul><li>hive -f hql ,  hql 是文件, 执行文件</li><li>执行完了之后,  就离开 hive 命令行</li></ul></li><li><strong>hive -i /home/ap/hive-init.sql</strong> <ul><li>执行完了,还在控制台, 可以继续操作</li></ul></li><li><strong>hive&gt;source file</strong><ul><li>source + 文件名  : 直接执行当前目录文件</li><li>source /home/ap/xx.sql;</li></ul></li></ul><h3 id="hive与依赖环境的交互"><a href="#hive与依赖环境的交互" class="headerlink" title="hive与依赖环境的交互"></a>hive与依赖环境的交互</h3><ul><li><strong>与linux交互命令 ！</strong><ul><li><code>!ls</code></li><li><code>!pwd</code></li></ul></li><li><strong>与hdfs交互命令</strong><ul><li><code>dfs -ls /</code></li><li><code>dfs -mkdir /hive</code></li><li><code>hive (default)&gt; dfs -rm -r /user/hive/warehouse/t5;</code></li></ul></li></ul><h3 id="Hive-的-JDBC-模式"><a href="#Hive-的-JDBC-模式" class="headerlink" title="Hive 的 JDBC 模式"></a>Hive 的 JDBC 模式</h3><ul><li>JAVA API交互执行方式 </li><li>hive 远程服务 (端口号1000  0) 启动方式<ul><li><code>hive --service hiveserver2</code></li><li><code>org.apache.hive.jdbc.HiveDriver</code></li></ul></li><li>在java代码中调用hive的JDBC建立连接</li><li>可以用 beeline 连接</li></ul><h3 id="SET命令使用"><a href="#SET命令使用" class="headerlink" title="SET命令使用"></a>SET命令使用</h3><ul><li>Hive 控制台set 命令<ul><li>set;    set -v;  显示所有的环境变量</li><li><code>set hive.cli.print.current.db=true;</code></li><li><code>set hive.cli.print.header=true;</code> </li><li><code>set hive.metastore.warehouse.dir=/hive;</code></li></ul></li><li><strong>hive参数初始化配置set命令:</strong><ul><li><strong>~/.hiverc</strong><ul><li>创建此文件, 在此文件中配置初始化命令</li></ul></li><li>补充：<br><strong>hive历史操作命令集</strong><br><strong>~/.hivehistory</strong></li></ul></li></ul><p><br></p><h1 id="2-Hive数据类型"><a href="#2-Hive数据类型" class="headerlink" title="2)  Hive数据类型"></a>2)  Hive数据类型</h1><h3 id="a-基本数据类型"><a href="#a-基本数据类型" class="headerlink" title="a) 基本数据类型"></a>a) 基本数据类型</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-13-163316.jpg" alt=""></p><h3 id="b-复合数据类型"><a href="#b-复合数据类型" class="headerlink" title="b) 复合数据类型"></a>b) 复合数据类型</h3><blockquote><p>创建学生表</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> student(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">INT</span>,</span><br><span class="line">    <span class="keyword">name</span> <span class="keyword">STRING</span>,</span><br><span class="line">    favors <span class="built_in">ARRAY</span>\&lt;<span class="keyword">STRING</span>&gt;,</span><br><span class="line">    scores <span class="keyword">MAP</span>&lt;<span class="keyword">STRING</span>, <span class="built_in">FLOAT</span>&gt;</span><br><span class="line">);</span><br></pre></td></tr></table></figure><table><thead><tr><th>默认分隔符</th><th>描述</th><th>语句</th></tr></thead><tbody><tr><td>\n</td><td>分隔行</td><td>LINES   TERMINATED BY ‘\t’</td></tr><tr><td>^A</td><td>分隔字段(列)，显示编码使用\001</td><td>FIELDS   TERMINATED BY ‘\001’</td></tr><tr><td>^B</td><td>分隔复合类型中的元素，显示编码使用\002</td><td>COLLECTION   ITEMS TERMINATED BY ‘\002’</td></tr><tr><td>^C</td><td>分隔map元素的key和value，显示编码使用\003</td><td>MAP   KEYS TERMINATED BY ‘\003’</td></tr></tbody></table><h4 id="a-Struct-使用"><a href="#a-Struct-使用" class="headerlink" title="a.  Struct 使用"></a>a.  Struct 使用</h4><p><strong>Structs内部的数据可以通过DOT（.）来存取</strong>，例如，表中一列c的类型为<code>STRUCT{a INT; b INT}</code>，我们可以通过c.a来访问域a</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># 数据</span><br><span class="line">1001,zhangsan:24</span><br><span class="line">1002,lisi:28</span><br><span class="line">1003,wangwu:25</span><br><span class="line"></span><br><span class="line"># 1.创建表</span><br><span class="line">hive&gt; create table student_test(id INT, info struct&lt;name:STRING, age:INT&gt;)ROW FORMAT DELIMITED FIELDS TERMINATED BY ','COLLECTION ITEMS TERMINATED BY ':';</span><br><span class="line"></span><br><span class="line"># 2.加载表</span><br><span class="line">hive&gt; load data local inpath "student_test" into table student_test;</span><br><span class="line"></span><br><span class="line"># 3.顺便设置 显示表头,和当前数据库</span><br><span class="line">hive&gt; set hive.cli.print.header=true;</span><br><span class="line">hive&gt; set hive.cli.print.current.db=true;</span><br><span class="line"></span><br><span class="line"># 4. 展示所有的</span><br><span class="line">hive (default)&gt; select * from student_test;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">student_test.idstudent_test.info</span><br><span class="line">1001&#123;"name":"zhangsan","age":24&#125;</span><br><span class="line">1002&#123;"name":"lisi","age":28&#125;</span><br><span class="line">1003&#123;"name":"wangwu","age":25&#125;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line"></span><br><span class="line"># Struct -结构体-使用 . </span><br><span class="line">hive (default)&gt; select id,info.name,info.age from student_test;</span><br><span class="line">idnameage</span><br><span class="line">1001zhangsan24</span><br><span class="line">1002lisi28</span><br><span class="line">1003wangwu25</span><br></pre></td></tr></table></figure><h4 id="b-Array-使用"><a href="#b-Array-使用" class="headerlink" title="b. Array 使用"></a>b. Array 使用</h4><p><strong>Array中的数据为相同类型</strong>，例如，假如array A中元素<code>[&#39;a&#39;,&#39;b&#39;,&#39;c’]</code>，则A[1]的值为’b’</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># 原始数据</span><br><span class="line">zhangsan,24:25:27:37</span><br><span class="line">lisi,28:39:23:43</span><br><span class="line">wangwu,25:23:02:54</span><br><span class="line"></span><br><span class="line"># 创建表</span><br><span class="line">hive (default)&gt; create table class_test(name string,student_id_list array&lt;int&gt;) row format delimited fields terminated by ',' collection items terminated by ':';</span><br><span class="line"></span><br><span class="line"># 加载表</span><br><span class="line">hive (default)&gt; load data local inpath "class_test" into table class_test;</span><br><span class="line"></span><br><span class="line"># 查看表</span><br><span class="line">hive (default)&gt; select * from class_test;</span><br><span class="line">OK</span><br><span class="line">class_test.nameclass_test.student_id_list</span><br><span class="line">zhangsan[24,25,27,37]</span><br><span class="line">lisi[28,39,23,43]</span><br><span class="line">wangwu[25,23,2,54]</span><br><span class="line"></span><br><span class="line"># 查看数据中某个元素</span><br><span class="line">hive (default)&gt; select name, student_id_list[0] from class_test where name='zhangsan';</span><br><span class="line">OK</span><br><span class="line">name_c1</span><br><span class="line">zhangsan24</span><br></pre></td></tr></table></figure><h4 id="c-Map-使用"><a href="#c-Map-使用" class="headerlink" title="c. Map 使用"></a>c. Map 使用</h4><p>访问指定域可以通过[“指定域名称”]进行，例如，一个Map M包含了一个group-&gt;gid的kv对，<strong>gid的值可以通过M[‘group’]来获取</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"># 原始数据</span><br><span class="line">1001job:80,team:60,person:70</span><br><span class="line">1002job:60,team:80,person:80</span><br><span class="line">1003job:90,team:70,person:100</span><br><span class="line"></span><br><span class="line"># 创建表</span><br><span class="line">hive (default)&gt; create table employee(id string,perf map&lt;string,int&gt;) row format delimited fields terminated by '\t' collection items terminated by ',' map keys terminated by ':';</span><br><span class="line"></span><br><span class="line"># 导入</span><br><span class="line">hive (default)&gt; load data local inpath "employee_data" into table employee;</span><br><span class="line"></span><br><span class="line"># 查看</span><br><span class="line">hive (default)&gt; select * from employee;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">employee.idemployee.perf</span><br><span class="line">1001&#123;"job":80,"team":60,"person":70&#125;</span><br><span class="line">1002&#123;"job":60,"team":80,"person":80&#125;</span><br><span class="line">1003&#123;"job":90,"team":70,"person":100&#125;</span><br><span class="line">Time taken: 0.228 seconds, Fetched: 3 row(s)</span><br><span class="line"></span><br><span class="line"># 查看单个</span><br><span class="line">hive (default)&gt; select id,perf['job'],perf['team'],perf['person'] from employee;</span><br><span class="line">OK</span><br><span class="line">id_c1_c2_c3</span><br><span class="line">1001806070</span><br><span class="line">1002608080</span><br><span class="line">10039070100</span><br><span class="line"></span><br><span class="line"># 显示别名</span><br><span class="line">hive (default)&gt; select id,perf['job'] as job,perf['team'] as team,perf['person'] as person from employee;</span><br><span class="line">OK</span><br><span class="line">idjobteamperson</span><br><span class="line">1001806070</span><br><span class="line">1002608080</span><br><span class="line">10039070100</span><br></pre></td></tr></table></figure><p><br></p><h1 id="3-数据定义"><a href="#3-数据定义" class="headerlink" title="3) 数据定义"></a>3) 数据定义</h1><h3 id="a-数据库定义"><a href="#a-数据库定义" class="headerlink" title="a)  数据库定义"></a>a)  数据库定义</h3><ul><li><p>默认数据库”default”</p></li><li><p>使用某个数据库 <code>use &lt;数据库名&gt;</code></p></li><li><p>创建一个新库</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span>  </span><br><span class="line">[<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] mydb  </span><br><span class="line">[LOCATION] <span class="string">'/.......'</span>  </span><br><span class="line">[<span class="keyword">COMMENT</span>] <span class="string">'....’;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">hive&gt;SHOW DATABASES;</span></span><br><span class="line"><span class="string">hive&gt;DESCRIBE DATABASE [extended] mydb;</span></span><br><span class="line"><span class="string">hive&gt;DROP DATABASE [IF EXISTS] mydb [CASCADE];</span></span><br></pre></td></tr></table></figure></li><li><p>创建</p><ul><li><code>create database db1;</code></li></ul></li><li><p>删除</p><ul><li><code>drop database if exists db1;</code></li><li><strong>级联删除</strong><ul><li><code>drop database if exists db1 cascade;</code></li></ul></li></ul></li></ul><h3 id="b-表定义"><a href="#b-表定义" class="headerlink" title="b)  表定义"></a>b)  表定义</h3><ul><li><p>创建表</p><ul><li><p>hive&gt;CREATE TABLE IF NOT EXISTS t1(…) </p><p>[COMMENT ‘….’] </p><p>[LOCATION ‘…’] </p><ul><li><code>hive (default)&gt; create table t4(name string,age int) row format delimited fields terminated by &quot;\t”;</code></li></ul></li><li><p>hive&gt; SHOW TABLES in mydb;</p><ul><li><code>show tables in mydb ‘’class*“</code> : 查看以 mydb 库中, 以 class 开头的表</li></ul></li><li><p><strong>hive&gt;CREATE TABLE t2 LIKE t1;</strong>   复制表</p><ul><li><strong>只会复制表结构</strong></li><li><code>hive (default)&gt; create table t2 like t1;</code></li><li><code>hive (mydb)&gt; create table t3 like default.employee;</code><ul><li>复制其它库的表</li></ul></li></ul></li><li><p>hive&gt;DESCRIBE t2;</p><ul><li>desc t2;  # 效果一样的</li><li><code>desc extended t1;</code>    # 查看更详细的表信息</li><li><strong><code>hive (default)&gt; desc formatted t1;</code>  # 格式化查看表的详细信息</strong></li></ul></li><li><p>drop  table xxx;</p><ul><li>删除表</li></ul></li></ul></li></ul><h3 id="c-列定义"><a href="#c-列定义" class="headerlink" title="c) 列定义"></a>c) 列定义</h3><ul><li><p>修改列的名称、类型、位置、注释</p><ul><li><p><code>ALTER TABLE t3 CHANGE COLUMN old_name new_name String COMMENT &#39;...&#39; AFTER column2;</code></p></li><li><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 修改列名</span><br><span class="line">hive (default)&gt; alter table t1 change column name username string comment 'new name';</span><br><span class="line"># 查看表</span><br><span class="line">hive (default)&gt; desc t1;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">col_namedata_type<span class="keyword">comment</span></span><br><span class="line">username            <span class="keyword">string</span>              <span class="keyword">new</span> <span class="keyword">name</span></span><br><span class="line">age                 <span class="built_in">int</span></span><br><span class="line"><span class="comment">---</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>增加列</p><ul><li><p><code>hive&gt; ALTER TABLE t3 ADD COLUMNS(gender int);</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 查看表结构</span><br><span class="line">hive (default)&gt; desc t3;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">col_namedata_type<span class="keyword">comment</span></span><br><span class="line"><span class="keyword">name</span>                <span class="keyword">string</span></span><br><span class="line"><span class="comment">---</span></span><br><span class="line"># 添加列</span><br><span class="line">hive (<span class="keyword">default</span>)&gt; <span class="keyword">alter</span> <span class="keyword">table</span> t3 <span class="keyword">add</span> <span class="keyword">columns</span>(gender <span class="built_in">int</span>);</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">hive (default)&gt; desc t3;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">col_namedata_type<span class="keyword">comment</span></span><br><span class="line"><span class="keyword">name</span>                <span class="keyword">string</span></span><br><span class="line">gender              <span class="built_in">int</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>删除列  replace </p><ul><li><strong>非常不建议使用</strong>, 会造成数据错乱, 一般采取重新创建一张表的方式.</li></ul></li></ul><h1 id="4-Hive的数据模型"><a href="#4-Hive的数据模型" class="headerlink" title="4) Hive的数据模型"></a>4) Hive的数据模型</h1><h3 id="a-管理表-又称为内部表-受控表"><a href="#a-管理表-又称为内部表-受控表" class="headerlink" title="a) 管理表 - 又称为内部表, 受控表"></a>a) 管理表 - 又称为内部表, 受控表</h3><h4 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a><strong>基本操作</strong></h4><ul><li>创建数据文件inner_table.dat</li><li>创建表<ul><li><code>hive&gt;create table inner_table (key string);</code></li></ul></li><li><strong>加载数据</strong><ul><li><strong>加载本地数据</strong><ul><li><code>hive&gt;load data local inpath &#39;/root/inner_table.dat&#39; into table inner_table;</code></li></ul></li><li><strong>加载HDFS 上数据</strong><ul><li><code>hive&gt;load data inpath ‘xxx’ into table xxx;</code></li></ul></li><li><strong>区别</strong><ul><li>加载 hdfs 上的数据没有 <strong>local</strong></li><li><strong>加载本地数据是 copy 一份, 加载 hdfs 上的数据是直接移动数据到加载的表目录下– mv</strong></li></ul></li></ul></li><li>查看数据<ul><li><code>select * from inner_table</code></li><li><code>select count(*) from inner_table</code></li></ul></li><li><strong>删除表 <code>drop table inner_table</code></strong></li><li><strong>清空表 <code>truncate table table_name;</code></strong> </li><li><strong>注意: </strong><ul><li><strong>如果创建表的时候, 只指定了目录, 没有指定表名, 删除表的时候, 会把该目录下的所有表全部删掉</strong></li><li><code>hive (mydb)&gt; create table t2(id int)location &#39;/home/t2&#39;;</code></li></ul></li></ul><h4 id="内部表解释"><a href="#内部表解释" class="headerlink" title="内部表解释"></a><strong>内部表解释</strong></h4><ul><li>管理表，也称作<strong>内部表</strong>,受控表<ul><li>所有的 Table 数据（不包括 External Table）<strong>都保存在warehouse这个目录中。</strong></li><li><strong>删除表时，元数据与数据都会被删除</strong></li><li>创建过程和数据加载过程（这两个过程可以在同一个语句中完成），<strong>在加载数据的过程中，实际数据会被移动到数据仓库目录中</strong>；之后<strong>对数据对访问</strong>将会<strong>直接在数据仓库目录中</strong>完成。删除表时，表中的数据和元数据将会被同时删除</li></ul></li></ul><h4 id="内部表转为外部表-外部表转为内部表"><a href="#内部表转为外部表-外部表转为内部表" class="headerlink" title="内部表转为外部表,  外部表转为内部表"></a><strong>内部表转为外部表,  外部表转为内部表</strong></h4><ul><li><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">hive (mydb)&gt; create table t1(id int);</span><br><span class="line"></span><br><span class="line"># manage_table 转换为 外部表 external_table </span><br><span class="line">## 注意: 修改为外部表时, 后面2个都要大写</span><br><span class="line">hive (mydb)&gt; alter table t1 set tblproperties('EXTERNAL'='TRUE');</span><br><span class="line">## 修改为内部表</span><br><span class="line">hive (mydb)&gt; alter table t1 set tblproperties('EXTERNAL'='FALSE');</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 查看t1详细信息</span><br><span class="line">desc formatted t1;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">Location:           hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1</span><br><span class="line">Table Type:         EXTERNAL_TABLE</span><br><span class="line"><span class="comment">-- </span></span><br><span class="line"></span><br><span class="line"># 删除t1</span><br><span class="line">hive (mydb)&gt; drop table t1;</span><br><span class="line"># 此时再查看, 已经没了</span><br><span class="line">hive (mydb)&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">tab_name</span><br><span class="line"></span><br><span class="line"># 但是查看hdfs 路径会发现还在, 因为此表现在已经是外部表, 删除不会删除数据</span><br><span class="line">dfs -ls /user/hive/warehouse/mydb.db/t1</span><br><span class="line"></span><br><span class="line"># 如果此时再创建一个新表 t1, 表结构一样, 则数据会自动加载</span><br></pre></td></tr></table></figure></li></ul><h3 id="b-※-外部表"><a href="#b-※-外部表" class="headerlink" title="b) ※ 外部表"></a>b) ※ 外部表</h3><h4 id="基本操作-1"><a href="#基本操作-1" class="headerlink" title="基本操作"></a><strong>基本操作</strong></h4><ul><li>创建数据文件external_table.dat</li><li>创建表<ul><li><code>hive&gt;create external table external_table1 (key string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39; location &#39;/home/external’;</code></li></ul></li><li>在HDFS创建目录/home/external<ul><li><code>#hadoop fs -put /home/external_table.dat /home/external</code></li><li>在工作中, 一般都这样使用, 把数据上传到 hdfs 中</li></ul></li><li>加载数据<ul><li><code>LOAD DATA  &#39;/home/external_table1.dat&#39; INTO TABLE external_table1;</code></li></ul></li><li>查看数据<ul><li><code>select * from external_table</code></li><li><code>select count(*) from external_table</code></li></ul></li><li>删除表 <ul><li><code>drop table external_table</code></li></ul></li></ul><h4 id="外部表解释"><a href="#外部表解释" class="headerlink" title="外部表解释"></a><strong>外部表解释</strong></h4><ul><li>包含External 的表叫外部表<ul><li>删除外部表只删除metastore的元数据，不删除hdfs中的表数据</li><li>外部表 只有一个过程，加载数据和创建表同时完成，并不会移动到数据仓库目录中，只是与外部数据建立一个链接。当删除一个 外部表 时，仅删除该链接</li><li>指向已经在 HDFS 中存在的数据，可以创建 Partition</li><li>它和 内部表 在元数据的组织上是相同的，而实际数据的存储则有较大的差异</li></ul></li></ul><h4 id="外部表语法"><a href="#外部表语法" class="headerlink" title="外部表语法"></a>外部表语法</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> page_view</span><br><span class="line">( viewTime <span class="built_in">INT</span>, </span><br><span class="line">  userid <span class="built_in">BIGINT</span>,</span><br><span class="line">  page_url <span class="keyword">STRING</span>, </span><br><span class="line"> referrer_url <span class="keyword">STRING</span>, </span><br><span class="line">  ip <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'IP Address of the User'</span>,</span><br><span class="line">  country <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'country of origination‘</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">    COMMENT '</span>This <span class="keyword">is</span> the staging page <span class="keyword">view</span> <span class="keyword">table</span><span class="string">'</span></span><br><span class="line"><span class="string">    ROW FORMAT DELIMITED FIELDS TERMINATED BY '</span>\t<span class="string">' LINES TERMINATED BY '</span>\n<span class="string">'</span></span><br><span class="line"><span class="string">    STORED AS TEXTFILE</span></span><br><span class="line"><span class="string">    LOCATION '</span>hdfs://centos:<span class="number">9000</span>/<span class="keyword">user</span>/<span class="keyword">data</span>/staging/page_view<span class="string">';</span></span><br></pre></td></tr></table></figure><h4 id="外部表注意点"><a href="#外部表注意点" class="headerlink" title="外部表注意点:"></a>外部表注意点:</h4><ul><li><p>先创建外部表/内部表, 表名为<code>t3</code>, 再往<code>t3</code>传对应字段的数据, 就可以直接 select 数据了</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">## 先创建表(内部/外部都一样), 再往表中上传数据, 数据可以直接加载</span><br><span class="line">hive (mydb)&gt; create external table t3(id int);</span><br><span class="line">hive (mydb)&gt; create table t3(id int);</span><br><span class="line"></span><br><span class="line"><span class="comment">--</span></span><br><span class="line">intdata: </span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line"><span class="comment">--</span></span><br><span class="line">[ap@cs2]~% hadoop fs -put intdata hdfs://cs1:9000/user/hive/warehouse/mydb.db/t3</span><br><span class="line"></span><br><span class="line">hive (mydb)&gt; select * from t3;</span><br><span class="line"></span><br><span class="line">t3.id</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td></tr></table></figure></li><li><p><u>删除外部表之后, 原本数据不会删除</u>, 此时<strong>在相同的父路径</strong>创建与被删除表<strong>字段相同&amp;名称相同</strong>的<strong>内部/外部表</strong>, 数据<strong>也会直接加载</strong></p></li><li><p><strong>再看一个操作</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 在 mydb.db 库下创建一个外部表 t5, 指定路径为 '/external/t5'</span><br><span class="line"># 此时在 mydb.db 库的路径下是不存在 t5表的, 而是存在 '/external/t5' 中</span><br><span class="line"># 但是使用 'show tables' 是存在 t5 的</span><br><span class="line">hive (mydb)&gt; create external table t5(id int) location '/external/t5';</span><br><span class="line"># 往此目录传数据, 注意: 此时传过去, intdata 数据存在 t5目录下</span><br><span class="line">[ap@cs2]~% hadoop fs -put intdata /external/t5</span><br><span class="line">[ap@cs2]~% hadoop fs -put intdata /external/t5/i2</span><br><span class="line"># 继续传数据, 查询的时候, 就是传的全部数据'相当于所有的数据都在 t5表中'</span><br><span class="line">[ap@cs2]~% hadoop fs -put intdata /external/t5</span><br><span class="line"># 注意: 如果传到 t5 目录下, 没有指定数据文件名的话, 会默认采用数据的名称文件.</span><br></pre></td></tr></table></figure></li></ul><h3 id="c-※-分区表"><a href="#c-※-分区表" class="headerlink" title="c)  ※ 分区表"></a>c)  ※ 分区表</h3><h4 id="基本概念和操作"><a href="#基本概念和操作" class="headerlink" title="基本概念和操作"></a>基本概念和操作</h4><ul><li>分区可以理解为分类，通过分类把不同类型的数据放到不同的目录下。</li><li>分类的标准就是分区字段，可以一个，也可以多个。</li><li>分区表的意义在于优化查询。查询时尽量利用分区字段。如果不使用分区字段，就会全部扫描。</li></ul><blockquote><p><strong>创建分区表, 指定分区字段</strong></p><p><code>hive&gt;CREATE TABLE t3(...) PARTITIONED BY (province string);</code></p><ul><li>创建表的时候, 指定分区字段 key<code>province</code></li></ul><p><strong>为分区字段添加一个值</strong></p><p><code>hive&gt;ALTER TABLE t3 ADD [IF NOT EXISTS] PARTITION(...) LOCATION &#39;...’;</code></p><ul><li><code>alter table t3 add if not exists partition(province=‘hubei‘’’);</code></li><li><code>alter table t3 add if not exists partition(province=“jiangsu‘’’);</code></li><li>可为此分区字段添加多个值,  为 province 添加 hubei, hunan….</li></ul><p><strong>查看表的分区字段&amp;值</strong></p><p><code>hive&gt;SHOW PARTITIONS t3 [partition (province=&#39;beijing&#39;)];</code></p><p><strong>删除分区</strong></p><p><code>hive&gt;ALTER TABLE t3 DROP PARTITION(province=‘beijing’.);</code></p><ul><li>这里是删除北京的分区 (如果是内部表, 会连数据一起删除)</li></ul><p><strong>设置表不能被删除/查询</strong>  ——– 这里报语法错误, :TODO</p><ul><li>防止分区被删除:<code>alter table student_p partition (part=&#39;aa&#39;) enable no_drop;</code></li><li>防止分区被查询:<code>alter table student_p partition (part=&#39;aa&#39;) enable offline;</code></li><li>enable 和 disable 是反向操作</li></ul><p><strong>其它一些相关命令</strong></p><p>SHOW TABLES; # 查看所有的表</p><p>SHOW TABLES ‘<em>TMP</em>‘; #支持模糊查询</p><p><code>SHOW PARTITIONS TMP_TABLE;</code> #查看表有哪些分区</p><p>DESC TMP_TABLE; #查看表结构</p></blockquote><h4 id="创建分区表完整语法"><a href="#创建分区表完整语法" class="headerlink" title="创建分区表完整语法"></a>创建分区表完整语法</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> tmp_table #表名</span><br><span class="line">(</span><br><span class="line">title   <span class="keyword">string</span>, # 字段名称 字段类型</span><br><span class="line">minimum_bid     <span class="keyword">double</span>,</span><br><span class="line">quantity        <span class="built_in">bigint</span>,</span><br><span class="line">have_invoice    <span class="built_in">bigint</span></span><br><span class="line">)<span class="keyword">COMMENT</span> <span class="string">'注释：XXX'</span> #表注释</span><br><span class="line"> PARTITIONED <span class="keyword">BY</span>(pt <span class="keyword">STRING</span>) #分区表字段（如果你文件非常之大的话，采用分区表可以快过滤出按分区字段划分的数据）</span><br><span class="line"> <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> </span><br><span class="line">   <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\001'</span>   # 字段是用什么分割开的</span><br><span class="line">STORED AS SEQUENCEFILE; #用哪种方式存储数据，SEQUENCEFILE是hadoop自带的文件压缩格式</span><br></pre></td></tr></table></figure><h4 id="分区表注意点-错误点"><a href="#分区表注意点-错误点" class="headerlink" title="分区表注意点(错误点)"></a>分区表注意点(错误点)</h4><ul><li><p><strong>1) 分区表在 load 数据的时候, 得指定分区, 否则会报错</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 错误1: 分区表在 load 数据的时候, 得指定分区</span><br><span class="line">hive (mydb)&gt; load data local inpath '~/ihivedata/intdata' into table t6;</span><br><span class="line">FAILED: SemanticException [Error 10062]: Need to specify partition columns because the destination table is partitioned</span><br><span class="line"></span><br><span class="line"># 错误2: 导入本地数据的时候, 'path'是从当前所在路径开始的</span><br><span class="line">hive (mydb)&gt; load data local inpath '~/ihivedata/intdata' into table t6 partition(class='job1');</span><br><span class="line">FAILED: SemanticException Line 1:23 Invalid path ''&lt;sub&gt;/ihivedata/intdata'': No files matching path file:/home/ap/&lt;/sub&gt;/ihivedata/intdata</span><br><span class="line"></span><br><span class="line"># 这里就正确了</span><br><span class="line">hive (mydb)&gt; load data local inpath 'ihivedata/intdata' into table t6 partition(class='job1');</span><br><span class="line">Loading data to table mydb.t6 partition (class=job1)</span><br><span class="line">OK</span><br></pre></td></tr></table></figure><ul><li><p><strong>本质原因:</strong> </p><ul><li><strong>分区表的分区, 就是在 hdfs 上, 原表的文件夹下面创建了一个子文件夹, 文件夹名就是分区名.</strong></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-14-113618.png" alt="image-20180614193618027"></p><ul><li><strong>从本地 load 数据:</strong> <code>hive (mydb)&gt; load data local inpath &#39;ihivedata/intdata&#39; into table t6 partition(class=&#39;job1&#39;);</code></li><li><strong>load 数据指定分区之后, 会直接 load 到数据文件夹里面</strong></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-14-113550.png" alt="image-20180614193549709"></p></li><li><p><strong>2) 没有添加分区时, 直接往不存在的分区导入数据, 分区会自动创建</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 直接往不存在的分区load数据, 分区会自动创建</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'ihivedata/intdata'</span> <span class="keyword">into</span> <span class="keyword">table</span> t6 <span class="keyword">partition</span>(<span class="keyword">class</span>=<span class="string">'job110'</span>);</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p><strong>3) 手动在表中创建分区(文件夹), 并直接向此文件夹中导入数据</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"># 直接创建目录</span><br><span class="line">hive (mydb)&gt; dfs -mkdir /user/hive/warehouse/mydb.db/t6/class=job120;</span><br><span class="line"></span><br><span class="line"># 直接从 hadoop 端传数据</span><br><span class="line">hadoop fs -put ihivedata/intdata /user/hive/warehouse/mydb.db/t6/class=job120</span><br><span class="line"></span><br><span class="line"># 此时再 show partitions t6; 会发现并没有此分区</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">partition</span><br><span class="line">class=job1</span><br><span class="line">class=job110</span><br><span class="line">class=job2</span><br><span class="line">class=job3</span><br><span class="line">class=job4</span><br><span class="line"></span><br><span class="line"># 此时就需要手动'激活'此分区, 加入了就有了</span><br><span class="line">hive (mydb)&gt; alter table t6 add partition(class='job120');</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">hive (mydb)&gt; show partitions t6;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">partition</span><br><span class="line">class=job1</span><br><span class="line">class=job110</span><br><span class="line">class=job120</span><br><span class="line">class=job2</span><br><span class="line">class=job3</span><br><span class="line">class=job4</span><br><span class="line"></span><br><span class="line"># 查看分区信息</span><br><span class="line">hive (mydb)&gt; select * from t6 where class='job110';</span><br><span class="line">OK</span><br><span class="line">t6.idt6.class</span><br><span class="line">1job110</span><br><span class="line">2job110</span><br><span class="line">3job110</span><br><span class="line">4job110</span><br><span class="line">5job110</span><br></pre></td></tr></table></figure></li></ul><h4 id="复合分区"><a href="#复合分区" class="headerlink" title="复合分区"></a>复合分区</h4><h5 id="基本操作-2"><a href="#基本操作-2" class="headerlink" title="基本操作"></a>基本操作</h5><ul><li>创建数据文件<code>partition_table.dat</code></li><li>创建表<ul><li><code>create table t7(name string,age int)partitioned by(class string,city string)row format delimited fields terminated by &#39;\t&#39; stored as TEXTFILE;</code></li></ul></li><li><strong>在 Hive 下加载数据到分区</strong><ul><li><code>load data local inpath &#39;ihivedata/partidata&#39; into table t7 partition(class=&#39;job1&#39;,city=&#39;beijing&#39;);</code></li><li><code>load data local inpath &#39;ihivedata/partidata&#39; into table t7 partition(class=&#39;job1&#39;,city=&#39;shanghai&#39;);</code></li><li><code>load data local inpath &#39;ihivedata/partidata&#39; into table t7 partition(class=&#39;job2&#39;,city=&#39;ss&#39;);</code></li><li><strong>注意: 多级分区其实就是多级目录</strong><ul><li>越靠近左边, 目录层级越高; </li><li>越靠近右边, 目录层级越低; </li></ul></li><li><strong>load 数据到多级分区, load层级必须和整个层级数量相同</strong><ul><li>也就是说, 如果<strong>分区有2层</strong>, <strong>传数据</strong>的时候, 也<strong>必须传2层分区</strong>, 并且<strong>层级顺序必须一致</strong></li></ul></li></ul></li><li><strong>从Linux 本地直接导数据到分区</strong><ul><li><strong>可以直接在 hadoop UI 页面, 查看路径, 然后直接传到此路径中</strong></li><li><code>hadoop fs -put ihivedata/partidata /user/hive/warehouse/mydb.db/t7/class=job1/city=beijing/p2</code></li></ul></li><li>查看数据<ul><li><code>select * from partition_table</code></li><li><code>select count(*) from partition_table</code></li></ul></li><li>删除表<ul><li><code>drop table partition_table</code></li></ul></li><li>工作中 用的最多的是 <strong>外部表 + 分区表</strong> </li></ul><h3 id="d-桶表-主要用于抽样查询"><a href="#d-桶表-主要用于抽样查询" class="headerlink" title="d) 桶表 - 主要用于抽样查询"></a>d) 桶表 - 主要用于抽样查询</h3><h4 id="桶表的基本操作"><a href="#桶表的基本操作" class="headerlink" title="桶表的基本操作"></a>桶表的基本操作</h4><ul><li><p>桶表是对数据进行哈希取值，然后放到不同文件中存储。</p></li><li><p>创建桶表 <strong>clustered by(xx)</strong></p><ul><li><code>create table bucket_table(id string) clustered by(id) into 4 buckets;</code></li></ul></li><li><p><strong>加载数据</strong></p><ul><li><code>set hive.enforce.bucketing = true;</code></li><li><strong>insert into</strong> 插入<ul><li><code>insert into table bucket_table select id from t1;</code><ul><li><strong>桶表只能从其它表查询 加载, 不能直接导数据.</strong></li><li>如果查询到的数据为空, 同样会生成指定个数的表</li><li><strong>这里会走 MR</strong>, 不过貌似是可以调的? :TODO</li><li><code>Stage-Stage-1: Map: 1  Reduce: 4   Cumulative CPU: 13.9 sec   HDFS Read: 17522 HDFS Write: 302 SUCCESS</code></li></ul></li></ul></li><li><strong>insert overwrite</strong> 覆盖<ul><li><code>insert overwrite table bucket_table select id from t2;</code></li></ul></li></ul></li><li><p><strong>数据加载到桶表时，会对字段取hash值，然后与桶的数量取模。把数据放到对应的文件中。</strong></p><ul><li>所以<strong>顺序是打乱的</strong>, 不是原始 t1的数据顺序</li></ul></li><li><p><strong>查看数据</strong></p><ul><li><p>可以直接select * 查看全部</p></li><li><p>也可以直接单独查看每个桶的数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hive (mydb)&gt; dfs -cat /user/hive/warehouse/mydb.db/bucket_table/000000_0;</span><br><span class="line">4</span><br><span class="line">hive (mydb)&gt; dfs -cat /user/hive/warehouse/mydb.db/bucket_table/000001_0;</span><br><span class="line">5</span><br><span class="line">1</span><br><span class="line">hive (mydb)&gt; dfs -cat /user/hive/warehouse/mydb.db/bucket_table/000002_0;</span><br><span class="line">2</span><br><span class="line">hive (mydb)&gt; dfs -cat /user/hive/warehouse/mydb.db/bucket_table/000003_0;</span><br><span class="line">3</span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>修改桶的个数</strong></p><ul><li><code>alter table bucket_table clustered by (id) sorted by(id) into 10 buckets;</code><ul><li>但是这样修改之后, 生成的是原来的 copy, 并且里面的数据也很奇怪, 不知道是按照什么来执行的? <strong>:TODO</strong></li></ul></li></ul></li><li><p><strong>注意：</strong></p><ul><li><strong>物理上，每个桶就是表(或分区）目录里的一个文件</strong></li><li>一个作业产生的<strong>桶(输出文件)和reduce任务个数相同</strong></li></ul></li><li><p><strong>桶表工作中容易遇到的错误</strong></p><ul><li><p><strong>向桶表中插入其它表查出的数据的时候,  必须指定字段名</strong>, 否则会报字段不匹配.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">FAILED: SemanticException [Error 10044]: Line 1:12 Cannot <span class="keyword">insert</span> <span class="keyword">into</span> target <span class="keyword">table</span> because <span class="keyword">column</span> <span class="built_in">number</span>/types <span class="keyword">are</span> different <span class="string">'bucket_table'</span>: <span class="keyword">Table</span> insclause<span class="number">-0</span> has <span class="number">1</span> <span class="keyword">columns</span>, but <span class="keyword">query</span> has <span class="number">2</span> columns.</span><br><span class="line"></span><br><span class="line"># 应该是这样</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> bucket_table <span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> t6;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h4 id="※-桶表的抽样查询"><a href="#※-桶表的抽样查询" class="headerlink" title="※ 桶表的抽样查询"></a>※ 桶表的抽样查询</h4><ul><li>桶表的抽样查询<ul><li>select * from bucket_table tablesample<strong>(bucket 1 out of 4 on id);</strong></li></ul></li><li>tablesample是抽样语句<ul><li>语法解析：<strong>TABLESAMPLE(BUCKET x OUT OF y)</strong></li><li><strong>y必须是table总bucket数的<u>倍数</u>或者<u>因子</u>。</strong></li><li>hive根据y的大小，决定抽样的比例。</li><li><strong>例如</strong>，table总共分了64份，当y=32时，抽取(64/32=)2个bucket的数据，当y=128时，抽取(64/128=)1/2个bucket的数据。x表示从哪个bucket开始抽取。</li><li><strong>例如</strong>，table总bucket数为32，tablesample(bucket 3 out of 16)，表示总共抽取（32/16=）2个bucket的数据，分别为第3个bucket和第（3+16=）19个bucket的数据。</li></ul></li></ul><h1 id="5-Hive-视图的操作"><a href="#5-Hive-视图的操作" class="headerlink" title="5) Hive 视图的操作"></a>5) Hive 视图的操作</h1><ul><li>使用视图可以<strong>降低查询的复杂度</strong></li><li><strong>视图的创建</strong><ul><li><strong>create view</strong> v1 <strong>AS</strong> <u>select  t1.name from t1</u>;</li></ul></li><li><strong>视图的删除</strong><ul><li><strong>drop view</strong> if exists v1;</li></ul></li></ul><h1 id="6-Hive-索引的操作"><a href="#6-Hive-索引的操作" class="headerlink" title="6) Hive 索引的操作"></a>6) Hive 索引的操作</h1><ul><li><p><strong>创建索引</strong></p><ul><li><code>create index t1_index on table t1(id) as &#39;org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler&#39; with deferred rebuild in table t1_index_table;</code></li><li><code>t1_index</code>: <strong>索引名称</strong></li><li>as: 指定索引器</li><li><code>t1_index_table</code>: <strong>要创建的索引表</strong></li></ul></li><li><p><strong>显示索引</strong></p><ul><li><code>show formatted index on t1;</code></li></ul></li><li><p><strong>重建索引</strong></p><ul><li><p><strong>alter index</strong> t1_index <strong>on</strong> t1 <strong>rebuild</strong>;</p></li><li><p>重建完索引之后, 查看 t1_index_table 这张表, 就存了t1表文件具体的位置, 最后一列<code>t1_index_table._offsets</code>是 <strong>索引的偏移量, 类似于指针,  偏移量是索引的精髓</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (mydb)&gt; select * from t1_index_table;</span><br><span class="line">OK</span><br><span class="line">t1_index_table.idt1_index_table._bucketnamet1_index_table._offsets</span><br><span class="line">1hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata[0]</span><br><span class="line">2hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata[2]</span><br><span class="line">3hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata[4]</span><br><span class="line">4hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata[6]</span><br><span class="line">5hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata[8]</span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>分区字段本质上其实就是索引</strong></p></li></ul><h1 id="7-装载数据"><a href="#7-装载数据" class="headerlink" title="7) 装载数据"></a>7) 装载数据</h1><h3 id="普通装载数据"><a href="#普通装载数据" class="headerlink" title="普通装载数据:"></a><strong>普通装载数据:</strong></h3><ul><li><strong>从本地</strong>  put</li><li><p><strong>从 hive</strong> cp</p></li><li><p><strong>从文件中装载数据</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;LOAD DATA [LOCAL] INPATH '...' [OVERWRITE] INTO TABLE t2 [PARTITION (province='beijing')];</span><br></pre></td></tr></table></figure></li><li><p><strong>通过查询表装载数据</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 方式1</span><br><span class="line">hive&gt;INSERT OVERWRITE TABLE t2 PARTITION (province='beijing') SELECT * FROM xxx WHERE xxx;</span><br><span class="line"></span><br><span class="line"># 方式2</span><br><span class="line">hive&gt;FROM t4 </span><br><span class="line"> <span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> t3 <span class="keyword">PARTITION</span> (...) <span class="keyword">SELECT</span> ...WHERE... </span><br><span class="line"> <span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> t3 <span class="keyword">PARTITION</span> (...) <span class="keyword">SELECT</span> ...WHERE... </span><br><span class="line"> <span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> t3 <span class="keyword">PARTITION</span> (...) <span class="keyword">SELECT</span> ...WHERE...;</span><br></pre></td></tr></table></figure></li></ul><h3 id="动态装载数据"><a href="#动态装载数据" class="headerlink" title="动态装载数据"></a>动态装载数据</h3><ul><li><p>不开启只支持</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;INSERT OVERWRITE TABLE t3 PARTITION(province='bj', city='bj') </span><br><span class="line"><span class="keyword">SELECT</span> t.province, t.city <span class="keyword">FROM</span> temp t <span class="keyword">WHERE</span> t.province=<span class="string">'bj'</span>;</span><br></pre></td></tr></table></figure></li><li><p>开启动态分区支持</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;set hive.exec.dynamic.partition=true;</span><br><span class="line">hive&gt;set hive.exec.dynamic.partition.mode=nostrict;</span><br><span class="line">hive&gt;set hive.exec.max.dynamic.partitions.pernode=1000;</span><br></pre></td></tr></table></figure></li><li><p><strong>把 t6 表的所有的字段 (包括分区字段) 加载进 t9 对应的分区</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (mydb)&gt; insert overwrite table t9 partition(class) select id,class from t6;</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>单语句建表并同时装载数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;CREATE TABLE t4 AS SELECT ....</span><br></pre></td></tr></table></figure></li></ul><h1 id="8-导出数据"><a href="#8-导出数据" class="headerlink" title="8) 导出数据"></a>8) 导出数据</h1><ul><li><strong>在hdfs之间复制文件(夹)</strong><ul><li><code>hadoop fs -cp source destination</code></li><li><code>hive&gt; dfs -cp source destination</code></li><li>案例: <code>hive&gt;  dfs -get  /user/hive/warehouse/mydb.db/t9  /root/t9;</code><ul><li>从 hdfs 复制到本地</li></ul></li></ul></li><li>使用DIRECTORY<ul><li><code>hive&gt;INSERT OVERWRITE 【LOCAL】 DIRECTORY &#39;...&#39; SELECT ...FROM...WHERE ...;</code></li><li>案例:通过查询导出到 t9, 走的 MapReduce<ul><li>导到到 hdfs:  <code>insert overwrite directory &quot;/home/t9&quot; select * from t9;</code></li><li>导出到本地: <code>insert overwrite local directory &quot;/home/ap/t9&quot; select * from t9;</code></li></ul></li></ul></li></ul><h1 id="9-读模式-amp-写模式"><a href="#9-读模式-amp-写模式" class="headerlink" title="9) 读模式&amp;写模式"></a>9) 读模式&amp;写模式</h1><ul><li>RDBMS是写模式</li><li>Hive是读模式</li></ul><h1 id="10-完整建表语句语法"><a href="#10-完整建表语句语法" class="headerlink" title="10) 完整建表语句语法"></a>10) 完整建表语句语法</h1><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db_name.]table_name</span><br><span class="line">  [(col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)]</span><br><span class="line">    [<span class="keyword">COMMENT</span> table_comment]</span><br><span class="line">    [PARTITIONED <span class="keyword">BY</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)]</span><br><span class="line">    [CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) </span><br><span class="line">    [SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span>|<span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS]</span><br><span class="line">    [SKEWED <span class="keyword">BY</span> (col_name, col_name, ...) <span class="keyword">ON</span> ([(col_value, col_value, ...), ...|col_value, col_value, ...]) </span><br><span class="line">    [<span class="keyword">STORED</span> <span class="keyword">AS</span> DIRECTORIES]   ]</span><br><span class="line">  [ [<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format] </span><br><span class="line">    [<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format] | <span class="keyword">STORED</span> <span class="keyword">BY</span> <span class="string">'storage.handler.class.name'</span> [<span class="keyword">WITH</span> SERDEPROPERTIES (...)]   ]</span><br><span class="line">  [LOCATION hdfs_path]</span><br><span class="line">  [TBLPROPERTIES (property_name=property_value, ...)]     </span><br><span class="line">  [<span class="keyword">AS</span> select_statement]  (Note: <span class="keyword">not</span> supported <span class="keyword">when</span> creating <span class="keyword">external</span> tables.)</span><br></pre></td></tr></table></figure><h1 id="11-文件格式"><a href="#11-文件格式" class="headerlink" title="11) 文件格式"></a>11) 文件格式</h1><ul><li>TextFile</li><li>SequenceFile</li><li>RCFile</li><li>ORC</li></ul><h3 id="a-默认存储格式就是-TextFile"><a href="#a-默认存储格式就是-TextFile" class="headerlink" title="a) 默认存储格式就是 TextFile"></a>a) 默认存储格式就是 TextFile</h3><ul><li>存储空间消耗比较大，</li><li>并且压缩的text 无法分割和合并</li><li>查询的效率最低,可以直接存储，</li><li>加载数据的速度最高</li></ul><h3 id="b-使用SequenceFile存储"><a href="#b-使用SequenceFile存储" class="headerlink" title="b) 使用SequenceFile存储"></a>b) 使用SequenceFile存储</h3><ul><li>存储空间消耗大</li><li>压缩的文件可以分割和合并 </li><li>查询效率高</li><li>需要通过text文件转化来加载</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test2(<span class="keyword">str</span> <span class="keyword">STRING</span>) <span class="keyword">STORED</span> <span class="keyword">AS</span> SEQUENCEFILE;  </span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> hive.exec.compress.output=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> mapred.output.compress=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> mapred.output.compression.codec=com.hadoop.compression.lzo.LzoCodec;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> io.seqfile.compression.type=<span class="keyword">BLOCK</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> io.compression.codecs=com.hadoop.compression.lzo.LzoCodec;</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> test2 <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> test1;</span><br></pre></td></tr></table></figure><p><strong>注意点: SequenceFile 类型的表, 不能直接导入数据文件,  只能通过从他表查询</strong></p><ul><li><p><code>insert overwrite table t2 select * from t1;</code></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 查看此 'SequenceFile' 表</span><br><span class="line">hive (db2)&gt; dfs -cat /user/hive/warehouse/db2.db/t2/000000_0 ;</span><br><span class="line">SEQ"org.apache.hadoop.io.BytesWritableorg.apache.hadoop.io.Text*org.apache.hadoop.io.compress.DefaultCodec���/*&lt;bb�m�?x�c453x�c464x�c475x�c486x�c497hive (db2)&gt;</span><br></pre></td></tr></table></figure></li></ul><h3 id="c-使用RCFile存储"><a href="#c-使用RCFile存储" class="headerlink" title="c) 使用RCFile存储"></a>c) 使用RCFile存储</h3><p><strong>RCFILE是一种行列存储相结合的存储方式。首先，其将数据按行分块，保证同一个record在一个块上，避免读一个记录需要读取多个block。其次，块数据列式存储，有利于数据压缩和快速的列存取。</strong></p><ul><li>rcfile 存储空间最小</li><li>查询的效率最高</li><li>需要通过text文件转化来加载</li><li>加载的速度最低</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test3(<span class="keyword">str</span> <span class="keyword">STRING</span>)  <span class="keyword">STORED</span> <span class="keyword">AS</span> RCFILE;  </span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> hive.exec.compress.output=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> mapred.output.compress=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> mapred.output.compression.codec=com.hadoop.compression.lzo.LzoCodec;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> io.compression.codecs=com.hadoop.compression.lzo.LzoCodec;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> test3 <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> test1;</span><br></pre></td></tr></table></figure><p><strong>注意点:  RCFile 也只能从其它表导入数据</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (db2)&gt; dfs -cat /user/hive/warehouse/db2.db/t3/000000_0;</span><br><span class="line">RCF*org.apache.hadoop.io.compress.DefaultCodechive.io.rcfile.column.number1</span><br></pre></td></tr></table></figure><h3 id="d-使用ORC存储-最好的一种格式"><a href="#d-使用ORC存储-最好的一种格式" class="headerlink" title="d) 使用ORC存储(最好的一种格式)"></a>d) 使用ORC存储(最好的一种格式)</h3><p><strong>是一种针对 RCFile 优化的格式</strong></p><p>主要特点: <strong>压缩, 索引, 单文件输出</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-15-070816.jpg" alt=""></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t1_orc(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span> <span class="keyword">stored</span> <span class="keyword">as</span> orc </span><br><span class="line"></span><br><span class="line">tblproperties(<span class="string">"orc.compress"</span>=<span class="string">"ZLIB"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> ... [<span class="keyword">PARTITION</span> partition_spec] <span class="keyword">SET</span> FILEFORMAT ORC;</span><br><span class="line"></span><br><span class="line"># 也可以改为其它的, 修改的语法就是这样</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> t1 <span class="keyword">set</span> fileformat textfile;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SET</span> hive.default.fileformat=Orc;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> t1_orc <span class="keyword">select</span> * <span class="keyword">from</span> t1;</span><br></pre></td></tr></table></figure><p><strong>注意点:  </strong></p><ul><li><strong>ORC 也只能从其它表导入数据</strong></li><li><strong>占用空间大, 一个 block 有256M, 之前2种都是128M</strong></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive (db2)&gt; dfs -cat /user/hive/warehouse/db2.db/t4/000000_0;</span><br><span class="line">ORC</span><br><span class="line">P+</span><br><span class="line">P�6�b�``���ь@�H�</span><br><span class="line">                       1q01-</span><br><span class="line">P</span><br><span class="line">PK</span><br><span class="line"># ("</span><br><span class="line">       id0P:</span><br><span class="line">P@�;��"</span><br><span class="line">       (0��ORChive</span><br></pre></td></tr></table></figure><h1 id="12-序列化-amp-反序列化-Hive-SerDe"><a href="#12-序列化-amp-反序列化-Hive-SerDe" class="headerlink" title="12) 序列化 &amp; 反序列化 (Hive SerDe)"></a>12) 序列化 &amp; 反序列化 (Hive SerDe)</h1><h3 id="SerDe"><a href="#SerDe" class="headerlink" title="SerDe"></a>SerDe</h3><ul><li><strong>What is a SerDe?</strong><ul><li>SerDe 是 “Serializer and Deserializer.”的缩写</li><li>Hive 使用 SerDe和FileFormat进行行内容的读写.</li></ul></li><li><strong>Hive序列化流程</strong><ul><li><strong>从 HDFS 上读入文件 (反序列化)</strong><ul><li><code>HDFS文件 --&gt; InputFileFormat --&gt; &lt;key, value&gt; --&gt; Deserializer --&gt; 行对象</code></li></ul></li><li><strong>写出到 HDFS (序列化)</strong><ul><li><code>行对象 --&gt; Serializer --&gt; &lt;key, value&gt; --&gt; OutputFileFormat --&gt; HDFS文件</code></li></ul></li><li>注意: <strong>数据全部存在在value中，key内容无意义</strong></li></ul></li></ul><ul><li><p><strong>Hive 使用如下FileFormat 类读写 HDFS files:</strong></p><ul><li>TextInputFormat/HiveIgnoreKeyTextOutputFormat: 读写普通HDFS文本文件.</li><li>SequenceFileInputFormat/SequenceFileOutputFormat: 读写SequenceFile格式的HDFS文件</li><li>….</li></ul></li></ul><ul><li><p><strong>Hive 使用如下SerDe 类(反)序列化数据:</strong></p><ul><li>MetadataTypedColumnsetSerDe: 读写csv、tsv文件和默认格式文件</li><li>ThriftSerDe: 读写Thrift 序列化后的对象.</li><li>DynamicSerDe: 读写Thrift序列化后的对象, 不过不需要解读schema中的ddl.</li></ul></li></ul><h3 id="使用CSV-Serde"><a href="#使用CSV-Serde" class="headerlink" title="使用CSV Serde"></a>使用CSV Serde</h3><p>CSV格式的文件也称为逗号分隔值（Comma-Separated Values，CSV，有时也称为字符分隔值，因为分隔字符也可以不是逗号。在本文中的CSV格式的数据就不是简单的逗号分割的），其文件以纯文本形式存储表格数据（数字和文本）。CSV文件由任意数目的记录组成，记录间以某种换行符分隔；每条记录由字段组成，字段间的分隔符是其它字符或字符串，最常见的是逗号或制表符。通常，所有记录都有完全相同的字段序列。<br>默认的分隔符是</p><blockquote><p>DEFAULT_ESCAPE_CHARACTER \<br>DEFAULT_QUOTE_CHARACTER  “     —如果没有，则不需要指定<br>DEFAULT_SEPARATOR  , </p></blockquote><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> csv_table(a <span class="keyword">string</span>, b <span class="keyword">string</span>) <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> SERDE <span class="string">'org.apache.hadoop.hive.serde2.OpenCSVSerde'</span> <span class="keyword">WITH</span> SERDEPROPERTIES (<span class="string">"separatorChar"</span>=<span class="string">"\t"</span>, <span class="string">"quoteChar"</span>=<span class="string">"'"</span>, <span class="string">"escapeChar"</span>=<span class="string">"\\"</span>)  <span class="keyword">STORED</span> <span class="keyword">AS</span> TEXTFILE;</span><br><span class="line"></span><br><span class="line"># separatorChar：分隔符</span><br><span class="line"># quoteChar：引号符</span><br><span class="line"># escapeChar：转义符</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt;&gt;  :TODO 创建表没成功, 用到时再说</span><br></pre></td></tr></table></figure><h1 id="13-Lateral-View-语法"><a href="#13-Lateral-View-语法" class="headerlink" title="13) Lateral View 语法"></a>13) Lateral View 语法</h1><p>lateral view用于和split, explode等UDTF一起使用，它能够<strong>将一行数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合</strong>。lateral view首先为原始表的每行调用UDTF，UTDF会把一行拆分成一或者多行，lateral view再把结果组合，产生一个<strong>支持别名表的虚拟表</strong>。</p><p><strong>lateral: 侧面, 横切面</strong></p><p><strong>Lateral View: 切面表</strong></p><ul><li><p>创建表</p><ul><li><strong>create table t8(name string,nums array\&lt;int>)row format delimited fields terminated by “\t” COLLECTION ITEMS TERMINATED BY ‘:’;</strong></li></ul></li><li><p>数据切割</p><ul><li><strong>SELECT name,new_num FROM t8 LATERAL VIEW explode(nums) num AS new_num;</strong></li><li><code>select name,id from class_test lateral view explode(student_id_list) list as id;</code></li><li><strong>注意: as 前面的 <code>list</code> 貌似是可以随表起名的</strong></li></ul></li><li><p>效果演示</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from class_test;</span><br><span class="line">OK</span><br><span class="line">class_test.nameclass_test.student_id_list</span><br><span class="line">zhangsan[24,25,27,37]</span><br><span class="line">lisi[28,39,23,43]</span><br><span class="line">wangwu[25,23,2,54]</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">hive (default)&gt; select name,id from class_test lateral view explode(student_id_list) list as id;</span><br><span class="line">OK</span><br><span class="line">nameid</span><br><span class="line">zhangsan24</span><br><span class="line">zhangsan25</span><br><span class="line">zhangsan27</span><br><span class="line">zhangsan37</span><br><span class="line">lisi28</span><br><span class="line">lisi39</span><br><span class="line">lisi23</span><br><span class="line">lisi43</span><br><span class="line">wangwu25</span><br><span class="line">wangwu23</span><br><span class="line">wangwu2</span><br><span class="line">wangwu54</span><br></pre></td></tr></table></figure></li></ul><h1 id="14-Hive的高级函数"><a href="#14-Hive的高级函数" class="headerlink" title="14) Hive的高级函数"></a>14) Hive的高级函数</h1><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><ul><li>简单查询<ul><li>select … from…where…</li></ul></li><li><strong>使用各种函数</strong><ul><li><strong>hive&gt;show functions;</strong><ul><li>展示所有函数</li></ul></li><li><strong>hive&gt;describe function  xxx;</strong><ul><li>详细描述函数用法</li></ul></li></ul></li><li>LIMIT语句</li><li>列别名</li><li>嵌套select语句</li></ul><h3 id="高级函数分类"><a href="#高级函数分类" class="headerlink" title="高级函数分类"></a>高级函数分类</h3><ul><li><p>标准函数</p><ul><li>reverse()</li><li>upper()</li></ul></li><li><p>聚合函数</p><ul><li><p>avg()</p></li><li><p>sum()</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">avg</span>(<span class="keyword">id</span>) <span class="keyword">from</span> t3;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">sum</span>(<span class="keyword">id</span>) <span class="keyword">from</span> t3;</span><br><span class="line"></span><br><span class="line"># 最简答的嵌套</span><br><span class="line">hive (mydb)&gt; select t.id from (select * from t1 where id &gt; 3)t;</span><br><span class="line"></span><br><span class="line"># 最简单的 group by</span><br><span class="line"></span><br><span class="line"># if else 的效果</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">case</span> <span class="keyword">when</span> <span class="keyword">id</span> &lt;= <span class="number">2</span> <span class="keyword">then</span> <span class="string">'low'</span></span><br><span class="line"><span class="keyword">when</span> <span class="keyword">id</span>&gt;=<span class="number">3</span> <span class="keyword">and</span> <span class="keyword">id</span> &lt;<span class="number">4</span> <span class="keyword">then</span> <span class="string">'middle'</span></span><br><span class="line"><span class="keyword">when</span> <span class="keyword">id</span>&gt;=<span class="number">4</span> <span class="keyword">and</span> <span class="keyword">id</span> &lt;<span class="number">5</span> <span class="keyword">then</span> <span class="string">'high'</span></span><br><span class="line"><span class="keyword">else</span> <span class="string">'very high'</span></span><br><span class="line"><span class="keyword">end</span> <span class="keyword">as</span> id_highly <span class="keyword">from</span> t1;</span><br><span class="line"><span class="comment">----执行结果----</span></span><br><span class="line">idid_highly</span><br><span class="line">1low</span><br><span class="line">2low</span><br><span class="line">3middle</span><br><span class="line">4high</span><br><span class="line">5very high</span><br><span class="line"></span><br><span class="line"># cast 转换函数, 大概是这么用</span><br><span class="line">hive (mydb)&gt; select id from t1 where cast(id AS FLOAT) &lt;3.0;</span><br><span class="line">OK</span><br><span class="line">id</span><br><span class="line">1</span><br><span class="line">2</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p>自定义函数</p><ul><li>UDF</li></ul></li></ul><h1 id="15-Hive-性能调优"><a href="#15-Hive-性能调优" class="headerlink" title="15) Hive 性能调优"></a>15) Hive 性能调优</h1><h3 id="a-什么时候可以避免执行MapReduce？"><a href="#a-什么时候可以避免执行MapReduce？" class="headerlink" title="a) 什么时候可以避免执行MapReduce？"></a>a) 什么时候可以避免执行MapReduce？</h3><ul><li><p><code>select *  or  select field1,field2</code></p></li><li><p><code>limite 10</code></p></li><li><p>where语句中只有分区字段</p></li><li><p>使用本地<code>set hive.exec.mode.local.auto=true;</code></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t3;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,age <span class="keyword">from</span> t3;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,age <span class="keyword">from</span> t3 <span class="keyword">limit</span> <span class="number">2</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,age <span class="keyword">from</span> t3 <span class="keyword">where</span> age=<span class="number">25</span>;</span><br><span class="line"># 当 where 是本地字段(列中的字段), 是不走 MR的</span><br></pre></td></tr></table></figure></li><li><p>group by语句：</p><ul><li>通常和聚合函数一起使用，按照一个或者多个列对结果进行分组，然后对每组执行聚合操作</li></ul></li><li>having语句：<ul><li>限制结果的输出</li></ul></li><li>hive将查询转化为MapReduce执行，hive的优化可以转化为mapreduce的优化！</li></ul><h3 id="b-hive是如何将查询转化为MapReduce的？-EXPLAIN的使用"><a href="#b-hive是如何将查询转化为MapReduce的？-EXPLAIN的使用" class="headerlink" title="b) hive是如何将查询转化为MapReduce的？-EXPLAIN的使用"></a><strong>b) hive是如何将查询转化为MapReduce的？-EXPLAIN的使用</strong></h3><ul><li>hive对sql的查询计划信息解析</li><li><p>EXPLAIN SELECT COUNT(1) FROM T1;</p></li><li><p><strong>EXPLAIN EXTENDED</strong> </p><ul><li><p><strong>显示详细扩展查询计划信息</strong></p></li><li><p><strong><code>EXPLAIN EXTENDED SELECT COUNT(1) FROM T1;</code></strong></p></li><li><p>为啥我的 explain extended 只有固定的几行?</p><ul><li><strong>因为这个 count 没有调动 MR, 用 sum 就会启用 MR, 会出现长长的 log</strong></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">---不会启用 MR</span></span><br><span class="line">hive (mydb)&gt; explain EXTENDED select count(1) from t9;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line"><span class="keyword">Explain</span></span><br><span class="line">STAGE DEPENDENCIES:</span><br><span class="line">  Stage<span class="number">-0</span> <span class="keyword">is</span> a root stage</span><br><span class="line">  ...</span><br><span class="line"><span class="comment">-----------------------------</span></span><br><span class="line"><span class="comment">---这里会启用 MR</span></span><br><span class="line">hive (mydb)&gt; <span class="keyword">explain</span> <span class="keyword">EXTENDED</span> <span class="keyword">select</span> <span class="keyword">sum</span>(<span class="keyword">id</span>) <span class="keyword">from</span> t9;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line"><span class="keyword">Explain</span></span><br><span class="line">STAGE DEPENDENCIES:</span><br><span class="line">  Stage<span class="number">-1</span> <span class="keyword">is</span> a root stage</span><br><span class="line">  Stage<span class="number">-0</span> depends <span class="keyword">on</span> stages: Stage<span class="number">-1</span></span><br><span class="line"></span><br><span class="line">STAGE PLANS:</span><br><span class="line">  Stage: Stage<span class="number">-1</span></span><br><span class="line">  ....</span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="c-性能调优"><a href="#c-性能调优" class="headerlink" title="c) 性能调优"></a>c) 性能调优</h3><h4 id="①-本地mr"><a href="#①-本地mr" class="headerlink" title="① 本地mr"></a>① 本地mr</h4><ul><li><p><strong>本地模式设置方式：</strong></p><ul><li><code>set mapred.job.tracker=local;</code><ul><li>似乎不起作用? :TODO</li></ul></li><li><strong><code>set hive.exec.mode.local.auto=true;</code></strong><ul><li><strong>设置这里才会转成 local  hadoop</strong></li><li>按照这里设定的规则<code>hive.exec.mode.local.auto.input.files.max</code></li></ul></li><li>测试 select 1 from wlan limit 5;</li></ul></li><li><p><strong>下面两个参数是local mr中常用的控制参数:</strong></p><ul><li><p><code>hive.exec.mode.local.auto.inputbytes.max</code>默认134217728</p><ul><li><strong>设置local mr的最大输入数据量</strong>,当输入数据量小于这个值的时候会采用local  mr的方式</li></ul></li><li><p><strong><code>hive.exec.mode.local.auto.input.files.max</code>默认是4</strong></p><ul><li><p>设置local mr的最大输入文件个数,当输入文件个数小于这个值的时候会采用local mr的方式</p></li><li><p>大于此数时, 就不会转化为 local hadoop</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Cannot run job locally: Number of Input Files (= 6) is larger than hive.exec.mode.local.auto.input.files.max(= 4)</span><br></pre></td></tr></table></figure></li><li><p><strong>可以这样修改local mr的最大输入文件个数值, 主要在调试阶段使用</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (mydb)&gt; set hive.exec.mode.local.auto.input.files.max=8;</span><br><span class="line"></span><br><span class="line"># 这样设置了之后, 只要文件数&lt;=8, 就会在本地运行</span><br><span class="line">Job running in-process (local Hadoop)</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul><h4 id="②-开启并行计算"><a href="#②-开启并行计算" class="headerlink" title="② 开启并行计算"></a>② 开启并行计算</h4><ul><li>开启并行计算,增加集群的利用率<ul><li><code>set hive.exec.parallel=true;</code></li></ul></li></ul><h4 id="③-设置严格模式"><a href="#③-设置严格模式" class="headerlink" title="③ 设置严格模式"></a>③ 设置严格模式</h4><ul><li><strong>设置严格模式</strong><ul><li><code>set hive.mapred.mode=strict;</code></li></ul></li><li>设置非严格模式<ul><li><code>set hive.mapred.mode=nostrict;</code></li></ul></li><li><strong>strict可以禁止三种类型的查询：</strong><ul><li>强制分区表的where条件过滤</li><li>Order by语句必须使用limit<ul><li><code>hive (mydb)&gt; select id from t9 where class=&#39;job110&#39; order by id limit 3;</code></li></ul></li><li>限制笛卡尔积查询</li></ul></li></ul><h4 id="④-调整mapper和reducer的数量"><a href="#④-调整mapper和reducer的数量" class="headerlink" title="④ 调整mapper和reducer的数量"></a>④ 调整mapper和reducer的数量</h4><ul><li>调整mapper和reducer的数量<ul><li>太多map导致启动产生过多开销</li><li><code>marpred.min.split.size</code></li><li>按照输入数据量大小确定reducer数目,<ul><li><code>set mapred.reduce.tasks=  默认3</code></li><li>dfs -count  /分区目录/* </li><li>hive.exec.reducers.max设置阻止资源过度消耗</li></ul></li></ul></li><li>JVM重用<ul><li>小文件多或task多的业务场景</li><li><code>set mapred.job.reuse.jvm.num.task=10</code></li><li>会一直占用task槽</li></ul></li></ul><h4 id="⑤-排序方面的优化"><a href="#⑤-排序方面的优化" class="headerlink" title="⑤ 排序方面的优化"></a>⑤ 排序方面的优化</h4><ul><li><p><strong>order by</strong> 语句：      是<strong>全局</strong>排序</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 加个 desc 就是倒序排序</span><br><span class="line">hive (mydb)&gt; select id from bucket_table order by id desc limit 10;</span><br><span class="line">Automatically selecting local only mode for query</span><br></pre></td></tr></table></figure></li><li><p><strong>sort by</strong> 语句：      是<strong>单reduce</strong>排序</p><ul><li>用的比较少</li></ul></li><li><p><strong>distribute by</strong>语句：是<strong>分区字段</strong>排序;</p><ul><li><p>与 sort by 结合用的比较多</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (mydb)&gt; select id from bucket_table distribute by id sort by id desc limit 10;</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p><strong>cluster by</strong>语句：</p><ul><li>可以确保类似的数据的分发到同一个reduce task中，并且保证数据有序防止所有的数据分发到同一个reduce上，导致整体的job时间延长</li></ul></li><li><p><strong>cluster by语句的等价语句：</strong></p><ul><li>distribute by Word sort by Word ASC</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Hive-初探&quot;&gt;&lt;a href=&quot;#1-Hive-初探&quot; class=&quot;headerlink&quot; title=&quot;1)  Hive 初探&quot;&gt;&lt;/a&gt;1)  Hive 初探&lt;/h1&gt;&lt;h3 id=&quot;Hive-的数据存储&quot;&gt;&lt;a href=&quot;#Hive-的数据存储&quot;
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/categories/Hadoop/Hive/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/tags/Hive/"/>
    
      <category term="crxy" scheme="https://airpoet.github.io/tags/crxy/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce-简单总结</title>
    <link href="https://airpoet.github.io/2018/06/12/Hadoop/Study/2-MapReduce/MapReduce-%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93/"/>
    <id>https://airpoet.github.io/2018/06/12/Hadoop/Study/2-MapReduce/MapReduce-简单总结/</id>
    <published>2018-06-12T07:03:29.967Z</published>
    <updated>2018-06-12T07:05:09.710Z</updated>
    
    <content type="html"><![CDATA[<p>MapReduce在编程的时候，基本上一个固化的模式，没有太多可灵活改变的地方，<strong>除了以下几处</strong>：</p><p>1、输入数据接口：InputFormat   —&gt;     FileInputFormat(文件类型数据读取的通用抽象类)  DBInputFormat （数据库数据读取的通用抽象类）<br>   默认使用的实现类是： TextInputFormat     job.setInputFormatClass(TextInputFormat.class)<br>   TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回</p><p>2、逻辑处理接口： Mapper<br>   完全需要用户自己去实现其中  map()   setup()   clean()   </p><p>3、map输出的结果在shuffle阶段会被partition以及sort，此处有两个接口可自定义：<br>     Partitioner<br>        有默认实现 HashPartitioner，逻辑是  根据key和numReduces来返回一个分区号； key.hashCode()&amp;Integer.MAXVALUE % numReduces<br>    通常情况下，用默认的这个HashPartitioner就可以，如果业务上有特别的需求，可以自定义</p><pre><code>Comparable   当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，override其中的compareTo()方法</code></pre><p>4、reduce端的数据分组比较接口 ： Groupingcomparator<br>    reduceTask拿到输入数据（一个partition的所有数据）后，首先需要对数据进行分组，其分组的默认原则是key相同，然后对每一组kv数据调用一次reduce()方法，并且将这一组kv中的第一个kv的key作为参数传给reduce的key，将这一组数据的value的迭代器传给reduce()的values参数</p><pre><code>利用上述这个机制，我们可以实现一个高效的分组取最大值的逻辑：自定义一个bean对象用来封装我们的数据，然后改写其compareTo方法产生倒序排序的效果然后自定义一个Groupingcomparator，将bean对象的分组逻辑改成按照我们的业务分组id来分组（比如订单号）这样，我们要取的最大值就是reduce()方法中传进来key</code></pre><p>5、逻辑处理接口：Reducer<br>    完全需要用户自己去实现其中  reduce()   setup()   clean()   </p><p>6、输出数据接口： OutputFormat  —&gt; 有一系列子类  FileOutputformat  DBoutputFormat  …..<br>    默认实现类是TextOutputFormat，功能逻辑是：  将每一个KV对向目标文本文件中输出为一行</p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-12-070505.png" alt="image-20180612150505148">    </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;MapReduce在编程的时候，基本上一个固化的模式，没有太多可灵活改变的地方，&lt;strong&gt;除了以下几处&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;1、输入数据接口：InputFormat   —&amp;gt;     FileInputFormat(文件类型数据读取的通用抽象类) 
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/"/>
    
      <category term="学习笔记" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>Hive学习-1</title>
    <link href="https://airpoet.github.io/2018/06/12/Hadoop/Study/3-Hive/Hive%E5%AD%A6%E4%B9%A0-1/"/>
    <id>https://airpoet.github.io/2018/06/12/Hadoop/Study/3-Hive/Hive学习-1/</id>
    <published>2018-06-12T03:24:46.733Z</published>
    <updated>2018-06-15T10:40:47.161Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hive简介"><a href="#Hive简介" class="headerlink" title="Hive简介"></a>Hive简介</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-12-034055.png" alt="image-20180612114055129"></p><h3 id="Hive基本情况"><a href="#Hive基本情况" class="headerlink" title="Hive基本情况"></a>Hive基本情况</h3><p><strong>Hive</strong> 是建立在 <strong>Hadoop</strong> 上的数据仓库基础构架</p><ul><li>由facebook开源，最初用于解决海量结构化的日志数据统 计问题;<ul><li><strong>ETL</strong>(<strong>Extraction-Transformation-Loading</strong>)工具</li></ul></li><li>构建在Hadoop之上的数据仓库;<ul><li>数据计算使用<strong>MR</strong>，数据存储使用<strong>HDFS</strong></li><li>数据库&amp;数据仓库 的区别：<ul><li>概念上<ul><li><strong>数据库</strong>：用于管理精细化数据，一般情况下用于存储结果数据，分库分表进行存储</li><li><strong>数据仓库</strong>：<strong>存储、查询、分析大规模数据</strong> 。更像一个打包的过程，里面存储的数据没有细化区分，粒度较粗</li></ul></li><li>用途上：<ul><li>数据库：OLTP，on line Transation Processing 联机事务处理，增删改</li><li>数据仓库：OLAP，on line analysis Processing 联机事务分析处理，查询，hive不支持删除、修改。 支持插入。</li></ul></li><li>使用上：<ul><li>数据库：标准sql,  hbase: 非标准sql</li><li>数据仓库：方言版的sql， HQL</li></ul></li><li>模式上：<ul><li>数据库：写模式</li><li>数据仓库：读模式</li></ul></li></ul></li></ul></li><li>可以将结构化的数据映射成一张数据库表<ul><li>结构化数据映射成二维表</li><li><strong>将文本中每一行数据映射为数据库的每一条数据</strong></li><li><strong>将文本中每一列数据映射为hive的表字段</strong></li></ul></li><li>提供HQL 查询功能<ul><li>hive query language， 方言版sql</li></ul></li><li><strong>底层数据是存储在HDFS上</strong><ul><li>hive上建的表仅仅相当于<strong>对hdfs上的结构化数据进行映射管理</strong></li><li>hive仅仅是一个管理数据的作用，而<strong>不会存储数据</strong></li><li>hive想要管理hdfs上的数据，就要建立一个关联关系，关联hive上的表和hdfs上的数据路径</li><li>数据是依赖于一个元数据库</li><li>元数据库采用的是关系型数据库， 真实生产中一般使用mysql为hive的元数据库，hive内置默认的元数据库是 derby</li><li><strong>元数据：</strong> <strong>HCalalog</strong> <ul><li>hive中的表和hdfs的<strong>映射关系</strong>，以及<strong>hive表属性</strong>（内部表，外部表，视图）和<strong>字段信息</strong></li><li>元数据一旦修饰，hive的所有映射关系等都没了，就无法使用了</li><li>可与<strong>Pig</strong>、<strong>Presto</strong>等共享 </li></ul></li></ul></li><li>Hive的是<strong>构建在Hadoop之上的数据仓库</strong><ul><li>数据计算使用<strong>MR</strong>，数据存储使用<strong>HDFS</strong> </li></ul></li><li>通常用于进行离线数据处理(采用MapReduce) </li><li><strong>可认为是一个HQL—-&gt;MR的语言翻译器</strong> </li></ul><h3 id="Hive优缺点："><a href="#Hive优缺点：" class="headerlink" title="Hive优缺点："></a>Hive优缺点：</h3><ul><li><p>优点：</p><ul><li>简单，容易上手 <ul><li>提供了类<strong>SQL</strong>查询语言<strong>HQL</strong> </li></ul></li><li>为超大数据集设计的计算/扩展能力 <ul><li><strong>MR</strong>作为计算引擎，<strong>HDFS</strong>作为存储系统 </li></ul></li><li>统一的元数据管理(HCalalog) <ul><li>可与<strong>Pig</strong>、<strong>Presto</strong>等共享 </li></ul></li></ul></li><li><p><strong>缺点</strong></p><ul><li><p><strong>不支持 删除 &amp; 修改</strong>  delete&amp;update，<strong>不支持事务</strong></p><ul><li>因为是基于HDFS</li><li>hive做的最多的是查询</li></ul></li><li><p>Hive的HQL表达的能力有限 </p><ul><li>迭代式算法无法表达 </li><li>有些复杂运算用<strong>HQL</strong>不易表达 </li></ul></li><li><p><strong>Hive效率较低，查询延时高</strong></p><ul><li><strong>Hive</strong>自动生成<strong>MapReduce</strong>作业，通常不够智能 </li><li><strong>HQL</strong>调优困难，粒度较粗 </li><li>可控性差 </li></ul></li></ul></li></ul><h3 id="Hive与传统关系型数据库-RDBMS）对比"><a href="#Hive与传统关系型数据库-RDBMS）对比" class="headerlink" title="Hive与传统关系型数据库(RDBMS）对比"></a>Hive与传统关系型数据库(RDBMS）对比</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-12-082659.png" alt="image-20180612162659151"><br></p><h1 id="Hive的架构"><a href="#Hive的架构" class="headerlink" title="Hive的架构"></a>Hive的架构</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-12-061321.png" alt="image-20180612141321130"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-12-061413.png" alt="image-20180612141413285"></p><ul><li><p><strong>1) 用户接口</strong></p><ul><li><p>CLI：Command Line Interface，即Shell终端命令行，使用 Hive 命令行与 Hive 进行交互，最常用（学习，调试，生产），包括两种运行方式：</p><ul><li><p>hive命令方式：前提必须在hive安装节点上执行</p></li><li><p>hiveserver2方式：hive安装节点将hive启动为一个后台进程，客户机进行连接（类似于启动了一个hive服务端）</p></li></ul></li></ul></li></ul><pre><code>真实生产中常用！- 1) 修改配置文件，允许远程连接;  第一：修改 hdfs-site.xml，加入一条配置信息，启用 webhdfs；  第二：修改 core-site.xml，加入两条配置信息，设置 hadoop的代理用户。- 2) 启动服务进程  - 前台启动：hiveserver2  - 后台启动    - 记录日志：nohup hiveserver2 1&gt;/home/sigeon/hiveserver.log 2&gt;/home/sigeon/hiveserver.err &amp;      0：标准日志输入   1：标准日志输出   2：错误日志输出      如果我没有配置日志的输出路径，日志会生成在当前工作目录，默认的日志名称叫做：      nohup.xxx    - 不记录日志：nohup hiveserver2 1&gt;/dev/null 2&gt;/dev/null &amp;     - [补充：]      - nohup命令：no hang up的缩写，即不挂起，可以在你退出帐户/关闭终端之后继续运行相应的进程。      - 语法：nohup &lt;command&gt; &amp;- 3) 开启beenline客户端并连接：  - 方法一：     - beenline，开启beenline客户端;    - !connect jdbc:&lt;hive2://master:10000&gt;，回车。然后输入用户名和密码，这个用户名是安装 hadoop 集群的用户名  - 方法二：    - beeline -u jdbc:&lt;hive2://master:10000&gt; -n sigeon</code></pre><ul><li><p>JDBC/ODBC：是 Hive 的基于 JDBC 操作提供的客户端，用户（开发员，运维人员）通过 这连接至 Hive server 服务 </p></li><li><p>Web UI：通过浏览器访问 Hive，基本不会使用</p></li></ul><ul><li><p>2) 元数据库：保存元数据，一般会选用关系型数据库（如mysql，Hive 和 MySQL 之间通过 MetaStore 服务交互）</p></li><li><p>3) Thrift服务：Facebook 开发的一个软件框架，可以用来进行可扩展且跨语言的服务的开发， Hive 集成了该服务，能让不同的编程语言调用 Hive 的接口</p></li><li><p>4) 驱动Driver</p><ul><li>a. 解释器：解释器的作用是将 HiveSQL 语句转换为抽象语法树（AST） </li><li>b. 编译器：编译器是将语法树编译为逻辑执行计划 </li><li>c. 优化器：优化器是对逻辑执行计划进行优化 </li><li>d. 执行器：执行器是调用底层的运行框架执行逻辑执行计划 </li></ul></li></ul><p><br></p><h1 id="Hive的数据组织格式"><a href="#Hive的数据组织格式" class="headerlink" title="Hive的数据组织格式"></a>Hive的数据组织格式</h1><ul><li><p>1)库：database</p></li><li><p>2) 表</p><ul><li><p>a. 内部表（管理表：managed_table）</p></li><li><p>b. 外部表（external_table）</p></li><li><p>内部表和外部表区别：</p><ul><li>内部表和外部表是两个相对的概念，不可能有一个表同时是内部表又是外部表；</li><li>内部表删除表的时候会删除原始数据和元数据，而外部表删除表的时候只会删除元数据不会删除原始数据</li><li>一般情况下存储公共数据的表存放为外部表</li><li>大多数情况，他们的区别不明显。如果数据的所有处理都在 Hive 中进行，那么倾向于 选择内部表；但是如果 Hive 和其他工具要针对相同的数据集进行处理，外部表更合适。 </li><li>使用外部表访问存储在 HDFS 上的初始数据，然后通过 Hive 转换数据并存到内部表中，使用外部表的场景是针对一个数据集有多个不同的 Schema。 </li></ul></li><li><p>c. 分区表</p><ul><li>不同于hadoop中的分区，分区表是人为划分的</li><li>hive最终存储海量数据，海量数据查询一定注意避免全表扫描</li><li>查询的时候为了提升我们的查询性能，出现了分区表</li><li>将数据按照用户的业务存储到不同的目录下，在进行数据查询时只会对指定分区下的数据进行扫描<br>一般情况下生产中用日期作为分区字段</li></ul></li><li><p>d. 分桶表</p><ul><li><p>类似于hadoop中的分区，是由程序决定的，只能指定桶的个数（分区的个数）</p></li><li><p>根据hash算法将余数不同的输出到不同的文件中</p></li><li><p>作用：</p><ul><li>1）提升join的性能<br>思考这个问题：select <a href="http://a.id" target="_blank" rel="noopener">a.id</a>,<a href="http://a.name" target="_blank" rel="noopener">a.name</a>,b.addr from a join b on <a href="http://a.id" target="_blank" rel="noopener">a.id</a> = <a href="http://b.id" target="_blank" rel="noopener">b.id</a>;如果 a 表和 b 表已经是 分桶表，而且分桶的字段是 id 字段做这个 join 操作时，还需要全表做笛卡尔积</li><li>2）提升数据样本的抽取效率，直接拿一个桶中的数据作为样本数据</li></ul></li><li><p>分区表和分桶表的区别： </p><p><img src="/var/folders/6l/blvdbwd53hqglz0f09n3sd5c0000gn/T/abnerworks.Typora/image-20180612143606401.png" alt="image-20180612143606401"></p><ul><li>Hive 数据表可以根据某些字段进行分区操作，细化数据管理，可以让部分查询更快。</li><li>同时表和分区也可以进一步被划分为 Buckets，分桶表的原理和 MapReduce 编程中的 HashPartitioner 的原理类似 </li><li>分区和分桶都是细化数据管理，但是分区表是手动添加区分，由于 Hive 是读模式，所以对添加进分区的数据不做模式校验</li><li>分桶表中的数据是按照某些分桶字段进行 hash 散列 形成的多个文件，所以数据的准确性也高很多</li></ul></li></ul></li></ul></li></ul><p><br></p><ul><li><p><strong>3)视图：</strong></p><ul><li>hive中的视图仅仅相当于一个sql语句的别名</li><li><strong>在hive中仅仅存在逻辑视图，不存在物理视图</strong><ul><li>物理视图：讲sql语句的执行结果存在视图中</li><li>逻辑视图： <strong>仅仅是对查询结果的引用</strong></li></ul></li></ul></li><li><p><strong>4)数据存储：</strong></p><ul><li>原始数据中存在HDFS</li><li>元数据存在mysql</li></ul></li></ul><p><br></p><h1 id="Hive安装"><a href="#Hive安装" class="headerlink" title="Hive安装"></a>Hive安装</h1><p><strong>装hive其实不难，主要是安装mysql，解决mysql的权限问题</strong></p><h2 id="MySql安装"><a href="#MySql安装" class="headerlink" title="MySql安装"></a>MySql安装</h2><h3 id="RPM-安装MySQl："><a href="#RPM-安装MySQl：" class="headerlink" title="RPM 安装MySQl："></a>RPM 安装MySQl：</h3><ol><li><p>检查以前是否装过 MySQL </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -qa|grep -i mysql</span><br></pre></td></tr></table></figure></li><li><p>发现有的话就都卸载 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rpm -e --nodeps mysql-libs-5.1.73-5.el6_6.x86_64</span><br><span class="line">rpm -e --nodeps ....</span><br></pre></td></tr></table></figure></li><li><p>删除老版本 mysql 的开发头文件和库 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rm -rf /usr/lib64/mysql</span><br><span class="line"><span class="comment"># 在搜索 my.cnf 文件，有的话就删掉</span></span><br></pre></td></tr></table></figure></li><li><p>上传mysql 安装包到 Linux中，解压</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tar -xvf mysql-5.6.26-1.linux_glibc2.5.x86_64.rpm-bundle.tar</span><br><span class="line"><span class="comment"># 解压出来有这些文件</span></span><br><span class="line">MySQL-devel-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br><span class="line">MySQL-embedded-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br><span class="line">MySQL-server-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br><span class="line">MySQL-shared-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br><span class="line">MySQL-shared-compat-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br><span class="line">MySQL-test-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br></pre></td></tr></table></figure></li><li><p>安装 server &amp; client </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># server</span></span><br><span class="line">rpm -ivh MySQL-server-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br><span class="line"><span class="comment"># client</span></span><br><span class="line">rpm -ivh MySQL-client-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br></pre></td></tr></table></figure></li><li><p>启动Mysql</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service mysql start</span><br></pre></td></tr></table></figure></li><li><p>登录Mysql并改密码，等</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"># 初始密码在这个文件中</span><br><span class="line">cat /root/.mysql_sercert </span><br><span class="line"></span><br><span class="line"># 登录</span><br><span class="line">mysql -uroot -pxxxxx</span><br><span class="line"></span><br><span class="line"># 删除除了`%`之外的其他所有host</span><br><span class="line">use mysql;</span><br><span class="line">select host,user,password from user;</span><br><span class="line">delete from user where host in (&apos;localhost&apos;, &apos;127.0.0.1&apos;,&apos;::1&apos;, ...)</span><br><span class="line"></span><br><span class="line"># 修改密码 </span><br><span class="line">UPDATE user SET Password = PASSWORD(&apos;psd&apos;) WHERE user = &apos;root&apos;;</span><br><span class="line"></span><br><span class="line"># 为`%` 和 `*` 添加远程登录权限 </span><br><span class="line">#注意： 前面的 mysql 登录用户名， 123 是登录密码</span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;psd&apos; WITH GRANT OPTION;</span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;*&apos; IDENTIFIED BY &apos;psd&apos; WITH GRANT OPTION;</span><br><span class="line">FLUSH PRIVILEGES;</span><br><span class="line"></span><br><span class="line"># 退出登录</span><br><span class="line">exit;</span><br><span class="line"></span><br><span class="line"># 此时可以再登录试试</span><br><span class="line">mysql -uroot -ppsd</span><br><span class="line"></span><br><span class="line">======================================</span><br><span class="line"></span><br><span class="line"># 修改字符集为 utf-8</span><br><span class="line"># 新建一个文件</span><br><span class="line">vi /etc/my.cnf  </span><br><span class="line"># 添加以下内容</span><br><span class="line">[client]</span><br><span class="line">default-character-set=utf8</span><br><span class="line"></span><br><span class="line">[mysql]</span><br><span class="line">default-character-set=utf8</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line">character-set-server=utf8</span><br><span class="line"></span><br><span class="line"># 重启mysql</span><br><span class="line">sudo service mysql restart</span><br><span class="line"></span><br><span class="line">======================================</span><br><span class="line"></span><br><span class="line"># 忘记密码，修改密码的方法</span><br><span class="line"># 停止mysql服务的运行</span><br><span class="line">service mysql stop</span><br><span class="line"></span><br><span class="line"># 跳过授权表访问</span><br><span class="line">mysqld_safe --user=mysql --skip-grant-tables --skip-networking &amp; </span><br><span class="line"></span><br><span class="line"># 登录mysql</span><br><span class="line">mysql -u root mysql </span><br><span class="line"></span><br><span class="line"># 接下来可以修改密码了</span><br><span class="line">  ##在mysql5.7以下的版本如下：</span><br><span class="line">mysql&gt; UPDATE user SET Password=PASSWORD(&apos;newpassword&apos;) where USER=&apos;root’；</span><br><span class="line">  ##在mysql5.7版本如下：</span><br><span class="line">update mysql.user set authentication_string=password(&apos;newpassword&apos;) ;</span><br><span class="line"></span><br><span class="line"># 修改完了重启</span><br><span class="line">service mysql restart</span><br><span class="line"></span><br><span class="line">======================================</span><br></pre></td></tr></table></figure></li></ol><h4 id="MySql的其它错误"><a href="#MySql的其它错误" class="headerlink" title="MySql的其它错误"></a>MySql的其它错误</h4><p>在运行<code>schematool -dbType mysql -initSchema</code>手动初始化元数据库的时候</p><ul><li><p>报了一个log4j重复加载的问题</p><ul><li>解决：可以不用理睬</li></ul></li><li><p>链接mysql 密码过期问题 <code>Your password has expired.</code></p><ul><li><p>解决： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">mysql -uroot -p123</span><br><span class="line"></span><br><span class="line">mysql&gt; use mysql</span><br><span class="line">Reading table information for completion of table and column names</span><br><span class="line">You can turn off this feature to get a quicker startup with -A</span><br><span class="line"></span><br><span class="line">Database changed</span><br><span class="line">mysql&gt; select host,user,password_expired from user;</span><br><span class="line">+-----------+------+------------------+</span><br><span class="line">| host      | user | password_expired |</span><br><span class="line">+-----------+------+------------------+</span><br><span class="line">| %         | root | N                |</span><br><span class="line">| cs1       | root | Y                |</span><br><span class="line">| 127.0.0.1 | root | Y                |</span><br><span class="line">| ::1       | root | Y                |</span><br><span class="line">+-----------+------+------------------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br><span class="line">-------------</span><br><span class="line">mysql&gt; update user set password_expired=&apos;N&apos; where host=&apos;::1&apos;;</span><br><span class="line">Query OK, 1 row affected (0.01 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class="line"></span><br><span class="line">mysql&gt; update user set password_expired=&apos;N&apos; where host=&apos;cs1&apos;;</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class="line"></span><br><span class="line">mysql&gt; update user set password_expired=&apos;N&apos; where host=&apos;127.0.0.1&apos;;</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class="line"></span><br><span class="line">mysql&gt; FLUSH PRIVILEGES;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select host,user,password_expired from user;</span><br><span class="line">+-----------+------+------------------+</span><br><span class="line">| host      | user | password_expired |</span><br><span class="line">+-----------+------+------------------+</span><br><span class="line">| %         | root | N                |</span><br><span class="line">| cs1       | root | N                |</span><br><span class="line">| 127.0.0.1 | root | N                |</span><br><span class="line">| ::1       | root | N                |</span><br><span class="line">+-----------+------+------------------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mysql&gt; exit</span><br><span class="line">Bye</span><br><span class="line">[ap@cs1]~/apps/hive% sudo service mysql restart</span><br><span class="line"></span><br><span class="line"># 再重新初始化就好了</span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="Yum安装-Mysql"><a href="#Yum安装-Mysql" class="headerlink" title="Yum安装 Mysql"></a>Yum安装 Mysql</h3><blockquote><p><strong>因为笔者没装过，所以这部分暂且不表</strong></p></blockquote><hr><h2 id="step2-安装Hive"><a href="#step2-安装Hive" class="headerlink" title="step2-安装Hive"></a>step2-安装Hive</h2><p>MySql安装好之后， 安装Hive就很简答了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.上传到Linux</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.解压安装包到安装目录</span></span><br><span class="line">tar -zxvf apache-hive-2.3.2-bin.tar.gz -C ~/apps/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.把MySQL驱动包(mysql-connector-java-5.1.40-bin.jar)放置在hive 的根路径下的 lib 目录，此处是 ~/apps/apache-hive-2.3.2-bin/lib</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.修改配置文件</span></span><br><span class="line"><span class="built_in">cd</span> ~/apps/apache-hive-2.3.2-bin/conf</span><br><span class="line"><span class="comment"># 新建一个 hive-site.xml</span></span><br><span class="line">touch hive-site.xml</span><br><span class="line">vi hive-site.xml</span><br><span class="line"></span><br><span class="line">+++++++++++++++++++++++++++++++++++++++++++添加如下内容++++++++++++</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">&lt;value&gt;jdbc:mysql://localhost:3306/hivedb?createDatabaseIfNotExist=<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">&lt;description&gt;JDBC connect string <span class="keyword">for</span> a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;!-- 注意:如果mysql和hive 不在同一个服务器节点,需要使用mysql节点的 hostname 或 ip --&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">&lt;description&gt;Driver class name <span class="keyword">for</span> a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">&lt;value&gt;root&lt;/value&gt;</span><br><span class="line">&lt;description&gt;username to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">&lt;value&gt;123&lt;/value&gt;</span><br><span class="line">&lt;description&gt;password to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line">+++++++++++++++++++++++++++++++++++++++++++添加如下内容+++++++++++</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">&lt;description&gt;hive default warehouse, <span class="keyword">if</span> nessecory, change it</span><br><span class="line"></span><br><span class="line">&lt;!-- 这个是配置hive在HDFS上 db 的路存储径的，不配默认默认就是上述路径 --&gt;</span><br><span class="line">&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">++++++++++++++++++++++++++</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.配置环境变量 &amp; source</span></span><br><span class="line"><span class="comment">## 注意：ap是我的用户家目录，换上自己的用户家目录</span></span><br><span class="line"><span class="built_in">export</span> HIVE_HOME=/home/ap/apps/apache-hive-2.3.2-bin </span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HIVE_HOME</span>/bin</span><br><span class="line"><span class="comment"># 用bash的找 .bash_profile， 配置所有环境变量</span></span><br><span class="line"><span class="built_in">source</span> ~/.zshrc  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 6.此时基本安装完成了，验证Hive安装</span></span><br><span class="line">hive --helo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7.重点来了！ 初始化元数据库</span></span><br><span class="line">chematool -dbType mysql -initSchema</span><br><span class="line">&gt; 这里可能会遇到很多错误！！ </span><br><span class="line">&gt; 但是如果前面按照我的方法装的, 应该就问题不大了</span><br><span class="line">&gt; 主要是 mysql 连接权限的问题！！</span><br><span class="line"><span class="comment"># 一定会出现的2个</span></span><br><span class="line">1&gt; 找不到 hive命令的一长串环境变量 (不用理会)</span><br><span class="line">2&gt; 两个log4j，jar包重复的问题 (不用理会)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 8.初始化完成后，可以看下数据库中有没有 hivedb 这个库，成功的话是会有的</span></span><br><span class="line"><span class="comment">## 启动hive</span></span><br><span class="line">hive --service cli  </span><br><span class="line">&gt;hive: show databases;</span><br><span class="line"><span class="comment"># 如果能显示数据库，就没啥问题了</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ❤️9.Hive的使用方式之  HiveServer2/beeline</span></span><br><span class="line">此处需要修改 hadoop 的配置文件</span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.1.首先关闭hdfs &amp; yarn服务 &amp; RunJar(hive服务)，修改hadoop配置文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.2.修改 hadoop 集群的 hdfs-site.xml 配置文件:加入一条配置信息，表示启用 webhdfs</span></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;</span><br><span class="line">&lt;value&gt;<span class="literal">true</span>&lt;/value&gt; </span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.3.修改 hadoop 集群的 core-site.xml 配置文件:加入两条配置信息:表示设置 hadoop 的代理用户</span></span><br><span class="line"><span class="comment">## 注意： 此处的ap是配置 hadoop 的用户名</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.proxyuser.ap.hosts&lt;/name&gt;</span><br><span class="line">&lt;value&gt;*&lt;/value&gt; </span><br><span class="line">&lt;!-- 表示任意节点使用 hadoop 集群的代理用户 hadoop 都能访问 hdfs 集群 --&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt; </span><br><span class="line">&lt;name&gt;hadoop.proxyuser.ap.groups&lt;/name&gt; </span><br><span class="line">&lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;!-- 表示代理用户的组所属 --&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"> 注意 </span><br><span class="line">修改完成后， 发给hadoop集群其他主机</span><br><span class="line">scp xxx.xx  xxx.ss  cs2:<span class="variable">$PWD</span></span><br><span class="line">scp xxx.xx  xxx.ss  cs3:<span class="variable">$PWD</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.4.重启hdfs &amp; yarn服务</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.5.启动 hiveserver2 服务</span></span><br><span class="line"><span class="comment">## 后台启动：</span></span><br><span class="line">nohup hiveserver2 1&gt;/home/hadoop/hiveserver.log 2&gt;/home/hadoop/hiveserver.err &amp; </span><br><span class="line"><span class="comment"># 或者:</span></span><br><span class="line">nohup hiveserver2 1&gt;/dev/null 2&gt;/dev/null &amp;</span><br><span class="line"><span class="comment"># 或者:</span></span><br><span class="line">nohup hiveserver2 &gt;/dev/null 2&gt;&amp;1 &amp;</span><br><span class="line"><span class="comment"># 以上 3 个命令是等价的，第一个表示记录日志，第二个和第三个表示不记录日志</span></span><br><span class="line"></span><br><span class="line">注意： nohup 可以在你退出帐户/关闭终端之后继续运行相应的进程。 nohup 就是不挂起的意思(no hang up)。</span><br><span class="line">该命令的一般形式为:nohup <span class="built_in">command</span> &amp;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.6 启动 beeline 客户端去连接</span></span><br><span class="line">方式1：执行命令:</span><br><span class="line">beeline -u jdbc:hive2://cs2:10000 -n ap</span><br><span class="line">-u : 指定元数据库的链接信息 -n : 指定用户名和密码</span><br><span class="line"></span><br><span class="line">方式2：</span><br><span class="line">先执行 </span><br><span class="line">beeline</span><br><span class="line">然后按图所示输入:</span><br><span class="line"><span class="comment"># 此处的cs2是只安装hive的hostname</span></span><br><span class="line">!connect jdbc:hive2://cs2:10000 按回车，然后输入用户名，密码，这个 用户名就是安装 hadoop 集群的用户名和密码</span><br></pre></td></tr></table></figure><h4 id="登录beeline-bee"><a href="#登录beeline-bee" class="headerlink" title="登录beeline  :bee:"></a>登录beeline  :bee:</h4><p>beeline -u jdbc:hive2://cs2:10000 -n ap</p><h4 id="登录-hive"><a href="#登录-hive" class="headerlink" title="登录 hive"></a>登录 hive</h4><p>hive</p><h4 id="PS-Linux环境变量失效"><a href="#PS-Linux环境变量失效" class="headerlink" title="PS: Linux环境变量失效"></a>PS: Linux环境变量失效</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 就是直接把环境变量设置为/bin:/usr/bin，因为常用的命令都在/bin这个文件夹中。</span></span><br><span class="line">PATH=/bin:/usr/bin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 接下来修改 .bashr_profile 或则 .zshrc中的内容即可， 修改完了重新source</span></span><br></pre></td></tr></table></figure><h1 id="Hive-–-DDL"><a href="#Hive-–-DDL" class="headerlink" title="Hive – DDL"></a>Hive – DDL</h1><h3 id="库的操作"><a href="#库的操作" class="headerlink" title="库的操作"></a>库的操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">  库的操作 </span><br><span class="line">============================================================================</span><br><span class="line"></span><br><span class="line">#  建库 </span><br><span class="line">CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment]</span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[WITH DBPROPERTIES (property_name=property_value, ...)];</span><br><span class="line"></span><br><span class="line">1、创建普通库</span><br><span class="line">create database dbname;</span><br><span class="line"></span><br><span class="line">2、创建库的时候检查存与否</span><br><span class="line">create databse if not exists dbname;</span><br><span class="line"></span><br><span class="line">3、创建库的时候带注释</span><br><span class="line">create database if not exists dbname comment &apos;create my db named dbname&apos;;</span><br><span class="line"></span><br><span class="line">4、查看创建库的详细语句</span><br><span class="line">show create database mydb;</span><br><span class="line"></span><br><span class="line">#  查看库 </span><br><span class="line">1、查看有哪些数据库 </span><br><span class="line">show databases;</span><br><span class="line"></span><br><span class="line">2、显示数据库的详细属性信息</span><br><span class="line">语法:desc database [extended] dbname; </span><br><span class="line">示例:desc database extended myhive;</span><br><span class="line"></span><br><span class="line">3、查看正在使用哪个库 </span><br><span class="line">select current_database();</span><br><span class="line"></span><br><span class="line">4、查看创建库的详细语句 </span><br><span class="line">show create database mydb;</span><br><span class="line"></span><br><span class="line">5、查看以xx开头的库</span><br><span class="line">show databases like &apos;s*&apos;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#  删除库 </span><br><span class="line">删除库操作:</span><br><span class="line">drop database dbname;</span><br><span class="line">drop database if exists dbname;</span><br><span class="line"></span><br><span class="line">默认情况下，hive 不允许删除包含表的数据库，有两种解决办法:</span><br><span class="line">1、 手动删除库下所有表，然后删除库</span><br><span class="line">2、 使用 cascade 关键字</span><br><span class="line">drop database if exists dbname cascade;</span><br><span class="line"></span><br><span class="line">    默认情况下就是 restrict（严格模式）, 后2行效果一样</span><br><span class="line">drop database if exists myhive </span><br><span class="line">drop database if exists myhive restrict</span><br><span class="line"></span><br><span class="line">#  切换库 </span><br><span class="line">切换库操作:</span><br><span class="line">- 语法:</span><br><span class="line">use database_name </span><br><span class="line">- 实例:</span><br><span class="line">use myhive;</span><br></pre></td></tr></table></figure><h3 id="表的操作"><a href="#表的操作" class="headerlink" title="表的操作"></a>表的操作</h3><h4 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h4><h5 id="建表语法"><a href="#建表语法" class="headerlink" title="建表语法"></a><strong>建表语法</strong></h5><ul><li><code>CREATE [EXTERNAL] TABLE [IF NOT EXISTS] &lt;table_name&gt;</code><br>创建内部表；添加EXTERNAL参数会创建外部表</li><li><code>(col_name data_type [COMMENT col_comment], ...)</code><br>添加字段和字段描述</li><li><code>[COMMENT table_comment]</code><br>添加表描述</li><li><code>[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]</code><br>指定分区字段和字段描述，分区字段不能为建表字段！</li><li><code>[CLUSTERED BY (col_name, col_name, ...)]  SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]</code><ul><li>指定分桶字段，分桶字段必须为建表字段！</li><li>指定排序字段，此字段也必须为建表字段，指定的是分桶内的排序</li><li>指定分桶个数（hash后再取模）</li></ul></li><li><code>[ROW FORMAT row_format]</code><br>指定分隔符，row_format 格式：<pre><code>列分隔符：`delimited fields terminated by ‘x&apos;`行分隔符：`line terminated by ‘x&apos;`</code></pre></li><li><code>[STORED AS file_format]</code><br>指定存储格式<pre><code>textfile：文本格式，默认rcfile：行列结合格式parquet：压缩格式</code></pre></li><li><code>[LOCATION hdfs_path]</code><br>指定表在hsfs上的存储路径，不指定的话就按配置的路径存储，如果也没指定就在hive默认的路经 <code>/user/hive/warehouse</code></li></ul><h5 id="建表代码"><a href="#建表代码" class="headerlink" title="建表代码"></a>建表代码</h5><ul><li>a. 创建内部表<ul><li><code>create table mytable (id int, name string) row format delimited fields terminated by &#39;,&#39; stored as textfile;</code></li></ul></li><li>b. 创建外部表<ul><li><code>create  external table mytable2 (id int, name string) row format delimited  fields terminated by &#39;,&#39; location &#39;/user/hive/warehouse/mytable2&#39;;</code></li></ul></li><li>c. 创建分区表<ul><li><code>create  table table3(id int, name string) partitioned by(sex string) row format  delimited fields terminated by &#39;,&#39; stored as textfile;</code></li><li>插入分区数据：<code>load data local inpath &#39;/root/hivedata/mingxing.txt&#39; overwrite into table mytable3 partition(sex=&#39;girl’);</code></li><li>查询表分区： <code>show partitions mytable3</code></li></ul></li><li>d. 创建分桶表<ul><li><code>create  table stu_buck(Sno int,Sname string,Sex string,Sage int,Sdept string)  clustered by(Sno) sorted by(Sno DESC) into 4 buckets row format  delimited fields terminated by &#39;,’;</code></li></ul></li><li>e. 复制表<ul><li><code>create [external] table [if not exists] new_table like table_name;</code></li></ul></li><li>f. 查询表<ul><li><code>create table table_a as select * from  teble_b;</code></li></ul></li></ul><h4 id="查看表"><a href="#查看表" class="headerlink" title="查看表"></a>查看表</h4><ul><li><code>desc &lt;table_name&gt;</code>：显示表的字段信息</li><li><code>desc formatted &lt;table_name&gt;</code>：格式化显示表的详细信息</li><li><code>desc extended &lt;table_name&gt;</code>：显示表的详细信息</li></ul><h4 id="修改表"><a href="#修改表" class="headerlink" title="修改表"></a>修改表</h4><ul><li>重命名<ul><li><code>ALTER TABLE old_name RENAME TO new_name</code></li></ul></li><li>修改属性<ul><li><code>ALTER TABLE table_name SET TBLPROPERTIES (&#39;comment&#39; = &#39;my new students table’);</code></li><li>不支持修改表名，和表的数据存储目录</li></ul></li><li>增加/修改/替换字段<ul><li><code>ALTER TABLE table_name ADD COLUMNS (col_spec [, col_spec ...])</code><br>新增的字段位置在所有列后面 ( partition 列前 )</li><li><code>ALTER TABLE table_name CHANGE c_name new_c_name new_c_type [FIRST | AFTER c_name]</code><br>注意修改字段时，类型只能由小类型转为大类型，不让回报错；（在hive1.2.2中并没有此限制）</li><li><code>ALTER TABLE table_name REPLACE COLUMNS (col_spec [, col_spec ...])</code><br>REPLACE 表示替换表中所有字段</li></ul></li><li>添加/删除分区<ul><li>添加分区：<code>ALTER TABLE table_name ADD [IF NOT EXISTS]  PARTITION (partition_col = col_value1 [ ... ] ) [LOCATION &#39;location1’]</code></li><li>删除分区：<code>ALTER TABLE table_name DROP [IF EXISTS] PARTITION (partition_col = col_value1 [ ... ] )</code></li><li>修改分区路径：<code>ALTER TABLE student_p PARTITION (part=&#39;bb&#39;) SET LOCATION &#39;/myhive_bbbbb’;</code></li><li><strong>[补充：]</strong><ul><li>1、 防止分区被删除：alter table student_p partition (part=’aa’) enable no_drop;</li><li>2、 防止分区被查询：alter table student_p partition (part=’aa’) enable offline;<br>enable 和 disable 是反向操作 </li></ul></li></ul></li></ul><h4 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h4><ul><li><code>drop table if exists &lt;table_name&gt;;</code></li></ul><h4 id="清空表"><a href="#清空表" class="headerlink" title="清空表"></a>清空表</h4><p>会保留表结构</p><ul><li><code>truncate table table_name;</code></li><li><code>truncate table table_name partition(city=&#39;beijing’);</code></li></ul><h4 id="其他辅助命令"><a href="#其他辅助命令" class="headerlink" title="其他辅助命令"></a>其他辅助命令</h4><ul><li>a. 查看数据库列表<ul><li><code>show databases;</code></li><li><code>show databases like &#39;my*&#39;;</code></li></ul></li><li>b. 查看数据表<ul><li><code>show tables;</code></li><li><code>show tables in db_name;</code></li></ul></li><li>c. 查看数据表的建表语句<ul><li><code>show create table table_name;</code></li></ul></li><li>d. 查看 hive 函数列表<ul><li><code>show functions;</code></li></ul></li><li>e. 查看 hive 表的分区<ul><li><code>show partitions table_name;</code></li><li><code>show partitions table_name partition(city=&#39;beijing&#39;)</code></li></ul></li><li>f. 查看表的详细信息（元数据信息） <ul><li><code>desc table_name;</code></li><li><code>desc extended table_name;</code></li></ul></li><li>g. 查看数据库的详细属性信息<ul><li><code>desc formatted table_name;</code></li><li><code>desc database db_name; desc database extended db_name;</code></li></ul></li><li>h. 清空数据表<ul><li><code>truncate table table_name;</code></li></ul></li></ul><p><br></p><h1 id="Hive-–-DML"><a href="#Hive-–-DML" class="headerlink" title="Hive – DML"></a>Hive – DML</h1><h3 id="装载数据"><a href="#装载数据" class="headerlink" title="装载数据"></a>装载数据</h3><ul><li><code>LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE table_name [PARTITION (partcol1=val1, partcol2=val2 ...)]</code></li><li>注意：<ul><li>LOAD 操作只是单纯的 复制（本地文件）或者 移动（hdfs文件，一般是公共数据，需要建立外部表）操作，将数据文件移动到 Hive 表对应的位置</li><li>如果指定了 LOCAL 就去本地文件系统中查找，否则按 inpath 中的 uri 在 hdfs 上查找</li><li>inpath 子句中的文件路径下，不能再有文件夹</li><li>如果使用了 OVERWRITE 关键字，则目标表（或者分区）中的内容会被删除，然后再将 filepath 指向的文件/目录中的内容添加到表/分区中。 </li><li>如果目标表（分区）已经有一个文件，并且文件名和 filepath 中的文件名冲突，那么现有的文件会被新文件所替代。???不是自动重命名为xxx_copy_1</li></ul></li></ul><h3 id="插入数据"><a href="#插入数据" class="headerlink" title="插入数据"></a>插入数据</h3><ul><li><p>a. 单条插入</p><ul><li><code>INSERT INTO TABLE table_name VALUES(value1, value2, ...);</code></li></ul></li><li><p>b. 单重插入</p><ul><li><code>INSERT INTO TABLE table_name [PARTITION (partcol1=val1, ...)] &lt;select_statement1 FROM from_statement&gt;</code></li></ul></li><li><p>c. 多重插入</p><ul><li>FROM from_statement<br>从基表中按不同的字段查询得到的结果分别插入不同的 hive 表<br>只会扫描一次基表，提高查询性能</li><li><code>INSERT INTO TABLE table_name1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 [WHERE where_statement]</code></li><li><code>INSERT INTO TABLE table_name2 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement2 [WHERE where_statement]</code></li><li>[ … ];</li></ul></li><li><p>d. 分区插入</p><ul><li><p>分区插入有两种：一种是静态分区，另一种是动态分区。</p></li><li><p>如果混合使用静态分区和动态分区， 则静态分区必须出现在动态分区之前。</p></li><li><p>静态分区</p><ul><li>A)、创建静态分区表 </li><li>B)、从查询结果中导入数据（单重插入）<br>记得加载数据前要添加分区，然后指定要加载到那个分区</li><li>C)、查看插入结果 </li></ul></li><li><p>动态分区</p><ul><li><p>静态分区添加数据前需要指定分区，当分区个数不确定的时候就很不方便了，这个时候可以使用动态分区</p></li><li><p>重要且常用，尤其是按照日期分区时！</p></li><li><p>A)、创建分区表 </p></li><li><p>B)、参数设置</p></li></ul></li></ul></li></ul><pre><code>  hive-1.2版本  -  set hive.exec.dynamic.partition=true;   //动态分区开启状态，默认开启  -  set hive.exec.dynamic.partition.mode=nonstrict;   //动态分区执行模式，默认&quot;strict&quot;，在这种模式 下要求至少有一列分区字段是静态的，这有助于阻止因设计错误导致查询产生大量的分区  - \# 可选设置项    如果这些参数被更改了又想还原，则执行一次 reset 命令即可    - set hive.exec.max.dynamic.partitions.pernode=100;   //每个节点生成动态分区最大个数     - set hive.exec.max.dynamic.partitions=1000;   //生成动态分区最大个数，如果自动分区数大于这个参数，将会报错     - set hive.exec.max.created.files=100000;   //一个任务最多可以创建的文件数目     - set dfs.datanode.max.xcievers=4096;   //限定一次最多打开的文件数 set     - hive.error.on.empty.partition=false;   //表示当有空分区产生时，是否抛出异常- C)、动态数据插入   - 单个分区字段    - insert into table test2 partition (age) select name,address,school,age from students;   - 多个分区字段    多重分区中目录结构是按照分区字段顺序进行划分的    - insert into table student_ptn2 partition(department, age) select id, name, sex, department,age from students;      分区字段都是动态的    - insert into table student_ptn2 partition(city=&apos;sa&apos;, zipcode) select id, name, sex, age, department, department as zipcode from students;      第一个分区字段时静态的，第二字department字段动态的，重命名为zipcode？  - [注意：]    - 查询语句 select 查询出来的动态分区 age 和 zipcode 必须放在 最后，和分区字段对应，不然结果会出错- D)、查看插入结果  - select * from student_ptn2 where city=&apos;sa&apos; and zipcode=&apos;MA&apos;;</code></pre><ul><li><p>e. 分桶插入</p><ul><li>A)、创建分桶表</li><li>B)、从查询结果中导入数据<br>只能使用insert方式</li><li>C)、查看插入结果 </li><li># 几个命令<ul><li>set hive.exec.reducers.bytes.per.reducer=<number>  // 设置每个reducer的吞吐量，单位byte，默认256M</number></li><li>set hive.exec.reducers.max=<number>  //reduceTask最多执行个数，默认1009</number></li><li>set mapreduce.job.reduces=<number>   //设置reducetask实际运行数，默认-1，代表没有设置，即reducetask默认数为1</number></li><li>set hive.exec.mode.local.auto=true //设置hive本地模式</li></ul></li></ul></li></ul><h3 id="导出数据（了解）"><a href="#导出数据（了解）" class="headerlink" title="导出数据（了解）"></a>导出数据（了解）</h3><ul><li>单模式导出<ul><li><code>INSERT OVERWRITE [LOCAL] DIRECTORY directory1 &lt;select_statement&gt;;</code></li></ul></li><li>多模式导出<ul><li><code>FROM from_statement</code></li><li><code>INSERT OVERWRITE [LOCAL] DIRECTORY directory1 &lt;select_statement1&gt;</code></li><li><code>[INSERT OVERWRITE [LOCAL] DIRECTORY directory2 &lt;select_statement2&gt;] ...</code></li></ul></li></ul><h3 id="查询数据"><a href="#查询数据" class="headerlink" title="查询数据"></a>查询数据</h3><ul><li>Hive 中的 SELECT 基础语法和标准 SQL 语法基本一致，支持 WHERE、DISTINCT、GROUP BY、 ORDER BY、HAVING、LIMIT、子查询等；<ul><li>1、<code>select * from db.table1</code><br>虽然可以，但是要尽量避免 select * 这样的全表扫描操作，效率太低又费时</li><li>2、<code>select count(distinct uid) from db.table1</code></li><li>3、支持 select、union all、join（left、right、full join）、like、where、having、各种聚合函数、 支持 json 解析 </li><li>4、UDF/ UDAF/UDTF <ul><li>UDF：User Defined Function，自定义函数，一对一</li><li>UDAF：User Defined Aggregate Function，自定义聚合函数，多对一，如sum()，count()</li><li>UDTF ：User Defined Table Function，自定义表函数，一对多，如explode()</li></ul></li><li>5、不支持 update 和 delete </li><li>6、hive 虽然支持 in/exists（老版本是不支持的），但是 hive 推荐使用 semi join 的方式来代替 实现，而且效率更高。 <ul><li>半连接<ul><li>左半连接：left semi join，以左表为基表，右表有的，只显示左表相应记录（即一半）</li><li>右半连接：right semi join，与左半连接相反</li></ul></li><li>内连接：inner join，两表中都有的才会连接</li><li>外连接<ul><li>左外连接：left outer join，坐标为基表，右表有的会关联，右表没有的以null表示；左表没有右表有的不会关联</li><li>右外连接：right outer join，与左外连接相反</li><li>全外连接：full outer join，两表合并</li></ul></li></ul></li><li>7、支持 case … when …</li></ul></li><li><strong>语法结构</strong><ul><li><code>SELECT [ALL | DISTINCT] select_ condition, select_ condition, ...</code></li><li>FROM table_name a </li><li>[JOIN table_other b ON <a href="http://a.id" target="_blank" rel="noopener">a.id</a> = <a href="http://b.id" target="_blank" rel="noopener">b.id</a>]<br>表连接</li><li>[WHERE where_condition]<br>过滤条件</li><li>[GROUP BY col_list [HAVING condition]]<br>分组条件</li><li>[CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY col_list | ORDER BY col_list] [DESC]]<br>排序条件：<br>\1. order by：全局排序，默认升序，DESC表示降序。只有一个 reduce task 的结果，比如文<br>件名是 000000_0，会导致当输入规模较大时，需要较长的计算时间。<br>\2.  sort by：局部排序，其在数据进入 reducer 前完成排序。因此，如果用 sort by 进行排序，并且设置  mapred.reduce.tasks &gt; 1，则 sort by 只保证每个 reducer 的输出有序，不保证全局有序。<br>\3. distribute by：类似于分桶，根据指定的字段将数据分到不同的 reducer，且分发算法是 hash 散列<br>\4. cluster by：除了具有 Distribute by 的功能外，还会对该字段进行排序。<br>注意：如果 distribute 和 sort 字段是同一个时，cluster by = distribute by + sort by；<br>如果分桶字段和排序字段不一样，那么就不能使用 clustered by </li><li>[LIMIT number]<br>显示结果的前几个记录</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hive简介&quot;&gt;&lt;a href=&quot;#Hive简介&quot; class=&quot;headerlink&quot; title=&quot;Hive简介&quot;&gt;&lt;/a&gt;Hive简介&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/study/2018-
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/categories/Hadoop/Hive/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>i-数据分析系统Hive</title>
    <link href="https://airpoet.github.io/2018/06/12/Hadoop/Study/3-Hive/i-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9FHive/"/>
    <id>https://airpoet.github.io/2018/06/12/Hadoop/Study/3-Hive/i-数据分析系统Hive/</id>
    <published>2018-06-11T16:42:14.342Z</published>
    <updated>2018-06-14T01:19:27.806Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hive背景及应用场景"><a href="#Hive背景及应用场景" class="headerlink" title="Hive背景及应用场景"></a>Hive背景及应用场景</h1><h2 id="Hive是什么？"><a href="#Hive是什么？" class="headerlink" title="Hive是什么？"></a>Hive是什么？</h2><p><strong>看一下MR的 wordcount 和 Hive 的 wordcount</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173251.png" alt="image-20180612013251049"></p><p><br></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173355.png" alt="image-20180612013349514"></p><h2 id="Hive典型应用场景"><a href="#Hive典型应用场景" class="headerlink" title="Hive典型应用场景"></a>Hive典型应用场景</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173443.png" alt="image-20180612013442768"></p><p><br></p><h2 id="为什么使用Hive？"><a href="#为什么使用Hive？" class="headerlink" title="为什么使用Hive？"></a>为什么使用Hive？</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173532.png" alt="image-20180612013531999"></p><p><br></p><h2 id="有了Hive，还需要自己写MR程序吗？"><a href="#有了Hive，还需要自己写MR程序吗？" class="headerlink" title="有了Hive，还需要自己写MR程序吗？"></a>有了Hive，还需要自己写MR程序吗？</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173601.png" alt="image-20180612013600542"></p><p><br></p><p><br></p><h1 id="Hive基本架构"><a href="#Hive基本架构" class="headerlink" title="Hive基本架构"></a>Hive基本架构</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173645.png" alt="image-20180612013645384"></p><h2 id="Hive各模块组成"><a href="#Hive各模块组成" class="headerlink" title="Hive各模块组成"></a>Hive各模块组成</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173742.png" alt="image-20180612013742424"></p><p><br></p><h2 id="Hive部署架构-实验环境"><a href="#Hive部署架构-实验环境" class="headerlink" title="Hive部署架构-实验环境"></a>Hive部署架构-实验环境</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173814.png" alt="image-20180612013813506"></p><p><br></p><h2 id="Hive部署架构-生产环境"><a href="#Hive部署架构-生产环境" class="headerlink" title="Hive部署架构-生产环境"></a>Hive部署架构-生产环境</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173838.png" alt="image-20180612013837922"></p><p><br></p><h2 id="Hive部署架构-metastore服务"><a href="#Hive部署架构-metastore服务" class="headerlink" title="Hive部署架构-metastore服务"></a>Hive部署架构-metastore服务</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173904.png" alt="image-20180612013903730"></p><p><br></p><p><br></p><h1 id="Hive使用方式"><a href="#Hive使用方式" class="headerlink" title="Hive使用方式"></a>Hive使用方式</h1><h2 id="CLI（Command-Line-Interface）"><a href="#CLI（Command-Line-Interface）" class="headerlink" title="CLI（Command Line Interface）"></a>CLI（Command Line Interface）</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-173952.png" alt="image-20180612013951537"></p><p><br></p><h2 id="CLI—hive外部资源"><a href="#CLI—hive外部资源" class="headerlink" title="CLI—hive外部资源"></a>CLI—hive外部资源</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174023.png" alt="image-20180612014022323"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174103.png" alt="image-20180612014102686"></p><p><br></p><h2 id="Hive-Web-UI"><a href="#Hive-Web-UI" class="headerlink" title="Hive Web UI"></a>Hive Web UI</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174201.png" alt="image-20180612014200724"></p><p><br></p><h2 id="Hive客户端程序"><a href="#Hive客户端程序" class="headerlink" title="Hive客户端程序"></a>Hive客户端程序</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174222.png" alt="image-20180612014221547"></p><p><br></p><p><br></p><h1 id="HQL查询语句"><a href="#HQL查询语句" class="headerlink" title="HQL查询语句"></a>HQL查询语句</h1><h2 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174257.png" alt="image-20180612014257010"></p><p><br></p><h2 id="数据类型（不断增加中……）"><a href="#数据类型（不断增加中……）" class="headerlink" title="数据类型（不断增加中……）"></a>数据类型（不断增加中……）</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174340.png" alt="image-20180612014340830"></p><p><br></p><ul><li><strong>自有的特殊类型</strong></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174412.png" alt="image-20180612014411860"></p><p><br></p><h2 id="数据定义语句（DDL）"><a href="#数据定义语句（DDL）" class="headerlink" title="数据定义语句（DDL）"></a>数据定义语句（DDL）</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174515.png" alt="image-20180612014514763"></p><p><br></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174533.png" alt="image-20180612014533008"></p><p><br></p><p><strong>分隔符开发中这样写</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174607.png" alt="image-20180612014606665"></p><p><br></p><h2 id="Hive-Partition与Bucket"><a href="#Hive-Partition与Bucket" class="headerlink" title="Hive Partition与Bucket"></a>Hive Partition与Bucket</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174853.png" alt="image-20180612014852827"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174912.png" alt="image-20180612014912295"></p><p><br></p><p><br></p><h2 id="Hive数据格式"><a href="#Hive数据格式" class="headerlink" title="Hive数据格式"></a>Hive数据格式</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-174952.png" alt="image-20180612014951697"></p><h3 id="应用举例：日志清理"><a href="#应用举例：日志清理" class="headerlink" title="应用举例：日志清理"></a>应用举例：日志清理</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175053.png" alt="image-20180612015053452"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175104.png" alt="image-20180612015104125"></p><p><br></p><p><br></p><h2 id="数据操作语句（DML）"><a href="#数据操作语句（DML）" class="headerlink" title="数据操作语句（DML）"></a>数据操作语句（DML）</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175153.png" alt="image-20180612015153237"></p><h3 id="数据加载与插入语句"><a href="#数据加载与插入语句" class="headerlink" title="数据加载与插入语句"></a>数据加载与插入语句</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175226.png" alt="image-20180612015225942"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175246.png" alt="image-20180612015245860"></p><h3 id="几个实例"><a href="#几个实例" class="headerlink" title="几个实例"></a>几个实例</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175318.png" alt="image-20180612015317619"></p><h2 id="数据查询语句SELECT"><a href="#数据查询语句SELECT" class="headerlink" title="数据查询语句SELECT"></a>数据查询语句SELECT</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175356.png" alt="image-20180612015355128"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175419.png" alt="image-20180612015419432"></p><p><br></p><h2 id="两种分布式Join算法"><a href="#两种分布式Join算法" class="headerlink" title="两种分布式Join算法"></a>两种分布式Join算法</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175511.png" alt="image-20180612015511444"></p><p><br></p><h2 id="HQL中两种特殊Join"><a href="#HQL中两种特殊Join" class="headerlink" title="HQL中两种特殊Join"></a>HQL中两种特殊Join</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-175606.png" alt="image-20180612015606096"></p><p><br></p><h2 id="Order-By与Sort-By"><a href="#Order-By与Sort-By" class="headerlink" title="Order By与Sort By"></a>Order By与Sort By</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-231641.png" alt="image-20180612071641233"></p><p><br></p><h2 id="Distribute-by与Cluster-by"><a href="#Distribute-by与Cluster-by" class="headerlink" title="Distribute by与Cluster by"></a>Distribute by与Cluster by</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-231718.png" alt="image-20180612071718342"></p><p><br></p><h2 id="Transform语法（Streaming）"><a href="#Transform语法（Streaming）" class="headerlink" title="Transform语法（Streaming）"></a>Transform语法（Streaming）</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-231750.png" alt="image-20180612071749235"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-231813.png" alt="image-20180612071813456"></p><p><br></p><h2 id="用户自定义函数（UDF）"><a href="#用户自定义函数（UDF）" class="headerlink" title="用户自定义函数（UDF）"></a>用户自定义函数（UDF）</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-231839.png" alt="image-20180612071839711"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-231921.png" alt="image-20180612071920788"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-231940.png" alt="image-20180612071940199"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232009.png" alt="image-20180612072009048"></p><p>UDAF</p><ul><li><a href="http://svn.apache.org/repos/asf/hive/branches/branch0.13/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFAverage.java" target="_blank" rel="noopener">http://svn.apache.org/repos/asf/hive/branches/branch0.13/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFAverage.java</a></li></ul><p>UDTF</p><ul><li><a href="http://svn.apache.org/repos/asf/hive/branches/branch0.13/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFExplode.java" target="_blank" rel="noopener">http://svn.apache.org/repos/asf/hive/branches/branch0.13/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFExplode.java</a></li></ul><p><br></p><h2 id="其它问题"><a href="#其它问题" class="headerlink" title="其它问题"></a>其它问题</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232136.png" alt="image-20180612072136290"></p><p><br></p><h2 id="Hive-On-HBase"><a href="#Hive-On-HBase" class="headerlink" title="Hive On HBase"></a>Hive On HBase</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232225.png" alt="image-20180612072225509"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232245.png" alt="image-20180612072244391"></p><h1 id="Hive总结及其类似开源系统"><a href="#Hive总结及其类似开源系统" class="headerlink" title="Hive总结及其类似开源系统"></a>Hive总结及其类似开源系统</h1><h2 id="Stinger"><a href="#Stinger" class="headerlink" title="Stinger"></a>Stinger</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232321.png" alt="image-20180612072320862"></p><p><br></p><h2 id="Hive-MR-vs-Hive-Tez"><a href="#Hive-MR-vs-Hive-Tez" class="headerlink" title="Hive-MR vs Hive-Tez"></a>Hive-MR vs Hive-Tez</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232350.png" alt="image-20180612072350053"></p><p><br></p><h2 id="Shark"><a href="#Shark" class="headerlink" title="Shark"></a>Shark</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232415.png" alt="image-20180612072415680"></p><p><br></p><h2 id="Impala"><a href="#Impala" class="headerlink" title="Impala"></a>Impala</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232439.png" alt="image-20180612072438860"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232454.png" alt="image-20180612072454317"></p><p><br></p><h2 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a>性能比较</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232528.png" alt="image-20180612072528668"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-232548.png" alt="image-20180612072547727"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hive背景及应用场景&quot;&gt;&lt;a href=&quot;#Hive背景及应用场景&quot; class=&quot;headerlink&quot; title=&quot;Hive背景及应用场景&quot;&gt;&lt;/a&gt;Hive背景及应用场景&lt;/h1&gt;&lt;h2 id=&quot;Hive是什么？&quot;&gt;&lt;a href=&quot;#Hive是什么？&quot;
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/categories/Hadoop/Hive/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>偶然发现的好歌</title>
    <link href="https://airpoet.github.io/2018/06/11/Songs/%E5%81%B6%E7%84%B6%E5%8F%91%E7%8E%B0%E7%9A%84%E5%A5%BD%E6%AD%8C/"/>
    <id>https://airpoet.github.io/2018/06/11/Songs/偶然发现的好歌/</id>
    <published>2018-06-11T11:04:05.646Z</published>
    <updated>2018-06-11T11:32:43.834Z</updated>
    
    <content type="html"><![CDATA[ <blockquote class="blockquote-center"><h1 id="Dealbreaker"><a href="#Dealbreaker" class="headerlink" title="Dealbreaker"></a>Dealbreaker</h1><ul><li>专辑：<a href="orpheus://orpheus/pub/app.html#/m/album/?id=1715512" target="_blank" rel="noopener">Chesapeake</a></li><li>歌手：<a href="orpheus://orpheus/pub/app.html#/m/artist/?id=72720" target="_blank" rel="noopener">Rachael Yamagata</a></li><li>链接：<a href="http://music.163.com/#/m/song?id=18733198" target="_blank" rel="noopener">http://music.163.com/#/m/song?id=18733198</a></li></ul><p><br></p><h1 id="You-Won’t-Let-Me"><a href="#You-Won’t-Let-Me" class="headerlink" title="You Won’t Let Me"></a>You Won’t Let Me</h1><ul><li>专辑：<a href="orpheus://orpheus/pub/app.html#/m/album/?id=1715512" target="_blank" rel="noopener">Chesapeake</a></li><li>歌手：<a href="orpheus://orpheus/pub/app.html#/m/artist/?id=72720" target="_blank" rel="noopener">Rachael Yamagata</a></li><li>链接：<a href="http://music.163.com/#/song?id=18733192" target="_blank" rel="noopener">http://music.163.com/#/song?id=18733192</a></li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      
      
         &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h1 id=&quot;Dealbreaker&quot;&gt;&lt;a href=&quot;#Dealbreaker&quot; class=&quot;headerlink&quot; title=&quot;Dealbreaker&quot;&gt;&lt;/a&gt;Dealbreaker&lt;/h
      
    
    </summary>
    
      <category term="文艺" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/"/>
    
      <category term="Songs" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/Songs/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="文艺" scheme="https://airpoet.github.io/tags/%E6%96%87%E8%89%BA/"/>
    
      <category term="Songs" scheme="https://airpoet.github.io/tags/Songs/"/>
    
  </entry>
  
  <entry>
    <title>_HDFS应用场景&amp;原理&amp;基本架构及使用方法</title>
    <link href="https://airpoet.github.io/2018/06/11/Hadoop/Study/1-HDFS/HDFS%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF&amp;%E5%8E%9F%E7%90%86&amp;%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84%E5%8F%8A%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <id>https://airpoet.github.io/2018/06/11/Hadoop/Study/1-HDFS/HDFS应用场景&amp;原理&amp;基本架构及使用方法/</id>
    <published>2018-06-11T00:01:43.146Z</published>
    <updated>2018-06-11T16:27:43.347Z</updated>
    
    <content type="html"><![CDATA[<h1 id="HDFS基本架构和原理"><a href="#HDFS基本架构和原理" class="headerlink" title="HDFS基本架构和原理"></a>HDFS基本架构和原理</h1><h2 id="HDFS设计思想"><a href="#HDFS设计思想" class="headerlink" title="HDFS设计思想"></a>HDFS设计思想</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-142401.png" alt="image-20180611222401493"></p><h2 id="HDFS架构"><a href="#HDFS架构" class="headerlink" title="HDFS架构"></a>HDFS架构</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-142442.png" alt="image-20180611222442066"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-142505.png" alt="image-20180611222505133"></p><h2 id="HDFS数据块（block）"><a href="#HDFS数据块（block）" class="headerlink" title="HDFS数据块（block）"></a>HDFS数据块（block）</h2><ul><li><strong>注意： Hadoop2.x，block默认大小是128MB</strong></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-142646.png" alt="image-20180611222645888"></p><h2 id="HDFS写流程"><a href="#HDFS写流程" class="headerlink" title="HDFS写流程"></a>HDFS写流程</h2><ul><li>创建Distributed FileSystem类 </li><li>询问 NameNode 要写的文件对否存在</li><li>不存在就写入到 FSDataOutputStream 流中</li><li>流写出去到一个 DataNode</li><li>…</li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-143228.png" alt="image-20180611223228053"></p><h2 id="HDFS读流程"><a href="#HDFS读流程" class="headerlink" title="HDFS读流程"></a>HDFS读流程</h2><ul><li>客户端向 NameNode 询问 block 的位置</li><li>按照客户端按照拿到的位置，向不同的DataNode 请求数据</li><li>……</li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-143510.png" alt="image-20180611223510239"></p><h2 id="HDFS典型物理拓扑"><a href="#HDFS典型物理拓扑" class="headerlink" title="HDFS典型物理拓扑"></a>HDFS典型物理拓扑</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-143914.png" alt="image-20180611223914168"></p><h2 id="HDFS副本放置策略"><a href="#HDFS副本放置策略" class="headerlink" title="HDFS副本放置策略"></a>HDFS副本放置策略</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-143940.png" alt="image-20180611223939952"></p><h2 id="HDFS可靠性策略"><a href="#HDFS可靠性策略" class="headerlink" title="HDFS可靠性策略"></a>HDFS可靠性策略</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-144153.png" alt="image-20180611224152799"></p><h2 id="HDFS不适合存储小文件"><a href="#HDFS不适合存储小文件" class="headerlink" title="HDFS不适合存储小文件"></a>HDFS不适合存储小文件</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-144344.png" alt="image-20180611224344481"></p><p><br></p><h1 id="HDFS程序设计"><a href="#HDFS程序设计" class="headerlink" title="HDFS程序设计"></a>HDFS程序设计</h1><h2 id="HDFS访问方式"><a href="#HDFS访问方式" class="headerlink" title="HDFS访问方式"></a>HDFS访问方式</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-144935.png" alt="image-20180611224934785"></p><h2 id="HDFS-Shell命令"><a href="#HDFS-Shell命令" class="headerlink" title="HDFS Shell命令"></a>HDFS Shell命令</h2><h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145132.png" alt="image-20180611225131966"></p><h3 id="HDFS-Shell命令—文件操作命令"><a href="#HDFS-Shell命令—文件操作命令" class="headerlink" title="HDFS Shell命令—文件操作命令"></a>HDFS Shell命令—文件操作命令</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145219.png" alt="image-20180611225219211"></p><h3 id="HDFS-Shell命令—文件操作命令-1"><a href="#HDFS-Shell命令—文件操作命令-1" class="headerlink" title="HDFS Shell命令—文件操作命令"></a>HDFS Shell命令—文件操作命令</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145604.png" alt="image-20180611225604218"></p><h3 id="HDFS-Shell命令—管理命令"><a href="#HDFS-Shell命令—管理命令" class="headerlink" title="HDFS Shell命令—管理命令"></a>HDFS Shell命令—管理命令</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145728.png" alt="image-20180611225727910"></p><h3 id="HDFS-Shell命令—管理脚本"><a href="#HDFS-Shell命令—管理脚本" class="headerlink" title="HDFS Shell命令—管理脚本"></a>HDFS Shell命令—管理脚本</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145803.png" alt="image-20180611225802894"></p><h3 id="HDFS-Shell命令—文件管理命令fsck"><a href="#HDFS-Shell命令—文件管理命令fsck" class="headerlink" title="HDFS Shell命令—文件管理命令fsck"></a>HDFS Shell命令—文件管理命令fsck</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145932.png" alt="image-20180611225931880"></p><ul><li><strong>查看帮助</strong></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145915.png" alt="image-20180611225914482"></p><ul><li><strong>用法示例</strong></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-150253.png" alt="image-20180611230252655"></p><h3 id="HDFS-Shell命令—数据均衡器balancer"><a href="#HDFS-Shell命令—数据均衡器balancer" class="headerlink" title="HDFS Shell命令—数据均衡器balancer"></a>HDFS Shell命令—数据均衡器balancer</h3><ul><li><strong>一般设置10% —— 15% 就差不多了</strong></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-150614.png" alt="image-20180611230613854"></p><h3 id="HDFS-Shell命令—设置目录份额"><a href="#HDFS-Shell命令—设置目录份额" class="headerlink" title="HDFS Shell命令—设置目录份额"></a>HDFS Shell命令—设置目录份额</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-150704.png" alt="image-20180611230703142"></p><h3 id="※-HDFS-Shell命令—增加-移除节点-※"><a href="#※-HDFS-Shell命令—增加-移除节点-※" class="headerlink" title="※ HDFS Shell命令—增加/移除节点 ※"></a>※ HDFS Shell命令—增加/移除节点 ※</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-150741.png" alt="image-20180611230741126"></p><h2 id="HDFS-Java"><a href="#HDFS-Java" class="headerlink" title="HDFS Java"></a>HDFS Java</h2><h3 id="API介绍"><a href="#API介绍" class="headerlink" title="API介绍"></a>API介绍</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-150953.png" alt="image-20180611230953359"></p><h3 id="HDFS-Java程序举例"><a href="#HDFS-Java程序举例" class="headerlink" title="HDFS Java程序举例"></a>HDFS Java程序举例</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-151048.png" alt="image-20180611231047354"></p><h2 id="HDFS-多语言API—借助thrift"><a href="#HDFS-多语言API—借助thrift" class="headerlink" title="HDFS 多语言API—借助thrift"></a>HDFS 多语言API—借助thrift</h2><p><a href="http://thrift.apache.org/" target="_blank" rel="noopener">也是Apache的顶级项目</a></p><h3 id="thrift执行流程"><a href="#thrift执行流程" class="headerlink" title="thrift执行流程"></a>thrift执行流程</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-151149.png" alt="image-20180611231148955"></p><h3 id="hadoopfs-thrift接口定义"><a href="#hadoopfs-thrift接口定义" class="headerlink" title="hadoopfs.thrift接口定义"></a>hadoopfs.thrift接口定义</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-151634.png" alt="image-20180611231634046"></p><h3 id="PHP语言访问HDFS"><a href="#PHP语言访问HDFS" class="headerlink" title="PHP语言访问HDFS"></a>PHP语言访问HDFS</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-151654.png" alt="image-20180611231653750"></p><h3 id="Python语言访问HDFS"><a href="#Python语言访问HDFS" class="headerlink" title="Python语言访问HDFS"></a>Python语言访问HDFS</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-152052.png" alt="image-20180611232052633"></p><hr><h1 id="Hadoop-2-0新特性"><a href="#Hadoop-2-0新特性" class="headerlink" title="Hadoop 2.0新特性"></a>Hadoop 2.0新特性</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-152948.png" alt="image-20180611232948514"></p><h2 id="HA-高可用-与Federation-联邦"><a href="#HA-高可用-与Federation-联邦" class="headerlink" title="HA(高可用)与Federation(联邦)"></a>HA(高可用)与Federation(联邦)</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153058.png" alt="image-20180611233058385"></p><h2 id="异构层级存储结构"><a href="#异构层级存储结构" class="headerlink" title="异构层级存储结构"></a>异构层级存储结构</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153117.png" alt="image-20180611233117295"></p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153206.png" alt="image-20180611233206158"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153228.png" alt="image-20180611233227447"></p><h2 id="HDFS-ACL"><a href="#HDFS-ACL" class="headerlink" title="HDFS ACL"></a>HDFS ACL</h2><h3 id="背景：现有权限管理的局限性"><a href="#背景：现有权限管理的局限性" class="headerlink" title="背景：现有权限管理的局限性"></a>背景：现有权限管理的局限性</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153255.png" alt="image-20180611233255196"></p><h3 id="基于POSIX-ACL的实现"><a href="#基于POSIX-ACL的实现" class="headerlink" title="基于POSIX ACL的实现"></a>基于POSIX ACL的实现</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153310.png" alt="image-20180611233310082"></p><h2 id="HDFS快照"><a href="#HDFS快照" class="headerlink" title="HDFS快照"></a>HDFS快照</h2><h3 id="背景-1"><a href="#背景-1" class="headerlink" title="背景"></a>背景</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153338.png" alt="image-20180611233337709"></p><h3 id="基本使用方法"><a href="#基本使用方法" class="headerlink" title="基本使用方法"></a>基本使用方法</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153358.png" alt="image-20180611233357356"></p><h2 id="HDFS缓存"><a href="#HDFS缓存" class="headerlink" title="HDFS缓存"></a>HDFS缓存</h2><h3 id="背景-2"><a href="#背景-2" class="headerlink" title="背景"></a>背景</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153423.png" alt="image-20180611233423131"></p><h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153446.png" alt="image-20180611233446072"></p><h3 id="实现情况"><a href="#实现情况" class="headerlink" title="实现情况"></a>实现情况</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153502.png" alt="image-20180611233502379"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153521.png" alt="image-20180611233521502"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;HDFS基本架构和原理&quot;&gt;&lt;a href=&quot;#HDFS基本架构和原理&quot; class=&quot;headerlink&quot; title=&quot;HDFS基本架构和原理&quot;&gt;&lt;/a&gt;HDFS基本架构和原理&lt;/h1&gt;&lt;h2 id=&quot;HDFS设计思想&quot;&gt;&lt;a href=&quot;#HDFS设计思想&quot;
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="HDFS" scheme="https://airpoet.github.io/categories/Hadoop/HDFS/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="HDFS" scheme="https://airpoet.github.io/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>关于hexo的时序图插件 hexo-filter-sequence 的巨坑</title>
    <link href="https://airpoet.github.io/2018/06/10/Tools/Hexo/%E5%85%B3%E4%BA%8Ehexo%E7%9A%84%E6%97%B6%E5%BA%8F%E5%9B%BE%E6%8F%92%E4%BB%B6-hexo-filter-sequence-%E7%9A%84%E5%B7%A8%E5%9D%91/"/>
    <id>https://airpoet.github.io/2018/06/10/Tools/Hexo/关于hexo的时序图插件-hexo-filter-sequence-的巨坑/</id>
    <published>2018-06-10T13:13:02.149Z</published>
    <updated>2018-06-10T13:19:32.909Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在写代码的过程中，发现时序图还是挺管用的，简洁明了，于是想用一用。</p><p>结果发到站上，不显示。</p><p>在网上查了下，发现是hexo所使用的Markdown渲染语法不支持。</p><p>这里吐槽下，这里渲染的确实烂，作者为啥不改改..</p><p><br></p><p>于是开始找解决方案，发现大多数都推荐了一个叫<code>hexo-filter-sequence</code>的插件，故安装之。</p><p>结果死活还是不行。</p><p>装了其它的几个flow图，却可以显示。</p><p><strong>当flow图和sequence图同时存在的时候，sequence可以显示，但是sequence单独存在时，死活不显示。</strong></p><p>难不成有啥依赖？必须先使用flow才能使用sequence？逻辑上不应该啊！</p><p>但是事实却是这样！</p><p><br></p><p>网上有一篇大神说要改源码才行，开始没注意，觉得应该不会吧，这么多人使用，源码还会有问题？？</p><p><strong>仔细一看源码，妈的！！心里千万匹草泥马呼啸而过！</strong></p><p><strong>把初始化 sequence，写成了初始化 flow！！！</strong></p><p><strong>把 flow 改成 sequence， 再把 js CDN源换成国内的！</strong></p><p><strong>可以了！！</strong></p><p>再仔细一看，发现最后一次更新是在1年前！</p><p>坑爹的作者，浪费了我至少3-5个小时！！</p><p><br></p><hr><h2 id="下面为部分摘抄"><a href="#下面为部分摘抄" class="headerlink" title="下面为部分摘抄"></a>下面为部分摘抄</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p><a href="https://github.com/bubkoo/hexo-filter-sequence" target="_blank" rel="noopener">hexo-filter-sequence</a> 插件:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install --save hexo-filter-sequence</span><br></pre></td></tr></table></figure><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>站点配置文件 <code>_config.yml</code> 中增加如下配置:</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sequence:</span><br><span class="line">  webfont: https:<span class="comment">//cdn.bootcss.com/webfont/1.6.28/webfontloader.js</span></span><br><span class="line">  raphael: https:<span class="comment">//cdn.bootcss.com/raphael/2.2.7/raphael.min.js</span></span><br><span class="line">  underscore: https:<span class="comment">//cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js</span></span><br><span class="line">  sequence: https:<span class="comment">//cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js</span></span><br><span class="line">  css: # optional, the url for css, such as hand drawn theme </span><br><span class="line">  options: </span><br><span class="line">    theme: simple</span><br><span class="line">    css_class:</span><br></pre></td></tr></table></figure><h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><p>源码修改后才能正常使用，进入插件目录作如下修改：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// index.js</span></span><br><span class="line"><span class="keyword">var</span> assign = <span class="built_in">require</span>(<span class="string">'deep-assign'</span>);</span><br><span class="line"><span class="keyword">var</span> renderer = <span class="built_in">require</span>(<span class="string">'./lib/renderer'</span>);</span><br><span class="line"></span><br><span class="line">hexo.config.sequence = assign(&#123;</span><br><span class="line">  webfont: <span class="string">'https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js'</span>,</span><br><span class="line">  raphael: <span class="string">'https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js'</span>,</span><br><span class="line">  underscore: <span class="string">'https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js'</span>,</span><br><span class="line">  sequence: <span class="string">'https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js'</span>,</span><br><span class="line">  css: <span class="string">''</span>,</span><br><span class="line">  options: &#123;</span><br><span class="line">    theme: <span class="string">'simple'</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;, hexo.config.sequence);</span><br><span class="line"></span><br><span class="line">hexo.extend.filter.register(<span class="string">'before_post_render'</span>, renderer.render, <span class="number">9</span>);</span><br></pre></td></tr></table></figure><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// lib/renderer.js, 25 行</span></span><br><span class="line"><span class="keyword">if</span> (sequences.length) &#123;</span><br><span class="line">      <span class="keyword">var</span> config = <span class="keyword">this</span>.config.sequence;</span><br><span class="line">      <span class="comment">// resources</span></span><br><span class="line">      data.content += <span class="string">'&lt;script src="'</span> + config.webfont + <span class="string">'"&gt;&lt;/script&gt;'</span>;</span><br><span class="line">      data.content += <span class="string">'&lt;script src="'</span> + config.raphael + <span class="string">'"&gt;&lt;/script&gt;'</span>;</span><br><span class="line">      data.content += <span class="string">'&lt;script src="'</span> + config.underscore + <span class="string">'"&gt;&lt;/script&gt;'</span>;</span><br><span class="line">      data.content += <span class="string">'&lt;script src="'</span> + config.sequence + <span class="string">'"&gt;&lt;/script&gt;'</span>;</span><br><span class="line">      ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>新建代码块，增加如下内容：</p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-10-131910.png" alt="image-20180610211909603"></p><div id="sequence-0"></div><p><a href="http://wewelove.github.io/fcoder/2017/09/06/markdown-sequence/index.html" target="_blank" rel="noopener">详情参考</a></p><p><script src="https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js"></script><script src="https://cdn.bootcss.com/snap.svg/0.5.1/snap.svg-min.js"></script><script src="https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js"></script><script src="https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js"></script><script src="https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js"></script><textarea id="sequence-0-code" style="display: none">Alice->Bob: Hello Bob, how are you?Note right of Bob: Bob thinksBob-->Alice: I am good thanks!</textarea><textarea id="sequence-0-options" style="display: none">{"theme":"simple"}</textarea><script>  var code = document.getElementById("sequence-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value));  var diagram = Diagram.parse(code);  diagram.drawSVG("sequence-0", options);</script></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在写代码的过程中，发现时序图还是挺管用的，简洁明了，于是想用一用。&lt;/p&gt;
&lt;p&gt;结果发到站上，不显示。&lt;/p&gt;
&lt;p&gt;在网上查了下，发现是
      
    
    </summary>
    
      <category term="工具" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="hexo" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/hexo/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="工具" scheme="https://airpoet.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="hexo" scheme="https://airpoet.github.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce-分组浅探</title>
    <link href="https://airpoet.github.io/2018/06/10/Hadoop/Study/2-MapReduce/MapReduce-%E5%88%86%E7%BB%84%E6%B5%85%E6%8E%A2/"/>
    <id>https://airpoet.github.io/2018/06/10/Hadoop/Study/2-MapReduce/MapReduce-分组浅探/</id>
    <published>2018-06-10T08:49:24.337Z</published>
    <updated>2018-06-11T11:46:23.533Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>最近学分组的时候，一直弄不明白，总感觉一头雾水，通过近3个小时的断点调试和log输出。 大概了解了一些表象，先记录。</p><p>案例是这个</p><blockquote><p>求出每门课程参考学生成绩最高平均分的学生的信息：</p><p>课程，姓名和平均分，详细见<a href="https://airpoet.github.io/2018/06/09/Hadoop/Study/MapReduce%E7%AC%94%E8%AE%B0-%E7%BB%83%E4%B9%A0/"><sup>MapReduce笔记-练习第二题第3小题</sup></a></p><p>数据格式是这样的：</p><blockquote><p>第一个是课程名称，总共四个课程，computer，math，english，algorithm，</p><p>第二个是学生姓名，后面是每次考试的分数</p><p><em>math,huangxiaoming,85,75,85,99,66,88,75,91</em></p><p><em>english,huanglei,85,75,85,99,66,88,75,91</em></p><p>… </p></blockquote></blockquote><p><br></p><p><br></p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><div id="sequence-0"></div><h4 id="执行流程结论"><a href="#执行流程结论" class="headerlink" title="执行流程结论"></a><strong>执行流程结论</strong></h4><ul><li>map每读一行就 write 到 context 一次，按照指定的<code>key</code>进行分发</li><li><p>map 把所有的数据都读完了之后，大概执行到<code>67%</code>的时候，开始进入 <code>CustomBean</code>，执行<code>CustomBean</code>的<code>compareTo()</code>方法，会按照自己写的规则一条一条数据比较</p></li><li><p>上述都比较完毕之后，map阶段就结束了，此时来到了 reduce阶段，但是是到了<code>67%</code>了</p></li><li>到了reduce阶段，直接进入了<strong>MyGroup</strong>中自定义的<strong>compare</strong>方法。</li><li>MyGroup的<code>compare()</code>方法，如果返回<strong>非0</strong>， 就会进入 reduce 方法<strong>写出到context</strong></li></ul><h4 id="MyGroup进入Reduce的条件是"><a href="#MyGroup进入Reduce的条件是" class="headerlink" title="MyGroup进入Reduce的条件是"></a><strong>MyGroup进入Reduce的条件是</strong></h4><ul><li>MyReduce中，如果compare的结果不等于0，也就是比较的2者不相同， 此时就进入Reduce， 写出到上下文</li><li>如果相同，会一直往下读，直到读到不同的， 此时写出读到上下文。</li><li>因为MyGroup会在Reduce阶段执行，而<code>CustomBean</code>中的<code>compareTo()</code>是在map阶段执行，所以需要在<code>CustomBean</code>中就把组排好序，此时<strong>分组功能</strong>才能正常运作</li></ul><h4 id="指定分组类MyGroup和不指定的区别"><a href="#指定分组类MyGroup和不指定的区别" class="headerlink" title="指定分组类MyGroup和不指定的区别"></a>指定分组类MyGroup和不指定的区别</h4><p><em>指定与不指定是指：在Driver类中，是否加上<code>job.setGroupingComparatorClass(MyGrouper.class);</code>这一句。</em></p><ul><li><strong>指定分组类</strong>：<ul><li>会按照分组类中，自定义的<code>compare()</code>方法比较，相同的为一组，分完一组就进入一次reduce方法</li></ul></li><li><strong>不指定分组类：（目前存疑）</strong><ul><li>是否是按照key进行分组</li><li>如果是自定义类为key，是否是按照此key中值相同的分为一组</li><li>如果是hadoop内置类，是否是按照此类的值分组（Text-String的值，IntWritable-int值等..）</li><li><strong>依然是走得以上这套分组逻辑，一组的数据读完才进入到Reduce阶段做归并</strong></li></ul></li></ul><p><br></p><p><br></p><h3 id="Log信息"><a href="#Log信息" class="headerlink" title="Log信息"></a>Log信息</h3><h5 id="CustomBean中没有进行分组-组内排序的log"><a href="#CustomBean中没有进行分组-组内排序的log" class="headerlink" title="CustomBean中没有进行分组, 组内排序的log"></a><code>CustomBean</code>中没有进行<code>分组</code>, <code>组内排序</code>的log</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ==================MyGroup中compare()方法=======================</span></span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// ======================reduce中的执行log==========================</span></span><br><span class="line"></span><br><span class="line">==================第<span class="number">1</span>次进入reduce</span><br><span class="line">computerhuangjiaju<span class="number">83.2</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerliutao<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">2</span>次进入reduce</span><br><span class="line">mathhuangxiaoming<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">3</span>次进入reduce</span><br><span class="line">englishhuanglei<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">4</span>次进入reduce</span><br><span class="line">mathhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">5</span>次进入reduce</span><br><span class="line">algorithmhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmliutao<span class="number">82.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">6</span>次进入reduce</span><br><span class="line">computerhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">7</span>次进入reduce</span><br><span class="line">englishliuyifei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">8</span>次进入reduce</span><br><span class="line">algorithmhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">9</span>次进入reduce</span><br><span class="line">mathhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">10</span>次进入reduce</span><br><span class="line">algorithmhuangzitao<span class="number">72.75</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">11</span>次进入reduce</span><br><span class="line">mathliujialing<span class="number">72.75</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">12</span>次进入reduce</span><br><span class="line">computerhuangzitao<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">13</span>次进入reduce</span><br><span class="line">englishhuangxiaoming<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">14</span>次进入reduce</span><br><span class="line">mathwangbaoqiang<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">15</span>次进入reduce</span><br><span class="line">computerhuangxiaoming<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">16</span>次进入reduce</span><br><span class="line">mathxuzheng<span class="number">69.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">17</span>次进入reduce</span><br><span class="line">englishzhaobenshan<span class="number">69.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">18</span>次进入reduce</span><br><span class="line">computerhuangbo<span class="number">65.25</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerxuzheng<span class="number">65.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">19</span>次进入reduce</span><br><span class="line">englishzhouqi<span class="number">64.18181818181819</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">20</span>次进入reduce</span><br><span class="line">computerliujialing<span class="number">64.11111111111111</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">21</span>次进入reduce</span><br><span class="line">algorithmliuyifei<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">22</span>次进入reduce</span><br><span class="line">englishliujialing<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">23</span>次进入reduce</span><br><span class="line">computerliuyifei<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">24</span>次进入reduce</span><br><span class="line">englishliuyifei<span class="number">59.57142857142857</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">25</span>次进入reduce</span><br><span class="line">mathliutao<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">26</span>次进入reduce</span><br><span class="line">algorithmhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">27</span>次进入reduce</span><br><span class="line">computerhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">28</span>次进入reduce</span><br><span class="line">englishhuangbo<span class="number">55.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br></pre></td></tr></table></figure><h5 id="CustomBean中做了分组-amp-组内排序-的"><a href="#CustomBean中做了分组-amp-组内排序-的" class="headerlink" title="CustomBean中做了分组&amp;组内排序 的"></a><code>CustomBean</code>中做了<code>分组&amp;组内排序</code> 的</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// **Bean中相同的组进行分数降序， 组进行字典排序，此时MyGroup中执行的**</span></span><br><span class="line"></span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line"></span><br><span class="line"><span class="comment">// ======================reduce中执行==========================</span></span><br><span class="line"></span><br><span class="line">==================第<span class="number">1</span>次进入reduce</span><br><span class="line">algorithmhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmliutao<span class="number">82.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmhuangzitao<span class="number">72.75</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmliuyifei<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">===================离开reduce</span><br><span class="line">==================第<span class="number">2</span>次进入reduce</span><br><span class="line">computerhuangjiaju<span class="number">83.2</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerliutao<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerhuangzitao<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerhuangxiaoming<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerhuangbo<span class="number">65.25</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerxuzheng<span class="number">65.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerliujialing<span class="number">64.11111111111111</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerliuyifei<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">===================离开reduce</span><br><span class="line">==================第<span class="number">3</span>次进入reduce</span><br><span class="line">englishhuanglei<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishliuyifei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishhuangxiaoming<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishzhaobenshan<span class="number">69.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishzhouqi<span class="number">64.18181818181819</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishliujialing<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishliuyifei<span class="number">59.57142857142857</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishhuangbo<span class="number">55.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">===================离开reduce</span><br><span class="line">==================第<span class="number">4</span>次进入reduce</span><br><span class="line">mathhuangxiaoming<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathliujialing<span class="number">72.75</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathwangbaoqiang<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathxuzheng<span class="number">69.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathliutao<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">===================离开reduce</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//  如果只取一个每次values的第一个的话 </span></span><br><span class="line"></span><br><span class="line">algorithmhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">==================第<span class="number">1</span>次进入reduce</span><br><span class="line">computerhuangjiaju<span class="number">83.2</span></span><br><span class="line">==================第<span class="number">2</span>次进入reduce</span><br><span class="line">englishhuanglei<span class="number">83.0</span></span><br><span class="line">==================第<span class="number">3</span>次进入reduce</span><br><span class="line">mathhuangxiaoming<span class="number">83.0</span></span><br><span class="line">==================第<span class="number">4</span>次进入reduce</span><br></pre></td></tr></table></figure><p><br></p><p><br></p><h3 id="其它疑点"><a href="#其它疑点" class="headerlink" title="其它疑点"></a>其它疑点</h3><ul><li>通过log可以看出来， MyGroup是在读到了不同的数据，才会进入到 reduce 类中写出；</li><li>但是 通过 <strong>断点调试</strong>时， 现象是，第一次读到了2个相同的，就去reduce去写出了；</li><li>后面再读的就不做写出操作，直到下一次读到2个相同的，再在reduce中写出。</li></ul><p><script src="https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js"></script><script src="https://cdn.bootcss.com/snap.svg/0.5.1/snap.svg-min.js"></script><script src="https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js"></script><script src="https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js"></script><script src="https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js"></script><textarea id="sequence-0-code" style="display: none">title: 执行流程时序图 Mapper(map)->ScoreBean: k:ScoreBean(courseName,avgScore)Mapper(map)->ScoreBean: v:Text(stuName)Mapper(ScoreBean)->Reducer(MyGroup): course按字典升序Mapper(ScoreBean)->Reducer(MyGroup): course内成绩降序Reducer(MyGroup)->Reducer(reduce): 根据自定义的分组规则按组输出Reducer(MyGroup)->Reducer(reduce): 一组只调用reduce一次Reducer(reduce)-->Reducer(reduce):</textarea><textarea id="sequence-0-options" style="display: none">{"theme":"simple"}</textarea><script>  var code = document.getElementById("sequence-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value));  var diagram = Diagram.parse(code);  diagram.drawSVG("sequence-0", options);</script></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;最近学分组的时候，一直弄不明白，总感觉一头雾水，通过近3个小时的断点调试和log输出。 大概了解了一些表象，先记录。&lt;/p&gt;
&lt;p&gt;案例是这
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>▍开始</title>
    <link href="https://airpoet.github.io/2018/06/10/Poetry/%E5%BC%80%E5%A7%8B/"/>
    <id>https://airpoet.github.io/2018/06/10/Poetry/开始/</id>
    <published>2018-06-09T17:03:10.001Z</published>
    <updated>2018-06-09T17:19:40.887Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-09-601528563678_.pic.jpg" alt=""></p><p>月亮落下一两片羽毛在田野上。</p><p>黑暗中的麦子聆听着。</p><p>快静下来。</p><p>快。</p><p>就在那儿，月亮的孩子们正试着</p><p>挥动翅膀。</p><p>在两棵树之间，身材修长的女子抬起面庞，</p><p>美丽的剪影。接着，她步入空中，接着，</p><p>她完全消失在空中。</p><p>我独自站在一棵接骨木旁，不敢呼吸，</p><p>也不敢动。</p><p>我聆听着。</p><p>麦子向后靠着自己的黑暗，</p><p>而我靠着我的。</p><p><br></p><p><strong>作者 / [美国] 詹姆斯·赖特</strong></p><p>翻译 / 张文武</p><hr><h3 id="▍Beginning"><a href="#▍Beginning" class="headerlink" title="▍Beginning"></a><strong>▍Beginning</strong></h3><p><br></p><p>The moon drops one or two feathers into the fields.</p><p>The dark wheat listens.</p><p>Be still.</p><p>Now.</p><p>There they are, the moon’s young, trying</p><p>Their wings.</p><p>Between trees, a slender woman lifts up the lovely shadow</p><p>Of her face, and now she steps into the air, now she is gone</p><p>Wholly, into the air.</p><p>I stand alone by an elder tree, I do not dare breathe</p><p>Or move.</p><p>I listen.</p><p>The wheat leans back toward its own darkness,</p><p>And I lean toward mine.</p><p><br></p><p><strong>Author / James Wright</strong></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-09-601528563678_.pic.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;月
      
    
    </summary>
    
      <category term="文艺" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/"/>
    
      <category term="诗歌" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/%E8%AF%97%E6%AD%8C/"/>
    
    
      <category term="诗歌" scheme="https://airpoet.github.io/tags/%E8%AF%97%E6%AD%8C/"/>
    
  </entry>
  
  <entry>
    <title>Markdown高阶语法</title>
    <link href="https://airpoet.github.io/2018/06/10/Tools/Markdown/Markdown%E9%AB%98%E9%98%B6%E8%AF%AD%E6%B3%95/"/>
    <id>https://airpoet.github.io/2018/06/10/Tools/Markdown/Markdown高阶语法/</id>
    <published>2018-06-09T16:49:50.407Z</published>
    <updated>2018-06-10T13:21:24.745Z</updated>
    
    <content type="html"><![CDATA[<h3 id="时序图的写法"><a href="#时序图的写法" class="headerlink" title="时序图的写法"></a>时序图的写法</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-09-165900.png" alt="image-20180610005859980"></p><div id="sequence-0"></div><h3 id="流程图的写法"><a href="#流程图的写法" class="headerlink" title="流程图的写法"></a>流程图的写法</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-09-165930.png" alt="image-20180610005930019"></p><div id="flowchart-0" class="flow-chart"></div><h3 id="类图的写法"><a href="#类图的写法" class="headerlink" title="类图的写法"></a>类图的写法</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-09-165948.png" alt="image-20180610005948130"></p><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLl2jz4qiA4Wjud98pKi12WC0"></p><p><script src="https://cdnjs.cloudflare.com/ajax/libs/raphael/2.2.7/raphael.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/flowchart/1.6.5/flowchart.min.js"></script><textarea id="flowchart-0-code" style="display: none">st=>start: Start|past:>http://www.google.com[blank]e=>end: End:>http://www.google.comop1=>operation: My Operation|pastop2=>operation: Stuff|currentsub1=>subroutine: My Subroutine|invalidcond=>condition: Yesor No?|approved:>http://www.google.comc2=>condition: Good idea|rejectedio=>inputoutput: catch something...|requestst->op1(right)->condcond(yes, right)->c2cond(no)->sub1(left)->op1c2(yes)->io->ec2(no)->op2->e</textarea><textarea id="flowchart-0-options" style="display: none">{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("flowchart-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value));  var diagram = flowchart.parse(code);  diagram.drawSVG("flowchart-0", options);</script><script src="https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js"></script><script src="https://cdn.bootcss.com/snap.svg/0.5.1/snap.svg-min.js"></script><script src="https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js"></script><script src="https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js"></script><script src="https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js"></script><textarea id="sequence-0-code" style="display: none">mapper->Bob: Hello Bob, how are you?Note right of Bob: Bob thinksBob-->reducer: I am good thanks!ad u?reducer->out: I'm fine tooout->me: ok, you winme-->Bob: nono, not</textarea><textarea id="sequence-0-options" style="display: none">{"theme":"simple"}</textarea><script>  var code = document.getElementById("sequence-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value));  var diagram = Diagram.parse(code);  diagram.drawSVG("sequence-0", options);</script></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;时序图的写法&quot;&gt;&lt;a href=&quot;#时序图的写法&quot; class=&quot;headerlink&quot; title=&quot;时序图的写法&quot;&gt;&lt;/a&gt;时序图的写法&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/study/2018-
      
    
    </summary>
    
      <category term="工具" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="Markdown" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/Markdown/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="工具" scheme="https://airpoet.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="Markdown" scheme="https://airpoet.github.io/tags/Markdown/"/>
    
  </entry>
  
  <entry>
    <title>About Sublime Text3</title>
    <link href="https://airpoet.github.io/2018/06/09/Tools/Sublime/%E5%AE%89%E8%A3%85%E4%B8%BB%E9%A2%98/"/>
    <id>https://airpoet.github.io/2018/06/09/Tools/Sublime/安装主题/</id>
    <published>2018-06-09T04:43:17.189Z</published>
    <updated>2018-06-10T01:09:27.890Z</updated>
    
    <content type="html"><![CDATA[<h3 id="主题"><a href="#主题" class="headerlink" title="主题"></a>主题</h3><p><a href="https://scotch.io/bar-talk/best-sublime-text-3-themes-of-2015-and-2016" target="_blank" rel="noopener">详情参见这个网站</a></p><h3 id="详细操作"><a href="#详细操作" class="headerlink" title="详细操作"></a>详细操作</h3><p><a href="http://zh.lucida.me/blog/sublime-text-complete-guide/" target="_blank" rel="noopener">见此站</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;主题&quot;&gt;&lt;a href=&quot;#主题&quot; class=&quot;headerlink&quot; title=&quot;主题&quot;&gt;&lt;/a&gt;主题&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://scotch.io/bar-talk/best-sublime-text-3-themes-of-2015
      
    
    </summary>
    
      <category term="工具" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="Sublime" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/Sublime/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="工具" scheme="https://airpoet.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="Sublime" scheme="https://airpoet.github.io/tags/Sublime/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce笔记-Bug汇总</title>
    <link href="https://airpoet.github.io/2018/06/09/Hadoop/Study/2-MapReduce/MapReduce-Bug%E6%B1%87%E6%80%BB/"/>
    <id>https://airpoet.github.io/2018/06/09/Hadoop/Study/2-MapReduce/MapReduce-Bug汇总/</id>
    <published>2018-06-09T03:11:48.470Z</published>
    <updated>2018-06-11T11:46:16.724Z</updated>
    
    <content type="html"><![CDATA[<h4 id="1、reduce-输出路径必须是新创建的。不能已经存在"><a href="#1、reduce-输出路径必须是新创建的。不能已经存在" class="headerlink" title="1、reduce 输出路径必须是新创建的。不能已经存在"></a>1、reduce 输出路径必须是新创建的。不能已经存在</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread <span class="string">"main"</span> org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs:<span class="comment">//cs1:9000/flowout01 already exists</span></span><br></pre></td></tr></table></figure><h4 id="2、在初始化-job-的时候，-没有传-conf-，-导致后面一直找不到文件，-因为不知道到哪里去找"><a href="#2、在初始化-job-的时候，-没有传-conf-，-导致后面一直找不到文件，-因为不知道到哪里去找" class="headerlink" title="2、在初始化 job 的时候， 没有传 conf ， 导致后面一直找不到文件， 因为不知道到哪里去找"></a>2、在初始化 job 的时候， 没有传 <code>conf</code> ， 导致后面一直找不到文件， 因为不知道到哪里去找</h4><h4 id="3、Text导包倒错-导的时候要注意"><a href="#3、Text导包倒错-导的时候要注意" class="headerlink" title="3、Text导包倒错, 导的时候要注意"></a>3、Text导包倒错, 导的时候要注意</h4><p><strong>应该是这个 <code>import org.apache.hadoop.io.Text;</code></strong></p><h4 id="4、进行字符串拼接的时候，把-StringBuilder-写到了-reduce-方法外，-这样导致-sb-会越来越多，当然，也可以每次拼接完了清空"><a href="#4、进行字符串拼接的时候，把-StringBuilder-写到了-reduce-方法外，-这样导致-sb-会越来越多，当然，也可以每次拼接完了清空" class="headerlink" title="4、进行字符串拼接的时候，把 StringBuilder 写到了 reduce 方法外， 这样导致 sb 会越来越多，当然，也可以每次拼接完了清空"></a>4、进行字符串拼接的时候，把 <strong>StringBuilder 写到了 reduce 方法外</strong>， 这样导致 sb 会越来越多，当然，也可以每次拼接完了清空</h4><p>类似于这样</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">AF,I,O,K,G,D,C,H,B</span><br><span class="line">BF,I,O,K,G,D,C,H,B,E,J,F,A</span><br><span class="line">CF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F</span><br><span class="line">DF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L</span><br><span class="line">EF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H</span><br><span class="line">FF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G</span><br><span class="line">GF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M</span><br><span class="line">HF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O</span><br><span class="line">IF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C</span><br><span class="line">JF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O</span><br><span class="line">KF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,B</span><br><span class="line">LF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,B,D,E</span><br><span class="line">MF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,B,D,E,E,F</span><br><span class="line">OF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,B,D,E,E,F,A,H,I,J,F</span><br></pre></td></tr></table></figure><h4 id="4、mapreduce执行错误Mapper-错误"><a href="#4、mapreduce执行错误Mapper-错误" class="headerlink" title="4、mapreduce执行错误Mapper.\错误"></a>4、mapreduce执行错误Mapper.\<init>错误</init></h4><ul><li>Mapper &amp; Reducer 写成内部类的时候，有没有加上 <code>static</code></li><li>Bean类有没有无参构造</li></ul><h4 id="5、排序过程中，自定义了排序类，bean类的-compareTo-方法，只写了按照分数大小排序。"><a href="#5、排序过程中，自定义了排序类，bean类的-compareTo-方法，只写了按照分数大小排序。" class="headerlink" title="5、排序过程中，自定义了排序类，bean类的 compareTo()方法，只写了按照分数大小排序。"></a>5、排序过程中，自定义了排序类，bean类的 <code>compareTo()</code>方法，只写了按照分数大小排序。</h4><p><u>会出现如下错误： 课程并没有分组</u></p><p>没有在相同的一组课程中比较分数， 而是比较的所有的分数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">computerhuangjiaju<span class="number">83.2</span></span><br><span class="line">mathhuangxiaoming<span class="number">83.0</span></span><br><span class="line">englishhuanglei<span class="number">83.0</span></span><br><span class="line">mathhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">algorithmhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">computerhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">englishliuyifei<span class="number">74.42857142857143</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><p><strong>此时应该在Bean对象中做如下事情</strong></p><ul><li>相同课程的按照分数降序排序</li><li>课程名按照自然（升序）排序</li><li><strong>换言之，就是CustomBean 对象要输出的数据是 <code>组名升序排序，组内按成绩降序排序</code></strong></li><li><a href="https://airpoet.github.io/2018/06/10/Hadoop/Study/MapReduce-%E5%88%86%E7%BB%84%E6%B5%85%E6%8E%A2/#more">具体分析参阅</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;1、reduce-输出路径必须是新创建的。不能已经存在&quot;&gt;&lt;a href=&quot;#1、reduce-输出路径必须是新创建的。不能已经存在&quot; class=&quot;headerlink&quot; title=&quot;1、reduce 输出路径必须是新创建的。不能已经存在&quot;&gt;&lt;/a&gt;1、red
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce笔记-练习</title>
    <link href="https://airpoet.github.io/2018/06/09/Hadoop/Study/2-MapReduce/MapReduce-%E7%BB%83%E4%B9%A0/"/>
    <id>https://airpoet.github.io/2018/06/09/Hadoop/Study/2-MapReduce/MapReduce-练习/</id>
    <published>2018-06-09T03:11:16.234Z</published>
    <updated>2018-06-11T14:15:19.807Z</updated>
    
    <content type="html"><![CDATA[<h2 id="求微博共同粉丝"><a href="#求微博共同粉丝" class="headerlink" title="求微博共同粉丝"></a>求微博共同粉丝</h2><h3 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h3><p><strong>涉及知识点： 多 Job 串联</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">A:B,C,D,F,E,O</span><br><span class="line">B:A,C,E,K</span><br><span class="line">C:F,A,D,I</span><br><span class="line">D:A,E,F,L</span><br><span class="line">E:B,C,D,M,L</span><br><span class="line">F:A,B,C,D,E,O,M</span><br><span class="line">G:A,C,D,E,F</span><br><span class="line">H:A,C,D,E,O</span><br><span class="line">I:A,O</span><br><span class="line">J:B,O</span><br><span class="line">K:A,C,D</span><br><span class="line">L:D,E,F</span><br><span class="line">M:E,F,G</span><br><span class="line">O:A,H,I,J,K</span><br></pre></td></tr></table></figure><blockquote><p>以上是数据：<br>A:B,C,D,F,E,O<br>表示：A用户 关注B,C,D,E,F,O</p><blockquote><p>求所有两两用户之间的共同关注对象</p></blockquote></blockquote><h3 id="答案："><a href="#答案：" class="headerlink" title="答案："></a>答案：</h3><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLd3EpytDptDBp2jsIStDvt98pKi1IW80"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._01_多Job串联;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CommonFansDemo</span> </span>&#123;</span><br><span class="line"><span class="meta">@SuppressWarnings</span>(<span class="string">"deprecation"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// Job 逻辑</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 HDFS 相关的参数</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// 新建一个 job1</span></span><br><span class="line">Job job1 = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置 Jar 包所在路径</span></span><br><span class="line">job1.setJarByClass(CommonFansDemo.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 mapper 类和 reducer 类</span></span><br><span class="line">job1.setMapperClass(MyMapper_Step1.class);</span><br><span class="line">job1.setReducerClass(MyReducer_Step1.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 maptask 的输出类型</span></span><br><span class="line">job1.setMapOutputKeyClass(Text.class);</span><br><span class="line">job1.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定最终的输出类型(reduce存在时,就是指 ReduceTask 的输出类型)</span></span><br><span class="line">job1.setOutputKeyClass(Text.class);</span><br><span class="line">job1.setOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定该 MapReduce 程序数据的输入输出路径</span></span><br><span class="line">FileInputFormat.setInputPaths(job1, <span class="keyword">new</span> Path(<span class="string">"/in/commonfriend"</span>));</span><br><span class="line">FileOutputFormat.setOutputPath(job1, <span class="keyword">new</span> Path(<span class="string">"/out/job1"</span>));</span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// 新建一个 job2</span></span><br><span class="line">Job job2 = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置 Jar 包所在路径</span></span><br><span class="line">job2.setJarByClass(CommonFansDemo.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 mapper 类和 reducer 类</span></span><br><span class="line">job2.setMapperClass(MyMapper_Step2.class);</span><br><span class="line">job2.setReducerClass(MyReducer_Step2.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 maptask 的输出类型</span></span><br><span class="line">job2.setMapOutputKeyClass(Text.class);</span><br><span class="line">job2.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定最终的输出类型(reduce存在时,就是指 ReduceTask 的输出类型)</span></span><br><span class="line">job2.setOutputKeyClass(Text.class);</span><br><span class="line">job2.setOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定该 MapReduce 程序数据的输入输出路径</span></span><br><span class="line">FileInputFormat.setInputPaths(job2, <span class="keyword">new</span> Path(<span class="string">"/out/job1"</span>));</span><br><span class="line">FileOutputFormat.setOutputPath(job2, <span class="keyword">new</span> Path(<span class="string">"/out/job2"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将多个 job 当做一个组中的 job 提交, 参数名是组名</span></span><br><span class="line"><span class="comment"> * 注意: JobControl 是实现了 Runnable 接口的 </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">JobControl jControl = <span class="keyword">new</span> JobControl(<span class="string">"common_friend"</span>);</span><br><span class="line"><span class="comment">// 将原生的 job携带配置 转换为可控的 job</span></span><br><span class="line">ControlledJob aJob = <span class="keyword">new</span> ControlledJob(job1.getConfiguration());</span><br><span class="line">ControlledJob bJob = <span class="keyword">new</span> ControlledJob(job2.getConfiguration());</span><br><span class="line"><span class="comment">// 添加依赖关系</span></span><br><span class="line">bJob.addDependingJob(aJob);</span><br><span class="line"><span class="comment">// 添加 job 到组中</span></span><br><span class="line">jControl.addJob(aJob);</span><br><span class="line">jControl.addJob(bJob);</span><br><span class="line"><span class="comment">// 启动一个线程</span></span><br><span class="line">Thread jobThread = <span class="keyword">new</span> Thread(jControl);</span><br><span class="line">jobThread.start();</span><br><span class="line"><span class="keyword">while</span> (!jControl.allFinished()) &#123;</span><br><span class="line">Thread.sleep(<span class="number">500</span>);</span><br><span class="line">&#125;</span><br><span class="line">jobThread.stop();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper_Step1</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">String[] user_attentions;</span><br><span class="line">String[] attentions;</span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value,</span></span></span><br><span class="line"><span class="function"><span class="params">Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">user_attentions = value.toString().split(<span class="string">":"</span>);</span><br><span class="line">attentions = user_attentions[<span class="number">1</span>].trim().split(<span class="string">","</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (String att : attentions) &#123;</span><br><span class="line">k.set(att);</span><br><span class="line">v.set(user_attentions[<span class="number">0</span>].trim());</span><br><span class="line">context.write(k, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 将两两粉丝(普通用户)拼接起来, 格式a-f:c =&gt; a,b 都共同关注了 c</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> *  AF,I,O,K,G,D,C,H,B</span></span><br><span class="line"><span class="comment">BE,J,F,A</span></span><br><span class="line"><span class="comment">CB,E,K,A,H,G,F</span></span><br><span class="line"><span class="comment">DH,C,G,F,E,A,K,L</span></span><br><span class="line"><span class="comment">EA,B,L,G,M,F,D,H</span></span><br><span class="line"><span class="comment">FC,M,L,A,D,G</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper_Step2</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">String[] attenion_users;</span><br><span class="line">String[] users;</span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value,</span></span></span><br><span class="line"><span class="function"><span class="params">Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">attenion_users = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">users = attenion_users[<span class="number">1</span>].trim().split(<span class="string">","</span>);</span><br><span class="line"><span class="keyword">for</span> (String u1 : users) &#123;</span><br><span class="line"><span class="keyword">for</span> (String u2 : users) &#123;</span><br><span class="line"><span class="keyword">if</span> (u1.compareTo(u2) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">String users = u1 + <span class="string">"-"</span> + u2;</span><br><span class="line">k.set(users);</span><br><span class="line">v.set(attenion_users[<span class="number">0</span>].trim());</span><br><span class="line">context.write(k, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> *需要统计的是, 某人拥有的全部粉丝</span></span><br><span class="line"><span class="comment"> *  key: 传过来的 key</span></span><br><span class="line"><span class="comment"> *  value:  用,分割 </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer_Step1</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Reducer&lt;Text, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注意: 这里 sb 不能写在外面,会不断的拼接</span></span><br><span class="line">StringBuilder sb = <span class="keyword">new</span> StringBuilder();</span><br><span class="line"><span class="keyword">for</span> (Text v : values) &#123;</span><br><span class="line">sb.append(v.toString()).append(<span class="string">","</span>);</span><br><span class="line">&#125;</span><br><span class="line">k.set(key);</span><br><span class="line">v.set(sb.substring(<span class="number">0</span>, sb.length() - <span class="number">1</span>));</span><br><span class="line">context.write(k, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 拿到的数据: a-b c</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer_Step2</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Reducer&lt;Text, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">StringBuilder sb = <span class="keyword">new</span> StringBuilder();</span><br><span class="line"><span class="keyword">for</span> (Text attention : values) &#123;</span><br><span class="line">sb.append(attention.toString()).append(<span class="string">","</span>);</span><br><span class="line">&#125;</span><br><span class="line">k.set(key);</span><br><span class="line">v.set(sb.substring(<span class="number">0</span>, sb.length() - <span class="number">1</span>));</span><br><span class="line">context.write(k, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// job1的输出</span></span><br><span class="line">AF,I,O,K,G,D,C,H,B</span><br><span class="line">BE,J,F,A</span><br><span class="line">CB,E,K,A,H,G,F</span><br><span class="line">DH,C,G,F,E,A,K,L</span><br><span class="line">EA,B,L,G,M,F,D,H</span><br><span class="line">FC,M,L,A,D,G</span><br><span class="line">GM</span><br><span class="line">HO</span><br><span class="line">IO,C</span><br><span class="line">JO</span><br><span class="line">KO,B</span><br><span class="line">LD,E</span><br><span class="line">ME,F</span><br><span class="line">OA,H,I,J,F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// job2的输出</span></span><br><span class="line">A-BE,C</span><br><span class="line">A-CD,F</span><br><span class="line">A-DF,E</span><br><span class="line">A-EC,D,B</span><br><span class="line">A-FO,B,E,D,C</span><br><span class="line">A-GE,F,D,C</span><br><span class="line">A-HO,E,D,C</span><br><span class="line">A-IO</span><br><span class="line">A-JB,O</span><br><span class="line">A-KD,C</span><br><span class="line">A-LD,F,E</span><br><span class="line">A-ME,F</span><br><span class="line">B-CA</span><br><span class="line">B-DE,A</span><br><span class="line">B-EC</span><br><span class="line">B-FA,E,C</span><br><span class="line">B-GC,A,E</span><br><span class="line">B-HA,E,C</span><br><span class="line">B-IA</span><br><span class="line">B-KC,A</span><br><span class="line">B-LE</span><br><span class="line">B-ME</span><br><span class="line">B-OA,K</span><br><span class="line">C-DA,F</span><br><span class="line">C-ED</span><br><span class="line">C-FD,A</span><br><span class="line">C-GF,A,D</span><br><span class="line">C-HD,A</span><br><span class="line">C-IA</span><br><span class="line">C-KA,D</span><br><span class="line">C-LF,D</span><br><span class="line">C-MF</span><br><span class="line">C-OI,A</span><br><span class="line">D-EL</span><br><span class="line">D-FE,A</span><br><span class="line">D-GA,F,E</span><br><span class="line">D-HE,A</span><br><span class="line">D-IA</span><br><span class="line">D-KA</span><br><span class="line">D-LF,E</span><br><span class="line">D-MF,E</span><br><span class="line">D-OA</span><br><span class="line">E-FC,B,M,D</span><br><span class="line">E-GC,D</span><br><span class="line">E-HC,D</span><br><span class="line">E-JB</span><br><span class="line">E-KD,C</span><br><span class="line">E-LD</span><br><span class="line">F-GA,D,C,E</span><br><span class="line">F-HA,E,C,D,O</span><br><span class="line">F-IO,A</span><br><span class="line">F-JO,B</span><br><span class="line">F-KC,A,D</span><br><span class="line">F-LE,D</span><br><span class="line">F-ME</span><br><span class="line">F-OA</span><br><span class="line">G-HA,C,D,E</span><br><span class="line">G-IA</span><br><span class="line">G-KC,A,D</span><br><span class="line">G-LD,E,F</span><br><span class="line">G-MF,E</span><br><span class="line">G-OA</span><br><span class="line">H-IO,A</span><br><span class="line">H-JO</span><br><span class="line">H-KA,D,C</span><br><span class="line">H-LE,D</span><br><span class="line">H-ME</span><br><span class="line">H-OA</span><br><span class="line">I-JO</span><br><span class="line">I-KA</span><br><span class="line">I-OA</span><br><span class="line">K-LD</span><br><span class="line">K-OA</span><br><span class="line">L-MF,E</span><br></pre></td></tr></table></figure><h2 id="求学生成绩"><a href="#求学生成绩" class="headerlink" title="求学生成绩"></a>求学生成绩</h2><h3 id="题目-1"><a href="#题目-1" class="headerlink" title="题目"></a>题目</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">computer,huangxiaoming,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span></span><br><span class="line">computer,xuzheng,<span class="number">54</span>,<span class="number">52</span>,<span class="number">86</span>,<span class="number">91</span>,<span class="number">42</span></span><br><span class="line">computer,huangbo,<span class="number">85</span>,<span class="number">42</span>,<span class="number">96</span>,<span class="number">38</span></span><br><span class="line">english,zhaobenshan,<span class="number">54</span>,<span class="number">52</span>,<span class="number">86</span>,<span class="number">91</span>,<span class="number">42</span>,<span class="number">85</span>,<span class="number">75</span></span><br><span class="line">english,liuyifei,<span class="number">85</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">21</span>,<span class="number">85</span>,<span class="number">96</span>,<span class="number">14</span></span><br><span class="line">algorithm,liuyifei,<span class="number">75</span>,<span class="number">85</span>,<span class="number">62</span>,<span class="number">48</span>,<span class="number">54</span>,<span class="number">96</span>,<span class="number">15</span></span><br><span class="line">computer,huangjiaju,<span class="number">85</span>,<span class="number">75</span>,<span class="number">86</span>,<span class="number">85</span>,<span class="number">85</span></span><br><span class="line">english,liuyifei,<span class="number">76</span>,<span class="number">95</span>,<span class="number">86</span>,<span class="number">74</span>,<span class="number">68</span>,<span class="number">74</span>,<span class="number">48</span></span><br><span class="line">english,huangdatou,<span class="number">48</span>,<span class="number">58</span>,<span class="number">67</span>,<span class="number">86</span>,<span class="number">15</span>,<span class="number">33</span>,<span class="number">85</span></span><br><span class="line">algorithm,huanglei,<span class="number">76</span>,<span class="number">95</span>,<span class="number">86</span>,<span class="number">74</span>,<span class="number">68</span>,<span class="number">74</span>,<span class="number">48</span></span><br><span class="line">algorithm,huangjiaju,<span class="number">85</span>,<span class="number">75</span>,<span class="number">86</span>,<span class="number">85</span>,<span class="number">85</span>,<span class="number">74</span>,<span class="number">86</span></span><br><span class="line">computer,huangdatou,<span class="number">48</span>,<span class="number">58</span>,<span class="number">67</span>,<span class="number">86</span>,<span class="number">15</span>,<span class="number">33</span>,<span class="number">85</span></span><br><span class="line">english,zhouqi,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span>,<span class="number">75</span>,<span class="number">55</span>,<span class="number">47</span>,<span class="number">22</span></span><br><span class="line">english,huangbo,<span class="number">85</span>,<span class="number">42</span>,<span class="number">96</span>,<span class="number">38</span>,<span class="number">55</span>,<span class="number">47</span>,<span class="number">22</span></span><br><span class="line">algorithm,liutao,<span class="number">85</span>,<span class="number">75</span>,<span class="number">85</span>,<span class="number">99</span>,<span class="number">66</span></span><br><span class="line">computer,huangzitao,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span></span><br><span class="line">math,wangbaoqiang,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span></span><br><span class="line">computer,liujialing,<span class="number">85</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">21</span>,<span class="number">85</span>,<span class="number">96</span>,<span class="number">14</span>,<span class="number">74</span>,<span class="number">86</span></span><br><span class="line">computer,liuyifei,<span class="number">75</span>,<span class="number">85</span>,<span class="number">62</span>,<span class="number">48</span>,<span class="number">54</span>,<span class="number">96</span>,<span class="number">15</span></span><br><span class="line">computer,liutao,<span class="number">85</span>,<span class="number">75</span>,<span class="number">85</span>,<span class="number">99</span>,<span class="number">66</span>,<span class="number">88</span>,<span class="number">75</span>,<span class="number">91</span></span><br><span class="line">computer,huanglei,<span class="number">76</span>,<span class="number">95</span>,<span class="number">86</span>,<span class="number">74</span>,<span class="number">68</span>,<span class="number">74</span>,<span class="number">48</span></span><br><span class="line">english,liujialing,<span class="number">75</span>,<span class="number">85</span>,<span class="number">62</span>,<span class="number">48</span>,<span class="number">54</span>,<span class="number">96</span>,<span class="number">15</span></span><br><span class="line">math,huanglei,<span class="number">76</span>,<span class="number">95</span>,<span class="number">86</span>,<span class="number">74</span>,<span class="number">68</span>,<span class="number">74</span>,<span class="number">48</span></span><br><span class="line">math,huangjiaju,<span class="number">85</span>,<span class="number">75</span>,<span class="number">86</span>,<span class="number">85</span>,<span class="number">85</span>,<span class="number">74</span>,<span class="number">86</span></span><br><span class="line">math,liutao,<span class="number">48</span>,<span class="number">58</span>,<span class="number">67</span>,<span class="number">86</span>,<span class="number">15</span>,<span class="number">33</span>,<span class="number">85</span></span><br><span class="line">english,huanglei,<span class="number">85</span>,<span class="number">75</span>,<span class="number">85</span>,<span class="number">99</span>,<span class="number">66</span>,<span class="number">88</span>,<span class="number">75</span>,<span class="number">91</span></span><br><span class="line">math,xuzheng,<span class="number">54</span>,<span class="number">52</span>,<span class="number">86</span>,<span class="number">91</span>,<span class="number">42</span>,<span class="number">85</span>,<span class="number">75</span></span><br><span class="line">math,huangxiaoming,<span class="number">85</span>,<span class="number">75</span>,<span class="number">85</span>,<span class="number">99</span>,<span class="number">66</span>,<span class="number">88</span>,<span class="number">75</span>,<span class="number">91</span></span><br><span class="line">math,liujialing,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span>,<span class="number">75</span></span><br><span class="line">english,huangxiaoming,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span></span><br><span class="line">algorithm,huangdatou,<span class="number">48</span>,<span class="number">58</span>,<span class="number">67</span>,<span class="number">86</span>,<span class="number">15</span>,<span class="number">33</span>,<span class="number">85</span></span><br><span class="line">algorithm,huangzitao,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span>,<span class="number">75</span></span><br></pre></td></tr></table></figure><blockquote><p>一、数据解释</p><p>数据字段个数不固定：<br>第一个是课程名称，总共四个课程，computer，math，english，algorithm，<br>第二个是学生姓名，后面是每次考试的分数</p><p>二、统计需求：<br>1、统计每门课程的参加考试人数和课程平均分</p><p>2、统计每门课程参考学生的平均分，并且按课程存入不同的结果文件，要求一门课程一个结果文件</p><p>3、求出每门课程参考学生成绩最高平均分的学生的信息：课程，姓名和平均分</p></blockquote><h3 id="答案"><a href="#答案" class="headerlink" title="答案"></a>答案</h3><h4 id="第1小题"><a href="#第1小题" class="headerlink" title="第1小题"></a>第1小题</h4><p><strong>统计每门课程的参考人数和课程平均分</strong></p><p><strong>涉及知识点: 去重， 自定义类</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//  ScoreBean </span></span><br><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._02_分组组件;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.AllArgsConstructor;</span><br><span class="line"><span class="keyword">import</span> lombok.Getter;</span><br><span class="line"><span class="keyword">import</span> lombok.NoArgsConstructor;</span><br><span class="line"><span class="keyword">import</span> lombok.Setter;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Getter</span></span><br><span class="line"><span class="meta">@Setter</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ScoreBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">ScoreBean</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> String courseName;</span><br><span class="line"><span class="keyword">private</span> String stuName; </span><br><span class="line"><span class="keyword">private</span> Double score;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">out.writeUTF(courseName);</span><br><span class="line">out.writeUTF(stuName);</span><br><span class="line">out.writeDouble(score);</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.courseName = in.readUTF();</span><br><span class="line"><span class="keyword">this</span>.stuName = in.readUTF();</span><br><span class="line"><span class="keyword">this</span>.score = in.readDouble();</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 如果是相同课程, 按照分数降序排列的</span></span><br><span class="line"><span class="comment"> * 如果是不同课程, 按照课程名称升序排列</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(ScoreBean o)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 测试一下只写按分数降序排序</span></span><br><span class="line"><span class="comment">//return o.getScore().compareTo(this.getScore());</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*// 首先分组(只在相同的组内进行比较)</span></span><br><span class="line"><span class="comment">int nameRes = this.getCourseName().compareTo(o.getCourseName());</span></span><br><span class="line"><span class="comment">if (nameRes == 0) &#123;</span></span><br><span class="line"><span class="comment">// 课程相同的时候才进行降序排序</span></span><br><span class="line"><span class="comment">int scoreRes = </span></span><br><span class="line"><span class="comment">return scoreRes;</span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment">return nameRes;*/</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString1</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> stuName + <span class="string">"\t"</span> + score;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> courseName + <span class="string">"\t"</span> + stuName</span><br><span class="line">+ <span class="string">"\t"</span> + score;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ScoreBean</span><span class="params">(String stuName, Double score)</span> </span>&#123;</span><br><span class="line"><span class="keyword">super</span>();</span><br><span class="line"><span class="keyword">this</span>.stuName = stuName;</span><br><span class="line"><span class="keyword">this</span>.score = score;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//  ScorePlusDemo1 </span></span><br><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._02_分组组件;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.HashSet;</span><br><span class="line"><span class="keyword">import</span> java.util.Set;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ScorePlusDemo1</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line">job.setJarByClass(ScorePlusDemo1.class);</span><br><span class="line"></span><br><span class="line">job.setMapperClass(MyMapper.class);</span><br><span class="line">job.setReducerClass(MyReducer.class);</span><br><span class="line"></span><br><span class="line">job.setMapOutputKeyClass(Text.class);</span><br><span class="line">job.setMapOutputValueClass(ScoreBean.class);</span><br><span class="line"></span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">String inP = <span class="string">"/in/newScoreIn"</span>;</span><br><span class="line">String outP = <span class="string">"/out/ans1"</span>;</span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(inP));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(outP));</span><br><span class="line"></span><br><span class="line">        Path mypath = <span class="keyword">new</span> Path(outP);</span><br><span class="line">        FileSystem hdfs = mypath.getFileSystem(conf);</span><br><span class="line">        <span class="keyword">if</span> (hdfs.isDirectory(mypath)) &#123;</span><br><span class="line">            hdfs.delete(mypath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">Boolean waitForComp = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">System.exit(waitForComp?<span class="number">0</span>:<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">ScoreBean</span>&gt;  </span>&#123;</span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">// 1.截取</span></span><br><span class="line">String[] datas = value.toString().trim().split(<span class="string">","</span>);</span><br><span class="line">String courseName = datas[<span class="number">0</span>].trim();</span><br><span class="line">String stuName = datas[<span class="number">1</span>].trim();</span><br><span class="line"><span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;datas.length; i++) &#123;</span><br><span class="line">sum += Integer.parseInt(datas[i]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">double</span> avgScore = sum/(datas.length-<span class="number">2</span>);</span><br><span class="line">ScoreBean sb = <span class="keyword">new</span> ScoreBean(courseName, stuName, avgScore);</span><br><span class="line">k.set(courseName);</span><br><span class="line">context.write(k, sb);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">ScoreBean</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;ScoreBean&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Reducer&lt;Text, ScoreBean, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">Set&lt;String&gt; stuNames = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line"><span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (ScoreBean sb : values) &#123;</span><br><span class="line">stuNames.add(sb.getStuName());</span><br><span class="line">count ++;</span><br><span class="line">sum += sb.getScore();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">int</span> size = stuNames.size();</span><br><span class="line">String val = size + <span class="string">"\t"</span> + (<span class="keyword">double</span>)sum/count;</span><br><span class="line">v.set(val);</span><br><span class="line">context.write(key, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行结果 </span></span><br><span class="line">algorithm<span class="number">6</span><span class="number">71.33333333333333</span></span><br><span class="line">computer<span class="number">10</span><span class="number">69.6</span></span><br><span class="line">english<span class="number">8</span><span class="number">66.0</span></span><br><span class="line">math<span class="number">7</span><span class="number">72.57142857142857</span></span><br></pre></td></tr></table></figure><h4 id="第2小题"><a href="#第2小题" class="headerlink" title="第2小题"></a>第2小题</h4><p><strong>统计每门课程参考学生的平均分，并且按课程存入不同的结果文件，要求一门课程一个结果文件</strong></p><p><strong>涉及知识点： 分区, 字符串组合key， Partitioner</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._02_分组组件;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.DoubleWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 注意: 此题因为数据中有2条 course 和 stuName相同的数据(english liuyifei), 所以必须再在reduce中继续去重一下, 再计算一下平均分</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 否则, 可以不用写reduce, 因为Mapper中已经把逻辑处理完了,可以直接输出</span></span><br><span class="line"><span class="comment"> </span></span><br><span class="line"><span class="comment"> * 最终输出: </span></span><br><span class="line"><span class="comment"> * computer liuyifei 43</span></span><br><span class="line"><span class="comment"> * computer huanglei 63</span></span><br><span class="line"><span class="comment"> * math liutao   64</span></span><br><span class="line"><span class="comment"> * ...</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ScorePlusDemo2</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定HDFS相关参数</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//  创建/配置 Job</span></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Jar包类型</span></span><br><span class="line">job.setJarByClass(ScorePlusDemo2.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Map Reduce执行类</span></span><br><span class="line">job.setMapperClass(MyMapper.class);</span><br><span class="line">job.setReducerClass(MyReducer.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Map输出类</span></span><br><span class="line">job.setMapOutputKeyClass(Text.class);</span><br><span class="line">job.setMapOutputValueClass(DoubleWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Reduce输出类</span></span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(DoubleWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//  设置分区 </span></span><br><span class="line">job.setPartitionerClass(MyPartition.class);</span><br><span class="line">job.setNumReduceTasks(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置输入 输出路径</span></span><br><span class="line">String inP = <span class="string">"/in/newScoreIn"</span>;</span><br><span class="line">String outP = <span class="string">"/out/scorePlus2"</span>;</span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(inP));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(outP));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置如果存在路径就删除</span></span><br><span class="line">        Path mypath = <span class="keyword">new</span> Path(outP);</span><br><span class="line">        FileSystem hdfs = mypath.getFileSystem(conf);</span><br><span class="line">        <span class="keyword">if</span> (hdfs.isDirectory(mypath)) &#123;</span><br><span class="line">            hdfs.delete(mypath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//  执行job</span></span><br><span class="line">        <span class="keyword">boolean</span> waitForCompletion = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(waitForCompletion?<span class="number">0</span>:-<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line">    </span><br><span class="line">===============================================================</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">DoubleWritable</span>&gt; </span>&#123;</span><br><span class="line"><span class="comment">// 把 课程+学生 作为 key</span></span><br><span class="line"></span><br><span class="line">Text k = <span class="keyword">new</span> Text();  <span class="comment">//只有输出String类型的, 才需要在这里设置Text</span></span><br><span class="line">DoubleWritable v = <span class="keyword">new</span> DoubleWritable();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">String[] datas = value.toString().trim().split(<span class="string">","</span>);</span><br><span class="line">String kStr = datas[<span class="number">0</span>].trim() + <span class="string">"\t"</span> + datas[<span class="number">1</span>].trim();</span><br><span class="line"><span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>; i &lt; datas.length; i++) &#123;</span><br><span class="line">sum += Integer.parseInt(datas[i]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">double</span> avg = sum / (datas.length - <span class="number">2</span>);</span><br><span class="line">k.set(kStr);</span><br><span class="line">v.set(avg);</span><br><span class="line">context.write(k, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">===============================================================</span><br><span class="line">    </span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 注意: 此题因为数据中有2条 course 和 stuName相同的数据, 所以必须再在reduce中</span></span><br><span class="line"><span class="comment"> * 继续去重一下, 再计算一下平均分</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 否则, 可以不用写reduce, 因为Mapper中已经把逻辑处理完了,可以直接输出</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">DoubleWritable</span>, <span class="title">Text</span>, <span class="title">DoubleWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">DoubleWritable v = <span class="keyword">new</span> DoubleWritable();</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;DoubleWritable&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Reducer&lt;Text, DoubleWritable, Text, DoubleWritable&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 考虑到有 课程, 学生名相同, 后面的数据不同的情况, 这里再做一个平均求和</span></span><br><span class="line"><span class="comment"> * 可以验证打印下</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">Double sum = <span class="number">0.0</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (DoubleWritable avg : values) &#123;</span><br><span class="line"><span class="keyword">if</span> (count &gt; <span class="number">0</span>) &#123;</span><br><span class="line"><span class="comment">// 有key完全相同的情况才会进到这里</span></span><br><span class="line">System.out.println(<span class="string">"这是第"</span> +count +<span class="string">"次, 说明课程和姓名有相同的两条数据\n课程姓名是: "</span>+key.toString());</span><br><span class="line">&#125;</span><br><span class="line">sum += avg.get();</span><br><span class="line">count ++;</span><br><span class="line">&#125;</span><br><span class="line">Double finAvg = sum/count;</span><br><span class="line">v.set(finAvg);</span><br><span class="line">context.write(key, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">===============================================================</span><br><span class="line">===============================================================</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 继承 Partitioner, 实现自定义分区</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyPartition</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">DoubleWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> HashMap&lt;String, Integer&gt; courseMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line">courseMap.put(<span class="string">"algorithm"</span>, <span class="number">0</span>);</span><br><span class="line">courseMap.put(<span class="string">"computer"</span>, <span class="number">1</span>);</span><br><span class="line">courseMap.put(<span class="string">"english"</span>, <span class="number">2</span>);</span><br><span class="line">courseMap.put(<span class="string">"math"</span>, <span class="number">3</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text key, DoubleWritable value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 取出Map输出的key中的前半部分--courseName</span></span><br><span class="line">Integer code = courseMap.get(key.toString().trim().split(<span class="string">"\t"</span>)[<span class="number">0</span>]);</span><br><span class="line"><span class="keyword">if</span> (code != <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="keyword">return</span> code;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="number">5</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">===============================================================</span><br><span class="line"> ===============================================================</span><br><span class="line">    </span><br><span class="line"> 执行结果 </span><br><span class="line">algorithmhuangdatou<span class="number">56.0</span></span><br><span class="line">algorithmhuangjiaju<span class="number">82.0</span></span><br><span class="line">algorithmhuanglei<span class="number">74.0</span></span><br><span class="line">algorithmhuangzitao<span class="number">72.0</span></span><br><span class="line">algorithmliutao<span class="number">82.0</span></span><br><span class="line">algorithmliuyifei<span class="number">62.0</span></span><br><span class="line">----------</span><br><span class="line">computerhuangbo<span class="number">65.0</span></span><br><span class="line">computerhuangdatou<span class="number">56.0</span></span><br><span class="line">computerhuangjiaju<span class="number">83.0</span></span><br><span class="line">computerhuanglei<span class="number">74.0</span></span><br><span class="line">computerhuangxiaoming<span class="number">72.0</span></span><br><span class="line">computerhuangzitao<span class="number">72.0</span></span><br><span class="line">computerliujialing<span class="number">64.0</span></span><br><span class="line">computerliutao<span class="number">83.0</span></span><br><span class="line">computerliuyifei<span class="number">62.0</span></span><br><span class="line">computerxuzheng<span class="number">65.0</span></span><br><span class="line">---------</span><br><span class="line">englishhuangbo<span class="number">55.0</span></span><br><span class="line">englishhuangdatou<span class="number">56.0</span></span><br><span class="line">englishhuanglei<span class="number">83.0</span></span><br><span class="line">englishhuangxiaoming<span class="number">72.0</span></span><br><span class="line">englishliujialing<span class="number">62.0</span></span><br><span class="line">englishliuyifei<span class="number">66.5</span></span><br><span class="line">englishzhaobenshan<span class="number">69.0</span></span><br><span class="line">englishzhouqi<span class="number">64.0</span></span><br><span class="line">------------</span><br><span class="line">mathhuangjiaju<span class="number">82.0</span></span><br><span class="line">mathhuanglei<span class="number">74.0</span></span><br><span class="line">mathhuangxiaoming<span class="number">83.0</span></span><br><span class="line">mathliujialing<span class="number">72.0</span></span><br><span class="line">mathliutao<span class="number">56.0</span></span><br><span class="line">mathwangbaoqiang<span class="number">72.0</span></span><br><span class="line">mathxuzheng<span class="number">69.0</span></span><br></pre></td></tr></table></figure><h4 id="第3小题"><a href="#第3小题" class="headerlink" title="第3小题"></a>第3小题</h4><p>求出 <strong><em>每门课程</em></strong><sup>①</sup>参与考试的学生成绩 <strong><em>最高平局分<sup>②</sup></em></strong>   的学生的信息：<u>课程，姓名和平均分</u></p><p><br></p><p><strong>解题思路：</strong> </p><ul><li>通过题意得出<strong>2个结论</strong><ul><li><strong>课程要分组</strong></li><li><strong>平均分要排序</strong></li></ul></li><li>排序的话，交给<strong>key</strong>来做无疑是最好的，因为<strong>MapReduce</strong>会<strong>自动</strong>对<strong>key</strong>进行<strong>分组&amp;排序</strong></li><li>因此可以把 <code>课程&amp;平均分</code> 作为一个<strong>联合key</strong></li><li>为了操作方便，可以<strong>封装到一个对象</strong>中去： <strong>ScoreBean</strong></li><li><strong>分组和排序</strong>需要在 <strong>ScoreBean</strong>重写的<strong><code>compareTo()</code>方法中完成</strong></li><li>因为最后结果是求<strong>每门课程</strong>的<strong>最高平均分</strong>，因此需要对课程进行分组。</li><li>此时原本的默认分组（以Bean对象整体分组）就不管用了，需要<strong>自定义分组</strong></li><li>自定义分组要<strong>继承<code>WritableComparator</code></strong>，重写<code>compare()</code>方法，指定分组的规则。</li><li><strong>ScoreBean</strong>先按照组别进行排序，到<strong>reduce</strong>中时，已经是按照组，排好的数据，<strong>MyGroup</strong> 会把相同的比较结果放到同一个组中，分发到<strong>reduce</strong>.</li><li><strong>reduce</strong>中，只需要取出每组的第一个元素输出到上下文即可</li></ul><p><br></p><p><strong>图示</strong></p><div id="sequence-0"></div><p><br></p><p><strong>涉及知识点： mr中key的作用，自定义对象的用法，自定义分组，mr的执行流程</strong></p><ul><li>利用“班级和平均分”作为 key，可以将 map 阶段读取到的所有学生成绩数据按照班级 和成绩排倒序，发送到 reduce</li><li>在 reduce 端利用 GroupingComparator 将班级相同的 kv 聚合成组，然后取第一个即是最 大值</li></ul><p><br></p><h5 id="先贴个结论："><a href="#先贴个结论：" class="headerlink" title="先贴个结论："></a><strong>先贴个结论：</strong></h5><p><strong>执行流程结论</strong></p><ul><li>map每读一行就 write 到 context 一次，按照指定的<code>key</code>进行分发</li><li><p>map 把所有的数据都读完了之后，大概执行到<code>67%</code>的时候，开始进入 <code>CustomBean</code>，执行<code>CustomBean</code>的<code>compareTo()</code>方法，会按照自己写的规则一条一条数据比较</p></li><li><p>上述都比较完毕之后，map阶段就结束了，此时来到了 reduce阶段，但是是到了<code>67%</code>了</p></li><li>到了reduce阶段，直接进入了<strong>MyGroup</strong>中自定义的<strong>compare</strong>方法。</li><li>MyGroup的<code>compare()</code>方法，如果返回<strong>非0</strong>， 就会进入 reduce 方法<strong>写出到context</strong></li></ul><p><strong>MyGroup进入Reduce的条件是</strong></p><ul><li>MyReduce中，如果compare的结果不等于0，也就是比较的2者不相同， 此时就进入Reduce， 写出到上下文</li><li>如果相同，会一直往下读，直到读到不同的， 此时写出读到上下文。</li><li>因为MyGroup会在Reduce阶段执行，而<code>CustomBean</code>中的<code>compareTo()</code>是在map阶段执行，所以需要在<code>CustomBean</code>中就把组排好序，此时<strong>分组功能</strong>才能<strong>正常运作</strong></li></ul><p><strong>指定分组类MyGroup和不指定的区别</strong></p><p><u>指定与不指定是指：在Driver类中，是否加上<code>job.setGroupingComparatorClass(MyGrouper.class);</code>这一句。</u></p><ul><li><strong>指定分组类</strong>：<ul><li>会按照分组类中，自定义的<code>compare()</code>方法比较，相同的为一组，分完一组就进入一次reduce方法</li></ul></li><li><strong>不指定分组类：（目前存疑）</strong><ul><li>是否是按照key进行分组</li><li>如果是自定义类为key，是否是按照此key中值相同的分为一组</li><li>如果是hadoop内置类，是否是按照此类的值分组（Text-String的值，IntWritable-int值等..）</li><li><strong>依然是走得以上这套分组逻辑，一组的数据读完才进入到Reduce阶段做归并</strong></li></ul></li></ul><p><br></p><h5 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ScoreBean2 </span></span><br><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._02_分组组件;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.Getter;</span><br><span class="line"><span class="keyword">import</span> lombok.Setter;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Getter</span></span><br><span class="line"><span class="meta">@Setter</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ScoreBean2</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">ScoreBean2</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> String courseName;</span><br><span class="line"><span class="keyword">private</span> Double score;</span><br><span class="line">    </span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">out.writeUTF(courseName);</span><br><span class="line">out.writeDouble(score);</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.courseName = in.readUTF();</span><br><span class="line"><span class="keyword">this</span>.score = in.readDouble();</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 如果是相同课程, 按照分数降序排列的</span></span><br><span class="line"><span class="comment"> * 如果是不同课程, 按照课程名称升序排列</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(ScoreBean2 o)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 测试一下只写按分数降序排序</span></span><br><span class="line"><span class="comment">//return o.getScore().compareTo(this.getScore());</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 首先分组(只在相同的组内进行比较)</span></span><br><span class="line"><span class="keyword">int</span> nameRes = <span class="keyword">this</span>.getCourseName().compareTo(o.getCourseName());</span><br><span class="line"><span class="keyword">if</span> (nameRes == <span class="number">0</span>) &#123;</span><br><span class="line"><span class="comment">// 课程相同的时候才进行降序排序</span></span><br><span class="line"><span class="keyword">int</span> scoreRes = o.getScore().compareTo(<span class="keyword">this</span>.getScore());</span><br><span class="line"><span class="keyword">return</span> scoreRes;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> nameRes;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 实际上ScoreBean中是包含所有的参数的, 这里的输出可以自己设置</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> courseName + <span class="string">"\t"</span> + score;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ScoreBean2</span><span class="params">(String courseName, Double score)</span> </span>&#123;</span><br><span class="line"><span class="keyword">super</span>();</span><br><span class="line"><span class="keyword">this</span>.courseName = courseName;</span><br><span class="line"><span class="keyword">this</span>.score = score;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ScoreBean2</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">super</span>();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// ScorePlusDemo3 </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._02_分组组件;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparator;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ScorePlusDemo3</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line"> main     </span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line">job.setJarByClass(ScorePlusDemo3.class);</span><br><span class="line"></span><br><span class="line">job.setMapperClass(MyMapper.class);</span><br><span class="line">job.setReducerClass(MyReducer.class);</span><br><span class="line"></span><br><span class="line">job.setMapOutputKeyClass(ScoreBean2.class);</span><br><span class="line">job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">job.setGroupingComparatorClass(MyGrouper.class);</span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">String outP = <span class="string">"/out/scorePlus3"</span>;</span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"/in/newScoreIn"</span>));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(outP));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果输出目录存在,就先删除</span></span><br><span class="line">Path myPath = <span class="keyword">new</span> Path(outP);</span><br><span class="line">FileSystem fs = myPath.getFileSystem(conf);</span><br><span class="line"><span class="keyword">if</span> (fs.isDirectory(myPath)) &#123;</span><br><span class="line">fs.delete(myPath, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">boolean</span> waitForCompletion = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">System.exit(waitForCompletion ? <span class="number">0</span> : -<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"> Mapper </span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 输出: key: course</span></span><br><span class="line"><span class="comment"> *     value: score ...</span></span><br><span class="line"><span class="comment"> * 思路:</span></span><br><span class="line"><span class="comment"> * 1.不同课程要分开展示, 以 课程+分数 作为key, 在mapper中完成排序 </span></span><br><span class="line"><span class="comment"> * 2.在reduce中按照 MyGrouper 完成分组</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span></span></span><br><span class="line"><span class="class"><span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">ScoreBean2</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> String[] datas;</span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">datas = value.toString().trim().split(<span class="string">","</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>; i &lt; datas.length; i++) &#123;</span><br><span class="line">sum += Integer.parseInt(datas[i]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">double</span> avg = (<span class="keyword">double</span>) sum / (datas.length - <span class="number">2</span>);</span><br><span class="line">ScoreBean2 sb = <span class="keyword">new</span> ScoreBean2(datas[<span class="number">0</span>].trim(), avg);</span><br><span class="line"></span><br><span class="line">v.set(datas[<span class="number">1</span>].trim());</span><br><span class="line">context.write(sb, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"> Redecer     </span><br><span class="line"><span class="keyword">static</span> <span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span></span></span><br><span class="line"><span class="class"><span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">ScoreBean2</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line"><span class="keyword">int</span> count = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(ScoreBean2 key, Iterable&lt;Text&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 如果没有其它问题</span></span><br><span class="line"><span class="comment"> * 此时是按照课程分好组了, 同一个课程的所有学生都过来了, 并且学生成绩是排好的,</span></span><br><span class="line"><span class="comment"> * 如果此时求最大值, 只需要取出第一个即可 </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">// 进来一次只取第一个</span></span><br><span class="line">Text name = values.iterator().next();</span><br><span class="line">k.set(key.getCourseName() + <span class="string">"\t"</span> + name.toString() + <span class="string">"\t"</span></span><br><span class="line">+ key.getScore());</span><br><span class="line">context.write(k, NullWritable.get());</span><br><span class="line">context.write(<span class="keyword">new</span> Text(<span class="string">"==================第"</span>+count+<span class="string">"次进入reduce"</span>), NullWritable.get());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/*context.write(new Text("==================第"+count+"次进入reduce"), NullWritable.get());</span></span><br><span class="line"><span class="comment">for (Text name : values) &#123;</span></span><br><span class="line"><span class="comment">k.set(key.getCourseName() + "\t" + name.toString() + "\t"</span></span><br><span class="line"><span class="comment">+ key.getScore());</span></span><br><span class="line"><span class="comment">context.write(k, NullWritable.get());</span></span><br><span class="line"><span class="comment">context.write(new Text("---------in for write------"), NullWritable.get());</span></span><br><span class="line"><span class="comment">&#125;*/</span></span><br><span class="line">count++;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"> MyGrouper </span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 自定义分组  需要继承一个类WritableComparator</span></span><br><span class="line"><span class="comment"> * 重写compare方法</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyGrouper</span> <span class="keyword">extends</span> <span class="title">WritableComparator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// WritableComparator 此方法的默认无参构造是不会创建对象的, 需要自己重写</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">MyGrouper</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="comment">// 中间省去的参数是 Configuration, 如果为空, 会创建一个新的</span></span><br><span class="line"><span class="keyword">super</span>(ScoreBean2.class, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 此处比较的是2个  WritableComparable 对象, 需要强转一下具体的类对象</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@SuppressWarnings</span>(<span class="string">"rawtypes"</span>)</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> </span>&#123;</span><br><span class="line">ScoreBean2 aBean = (ScoreBean2) a;</span><br><span class="line">ScoreBean2 bBean = (ScoreBean2) b;</span><br><span class="line"><span class="comment">// 返回分组规则</span></span><br><span class="line">System.out.println(aBean.getCourseName()+<span class="string">"---MyGroup中比较---"</span>+(bBean.getCourseName()));</span><br><span class="line"><span class="keyword">return</span> aBean.getCourseName().compareTo(bBean.getCourseName());</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">================================================================================</span><br><span class="line"> 执行结果 </span><br><span class="line">================================================================================</span><br><span class="line"></span><br><span class="line">algorithmhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">==================第<span class="number">1</span>次进入reduce</span><br><span class="line">computerhuangjiaju<span class="number">83.2</span></span><br><span class="line">==================第<span class="number">2</span>次进入reduce</span><br><span class="line">englishhuanglei<span class="number">83.0</span></span><br><span class="line">==================第<span class="number">3</span>次进入reduce</span><br><span class="line">mathhuangxiaoming<span class="number">83.0</span></span><br><span class="line">==================第<span class="number">4</span>次进入reduce</span><br></pre></td></tr></table></figure><h2 id="MR实现两个表的数据关联Join"><a href="#MR实现两个表的数据关联Join" class="headerlink" title="MR实现两个表的数据关联Join"></a>MR实现两个表的数据关联<code>Join</code></h2><h3 id="题目-2"><a href="#题目-2" class="headerlink" title="题目"></a>题目</h3><blockquote><p>订单数据表t_order：  flag=0<br>id    date    pid    amount<br>1001    20150710    P0001    2<br>1002    20150710    P0001    3<br>1003    20150710    P0002    3<br>Id:数据记录id<br>Date   日期<br>Pid     商品id<br>Amount    库存数量</p><p>6.商品信息表t_product   flag=1<br>pid    name    category_id    price<br>P0001    小米5    C01    2000<br>P0002    锤子T1    C01    3500</p><p>mr实现两个表的数据关联<br>id   pid    date    amount    name    category_id     price</p></blockquote><p><br></p><h3 id="答案1-Reducer-端-实现-Join"><a href="#答案1-Reducer-端-实现-Join" class="headerlink" title="答案1 : Reducer 端 实现 Join"></a>答案1 : Reducer 端 实现 <code>Join</code></h3><h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><ul><li><p>map端</p><ul><li><p>读取到当前路径下，所有文件的切片信息， 根据文件名判断是那张表</p><ul><li><p>在setup中，从文件切片中获取到文件名</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取读取到的切片相关信息,一个切片对应一个 maptask</span></span><br><span class="line">InputSplit inputSplit = context.getInputSplit();</span><br><span class="line"><span class="comment">// 转换为文件切片</span></span><br><span class="line">FileSplit fs = (FileSplit)inputSplit;</span><br><span class="line"><span class="comment">// 获取文件名</span></span><br><span class="line">filename = fs.getPath().getName();</span><br></pre></td></tr></table></figure></li><li><p>这里总共会获得2个文件名（指定目录存了2个指定文件），一个文件名对应一个切片</p></li></ul></li><li><p>关联字段作为key， 其它的作为value，在value前面加上当前文件的名称标记</p></li></ul></li><li><p>reduce端</p><ul><li>通过标记区分两张表，把读取到的信息，分别存入2个list中</li><li>遍历大的表，与小表进行拼接（小表的相同pid记录只会有一条）</li><li>拼接完成后即可写出</li></ul></li></ul><h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._03_join2表的数据关联;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReduceJoinDemo</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 指定HDFS相关参数</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//  创建/配置 Job</span></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Jar包类型</span></span><br><span class="line">job.setJarByClass(ReduceJoinDemo.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Map Reduce执行类</span></span><br><span class="line">job.setMapperClass(MyMapper.class);</span><br><span class="line">job.setReducerClass(MyReducer.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Map输出类</span></span><br><span class="line">job.setMapOutputKeyClass(Text.class);</span><br><span class="line">job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Reduce输出类</span></span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置输入 输出路径</span></span><br><span class="line">String inP = <span class="string">"/in/joindemo"</span>;</span><br><span class="line">String outP = <span class="string">"/out/joinout1"</span>;</span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(inP));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(outP));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置如果存在路径就删除</span></span><br><span class="line">        Path mypath = <span class="keyword">new</span> Path(outP);</span><br><span class="line">        FileSystem hdfs = mypath.getFileSystem(conf);</span><br><span class="line">        <span class="keyword">if</span> (hdfs.isDirectory(mypath)) &#123;</span><br><span class="line">            hdfs.delete(mypath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//  执行job</span></span><br><span class="line">        <span class="keyword">boolean</span> waitForCompletion = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(waitForCompletion?<span class="number">0</span>:-<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 思路: 读取2个表中的数据,进行标记发送</span></span><br><span class="line"><span class="comment"> * key: 两表需要关联的字段</span></span><br><span class="line"><span class="comment"> * value: 其它值, 需要标记， 标记数据的来源</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * **核心： 关联条件**</span></span><br><span class="line"><span class="comment">- 想要在 reduce 端完成 join， 要在 reduce 端可以同时接收到两个表中的数据</span></span><br><span class="line"><span class="comment">- 要保证在 Map 端进行读文件的时候， 读到2个表的数据， 并且需要对2个表的数据进行区分</span></span><br><span class="line"><span class="comment">- 将2个表放在同一个目录下</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">解决: </span></span><br><span class="line"><span class="comment">mapper 开始执行时, 在setup方法中, 从上下文中取到文件名, 根据文件名打标记</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">String filename = <span class="string">""</span>;</span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">// 获取读取到的切片相关信息,一个切片对应一个 maptask</span></span><br><span class="line">InputSplit inputSplit = context.getInputSplit();</span><br><span class="line"><span class="comment">// 转换为文件切片</span></span><br><span class="line">FileSplit fs = (FileSplit)inputSplit;</span><br><span class="line"><span class="comment">// 获取文件名</span></span><br><span class="line">filename = fs.getPath().getName();</span><br><span class="line">System.out.println(<span class="string">"本次获取到的文件名为-----"</span>+filename);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value,</span></span></span><br><span class="line"><span class="function"><span class="params">Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">// 解析出来每一行内容, 打标记, 发送</span></span><br><span class="line">String[] infos = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (filename.equals(<span class="string">"order"</span>)) &#123;</span><br><span class="line">k.set(infos[<span class="number">2</span>]);</span><br><span class="line"><span class="comment">// 设置标记前缀为 OR</span></span><br><span class="line">v.set(<span class="string">"OR"</span>+infos[<span class="number">0</span>]+<span class="string">"\t"</span>+infos[<span class="number">1</span>]+<span class="string">"\t"</span>+infos[<span class="number">3</span>]);</span><br><span class="line">&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">k.set(infos[<span class="number">0</span>]);</span><br><span class="line"><span class="comment">// 设置标记前缀为 PR</span></span><br><span class="line">v.set(<span class="string">"PR"</span>+infos[<span class="number">1</span>]+<span class="string">"\t"</span>+infos[<span class="number">2</span>]+<span class="string">"\t"</span>+infos[<span class="number">3</span>]);</span><br><span class="line">&#125;</span><br><span class="line">context.write(k, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Reducer&lt;Text, Text, Text, NullWritable&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 首先明确 product 和 order 是 一对多的关系</span></span><br><span class="line"><span class="comment"> * 根据前缀不同,取到2个不同的表存进2个容器中</span></span><br><span class="line"><span class="comment"> * 遍历多的表, 与一进行拼接</span></span><br><span class="line"><span class="comment"> * 最后写出到上下文</span></span><br><span class="line"><span class="comment"> * 最终的输出格式 id   pid    date    amount    name    category_id     price</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">// 因为每次遍历到不同的pid, 都会走进来一次, list也会有新的输出,所以必须定义在里面,每次进来都要初始化</span></span><br><span class="line">List&lt;String&gt; productList =<span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">List&lt;String&gt; orderList =<span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (Text v : values) &#123;</span><br><span class="line">String vStr = v.toString();</span><br><span class="line"><span class="keyword">if</span> (vStr.startsWith(<span class="string">"OR"</span>)) &#123;</span><br><span class="line">orderList.add(vStr.substring(<span class="number">2</span>));</span><br><span class="line">&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">productList.add(vStr.substring(<span class="number">2</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 此时2个list添加完了本次 相同的 key(pid) 的所有商品</span></span><br><span class="line"><span class="comment">// 遍历多的进行拼接</span></span><br><span class="line"><span class="keyword">for</span> (String or : orderList) &#123;</span><br><span class="line"><span class="comment">// 相同的 pid的 product 只有一个, productList中的数量是1</span></span><br><span class="line"><span class="comment">// 但是相同pid 的 订单 可能有多个</span></span><br><span class="line">String res =  key.toString() + <span class="string">"\t"</span> + or + productList.get(<span class="number">0</span>);</span><br><span class="line">k.set(res);</span><br><span class="line">context.write(k, NullWritable.get());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><br></p><h3 id="※-答案2-：-Mapper-端实现-Join-※"><a href="#※-答案2-：-Mapper-端实现-Join-※" class="headerlink" title="※ 答案2 ： Mapper 端实现 Join  ※"></a>※ 答案2 ： Mapper 端实现 <code>Join</code>  ※</h3><h4 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h4><ul><li>创建job的时候,把小表加入缓存 在map的setup中, </li><li>读取缓存中的数据, 存入一个成员变量 map中<ul><li>map方法中,只需要读一个表, 然后根据关联条件(关联key: pid)消除笛卡尔集,进行拼接</li><li>map直接输出, 甚至都不需要reduce</li></ul></li></ul><h4 id="注意点"><a href="#注意点" class="headerlink" title="注意点:"></a>注意点:</h4><ul><li><p>需要达成jar包运行, 直接用Eclipse会找不到缓存</p></li><li><p><strong>jar包执行方法</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果代码内部指定了输入输出路径，后面的/in，/out参数可以不加</span></span><br><span class="line">hadoop jar xxxx.jar com.rox.xxx.xxxx(主方法)  /<span class="keyword">in</span>/xx   /out/xx</span><br></pre></td></tr></table></figure></li><li><p>如果没有Reduce方法</p><ul><li><p>main方法中，设置map的写出key，value,应该用 <code>setOutputKeyClass</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//// 设置Map输出类 (因为这里没有Reduce, 所以这里是最终输出,一定要注意!!!)//////////</span></span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(NullWritable.class);</span><br></pre></td></tr></table></figure></li><li><p>要设置reduce task 的个数为0 </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setNumReduceTasks(<span class="number">0</span>);</span><br></pre></td></tr></table></figure></li><li><p>把小文件加载到缓存中的方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">////////////// 将小文件加载到缓存  </span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"/in/joindemo/product"</span>));</span><br></pre></td></tr></table></figure></li></ul></li></ul><p>  ​    </p><h4 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._03_join;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.FileReader;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MapJoinDemo</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 指定HDFS相关参数</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//  创建/配置 Job</span></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Jar包类型:这里千万别写错了</span></span><br><span class="line">job.setJarByClass(MapJoinDemo.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Map Reduce执行类</span></span><br><span class="line">job.setMapperClass(MyMapper.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">///////////// 设置Map输出类 (因为这里没有Reduce, 所以这里是最终输出,一定要注意!!!)//////////</span></span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">////////////// 设置reduce执行个数为0</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">////////////// 将小文件加载到缓存  </span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"/in/joindemo/product"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置输入 输出路径</span></span><br><span class="line">String inP = <span class="string">"/in/joindemo/order"</span>;</span><br><span class="line">String outP = <span class="string">"/out/joinout2"</span>;</span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(inP));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(outP));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置如果存在路径就删除</span></span><br><span class="line">        Path mypath = <span class="keyword">new</span> Path(outP);</span><br><span class="line">        FileSystem hdfs = mypath.getFileSystem(conf);</span><br><span class="line">        <span class="keyword">if</span> (hdfs.isDirectory(mypath)) &#123;</span><br><span class="line">            hdfs.delete(mypath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//  执行job</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="keyword">true</span>)?<span class="number">0</span>:-<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 思路: </span></span><br><span class="line"><span class="comment"> * 创建job的时候,把小表加入缓存</span></span><br><span class="line"><span class="comment"> * 在map的setup中, 读取缓存中的数据, 存入一个成员变量 map中</span></span><br><span class="line"><span class="comment"> * map方法中,只需要读一个表, 然后根据关联条件(关联key: pid)消除笛卡尔集,进行拼接</span></span><br><span class="line"><span class="comment"> * 直接输出, 甚至都不需要reduce</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 注意点: </span></span><br><span class="line"><span class="comment"> * 需要达成jar包运行, 直接用Eclipse会找不到缓存</span></span><br><span class="line"><span class="comment"> * 格式: hadoop jar包本地路径 jar包主方法全限定名 hadoop输入  hadoop输出</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建装载小表的map, key存储 关联键, value存其它</span></span><br><span class="line">Map&lt;String, String&gt; proMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">// 获取缓存中存储的小表 (一般是 一对多中的 一), 因为只存了1个,所以直接取第0个</span></span><br><span class="line">Path path = context.getLocalCacheFiles()[<span class="number">0</span>];</span><br><span class="line">String pString = path.toString();</span><br><span class="line"><span class="comment">// 开启in流, BufferedReader 逐行读取文件</span></span><br><span class="line">BufferedReader br = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> FileReader(pString));</span><br><span class="line">String line = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">while</span> ((line = br.readLine()) != <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="comment">// 成功读取一行</span></span><br><span class="line">String[] infos = line.split(<span class="string">"\t"</span>);</span><br><span class="line"><span class="comment">// 存进proMap</span></span><br><span class="line">proMap.put(infos[<span class="number">0</span>],</span><br><span class="line">infos[<span class="number">1</span>] + <span class="string">"\t"</span> + infos[<span class="number">2</span>] + <span class="string">"\t"</span> + infos[<span class="number">3</span>]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//br.close();</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 直接从路径读取大文件</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">String[] infos = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">String pid = infos[<span class="number">2</span>];</span><br><span class="line"><span class="comment">//进行关联   pid到map中匹配   如果包含  证明匹配上了</span></span><br><span class="line"><span class="comment">// 艹, 这里pid之前加了 "", 妈的,当然找不到啦!!!</span></span><br><span class="line"><span class="keyword">if</span> (proMap.containsKey(pid)) &#123;</span><br><span class="line">String res = value.toString() + <span class="string">"\t"</span> + proMap.get(pid);</span><br><span class="line">k.set(res);</span><br><span class="line">context.write(k, NullWritable.get());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><script src="https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js"></script><script src="https://cdn.bootcss.com/snap.svg/0.5.1/snap.svg-min.js"></script><script src="https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js"></script><script src="https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js"></script><script src="https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js"></script><textarea id="sequence-0-code" style="display: none">title: 执行流程时序图 Mapper(map)->ScoreBean: k:ScoreBean(courseName,avgScore)Mapper(map)->ScoreBean: v:Text(stuName)Mapper(ScoreBean)->Reducer(MyGroup): course按字典升序Mapper(ScoreBean)->Reducer(MyGroup): course内成绩降序Reducer(MyGroup)->Reducer(reduce): 根据自定义的分组规则按组输出Reducer(MyGroup)->Reducer(reduce): 一组只调用reduce一次</textarea><textarea id="sequence-0-options" style="display: none">{"theme":"simple"}</textarea><script>  var code = document.getElementById("sequence-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value));  var diagram = Diagram.parse(code);  diagram.drawSVG("sequence-0", options);</script></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;求微博共同粉丝&quot;&gt;&lt;a href=&quot;#求微博共同粉丝&quot; class=&quot;headerlink&quot; title=&quot;求微博共同粉丝&quot;&gt;&lt;/a&gt;求微博共同粉丝&lt;/h2&gt;&lt;h3 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce笔记-3</title>
    <link href="https://airpoet.github.io/2018/06/08/Hadoop/Study/2-MapReduce/MapReduce%E7%AC%94%E8%AE%B0-3/"/>
    <id>https://airpoet.github.io/2018/06/08/Hadoop/Study/2-MapReduce/MapReduce笔记-3/</id>
    <published>2018-06-08T01:23:27.815Z</published>
    <updated>2018-06-13T13:33:22.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-多-Job-串联"><a href="#1-多-Job-串联" class="headerlink" title="1.多 Job 串联"></a>1.多 Job 串联</h2><h3 id="1-概念"><a href="#1-概念" class="headerlink" title="1.概念"></a>1.概念</h3><p>当程序中有多个 Job， 并且多个 job 之间相互依赖， a ， job 需要依赖另一个b，job 的执行结果时候， 此时需要使用多 job 串联</p><h3 id="2-涉及到昨天的微博求共同粉丝题目"><a href="#2-涉及到昨天的微博求共同粉丝题目" class="headerlink" title="2. 涉及到昨天的微博求共同粉丝题目"></a>2. 涉及到昨天的微博求共同粉丝题目</h3><blockquote><p>A:B,C,D,F,E,O<br>B:A,C,E,K<br>C:F,A,D,I<br>D:A,E,F,L<br>E:B,C,D,M,L<br>F:A,B,C,D,E,O,M<br>G:A,C,D,E,F<br>H:A,C,D,E,O<br>I:A,O<br>J:B,O<br>K:A,C,D<br>L:D,E,F<br>M:E,F,G<br>O:A,H,I,J,K</p><p>以上是数据：<br>A:B,C,D,F,E,O<br>表示：A用户 关注B,C,D,E,F,O</p><p>求所有两两用户之间的共同关注对象</p></blockquote><h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><ul><li>要写2个MapReduce， 开启2个job</li><li>后一个job依赖于前一个的执行结果</li><li>后一个job的输入文件路径，就是前一个job的输出路径</li><li>2个job需要添加依赖</li></ul><h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><p><a href="https://airpoet.github.io/2018/06/09/Hadoop/Study/2-MapReduce/MapReduce-%E7%BB%83%E4%B9%A0/#more">参考练习-第一题-求微博共同好友</a></p><h4 id="多-Job-串联部分代码"><a href="#多-Job-串联部分代码" class="headerlink" title="多 Job 串联部分代码"></a><strong>多 Job 串联部分代码</strong></h4><ul><li><p>基本的写到一起， job1， job2</p></li><li><p>用<code>JobControl</code>对象管理多 job， 会将多个 job 当做一个组中的 job 提交， 参数指的是组名， 随意起</p></li><li><p>原生的 job 要转为可控制的 job</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建 JobControl 组</span></span><br><span class="line">JobControl jc = <span class="keyword">new</span> JobControl(<span class="string">"common_friend"</span>);</span><br><span class="line"><span class="comment">// job 拿好配置， 加入 ControlledJob 管理, 变成可控制的 job</span></span><br><span class="line">ControlledJob ajob = <span class="keyword">new</span> ControlledJob(job1.getConfiguration());</span><br><span class="line">ControlledJob bjob = <span class="keyword">new</span> ControlledJob(job2.getConfiguration());</span><br><span class="line"><span class="comment">// 添加依赖关系</span></span><br><span class="line">bjob.addDependingJob(ajob);  </span><br><span class="line"><span class="comment">//  添加 job进 JC</span></span><br><span class="line">jc.addJob(ajob);</span><br><span class="line">jc.addJob(bjob);</span><br><span class="line"><span class="comment">// 启动线程</span></span><br><span class="line">Thread jobControlTread = <span class="keyword">new</span> Thread(jc);</span><br><span class="line">jobControlTread.start();</span><br><span class="line"><span class="comment">// 在线程完成之后关闭</span></span><br><span class="line"><span class="keyword">while</span>(!jc.allFinished()) &#123;</span><br><span class="line">Thread.sleep(<span class="number">500</span>);</span><br><span class="line">&#125;</span><br><span class="line">jobControl.stop();</span><br></pre></td></tr></table></figure></li></ul><hr><h2 id="2-分组组件"><a href="#2-分组组件" class="headerlink" title="2. 分组组件"></a>2. 分组组件</h2><p><strong>map–分组–reduce</strong>  </p><p>reduce 接收到的数据是按照 map 输出的 key 进行分组的, 分组的时候按照 key 相同的时候为一组,  默认都实现了 <code>WritableComparable</code>接口， 其中的 compareTo（）方法返回为0的时候 默认为一组， 返回不为0， 则分到下一组</p><p><br></p><p><strong>自定义分组使用场景：</strong> 默认的数据分组不能满足需求</p><blockquote><p>一、数据解释</p><p>数据字段个数不固定：<br>第一个是课程名称，总共四个课程，computer，math，english，algorithm，<br>第二个是学生姓名，后面是每次考试的分数</p><p>二、统计需求：<br>1、统计每门课程的参考人数和课程平均分</p><p>2、统计每门课程参考学生的平均分，并且按课程存入不同的结果文件，要求一门课程一个结果文件</p><p>3、求出每门课程参考学生成绩最高平均分的学生的信息：课程，姓名和平均分</p></blockquote><p><strong>第三题： 要求就是分组求最大值， 两件事情： 分组， 排序（shuffle）</strong></p><p><strong>总结：</strong> </p><p>1、利用“班级和平均分”作为 key，可以将 map 阶段读取到的所有学生成绩数据按照班级 和成绩排倒序，发送到 reduce</p><p>2、在 reduce 端利用 GroupingComparator 将班级相同的 kv 聚合成组，然后取第一个即是最 大值</p><h4 id="具体参考："><a href="#具体参考：" class="headerlink" title="具体参考："></a>具体参考：</h4><p><a href="https://airpoet.github.io/2018/06/09/Hadoop/Study/2-MapReduce/MapReduce-%E7%BB%83%E4%B9%A0/#more">练习-求学生成绩-第三小题</a></p><hr><h2 id="3-Reduce-中的2个坑"><a href="#3-Reduce-中的2个坑" class="headerlink" title="3. Reduce 中的2个坑"></a>3. Reduce 中的2个坑</h2><h3 id="坑1"><a href="#坑1" class="headerlink" title="坑1"></a>坑1</h3><p><strong>Iterable\<text>只能循环遍历一次</text></strong></p><p>迭代器每次循环遍历完成， 指针都会移动到最后一个</p><p>系统类型，没事</p><p>自定义类型 ，有问题？</p><h3 id="坑2"><a href="#坑2" class="headerlink" title="坑2"></a>坑2</h3><p><strong>迭代器中所有对象公用同一个地址</strong></p><hr><h2 id="4-Reduce-端的-Join"><a href="#4-Reduce-端的-Join" class="headerlink" title="4. Reduce 端的 Join"></a>4. Reduce 端的 Join</h2><p><strong>牺牲效率换执行</strong></p><p>思路： </p><p><strong>核心： 关联条件</strong></p><ul><li>想要在 reduce 端完成 join， 要在 reduce 端可以同时接收到两个表中的数据</li><li>要保证在 Map 端进行读文件的时候， 读到2个表的数据， 并且需要对2个表的数据进行区分</li><li>将2个表放在同一个目录下</li></ul><h4 id="Map-端"><a href="#Map-端" class="headerlink" title="Map 端"></a>Map 端</h4><ul><li><strong>读取两个表中的数据， 进行切分、发送</strong></li><li>key ： 公共字段–关联字段–<code>pid</code></li><li>value： 剩下的字段， 标记数据的来源表</li></ul><h4 id="Reduce-端"><a href="#Reduce-端" class="headerlink" title="Reduce 端"></a>Reduce 端</h4><ul><li>通过编辑分离出2个表的数据</li><li>分别存到2个容器中（ArrayList）</li><li>遍历大表，拼接小表</li></ul><h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><p><a href="https://airpoet.github.io/2018/06/09/Hadoop/Study/2-MapReduce/MapReduce-%E7%BB%83%E4%B9%A0/#more">参考练习-第三题MR实现2个表之间的Join</a></p><h3 id="缺陷"><a href="#缺陷" class="headerlink" title="缺陷"></a>缺陷</h3><p><strong>1. ReduceTask 的并行度问题：</strong></p><ul><li>建议0.95*datanode 的个数</li><li>并行度不高， 性能不高</li></ul><p><strong>2. 容器性能</strong></p><ul><li>list 等， 不提倡， reduce 接收的数据， 可能会很大</li></ul><p><strong>3. ReduceTask 容易产生数据倾斜</strong></p><ul><li>假设我们设置多个 ReduceTask， 根据分区规则， 默认 hash</li><li>以 key关联条件分，  ReduceTask数据倾斜， 每个 ReduceTask 分工不均， 非常影响性能，没有合理的利用集群资源</li><li>在真实的生产中一定要尽量的避免数据倾斜</li><li>最好的做法：将分区设计的足够完美，难度比较大</li><li>因此，ReduceTask 一般不会完成 John工作</li><li><strong>放在 Map 端完成就不会有这个问题了</strong></li></ul><hr><h5 id="补充：Mapper-中的源码分析"><a href="#补充：Mapper-中的源码分析" class="headerlink" title="补充：Mapper 中的源码分析"></a>补充：Mapper 中的源码分析</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 在 maptask 执行之前调用一次， 一个 maptask 只会调用一次。setup 中通常会帮 map 中初始化一些变量和资源， 比如数据库的连接等。</span></span><br><span class="line">    <span class="comment">// 主要目的：减少资源的初始化次数而提升程序的性能</span></span><br><span class="line">    setup(context);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 获取文件是否还有下一行， 一行只调用一次</span></span><br><span class="line">      <span class="keyword">while</span> (context.nextKeyValue()) &#123;</span><br><span class="line">        map(context.getCurrentKey(), context.getCurrentValue(), context);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">       <span class="comment">// maptask 任务执行完成之后会调用一次，一个 maptask 只会调用一次</span></span><br><span class="line">       <span class="comment">// 帮 map 处理一些善后工作， 比如：资源的关闭</span></span><br><span class="line">      cleanup(context);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><hr><h2 id="5-Map-端的-Join"><a href="#5-Map-端的-Join" class="headerlink" title="5. Map 端的 Join"></a>5. Map 端的 Join</h2><p><strong>注意点：这种方式只能通过 Jar 包上传的方式，直接用 Eclipse 会找不到缓存</strong> </p><p>为了提升 Map 端 Join 性能， 我们的策略是， <strong>将小表的数据加载到每个运行的 MapTask 的内存中</strong>。</p><p>如果小表被加载到了内存中， 我们<strong>每次在 Map 端只需要读取大表，当读取到大表的每一行数据，可以直接和内存中的小表进行关联。</strong></p><p>这个时候，<strong>只需要 Map 就可以完成 Join 操作了</strong>。</p><p><br></p><h3 id="1-如何将小表加入到内存中？"><a href="#1-如何将小表加入到内存中？" class="headerlink" title="1. 如何将小表加入到内存中？"></a>1. 如何将小表加入到内存中？</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将指定路径文件加载到缓存中</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"/xxx"</span>));</span><br></pre></td></tr></table></figure><h3 id="2-Map-端怎样读取缓存中的数据"><a href="#2-Map-端怎样读取缓存中的数据" class="headerlink" title="2. Map 端怎样读取缓存中的数据"></a>2. Map 端怎样读取缓存中的数据</h3><p>想要在 Java 中使用缓存中的数据，缓存中的数据必须封装到 Java 的容器中</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取缓存文件</span></span><br><span class="line">context.getLocalCacheFiles()[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h3 id="3-代码"><a href="#3-代码" class="headerlink" title="3. 代码"></a>3. 代码</h3><p><a href="https://airpoet.github.io/2018/06/09/Hadoop/Study/2-MapReduce/MapReduce-%E7%BB%83%E4%B9%A0/#more">参考练习-第3题</a></p><p><strong>代码注意点：</strong> </p><p><strong>setup</strong>：从缓存读取一文件（多对一的一）到 HashMap</p><p><strong>main 方法中注意点</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 指定文件加入缓存</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"/xxx"</span>)); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果没有ReduceTask， 要设置为0</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">0</span>);</span><br></pre></td></tr></table></figure><hr><h2 id="6-对比"><a href="#6-对比" class="headerlink" title="6. 对比"></a>6. 对比</h2><p><strong>MapJoin 的方式： 大 &amp; 小表</strong></p><p>因为有一个表需要加载到内存中，注定加载到内存中的表不能过大（hive 中默认是256M）</p><p><strong>大表 &amp; 大表 如何设计</strong></p><ul><li>ReduceJoin ： 解决数据倾斜的问题，合理设计分区。 —很难做到</li><li>将其中一个<strong>大表进行切分</strong>，切分成小表， <strong>最终执行 大表 &amp; 小表</strong></li></ul><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul><li><p>并行度高，不存在数据倾斜的问题，运行效率高</p></li><li><p><strong>优先选择MapJoin</strong></p></li></ul><hr><h2 id="arrow-forward-7-排序算法-（待整理：TODO）"><a href="#arrow-forward-7-排序算法-（待整理：TODO）" class="headerlink" title=":arrow_forward: 7. 排序算法 （待整理：TODO）"></a>:arrow_forward: 7. 排序算法 （待整理：TODO）</h2><h3 id="1-快速排序"><a href="#1-快速排序" class="headerlink" title="1. 快速排序"></a>1. 快速排序</h3><p><strong>边界值始终是不变的。</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-08-083614.png" alt="image-20180608163614370"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-08-083842.png" alt="image-20180608163841417"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-08-084004.png" alt="image-20180608164004311"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-08-084128.png" alt="image-20180608164127167"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-08-084319.png" alt="image-20180608164318335"></p><h3 id="2-归并排序"><a href="#2-归并排序" class="headerlink" title="2. 归并排序"></a>2. 归并排序</h3><p>一般情况针对<strong>有序的</strong>，<strong>多个</strong>， <strong>小</strong>数据集</p><blockquote><p>应用场景：想到了多个Reduce 任务产生的多个文件的合并</p></blockquote><h4 id="1-归并排序前传：-合并多个数组"><a href="#1-归并排序前传：-合并多个数组" class="headerlink" title="1. 归并排序前传： 合并多个数组"></a>1. 归并排序前传： 合并多个数组</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-08-085031.png" alt="image-20180608165031396"></p><h4 id="2-归并排序-之-一个大数据集"><a href="#2-归并排序-之-一个大数据集" class="headerlink" title="2. 归并排序 之 一个大数据集"></a>2. 归并排序 之 一个大数据集</h4><h5 id="—归———"><a href="#—归———" class="headerlink" title="—归———-"></a>—归———-</h5><p><strong>切分成单个的数据集</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-08-085652.png" alt="image-20180608165651868"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-08-085727.png" alt="image-20180608165727484"></p><h5 id="—-并———"><a href="#—-并———" class="headerlink" title="—-并———"></a>—-并———</h5><ol><li>两两相并， 并成新的数组， 小的先放入数组， 再放大的</li><li>新的数组再不断执行 上述的 <strong>合并多个数组</strong></li></ol><hr><h2 id="8-※-Shuffle-过程-※"><a href="#8-※-Shuffle-过程-※" class="headerlink" title="8.  ※ Shuffle 过程 ※"></a>8.  ※ Shuffle 过程 ※</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-024417.png" alt="image-20180611104416430"></p><ul><li>mapper 阶段处理的数据如何传递给 reducer 阶段，是 MapReduce 框架中 最关键的一个流程，这个流程就叫 Shuffle</li><li>Shuffle<ul><li>即 数据混洗  ——  核心机制：数据分区，排序，局部聚合，缓存，拉取，再合并，排序； </li><li>环形缓冲区<ul><li>内存中的一种首尾相连的数据结构（底层是字节数组），kvbuffer 包含原始数据区和元数据区，一个mapTask任务对应一个环形缓冲区</li><li>默认大小 100M，默认阈(yu)值 0.8，即当达到 80M 后，会触发 spill溢写 操作，将数据写入磁盘，此时mapper输出会继续向剩余20M中写数据<br>缓冲区大小 mapred-site.xml：mapreduce.task.io.sort.mb<br>阈值 mapred-site.xml：mapreduce.map.sort.spill.percent<br>路径：mapred-site.xml：mapreduce.cluster.local.dir</li><li>如果此80M数据写入磁盘完成前，剩余20M缓冲区也写完，则会进入阻塞状态，直到是spill完成腾出缓冲区空间</li><li>赤道（equtor）：环形缓冲区中原始数据和元数据的边界<ul><li>原始数据：mapTask输出的数据</li><li>元数据<ul><li>记录原始数据的数据，包含4部分内容，占16*4字节；</li><li>每一条元数据占用空间是一样的，排序可以通过交换元数据实现</li><li>分类<ul><li>a. 原始数据中key的起始位置</li><li>b. 原始数据中value的起始位置</li><li>c. value的长度</li><li>d. 分区信息，即该条信息属于哪个分区</li></ul></li></ul></li></ul></li></ul></li><li>核心操作<ul><li>\1. 分区 partition（如果 reduceTask 只有一个或者没有，那么 partition 将不起作用） </li><li>\2. Sort 根据 key 排序（MapReduce 编程中的 sort 是一定会做的，并且 只能按照 key排序， 当然 如果没有 reducer 阶段，那么就不会对 key 排序） </li><li>\3. Combiner 进行局部 value 的合并（Combiner 是可选的组件，作用只是为了提高任务的执行效率）</li></ul></li><li>详细过程<ul><li>\1. 一个大文件需要处理，它在在 HDFS 上是以 block 块形式存放，每个 block 默认为 128M 存 3 份；运行时每个 map 任务会处理一个切块（split），如果 block 大和 split 相同，有多少个 block 就有多少个 map 任务；所以对整个文件处理时会有很多 map 任务进行并行计算。</li><li>\2. 每个 map 任务处理完输入的切块后会把结果写入到内存的一个 环形缓冲区，写入过程中会进行简单排序，当缓冲区的大小阀值，一个后台的线程就会启动把缓冲区中的数据溢写（spill）到本地磁盘中，同时Mapper继续时向环形缓冲区中写入数据。<ul><li>数据溢写入到磁盘之前，首先会根据 reducer 的数量划分成同数量的分区（partition），每个分区中的都数据会有后台线程根据 map 任务的输出结果 key 进行排序；</li><li>如果有 combiner，它会在 缓冲区溢写到磁盘之前 和 mapTask排好序的输出上 运行，使写到本地磁盘和传给 reducer 的数据更少；<br>Combiner即是把同一分区中的同一key的数据进行合并，整个shuffle过程会调用两个Combiner !</li><li>最后在本地生成分好区且排好序的小文件。</li><li>注意：如果 map 向环形缓冲区写入数据的速度大于向本地写入数据的速度，环形缓冲区会被写满，向环形缓冲区写入数据的线程会阻塞直至缓冲区中的内容全部溢写到磁盘后再次启动，到阀值后会向本地磁盘新建一个溢写文件；</li></ul></li><li>\3. map 任务完成之前，会把本地磁盘溢写的所有文件 不停地 合并（merge）成得到一个结果文件，合并得到的结果文件会根据小溢写文件的分区而分区，每个分区的数据会再次根据 key 进行 排序，得到的结果文件是分好区且排好序的（可以合并成一个文件的溢写文件数量默认为10）；<br>默认合并溢写文件数量 mapred-site.xml：mapreduce.task.io.sort.factor</li><li>\4.  reduce 任务启动，Reducer 中的一个线程定期向 MRAppMaster 询问 Mapper 输出结果文件位置，Mapper  结束后会向 MRAppMaster 汇报信息，从而 Reducer 会得知 Mapper 状态并得到 map 结果文件目录；<br>reduce任务数配置<br>a) mapred-site.xml：mapreduce.job.reduces<br>b) job.setNumReduceTasks(num)</li><li>\6. 当有一个 Mapper 结束时，reduce 任务进入复制阶段，reduce 任务通过 http 协议（hadoop 内置了netty容器）把所有 Mapper 结果文件的 对应的分区数据 拉取（fetch）过来，Reducer  可以并行复 制 Mapper 的 结果 ， 默认线程数为5； 所有 Reducer 复制完成 map 结果文件后，由于 Reducer  可能会失败，NodeManager 并不会在第一个 map 结果文件复制完成后就删除它，而是直到作业完成后 MRAppMaster 通知  NodeManager 进行删除； 另外如果 map 结果文件相当小，则会被直接复制到 reduce NodeManager  的内存；一旦缓冲区达到 reduce 的阈值大小 0.66 或 写入到 reduce NodeManager 内 存 中 文 件 个 数 达 到  map 输出阈值 1000，reduce 就会把 map 结果文件合并（merge）溢写到本地；<br>默认线程数 mapred-site.xml：mapreduce.reduce.shuffle.parallelcopies<br>缓冲区大小 mapred-site.xml:mapreduce.reduce.shuffle.input.buffer.percent，默认0.7<br>阈值：mapred-site.xml:mapreduce.reduce.shuffle.merge.percent，默认0.66<br>map输出阈值1000：mapred-site.xml:mapreduce.reduce.merge.inmem.threshold</li><li>\7. 复制阶段完成后，Reducer 进入 Merge 阶段，循环地合并 map 结果文件，维持其顺序排序，合并因子默认为 10，经过不断地 Merge 后得到一个”最终文件”，可能存储在磁盘也可能存在内存中； </li><li>\8. “最终文件”输入到 reduce 进行计算，计算结果输入到 HDFS。</li><li>[ 注意 ]<ul><li>溢写前会先按照分区进行排序，再按key进行排序，采用 快速排序<br>排序是按照原始数据排序，但是由于原始数据不好移动且原始数据包含了原始数据的位置信息，所以移动的其实是元数据；写入时读的是元数据，真正写入的时原始数据</li><li>最后的数据如果不够80M，也会被强制flush到磁盘</li><li>每个mapTask任务生成的磁盘小数据最后都会merge成一个大文件，采用 归并排序</li><li>Shuffle 中的缓冲区大小会影响到 mapreduce 程序的执行效率，原则上说，缓冲区越大，磁 盘io的次数越少，执行速度就越快。</li></ul></li></ul></li></ul></li></ul><p><br></p><h2 id="10、自定义输入-InputFormat"><a href="#10、自定义输入-InputFormat" class="headerlink" title="10、自定义输入 InputFormat"></a>10、自定义输入 InputFormat</h2><ul><li>默认的文件加载：TextInputFormat</li><li>默认的文件读取：LineRecordReader</li><li>源码追踪过程：context  –&gt; mappercontext –&gt; mapcontext –&gt; reader –&gt; input  –&gt; real –&gt;  inputFormat.createRecordReader（split，taskContext），然后查找 inputFormat  –&gt; createRecordReader（split，taskContext），inputFormat –&gt;  TextInputFormat实例对象</li><li>案例：多个小文件合并   word1.txt ~word10.txt   每次读取一个小文件<ul><li>自定义输入，需要创建两个类，并通过Job对象指定自定义输入</li><li>\1. 创建XxxInputFormat类，继承FileInputFormat&lt;&gt;，重写 createRecordReader() 方法</li><li>\2. 创建XxxRecordReader类，继承RecordReader&lt;&gt;，重写以下方法：<ul><li>initialize()：初始化方法，类似于setup()，对属性、链接或流进行初始化</li><li>getCurrentKey()：返回key</li><li>getCurrentValue()：返回value</li><li>getProgress()：返回文件执行进度</li><li>nextKeyValue()：返回文件是否读取结束</li><li>close()：进行一些资源的释放</li></ul></li><li>\3. 在mapreduce类的main()方法中指定自定义输入：job.setInputFormatClass(XxxInputFormat.class);</li></ul></li></ul><h4 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h4><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLWZFoCz9TSlCIVNCAoWjSSiloaqiuN98pKi1AW40"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr4._02_inputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.JobContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordReader;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WholeFileInputFormat</span> <span class="keyword">extends</span> <span class="title">FileInputFormat</span>&lt;<span class="title">NullWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 设置每个小文件不可分片,保证一个小文件生成一个key-v键值对</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">isSplitable</span><span class="params">(JobContext context, Path filename)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> RecordReader&lt;NullWritable, Text&gt; <span class="title">createRecordReader</span><span class="params">(InputSplit split,</span></span></span><br><span class="line"><span class="function"><span class="params">TaskAttemptContext context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">WholeFileRecordReader reader = <span class="keyword">new</span> WholeFileRecordReader();</span><br><span class="line">reader.initialize(split, context);</span><br><span class="line"><span class="keyword">return</span> reader;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLWZFoCz9TSlCIVNCAoWjSSiloaqiuN98pKi1AW40"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr4._02_inputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordReader;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WholeFileRecordReader</span> <span class="keyword">extends</span> <span class="title">RecordReader</span>&lt;<span class="title">NullWritable</span>, <span class="title">Text</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> FileSplit fileSplit;</span><br><span class="line"><span class="keyword">private</span> Configuration conf;</span><br><span class="line"><span class="keyword">private</span> Text value = <span class="keyword">new</span> Text(); </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">boolean</span> processed = <span class="keyword">false</span>;  <span class="comment">// 标识文件是否读取完成</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 初始化方法</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(InputSplit split, TaskAttemptContext context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.fileSplit=(FileSplit)split;</span><br><span class="line"><span class="keyword">this</span>.conf = context.getConfiguration();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">nextKeyValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (!processed) &#123;</span><br><span class="line"><span class="keyword">byte</span>[] contents = <span class="keyword">new</span> <span class="keyword">byte</span>[(<span class="keyword">int</span>)fileSplit.getLength()];</span><br><span class="line">Path file = fileSplit.getPath();</span><br><span class="line">FileSystem fs = file.getFileSystem(conf);</span><br><span class="line">FSDataInputStream in = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">in = fs.open(file);</span><br><span class="line"><span class="comment">// 把输入流上的数据全部读取到contents字节数组中</span></span><br><span class="line">IOUtils.readFully(in, contents, <span class="number">0</span>, contents.length);</span><br><span class="line"><span class="comment">// 把读取到的数据设置到value里</span></span><br><span class="line">value.set(contents,<span class="number">0</span>,contents.length);</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">IOUtils.closeStream(in);</span><br><span class="line">&#125;</span><br><span class="line">processed = <span class="keyword">true</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> NullWritable <span class="title">getCurrentKey</span><span class="params">()</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="keyword">return</span> NullWritable.get();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Text <span class="title">getCurrentValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="keyword">return</span> value;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">float</span> <span class="title">getProgress</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="keyword">return</span> processed ? <span class="number">1.0f</span> : <span class="number">0.0f</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLWZEJSp9SSlCIIrspiyhIoqg2Sbtoapt3U9oICrB0Ie30000"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr4._02_inputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SmallFilesConvertToBigMR</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">int</span> exitCode = ToolRunner.run(<span class="keyword">new</span> SmallFilesConvertToBigMR(), args);</span><br><span class="line">System.exit(exitCode);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line"></span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line">Job job = Job.getInstance(conf, <span class="string">"combine small files to bigfile"</span>);</span><br><span class="line"></span><br><span class="line">job.setJarByClass(SmallFilesConvertToBigMR.class);</span><br><span class="line"></span><br><span class="line">job.setMapOutputKeyClass(Text.class);</span><br><span class="line"></span><br><span class="line">job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">job.setMapperClass(SmallFilesConvertToBigMRMapper.class);</span><br><span class="line"></span><br><span class="line">job.setReducerClass(SmallFilesConvertToBigMRReducer.class);</span><br><span class="line"></span><br><span class="line">job.setOutputKeyClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">job.setOutputValueClass(Text.class);</span><br><span class="line"><span class="comment">////////</span></span><br><span class="line">job.setInputFormatClass(WholeFileInputFormat.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// job.setOutputFormatClass(SequenceFileOutputFormat.class);</span></span><br><span class="line"></span><br><span class="line">Path input = <span class="keyword">new</span> Path(<span class="string">"/in/joindemo"</span>);</span><br><span class="line"></span><br><span class="line">Path output = <span class="keyword">new</span> Path(<span class="string">"/out/bigfile"</span>);</span><br><span class="line"></span><br><span class="line">FileInputFormat.setInputPaths(job, input);</span><br><span class="line"></span><br><span class="line">FileSystem fs = FileSystem.get(conf);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (fs.exists(output)) &#123;</span><br><span class="line"></span><br><span class="line">fs.delete(output, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">FileOutputFormat.setOutputPath(job, output);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> status = job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> status;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SmallFilesConvertToBigMRMapper</span></span></span><br><span class="line"><span class="class"><span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">NullWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> Text filenameKey;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">Mapper&lt;NullWritable, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">InputSplit split = context.getInputSplit();</span><br><span class="line">Path path = ((FileSplit) split).getPath();</span><br><span class="line">filenameKey = <span class="keyword">new</span> Text(path.toString());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(NullWritable key, Text value,</span></span></span><br><span class="line"><span class="function"><span class="params">Mapper&lt;NullWritable, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">context.write(filenameKey, value);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SmallFilesConvertToBigMRReducer</span></span></span><br><span class="line"><span class="class"><span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text filename, Iterable&lt;Text&gt; bytes,</span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">context.write(NullWritable.get(), bytes.iterator().next());</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><br></p><h2 id="11、自定义输出-OutputFormat"><a href="#11、自定义输出-OutputFormat" class="headerlink" title="11、自定义输出  OutputFormat"></a>11、自定义输出  OutputFormat</h2><ul><li>默认的文件加载：TextOutputFormat</li><li>默认的文件读取：LineRecordWriter</li><li>源码追踪过程 略</li><li>案例：将考试成绩合格的输出到一个文件夹，不及格的输出到另一个文件夹（注意，不同于分区，分区只是量结果输出到同一文件夹下不同文件）<ul><li>自定义输出，需要创建两个类，并通过Job对象指定自定义输入</li><li>\1. 创建XxxOutputFormat类，继承FileOutputFormat&lt;&gt;，重写getRecordWriter()方法</li><li>\2. 创建XxxRecordWriter类，继承RecordWriter&lt;&gt;，重写以下方法：<ul><li>write()：真正向外写出的方法，需要将结果输出到几个不同文件夹，就需要创建几个输出流<ul><li>而输出流通过FileSystem对象获取，FileSystem对象获取需要配置文件</li><li>一般可以通过构造方法直接传入FileSystem对象</li></ul></li><li>close()：释放资源</li></ul></li><li>\3. 在mapreduce类的main()方法中指定自定义输入：job.setOutputFormatClass(XxxOutputFormat.class);</li></ul></li></ul><h4 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h4><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLl0jpId9BCX9zIyjAIWjyGtYSaZDIm5A0m00"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.outputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MultipleOutputMR</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 指定HDFS相关参数</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//  创建/配置 Job</span></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Jar包类型:这里千万别写错了</span></span><br><span class="line">job.setJarByClass(MultipleOutputMR.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Map Reduce执行类</span></span><br><span class="line">job.setMapperClass(MultipleOutputMRMapper.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Map输出类</span></span><br><span class="line">job.setMapOutputKeyClass(Text.class);</span><br><span class="line">job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">////////////// 设置reduce执行个数为0</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">///////////// 设置MapOutputFormatClass</span></span><br><span class="line">job.setOutputFormatClass(MyOutputFormat.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置输入 输出路径</span></span><br><span class="line">String inP = <span class="string">"/in/newScoreIn"</span>;</span><br><span class="line">String outP = <span class="string">"/out/myoutformat/mulWriteSuc"</span>;</span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(inP));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(outP));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置如果存在路径就删除</span></span><br><span class="line">        Path mypath = <span class="keyword">new</span> Path(outP);</span><br><span class="line">        FileSystem hdfs = mypath.getFileSystem(conf);</span><br><span class="line">        <span class="keyword">if</span> (hdfs.exists(mypath)) &#123;</span><br><span class="line">            hdfs.delete(mypath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//  执行job</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="keyword">true</span>)?<span class="number">0</span>:-<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MultipleOutputMRMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value,</span></span></span><br><span class="line"><span class="function"><span class="params">Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">// 参考次数&gt;7次 算合格</span></span><br><span class="line">String[] splits = value.toString().split(<span class="string">","</span>);</span><br><span class="line"><span class="keyword">if</span> (splits.length &gt; <span class="number">9</span>) &#123;</span><br><span class="line">context.write(<span class="keyword">new</span> Text(<span class="string">"1::"</span>+value.toString()), NullWritable.get());</span><br><span class="line">&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">context.write(<span class="keyword">new</span> Text(<span class="string">"2::"</span>+value.toString()), NullWritable.get());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLl2jz2yjAIWjSSiloaqiuN98pKi1IW80"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.outputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyOutputFormat</span> <span class="keyword">extends</span> <span class="title">FileOutputFormat</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> RecordWriter&lt;Text, NullWritable&gt; <span class="title">getRecordWriter</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">TaskAttemptContext job)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">Configuration configuration = job.getConfiguration();</span><br><span class="line">FileSystem fs = FileSystem.get(configuration);</span><br><span class="line"></span><br><span class="line">Path p1 = <span class="keyword">new</span> Path(<span class="string">"/out/myoutformat/out1"</span>);</span><br><span class="line">Path p2 = <span class="keyword">new</span> Path(<span class="string">"/out/myoutformat/out2"</span>);</span><br><span class="line"></span><br><span class="line">FSDataOutputStream out1 = fs.create(p1);</span><br><span class="line">FSDataOutputStream out2 = fs.create(p2);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> MyRecordWriter(out1,out2);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLl2j34fDpYzA2I_AB4ajud98pKi1IW80"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.outputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyRecordWriter</span> <span class="keyword">extends</span> <span class="title">RecordWriter</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">FSDataOutputStream fsout = <span class="keyword">null</span>;</span><br><span class="line">FSDataOutputStream fsout1 = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">MyRecordWriter</span><span class="params">(FSDataOutputStream fsout, FSDataOutputStream fsout1)</span> </span>&#123;</span><br><span class="line"><span class="keyword">super</span>();</span><br><span class="line"><span class="keyword">this</span>.fsout = fsout;</span><br><span class="line"><span class="keyword">this</span>.fsout1 = fsout1;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Text key, NullWritable value)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"> String[] strs = key.toString().split(<span class="string">"::"</span>);</span><br><span class="line"> <span class="keyword">if</span> (strs[<span class="number">0</span>].equals(<span class="string">"1"</span>)) &#123;</span><br><span class="line">fsout.write((strs[<span class="number">1</span>]+<span class="string">"\n"</span>).getBytes());</span><br><span class="line">&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">fsout1.write((strs[<span class="number">1</span>]+<span class="string">"\n"</span>).getBytes());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(TaskAttemptContext context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">IOUtils.closeStream(fsout);</span><br><span class="line">IOUtils.closeStream(fsout1);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="12、Yarn"><a href="#12、Yarn" class="headerlink" title="12、Yarn"></a>12、Yarn</h2><h3 id="1、Yarn图示简介"><a href="#1、Yarn图示简介" class="headerlink" title="1、Yarn图示简介"></a>1、Yarn图示简介</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-081949.png" alt="image-20180611161948431"></p><h4 id="在Hadoop1-x时，-只有两个主要组件：hdfs（文件存储），-MapReduce（计算）"><a href="#在Hadoop1-x时，-只有两个主要组件：hdfs（文件存储），-MapReduce（计算）" class="headerlink" title="在Hadoop1.x时， 只有两个主要组件：hdfs（文件存储）， MapReduce（计算）"></a><strong>在Hadoop1.x时， 只有两个主要组件：hdfs（文件存储）， MapReduce（计算）</strong></h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-082801.png" alt="image-20180611162800671"></p><p><strong>所有的计算相关的全部放在MapReduce上</strong></p><ul><li><strong>JobTracker</strong>: 整个计算程序的老大<ul><li>资源调度：随机调度</li><li>监控程序运行的状态，启动运行程序</li><li>存在单点故障问题</li></ul></li><li><strong>TaskTracker</strong>：负责计算程序的执行<ul><li>强行的将计算资源分成2部分<ul><li>MapSlot</li><li>ReduceSlot</li></ul></li><li>每一部分资源只能跑对应的任务</li></ul></li><li><strong>缺陷：</strong><ul><li>单点故障</li><li>资源调度随机，会造成资源浪费</li><li>JobTracker的运行压力过大</li></ul></li></ul><p><br></p><h4 id="Hadoop2-x：-分理出Yarn，专门负责集群的资源管理和调度"><a href="#Hadoop2-x：-分理出Yarn，专门负责集群的资源管理和调度" class="headerlink" title="Hadoop2.x： 分理出Yarn，专门负责集群的资源管理和调度"></a><strong>Hadoop2.x： 分理出Yarn，专门负责集群的资源管理和调度</strong></h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-082010.png" alt="image-20180611162010610"></p><h5 id="Yarn的进程："><a href="#Yarn的进程：" class="headerlink" title="Yarn的进程："></a>Yarn的进程：</h5><p><strong>ResourceManager:</strong></p><ul><li><p>整个资源调度的老大</p><ul><li>接受hadoop客户端的请求</li><li>接受NodeManager 的状态报告， NM的资源状态和存活状态</li><li>资源调度，整个计算程序的资源调度，调度的运行资源和节点</li></ul></li></ul><ul><li><p><strong>内部组件</strong>： </p><ul><li><strong>ASM</strong>——ApplicationsManager<ul><li>所有应用程序的管理者，负责调度应用程序</li></ul></li><li><strong>Scheduler</strong>——调度器概念<ul><li>调度的是什么时候执行哪个计算程序</li><li><strong>调度器：</strong><ul><li><strong>FIFO</strong>: first in first out<ul><li>先提交的先执行，后提交的后执行</li><li>内部维护一个队列</li></ul></li><li><strong>FAIR</strong>: 公平调度器<ul><li>大家平分资源运行</li><li>假设刚开始只有一个任务，占资源100%，此时又来了一个任务，这是进行资源平分，每人50%</li><li>内部也是维护一个队列</li></ul></li><li><strong>CAPICITPY</strong>: 可以按需进行配置，使用资源<ul><li>内部<strong>可维护多个队列</strong>，多个队列之间可以进行资源分配</li><li>例如：分配两个队列<ul><li>队列1：60%</li><li>队列2：40%</li><li>每个队列中都是执行FIFO的</li></ul></li></ul></li></ul></li></ul></li></ul></li></ul><p><strong>NodeManager：</strong></p><ul><li>负责真正的提供资源，运行计算程序<ul><li>接受ResourceManager的命令</li><li>提供资源运行计算程序</li></ul></li></ul><p><strong><u>MRAppMaster</u></strong>: </p><ul><li>单个计算程序的老大, 类似于项目经理<ul><li>负责帮助当前计算程序向ResourceManager申请资源</li><li>负责启动 MapTask 和 ReduceTask 任务</li></ul></li></ul><p><u><strong>Container:</strong></u></p><ul><li>抽象资源容器，封装这一定的cpu，io 和网络资源（逻辑概念）</li><li>是运行MapTask，ReduceTask等的<strong>运行资源单位</strong></li><li>1个split —— 1个MapTask (ReduceTask) —— 1个Container —— 显示为YarnChild，底层运行的资源单位就是<code>Container</code></li></ul><h3 id="2、Yarn运行过程"><a href="#2、Yarn运行过程" class="headerlink" title="2、Yarn运行过程"></a>2、Yarn运行过程</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-12-013414.png" alt="image-20180612093413719"></p><ul><li>MRAppMaster会在所有的MapTask执行到0.8的时候，开启ReduceTask任务</li></ul><p><strong>YARN 作业执行流程:</strong></p><ol><li>用户向 YARN 中提交应用程序，其中包括 MRAppMaster 程序，启动 MRAppMaster 的命令， 用户程序等。</li><li>ResourceManager 为该程序分配第一个 Container，并与对应的 NodeManager 通讯，要求 它在这个 Container 中启动应用程序 MRAppMaster。</li><li>MRAppMaster 首先向 ResourceManager 注册，这样用户可以直接通过 ResourceManager 查看应用程序的运行状态，然后将为各个任务申请资源，并监控它的运行状态，直到运行结 束，重复 4 到 7 的步骤。</li><li>MRAppMaster 采用轮询的方式通过 RPC 协议向 ResourceManager 申请和领取资源。 </li><li>一旦 MRAppMaster 申请到资源后，便与对应的 NodeManager 通讯，要求它启动任务。</li><li>NodeManager 为任务设置好运行环境(包括环境变量、JAR 包、二进制程序等)后，将 任务启动命令写到一个脚本中，并通过运行该脚本启动任务。</li><li>各个任务通过某个 RPC 协议向 MRAppMaster 汇报自己的状态和进度，以让 MRAppMaster 随时掌握各个任务的运行状态，从而可以在任务败的时候重新启动任务。 8、应用程序运行完成后，MRAppMaster 向 ResourceManager 注销并关闭自己。</li></ol><h3 id="3、Job的提交过程-待整理"><a href="#3、Job的提交过程-待整理" class="headerlink" title="3、Job的提交过程(待整理)"></a>3、Job的提交过程(待整理)</h3><ul><li>客户端向rm发送 提交job请求</li><li>rm向客户端发送 共享资源路径 和 applicationId</li><li>客户端将程序运行需要的共享资源放进共享资源路径<br>包括：程序jar包，xml配置文件，split切片信息</li><li>客户端向rm发送资源放置成功的报告，并真正 提交应用程序</li><li>rm接收到客户端的请求，会返回一个空闲的资源节点(比如：node01)</li><li>到资源节点(node01)上启动container</li><li>启动MRAppMaster</li><li>创建作业簿 记录 maptask 和 reducetask 的 运行状态和进度 等信息</li><li>mrappmaster去共享资源路径下 ,获取 切片 和 配置文件 等信息</li><li>mrappmaster 向 rm 申请maptask 和 reducetask的资源</li><li>rm 在处理 mrappmaster 请求时，会 优先处理有关maptask的请求</li><li>rm 向 mrappmaster 返回空闲节点（数据本地优先原则），运行maptask 或 reducetask<br>优先返回有数据的节点。</li><li>对象节点需要到hdfs 共享路径下下载程序jar包等 共享资源 到本地</li><li>mrappmaster 到 对应的节点上，启动container 和 maptask</li><li>maptask需要向 mrappmaster 汇报自身的 运行状态和进度</li><li>mrappmaster 监控到所有的maptask 运行进度到 80%，启动reducetask（启动前，也会下载共享资源路径下的响应文件，程序jar包，配置文件等）</li><li>reducetask 时刻和 mrappmaster 通信，汇报自身的 运行状态和进度</li><li>整个运行过程中，maptask运行完成 ， 都会向mrappmaster 申请注销 自己</li><li>当所有的maptask 和 reducetask 运行完成 ， mrappmaster 就会向rm 申请注销，进行资源回收</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-多-Job-串联&quot;&gt;&lt;a href=&quot;#1-多-Job-串联&quot; class=&quot;headerlink&quot; title=&quot;1.多 Job 串联&quot;&gt;&lt;/a&gt;1.多 Job 串联&lt;/h2&gt;&lt;h3 id=&quot;1-概念&quot;&gt;&lt;a href=&quot;#1-概念&quot; class=&quot;head
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/"/>
    
      <category term="学习笔记" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce笔记-2 三大组件-Partitioner分区,sort排序,Combiner局部分区</title>
    <link href="https://airpoet.github.io/2018/06/07/Hadoop/Study/2-MapReduce/MapReduce%E7%AC%94%E8%AE%B0-2/"/>
    <id>https://airpoet.github.io/2018/06/07/Hadoop/Study/2-MapReduce/MapReduce笔记-2/</id>
    <published>2018-06-07T03:27:55.684Z</published>
    <updated>2018-06-11T11:46:14.096Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Combiner-组件"><a href="#1-Combiner-组件" class="headerlink" title="1.  Combiner 组件"></a>1.  Combiner 组件</h2><h3 id="1-产生缘由："><a href="#1-产生缘由：" class="headerlink" title="1. 产生缘由："></a>1. 产生缘由：</h3><p>Combiner 是 MapReduce 程序中 Mapper 和 Reducer 之外的一种组件，它的作用是在 maptask 之后给 maptask 的结果进行局部汇总，以减轻 reducetask 的计算负载，减少网络传输</p><p><strong>Combiner 组件的作用：</strong></p><ul><li>减少 reduce 端的数据量</li><li>减少 shuffle 过程的数据量</li><li>在 map 端做了一次合并，提高分布式计算程序的整体性能</li></ul><p><strong>Combiner 组件帮 reduce 分担压力， 因此其业务逻辑和 reduce 中的业务逻辑相似</strong></p><h3 id="2-自定义-Combiner-组件："><a href="#2-自定义-Combiner-组件：" class="headerlink" title="2.自定义 Combiner 组件："></a>2.自定义 Combiner 组件：</h3><p>默认情况下没有 Combiner 组件，Combiner 作用时间点 — map–combiner–reduce</p><ol><li><p>继承 Reduce 类</p><ul><li>public class  MyCombiner extends Reducer&lt;前两个： map 的输出， 后两个： reduce 的输入&gt;{}</li><li>我们在写 MapReduce 程序的时候， map 的输出就是 reduce 的输入</li><li>也就是说， 这个 MyCombiner() 的前两个泛型和后两个泛型的类型一致</li></ul></li><li><p>重写 reduce 方法</p><ul><li><p><strong>Combiner 本质上相当于 在 map 端进行了一次 reduce 操作， 通常情况下直接使用 reducer 的类作为 Combiner 的类，不再单独写 Combiner 代码逻辑</strong></p></li><li><p><strong>在 Job 中加上<code>job.setCombinerClass(WorldcountReduce.class)</code>， 就会调用 Combiner</strong></p></li></ul></li></ol><ol start="3"><li><p><strong>Combiner 使用原则</strong></p><ul><li>有或没有都<strong>不能影响业务逻辑</strong>，都<strong>不能影响最终结果</strong>。比如累加，最大值等，求平均值就不能用。</li></ul></li></ol><hr><h2 id="2、MapReduce-中的序列化"><a href="#2、MapReduce-中的序列化" class="headerlink" title="2、MapReduce 中的序列化"></a>2、MapReduce 中的序列化</h2><h3 id="2-1、概述"><a href="#2-1、概述" class="headerlink" title="2.1、概述"></a>2.1、概述</h3><p><strong>Java</strong> 的<strong>序列化</strong>是一个<strong>重量级序列化框架（Serializable）</strong>，一个对象被序列化后，会附带很多额 外的信息（各种校验信息，header，继承体系等），<strong>不便于在网络中高效传输</strong>；</p><p><strong>Hadoop 自己开发了一套序列化机制</strong>（参与序列化的对象的类都要<strong>实现 Writable 接口</strong>），精简，高效</p><h4 id="Java-基本类型-amp-Hadoop-类型对照表"><a href="#Java-基本类型-amp-Hadoop-类型对照表" class="headerlink" title="Java 基本类型 &amp; Hadoop 类型对照表"></a>Java 基本类型 &amp; Hadoop 类型对照表</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Java &amp; Hadoop类型参照</span></span><br><span class="line">hadoop数据类型      &lt;------------&gt;  java数据类型:  </span><br><span class="line">布尔型：  </span><br><span class="line">BooleanWritable     &lt;------------&gt;  <span class="keyword">boolean</span>  </span><br><span class="line">整型：  </span><br><span class="line">ByteWritable        &lt;------------&gt;  <span class="keyword">byte</span>  </span><br><span class="line">ShortWritable       &lt;------------&gt;  <span class="keyword">short</span>  </span><br><span class="line">IntWritable         &lt;------------&gt;  <span class="keyword">int</span>  </span><br><span class="line">LongWritable        &lt;------------&gt;  <span class="keyword">long</span>  </span><br><span class="line">浮点型：  </span><br><span class="line">FloatWritable       &lt;------------&gt;  <span class="keyword">float</span>  </span><br><span class="line">DoubleWritable      &lt;------------&gt;  <span class="keyword">double</span>  </span><br><span class="line">字符串（文本）：  </span><br><span class="line">Text                &lt;------------&gt;  String  </span><br><span class="line">数组：  </span><br><span class="line">ArrayWritable       &lt;------------&gt;  Array  </span><br><span class="line">map集合：  </span><br><span class="line">MapWritable         &lt;------------&gt;  map</span><br></pre></td></tr></table></figure><h3 id="2-2、自定义对象实现-MapReduce-框架的序列化"><a href="#2-2、自定义对象实现-MapReduce-框架的序列化" class="headerlink" title="2.2、自定义对象实现 MapReduce 框架的序列化"></a>2.2、自定义对象实现 MapReduce 框架的序列化</h3><p><strong>要实现<code>WritableComparable</code>接口</strong>，因为 MapReduce 框架中的 shuffle 过程一定会对 key 进行排序</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//序列化方法</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    out.writeUTF(phone);</span><br><span class="line">    out.writeLong(upfFlow);</span><br><span class="line">    out.writeLong(downFlow);</span><br><span class="line">    out.writeLong(sumFlow);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//反序列化方法</span></span><br><span class="line"><span class="comment">//注意： 字段的反序列化顺序与序列化时的顺序保持一致,並且类型也一致</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.phone = in.readUTF();</span><br><span class="line"><span class="keyword">this</span>.upfFlow = in.readLong();</span><br><span class="line"><span class="keyword">this</span>.downFlow = in.readLong();</span><br><span class="line"><span class="keyword">this</span>.sumFlow = in.readLong();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h2 id="3-MapReduce中的Sort-–TODO。。"><a href="#3-MapReduce中的Sort-–TODO。。" class="headerlink" title="3. MapReduce中的Sort –TODO。。"></a>3. MapReduce中的Sort –TODO。。</h2><p>MapTask –&gt; ReduceTask 之间， 框架<strong>默认</strong>添加了排序</p><p>排序的规则是<strong>按照Map 端输出的 key 的字典顺序进行排序</strong></p><h5 id="1、-如果没有重写-WritableComparable-时"><a href="#1、-如果没有重写-WritableComparable-时" class="headerlink" title="1、 如果没有重写 WritableComparable 时"></a>1、 如果没有重写 WritableComparable 时</h5><p> 按单词统计中词频出现的此处进行排序， 按照出现的次数， 从低到高</p><p>如果想要对词频进行排序， 那么词频应该放在 map 输出 key 的位置</p><p><strong>代码实现</strong>：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> Map </span><br><span class="line"><span class="comment">//词频为 key， 其它为 value</span></span><br><span class="line"></span><br><span class="line"> Reduce </span><br><span class="line"><span class="comment">// 将 map 输入的结果反转(k,v 换位置), 输出最终结果</span></span><br><span class="line"><span class="comment">// 最后输出还是按照左边词, 右边次数</span></span><br><span class="line"><span class="comment">// ps： 如果倒序排的时候, map 的时候发的时候 加上-,  reduce 发的时候, 再加上-, 转成 IntWritable</span></span><br></pre></td></tr></table></figure><h5 id="2、自定义排序要实现WritableComparable接口"><a href="#2、自定义排序要实现WritableComparable接口" class="headerlink" title="2、自定义排序要实现WritableComparable接口"></a>2、<strong>自定义排序要实现<code>WritableComparable</code>接口</strong></h5><ul><li><strong>自定义的类必须放在 key 的位置</strong></li><li><strong>实现<code>WritableComparable</code>接口</strong>， 重写 <code>compareTo()</code>方法</li><li>待扩展…</li></ul><blockquote><p> 作业： <strong>增强需求： 按照总流量排序， 总流量相同时， 按照手机号码排序</strong></p></blockquote><hr><h2 id="4、MapReduce-中的数据分发组件-Partitioner（分区）"><a href="#4、MapReduce-中的数据分发组件-Partitioner（分区）" class="headerlink" title="4、MapReduce 中的数据分发组件 Partitioner（分区）"></a>4、MapReduce 中的数据分发组件 <code>Partitioner（分区）</code></h2><p><strong>需求：</strong> 根据归属地<strong>输出</strong>流量统计数据<strong>结果到不同文件</strong>，以便于在查询统计结果时可以定位到 省级范围进行</p><p><strong>思路</strong>：MapReduce 中会将 map 输出的 kv 对，按照相同 key 分组，然后分发给不同的 reducetask</p><p><strong>执行时机</strong>: <strong>在Map输出 kv 对之后, 所携带的 k,v 参数，跟 Map 输出相同</strong></p><p><br></p><h4 id="MapReduce-默认的分发规则为："><a href="#MapReduce-默认的分发规则为：" class="headerlink" title="MapReduce 默认的分发规则为："></a><strong>MapReduce 默认的分发规则为</strong>：</h4><p><strong>根据 <code>key</code> 的 <code>hashcode%reducetask</code> 数来分发</strong>，所以：<strong>如果要按照我们自 己的需求进行分组，则需要改写数据分发（分区）组件 Partitioner</strong></p><p><br></p><h4 id="Partition重点总结："><a href="#Partition重点总结：" class="headerlink" title="Partition重点总结："></a>Partition重点总结：</h4><ul><li><p><strong>Partition 的 key value, 就是Mapper输出的key value</strong></p><p><code>public abstract int getPartition(KEY key, VALUE value, int numPartitions);</code></p><p><strong>输入是Map的结果对&lt;key, value&gt;和Reducer的数目，输出则是分配的Reducer（整数编号）</strong>。<strong>就是指定Mappr输出的键值对到哪一个reducer上去</strong>。系统缺省的Partitioner是HashPartitioner，它以key的Hash值对Reducer的数目取模，得到对应的Reducer。<strong>这样保证如果有相同的key值，肯定被分配到同一个reducre上。如果有N个reducer，编号就为0,1,2,3……(N-1)</strong>。</p></li><li><p>MapReduce 中会将 map 输出的 kv 对，按照相同 key 分组，然后分发给不同的 reducetask 默认的分发规则为:根据 key 的 hashcode%reducetask 数来分发，所以:如果要按照我们自 己的需求进行分组，则需要改写数据分发(分组)组件 Partitioner, 自定义一个 CustomPartitioner 继承抽象类:Partitioner</p></li><li><strong>因此， Partitioner 的执行时机， 是在Map输出 kv 对之后</strong></li></ul><h5 id="Partitioner-实现过程"><a href="#Partitioner-实现过程" class="headerlink" title="Partitioner 实现过程"></a><strong>Partitioner 实现过程</strong></h5><ol><li>先分析一下具体的业务逻辑，确定大概有多少个分区</li><li>首先书写一个类，它要<strong>继承 <code>org.apache.hadoop.mapreduce.Partitioner</code>这个抽象类</strong></li><li><strong>重写<code>public int getPartition</code>这个方法，根据具体逻辑，读数据库或者配置返回相同的数字</strong></li><li><strong>在<code>main</code>方法中设置<code>Partioner</code>的类，<code>job.setPartitionerClass(DataPartitioner.class)</code>;</strong></li><li><strong>设置<code>Reducer</code>的数量，<code>job.setNumReduceTasks(6)</code>;</strong></li></ol><h4 id="典型的-Partitioner-代码实现"><a href="#典型的-Partitioner-代码实现" class="headerlink" title="典型的 Partitioner 代码实现"></a>典型的 Partitioner 代码实现</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProvincePartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> HashMap&lt;String, Integer&gt; provincMap = <span class="keyword">new</span> HashMap&lt;String, Integer&gt;();</span><br><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line">provincMap.put(<span class="string">"138"</span>, <span class="number">0</span>);</span><br><span class="line">provincMap.put(<span class="string">"139"</span>, <span class="number">1</span>);</span><br><span class="line">provincMap.put(<span class="string">"136"</span>, <span class="number">2</span>);</span><br><span class="line">provincMap.put(<span class="string">"137"</span>, <span class="number">3</span>);</span><br><span class="line">provincMap.put(<span class="string">"135"</span>, <span class="number">4</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text key, FlowBean value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">Integer code = provincMap.get(key.toString().substring(<span class="number">0</span>, <span class="number">3</span>));</span><br><span class="line"><span class="keyword">if</span> (code != <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="keyword">return</span> code;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="number">5</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h2 id="5、全局计数器"><a href="#5、全局计数器" class="headerlink" title="5、全局计数器"></a>5、全局计数器</h2><h3 id="1-框架内置计数器："><a href="#1-框架内置计数器：" class="headerlink" title="1.  框架内置计数器："></a>1.  框架内置计数器：</h3><ul><li>Hadoop内置的计数器，主要用来记录作业的执行情况</li><li>内置计数器包括 <strong>MapReduce框架计数器</strong>（Map-Reduce Framework）<ul><li><strong>文件系统计数器（FielSystemCounters）</strong></li><li><strong>作业计数器（Job Counters）</strong></li><li><strong>文件输入格式计数器（File Output Format Counters）</strong></li><li><strong>文件输出格式计数器（File Input Format Counters)</strong></li></ul></li><li>计数器由相关的task进行维护，定期传递给tasktracker，再由tasktracker传给jobtracker；</li><li>最终的作业计数器实际上是有jobtracker维护，所以计数器可以被全局汇总，同时也不必在整个网络中传递</li><li>只有当一个作业执行成功后，最终的计数器的值才是完整可靠的；</li></ul><h3 id="2-自定义的计数器"><a href="#2-自定义的计数器" class="headerlink" title="2. 自定义的计数器"></a>2. 自定义的计数器</h3><h5 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h5><ul><li>用来统计运行过程中的进度和状态， 类似于 job 运行的一个报告、日志</li><li>要将数据处理过程中遇到的不合规数据行进行全局计数，类似这 种需求可以借助 MapReduce 框架中提供的全局计数器来实现</li><li><strong>计数器的值可以在mapper或reducer中增加</strong></li></ul><p><strong>使用方式</strong></p><ol><li><p>定义枚举类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">enum</span> Temperature&#123;  </span><br><span class="line">MISSING,  </span><br><span class="line">TOTAL  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>在map或者reduce中使用计数器 </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.自定义计数器</span></span><br><span class="line">Counter counter = context.getCounter(Temperature.TOTAL);  </span><br><span class="line"><span class="comment">// 2.为计数器赋初始值</span></span><br><span class="line">counter.setValue(<span class="keyword">long</span> value);</span><br><span class="line"><span class="comment">// 3.计数器工作</span></span><br><span class="line">counter.increment(<span class="keyword">long</span> incr);</span><br></pre></td></tr></table></figure></li><li><p>获取计数器</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Counters counters=job.getCounters(); </span><br><span class="line">Counter counter=counters.findCounter(LOG_PROCESSOR_COUNTER.BAD_RECORDS_LONG);<span class="comment">// 查找枚举计数器，假如Enum的变量为BAD_RECORDS_LONG </span></span><br><span class="line"><span class="keyword">long</span> value=counter.getValue();<span class="comment">//获取计数值</span></span><br></pre></td></tr></table></figure></li></ol><h4 id="计数器使用完整代码"><a href="#计数器使用完整代码" class="headerlink" title="计数器使用完整代码"></a>计数器使用完整代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** </span></span><br><span class="line"><span class="comment">* <span class="doctag">@Description</span> 假如一个文件，规范的格式是3个字段，“\t”作为分隔符，其中有2条异常数据，一条数据是只有2个字段，一条数据是有4个字段</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyCounter</span> </span>&#123;</span><br><span class="line">    <span class="comment">// \t键</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String TAB_SEPARATOR = <span class="string">"\t"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyCounterMap</span> <span class="keyword">extends</span></span></span><br><span class="line"><span class="class">            <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">        <span class="comment">// 定义枚举对象</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">enum</span> LOG_PROCESSOR_COUNTER &#123;</span><br><span class="line">            BAD_RECORDS_LONG, BAD_RECORDS_SHORT</span><br><span class="line">        &#125;;</span><br><span class="line">        </span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            String arr_value[] = value.toString().split(TAB_SEPARATOR);</span><br><span class="line">            <span class="keyword">if</span> (arr_value.length &gt; <span class="number">3</span>) &#123;</span><br><span class="line">                <span class="comment">/* 自定义计数器 */</span></span><br><span class="line">                context.getCounter(<span class="string">"ErrorCounter"</span>, <span class="string">"toolong"</span>).increment(<span class="number">1</span>);</span><br><span class="line">                <span class="comment">/* 枚举计数器 */</span></span><br><span class="line">                context.getCounter(LOG_PROCESSOR_COUNTER.BAD_RECORDS_LONG).increment(<span class="number">1</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (arr_value.length &lt; <span class="number">3</span>) &#123;</span><br><span class="line">                <span class="comment">// 自定义计数器</span></span><br><span class="line">                context.getCounter(<span class="string">"ErrorCounter"</span>, <span class="string">"tooshort"</span>).increment(<span class="number">1</span>);</span><br><span class="line">                <span class="comment">// 枚举计数器</span></span><br><span class="line">                context.getCounter(LOG_PROCESSOR_COUNTER.BAD_RECORDS_SHORT).increment(<span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@SuppressWarnings</span>(<span class="string">"deprecation"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException </span>&#123;</span><br><span class="line">        String[] args0 = &#123; </span><br><span class="line">                <span class="string">"hdfs://hadoop2:9000/buaa/counter/counter.txt"</span>,</span><br><span class="line">                <span class="string">"hdfs://hadoop2:9000/buaa/counter/out/"</span> </span><br><span class="line">            &#125;;</span><br><span class="line">        <span class="comment">// 读取配置文件</span></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 如果输出目录存在，则删除</span></span><br><span class="line">        Path mypath = <span class="keyword">new</span> Path(args0[<span class="number">1</span>]);</span><br><span class="line">        FileSystem hdfs = mypath.getFileSystem(conf);</span><br><span class="line">        <span class="keyword">if</span> (hdfs.isDirectory(mypath)) &#123;</span><br><span class="line">            hdfs.delete(mypath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 新建一个任务</span></span><br><span class="line">        Job job = <span class="keyword">new</span> Job(conf, <span class="string">"MyCounter"</span>);</span><br><span class="line">        <span class="comment">// 主类</span></span><br><span class="line">        job.setJarByClass(MyCounter.class);</span><br><span class="line">        <span class="comment">// Mapper</span></span><br><span class="line">        job.setMapperClass(MyCounterMap.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 输入目录</span></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(args0[<span class="number">0</span>]));</span><br><span class="line">        <span class="comment">// 输出目录</span></span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args0[<span class="number">1</span>]));</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 提交任务，并退出</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>注意点：在没有 ReduceTask 的时候，  <code>job.setNumReduceTasks(0);</code></strong></p><p><a href="https://blog.csdn.net/qq_35732963/article/details/53358033" target="_blank" rel="noopener">关于计数器，详情可参考</a></p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-Combiner-组件&quot;&gt;&lt;a href=&quot;#1-Combiner-组件&quot; class=&quot;headerlink&quot; title=&quot;1.  Combiner 组件&quot;&gt;&lt;/a&gt;1.  Combiner 组件&lt;/h2&gt;&lt;h3 id=&quot;1-产生缘由：&quot;&gt;&lt;a href=
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce笔记-1  WordCount, MapReduce运行机制</title>
    <link href="https://airpoet.github.io/2018/06/06/Hadoop/Study/2-MapReduce/MapReduce%E7%AC%94%E8%AE%B0-1/"/>
    <id>https://airpoet.github.io/2018/06/06/Hadoop/Study/2-MapReduce/MapReduce笔记-1/</id>
    <published>2018-06-06T09:41:22.111Z</published>
    <updated>2018-06-14T23:57:32.018Z</updated>
    
    <content type="html"><![CDATA[<p>参考链接:</p><p><a href="https://mubu.com/doc/254d__SRSn" target="_blank" rel="noopener">hdfs 笔记</a> </p><p><a href="https://mubu.com/doc/1BuHQkjk0G" target="_blank" rel="noopener">mapreduce 笔记</a></p><h1 id="1、MapReduce-入门"><a href="#1、MapReduce-入门" class="headerlink" title="1、MapReduce 入门"></a>1、MapReduce 入门</h1><h3 id="1-1、MapReduce概念"><a href="#1-1、MapReduce概念" class="headerlink" title="1.1、MapReduce概念"></a>1.1、MapReduce概念</h3><p><strong>hadoop 的四大组件：</strong></p><ul><li><strong>HDFS</strong>：分布式存储系统</li><li><strong>MapReduce</strong>：分布式计算系统</li><li><strong>YARN</strong>：hadoop 的资源调度系统</li><li><strong>Common</strong>：以上三大组件的底层支撑组件，主要提供基础工具包和 RPC 框架等</li></ul><p>MapReduce 是一个分布式运算程序的编程框架，是用户开发“<strong>基于 Hadoop 的数据分析应用</strong>” 的核心框架</p><p>MapReduce <strong>核心功能</strong> ：<strong>将用户编写的业务逻辑代码</strong>和<strong>自带默认组件</strong>整合成一个完整的<strong>分布 式运算程序</strong>，<strong>并发运行</strong>在一个 Hadoop <strong>集群</strong>上</p><h3 id="1-2、为什么需要-MapReduce？"><a href="#1-2、为什么需要-MapReduce？" class="headerlink" title="1.2、为什么需要 MapReduce？"></a>1.2、为什么需要 MapReduce？</h3><p>引入 MapReduce 框架后，<strong>开发人员</strong>可以将绝大部分工作<strong>集中在业务逻辑</strong>的开发上，而将 <strong>分布式计算中的复杂性交由框架来处理</strong></p><p>Hadoop 当中的 <strong>MapReduce</strong> <strong>分布式程序运算框架</strong>的<strong>整体结构</strong>如下：</p><blockquote><p><strong>MRAppMaster</strong>：MapReduce Application Master，分配任务，协调任务的运行</p><p><strong>MapTask</strong>：阶段并发任，负责 mapper 阶段的任务处理</p><p>YARNChild</p><p><strong>ReduceTask</strong>：阶段汇总任务，负责 reducer 阶段的任务处理</p><p>YARNChild</p></blockquote><h3 id="1-3、MapReduce-的编写规范"><a href="#1-3、MapReduce-的编写规范" class="headerlink" title="1.3、MapReduce 的编写规范"></a>1.3、MapReduce 的编写规范</h3><p>MapReduce 程序编写规范：</p><ol><li>用户编写的程序分成<strong>三个部分</strong>：<strong>Mapper，Reducer，Driver</strong>(提交运行 MR 程序的客户端)</li><li>Mapper 的输入数据是 KV 对的形式（KV 的类型可自定义）</li><li>Mapper 的输出数据是 KV 对的形式（KV 的类型可自定义）</li><li>Mapper 中的业务逻辑写在 map()方法中</li><li>map()方法（maptask 进程）对每一个&lt;K,V&gt;调用一次</li><li>Reducer 的输入数据类型对应 Mapper 的输出数据类型，也是 KV 对的形式</li><li>Reducer 的业务逻辑写在 reduce()方法中</li><li>Reducetask 进程对每一组相同 k 的&lt;K,V&gt;组调用一次 reduce()方法</li><li>用户自定义的 Mapper 和 Reducer 都要继承各自的父类</li><li>整个程序需要一个 Drvier 来进行提交，提交的是一个描述了各种必要信息的 job 对象</li></ol><h3 id="1-4、WordCount-程序"><a href="#1-4、WordCount-程序" class="headerlink" title="1.4、WordCount 程序"></a>1.4、WordCount 程序</h3><h4 id="1、业务逻辑"><a href="#1、业务逻辑" class="headerlink" title="1、业务逻辑"></a>1、业务逻辑</h4><ol><li><p>maptask阶段处理每个数据分块的单词统计分析，思路是每遇到一个单词则把其转换成一个 key-value对，比如单词  hello，就转换成&lt;’hello’,1&gt;发送给 reducetask去汇总</p></li><li><p>reducetask阶段将接受  maptask的结果，来做汇总计数</p></li></ol><h4 id="2、具体代码实现"><a href="#2、具体代码实现" class="headerlink" title="2、具体代码实现"></a>2、具体代码实现</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"> Map </span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">// 计算任务代码：切割单词，输出每个单词计 1 的 key-value 对</span></span><br><span class="line">String[] words = value.toString().split(<span class="string">" "</span>);</span><br><span class="line"><span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">context.write(<span class="keyword">new</span> Text(word), <span class="keyword">new</span> IntWritable(<span class="number">1</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> Reduce </span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">// 汇总计算代码：对每个 key 相同的一组 key-value 做汇总统计</span></span><br><span class="line"><span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (IntWritable v : values) &#123;</span><br><span class="line">sum += v.get();</span><br><span class="line">&#125;</span><br><span class="line">context.write(key, <span class="keyword">new</span> IntWritable(sum));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> main </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 指定 hdfs 相关的参数</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://hadoop02:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"hadoop"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 新建一个 job 任务</span></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置 jar 包所在路径</span></span><br><span class="line">job.setJarByClass(WordCountMR.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 mapper 类和 reducer 类</span></span><br><span class="line">job.setMapperClass(WordCountMapper.class);</span><br><span class="line">job.setReducerClass(WordCountReducer.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 maptask 的输出类型</span></span><br><span class="line">job.setMapOutputKeyClass(Text.class);</span><br><span class="line">job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 reducetask 的输出类型</span></span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定该 mapreduce 程序数据的输入和输出路径</span></span><br><span class="line">Path inputPath = <span class="keyword">new</span> Path(<span class="string">"/wordcount/input"</span>);</span><br><span class="line">Path outputPath = <span class="keyword">new</span> Path(<span class="string">"/wordcount/output"</span>);</span><br><span class="line">FileInputFormat.setInputPaths(job, inputPath);</span><br><span class="line">FileOutputFormat.setOutputPath(job, outputPath);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 最后提交任务</span></span><br><span class="line"><span class="keyword">boolean</span> waitForCompletion = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">System.exit(waitForCompletion?<span class="number">0</span>:<span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="2、MapReduce-程序的核心运行机制"><a href="#2、MapReduce-程序的核心运行机制" class="headerlink" title="2、MapReduce 程序的核心运行机制"></a>2、MapReduce 程序的核心运行机制</h1><h3 id="2-1、概述"><a href="#2-1、概述" class="headerlink" title="2.1、概述"></a>2.1、概述</h3><p>一个完整的 MapReduce 程序在分布式运行时有两类实例进程：</p><ol><li>MRAppMaster：负责整个程序的过程调度及状态协调</li><li>Yarnchild：负责 map 阶段的整个数据处理流程</li><li>Yarnchild：负责 reduce 阶段的整个数据处理流程</li></ol><p>以上两个阶段 MapTask 和 ReduceTask 的进程都是 YarnChild ， 并不是说这 MapTask 和 ReduceTask 就跑在同一个 YarnChild 进行里</p><h3 id="2-2、MapReduce-程序的运行流程"><a href="#2-2、MapReduce-程序的运行流程" class="headerlink" title="2.2、MapReduce 程序的运行流程"></a>2.2、MapReduce 程序的运行流程</h3><ol><li>一个 mr 程序启动的时候，最先启动的是 MRAppMaster，MRAppMaster 启动后根据本次 job 的描述信息，计算出需要的 maptask 实例数量，然后向集群申请机器启动相应数量的 maptask 进程</li><li>maptask 进程启动之后，根据给定的数据切片(哪个文件的哪个偏移量范围)范围进行数 据处理，主体流程为：<ul><li>利用客户指定的 InputFormat 来获取 RecordReader 读取数据，形成输入 KV 对</li><li>将输入 KV 对传递给客户定义的 map()方法，做逻辑运算，并将 map()方法输出的 KV 对收 集到缓存</li><li>将缓存中的 KV 对按照 K 分区排序后不断溢写到磁盘文件</li></ul></li><li>MRAppMaster 监控到所有 maptask 进程任务完成之后（真实情况是，某些 maptask 进 程处理完成后，就会开始启动 reducetask 去已完成的 maptask 处 fetch 数据），会根据客户指 定的参数启动相应数量的 reducetask 进程，并告知 reducetask 进程要处理的数据范围（数据 分区）</li><li>Reducetask 进程启动之后，根据 MRAppMaster 告知的待处理数据所在位置，从若干台 maptask 运行所在机器上获取到若干个 maptask 输出结果文件，并在本地进行重新<strong>归并排序</strong>， 然后按照相同 key 的 KV 为一个组，调用客户定义的 reduce()方法进行逻辑运算，并收集运 算输出的结果 KV，然后调用客户指定的 OutputFormat 将结果数据输出到外部存储</li></ol><h3 id="2-3、MapTask-并行度决定机制"><a href="#2-3、MapTask-并行度决定机制" class="headerlink" title="2.3、MapTask 并行度决定机制"></a>2.3、MapTask 并行度决定机制</h3><p><strong>将待处理数据执行逻辑切片（即按照一个特定切片大小，将待处理数据划分成逻辑上的多 个 split），然后每一个 split 分配一个 mapTask 并行实例处理</strong>。</p><p>这段逻辑及形成的切片规划描述文件，是由 FileInputFormat 实现类的 getSplits()方法完成的。 该方法返回的是 List<inputsplit>，InputSplit 封装了每一个逻辑切片的信息，包括长度和位置 信息，而 getSplits()方法返回一组 InputSplit。</inputsplit></p><h3 id="2-4、切片机制"><a href="#2-4、切片机制" class="headerlink" title="2.4、切片机制"></a>2.4、切片机制</h3><h4 id="FileInputFormat-中默认的切片机制"><a href="#FileInputFormat-中默认的切片机制" class="headerlink" title="FileInputFormat 中默认的切片机制"></a><strong>FileInputFormat 中默认的切片机制</strong></h4><ol><li>简单地按照文件的内容长度进行切片</li><li>切片大小，默认等于 block 大小</li><li>切片时不考虑数据集整体，而是逐个针对每一个文件单独切片</li></ol><blockquote><p>比如待处理数据有两个文件：</p><p>File1.txt    200M </p><p>File2.txt    100M </p><p>经过 getSplits()方法处理之后，形成的切片信息是：</p><p>File1.txt-split1    0-128M</p><p>File1.txt-split2    129M-200M</p><p>File2.txt-split1    0-100M</p></blockquote><h4 id="FileInputFormat-中切片的大小的参数配置"><a href="#FileInputFormat-中切片的大小的参数配置" class="headerlink" title="FileInputFormat 中切片的大小的参数配置"></a>FileInputFormat 中切片的大小的参数配置</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 通过分析源码，在 FileInputFormat 中，计算切片大小的逻辑：</span></span><br><span class="line"><span class="keyword">long</span> splitSize = computeSplitSize(blockSize, minSize, maxSize)，翻译一下就是求这三个值的中 间值</span><br><span class="line"></span><br><span class="line"><span class="comment">// 切片主要由这几个值来运算决定：</span></span><br><span class="line">blocksize：默认是 <span class="number">128</span>M，可通过 dfs.blocksize 修改</span><br><span class="line">minSize：默认是 <span class="number">1</span>，可通过 mapreduce.input.fileinputformat.split.minsize 修改</span><br><span class="line">maxsize：默认是 Long.MaxValue，可通过 mapreduce.input.fileinputformat.split.maxsize 修改</span><br><span class="line"></span><br><span class="line"><span class="comment">//因此，如果 maxsize 调的比 blocksize 小，则切片会小于 blocksize;  如果 minsize 调的比 blocksize 大，则切片会大于 blocksize</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 但是，不论怎么调参数，都不能让多个小文件“划入”一个 split</span></span><br></pre></td></tr></table></figure><h3 id="2-5、MapTask-并行度经验之谈"><a href="#2-5、MapTask-并行度经验之谈" class="headerlink" title="2.5、MapTask 并行度经验之谈"></a>2.5、MapTask 并行度经验之谈</h3><p>如果硬件配置为 2*12core + 64G，恰当的 map 并行度是大约每个节点 20-100 个 map，最好 每个 map 的执行时间至少一分钟。</p><ol><li>如果 job 的每个 map 或者 reduce task 的运行时间都只有 30-40 秒钟，那么就减少该 job 的 map 或者 reduce 数，每一个 task(map|reduce)的 setup 和加入到调度器中进行调度，这个 中间的过程可能都要花费几秒钟，所以如果每个 task 都非常快就跑完了，就会在 task 的开 始和结束的时候浪费太多的时间。</li></ol><p><strong>配置 task 的 JVM 重用</strong>可以改善该问题：</p><ul><li><strong>mapred.job.reuse.jvm.num.tasks</strong>，默认是 1，表示一个 JVM 上最多可以顺序执行的 task 数目（属于同一个 Job）是 1。也就是说一个 task 启一个 JVM。</li><li>这个值可以在 <strong>mapred-site.xml</strong> 中进行更改，当设置成多个，就意味着这多个 task 运行在同一个 JVM 上，但不是同时执行， 是排队顺序执行</li></ul><ol start="2"><li>如果 input 的<strong>文件非常的大</strong>，比如 1TB，<strong>可以考虑</strong>将 hdfs 上的每个 <strong>blocksize 设大</strong>，比如 设成 256MB 或者 512MB</li></ol><h3 id="2-6、ReduceTask-并行度决定机制"><a href="#2-6、ReduceTask-并行度决定机制" class="headerlink" title="2.6、ReduceTask 并行度决定机制"></a>2.6、ReduceTask 并行度决定机制</h3><p><strong>reducetask</strong> 的并行度同样影响整个 job 的执行并发度和执行效率，但与 maptask 的并发数由 切片数决定不同，Reducetask 数量的决定是可以直接手动设置：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置 ReduceTask 的并行度</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">4</span>);</span><br></pre></td></tr></table></figure><p>默认值是 1，</p><p>手动设置为 4，表示运行 4 个 reduceTask，</p><p>设置为 0，表示不运行 reduceTask 任务，也就是没有 reducer 阶段，只有 mapper 阶段</p><p><br></p><p>如果<strong>数据分布不均匀</strong>，就有<strong>可能在 reduce 阶段产生数据倾斜</strong></p><p>注意：reducetask 数量并不是任意设置，还要<strong>考虑业务逻辑需求</strong>，有些情况下，需要计算<strong>全局汇总结果</strong>，就<strong>只能有 1 个 reducetask</strong></p><p>尽量不要运行太多的 reducetask。对大多数 job 来说，最好 rduce 的个数最多和集群中的 reduce 持平，或者比集群的 reduce slots 小。这个对于小集群而言，尤其重要。</p><p><strong>最好的ReduceTask 个数是：datanode 个数 *  0.75~0.95 左右</strong></p><hr><h1 id="3-昨日复习"><a href="#3-昨日复习" class="headerlink" title="3. 昨日复习"></a>3. 昨日复习</h1><p>1.MapReduce 的 wc 编程</p><ul><li>手写代码<ul><li>Mapper</li><li>Reducer</li><li>Driver</li></ul></li></ul><p>2.MapTask 的并行度</p><ul><li>在程序执行的时候运行的 maptask 的总个数</li></ul><p>3.ReduceTask的并行度问题</p><ul><li>ReduceTask 的并行度设置依赖于自己传入的参数</li><li>一般经验： ReduceTask 的个数应该 = datanode 的阶段数 * （0.75~0.95）</li><li>ReduceTask 在设置的时候的并行度有一定的瓶颈</li><li>分区： 决定 ReduceTask 中的数据怎么分配的<ul><li>默认分区方式</li><li>自定义分区</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;参考链接:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://mubu.com/doc/254d__SRSn&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;hdfs 笔记&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://mubu.com/doc/1
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>▍Do Not Go Gentle into That Good Night</title>
    <link href="https://airpoet.github.io/2018/06/04/Poetry/%E4%B8%8D%E8%A6%81%E6%B8%A9%E5%92%8C%E7%9A%84%E8%B5%B0%E8%BF%9B%E9%82%A3%E4%B8%AA%E8%89%AF%E5%A4%9C/"/>
    <id>https://airpoet.github.io/2018/06/04/Poetry/不要温和的走进那个良夜/</id>
    <published>2018-06-03T16:38:57.809Z</published>
    <updated>2018-06-09T17:19:23.133Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-03-521528043568_.pic.jpg" alt=""></p><p> <br></p><p>Do not go gentle into that good night,</p><p>Old age should burn and rave at close of the day;</p><p>Rage, rage against the dying of the light.</p><p><br></p><p>Though wise men at their end know dark is right,</p><p>Because their words had forked no lightning they</p><p>Do not go gentle into that good night.</p><p> <br></p><p>Good men, the last wave by, crying how bright</p><p>Their frail deeds might have danced in a green bay,</p><p>Rage, rage against the dying of the light.</p><p><br></p><p>Wild men, who caught and sang the sun in flight,</p><p>And learn, too late, they grieved it on its way,</p><p>Do not go gentle into that good night.</p><p><br></p><p>Grave men, near death, who see with blinding sight</p><p>Blind eyes could blaze like meteors and be gay,</p><p>Rage, rage against the dying of the light.</p><p><br></p><p>And you, my father, there on the sad height,</p><p>Curse, bless, me now with your fierce tears, I pray.</p><p>Do not go gentle into that good night.</p><p>Rage, rage against the dying of the light.</p><p><br></p><hr><p>  <br></p><p>《不要温和地走进那个良夜》 -巫宁坤译本</p><p>​    <br></p><p>不要温和地走进那个良夜，</p><p>老年应当在日暮时燃烧咆哮；</p><p>怒斥，怒斥光明的消逝。</p><p>  <br></p><p>虽然智慧的人临终时懂得黑暗有理，</p><p>因为他们的话没有进发出闪电，他们</p><p>也并不温和地走进那个良夜。</p><p><br>  </p><p>善良的人，当最后一浪过去，高呼他们脆弱的善行</p><p>可能曾会多么光辉地在绿色的海湾里舞蹈，</p><p>怒斥，怒斥光明的消逝。</p><p>狂暴的人抓住并歌唱过翱翔的太阳，</p><p>懂得，但为时太晚，他们使太阳在途中悲伤，</p><p>也并不温和地走进那个良夜。</p><p>  <br></p><p>严肃的人，接近死亡，用炫目的视觉看出</p><p>失明的跟睛可以像流星一样闪耀欢欣，</p><p>怒斥，恕斥光明的消逝。</p><p>  <br></p><p>您啊，我的父亲，在那悲哀的高处。</p><p>现在用您的热泪诅咒我，祝福我吧。我求您</p><p>不要温和地走进那个良夜。</p><p>怒斥，怒斥光明的消逝</p><p><br></p><hr><p>《不要温顺地走入那长夜》 -和菜头译本</p><p><br></p><p>白日将尽，暮年仍应燃烧咆哮</p><p>狂怒吧，狂怒吧！</p><p>对抗着光明渐逝</p><p><br></p><p>虽然智者深知</p><p>人之将死，黑暗自有其时</p><p>只因他们所言未曾裂天如电</p><p>他们不要温顺地走入那长夜</p><p><br></p><p>随着最后一浪，善人在哭喊</p><p>哭喊那脆弱的善行</p><p>它本应何其欢快</p><p>在绿色峡湾里起舞</p><p>狂怒吧，狂怒吧！</p><p>对抗着光明渐逝。</p><p><br></p><p>狂人曾抓住飞驰的太阳</p><p>放声歌唱</p><p>太晚，他们才感到其中的伤感</p><p>不要温顺地走进那长夜</p><p><br></p><p>严肃的人行将死去时</p><p>用那渐渐失神的目光去看</p><p>盲瞳却如流星璀璨，欢欣溢满</p><p>狂怒吧，狂怒吧！</p><p>对抗着光明渐逝</p><p><br></p><p>还有你啊，我的父亲，远在悲伤的高地</p><p>我恳请你现在</p><p>就让你诅咒，你的祝福</p><p>随着热泪落下</p><p>不要温顺地走进那长夜</p><p>狂怒吧，狂怒吧！</p><p>对抗这光明渐逝</p><p><br></p><hr><p>​       《 绝不向黑夜请安》  -高晓松译本</p><p>绝不向黑夜请安</p><p>老朽请于白日尽头涅槃</p><p>咆哮于光之消散</p><p><br></p><p>先哲虽败于幽暗</p><p>诗歌终不能将苍穹点燃</p><p>绝不向黑夜请安</p><p><br></p><p>贤者舞蹈于碧湾</p><p>为惊涛淹没的善行哭喊</p><p>咆哮于光之消散</p><p><br></p><p>狂者如夸父逐日</p><p>高歌中顿觉迟来的伤感</p><p>绝不向黑夜请安</p><p><br></p><p>逝者于临终迷幻</p><p>盲瞳怒放出流星的灿烂</p><p>咆哮于光之消散</p><p><br></p><p>那么您，我垂垂将死的父亲</p><p>请掬最后一捧热泪降临</p><p>请诅咒，请保佑</p><p>我祈愿，绝不向</p><p>黑夜请安，咆哮</p><p>于光之消散</p><p><br> </p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-03-521528043568_.pic.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt; 
      
    
    </summary>
    
      <category term="文艺" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/"/>
    
      <category term="诗歌" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/%E8%AF%97%E6%AD%8C/"/>
    
    
      <category term="诗歌" scheme="https://airpoet.github.io/tags/%E8%AF%97%E6%AD%8C/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop学习笔记-3 HDFS 原理剖析</title>
    <link href="https://airpoet.github.io/2018/06/03/Hadoop/Study/1-HDFS/HDFS-2-%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90/"/>
    <id>https://airpoet.github.io/2018/06/03/Hadoop/Study/1-HDFS/HDFS-2-原理剖析/</id>
    <published>2018-06-03T09:12:34.396Z</published>
    <updated>2018-06-11T11:45:23.066Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-HDFS体系结构"><a href="#1-HDFS体系结构" class="headerlink" title="1.  HDFS体系结构"></a>1.  HDFS体系结构</h2><p>主从。。。</p><p><br></p><h2 id="2-NameNode"><a href="#2-NameNode" class="headerlink" title="2.NameNode"></a>2.NameNode</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><ul><li><p>[x] 是整个文件系统的管理节点。它维护着整个文件系统的文件目录树，文件/目录的元信息和每个文件对应的数据块列表。接收用户的操作请求。 </p></li><li><p>[x] <strong>在<code>hdfs-site.xml</code>中的<code>dfs.namenode.name.dir</code>属性</strong> </p></li><li><p>[x] 文件包括： </p><ul><li><p>[x] 文件包括:</p><p><strong>①fsimage</strong>:元数据镜像文件。存储某一时段<code>NameNode</code>内存元数据信息。</p><p><strong>②edits</strong>:操作日志文件。</p><p><strong>③fstime</strong>:保存最近一次<code>checkpoint</code>的时间</p><p><strong>以上这些文件是保存在linux的文件系统中。</strong></p></li></ul></li></ul><h3 id="查看-fsimage-和-edits的内容"><a href="#查看-fsimage-和-edits的内容" class="headerlink" title="查看 fsimage 和 edits的内容"></a>查看 <code>fsimage</code> 和 <code>edits</code>的内容</h3><ol><li><p>查看 <code>NameNode中</code> <code>fsimage</code> 的内容 </p><ul><li><p>查看 <strong>fsimage镜像文件</strong>内容<code>Usage: bin/hdfs oiv [OPTIONS] -i INPUTFILE -o  OUTPUTFILE</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以知道数据存在那个哪个 fsimage 镜像中</span></span><br><span class="line">------------------------------------</span><br><span class="line"><span class="comment"># 使用离线的查看器 输出到网页查看</span></span><br><span class="line">oiv -i hadoopdata/namenode/current/fsimage_0000000000000000250 -o 0000000000000000250</span><br><span class="line"></span><br><span class="line"><span class="comment"># 出现这样的提示</span></span><br><span class="line">INFO offlineImageViewer.WebImageViewer: WebImageViewer started. Listening on /127.0.0.1:5978. Press Ctrl+C to stop the viewer.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 另起一个窗口查看</span></span><br><span class="line">hadoop fs -ls -R webhdfs://127.0.0.1:5978</span><br><span class="line">------------------------------------</span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以导出到 xml 文件</span></span><br><span class="line">bin/hdfs oiv -p XML -i  tmp/dfs/name/current/fsimage_0000000000000000055 -o fsimage.xml</span><br></pre></td></tr></table></figure></li></ul></li></ol><ol start="2"><li><p>查看<code>edits</code>文件， 也可以导出到 xml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看edtis内容</span></span><br><span class="line">bin/hdfs oev -i tmp/dfs/name/current/edits_0000000000000000057-0000000000000000186   -o edits.xml</span><br></pre></td></tr></table></figure></li></ol><p><br></p><h2 id="3-Datanode"><a href="#3-Datanode" class="headerlink" title="3.  Datanode"></a>3.  Datanode</h2><p><strong>提供真实文件数据的存储服务</strong></p><ul><li><p><strong>Datanode 节点的数据切块存储位置</strong></p><ul><li><p><code>~/hadoopdata/datanode/current/BP-1070660005-192.168.170.131-1527865615372/current/finalized/subdir0/subdir0</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[ap@cs2]~/hadoopdata/datanode/current/BP-1070660005-192.168.170.131-1527865615372/current/finalized/subdir0/subdir0% ll</span><br><span class="line">总用量 213340</span><br><span class="line">-rw-r--r-- 1 ap ap 134217728 6月   2 13:35 blk_1073741842</span><br><span class="line">-rw-r--r-- 1 ap ap   1048583 6月   2 13:35 blk_1073741842_1018.meta</span><br><span class="line">-rw-r--r-- 1 ap ap  82527955 6月   2 13:35 blk_1073741843</span><br><span class="line">-rw-r--r-- 1 ap ap    644759 6月   2 13:35 blk_1073741843_1019.meta</span><br><span class="line">-rw-r--r-- 1 ap ap        13 6月   3 02:12 blk_1073741850</span><br><span class="line">-rw-r--r-- 1 ap ap        11 6月   3 02:12 blk_1073741850_1028.meta</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p>文件块（block）：最基本的存储单位。对于文件内容而言，一个文件的长度大小是size，那么从文件的０偏移开始，按照固定的大小，顺序对文件进行划分并编号，划分好的每一个块称一个Block。HDFS默认Block大小是128MB，以一个256MB文件，共有256/128=2个Block.</p></li><li><p>不同于普通文件系统的是，<strong>HDFS中，如果一个文件小于一个数据块的大小，并不占用整个数据块存储空间, 按文件大小的实际容量存储</strong></p></li><li><p>Replication。多复本。默认是三个。</p><ul><li><code>hdfs-site.xml</code>的<code>dfs.replication</code>属性 </li><li>手动设置某个文件的副本数为3个<ul><li><code>bin/hdfs dfs -setrep 3 /a.txt</code></li></ul></li></ul></li></ul><h2 id="4-数据存储：-写文件解析"><a href="#4-数据存储：-写文件解析" class="headerlink" title="4.  数据存储： 写文件解析"></a>4.  数据存储： 写文件解析</h2><ul><li><p>[x] <strong>疑点</strong>： HDFS client上传数据到HDFS时，会<strong>首先在本地缓存数据</strong>，当<strong>数据达到一个block大小时，请求NameNode分配一个block。</strong>NameNode会把block所在的DataNode的地址告诉HDFS client。HDFS client会直接和DataNode通信，把数据写到DataNode节点一个block文件中。 </p><blockquote><p>问题： 如果一直写的数据都没有达到一个 block 大小， 那怎么存储？？</p></blockquote></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-03-125641.png" alt="image-20180603205640605"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-03-125412.png" alt="image-20180603205412264"></p><h3 id="写文件的过程："><a href="#写文件的过程：" class="headerlink" title="写文件的过程："></a>写文件的过程：</h3><ol><li>首先调用FileSystem对象的open方法，其实是一个DistributedFileSystem的实例</li><li>DistributedFileSystem通过rpc获得文件的第一批个block的locations，同一block按照重复数会返回多个locations，这些locations按照hadoop拓扑结构排序，距离客户端近的排在前面.</li><li>前两步会返回一个FSDataInputStream对象，该对象会被封装成DFSInputStream对象，DFSInputStream可以方便的管理datanode和namenode数据流。客户端调用read方法，DFSInputStream最会找出离客户端最近的datanode并连接。</li><li>数据从datanode源源不断的流向客户端。</li><li>如果第一块的数据读完了，就会关闭指向第一块的datanode连接，接着读取下一块。这些操作对客户端来说是透明的，客户端的角度看来只是读一个持续不断的流。</li><li>如果第一批block都读完了，DFSInputStream就会去namenode拿下一批blocks的location，然后继续读，如果所有的块都读完，这时就会关闭掉所有的流。</li></ol><blockquote><p> 如果在读数据的时候，DFSInputStream和datanode的通讯发生异常，就会尝试正在读的block的排第二近的datanode,并且会记录哪个datanode发生错误，剩余的blocks读的时候就会直接跳过该datanode。DFSInputStream也会检查block数据校验和，如果发现一个坏的block,就会先报告到namenode节点，然后DFSInputStream在其他的datanode上读该block的镜像</p></blockquote><blockquote><p>该设计的方向就是客户端直接连接datanode来检索数据并且namenode来负责为每一个block提供最优的datanode，namenode仅仅处理block location的请求，这些信息都加载在namenode的内存中，hdfs通过datanode集群可以承受大量客户端的并发访问。</p></blockquote><p><br></p><h2 id="5-数据存储：-读文件解析"><a href="#5-数据存储：-读文件解析" class="headerlink" title="5.  数据存储： 读文件解析"></a>5.  数据存储： 读文件解析</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-03-125556.png" alt="image-20180603205556077"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-03-125203.png" alt="image-20180603205203151"></p><h3 id="读文件的过程"><a href="#读文件的过程" class="headerlink" title="读文件的过程"></a>读文件的过程</h3><ol><li>首先调用FileSystem对象的open方法，其实是一个DistributedFileSystem的实例</li><li>DistributedFileSystem通过rpc获得文件的第一批个block的locations，同一block按照重复数会返回多个locations，这些locations按照hadoop拓扑结构排序，距离客户端近的排在前面.</li><li>前两步会返回一个FSDataInputStream对象，该对象会被封装成DFSInputStream对象，DFSInputStream可以方便的管理datanode和namenode数据流。客户端调用read方法，DFSInputStream最会找出离客户端最近的datanode并连接。</li><li>数据从datanode源源不断的流向客户端。</li><li>如果第一块的数据读完了，就会关闭指向第一块的datanode连接，接着读取下一块。这些操作对客户端来说是透明的，客户端的角度看来只是读一个持续不断的流。</li><li>如果第一批block都读完了，DFSInputStream就会去namenode拿下一批blocks的location，然后继续读，如果所有的块都读完，这时就会关闭掉所有的流。</li></ol><blockquote><p> 如果在读数据的时候，DFSInputStream和datanode的通讯发生异常，就会尝试正在读的block的排第二近的datanode,并且会记录哪个datanode发生错误，剩余的blocks读的时候就会直接跳过该datanode。DFSInputStream也会检查block数据校验和，如果发现一个坏的block,就会先报告到namenode节点，然后DFSInputStream在其他的datanode上读该block的镜像</p></blockquote><blockquote><p>该设计的方向就是客户端直接连接datanode来检索数据并且namenode来负责为每一个block提供最优的datanode，namenode仅仅处理block location的请求，这些信息都加载在namenode的内存中，hdfs通过datanode集群可以承受大量客户端的并发访问。</p></blockquote><p><br></p><h2 id="6-Hadoop-Archives-（HAR-files）"><a href="#6-Hadoop-Archives-（HAR-files）" class="headerlink" title="6.Hadoop Archives （HAR files）"></a>6.Hadoop Archives （HAR files）</h2><p><strong>Hadoop Archives (HAR files)</strong>是在0.18.0版本中引入的，它的出现就是为了<strong>缓解大量小文件消耗namenode内存</strong>的问题。HAR文件是<strong>通过在HDFS上构建一个层次化的文件系统</strong>来工作。一个HAR文件是<strong>通过hadoop的archive命令来创建</strong>，而这个命令<strong>实 际上也是运行了一个MapReduce任务来将小文件打包成HAR</strong>。对于client端来说，使用HAR文件没有任何影响。所有的原始文件都 （using har://URL）。但在HDFS端它内部的文件数减少了。</p><p>通过HAR来读取一个文件并不会比直接从HDFS中读取文件高效，而且实际上可能还会稍微低效一点，因为对每一个HAR文件的访问都需要完成两层 index文件的读取和文件本身数据的读取。并且尽管HAR文件可以被用来作为MapReduce job的input，但是并没有特殊的方法来使maps将HAR文件中打包的文件当作一个HDFS文件处理。</p><p><strong>打包出来的 har 文件在<code>xxx.har/part-0</code>  中， contentz-size 跟原来的文件总大小一样</strong></p><p><strong>创建文件</strong> <code>hadoop archive -archiveName xxx.har -p  /src  /dest</code><br><strong>查看内容</strong> <code>hadoop fs -lsr har:///dest/xxx.har</code> 可以原封不动的显示出来</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打包成 har</span></span><br><span class="line">hadoop archive -archiveName test.har -p /user/<span class="built_in">test</span> /</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看har 文件</span></span><br><span class="line">[ap@cs1]~% hadoop fs -count /test.har/part-0</span><br><span class="line">           0(目录数)            1(文件数)         72(文件大小)    /test.har/part-0 (文件名)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看打包前文件</span></span><br><span class="line">[ap@cs1]~% hadoop fs -count /user/<span class="built_in">test</span></span><br><span class="line">           1            2                 72 /user/<span class="built_in">test</span></span><br><span class="line">           </span><br><span class="line"><span class="comment"># 查看 har 文件， 把打包前的原本文件都显示出来了</span></span><br><span class="line">[ap@cs1]~% hadoop fs -ls -R har:///test.har</span><br><span class="line">-rw-r--r--   3 ap supergroup         50 2018-06-03 04:24 har:///test.har/a.txt</span><br><span class="line">-rw-r--r--   3 ap supergroup         22 2018-06-03 04:24 har:///test.har/b.txt</span><br></pre></td></tr></table></figure><p><strong>注意点：</strong></p><ul><li><strong>存储层面：</strong>为了解决小文件过多导致的 Namenode 压力过大问题， 把很多小文件打包成一个 har 文件。<ul><li><strong>使用层面：</strong> 但是实际处理的时候， 还是会还原出原本的小文件进行处理， 不会把 har 文件当成一个 HDFS 文件处理。 </li><li><strong>HDFS</strong> 上不支持 tar， <strong>只支持 har打包</strong></li></ul></li></ul><h2 id="7-HDFS-的-HA"><a href="#7-HDFS-的-HA" class="headerlink" title="7.HDFS 的 HA"></a>7.HDFS 的 HA</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-HDFS体系结构&quot;&gt;&lt;a href=&quot;#1-HDFS体系结构&quot; class=&quot;headerlink&quot; title=&quot;1.  HDFS体系结构&quot;&gt;&lt;/a&gt;1.  HDFS体系结构&lt;/h2&gt;&lt;p&gt;主从。。。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&quot;2-Na
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="HDFS" scheme="https://airpoet.github.io/categories/Hadoop/HDFS/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="HDFS" scheme="https://airpoet.github.io/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>▍回答</title>
    <link href="https://airpoet.github.io/2018/06/03/Poetry/%E5%9B%9E%E7%AD%94/"/>
    <id>https://airpoet.github.io/2018/06/03/Poetry/回答/</id>
    <published>2018-06-03T06:40:41.275Z</published>
    <updated>2018-06-09T17:12:20.524Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-03-511528043539_.pic.jpg" alt=""></p><p><br></p><p>卑鄙是卑鄙者的通行证，<br>高尚是高尚者的墓志铭，<br>看吧，在那镀金的天空中，<br>飘满了死者弯曲的倒影。</p><p><br></p><p>冰川纪过去了，<br>为什么到处都是冰凌？<br>好望角发现了，<br>为什么死海里千帆相竞？</p><p><br></p><p>我来到这个世界上，<br>只带着纸、绳索和身影，<br>为了在审判之前，<br>宣读那些被判决的声音。</p><p><br></p><p>告诉你吧，世界<br>我–不–相–信！<br>纵使你脚下有一千名挑战者，<br>那就把我算作第一千零一名。</p><p><br></p><p>我不相信天是蓝的，<br>我不相信雷的回声，<br>我不相信梦是假的，<br>我不相信死无报应。</p><p><br></p><p>如果海洋注定要决堤，<br>就让所有的苦水都注入我心中，<br>如果陆地注定要上升，<br>就让人类重新选择生存的峰顶。</p><p><br></p><p>新的转机和闪闪星斗，<br>正在缀满没有遮拦的天空。<br>那是五千年的象形文字，<br>那是未来人们凝视的眼睛。</p><p><br></p><p>作者 / 北岛</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-03-511528043539_.pic.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;
      
    
    </summary>
    
      <category term="文艺" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/"/>
    
      <category term="诗歌" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/%E8%AF%97%E6%AD%8C/"/>
    
    
      <category term="诗歌" scheme="https://airpoet.github.io/tags/%E8%AF%97%E6%AD%8C/"/>
    
  </entry>
  
</feed>
