<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>A.P的文艺杂谈</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://airpoet.github.io/"/>
  <updated>2018-07-03T04:04:08.352Z</updated>
  <id>https://airpoet.github.io/</id>
  
  <author>
    <name>airpoet</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kafka</title>
    <link href="https://airpoet.github.io/2018/07/01/Hadoop/7-Flume&amp;Kafka/Kafka/"/>
    <id>https://airpoet.github.io/2018/07/01/Hadoop/7-Flume&amp;Kafka/Kafka/</id>
    <published>2018-07-01T13:55:50.563Z</published>
    <updated>2018-07-03T04:04:08.352Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-概览"><a href="#1-概览" class="headerlink" title="1.概览"></a>1.概览</h2><p><strong>分布式流处理平台。</strong> </p><p>分布式, 副本, 容错的分布式存储流.</p><p>在系统之间构建实时数据流管道。 </p><p>以topic分类对记录进行存储 </p><p>每个记录包含key-value+timestamp </p><p>每秒钟百万消息吞吐量。 </p><p><strong>订阅模式 &amp; 组模式: 每组只能有一个消费者, 每个消费者可以单独为一组</strong></p><p><strong>关键词:</strong></p><ul><li>producer             //消息生产者</li><li>consumer              //消息消费者</li><li>consumer group        //消费者组</li><li>kafka server          //broker,kafka服务器</li><li>topic                     //主题,副本数,分区.</li><li>zookeeper            //hadoop namenoade + RM HA | hbase | kafka</li></ul><p>优化方案: </p><ul><li>把磁盘挂载在某个目录, 然后让这个目录, 只是存储某个分区的数据</li></ul><p><strong>Kafka 之所以存储快的原因</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-03-032535.png" alt="image-20180703112535633"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-03-032546.png" alt="image-20180703112545937"></p><p><strong>顺序读写</strong></p><ul><li>追加数据, 是追加到最后</li><li>读取是从开头读取</li><li>可用 offset 跳转到指定的数据段</li><li>数据可重复消费</li></ul><p><strong>Consumers</strong></p><ul><li>一条数据只能被同组内一个成员消费</li><li>不同组的可以消费同一个数据</li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-03-032635.png" alt="image-20180703112634857"></p><p><strong>SSD读写速度: 1200 ~ 3500 MB/s</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-03-032302.png" alt="image-20180703112302238"></p><p><strong>注意点:</strong> </p><ul><li>Kafka 的分区数, 可以动态调整</li><li>Kafka 的每个分区都有对应的冗余数量, 默认是1</li><li>每个分区如果有多个副本, 这些副本中, 会有一个是 active 的状态, 负责读写, 其它的都是 standby 的状态</li><li>对于HDFS和kafka的  block  partition 对于不同的文件或者topic来说，都可以有不同的副本数！(每个 topic 可以单独设置)<br>设置的副本数不能超过broker的数量！！！</li></ul><hr><h4 id="Kafka-的大致工作模式："><a href="#Kafka-的大致工作模式：" class="headerlink" title="Kafka 的大致工作模式："></a><strong>Kafka 的大致工作模式：</strong></h4><p>1、启动 ZooKeeper 的 server</p><p>2、启动 Kafka 的 server</p><p>3、Producer 生产数据，然后通过 ZooKeeper 找到 Broker，再将数据 push 到 Broker 保存</p><p>4、Consumer 通过 ZooKeeper 找到 Broker，然后再主动 pull 数据</p><p><strong>Kafka的核心概念</strong></p><ul><li><strong><em>Broker</em></strong>：Kafka 节点，一个 Kafka 节点就是一个 broker，多个 broker 可以组成一个 Kafka 集群。</li><li><strong><em>Topic</em></strong>：一类消息，消息存放的目录即主题，例如 page view 日志、click 日志等都可以以 topic 的形式存在，Kafka 集群能够同时负责多个 topic 的分发。</li><li><strong><em>Partition</em></strong>：topic 物理上的分组，一个 topic 可以分为多个 partition，每个 partition 是一个有 序的队列</li><li><strong>Segment</strong>：partition 物理上由多个 segment 组成，每个 Segment 存着 message 信息</li><li><strong><em>Producer</em></strong> : 生产 message 发送到 topic</li><li><strong><em>Consumer</em></strong> : 订阅 topic 消费 message，consumer 作为一个线程来消费</li><li><strong>Consumer Group</strong>：一个 Consumer Group 包含多个 consumer，这个是预先在配置文件中配置 好的</li></ul><h2 id="2-安装-kafka"><a href="#2-安装-kafka" class="headerlink" title="2.安装 kafka"></a>2.安装 kafka</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">0.选择cs1 ~ cs6,  6台主机都安装kafka</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line">1.准备zk</span><br><span class="line">略</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line">2.jdk</span><br><span class="line">略</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line">3.tar文件</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line">4.环境变量</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line">略</span><br><span class="line">5.配置kafka</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line">    [kafka/config/server.properties]</span><br><span class="line">    ...</span><br><span class="line">    broker.id=1</span><br><span class="line">    ...</span><br><span class="line">    listeners=PLAINTEXT://:9092</span><br><span class="line">    ...</span><br><span class="line">    log.dirs=/home/centos/kafka/logs</span><br><span class="line">    ...</span><br><span class="line">    zookeeper.connect=s201:2181,s202:2181,s203:2181</span><br><span class="line"></span><br><span class="line">6.分发整个文件 &amp; 符号链接,  .zshrc配置文件,     同时修改每个server.properties 文件的broker.id参数 </span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">7.启动kafka服务器</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line">    a)先启动zk</span><br><span class="line">    b)启动kafka       </span><br><span class="line">    b1)前台启动</span><br><span class="line">    [cs2 ~ cs4]</span><br><span class="line">    [ap@cs1]~% kafka-server-start.sh /home/ap/apps/kafka/config/server.properties</span><br><span class="line"></span><br><span class="line">    b2)后台启动</span><br><span class="line">    $&gt; kafka-server-start.sh  -daemon  /home/ap/apps/kafka/config/server.properties     </span><br><span class="line"></span><br><span class="line">    c)验证kafka服务器是否启动</span><br><span class="line">    $&gt;netstat -anop | grep 9092</span><br><span class="line"></span><br><span class="line">8.创建主题 </span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line">    [ap@cs2]~% kafka-topics.sh --create  --zookeeper cs1:2181 --replication-factor 3 --partitions 3 --topic <span class="built_in">test</span></span><br><span class="line"></span><br><span class="line">9.查看主题列表</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line">[ap@cs2]~% kafka-topics.sh --list --zookeeper cs1:2181</span><br><span class="line"></span><br><span class="line">10.启动控制台生产者  (前提是 操作本机 &amp; cs2 上已经启动kafka服务)</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line">    注意 : producer 不直接连接 zk, 而是连接 broker, 也就是 kafka server. </span><br><span class="line">    其它的都是连 zk</span><br><span class="line">    如果producer 连的某一台挂掉的话, 可以在 --broker-list 后面, 加上多台服务器,用逗号隔开</span><br><span class="line">    [ap@cs2]~% kafka-console-producer.sh --broker-list cs2:9092 --topic <span class="built_in">test</span></span><br><span class="line"></span><br><span class="line">11.启动控制台消费者 (前提是 操作本机 &amp; cs2 上已经启动卡夫卡服务)</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line">    <span class="comment"># 使用bootstrap-server参数 替代zookeeper</span></span><br><span class="line">    Using the ConsoleConsumer with old consumer is deprecated and will be removed <span class="keyword">in</span> a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].</span><br><span class="line">    [ap@cs3]~% kafka-console-consumer.sh --bootstrap-server cs2:9092 --topic <span class="built_in">test</span> --from-beginning</span><br><span class="line"></span><br><span class="line">12.在生产者控制台输入hello world</span><br><span class="line">---------------------------------------------------------------------</span><br></pre></td></tr></table></figure><h2 id="3-kafka集群在zk的配置"><a href="#3-kafka集群在zk的配置" class="headerlink" title="3.kafka集群在zk的配置"></a>3.kafka集群在zk的配置</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">/controller            ===&gt;    &#123;<span class="string">"version"</span>:1,<span class="string">"brokerid"</span>:202,<span class="string">"timestamp"</span>:<span class="string">"1490926369148"</span></span><br><span class="line"></span><br><span class="line">/controller_epoch    ===&gt;    1</span><br><span class="line"></span><br><span class="line">/brokers</span><br><span class="line">/brokers/ids</span><br><span class="line">/brokers/ids/202    ===&gt;    &#123;<span class="string">"jmx_port"</span>:-1,<span class="string">"timestamp"</span>:<span class="string">"1490926370304"</span>,<span class="string">"endpoints"</span>:[<span class="string">"PLAINTEXT://s202:9092"</span>],<span class="string">"host"</span>:<span class="string">"s202"</span>,<span class="string">"version"</span>:3,<span class="string">"port"</span>:9092&#125;</span><br><span class="line">/brokers/ids/203</span><br><span class="line">/brokers/ids/204    </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/brokers/topics/<span class="built_in">test</span>/partitions/0/state ===&gt;&#123;<span class="string">"controller_epoch"</span>:1,<span class="string">"leader"</span>:203,<span class="string">"version"</span>:1,<span class="string">"leader_epoch"</span>:0,<span class="string">"isr"</span>:[203,204,202]&#125;</span><br><span class="line">/brokers/topics/<span class="built_in">test</span>/partitions/1/state ===&gt;...</span><br><span class="line">/brokers/topics/<span class="built_in">test</span>/partitions/2/state ===&gt;...</span><br><span class="line"></span><br><span class="line">/brokers/seqid        ===&gt; null</span><br><span class="line"></span><br><span class="line">/admin</span><br><span class="line">/admin/delete_topics/<span class="built_in">test</span>        ===&gt;标记删除的主题, 但是实际删除后, 这里没出现</span><br><span class="line"></span><br><span class="line">/isr_change_notification</span><br><span class="line"></span><br><span class="line">/consumers/xxxx/</span><br><span class="line">/config</span><br></pre></td></tr></table></figure><h2 id="4-kafka-主题-amp-副本操作"><a href="#4-kafka-主题-amp-副本操作" class="headerlink" title="4.kafka 主题&amp;副本操作"></a>4.kafka 主题&amp;副本操作</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">---------------------------------------------------------------------</span><br><span class="line">PS: 查看帮助套路</span><br><span class="line">* 如果看 topic 相关帮助</span><br><span class="line">    * bin/kafka-topic.sh  脚本直接回车, 查看帮助</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">删除主题</span><br><span class="line">------------------------------------------------------------</span><br><span class="line">[ap@cs2]~% kafka-topics.sh --zookeeper cs2:2181  --delete --topic <span class="built_in">test</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">创建主题</span><br><span class="line">---------------------------------------------------------</span><br><span class="line">    repliation_factor 2 partitions 5</span><br><span class="line"></span><br><span class="line">    $&gt;kafka-topics.sh --zookeeper cs2:2181 --replication-factor 2 --partitions 3 --create --topic test2</span><br><span class="line"></span><br><span class="line">    2 x 3  = 6        //个文件夹</span><br><span class="line"></span><br><span class="line">    2份副本, 每份副本有3个分区</span><br><span class="line">    总分区数为 6</span><br><span class="line">    每个分区都有一个 leader,  follower 就是其它的副本</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">重新布局分区和副本，手动再平衡  :TODO 还未测试 </span><br><span class="line">------------------------------------------------------------</span><br><span class="line">alter 似乎是有问题的, create好像没问题</span><br><span class="line">此句是手动把分区放到 203 &amp; 204上了</span><br><span class="line">    $&gt;kafka-topics.sh --alter --zookeeper s202:2181 --topic test2 --replica-assignment 203:204,203:204,203:204,203:204,203:204</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">测试 producer 发消息后, 存在本地的logs 目录的文件</span><br><span class="line">--------------------------------------------------------------</span><br><span class="line">1. 开启一个producer, 发消息</span><br><span class="line">2. 查看本地 <span class="built_in">cd</span>  ~/kafka/logs</span><br><span class="line">3. 查看文件大小, 找出文件(一次性查看)</span><br><span class="line">    1. [ap@cs1]~% xcall.sh <span class="string">"ls -lh  kafka/logs/test2-*"</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">副本</span><br><span class="line">--------------------------------------------------------------</span><br><span class="line">broker存放消息以消息达到顺序存放。生产和消费都是副本感知的。</span><br><span class="line">支持到n-1故障。每个分区都有leader，follow.</span><br><span class="line">leader挂掉时，消息分区写入到本地<span class="built_in">log</span>或者，向生产者发送消息确认回执之前，生产者向新的leader发送消息。</span><br><span class="line"></span><br><span class="line">新leader的选举是通过isr进行，第一个注册的follower成为leader。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">kafka支持副本模式</span><br><span class="line">---------------------</span><br><span class="line">[同步复制]</span><br><span class="line">    1.producer联系zk识别leader</span><br><span class="line">    2.向leader发送消息</span><br><span class="line">    3.leadr收到消息写入到本地<span class="built_in">log</span></span><br><span class="line">    4.follower从leader pull消息</span><br><span class="line">    5.follower向本地写入<span class="built_in">log</span></span><br><span class="line">    6.follower向leader发送ack消息</span><br><span class="line">    7.leader收到所有follower的ack消息</span><br><span class="line">    8.leader向producer回传ack</span><br><span class="line">                    </span><br><span class="line">    </span><br><span class="line">[异步副本]</span><br><span class="line">--------------------------------------------------------------</span><br><span class="line">    和同步复制的区别在于 leader写入本地<span class="built_in">log</span>之后，</span><br><span class="line">    直接向client回传ack消息，不需要等待所有follower复制完成。</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">kafka 副本模式 &amp; leader 机制 | 官方文档</span><br><span class="line">----------------------------------------------------</span><br><span class="line">ps : 推选leader过程就是follower在zk中注册过程，第一个注册就是leader</span><br><span class="line"></span><br><span class="line">The process of choosing the new lead replica is that all followers<span class="string">' In-sync Replicas (ISRs) register themselves with ZooKeeper. The very first registered replica becomes the new lead replica, and the rest of the registered replicas become the followers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Kafka supports the following replication modes:</span></span><br><span class="line"><span class="string">• Synchronous replication: In synchronous replication, a producer first identifies the lead replica from ZooKeeper and publishes the message. As soon as the message is published, it is written to the log of the lead replica and all the followers of the lead start pulling the message, and by using a single channel, the order of messages is ensured. Each follower replica sends an acknowledgement to the lead replica once the message is written to its respective logs. Once replications are complete and all expected acknowledgements are received, the lead replica sends an acknowledgement to the producer.</span></span><br><span class="line"><span class="string">On the consumer side, all the pulling of messages is done from the lead replica.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">• Asynchronous replication: The only difference in this mode is that as soon as a lead replica writes the message to its local log, it sends the acknowledgement to the message client and does not wait for the acknowledgements from follower replicas. But as a down side, this mode does not ensure the message delivery in case of broker failure.</span></span><br></pre></td></tr></table></figure><h2 id="5-Kafka-java-API"><a href="#5-Kafka-java-API" class="headerlink" title="5.Kafka java API"></a>5.Kafka java API</h2><h4 id="5-0-pom-文件"><a href="#5-0-pom-文件" class="headerlink" title="5.0   pom 文件"></a>5.0   pom 文件</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.rox<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>Kafka_Demo<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.11<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="5-1实现消息生产者，发送-amp-接受-消息"><a href="#5-1实现消息生产者，发送-amp-接受-消息" class="headerlink" title="5.1实现消息生产者，发送&amp;接受 消息"></a>5.1实现消息生产者，发送&amp;接受 消息</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.kafkademo.test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> kafka.consumer.Consumer;</span><br><span class="line"><span class="keyword">import</span> kafka.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> kafka.consumer.ConsumerIterator;</span><br><span class="line"><span class="keyword">import</span> kafka.consumer.KafkaStream;</span><br><span class="line"><span class="keyword">import</span> kafka.producer.KeyedMessage$;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> kafka.javaapi.producer.Producer;</span><br><span class="line"><span class="keyword">import</span> kafka.producer.KeyedMessage;</span><br><span class="line"><span class="keyword">import</span> kafka.producer.ProducerConfig;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestProducer</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="comment">// 发送消息 producer</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testSend</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">// broker列表</span></span><br><span class="line">        props.put(<span class="string">"metadata.broker.list"</span>,<span class="string">"cs2:9092"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 串行化</span></span><br><span class="line">        props.put(<span class="string">"serializer.class"</span>, <span class="string">"kafka.serializer.StringEncoder"</span>);</span><br><span class="line">        props.put(<span class="string">"request.required.acks"</span>, <span class="string">"1"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建生产者配置对象</span></span><br><span class="line">        ProducerConfig config = <span class="keyword">new</span> ProducerConfig(props);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建生产者</span></span><br><span class="line">        Producer&lt;String, String&gt; producer = <span class="keyword">new</span> Producer&lt;String, String&gt;(config);</span><br><span class="line"></span><br><span class="line">        KeyedMessage&lt;String, String&gt; msg = <span class="keyword">new</span> KeyedMessage&lt;String, String&gt;(<span class="string">"test2"</span>, <span class="string">"100"</span>, <span class="string">"hello word fuck you kafka"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 发送</span></span><br><span class="line">        producer.send(msg);</span><br><span class="line">        System.out.println(<span class="string">"发送完成"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 接收消息 consumer</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testConsumer</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"zookeeper.connect"</span>, <span class="string">"cs2:2181"</span>);</span><br><span class="line">        props.put(<span class="string">"group.id"</span>, <span class="string">"g2"</span>);</span><br><span class="line">        props.put(<span class="string">"zookeeper.session.timeout.ms"</span>, <span class="string">"500"</span>);</span><br><span class="line">        props.put(<span class="string">"auto.commit.interval.ms"</span>, <span class="string">"1000"</span>);</span><br><span class="line">        props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"smallest"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建消费者配置对象</span></span><br><span class="line">        ConsumerConfig config = <span class="keyword">new</span> ConsumerConfig(props);</span><br><span class="line"></span><br><span class="line">        Map&lt;String, Integer&gt; map = <span class="keyword">new</span> HashMap&lt;String, Integer&gt;();</span><br><span class="line">        map.put(<span class="string">"test2"</span>, <span class="keyword">new</span> Integer(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">        Map&lt;String, List&lt;KafkaStream&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt;&gt;&gt; msgs = Consumer.createJavaConsumerConnector(<span class="keyword">new</span> ConsumerConfig(props)).createMessageStreams(map);</span><br><span class="line">        List&lt;KafkaStream&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt;&gt; msgList = msgs.get(<span class="string">"test2"</span>);</span><br><span class="line">        <span class="keyword">for</span>(KafkaStream&lt;<span class="keyword">byte</span>[],<span class="keyword">byte</span>[]&gt; stream : msgList)&#123;</span><br><span class="line">            ConsumerIterator&lt;<span class="keyword">byte</span>[],<span class="keyword">byte</span>[]&gt; it = stream.iterator();</span><br><span class="line">            <span class="keyword">while</span>(it.hasNext())&#123;</span><br><span class="line">                <span class="keyword">byte</span>[] message = it.next().message();</span><br><span class="line">                System.out.println(<span class="keyword">new</span> String(message));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="6-Flume-集成-kafak"><a href="#6-Flume-集成-kafak" class="headerlink" title="6.Flume 集成 kafak"></a>6.Flume 集成 kafak</h2><h4 id="6-1KafkaSink"><a href="#6-1KafkaSink" class="headerlink" title="6.1KafkaSink"></a>6.1KafkaSink</h4><p><strong>kafka 是 consumer (消费者)</strong></p><p><strong>flume 的消息, 经由 kafkaSink 导出到 kafka –最常用</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">[flume 为生产者]</span><br><span class="line">1.1) flume 参数配置 </span><br><span class="line">----------------------------------------------------</span><br><span class="line">    a1.sources = r1</span><br><span class="line">    a1.sinks = k1</span><br><span class="line">    a1.channels = c1</span><br><span class="line"></span><br><span class="line">    a1.sources.r1.type=netcat</span><br><span class="line">    a1.sources.r1.bind=localhost</span><br><span class="line">    a1.sources.r1.port=8888</span><br><span class="line"></span><br><span class="line">    a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">    a1.sinks.k1.kafka.topic = test3</span><br><span class="line">    a1.sinks.k1.kafka.bootstrap.servers = cs2:9092</span><br><span class="line">    a1.sinks.k1.kafka.flumeBatchSize = 20</span><br><span class="line">    a1.sinks.k1.kafka.producer.acks = 1</span><br><span class="line"></span><br><span class="line">    a1.channels.c1.type=memory</span><br><span class="line"></span><br><span class="line">    a1.sources.r1.channels = c1</span><br><span class="line">    a1.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line">1.2) 开启 kafka 消费者</span><br><span class="line">----------------------------------------------------</span><br><span class="line">    kafka-console-consumer.sh --bootstrap-server cs2:9092 --topic test3 --from-beginning</span><br><span class="line">    注意 :  Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper]. </span><br><span class="line">    使用 [bootstrap-server]代替[zookeeper]</span><br><span class="line"></span><br><span class="line">1.3) flume 执行 kafka_sink.conf </span><br><span class="line">----------------------------------------------------</span><br><span class="line">flume-ng agent -f apps/flume/conf/confs/kafka_sink.conf -n a1</span><br><span class="line"></span><br><span class="line">1.4) 验证端口</span><br><span class="line">----------------------------------------------------</span><br><span class="line">    netstat -anop | grep 8888  </span><br><span class="line">    [OR]</span><br><span class="line">    lsof -i tcp:8888</span><br><span class="line"></span><br><span class="line">1.5) nc 连接</span><br><span class="line">----------------------------------------------------</span><br><span class="line">    nc localhost 8888</span><br><span class="line">    发消息....</span><br><span class="line"></span><br><span class="line">1.6) 观察 kafka 消费者接受消息</span><br><span class="line">----------------------------------------------------</span><br><span class="line">发现特别快!</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">========================================================</span><br><span class="line">简单案例2</span><br><span class="line">--------------</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line">a1.sources.r1.type=<span class="built_in">exec</span></span><br><span class="line"><span class="comment">#-F 最后10行,如果从头开始收集 -c +0 -F:持续收集后续数据,否则进程停止。</span></span><br><span class="line">a1.sources.r1.command=tail -F -c +0  /home/ap/calllog/calllog.log</span><br><span class="line"></span><br><span class="line">a1.channels.c1.type=memory</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">a1.sinks.k1.kafka.topic = calllog</span><br><span class="line">a1.sinks.k1.kafka.bootstrap.servers = cs2:9092 cs3:9092 cs4:9092</span><br><span class="line"></span><br><span class="line"><span class="comment"># How many messages to process in one batch. Larger batches improve throughput while adding latency.</span></span><br><span class="line">a1.sinks.k1.kafka.flumeBatchSize = 20</span><br><span class="line"></span><br><span class="line"><span class="comment"># How many replicas must acknowledge a message before its considered successfully written. Accepted values are 0 (Never wait for acknowledgement), 1 (wait for leader only), -1 (wait for all replicas) Set this to -1 to avoid data loss in some cases of leader failure.</span></span><br><span class="line">a1.sinks.k1.kafka.producer.acks = 1</span><br><span class="line"></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><h4 id="6-2-KafkaSource-TODO"><a href="#6-2-KafkaSource-TODO" class="headerlink" title="6.2 KafkaSource :TODO"></a>6.2 KafkaSource :TODO</h4><p><strong>flume source 从 kafka 抓数据, flume source 是消费者,  kafka 开启 producer.</strong> </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[消费者]</span><br><span class="line">1) 配置</span><br><span class="line">    a1.sources = r1</span><br><span class="line">    a1.sinks = k1</span><br><span class="line">    a1.channels = c1</span><br><span class="line"></span><br><span class="line">    a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource</span><br><span class="line">    a1.sources.r1.batchSize = 5000</span><br><span class="line">    a1.sources.r1.batchDurationMillis = 2000</span><br><span class="line">    a1.sources.r1.kafka.bootstrap.servers =  cs2:9092</span><br><span class="line">    a1.sources.r1.kafka.topics = test3</span><br><span class="line">    a1.sources.r1.kafka.consumer.group.id = g4</span><br><span class="line"></span><br><span class="line">    a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line">    a1.channels.c1.type=memory</span><br><span class="line"></span><br><span class="line">    a1.sources.r1.channels = c1</span><br><span class="line">    a1.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line">2) 启动 flume </span><br><span class="line">    flume-ng agent -f /home/ap/apps/flume/conf/confs/kafka_source.conf -n a1 -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">3) 启动 kafka producer</span><br><span class="line">kafka-console-producer.sh --broker-list cs2:9092 --topic test3 </span><br><span class="line"></span><br><span class="line">4) kafka 发消息:  收不到!!!!  :TODO</span><br></pre></td></tr></table></figure><h4 id="6-3-KafkaChannel-TODO"><a href="#6-3-KafkaChannel-TODO" class="headerlink" title="6.3 KafkaChannel  :TODO"></a>6.3 KafkaChannel  :TODO</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">:TODO  也出错 ?? </span><br><span class="line">    越界</span><br><span class="line">            生产者 + 消费者</span><br><span class="line">    a1.sources = r1</span><br><span class="line">    a1.sinks = k1</span><br><span class="line">    a1.channels = c1</span><br><span class="line"></span><br><span class="line">    a1.sources.r1.type = avro</span><br><span class="line">    a1.sources.r1.bind = localhost</span><br><span class="line">    a1.sources.r1.port = 8888</span><br><span class="line"></span><br><span class="line">    a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line">    a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel</span><br><span class="line">    a1.channels.c1.kafka.bootstrap.servers = cs2:9092</span><br><span class="line">    a1.channels.c1.kafka.topic = test3</span><br><span class="line">    a1.channels.c1.kafka.consumer.group.id = g6</span><br><span class="line">    a1.channels.c1.kafka.parseAsFlumeEvent = <span class="literal">false</span></span><br><span class="line">    a1.channels.c1.zookeeperConnect= cs2:2181</span><br><span class="line"></span><br><span class="line">    a1.sources.r1.channels = c1</span><br><span class="line">    a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-概览&quot;&gt;&lt;a href=&quot;#1-概览&quot; class=&quot;headerlink&quot; title=&quot;1.概览&quot;&gt;&lt;/a&gt;1.概览&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;分布式流处理平台。&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;分布式, 副本, 容错的分布式存储流.&lt;/p&gt;
&lt;p&gt;
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="Kafka" scheme="https://airpoet.github.io/categories/Hadoop/Kafka/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="Kafka" scheme="https://airpoet.github.io/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Flume</title>
    <link href="https://airpoet.github.io/2018/06/30/Hadoop/7-Flume&amp;Kafka/Flume/"/>
    <id>https://airpoet.github.io/2018/06/30/Hadoop/7-Flume&amp;Kafka/Flume/</id>
    <published>2018-06-30T13:32:24.747Z</published>
    <updated>2018-07-02T13:44:40.969Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Flume-概念"><a href="#1-Flume-概念" class="headerlink" title="1.Flume 概念"></a>1.Flume 概念</h2><p>提供 <strong>收集、移动、聚合大量日志数据</strong>的服务。 </p><p>基于流数据的架构，用于在线日志分析。 </p><p>基于事件。 </p><p>在生产和消费者之间启动协调作用。 </p><p>提供了事务保证，确保消息一定被分发。 </p><p>多种 <code>Source</code>, <code>Channel</code>, <code>Sink</code>配置方式。 </p><p><strong>与 Sqoop 区别</strong></p><ul><li>Sqoop 用来采集关系型数据库数据</li><li>Flume 用 来采集流动型数据。 </li></ul><p><strong>关键词解释</strong></p><ul><li><p><strong>Client</strong> </p><ul><li>Client 是一个<strong>将原始 log 包装成 events</strong> 并且发送他们到一个或多个 agent 的实体, 交给 source 处理</li></ul></li><li><p><strong>Event</strong></p><ul><li>Event 由可选的 header 和载有数据的一个 byte array 构成。 </li></ul></li><li><p><strong>Source</strong></p><ul><li>接受数据，类型有多种。</li></ul></li><li><strong>Channel</strong><ul><li>临时存放地，对Source中来的数据进行缓冲，直到sink消费掉。</li></ul></li><li><strong>Sink</strong><ul><li>从channel提取数据存放到中央化存储(hadoop / hbase)。</li></ul></li><li><strong>Iterator</strong><ul><li>作用于 Source，按照预设的顺序在必要地方装饰和过滤 </li></ul></li><li><strong>events Channel Selector</strong><ul><li>允许 Source 基于预设的标准，从所有 channel 中，选择一个或者多个 channel</li></ul></li></ul><p><strong>A simple Flume agent with one source, channel, and sink</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-30-143349.png" alt="image-20180630223348497"></p><p><strong>Source ==&gt;Channel </strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-30-142908.png" alt="image-20180630222908165"></p><p><strong>Channel</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-30-142944.png" alt="image-20180630222943933"></p><hr><p>Flume 的数据流由事件(Event)贯穿始终。<strong>事件是 Flume 的基本数据单位</strong>，它携带日志数据(字 节数组形式)并且携带有头信息，这些 Event 由 Agent 外部的 Source 生成，当 Source 捕获事 件后会进行特定的格式化，然后 Source 会把事件推入(单个或多个)Channel 中。你可以把 Channel 看作是一个缓冲区，它将保存事件直到 Sink 处理完该事件。Sink 负责持久化日志或 者把事件推向另一个 Source。</p><p>Flume 以 agent 为最小的独立运行单位。<strong>一个 agent 就是一个 JVM</strong>。单 agent 由 Source、Sink 和 Channel 三大组件构成，如下图:</p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-02-020612.png" alt="image-20180702100612437"></p><p><a href="https://app.yinxiang.com/shard/s37/nl/7399077/e762aaa0-8e96-4539-ad5f-3c260ec697b2/" target="_blank" rel="noopener">参考文档</a></p><h2 id="2-安装-Flume"><a href="#2-安装-Flume" class="headerlink" title="2.安装 Flume"></a>2.安装 Flume</h2><h4 id="1-下载"><a href="#1-下载" class="headerlink" title="1.下载"></a>1.下载</h4><h4 id="2-tar"><a href="#2-tar" class="headerlink" title="2.tar"></a>2.tar</h4><h4 id="3-环境变量"><a href="#3-环境变量" class="headerlink" title="3.环境变量"></a>3.环境变量</h4><p><strong>注意</strong>: 配置的话, 使用脚本就方便些,  但是如果配置文件在 flume 文件夹中, 路径就比较长</p><p>​     不配置的话, 要进入到flume 路径下使用, 但是此时配置文件的路径就比较短了</p><p><strong>PS</strong>: 如果<code>.sh</code>文件没有执行权限, 即<code>x</code>权限,  要用相对路径来启动, 即如果在 bin 目录, 要用<code>./flume-ng</code>, 在 bin 外,要用 <code>bin/flume</code>执行, 有<code>x</code>权限的话,  就可以直接在 bin 下, 使用  <code>flume-ng</code>, 如果配置了环境变量, 可以在任何路径下使用  <code>flume-ng</code>了.</p><ul><li><p>Open <strong><code>flume-env.sh</code></strong> file and <strong>set the <code>JAVA_Home</code></strong> to the folder where Java was installed in your system.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/jdk1.8.0_73</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>In the <strong><code>.zshrc</code></strong> file, set <strong>FLUME_HOME</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#flume</span></span><br><span class="line"><span class="built_in">export</span> FLUME_HOME=/home/ap/apps/flume</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$FLUME_HOME</span>/bin</span><br><span class="line">---</span><br><span class="line"><span class="built_in">source</span> .zshrc</span><br></pre></td></tr></table></figure></li></ul><h4 id="4-验证"><a href="#4-验证" class="headerlink" title="4.验证"></a>4.验证</h4><pre><code>` $&gt; flume-ng version`            //next generation.下一代.</code></pre><hr><h2 id="3-配置flume准备工作"><a href="#3-配置flume准备工作" class="headerlink" title="3.配置flume准备工作"></a>3.配置flume准备工作</h2><h4 id="3-0如果-yum-没有设置网络源的-设置一下-阿里-网易"><a href="#3-0如果-yum-没有设置网络源的-设置一下-阿里-网易" class="headerlink" title="3.0如果 yum 没有设置网络源的, 设置一下 阿里 | 网易"></a>3.0<strong>如果 yum 没有设置网络源的, 设置一下 阿里 | 网易</strong></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先备份原来的 CentOS-Base.repo 为 CentOS-Base.repo.bak</span></span><br><span class="line">mv CentOS-Base.repo CentOS-Base.repo.bak</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载阿里基本源 </span></span><br><span class="line">$&gt;sudo wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载阿里epel源</span></span><br><span class="line">$&gt;sudo wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成缓存文件</span></span><br><span class="line">$&gt;sudo yum clean all</span><br><span class="line">$&gt;sudo yum makecache</span><br></pre></td></tr></table></figure><h5 id="3-1-所有机器安装-nc-或者-telnet"><a href="#3-1-所有机器安装-nc-或者-telnet" class="headerlink" title="3.1 所有机器安装 nc 或者 telnet"></a>3.1 所有机器安装 nc 或者 telnet</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$&gt;yum search nc</span><br><span class="line">$&gt; xcall.sh sudo yum install -y nc.x86_64</span><br><span class="line">$&gt; xcall.sh sudo yum install -y telnet</span><br></pre></td></tr></table></figure><h5 id="3-2-使用-nc"><a href="#3-2-使用-nc" class="headerlink" title="3.2 使用 nc"></a>3.2 使用 nc</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cs1_1&gt; nc -lk 8888  : 开启8888端口监听</span><br><span class="line">-------</span><br><span class="line">再开一个cs1 session</span><br><span class="line">cs1_2&gt; netstat -anop | grep 8888 :  查看是有有此端口</span><br><span class="line">或者  &gt;  lsof -i tcp:8888       : 可以直接看到 pid</span><br><span class="line">tcp        0      0 0.0.0.0:8888                0.0.0.0:*                   LISTEN      17251/nc            off (0.00/0/0)</span><br><span class="line">---------</span><br><span class="line">有的话就可以测试连接了 (客户端连接)</span><br><span class="line">cs1_2&gt; nc localhost 8888 </span><br><span class="line"></span><br><span class="line">==================</span><br><span class="line">telnet 使用方式类似, 可查看帮助</span><br></pre></td></tr></table></figure><h2 id="4-flume配置"><a href="#4-flume配置" class="headerlink" title="4. flume配置"></a>4. flume配置</h2><p><a href="http://flume.apache.org/FlumeUserGuide.html" target="_blank" rel="noopener"><strong>官方文档</strong></a></p><h4 id="总体概述"><a href="#总体概述" class="headerlink" title="总体概述"></a><strong>总体概述</strong></h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-02-021156.png" alt="image-20180702101155514"></p><h4 id="实时处理架构"><a href="#实时处理架构" class="headerlink" title="实时处理架构"></a>实时处理架构</h4><p><strong>flume:</strong> 监控日志文件</p><ul><li>每次产生一条新的数据, 会被实时的收集到下一跳</li></ul><p><strong>Kafka:</strong> 消息系统</p><ul><li>对消息进行简单处理, 当然, 具有简单缓冲作用</li></ul><p><strong>Storm / SparkStreaming</strong>:</p><ul><li>流式的分布式计算引擎</li></ul><p><strong>Redis:</strong> 缓存组件</p><ul><li>内存数据库</li></ul><p><strong>Hbase / HDFS /ES …</strong>: </p><ul><li>最后持久化到这些地方..</li></ul><h3 id="4-1-Flume-Source"><a href="#4-1-Flume-Source" class="headerlink" title="4.1  Flume Source"></a>4.1  Flume Source</h3><p><strong>注意:</strong> 启动 flume 配置的时候, 后面加上 <strong><code>-Dflume.root.logger=INFO,console</code></strong></p><p>还有一种<strong>更详细的</strong> <strong>: <code>-Dflume.root.logger=DEBUG,console</code></strong></p><p>会<strong>打印更详细的日志</strong></p><h4 id="4-1-1-netcat"><a href="#4-1-1-netcat" class="headerlink" title="4.1.1 netcat"></a>4.1.1 netcat</h4><p><strong>通过消息通信的方式, 进行采集</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">1.创建配置文件</span><br><span class="line">   [/soft/flume/conf/helloworld.conf]</span><br><span class="line">   <span class="comment">#声明三种组件</span></span><br><span class="line">   a1.sources = r1</span><br><span class="line">   a1.channels = c1</span><br><span class="line">   a1.sinks = k1</span><br><span class="line"></span><br><span class="line">   <span class="comment">#定义source信息</span></span><br><span class="line">   a1.sources.r1.type=netcat</span><br><span class="line">   a1.sources.r1.bind=localhost</span><br><span class="line">   a1.sources.r1.port=8888</span><br><span class="line"></span><br><span class="line">   <span class="comment">#定义sink信息</span></span><br><span class="line">   a1.sinks.k1.type=logger</span><br><span class="line"></span><br><span class="line">   <span class="comment">#定义channel信息</span></span><br><span class="line">   a1.channels.c1.type=memory</span><br><span class="line"></span><br><span class="line">   <span class="comment">#绑定在一起</span></span><br><span class="line">   ps: 注意:  一个<span class="built_in">source</span>(源) 可以输出到多个 channels(通道)</span><br><span class="line">                   一个sink(沉漕)只能从一个 channel(通道)中获取数据</span><br><span class="line"></span><br><span class="line">   a1.sources.r1.channels=c1</span><br><span class="line">   a1.sinks.k1.channel=c1</span><br><span class="line"></span><br><span class="line">   2.运行</span><br><span class="line">       a)启动flume agent</span><br><span class="line">           $&gt; bin/flume-ng agent -f  conf/helloworld.conf -n a1 -Dflume.root.logger=INFO,console</span><br><span class="line">       b)启动nc的客户端</span><br><span class="line">           $&gt;nc localhost 8888</span><br><span class="line">           <span class="variable">$nc</span>&gt;hello world</span><br><span class="line">       </span><br><span class="line">       c)在flume的终端输出hello world.</span><br></pre></td></tr></table></figure><h4 id="4-1-2-exec"><a href="#4-1-2-exec" class="headerlink" title="4.1.2  exec"></a>4.1.2  exec</h4><p><strong>实时日志收集,实时收集日志。</strong></p><p><strong>测试:</strong> </p><ol><li><p>先启动监控, 再创建文件, 能否监控到 </p><p>结论: 可以监控到</p></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">1. 配置</span><br><span class="line">    <span class="comment"># 声明3种组件</span></span><br><span class="line">    a1.sources=r1</span><br><span class="line">    a1.channels=c1</span><br><span class="line">    a1.sinks=k1</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义 source 信息</span></span><br><span class="line">    a1.sources.r1.type=<span class="built_in">exec</span></span><br><span class="line">    a1.sources.r1.command=tail -F /home/ap/test.txt   <span class="comment">#收集最后的10行</span></span><br><span class="line">    a1.sources.r1.command=tail -F -c +0 /home/ap/test.txt <span class="comment">#从第0行开始收集</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义 sink 信息</span></span><br><span class="line">    a1.sinks.k1.type=logger</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义 channel 信息</span></span><br><span class="line">    a1.channels.c1.type=memory</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绑定在一起</span></span><br><span class="line">    a1.sources.r1.channels=c1</span><br><span class="line">    a1.sinks.k1.channel=c1</span><br><span class="line"></span><br><span class="line">2&gt; 新建文件 test.txt</span><br><span class="line"></span><br><span class="line">3&gt; 启动</span><br><span class="line">[ap@cs1]~% flume-ng agent -f apps/flume/conf/confs/exec.conf -n a1 -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">4&gt; 修改 test.txt 文件</span><br><span class="line">$&gt;  <span class="built_in">echo</span> ddd &gt; test.txt   观察 agent 控制台变化</span><br></pre></td></tr></table></figure><h4 id="4-1-3-批量收集-spool"><a href="#4-1-3-批量收集-spool" class="headerlink" title="4.1.3 批量收集(spool)"></a>4.1.3 批量收集(spool)</h4><p><strong>监控一个文件夹，静态文件。</strong> </p><p><strong>文件最好是直接从外部移入</strong>, 文件夹内最好不要做文件编辑。 </p><p><strong>收集完之后，会重命名文件成新文件。.compeleted.</strong> </p><p><strong>之后就不会再次处理这个文件了.</strong></p><p><strong>只会监控新增加的文件, 不会监控删除的本地文件.</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">a) 配置</span><br><span class="line">    <span class="comment"># 声明3种组件</span></span><br><span class="line">    a1.sources=r1</span><br><span class="line">    a1.channels=c1</span><br><span class="line">    a1.sinks=k1</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义 source 信息</span></span><br><span class="line">    a1.sources.r1.type=spooldir</span><br><span class="line">    a1.sources.r1.spoolDir=/home/ap/spool</span><br><span class="line">    a1.sources.r1.fileHeader=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义 sink 信息</span></span><br><span class="line">    a1.sinks.k1.type=logger</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义 channel 信息</span></span><br><span class="line">    a1.channels.c1.type=memory</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绑定在一起</span></span><br><span class="line">    a1.sources.r1.channels=c1</span><br><span class="line">    a1.sinks.k1.channel=c1</span><br><span class="line"></span><br><span class="line">b)创建目录</span><br><span class="line">$&gt;mkdir ~/spool</span><br><span class="line"></span><br><span class="line">c)启动flume</span><br><span class="line">可以直接启动</span><br><span class="line">$&gt;flume-ng agent -f apps/flume/conf/confs/spool.conf -n a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><h4 id="4-1-4-序列sources-seq"><a href="#4-1-4-序列sources-seq" class="headerlink" title="4.1.4 序列sources (seq)"></a>4.1.4 序列sources (seq)</h4><p>一个简单的序列生成器, 它连续生成一个计数器, 它从0开始, 增量为 1, 并在 totalEvents 停止。</p><p><strong>主要用于测试。</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">1) 配置</span><br><span class="line"><span class="comment"># 声明3种组件</span></span><br><span class="line">a1.sources=r1</span><br><span class="line">a1.channels=c1</span><br><span class="line">a1.sinks=k1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 source 信息</span></span><br><span class="line">a1.sources.r1.type=seq</span><br><span class="line">a1.sources.r1.totalEvents=1000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 sink 信息</span></span><br><span class="line">a1.sinks.k1.type=logger</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 channel 信息</span></span><br><span class="line">a1.channels.c1.type=memory</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绑定在一起</span></span><br><span class="line">a1.sources.r1.channels=c1</span><br><span class="line">a1.sinks.k1.channel=c1</span><br><span class="line"></span><br><span class="line">2) 运行</span><br><span class="line">$&gt;bin/flume-ng agent -f conf/confs/helloworld.seq.conf -n a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><h4 id="4-1-5-压力-source-用于压力测试"><a href="#4-1-5-压力-source-用于压力测试" class="headerlink" title="4.1.5 压力 source (用于压力测试)"></a>4.1.5 压力 source (用于压力测试)</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = stresssource-1</span><br><span class="line">a1.channels = memoryChannel-1</span><br><span class="line">a1.sources.stresssource-1.type = org.apache.flume.source.StressSource</span><br><span class="line">a1.sources.stresssource-1.size = 10240</span><br><span class="line">a1.sources.stresssource-1.maxTotalEvents = 1000000</span><br><span class="line">a1.sources.stresssource-1.channels = memoryChannel-1</span><br></pre></td></tr></table></figure><h4 id="4-1-6-Multiplexing-Channel-Selector-TODO"><a href="#4-1-6-Multiplexing-Channel-Selector-TODO" class="headerlink" title="4.1.6 Multiplexing Channel Selector :TODO"></a>4.1.6 Multiplexing Channel Selector :TODO</h4><p><strong>flume 多路复用</strong></p><p><a href="http://flume.apache.org/FlumeUserGuide.html#multiplexing-channel-selector" target="_blank" rel="noopener">官网</a></p><hr><h3 id="4-2-Flume-Sink"><a href="#4-2-Flume-Sink" class="headerlink" title="4.2. Flume Sink"></a>4.2. Flume Sink</h3><p><strong>沉漕, source 经过 channel, 最后下沉到 sink, 再由 sink 输出</strong></p><p><strong>注意, flume 后面加上 <code>&amp;</code>是指后台运行</strong></p><p><a href="http://flume.apache.org/FlumeUserGuide.html#hdfs-sink" target="_blank" rel="noopener">官网</a></p><h4 id="4-2-1输出-sink-到-HDFS"><a href="#4-2-1输出-sink-到-HDFS" class="headerlink" title="4.2.1输出 (sink) 到 HDFS"></a>4.2.1输出 (sink) 到 HDFS</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">1) 配置</span><br><span class="line">===================================================================</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1</span><br><span class="line"></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 8888</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H/%M/%S</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = events-</span><br><span class="line"></span><br><span class="line"><span class="comment"># round目录 是否会产生新目录,每十分钟产生一个新目录,一般控制的目录方面。</span></span><br><span class="line"><span class="comment">#2017-12-12 --&gt;</span></span><br><span class="line"><span class="comment">#2017-12-12 --&gt;%H%M%S</span></span><br><span class="line">比如 写上 1 / day, 就是一天产生的数据文件都在 一个日期的目录下.</span><br><span class="line">---</span><br><span class="line">a1.sinks.k1.hdfs.round = <span class="literal">true</span>            </span><br><span class="line">a1.sinks.k1.hdfs.roundValue = 1</span><br><span class="line">a1.sinks.k1.hdfs.roundUnit = day</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用本地时间戳为时间序列头 </span></span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#是否产生新文件。 </span></span><br><span class="line">-------------</span><br><span class="line">只要3个条件中某一个满足, 就会滚动一个文件</span><br><span class="line"><span class="comment"># roll滚动, 此事件内的日志会写入到一个文件</span></span><br><span class="line"><span class="comment"># 目前的猜测: 10个字节就会触发, 如果10秒内 0 &lt; input &lt; 10字节,  也会滚动 =&gt; 是的</span></span><br><span class="line"><span class="comment"># 等待滚动当前的文件 10秒,Number of seconds to wait before rolling current file (0 = never roll based on time interval)</span></span><br><span class="line">a1.sinks.k1.hdfs.rollInterval=10      </span><br><span class="line"></span><br><span class="line"><span class="comment"># 这是指10个字节就触发滚动 File size to trigger roll, in bytes (0: never roll based on file size)                </span></span><br><span class="line">a1.sinks.k1.hdfs.rollSize=10           </span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of events written to file before it rolled (0 = never roll based on number of events) 一行就是一个事件  </span></span><br><span class="line">a1.sinks.k1.hdfs.rollCount=3          </span><br><span class="line"></span><br><span class="line">a1.channels.c1.type=memory</span><br><span class="line"></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line">-----------------</span><br><span class="line"><span class="comment"># sink其它可选配置参数(详情可查官网)</span></span><br><span class="line">agent1.sinks.sink1.type = hdfs</span><br><span class="line"><span class="comment">#a1.sinks.k1.channel = c1</span></span><br><span class="line">agent1.sinks.sink1.hdfs.path =hdfs://myha01/weblog/flume-event/%y-%m-%d/%H-%M agent1.sinks.sink1.hdfs.filePrefix = tomcat_</span><br><span class="line">agent1.sinks.sink1.hdfs.maxOpenFiles = 5000</span><br><span class="line">agent1.sinks.sink1.hdfs.batchSize= 100</span><br><span class="line">agent1.sinks.sink1.hdfs.fileType = DataStream</span><br><span class="line">agent1.sinks.sink1.hdfs.writeFormat =Text</span><br><span class="line">agent1.sinks.sink1.hdfs.rollSize = 102400</span><br><span class="line">agent1.sinks.sink1.hdfs.rollCount = 1000000</span><br><span class="line">agent1.sinks.sink1.hdfs.rollInterval = 60</span><br><span class="line">agent1.sinks.sink1.hdfs.round = <span class="literal">true</span></span><br><span class="line">agent1.sinks.sink1.hdfs.roundValue = 10</span><br><span class="line">agent1.sinks.sink1.hdfs.roundUnit = minute</span><br><span class="line">agent1.sinks.sink1.hdfs.useLocalTimeStamp = <span class="literal">true</span></span><br><span class="line">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span><br><span class="line">如果 HDFS 集群是高可用集群，那么必须要放入 core-site.xml 和 hdfs-site.xml 文件到 <span class="variable">$FLUME_HOME</span>/conf/confs  目录中, 就是跟配置文件同级目录.</span><br><span class="line">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span><br><span class="line"></span><br><span class="line">2) 开启agent</span><br><span class="line">===================================================================</span><br><span class="line">[ap@cs1]~/apps/flume/conf/confs% flume-ng agent -f hdfs.conf -n a1</span><br><span class="line"></span><br><span class="line">3) nc 连接</span><br><span class="line">===================================================================</span><br><span class="line">nc <span class="built_in">local</span> 8888</span><br><span class="line">连接上了之后, 发出去的消息就会写入 hdfs </span><br><span class="line"></span><br><span class="line">4) 写入时, 完整的 <span class="built_in">log</span> 是这样的</span><br><span class="line">===================================================================</span><br><span class="line">18/06/30 18:39:18 INFO hdfs.HDFSSequenceFile: writeFormat = Writable, UseRawLocalFileSystem = <span class="literal">false</span></span><br><span class="line">18/06/30 18:39:18 INFO hdfs.BucketWriter: Creating /user/ap/flume/events/18-06-30/18/39/00/events-.1530355158463.tmp</span><br><span class="line">18/06/30 18:39:28 INFO hdfs.BucketWriter: Closing /user/ap/flume/events/18-06-30/18/39/00/events-.1530355158463.tmp</span><br><span class="line">18/06/30 18:39:28 INFO hdfs.BucketWriter: Renaming /user/ap/flume/events/18-06-30/18/39/00/events-.1530355158463.tmp to /user/ap/flume/events/18-06-30/18/39/00/events-.1530355158463</span><br><span class="line">18/06/30 18:39:28 INFO hdfs.HDFSEventSink: Writer callback called.</span><br><span class="line"></span><br><span class="line">5) 查看 </span><br><span class="line">===================================================================</span><br><span class="line">注意: 序列文件用 text 查看</span><br></pre></td></tr></table></figure><h4 id="4-2-2-输出到-Hive"><a href="#4-2-2-输出到-Hive" class="headerlink" title="4.2.2 输出到 Hive"></a>4.2.2 输出到 Hive</h4><p><strong>写入太慢, 因为要转为 MR, 所以一般不会用</strong></p><h4 id="4-2-3-输出-sink-到-HBase"><a href="#4-2-3-输出-sink-到-HBase" class="headerlink" title="4.2.3 输出 (sink) 到 HBase"></a>4.2.3 输出 (sink) 到 HBase</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">3.1) 配置文件</span><br><span class="line">===================================================================</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1</span><br><span class="line"></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 8888</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type = hbase</span><br><span class="line">a1.sinks.k1.table = ns1:t12</span><br><span class="line">a1.sinks.k1.columnFamily = f1</span><br><span class="line">a1.sinks.k1.serializer = org.apache.flume.sink.hbase.RegexHbaseEventSerializer</span><br><span class="line"></span><br><span class="line">a1.channels.c1.type=memory</span><br><span class="line"></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line">3.2) 开启 nc 输入</span><br><span class="line">===================================================================</span><br><span class="line"></span><br><span class="line">3.3) 在 hbase shell 中 scan 表 <span class="string">'ns1:t12'</span></span><br><span class="line">===================================================================</span><br></pre></td></tr></table></figure><h4 id="4-2-4-输出到-kafka"><a href="#4-2-4-输出到-kafka" class="headerlink" title="4.2.4 输出到 kafka"></a>4.2.4 输出到 kafka</h4><p>:TODO</p><h3 id="4-3-Flume-Channel"><a href="#4-3-Flume-Channel" class="headerlink" title="4.3. Flume Channel"></a>4.3. Flume Channel</h3><h4 id="4-3-1-Memory-Channel"><a href="#4-3-1-Memory-Channel" class="headerlink" title="4.3.1.Memory Channel"></a>4.3.1.Memory Channel</h4><p><strong>以上用的都是Memory Channel, 不再显示</strong></p><h4 id="4-3-2-FileChannel"><a href="#4-3-2-FileChannel" class="headerlink" title="4.3.2 FileChannel"></a>4.3.2 FileChannel</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">1) 配置</span><br><span class="line">===================================================================</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks= k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line">a1.sources.r1.type=netcat</span><br><span class="line">a1.sources.r1.bind=localhost</span><br><span class="line">a1.sources.r1.port=8888</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type=logger</span><br><span class="line"></span><br><span class="line">a1.channels.c1.type = file</span><br><span class="line">a1.channels.c1.checkpointDir = /home/centos/flume/fc_check</span><br><span class="line">a1.channels.c1.dataDirs = /home/centos/flume/fc_data</span><br><span class="line"></span><br><span class="line">a1.sources.r1.channels=c1</span><br><span class="line">a1.sinks.k1.channel=c1</span><br><span class="line"></span><br><span class="line">2) 创建的文件大概是这样</span><br><span class="line">===================================================================</span><br><span class="line">$~/flumedata&gt; ls -r <span class="built_in">fc</span>*</span><br><span class="line">fc_data:</span><br><span class="line"><span class="built_in">log</span>-1.meta  <span class="built_in">log</span>-1  in_use.lock</span><br><span class="line"></span><br><span class="line">fc_check:</span><br><span class="line">queueset  in_use.lock  inflighttakes  inflightputs  checkpoint.meta  checkpoint</span><br></pre></td></tr></table></figure><h4 id="4-3-3-可溢出文件通道"><a href="#4-3-3-可溢出文件通道" class="headerlink" title="4.3.3 可溢出文件通道"></a>4.3.3 可溢出文件通道</h4><p><strong>This channel is currently experimental and not recommended for use in production.</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[spillable_channel.conf]</span><br><span class="line"></span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.channels.c1.type = SPILLABLEMEMORY</span><br><span class="line"><span class="comment">#0表示禁用内存通道，等价于文件通道</span></span><br><span class="line">a1.channels.c1.memoryCapacity = 0</span><br><span class="line"><span class="comment">#0,禁用文件通道，等价内存通道。</span></span><br><span class="line">a1.channels.c1.overflowCapacity = 2000</span><br><span class="line"></span><br><span class="line">a1.channels.c1.byteCapacity = 800000</span><br><span class="line">a1.channels.c1.checkpointDir = /user/centos/flume/fc_check</span><br><span class="line">a1.channels.c1.dataDirs = /user/centos/flume/fc_data</span><br></pre></td></tr></table></figure><h3 id="4-4-使用AvroSource和AvroSink实现跃点agent处理"><a href="#4-4-使用AvroSource和AvroSink实现跃点agent处理" class="headerlink" title="4.4 使用AvroSource和AvroSink实现跃点agent处理"></a>4.4 使用AvroSource和AvroSink实现跃点agent处理</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-30-144236.png" alt="image-20180630224236332"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"> [avro_hop.conf]</span><br><span class="line"><span class="comment">############ a1 ############</span></span><br><span class="line"><span class="comment"># 声明3种组件</span></span><br><span class="line">a1.sources=r1</span><br><span class="line">a1.channels=c1</span><br><span class="line">a1.sinks=k1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 source 信息</span></span><br><span class="line">a1.sources.r1.type=netcat</span><br><span class="line">a1.sources.r1.bind=localhost</span><br><span class="line">a1.sources.r1.port=8888</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 sink 信息</span></span><br><span class="line">a1.sinks.k1.type=avro</span><br><span class="line">a1.sinks.k1.hostname=localhost</span><br><span class="line">a1.sinks.k1.port=9999</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 channel 信息</span></span><br><span class="line">a1.channels.c1.type=memory</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绑定在一起</span></span><br><span class="line">a1.sources.r1.channels=c1</span><br><span class="line">a1.sinks.k1.channel=c1</span><br><span class="line"></span><br><span class="line"><span class="comment">############ a2 ############</span></span><br><span class="line"><span class="comment"># 声明3种组件</span></span><br><span class="line">a2.sources=r2</span><br><span class="line">a2.channels=c2</span><br><span class="line">a2.sinks=k2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 source 信息</span></span><br><span class="line"><span class="comment"># source 是 avro, 绑定端口9999</span></span><br><span class="line">a2.sources.r2.type=avro</span><br><span class="line">a2.sources.r2.bind=localhost</span><br><span class="line">a2.sources.r2.port=9999</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 sink 信息</span></span><br><span class="line"><span class="comment"># sink 打印到控制台</span></span><br><span class="line">a2.sinks.k2.type=logger</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 channel 信息</span></span><br><span class="line">a2.channels.c2.type=memory</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绑定在一起</span></span><br><span class="line">a2.sources.r2.channels=c2</span><br><span class="line">a2.sinks.k2.channel=c2</span><br><span class="line"></span><br><span class="line">2.启动a2</span><br><span class="line">===================================================================</span><br><span class="line">$&gt;flume-ng agent -f /soft/flume/conf/avro_hop.conf -n a2 -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">3.验证a2</span><br><span class="line">===================================================================</span><br><span class="line">$&gt;netstat -anop | grep 9999</span><br><span class="line"></span><br><span class="line">4.启动a1</span><br><span class="line">===================================================================</span><br><span class="line">$&gt;flume-ng agent -f /soft/flume/conf/avro_hop.conf -n a1</span><br><span class="line"></span><br><span class="line">5.验证a1</span><br><span class="line">===================================================================</span><br><span class="line">$&gt;netstat -anop | grep 8888</span><br><span class="line"></span><br><span class="line">6.nc 连接a1 &amp; 发消息</span><br><span class="line">===================================================================</span><br><span class="line">$&gt;nc localhost 8888</span><br></pre></td></tr></table></figure><h3 id="4-5-Flume-的高可用配置"><a href="#4-5-Flume-的高可用配置" class="headerlink" title="4.5 Flume 的高可用配置"></a>4.5 Flume 的高可用配置</h3><ul><li><p>是通过配置优先级来实现主从, 优先级是组内的优先级, 需要先设置为一组</p></li><li><p>优先级高的 down 掉了, 再启动起来, 依然是主 ==&gt; <strong>已验证</strong></p></li><li><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">==============================</span><br><span class="line">--设置优先级代配置如下----------</span><br><span class="line"><span class="comment">#set sink group 设置为一组</span></span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line"><span class="comment">#set failover 设置容灾相关的参数</span></span><br><span class="line">a1.sinkgroups.g1.processor.type = failover</span><br><span class="line"><span class="comment"># 设置 组内成员的优先级, 决定 主备</span></span><br><span class="line">a1.sinkgroups.g1.processor.priority.k1 = 10 </span><br><span class="line">a1.sinkgroups.g1.processor.priority.k2 = 1 </span><br><span class="line">a1.sinkgroups.g1.processor.maxpenalty = 10000</span><br></pre></td></tr></table></figure></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-02-080516.jpg" alt=""></p><ul><li><strong>以下配置方案与上图不同的, 用的是1个 <code>channel</code>, 2个`sink</strong>`</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line">1. 配置 agent ==&gt; cs1, cs2</span><br><span class="line">================================================================================================</span><br><span class="line">[ha_agent.conf]</span><br><span class="line">------------------</span><br><span class="line"></span><br><span class="line"><span class="comment">#agent name: a1 </span></span><br><span class="line">a1.channels = c1 </span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line"><span class="comment">#set gruop</span></span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line"></span><br><span class="line"><span class="comment">#set channel</span></span><br><span class="line">a1.channels.c1.type = memory </span><br><span class="line">a1.channels.c1.capacity = 1000 </span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="comment"># set sources</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sources.r1.type = <span class="built_in">exec</span></span><br><span class="line">a1.sources.r1.command = tail -F /home/ap/flumedata/testha.log</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里有疑问 : 这里设置拦截器的意义?? 随便设置的吗?? :TODO</span></span><br><span class="line">a1.sources.r1.interceptors = i1 i2 </span><br><span class="line">a1.sources.r1.interceptors.i1.type = static </span><br><span class="line">a1.sources.r1.interceptors.i1.key = Type </span><br><span class="line">a1.sources.r1.interceptors.i1.value = LOGIN </span><br><span class="line">a1.sources.r1.interceptors.i2.type = timestamp</span><br><span class="line"></span><br><span class="line"><span class="comment"># set sink1 -&gt; 通过 avro 沉到 cs3的 source 上</span></span><br><span class="line">a1.sinks.k1.channel = c1 </span><br><span class="line">a1.sinks.k1.type = avro </span><br><span class="line">a1.sinks.k1.hostname = cs3 </span><br><span class="line">a1.sinks.k1.port = 52020</span><br><span class="line"></span><br><span class="line"><span class="comment"># set sink2  -&gt; 通过 avro 沉到 cs4的 source 上</span></span><br><span class="line">a1.sinks.k2.channel = c1 </span><br><span class="line">a1.sinks.k2.type = avro </span><br><span class="line">a1.sinks.k2.hostname = cs4 </span><br><span class="line">a1.sinks.k2.port = 52020</span><br><span class="line"></span><br><span class="line"><span class="comment">#set sink group 设置为一组</span></span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line"><span class="comment">#set failover 设置容灾相关的参数</span></span><br><span class="line">a1.sinkgroups.g1.processor.type = failover</span><br><span class="line"><span class="comment"># 设置 组内成员的优先级, 决定 主备</span></span><br><span class="line">a1.sinkgroups.g1.processor.priority.k1 = 10 </span><br><span class="line">a1.sinkgroups.g1.processor.priority.k2 = 1 </span><br><span class="line">a1.sinkgroups.g1.processor.maxpenalty = 10000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2. 配置 collector ==&gt; cs3,cs4</span><br><span class="line">================================================================================================</span><br><span class="line">[ha_collector.conf]</span><br><span class="line"></span><br><span class="line"><span class="comment">#set agent name </span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1 </span><br><span class="line">a1.sinks = k1</span><br><span class="line"></span><br><span class="line"><span class="comment">#set channel</span></span><br><span class="line">a1.channels.c1.type = memory </span><br><span class="line">a1.channels.c1.capacity = 1000 </span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="comment"># other node,nna to nns</span></span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line"></span><br><span class="line"><span class="comment"># :TODO 这些 key, value 设置了之后, 用在什么地方???</span></span><br><span class="line"><span class="comment">## 当前主机是什么，就修改成什么主机名 </span></span><br><span class="line">a1.sources.r1.bind = cs3 </span><br><span class="line">a1.sources.r1.port = 52020 </span><br><span class="line">a1.sources.r1.interceptors = i1 </span><br><span class="line">a1.sources.r1.interceptors.i1.type = static </span><br><span class="line">a1.sources.r1.interceptors.i1.key = Collector</span><br><span class="line"><span class="comment">## 当前主机是什么，就修改成什么主机名 </span></span><br><span class="line">a1.sources.r1.interceptors.i1.value = cs3 </span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="comment">#set sink to hdfs</span></span><br><span class="line">a1.sinks.k1.type=hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path=/user/ap/flume/event_ha/loghdfs </span><br><span class="line">a1.sinks.k1.hdfs.fileType=DataStream </span><br><span class="line">a1.sinks.k1.hdfs.writeFormat=TEXT </span><br><span class="line">a1.sinks.k1.hdfs.rollInterval=10</span><br><span class="line">a1.sinks.k1.channel=c1 </span><br><span class="line">a1.sinks.k1.hdfs.filePrefix=%Y-%m-%d</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">3. 启动</span><br><span class="line">================================================================================================</span><br><span class="line"><span class="comment"># 先启动 cs3,cs4 ==&gt; collector </span></span><br><span class="line">$&gt; [ap@cs4]~/apps/flume/conf/confs%  </span><br><span class="line">flume-ng agent -f ha_collector.conf -n a1 -Dflume.root.logger=DEBUG,console</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再启动 cs1, cs2  ==&gt; agent, 连接 collector</span></span><br><span class="line">[ap@cs1]~/apps/flume/conf/confs% </span><br><span class="line">flume-ng agent  -f ha_agent.conf -n a1 -Dflume.root.logger=DEBUG,console</span><br></pre></td></tr></table></figure><h3 id="4-6-实用业务场景"><a href="#4-6-实用业务场景" class="headerlink" title="4.6 实用业务场景"></a>4.6 实用业务场景</h3><p>知识点: <strong>拦截器</strong> <strong>interceptors</strong></p><h4 id="4-6-1-需求"><a href="#4-6-1-需求" class="headerlink" title="4.6.1 需求:"></a>4.6.1 <strong>需求:</strong></h4><blockquote><p>A、B 两台日志服务机器实时生产日志主要类型为 access.log、nginx.log、web.log 现在要求:</p><p>把 A、B 机器中的 access.log、nginx.log、web.log 采集汇总到 C 机器上然后统一收集到 hdfs 中。</p><p>但是在 hdfs 中要求的目录为:</p><p>/source/logs/access/20160101/**</p><p>/source/logs/nginx/20160101/**</p><p>/source/logs/web/20160101/** </p></blockquote><h4 id="4-6-2-需求分析"><a href="#4-6-2-需求分析" class="headerlink" title="4.6.2 需求分析"></a>4.6.2 需求分析</h4><p><strong>需求分析图示</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-02-091340.png" alt="image-20180702171339961"></p><p><strong>数据处理流程分析</strong></p><ul><li>在<strong>每一台节点</strong>上都要 <strong>搜集 3个 source 源</strong></li><li>搜集完成后发往同一台节点</li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-02-091444.png" alt="image-20180702171443885"></p><h4 id="4-6-3-需求实现"><a href="#4-6-3-需求实现" class="headerlink" title="4.6.3 需求实现"></a>4.6.3 需求实现</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line">1.在cs1,cs2上配置flume, 通过 avro, sink到cs3</span><br><span class="line">=============================================================</span><br><span class="line">[exec_source_avro_sink.conf]</span><br><span class="line">------------------------------</span><br><span class="line"><span class="comment"># 指定各个核心组件 </span></span><br><span class="line">a1.sources = r1 r2 r3 </span><br><span class="line">a1.sinks = k1 </span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据源</span></span><br><span class="line"><span class="comment"># static 拦截器的功能就是往采集到的数据的 header 中插入自己定义的 key-value 对 </span></span><br><span class="line"><span class="comment"># sources-&gt;r1</span></span><br><span class="line">a1.sources.r1.type = <span class="built_in">exec</span></span><br><span class="line">a1.sources.r1.command = tail -F -c +0 /home/ap/flumedata/access.log </span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = static</span><br><span class="line">a1.sources.r1.interceptors.i1.key = <span class="built_in">type</span></span><br><span class="line">a1.sources.r1.interceptors.i1.value = access</span><br><span class="line"></span><br><span class="line"><span class="comment"># sources-&gt;r2</span></span><br><span class="line">a1.sources.r2.type = <span class="built_in">exec</span></span><br><span class="line">a1.sources.r2.command = tail -F  -c +0 /home/ap/flumedata/nginx.log </span><br><span class="line">a1.sources.r2.interceptors = i2</span><br><span class="line">a1.sources.r2.interceptors.i2.type = static </span><br><span class="line">a1.sources.r2.interceptors.i2.key = <span class="built_in">type</span> </span><br><span class="line">a1.sources.r2.interceptors.i2.value = nginx</span><br><span class="line"></span><br><span class="line"><span class="comment"># sources-&gt;r3</span></span><br><span class="line">a1.sources.r3.type = <span class="built_in">exec</span></span><br><span class="line">a1.sources.r3.command = tail -F  -c +0 /home/ap/flumedata/web.log </span><br><span class="line">a1.sources.r3.interceptors = i3</span><br><span class="line">a1.sources.r3.interceptors.i3.type = static </span><br><span class="line">a1.sources.r3.interceptors.i3.key = <span class="built_in">type</span> </span><br><span class="line">a1.sources.r3.interceptors.i3.value = web</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro </span><br><span class="line">a1.sinks.k1.hostname = cs3 </span><br><span class="line">a1.sinks.k1.port = 41414</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use a channel which buffers events in memory </span></span><br><span class="line">a1.channels.c1.type = memory </span><br><span class="line">a1.channels.c1.capacity = 20000 </span><br><span class="line">a1.channels.c1.transactionCapacity = 10000</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the source and sink to the channel </span></span><br><span class="line">a1.sources.r1.channels = c1 </span><br><span class="line">a1.sources.r2.channels = c1 </span><br><span class="line">a1.sources.r3.channels = c1 </span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line">2. 在 cs3上配置 flume, <span class="built_in">source</span> 为 avro, 接受来自 cs1,cs2下沉的数据</span><br><span class="line">======================================================================</span><br><span class="line">[avro_source_hdfs_sink.conf]</span><br><span class="line">-------------------------------</span><br><span class="line"><span class="comment">#定义 agent 名， source、channel、sink 的名称 </span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义 source </span></span><br><span class="line">a1.sources.r1.type = avro </span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port =41414</span><br><span class="line"></span><br><span class="line"><span class="comment">#添加时间拦截器</span></span><br><span class="line"><span class="comment">#Note:For all of the time related escape sequences, a header with the key “timestamp” must #exist among the headers of the event (unless hdfs.useLocalTimeStamp is set to true). One way #to add this automatically is to use the TimestampInterceptor. </span></span><br><span class="line">a1.sources.r1.interceptors = i1 a1.sources.r1.interceptors.i1.type=org.apache.flume.interceptor.TimestampInterceptor<span class="variable">$Builder</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义 channels</span></span><br><span class="line">a1.channels.c1.type = memory </span><br><span class="line"><span class="comment">#The maximum number of events stored in the channel</span></span><br><span class="line">a1.channels.c1.capacity = 20000 </span><br><span class="line"><span class="comment">#The maximum number of events the channel will take from a source or give to a sink per transaction</span></span><br><span class="line">a1.channels.c1.transactionCapacity = 10000</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义 sink</span></span><br><span class="line">a1.sinks.k1.type = hdfs </span><br><span class="line">a1.sinks.k1.hdfs.path = /user/ap/flume/event/%&#123;<span class="built_in">type</span>&#125;/%Y%m%d </span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = events</span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat = Text</span><br><span class="line"></span><br><span class="line"><span class="comment">#时间类型</span></span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = <span class="literal">true</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">#生成的文件不按条数生成, 否则就是按照设定的 event产生的, 1行就是一个 event </span></span><br><span class="line"><span class="comment">#Number of events written to file before it rolled (0 = never roll based on number of events)</span></span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 0</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成的文件按时间生成,单位是秒</span></span><br><span class="line"><span class="comment">#Number of seconds to wait before rolling current file (0 = never roll based on time interval)</span></span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 30</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成的文件按大小生成, 单位是 bytes</span></span><br><span class="line"><span class="comment">#1 MB = 1,024 KB = 1,048,576 Bytes </span></span><br><span class="line"><span class="comment">#File size to trigger roll, in bytes (0: never roll based on file size)</span></span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 10485760   <span class="comment">#这里是10MB</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#批量写入 hdfs 的个数</span></span><br><span class="line">a1.sinks.k1.hdfs.batchSize = 20</span><br><span class="line"></span><br><span class="line"><span class="comment">#flume 操作 hdfs 的线程数(包括新建，写入等) </span></span><br><span class="line">a1.sinks.k1.hdfs.threadsPoolSize=10</span><br><span class="line"></span><br><span class="line"><span class="comment">#操作 hdfs 超时时间</span></span><br><span class="line">a1.sinks.k1.hdfs.callTimeout=30000</span><br><span class="line"></span><br><span class="line"><span class="comment">#组装 source、channel、sink </span></span><br><span class="line">a1.sources.r1.channels = c1 </span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line">3.启动服务</span><br><span class="line">============================================================================</span><br><span class="line"><span class="comment"># 启动cs3 =&gt; 服务器 c</span></span><br><span class="line">[ap@cs3]~/apps/flume/conf/confs% flume-ng agent -f avro_source_hdfs_sink.conf -n a1 -Dflume.root.logger=DEBUG,console</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动cs1, cs2 =&gt; 服务器 a, b</span></span><br><span class="line"><span class="comment"># 注意: 千万别启错了, 否则很难找到原因..</span></span><br><span class="line">[ap@cs2]~/apps/flume/conf/confs% flume-ng agent -f exec_source_avro_sink.conf  -n a1 -Dflume.root.logger=DEBUG,console</span><br><span class="line"></span><br><span class="line">==============</span><br><span class="line">遇到问题: </span><br><span class="line">这种通过avro多点 sink 到一个 <span class="built_in">source</span> 的时候, 实现不创建文件夹/文件的, 好像确实写入不进去...</span><br><span class="line">===&gt; md, 原因应该是启动错配置了</span><br></pre></td></tr></table></figure><h2 id="5-Flume的Maven依赖"><a href="#5-Flume的Maven依赖" class="headerlink" title="5.Flume的Maven依赖"></a>5.Flume的Maven依赖</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.rox<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>Flume_test<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flume<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flume-ng-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.6.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flume.flume-ng-sinks<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flume-hdfs-sink<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.6.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flume.flume-ng-sinks<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flume-ng-hbase-sink<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.6.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flume.flume-ng-channels<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flume-file-channel<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.6.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-Flume-概念&quot;&gt;&lt;a href=&quot;#1-Flume-概念&quot; class=&quot;headerlink&quot; title=&quot;1.Flume 概念&quot;&gt;&lt;/a&gt;1.Flume 概念&lt;/h2&gt;&lt;p&gt;提供 &lt;strong&gt;收集、移动、聚合大量日志数据&lt;/strong&gt;的服务。
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="Flume" scheme="https://airpoet.github.io/categories/Hadoop/Flume/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="Flume" scheme="https://airpoet.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Azkaban</title>
    <link href="https://airpoet.github.io/2018/06/29/Hadoop/6-Sqoop&amp;Azkaban/Azkaban/"/>
    <id>https://airpoet.github.io/2018/06/29/Hadoop/6-Sqoop&amp;Azkaban/Azkaban/</id>
    <published>2018-06-29T02:46:03.592Z</published>
    <updated>2018-06-29T08:28:30.050Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-出现契机"><a href="#1-出现契机" class="headerlink" title="1.出现契机"></a>1.出现契机</h2><p><strong>应用场景举例:</strong> </p><p>1、通过 Hadoop 先将原始数据同步到 HDFS 上;</p><p>2、借助 MapReduce 计算框架对原始数据进行清洗转换，生成的数据以分区表的形式存储到 多张 Hive 表中;</p><p>3、需要对 Hive 中多个表的数据进行 Join 处理，得到一个明细数据 Hive 大表; 4、将明细数据进行各种统计分析，得到结果报表信息; 5、需要将统计分析得到的结果数据同步到业务系统中，供业务调用使用。</p><h2 id="2-常见工作流调度系统-amp-对比"><a href="#2-常见工作流调度系统-amp-对比" class="headerlink" title="2.常见工作流调度系统 &amp; 对比"></a>2.常见工作流调度系统 &amp; 对比</h2><p>在 Hadoop 领域，常见的工作流调度器有 Oozie，Azkaban，Cascading，Hamake 等 </p><h4 id="2-1-各种调度工具对比"><a href="#2-1-各种调度工具对比" class="headerlink" title="2.1.各种调度工具对比"></a>2.1.各种调度工具对比</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-29-024909.png" alt="image-20180629104909306"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-29-024925.png" alt="image-20180629104925001"></p><h4 id="2-2-Azkaban-与-Oozie-对比"><a href="#2-2-Azkaban-与-Oozie-对比" class="headerlink" title="2.2.Azkaban 与 Oozie 对比"></a>2.2.Azkaban 与 Oozie 对比</h4><p>ooize 相比 azkaban 是一个重量级的任务调度系统，功能全面，但配置使用也更复杂。如果可以不 在意某些功能的缺失，轻量级调度器 azkaban 是很不错的候选对象。 </p><p><strong> 工作流定义</strong></p><p>Azkaban 使用 Properties 文件定义工作流 Oozie 使用 XML 文件定义工作流</p><p><strong> 定时执行</strong></p><p>Azkaban 的定时执行任务是基于时间的 Oozie 的定时执行任务基于时间和输入数据</p><h4 id="2-3-Azkaban-组件关系"><a href="#2-3-Azkaban-组件关系" class="headerlink" title="2.3.Azkaban 组件关系"></a>2.3.Azkaban 组件关系</h4><p>关系数据库(目前仅支持 MySQL) web </p><p>管理服务器-AzkabanWebServer </p><p>执行服务器-AzkabanExecutorServer </p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-29-025120.png" alt="image-20180629105120180"></p><p>Azkaban 使用 MySQL 来存储它的状态信息，Azkaban Executor Server 和 Azkaban Web Server 均使用到了 MySQL 数据库。 </p><p><strong>它有如下功能特点:</strong></p><p> Web 用户界面</p><p> 方便上传工作流</p><p> 方便设置任务之间的关系  调度工作流</p><p> 认证/授权(权限的工作)</p><p> 能够杀死并重新启动工作流</p><p> 模块化和可插拔的插件机制</p><p> 项目工作区</p><p> 工作流和任务的日志记录和审计</p><h2 id="3-Azkaban-安装部署"><a href="#3-Azkaban-安装部署" class="headerlink" title="3.Azkaban 安装部署"></a>3.Azkaban 安装部署</h2><h4 id="3-1准备工作"><a href="#3-1准备工作" class="headerlink" title="3.1准备工作"></a>3.1准备工作</h4><p><strong>Azkaban Web 服务器</strong>:azkaban-web-server-2.5.0.tar.gz</p><p><strong>Azkaban Excutor 执行服务器</strong> :azkaban-executor-server-2.5.0.tar.gz </p><p><strong>Azkaban 初始化脚本文件</strong>:azkaban-sql-script-2.5.0.tar.gz</p><h4 id="3-2安装说明"><a href="#3-2安装说明" class="headerlink" title="3.2安装说明"></a>3.2安装说明</h4><p>将安装文件上传到集群,最好上传到安装hive、sqoop 的机器上,方便命令的执行。并最好同</p><p>一存放在 apps 目录下,用于存放源安装文件.新建 azkaban 目录,用于存放 azkaban 运行程序</p><h4 id="3-3解压"><a href="#3-3解压" class="headerlink" title="3.3解压"></a>3.3解压</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf azkaban-web-server-2.5.0.tar.gz -C /home/apps/ap/azkaban/</span><br><span class="line">tar -zxvf azkaban-executor-server-2.5.0.tar.gz -C /home/apps/ap/azkaban/</span><br><span class="line">tar -zxvf azkaban-sql-script-2.5.0.tar.gz -C /home/apps/ap/azkaban/</span><br></pre></td></tr></table></figure><h4 id="3-4进入-mysql-导入任务-job-表"><a href="#3-4进入-mysql-导入任务-job-表" class="headerlink" title="3.4进入 mysql 导入任务 job 表"></a>3.4进入 mysql 导入任务 job 表</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create database azkaban; </span><br><span class="line">Query OK, 1 row affected (0.01 sec)</span><br><span class="line">mysql&gt; use azkaban; </span><br><span class="line">Database changed</span><br><span class="line">mysql&gt; source /home/hadoop/apps/azkaban/azkaban-script-2.5.0/create-all-sql-2.5.0.sql;</span><br></pre></td></tr></table></figure><h4 id="3-5创建-SSL-配置"><a href="#3-5创建-SSL-配置" class="headerlink" title="3.5创建 SSL 配置"></a>3.5创建 SSL 配置</h4><p>最好是在 azkaban 目录下执行: </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 执行命令: </span></span><br><span class="line">keytool -keystore keystore -<span class="built_in">alias</span> jetty -genkey -keyalg RSA</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行此命令后,会提示输入当前生成 keystore 的密码及相应信息,输入密码请劳记,信息如下:</span></span><br><span class="line">Enter keystore password:</span><br><span class="line">Re-enter new password:</span><br><span class="line">What is your first and last name?</span><br><span class="line">[Unknown]:</span><br><span class="line">What is the name of your organizational unit?</span><br><span class="line">[Unknown]:</span><br><span class="line">What is the name of your organization?</span><br><span class="line">[Unknown]:</span><br><span class="line">What is the name of your City or Locality?</span><br><span class="line">[Unknown]:</span><br><span class="line">What is the name of your State or Province? </span><br><span class="line">[Unknown]:</span><br><span class="line">What is the two-letter country code <span class="keyword">for</span> this unit? </span><br><span class="line">[Unknown]: CN</span><br><span class="line">Is CN=Unknown, OU=Unknown, O=Unknown, L=Unknown, ST=Unknown, C=CN correct? </span><br><span class="line">[no]: y</span><br><span class="line"></span><br><span class="line">Enter key password <span class="keyword">for</span> &lt;jetty&gt;</span><br><span class="line">(RETURN <span class="keyword">if</span> same as keystore password):</span><br><span class="line"></span><br><span class="line">此时, 会发现目录下有一个 Keystore 文件</span><br><span class="line"></span><br><span class="line">------------------------------------------</span><br><span class="line">完成上述工作后,将在当前目录生成 keystore 证书文件,将 keystore 拷贝到 azkaban web 服务 器根目录中.如:</span><br><span class="line"></span><br><span class="line">cp keystore azkaban-web-2.5.0</span><br></pre></td></tr></table></figure><h4 id="3-6-修改配置文件"><a href="#3-6-修改配置文件" class="headerlink" title="3.6 修改配置文件"></a>3.6 修改配置文件</h4><p><strong>注意: 配置文件后面一定不要有空格</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">1) 生成时区配置文件 Asia/Shanghai，用交互式命令 tzselect 即可, 如果有的话就不用再生成了</span><br><span class="line">tzselect</span><br><span class="line"></span><br><span class="line">2) 拷贝该时区文件，覆盖系统本地时区配置</span><br><span class="line">sudo cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</span><br><span class="line"></span><br><span class="line">3)  azkaban web 服务器配置 </span><br><span class="line"></span><br><span class="line">3.1)  配置 web 的 azkaban.properties </span><br><span class="line"><span class="built_in">cd</span> ~/apps/azkaban/azkaban-web-2.5.0/conf/</span><br><span class="line">vi azkaban.properties</span><br><span class="line"> 以下是需要修改的内容 </span><br><span class="line"><span class="comment">#默认根 web 目录</span></span><br><span class="line">web.resource.dir=/home/ap/apps/azkaban/azkaban-web-2.5.0/web</span><br><span class="line"></span><br><span class="line"><span class="comment">#默认时区,已改为亚洲/上海 默认为美国</span></span><br><span class="line">default.timezone.id=Asia/Shanghai</span><br><span class="line"></span><br><span class="line"><span class="comment">#用户配置,具体配置参加下文</span></span><br><span class="line">user.manager.xml.file=/home/ap/apps/azkaban/azkaban-web-2.5.0/conf/azkaban-users.xml</span><br><span class="line"></span><br><span class="line"><span class="comment">#global配置文件所在位置</span></span><br><span class="line">executor.global.properties=/home/ap/apps/azkaban/azkaban-executor-2.5.0/conf/global.properties</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据库相关配置</span></span><br><span class="line">mysql.host=cs2</span><br><span class="line">mysql.database=azkaban</span><br><span class="line">mysql.user=root</span><br><span class="line">mysql.password=123</span><br><span class="line"></span><br><span class="line"><span class="comment">#SSL 文件名</span></span><br><span class="line">jetty.keystore=/home/ap/apps/azkaban/azkaban-web-2.5.0/keystore</span><br><span class="line"><span class="comment">#SSL 文件密码</span></span><br><span class="line">jetty.password=123456</span><br><span class="line"><span class="comment">#Jetty 主密码 与 keystore 文件相同</span></span><br><span class="line">jetty.keypassword=123456</span><br><span class="line"><span class="comment">#SSL 文件名</span></span><br><span class="line">jetty.truststore=/home/ap/apps/azkaban/azkaban-web-2.5.0/keystore</span><br><span class="line"><span class="comment"># SSL 文件密码</span></span><br><span class="line">jetty.trustpassword=123456</span><br><span class="line"></span><br><span class="line">3.2) 配置 web 的 azkaban-users.xml </span><br><span class="line"><span class="comment"># 增加 管理员用户</span></span><br><span class="line">vi azkaban-users.xml </span><br><span class="line"></span><br><span class="line">    &lt;azkaban-users&gt;</span><br><span class="line">        &lt;user username=<span class="string">"azkaban"</span> password=<span class="string">"azkaban"</span> roles=<span class="string">"admin"</span> groups=<span class="string">"azkaban"</span>/&gt;</span><br><span class="line">        &lt;user username=<span class="string">"metrics"</span> password=<span class="string">"metrics"</span> roles=<span class="string">"metrics"</span>/&gt;</span><br><span class="line">        <span class="comment"># 此行是加的, 指用户名密码都为 admin</span></span><br><span class="line">        &lt;user username=<span class="string">"admin"</span> password=<span class="string">"admin"</span> roles=<span class="string">"admin,metrics"</span>/&gt;</span><br><span class="line">        </span><br><span class="line">        &lt;role name=<span class="string">"admin"</span> permissions=<span class="string">"ADMIN"</span>/&gt;</span><br><span class="line">        &lt;role name=<span class="string">"metrics"</span> permissions=<span class="string">"METRICS"</span>/&gt;</span><br><span class="line">    &lt;/azkaban-users&gt;</span><br><span class="line">    </span><br><span class="line">4)  配置执行服务器 executor </span><br><span class="line"></span><br><span class="line"> <span class="built_in">cd</span> apps/azkaban/azkaban-executor-2.5.0/conf/ </span><br><span class="line"> vi azkaban.properties</span><br><span class="line">-----</span><br><span class="line">以下是需要修改的内容</span><br><span class="line">------</span><br><span class="line"><span class="comment">#时区</span></span><br><span class="line">default.timezone.id=Asia/Shanghai </span><br><span class="line"></span><br><span class="line"><span class="comment"># Azkaban JobTypes 插件配置，插件所在位置 </span></span><br><span class="line">azkaban.jobtype.plugin.dir=/home/hadoop/apps/azkaban/azkaban-executor-2.5.0/plugins/jobtypes</span><br><span class="line"></span><br><span class="line"><span class="comment">#Loader for projects</span></span><br><span class="line">executor.global.properties=/home/ap/apps/azkaban/azkaban-executor-2.5.0/conf/global.properties</span><br><span class="line"></span><br><span class="line"><span class="comment">#mysql 需要修改的地方</span></span><br><span class="line">mysql.host=cs2</span><br><span class="line">mysql.database=azkaban</span><br><span class="line">mysql.user=root</span><br><span class="line">mysql.password=123</span><br><span class="line"></span><br><span class="line">5) 配置环境变量 </span><br><span class="line">vi .zshrc </span><br><span class="line"><span class="comment"># azkaban</span></span><br><span class="line"><span class="built_in">export</span> AZKABAN_WEB_HOME=/home/ap/apps/azkaban/azkaban-web-2.5.0</span><br><span class="line"><span class="built_in">export</span> AZKABAN_EXE_HOME=/home/ap/apps/azkaban/azkaban-executor-2.5.0</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$AZKABAN_WEB_HOME</span>/bin:<span class="variable">$AZKABAN_EXE_HOME</span>/bin</span><br></pre></td></tr></table></figure><h4 id="3-7-启动"><a href="#3-7-启动" class="headerlink" title="3.7 启动"></a>3.7 启动</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">1)  启动 Azkaban Web Server: </span><br><span class="line"><span class="comment"># ps: .out 是日志, 可以先创建 logs 目录, 日志地址随便</span></span><br><span class="line"><span class="comment"># 前台启动</span></span><br><span class="line">azkaban-web-start.sh</span><br><span class="line"><span class="comment"># 后台启动</span></span><br><span class="line">nohup azkaban-web-start.sh 1&gt;/home/ap/logs/azwebstd.out 2&gt;/home/ap/logs/azweberr.out &amp;</span><br><span class="line"></span><br><span class="line">2)  启动 Azkaban Executor: </span><br><span class="line"><span class="comment">#前台启动</span></span><br><span class="line">azkaban-executor-start.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 后台启动</span></span><br><span class="line">nohup azkaban-executor-start.sh 1&gt;/home/ap/logs/azexstd.out 2&gt;/home/ap/logs/azexerr.out &amp;</span><br><span class="line"></span><br><span class="line">3)  登录 webUI </span><br><span class="line">https://cs2:8443/</span><br><span class="line">u: admin</span><br><span class="line">p: admin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意: 配置后面不能有空格!!!</span></span><br></pre></td></tr></table></figure><h2 id="4-Azkaban-实战演示"><a href="#4-Azkaban-实战演示" class="headerlink" title="4.Azkaban 实战演示"></a>4.Azkaban 实战演示</h2><p><a href="https://app.yinxiang.com/shard/s37/nl/7399077/1545999d-5e3f-47a6-9bc5-fb9c7bfd98f1/" target="_blank" rel="noopener">见这里</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-出现契机&quot;&gt;&lt;a href=&quot;#1-出现契机&quot; class=&quot;headerlink&quot; title=&quot;1.出现契机&quot;&gt;&lt;/a&gt;1.出现契机&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;应用场景举例:&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;1、通过 Hadoop 先将原始数据同步到
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="Azkaban" scheme="https://airpoet.github.io/categories/Hadoop/Azkaban/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="Azkaban" scheme="https://airpoet.github.io/tags/Azkaban/"/>
    
  </entry>
  
  <entry>
    <title>Sqoop</title>
    <link href="https://airpoet.github.io/2018/06/28/Hadoop/6-Sqoop&amp;Azkaban/Sqoop/"/>
    <id>https://airpoet.github.io/2018/06/28/Hadoop/6-Sqoop&amp;Azkaban/Sqoop/</id>
    <published>2018-06-28T01:45:13.084Z</published>
    <updated>2018-06-29T11:51:12.925Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-作用"><a href="#1-作用" class="headerlink" title="1. 作用"></a>1. 作用</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-28-123439.png" alt="image-20180628203438952"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-28-123419.png" alt="image-20180628203419288"></p><p>Sqoop 是 Apache 旗下一款“Hadoop 和关系数据库服务器之间传送数据”的工具。</p><p><strong>导入数据</strong>：<strong>MySQL，Oracle 导入数据到 Hadoop 的 HDFS、HIVE、HBASE 等数据存储系统</strong></p><p><strong>导出数据</strong>：从 Hadoop 的文件系统中导出数据到关系数据库 MySQL 等</p><p>Sqoop 的本质还是一个命令行工具，和 HDFS，Hive 相比，并没有什么高深的理论。</p><h2 id="2-工作机制"><a href="#2-工作机制" class="headerlink" title="2.工作机制"></a>2.工作机制</h2><p>将导入导出命令翻译成 MapReduce 程序来实现</p><p>在翻译出的 MapReduce 中主要是对 InputFormat 和 OutputFormat 进行定制</p><h2 id="3-安装"><a href="#3-安装" class="headerlink" title="3.安装"></a>3.安装</h2><p>注意: 目录下要有hive, 因为要拿到 hive 的 home, 执行 hive 操作</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">1、 准备安装包 sqoop-1.4.6.bin_hadoop-2.0.4-alpha.tar.gz</span><br><span class="line"></span><br><span class="line">2、 解压安装包到安装目录</span><br><span class="line">tar -zxvf sqoop-1.4.6.bin_hadoop-2.0.4-alpha.tar.gz -C apps/ cd apps</span><br><span class="line">mv sqoop-1.4.6.bin_hadoop-2.0.4-alpha/ sqoop-1.4.6</span><br><span class="line"></span><br><span class="line">3、 进入到 conf 文件夹，找到 sqoop-env-template.sh，修改其名称为 sqoop-env.sh cd conf</span><br><span class="line">mv sqoop-env-template.sh sqoop-env.sh</span><br><span class="line"></span><br><span class="line">4、 修改 sqoop-env.sh</span><br><span class="line"><span class="comment">-----</span></span><br><span class="line">export HADOOP_COMMON_HOME=/home/ap/apps/hadoop export HADOOP_MAPRED_HOME=/home/ap/apps/hadoop export HBASE_HOME=/home/ap/apps/hbase</span><br><span class="line">export HIVE_HOME=/home/ap/apps/hive</span><br><span class="line">export ZOOCFGDIR=/home/ap/apps/zookeeper/conf</span><br><span class="line"><span class="comment">-----</span></span><br><span class="line"></span><br><span class="line">5、 加入 mysql 驱动包到 sqoop-1.4.6/lib 目录下</span><br><span class="line">cp mysql-connector-java-5.1.40-bin.jar ~/apps/sqoop-1.4.6/lib/</span><br><span class="line"></span><br><span class="line">6、 配置系统环境变量 vi ~/.bashrc</span><br><span class="line">然后输入:</span><br><span class="line">export SQOOP_HOME=/home/hadoop/apps/sqoop-1.4.6 export PATH=$PATH:$SQOOP_HOME/bin</span><br><span class="line">然后保存退出</span><br><span class="line">source ~/.bashrc</span><br><span class="line"></span><br><span class="line">7、 验证安装是否成功</span><br><span class="line">sqoop-version 或者 sqoopversion</span><br><span class="line"></span><br><span class="line">ps : 吹出现警告, 不用管</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">[ap@cs2]~% sqoop version</span><br><span class="line">Warning: /home/ap/apps/sqoop/../hcatalog does not exist! HCatalog jobs will fail.</span><br><span class="line">Please <span class="keyword">set</span> $HCAT_HOME <span class="keyword">to</span> the root <span class="keyword">of</span> your HCatalog installation.</span><br><span class="line"><span class="keyword">Warning</span>: /home/ap/apps/sqoop/../accumulo does <span class="keyword">not</span> exist! Accumulo imports will fail.</span><br><span class="line">Please <span class="keyword">set</span> $ACCUMULO_HOME <span class="keyword">to</span> the root <span class="keyword">of</span> your Accumulo installation.</span><br><span class="line"><span class="keyword">Warning</span>: /home/ap/apps/sqoop/../zookeeper does <span class="keyword">not</span> exist! Accumulo imports will fail.</span><br><span class="line">Please <span class="keyword">set</span> $ZOOKEEPER_HOME <span class="keyword">to</span> the root <span class="keyword">of</span> your Zookeeper installation.</span><br><span class="line"><span class="number">18</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">20</span>:<span class="number">43</span>:<span class="number">52</span> INFO sqoop.Sqoop: Running Sqoop <span class="keyword">version</span>: <span class="number">1.4</span><span class="number">.6</span></span><br><span class="line">Sqoop <span class="number">1.4</span><span class="number">.6</span></span><br><span class="line">git <span class="keyword">commit</span> <span class="keyword">id</span> c0c5a81723759fa575844a0a1eae8f510fa32c25</span><br><span class="line"><span class="keyword">Compiled</span> <span class="keyword">by</span> root <span class="keyword">on</span> Mon Apr <span class="number">27</span> <span class="number">14</span>:<span class="number">38</span>:<span class="number">36</span> CST <span class="number">2015</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="number">8</span>\   可以配置记住密码 </span><br><span class="line">但是好像不起作用???</span><br><span class="line"></span><br><span class="line">查看任务详细或者执行任务的时候不输入密码  免密</span><br><span class="line">[sqoop-site.xml]</span><br><span class="line">添加: </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;<span class="keyword">name</span>&gt;sqoop.metastore.client.record.password&lt;/<span class="keyword">name</span>&gt;</span><br><span class="line">        &lt;<span class="keyword">value</span>&gt;<span class="literal">true</span>&lt;/<span class="keyword">value</span>&gt;</span><br><span class="line">        &lt;description&gt;<span class="keyword">If</span> <span class="literal">true</span>, <span class="keyword">allow</span> saved passwords <span class="keyword">in</span> the metastore.&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br></pre></td></tr></table></figure><h2 id="4-基本使用"><a href="#4-基本使用" class="headerlink" title="4. 基本使用"></a>4. 基本使用</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1) sqoop <span class="keyword">help</span> : 查看帮助</span><br><span class="line"><span class="number">2</span>) sqoop <span class="keyword">help</span> <span class="keyword">import</span> : 进一步层级查看</span><br></pre></td></tr></table></figure><h2 id="5-Sqoop-数据导入"><a href="#5-Sqoop-数据导入" class="headerlink" title="5. Sqoop 数据导入"></a>5. Sqoop 数据导入</h2><h5 id="常用指令"><a href="#常用指令" class="headerlink" title="常用指令"></a>常用指令</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--connect指定数据库链接url</span></span><br><span class="line"><span class="comment">--username指定数据库的用户名</span></span><br><span class="line"><span class="comment">--password指定数据库的密码</span></span><br><span class="line"><span class="comment">--table指定要导出数据的mysql数据库表</span></span><br><span class="line">-m指定MapTask的个数</span><br><span class="line"><span class="comment">--target-dir指定导出数据在HDFS上的存储目录</span></span><br><span class="line"><span class="comment">--fields-terminated-by指定每条记录中字段之间的分隔符</span></span><br><span class="line"><span class="comment">--where指定查询SQL的where条件</span></span><br><span class="line"><span class="comment">--query指定查询SQL</span></span><br><span class="line"><span class="comment">--columns指定查询列</span></span><br></pre></td></tr></table></figure><h4 id="5-1-list-mysql-的数据库-amp-表-复制mysql-中表结构相同表-–-gt-hive"><a href="#5-1-list-mysql-的数据库-amp-表-复制mysql-中表结构相同表-–-gt-hive" class="headerlink" title="5.1  list mysql 的数据库 &amp; 表, 复制mysql 中表结构相同表 –&gt; hive"></a>5.1  list mysql 的数据库 &amp; 表, 复制mysql 中表结构相同表 –&gt; hive</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">列出MySQL数据有哪些数据库：</span><br><span class="line"></span><br><span class="line">sqoop list-databases \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/ \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password 123</span></span><br><span class="line"></span><br><span class="line">列出MySQL中的某个数据库有哪些数据表：</span><br><span class="line"></span><br><span class="line">sqoop list-tables \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/mysql \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password 123</span></span><br><span class="line"></span><br><span class="line">创建一张跟mysql中的help_keyword表一样的hive表hk：(没有数据貌似)</span><br><span class="line"></span><br><span class="line">sqoop <span class="keyword">create</span>-hive-<span class="keyword">table</span> \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/mysql \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password 123 \</span></span><br><span class="line"><span class="comment">--table help_keyword \</span></span><br><span class="line"><span class="comment">--hive-table hk</span></span><br></pre></td></tr></table></figure><h4 id="5-2-导入-mysql中表-到-HDFS"><a href="#5-2-导入-mysql中表-到-HDFS" class="headerlink" title="5.2 导入 mysql中表 到 HDFS"></a>5.2 导入 mysql中表 到 HDFS</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">导入MySQL表中数据到HDFS中：</span><br><span class="line"></span><br><span class="line">// 普通导入：导入mysql库中的help_keyword的数据到HDFS上的默认路径：/user/ap/help_keyword</span><br><span class="line">sqoop import   \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/mysql   \</span></span><br><span class="line"><span class="comment">--username root  \</span></span><br><span class="line"><span class="comment">--password 123   \</span></span><br><span class="line"><span class="comment">--table help_keyword   \</span></span><br><span class="line">-m 1</span><br><span class="line"></span><br><span class="line">最后的1,是指使用1个 mapTask </span><br><span class="line">导入到hdfs后, 可以使用 hdfs dfs -text /.... 查看数据</span><br><span class="line"><span class="comment">----</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 导入： 指定分隔符 &amp; 导入路径 &amp; mapTask 个数</span><br><span class="line"></span><br><span class="line">sqoop import   \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/mysql   \</span></span><br><span class="line"><span class="comment">--username root  \</span></span><br><span class="line"><span class="comment">--password 123   \</span></span><br><span class="line"><span class="comment">--table help_keyword   \</span></span><br><span class="line"><span class="comment">--target-dir /user/ap/my_help_keyword1  \</span></span><br><span class="line"><span class="comment">--fields-terminated-by '\t'  \</span></span><br><span class="line">-m 3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 导入数据：带where条件</span><br><span class="line"></span><br><span class="line">sqoop import   \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/mysql   \</span></span><br><span class="line"><span class="comment">--username root  \</span></span><br><span class="line"><span class="comment">--password 123   \</span></span><br><span class="line"><span class="comment">--where "name='STRING' " \</span></span><br><span class="line"><span class="comment">--table help_keyword   \</span></span><br><span class="line"><span class="comment">--target-dir /user/ap/my_help_keyword2  \</span></span><br><span class="line">-m 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 查询指定列</span><br><span class="line"><span class="comment">-------------发现一个结论, 如果启动3个 mapper, 但是最后只有一个结论, 只会生成一个结果文件</span></span><br><span class="line"></span><br><span class="line">sqoop import   \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/mysql   \</span></span><br><span class="line"><span class="comment">--username root  \</span></span><br><span class="line"><span class="comment">--password 123   \</span></span><br><span class="line"><span class="comment">--columns "name" \</span></span><br><span class="line"><span class="comment">--where "name='STRING' " \</span></span><br><span class="line"><span class="comment">--table help_keyword  \</span></span><br><span class="line"><span class="comment">--target-dir /user/ap/my_help_keyword3  \</span></span><br><span class="line">-m 3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 导入：指定自定义查询SQL</span><br><span class="line"><span class="comment">--------------疑点:  split-by 的作用, 和 -m 数量的关系</span></span><br><span class="line"></span><br><span class="line">sqoop import   \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/mysql   \</span></span><br><span class="line"><span class="comment">--username root  \</span></span><br><span class="line"><span class="comment">--password 123   \</span></span><br><span class="line"><span class="comment">--target-dir /user/ap/my_help_keyword4  \</span></span><br><span class="line"><span class="comment">--query 'select help_keyword_id,name from help_keyword WHERE $CONDITIONS and  name = "STRING"' \</span></span><br><span class="line"><span class="comment">--split-by  help_keyword_id \</span></span><br><span class="line"><span class="comment">--fields-terminated-by '\t'  \</span></span><br><span class="line">-m 1</span><br><span class="line"></span><br><span class="line"><span class="comment">----------------------------</span></span><br><span class="line">sqoop import  \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/mysql \</span></span><br><span class="line"><span class="comment">--username root  \</span></span><br><span class="line"><span class="comment">--password 123  \</span></span><br><span class="line"><span class="comment">--target-dir /user/hadoop/my_help_keyword5 \</span></span><br><span class="line"><span class="comment">--query "select help_keyword_id,name from help_keyword WHERE \$CONDITIONS"  \</span></span><br><span class="line"><span class="comment">--split-by  help_keyword_id \</span></span><br><span class="line"><span class="comment">--fields-terminated-by '\t' \</span></span><br><span class="line">-m 1</span><br><span class="line"></span><br><span class="line"># 在以上需要按照自定义SQL语句导出数据到HDFS的情况下：</span><br><span class="line">1、引号问题，要么外层使用单引号，内层使用双引号，$CONDITIONS的$符号不用转义， 要么外层使用双引号，那么内层使用单引号，然后$CONDITIONS的$符号需要转义</span><br><span class="line">2、自定义的SQL语句中必须带有WHERE \$CONDITIONS</span><br></pre></td></tr></table></figure><h4 id="5-3-导入MySQL数据库中的表数据到Hive中："><a href="#5-3-导入MySQL数据库中的表数据到Hive中：" class="headerlink" title="5.3 导入MySQL数据库中的表数据到Hive中："></a>5.3 导入MySQL数据库中的表数据到Hive中：</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">导入MySQL数据库中的表数据到Hive中：</span><br><span class="line"></span><br><span class="line">// 普通导入：数据存储在默认的default hive库中，表名就是对应的mysql的表名：</span><br><span class="line">// 通过对比发现, sqoop 是默认导入到 hdfs 的, 导入到hdfs时, 不用加额外的参数</span><br><span class="line"></span><br><span class="line">sqoop import   \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/mysql  \</span></span><br><span class="line"><span class="comment">--username root  \</span></span><br><span class="line"><span class="comment">--password 123  \</span></span><br><span class="line"><span class="comment">--table help_keyword  \</span></span><br><span class="line"><span class="comment">--hive-import \</span></span><br><span class="line">-m 1</span><br><span class="line"></span><br><span class="line">hadoop fs -cat /user/myhive/warehouse/help_keyword/part-m-00000     // 查看数据</span><br><span class="line">当然也可以在 hive 中查看数据</span><br><span class="line"></span><br><span class="line">// 指定行分隔符和列分隔符，指定hive-import，指定覆盖导入，指定自动创建hive表，指定表名，指定删除中间结果数据目录 </span><br><span class="line"></span><br><span class="line">Database does not exist: mydb_test ???</span><br><span class="line">注意: 要先创建 database mydb_test</span><br><span class="line">问题: 为什么导出后, 变成了4个块</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sqoop import  \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/mysql  \</span></span><br><span class="line"><span class="comment">--username root  \</span></span><br><span class="line"><span class="comment">--password 123  \</span></span><br><span class="line"><span class="comment">--table help_keyword  \</span></span><br><span class="line"><span class="comment">--fields-terminated-by "\t"  \</span></span><br><span class="line"><span class="comment">--lines-terminated-by "\n"  \</span></span><br><span class="line"><span class="comment">--hive-import  \</span></span><br><span class="line"><span class="comment">--hive-overwrite  \</span></span><br><span class="line"><span class="comment">--create-hive-table  \</span></span><br><span class="line"><span class="comment">--delete-target-dir \</span></span><br><span class="line"><span class="comment">--hive-database mydb_test \</span></span><br><span class="line"><span class="comment">--hive-table new_help_keyword</span></span><br><span class="line"></span><br><span class="line">另外一种写法：</span><br><span class="line">sqoop import  \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/mysql  \</span></span><br><span class="line"><span class="comment">--username root  \</span></span><br><span class="line"><span class="comment">--password 123  \</span></span><br><span class="line"><span class="comment">--table help_keyword  \</span></span><br><span class="line"><span class="comment">--fields-terminated-by "\t"  \</span></span><br><span class="line"><span class="comment">--lines-terminated-by "\n"  \</span></span><br><span class="line"><span class="comment">--hive-import  \</span></span><br><span class="line"><span class="comment">--hive-overwrite  \</span></span><br><span class="line"><span class="comment">--create-hive-table  \</span></span><br><span class="line"><span class="comment">--hive-table mydb_test.new_help_keyword1 \</span></span><br><span class="line"><span class="comment">--delete-target-dir</span></span><br></pre></td></tr></table></figure><h4 id="5-4-增量导入到-HDFS"><a href="#5-4-增量导入到-HDFS" class="headerlink" title="5.4 增量导入到 HDFS"></a>5.4 增量导入到 HDFS</h4><p>—-应该也是可以导入到 Hive</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">第三部分：增量导入</span><br><span class="line"></span><br><span class="line">Incremental import arguments:</span><br><span class="line">   <span class="comment">--check-column &lt;column&gt;        Source column to check for incremental</span></span><br><span class="line">                                  <span class="keyword">change</span>  原始的列作为增长改变的</span><br><span class="line">   <span class="comment">--incremental &lt;import-type&gt;    Define an incremental import of type</span></span><br><span class="line">                                  <span class="string">'append'</span> <span class="keyword">or</span> <span class="string">'lastmodified'</span></span><br><span class="line">   <span class="comment">--last-value &lt;value&gt;           Last imported value in the incremental</span></span><br><span class="line">                                  <span class="keyword">check</span> <span class="keyword">column</span></span><br><span class="line"></span><br><span class="line">比较使用于自增长主键!! </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 增量导入</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sqoop <span class="keyword">import</span>   \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/mysql   \</span></span><br><span class="line"><span class="comment">--username root  \</span></span><br><span class="line"><span class="comment">--password 123   \</span></span><br><span class="line"><span class="comment">--table help_keyword  \</span></span><br><span class="line"><span class="comment">--target-dir /user/ap/my_help_keyword_import1  \</span></span><br><span class="line"><span class="comment">--incremental  append  \</span></span><br><span class="line"><span class="comment">--check-column  help_keyword_id \</span></span><br><span class="line"><span class="comment">--last-value 500  \</span></span><br><span class="line">-m <span class="number">3</span></span><br></pre></td></tr></table></figure><h4 id="5-5-第四部分：-导入数据到HBase"><a href="#5-5-第四部分：-导入数据到HBase" class="headerlink" title="5.5 第四部分： 导入数据到HBase"></a>5.5 第四部分： 导入数据到HBase</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">第四部分： 导入数据到HBase</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">导入MySQL数据库中的表数据到HBase中：</span><br><span class="line"></span><br><span class="line">sqoop import  \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/mysql  \</span></span><br><span class="line"><span class="comment">--username root  \</span></span><br><span class="line"><span class="comment">--password 123  \</span></span><br><span class="line"><span class="comment">--table help_keyword  \</span></span><br><span class="line"><span class="comment">--hbase-table new_help_keyword  \</span></span><br><span class="line"><span class="comment">--column-family person  \</span></span><br><span class="line"><span class="comment">--hbase-row-key help_keyword_id</span></span><br></pre></td></tr></table></figure><h4 id="5-6-导出"><a href="#5-6-导出" class="headerlink" title="5.6 导出"></a>5.6 导出</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">第五部分：导出：</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：导出的RDBMS的表必须自己预先创建，不会自动创建</span><br><span class="line"><span class="comment">------mysql-----先在 mysql 中创建库</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> sqoopdb <span class="keyword">default</span> <span class="built_in">character</span> <span class="keyword">set</span> utf8 <span class="keyword">COLLATE</span> utf8_general_ci; </span><br><span class="line"><span class="keyword">use</span> sqoopdb;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sqoopstudent ( </span><br><span class="line">   <span class="keyword">id</span> <span class="built_in">INT</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> PRIMARY <span class="keyword">KEY</span>, </span><br><span class="line">   <span class="keyword">name</span> <span class="built_in">VARCHAR</span>(<span class="number">20</span>), </span><br><span class="line">   sex <span class="built_in">VARCHAR</span>(<span class="number">20</span>),</span><br><span class="line">   age <span class="built_in">INT</span>,</span><br><span class="line">department <span class="built_in">VARCHAR</span>(<span class="number">20</span>)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 导出HDFS数据到MySQL：</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sqoop export \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/sqoopdb  \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password 123 \</span></span><br><span class="line"><span class="comment">--table sqoopstudent \</span></span><br><span class="line"><span class="comment">--export-dir /sqoopdata \</span></span><br><span class="line"><span class="comment">--fields-terminated-by ','</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 导出hive数据到MySQL：</span><br><span class="line">sqoop export \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://cs2:3306/sqoopdb \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password 123 \</span></span><br><span class="line"><span class="comment">--table uv_info \</span></span><br><span class="line"><span class="comment">--export-dir /user/hive/warehouse/uv/dt=2011-08-03 \</span></span><br><span class="line"><span class="comment">--input-fields-terminated-by '\t'</span></span><br></pre></td></tr></table></figure><h4 id="5-6-导出HBase数据到MySQL"><a href="#5-6-导出HBase数据到MySQL" class="headerlink" title="5.6 导出HBase数据到MySQL"></a>5.6 导出HBase数据到MySQL</h4><p>很遗憾，现在还没有直接的命令将 HBase 的数据导出到 MySQL</p><p>一般采用如下 3 种方法:</p><p>1、将 HBase 数据，扁平化成 HDFS 文件，然后再由 sqoop 导入</p><p>2、将 HBase 数据导入 Hive 表中，然后再导入 MySQL</p><p>3、直接使用 HBase 的 Java API 读取表数据，直接向 MySQL 导入，不需要使用 sqoop</p><h4 id="5-7-sqoop-支持-amp-不支持"><a href="#5-7-sqoop-支持-amp-不支持" class="headerlink" title="5.7 sqoop 支持 &amp; 不支持"></a>5.7 sqoop 支持 &amp; 不支持</h4><p>hdfs to mysql   可以直接使用。！！<br>hive to mysql   就是hdfs to mysql<br>hbase to mysql   很不幸，不支持。！</p><p>怎么实现？</p><pre><code>1、扁平化到HDFS2、hbase和hive做整合3、自己编写程序去实现从HBASE当中读取数据，然后写入到mYsql</code></pre><h2 id="6-Sqoop-Job-作业"><a href="#6-Sqoop-Job-作业" class="headerlink" title="6.Sqoop Job 作业"></a>6.Sqoop Job 作业</h2><p><strong>查看帮助:</strong> </p><p><code>sqoop help job</code></p><h4 id="6-0-mysql-创建数据库-spider-和-表-lagou"><a href="#6-0-mysql-创建数据库-spider-和-表-lagou" class="headerlink" title="6.0 mysql 创建数据库 spider 和 表 lagou"></a>6.0 mysql 创建数据库 spider 和 表 lagou</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 这样可以直接导入 sql 数据</span><br><span class="line">source /home/ap/temp/lagou.sql</span><br></pre></td></tr></table></figure><h4 id="6-1-创建作业-Job-–create"><a href="#6-1-创建作业-Job-–create" class="headerlink" title="6.1 创建作业 Job (–create)"></a>6.1 创建作业 Job (–create)</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意: mysql 中此时已经存在 spider 库, 库中有 lagou 表</span></span><br><span class="line"><span class="comment"># ps: 导入表</span></span><br><span class="line"></span><br><span class="line">sqoop job --create my_sqoop_job \</span><br><span class="line">-- import \</span><br><span class="line">--connect jdbc:mysql://cs2:3306/spider \</span><br><span class="line">--username root \</span><br><span class="line">--password 123 \</span><br><span class="line">--table lagou</span><br></pre></td></tr></table></figure><h4 id="6-2-查看作业-Job-–list"><a href="#6-2-查看作业-Job-–list" class="headerlink" title="6.2 查看作业 Job (–list)"></a>6.2 查看作业 Job (–list)</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sqoop job --list</span><br><span class="line">----------------------</span><br><span class="line">Available <span class="built_in">jobs</span>:</span><br><span class="line">  my_sqoop_job</span><br></pre></td></tr></table></figure><h4 id="6-3-查看作业详细信息-–show"><a href="#6-3-查看作业详细信息-–show" class="headerlink" title="6.3 查看作业详细信息 (–show)"></a>6.3 查看作业详细信息 (–show)</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop job --show my_sqoop_job</span><br></pre></td></tr></table></figure><h4 id="6-4-执行作业"><a href="#6-4-执行作业" class="headerlink" title="6.4 执行作业"></a>6.4 执行作业</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop job --<span class="built_in">exec</span> my_sqoop_job</span><br></pre></td></tr></table></figure><h4 id="6-5-删除作业"><a href="#6-5-删除作业" class="headerlink" title="6.5 删除作业"></a>6.5 删除作业</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop job --delete my_sqoop_job</span><br></pre></td></tr></table></figure><h2 id="7-Sqoop-导入导出原理"><a href="#7-Sqoop-导入导出原理" class="headerlink" title="7.Sqoop 导入导出原理"></a>7.Sqoop 导入导出原理</h2><h4 id="7-1-导入原理"><a href="#7-1-导入原理" class="headerlink" title="7.1 导入原理"></a>7.1 导入原理</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-29-092721.png" alt="image-20180629172721667"></p><p>1、第一步，Sqoop 会通过 JDBC 来获取所需要的数据库元数据，例如，导入表的列名，数据 类型等。</p><p>2、第二步，这些数据库的数据类型(varchar, number 等)会被映射成 Java 的数据类型(String, int 等)，根据这些信息，Sqoop 会生成一个与表名同名的类用来完成序列化工作，保存表中的 每一行记录。</p><p>3、第三步，Sqoop 启动 MapReducer 作业</p><p>4、第四步，启动的作业在 input 的过程中，会通过 JDBC 读取数据表中的内容，这时，会使 用 Sqoop 生成的类进行反序列化操作</p><p>5、第五步，最后将这些记录写到 HDFS 中，在写入到 HDFS 的过程中，同样会使用 Sqoop 生 成的类进行反序列化</p><h4 id="7-2-导出原理"><a href="#7-2-导出原理" class="headerlink" title="7.2 导出原理"></a>7.2 导出原理</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-29-092707.png" alt="image-20180629172706382"></p><p>1、 第一步，sqoop 依然会通过 JDBC 访问关系型数据库，得到需要导出数据的元数据信息</p><p>2、 第二步，根据获取到的元数据的信息，sqoop 生成一个 Java 类，用来进行数据的传输载</p><p>  体。该类必须实现序列化和反序列化</p><p>3、 第三步，启动 mapreduce 作业</p><p>4、 第四步，sqoop 利用生成的这个 java 类，并行的从 hdfs 中读取数据</p><p>5、 第五步，每个 map 作业都会根据读取到的导出表的元数据信息和读取到的数据，生成一 批 insert 语句，然后多个 map 作业会并行的向数据库 mysql 中插入数据</p><p>所以，数据是从 hdfs 中并行的进行读取，也是并行的进入写入，那并行的读取是依赖 hdfs 的性能，而并行的写入到 mysql 中，那就要依赖于 mysql 的写入性能嘞。</p><p><a href="http://sqoop.apache.org/docs/" target="_blank" rel="noopener"><strong>官网:</strong></a> </p><p><a href="https://app.yinxiang.com/shard/s37/nl/7399077/8530b06a-e01c-42fb-a833-7f14c918e0da/" target="_blank" rel="noopener"><strong>参考PDF:</strong></a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-作用&quot;&gt;&lt;a href=&quot;#1-作用&quot; class=&quot;headerlink&quot; title=&quot;1. 作用&quot;&gt;&lt;/a&gt;1. 作用&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-28-
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="Sqoop" scheme="https://airpoet.github.io/categories/Hadoop/Sqoop/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="Sqoop" scheme="https://airpoet.github.io/tags/Sqoop/"/>
    
  </entry>
  
  <entry>
    <title>i-HBase-1</title>
    <link href="https://airpoet.github.io/2018/06/26/Hadoop/5-HBase/i-HBase-1/"/>
    <id>https://airpoet.github.io/2018/06/26/Hadoop/5-HBase/i-HBase-1/</id>
    <published>2018-06-26T01:47:42.704Z</published>
    <updated>2018-06-29T16:51:50.191Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Hbase概念"><a href="#Hbase概念" class="headerlink" title="Hbase概念"></a>Hbase概念</h2><ul><li><p>hbase &amp; hdfs 关系图</p><p><img src="https://img.mubu.com/document_image/40f0d2a9-20f1-4f73-84d9-e31b52685aad-584150.jpg" alt="img"></p></li><li><p>hadoop数据库，分布式可伸缩大型数据存储。</p></li><li><p>用户对随机、实时读写数据。</p></li><li><p>十亿行 x 百万列。    </p></li><li><p>版本化、非关系型数据库。</p></li></ul><h2 id="Feature"><a href="#Feature" class="headerlink" title="Feature"></a>Feature</h2><ul><li>Linear and modular scalability.<br> //线性模块化扩展方式。</li><li>Strictly consistent reads and writes.<br> //严格一致性读写</li><li>Automatic and configurable sharding of tables<br>//自动可配置表切割</li><li>Automatic failover support between RegionServers.<br>//区域服务器之间自动容在</li><li>Convenient base classes for backing Hadoop MapReduce jobs with Apache HBase tables.        </li><li>Easy to use Java API for client access.<br>//java API</li><li>Block cache and Bloom Filters for real-time queries<br>//块缓存和布隆过滤器用于实时查询</li><li>Query predicate push down via server side Filters<br>//通过服务器端过滤器实现查询预测</li><li>Thrift gateway and a REST-ful Web service that supports XML, Protobuf, and binary data encoding options    </li><li>Extensible jruby-based (JIRB) shell                    </li><li>Support for exporting metrics via the Hadoop metrics subsystem to files or Ganglia; or via JMX<br> //可视化</li><li>面向列数据库。</li></ul><h2 id="HBase要点"><a href="#HBase要点" class="headerlink" title="HBase要点"></a>HBase要点</h2><ul><li>1) 它介于 NoSQL 和 RDBMS 之间，仅能通过主键(rowkey)和主键的 range 来检索数据</li><li>2) HBase 查询数据功能很简单，不支持 join 等复杂操作</li><li>3) 不支持复杂的事务，只支持行级事务(可通过 hive 支持来实现多表 join 等复杂操作)。</li><li>4) HBase 中支持的数据类型:byte<a href="底层所有数据的存储都是字节数组"></a></li><li>5) 主要用来存储结构化和半结构化的松散数据。</li></ul><h2 id="HBase-表特点"><a href="#HBase-表特点" class="headerlink" title="HBase 表特点"></a>HBase 表特点</h2><ul><li>大:一个表可以有上十亿行，上百万列</li><li>面向列:面向列(族)的存储和权限控制，列(簇)独立检索。</li><li>稀疏:对于为空(null)的列，并不占用存储空间，因此，表可以设计的非常稀疏。</li><li>无模式:每行都有一个可排序的主键和任意多的列，列可以根据需要动态的增加，同一 张表中不同的行可以有截然不同的列</li></ul><h2 id="HBase-存储机制"><a href="#HBase-存储机制" class="headerlink" title="HBase 存储机制"></a>HBase 存储机制</h2><ul><li>面向列存储，table是按row排序。</li><li>底层是 跳表 &amp; 布隆过滤器</li></ul><h2 id="HBase-定位机制-三级坐标"><a href="#HBase-定位机制-三级坐标" class="headerlink" title="HBase 定位机制(三级坐标)"></a>HBase 定位机制(三级坐标)</h2><ul><li>行<br>rowkey</li><li>列族 &amp; 列<br>column family &amp; column</li><li>时间戳<br>timestamp版本</li></ul><h2 id="表-行-列-列族的关系"><a href="#表-行-列-列族的关系" class="headerlink" title="表 行 列 列族的关系"></a>表 行 列 列族的关系</h2><ul><li><p>官网的关系图如下</p><p><img src="https://img.mubu.com/document_image/d4572c94-9da5-463f-a721-0c4dc42b87db-584150.jpg" alt="img"></p></li><li><p>其它版本</p><p><img src="https://img.mubu.com/document_image/feb7a6a8-6dfb-463a-8d57-181502d3d0e9-584150.jpg" alt="img"></p></li><li><p>Table is a collection of rows.<br>表是行的集合</p></li><li><p>Row is a collection of column families<br>行是列族的集合</p></li><li><p>Column family is a collection of columns.<br>列族是列的集合</p></li><li><p>Column is a collection of key value pairs.<br>列是键值对的集合(列其实就是 key)</p></li></ul><h2 id="RowKey-Column-Family-TimeStamp-Cell-概念解释"><a href="#RowKey-Column-Family-TimeStamp-Cell-概念解释" class="headerlink" title="RowKey, Column Family, TimeStamp, Cell 概念解释"></a>RowKey, Column Family, TimeStamp, Cell 概念解释</h2><ul><li>RowKey 是用来检索记录的主键<ul><li>rowkey 行键可以是任意字符串(最大长度是 64KB，实际应用中长度一般为 10-100bytes)，最好是 16。<ul><li>每一个物理文件中都会存一个 rk:TODO ??</li></ul></li><li>在 HBase 内部，rowkey 保存为字节数组。HBase 会对表中的数据按照 rowkey 排序 (字典顺序)</li><li>设计 key 时，要充分排序存储这 个特性，将经常一起读取的行存储放到一起。(位置相关性)</li><li>注意：字典序对 int 排序的结果是1,10,100,11,12,13,14,15,16,17,18,19,2,20,21,…9,91,92,93,94,95,96,97,98,99。要保持整形的自然序，行键必须用 0 作左填充。</li></ul></li><li>Column Family 列族是表的 Schema 的一部分(而列不是)，必须在使用表之前定义好，而且定义好了之后就不能更改。<ul><li>HBase 表中的每个列，都归属与某个列簇。</li><li>列簇越多，在取一行数据时所要参与 IO、搜寻的文件就越多，所以，如果没有必要，不要 设置太多的列簇（最好就一个列簇）</li></ul></li><li>TimeStamp<ul><li>HBase 中通过 rowkey 和 columns 确定的为一个存储单元称为 cell。</li><li>每个 cell 都保存着同一份 数据的多个版本。</li><li>版本通过时间戳来索引。</li><li>每个 cell 中，不同版本的数据按照时间倒序排序，即最新的数据排在最前面。</li><li>hbase 提供了两种数据版 本回收方式:<ul><li>保存数据的最后 n 个版本</li><li>保存最近一段时间内的版本(设置数据的生命周期 TTL)。</li></ul></li></ul></li><li>Cell 单元格<ul><li>由{rowkey, column( =<family> + <column>), version} 唯一确定的单元。</column></family></li><li>Cell 中的数据是没有类型的，全部是字节码形式存贮。</li></ul></li></ul><h2 id="HBase-的HA搭建"><a href="#HBase-的HA搭建" class="headerlink" title="HBase 的HA搭建"></a>HBase 的HA搭建</h2><ul><li><p>选择安装的主机cs1~cs4, 安装jdk, 解压hbase 包, 配置环境变量</p></li><li><p>验证安装是否成功<br>$&gt;hbase version</p></li><li><p>配置 hbase 完全分布式<br>[hbase/conf/<a href="http://hbase-env.sh" target="_blank" rel="noopener">hbase-env.sh</a>]<br>export JAVA_HOME=/usr/local/jdk1.8.0_73<br>export HBASE_MANAGES_ZK=false</p><p>[hbse-site.xml]</p><!-- 使用完全分布式 --><property><br>    <name>hbase.cluster.distributed</name><br>    <value>true</value><br></property><br><!-- 指定hbase数据在hdfs上的存放路径,存储路径由 namenode 来统一管理 --><br><property><br>    <name>hbase.rootdir</name><br>    <value><a href="hdfs://mycluster/hbase" target="_blank" rel="noopener">hdfs://mycluster/hbase</a></value><br></property><br><!-- 配置zk地址 --><br><property><br>    <name>hbase.zookeeper.quorum</name><br>    <value>cs1:2181,cs2:2181,cs3:2181</value><br></property><br><!-- zk的本地目录 --><br><property><br>    <name>hbase.zookeeper.property.dataDir</name><br>    <value>/home/ap/zookeeper</value><br></property> </li><li><p>配置 regonservers<br>[hbase/conf/regionservers]<br>cs2<br>cs3<br>cs4<br>cs5</p></li><li><p>修改 backup-masters（自行创建），指定备用的主HMaster</p><h2 id="vi-backup-masters"><a href="#vi-backup-masters" class="headerlink" title="vi backup-masters"></a>vi backup-masters</h2><p>cs6</p></li><li><p>最重要一步:</p><ul><li>傻逼版:<ul><li>要把 hadoop 的 hdfs-site.xml 和 core-site.xml 放到 hbase/conf 下, 这样hbase 才能通过命名空间mycluster 找到当前可用 namenode, 再通过 namenode 分配指定 hbase 在 HDFS上的存储路径<br>cp <sub>/apps/hadoop/etc/hadoop/core-site.xml </sub>/apps/hbase/conf/<br>cp <sub>/apps/hadoop/etc/hadoop/hdfs-site.xml </sub>/apps/hbase/conf/</li></ul></li><li>高级版:<ul><li>在hbase-env.sh文件添加hadoop配置文件目录到HBASE_CLASSPATH环境变量并分发.<br>[/soft/hbase/conf/<a href="http://hbase-env.sh" target="_blank" rel="noopener">hbase-env.sh</a>]<br>export HBASE_CLASSPATH=$HBASE_CLASSPATH:/soft/hadoop/etc/hadoop </li><li>在hbase/conf/目录下创建到hadoop的hdfs-site.xml符号连接。<br>$&gt;ln -s /soft/hadoop/etc/hadoop/hdfs-site.xml /soft/hbase/conf/hdfs-site.xml</li></ul></li></ul></li><li><p>启动 hbase 集群<br>$&gt;<a href="http://start-hbase.sh" target="_blank" rel="noopener">start-hbase.sh</a></p></li><li><p>登录 hbase 的 webUI</p></li></ul><p>  <a href="http://cs1:16010/" target="_blank" rel="noopener">http://cs1:16010/</a></p><ul><li>注意, 页面上显示的版本, 是~/apps/hbase/lib中自己存储的 jar 包的版本</li></ul><h2 id="启动-停止-hbase-进程"><a href="#启动-停止-hbase-进程" class="headerlink" title="启动/停止 hbase 进程"></a>启动/停止 hbase 进程</h2><ul><li><a href="http://start-hbase.sh" target="_blank" rel="noopener">start-hbase.sh</a></li></ul><p>  等价于</p><ul><li>hbase-daemon.sh start master</li><li>hbase-daemons.sh start regionserver</li><li>注意: 在哪个节点启动 <a href="http://start-hbase.sh" target="_blank" rel="noopener">start-hbase.sh</a> , 就在哪里启动 HMaster 进程, 如果</li></ul><ul><li><a href="http://stop-hbase.sh" target="_blank" rel="noopener">stop-hbase.sh</a><br>停止 hbase</li></ul><h2 id="HBase-shell-操作"><a href="#HBase-shell-操作" class="headerlink" title="HBase shell 操作"></a>HBase shell 操作</h2><h4 id="登录shell终端"><a href="#登录shell终端" class="headerlink" title="登录shell终端."></a>登录shell终端.</h4><ul><li>$hbase&gt; hbase shell                           </li></ul><h4 id="help-相关"><a href="#help-相关" class="headerlink" title="help 相关"></a>help 相关</h4><ul><li>$hbase&gt; help<br>查看所有帮助</li><li>$hbase&gt; table_help<br>关于表操作的另外一种方式的帮助文档</li><li>$hbase&gt; help “dml”<br>获取一组命令的提示</li><li>$hbase&gt;help  ‘list_namespace’<br>查看特定的命令帮助</li></ul><h4 id="desc-查看描述信息"><a href="#desc-查看描述信息" class="headerlink" title="desc 查看描述信息"></a>desc 查看描述信息</h4><ul><li>desc ‘t1’ : 下面这些在建表的时候都可以指定<br>{NAME =&gt; ‘f1’, BLOOMFILTER =&gt; ‘ROW’, VERSIONS =&gt; ‘1’, IN_MEMORY =&gt; ‘false’, KEEP_DELETED_CELLS =&gt;<br> ‘FALSE’, DATA_BLOCK_ENCODING =&gt; ‘NONE’, TTL =&gt; ‘FOREVER’, COMPRESSION =&gt; ‘NONE’, MIN_VERSIONS =&gt;<br> ‘0’, BLOCKCACHE =&gt; ‘true’, BLOCKSIZE =&gt; ‘65536’, REPLICATION_SCOPE =&gt; ‘0’}<br>1 row(s) in 0.1780 seconds</li></ul><h4 id="list-相关"><a href="#list-相关" class="headerlink" title="list 相关"></a>list 相关</h4><ul><li>$hbase&gt; list<br>列出所有非系统表</li><li>$hbase&gt;list_namespace<br>列出名字空间(数据库)</li><li>$hbase&gt;list_namespace_tables ‘defalut’<br>列出名字空间的所以表</li></ul><h4 id="create-相关"><a href="#create-相关" class="headerlink" title="create 相关"></a>create 相关</h4><ul><li>$hbase&gt;create_namespace ‘ns1’<br>创建名字空间</li><li>$hbase&gt;create ‘ns1:t1’,’f1’<br>创建表,指定空间下,列族<br>注意: 如果 t1已经创建了, 就不能通过这个方式来添加 列族</li><li>create ‘t1’,’f1’</li><li>create ‘user_info’,{NAME=&gt;’base_info’,VERSIONS=&gt;3 },{NAME=&gt;’extra_info’,VERSIONS=&gt;1 }<br>同时创建多个 列族</li></ul><h4 id="exist-相关"><a href="#exist-相关" class="headerlink" title="exist 相关"></a>exist 相关</h4><ul><li>exists ‘t1’<br>查看表是否存在</li></ul><h4 id="put-相关"><a href="#put-相关" class="headerlink" title="put 相关"></a>put 相关</h4><ul><li>注意: 如果往 同表 &amp; 同rowkey &amp; 同样的列 中插入 value, 会默认展示最近的一个时间戳的信息, 如果 get 的回收指定 VERSIONS数量, 则会展示此数量的版本信息</li><li>向 user 表中插入信息，row key 为 rk0001，列簇 info 中添加 name 列标示符，值为 zhangsan<ul><li>put ‘user’, ‘rk0001’, ‘info:name’, ‘zhangsan’, 1482077777777<br>手动指定时间戳<br>时间戳是可以自己指定的，如若不指定，则会自动获取系统的当前时间的时间戳</li></ul></li><li>向 user 表中插入信息，row key 为 rk0001，列簇 info 中添加 gender 列标示符，值为 female<ul><li>put ‘user’, ‘rk0001’, ‘info:gender’, ‘female’</li></ul></li><li>向 user 表中插入信息，row key 为 rk0001，列簇 info 中添加 age 列标示符，值为 20<ul><li>put ‘user’, ‘rk0001’, ‘info:age’, 20</li></ul></li><li>向 user 表中插入信息，row key 为 rk0001，列簇 data 中添加 pic 列标示符，值为 picture<ul><li>put ‘user’, ‘rk0001’, ‘data:pic’, ‘picture’</li></ul></li></ul><h4 id="get-相关"><a href="#get-相关" class="headerlink" title="get 相关"></a>get 相关</h4><ul><li>获取 user 表中 row key 为 rk0001 的所有信息              </li></ul><p>  注意: ‘表名’, ‘ rowkey’</p><ul><li>get ‘user’, ‘rk0001’</li></ul><ul><li>获取 user 表中 row key 为 rk0001，info 列簇的所有信息</li></ul><p>  注意: ‘表名’, ‘ rowkey’, ‘列族名’</p><ul><li>get ‘user’, ‘rk0001’, ‘info’</li></ul><ul><li>获取 user 表中 row key 为 rk0001，info 列簇的 name、age 列标示符的信息</li></ul><p>  注意: ‘表名’, ‘rowkey’, ‘列族名’:’列名’, ‘列族名’:’列名’</p><ul><li>get ‘user’, ‘rk0001’, ‘info:name’, ‘info:age’</li></ul><ul><li>获取 user 表中 row key 为 rk0001，info、data 列簇的信息</li></ul><p>  注意: ‘表名’, ‘rowkey’, ‘列族名’, ‘列族名’</p><ul><li>get ‘user’, ‘rk0001’, ‘info’, ‘data’</li><li>get ‘user’, ‘rk0001’, {COLUMN =&gt; [‘info’, ‘data’]}</li><li>get ‘user’, ‘rk0001’, {COLUMN =&gt; [‘info:name’, ‘data:pic’]}</li></ul><ul><li>获取 user 表中 row key 为 rk0001，列簇为 info，版本号最新 5 个的信息</li></ul><p>  注意: ‘表名’, ‘rowkey’, {COLUMN =&gt;’列族名’, 存储版本数=&gt; 数量}</p><ul><li>get ‘user’, ‘rk0001’, {COLUMN =&gt; ‘info’, VERSIONS =&gt; 2}</li><li>get ‘user’, ‘rk0001’, {COLUMN =&gt; ‘info:name’, VERSIONS =&gt; 5}</li><li>get ‘user’, ‘rk0001’, {COLUMN =&gt; ‘info:name’, VERSIONS =&gt; 5, TIMERANGE =&gt; [1392368783980, 1392380169184]}</li></ul><ul><li><p>获取 user 表中 row key 为 rk0001，列标示符中含有 a 的信息</p><ul><li>get ‘people’, ‘rk0001’, {FILTER =&gt; “(QualifierFilter(=,’substring:a’))”}</li></ul></li><li><p>获取 国籍 为中国的用户信息</p></li></ul><p>  注意:   ‘表名’,    ‘rowkey’,      ‘列族名’:’列名’,  ‘value’</p><ul><li>put ‘user’, ‘rk0002’, ‘info:name’, ‘fanbingbing’</li><li>put ‘user’, ‘rk0002’, ‘info:gender’, ‘female’</li><li>put ‘user’, ‘rk0002’, ‘info:nationality’, ‘中国’</li><li>get ‘user’, ‘rk0002’, {FILTER =&gt; “ValueFilter(=, ‘binary:中国’)”}</li></ul><ul><li>获取2个不同列中的指定字段</li></ul><p>  注意:   ‘表名’,    ‘rowkey’, {COLUMN  =&gt; [‘列族名’:’列名’,  ‘列族名’:’列名’]}</p><ul><li>get ‘user’, ‘rk0002’, {COLUMN =&gt; [‘info:name’, ‘data:pic’]}</li></ul><h4 id="scan-相关"><a href="#scan-相关" class="headerlink" title="scan 相关"></a>scan 相关</h4><ul><li><p>扫描元数据</p><ul><li><p>scan ‘hbase : meta’<br> t9,,1529936935368.f1a900954c069f0319195f16043c8e1a column=info:regioninfo, timestamp=1529936935933, value={ENCODED =&gt; f1a900954c069f0319195f16043c8e1a, NAME =&gt; ‘t9,,1529936935368.f1a900954c069f031919<br> .                                                  5f16043c8e1a.’, STARTKEY =&gt; ‘’, ENDKEY =&gt; ‘’}<br> t9,,1529936935368.f1a900954c069f0319195f16043c8e1a column=info:seqnumDuringOpen, timestamp=1529936935933, value=\x00\x00\x00\x00\x00\x00\x00\x02<br> .<br> t9,,1529936935368.f1a900954c069f0319195f16043c8e1a column=info:server, timestamp=1529936935933, value=cs5:16020<br> .<br> t9,,1529936935368.f1a900954c069f0319195f16043c8e1a column=info:serverstartcode, timestamp=1529936935933, value=1529923622913</p></li><li><p>$hbase&gt;split ‘ns1:t1’<br>//切割表</p></li><li><p>$hbase&gt;split ‘’<br> //切割区域, 见下面</p></li><li><p>切割之前</p><p><img src="https://img.mubu.com/document_image/13cfae11-e0c1-46fb-be2c-799578d701c9-584150.jpg" alt="img"></p></li><li><p>切割之后</p><p><img src="https://img.mubu.com/document_image/da7a0e5d-7157-458c-abfc-e17a2c81796f-584150.jpg" alt="img"></p></li><li><p>UI界面 table regions</p><p><img src="https://img.mubu.com/document_image/9a336ce6-5915-44bc-a3a6-f8ad86de7611-584150.jpg" alt="img"></p></li><li><p>继续进行 region 切割</p><ul><li>scan ‘hbase:meta’</li><li>找到切割点 STARTKEY =&gt; ‘row000550’ 的 那行的 NAME =&gt; 后面的值</li><li>从这个值开始切割到指定值<br>split ‘t9,row000551,1529938745618.90ed2fb6b7744ea5f0a7ff613d05aada.’,’row000888’</li></ul></li></ul></li><li><p>扫描表中所有的信息</p><ul><li>$hbase&gt;scan ‘ns1:t1’</li></ul></li><li><p>查询 user 表中列簇为 info 的信息</p><ul><li>hbase&gt; scan ‘hbase:meta’<br>扫描权标</li><li>$hbase&gt; scan ‘hbase:meta’, {COLUMNS =&gt; ‘info’}<br>注意: COLUMNS要大写</li><li>$hbase&gt; scan ‘ns1:t1’, {COLUMNS =&gt; [‘c1’, ‘c2’], LIMIT =&gt; 10, STARTROW =&gt; ‘xyz’}</li><li>scan ‘user’, {COLUMNS =&gt; ‘info’, RAW =&gt; true, VERSIONS =&gt; 5}</li><li>scan ‘persion’, {COLUMNS =&gt; ‘info’, RAW =&gt; true, VERSIONS =&gt; 3}<br>Scan 时可以设置是否开启 Raw 模式，开启 Raw 模式会返回包括已添加删除标记但是未 实际删除的数据。</li></ul></li><li><p>查询 user 表中列簇为 info 和 data 的信息</p><ul><li>scan ‘user’, {COLUMNS =&gt; [‘info’, ‘data’]}</li><li>scan ‘user’, {COLUMNS =&gt; [‘info:name’, ‘data:pic’]}</li></ul></li><li><p>查询 user 表中列簇为 info、列标示符为 name 的信息</p><ul><li>scan ‘user’, {COLUMNS =&gt; ‘info:name’}</li></ul></li><li><p>查询 user 表中列簇为 info、列标示符为 name 的信息,并且版本最新的 5 个</p><ul><li>scan ‘user’, {COLUMNS =&gt; ‘info:name’, VERSIONS =&gt; 5}</li></ul></li><li><p>查询 user 表中列簇为 info 和 data 且列标示符中含有 a 字符的信息</p><ul><li>scan ‘user’, {COLUMNS =&gt; [‘info’, ‘data’], FILTER =&gt; “(QualifierFilter(=,’substring:a’))”}</li></ul></li><li><p>查询 user 表中列簇为 info，rk 范围是[rk0001, rk0003)的数据</p><ul><li>scan ‘people’, {COLUMNS =&gt; ‘info’, STARTROW =&gt; ‘rk0001’, ENDROW =&gt; ‘rk0003’}</li></ul></li><li><p>查询 user 表中 row key 以 rk 字符开头的</p><ul><li>scan ‘user’,{FILTER=&gt;”PrefixFilter(‘rk’)”}</li></ul></li><li><p>查询 user 表中指定范围的数据</p><ul><li>scan ‘user’, {TIMERANGE =&gt; [1392368783980, 1392380169184]}</li></ul></li></ul><h4 id="delete-truncate-相关-删除-清空数据"><a href="#delete-truncate-相关-删除-清空数据" class="headerlink" title="delete/truncate 相关 (删除,清空数据)"></a>delete/truncate 相关 (删除,清空数据)</h4><ul><li>注意: 删除列族, 在 alter 中</li><li>删除 user 表 row key 为 rk0001，列标示符为 info:name 的字段数据<ul><li>delete ‘people’, ‘rk0001’, ‘info:name’</li></ul></li><li>删除 user 表 row key 为 rk0001，列标示符为 info:name，timestamp 为 1392383705316 的数据<ul><li>delete ‘user’, ‘rk0001’, ‘info:name’, 1392383705316</li></ul></li><li>清空 user 表中的数据<ul><li>truncate ‘people’<br>目前版本会先 disable , 再 truncate, 最后再 enable</li></ul></li></ul><h4 id="alter-相关"><a href="#alter-相关" class="headerlink" title="alter 相关"></a>alter 相关</h4><ul><li>注意,在修改列族 时</li></ul><p>  hbase&gt; alter ‘t1’, NAME =&gt; ‘f1’, VERSIONS =&gt; 5</p><ul><li>VERSIONS 是指能存多少个版本, 如果不指定的话, 退出 hbase 的时候, 默认只会留最新的一个版本</li><li>如果存在, 就是修改</li><li>不存在, 就是增加</li></ul><ul><li><p>添加两个列簇 f1 和 f2</p><ul><li>简写: alter ‘t1’, ‘f2’</li><li>alter ‘people’, NAME =&gt; ‘f1’</li><li>alter ‘user’, NAME =&gt; ‘f2’</li></ul></li><li><p>停用/启用 表</p><ul><li>enable ‘t9’<br>启用</li><li>is_enabled ‘t9’<br>是否可用</li><li>disable ‘t9’<br>停用</li><li>is_disabled ‘t9’<br>是否不可用</li></ul></li><li><p>删除一个列簇:</p><ul><li>注意: 当表中只有一个列族时, 无法将其删除</li><li>disable ‘user’(新版本不用)</li><li>alter ‘user’, NAME =&gt; ‘f1’, METHOD =&gt; ‘delete’</li><li>或</li><li>alter ‘user’, ‘delete’ =&gt; ‘f1’</li><li>enable ‘user’</li></ul></li><li><p>添加列簇 f1 同时删除列簇 f2</p><ul><li>disable ‘user’(新版本不用)</li><li>alter ‘user’, {NAME =&gt; ‘f1’}, {NAME =&gt; ‘f2’, METHOD =&gt; ‘delete’}</li><li>enable ‘user’</li></ul></li><li><p>将 user 表的 f1 列簇版本号改为 5</p><ul><li>disable ‘user’(新版本不用)</li><li>alter ‘people’, NAME =&gt; ‘info’, VERSIONS =&gt; 5<br>people 为 rowkey</li><li>enable ‘user’</li></ul></li></ul><h4 id="drop-相关"><a href="#drop-相关" class="headerlink" title="drop 相关"></a>drop 相关</h4><ul><li>disable ‘user’</li><li>drop ‘user’</li></ul><h4 id="count-相关"><a href="#count-相关" class="headerlink" title="count 相关"></a>count 相关</h4><ul><li>$hbase&gt;count ‘ns1:t1’<br>统计函数, 1000行统计一次</li></ul><h4 id="flush-相关"><a href="#flush-相关" class="headerlink" title="flush 相关"></a>flush 相关</h4><ul><li>把文件刷到磁盘的过程</li><li>hbase&gt; flush ‘TABLENAME’</li><li>hbase&gt; flush ‘REGIONNAME’</li><li>hbase&gt; flush ‘ENCODED_REGIONNAME’</li></ul><h4 id="ValueFilter-过滤器"><a href="#ValueFilter-过滤器" class="headerlink" title="ValueFilter 过滤器"></a>ValueFilter 过滤器</h4><ul><li>get ‘person’, ‘rk0001’, {FILTER =&gt; “ValueFilter(=, ‘binary:中国’)”}</li><li>get ‘person’, ‘rk0001’, {FILTER =&gt; “(QualifierFilter(=,’substring:a’))”}</li></ul><h4 id="exit-退出"><a href="#exit-退出" class="headerlink" title="exit 退出"></a>exit 退出</h4><p>$hbase&gt; exit</p><h4 id="遇到问题总结"><a href="#遇到问题总结" class="headerlink" title="遇到问题总结"></a>遇到问题总结</h4><ul><li>插入前, 必须创建好<ul><li>namespace<br>相当于数据库的概念</li><li>table<br>表命</li><li>column family<br>列族名</li></ul></li><li>不需要提前创建<ul><li>行名</li><li>列名</li><li>value</li></ul></li><li>如果表已经创建, 要增加列族的话, 只能用 alter, 不能继续用 create</li></ul><h2 id="通过Java-API访问Hbase"><a href="#通过Java-API访问Hbase" class="headerlink" title="通过Java API访问Hbase"></a>通过Java API访问Hbase</h2><ul><li><p>创建hbase模块</p></li><li><p>添加依赖<br>&lt;?xml version=”1.0” encoding=”UTF-8”?&gt;</p><project xmlns="<http://maven.apache.org/POM/4.0.0>" xmlns:xsi="<http://www.w3.org/2001/XMLSchema-instance>" xsi:schemalocation="<http://maven.apache.org/POM/4.0.0http://maven.apache.org/xsd/maven-4.0.0.xsd>"><br>    <modelversion>4.0.0</modelversion><br>    <groupid>com.rox</groupid><br>    <artifactid>HbaseDemo</artifactid><br>    <version>1.0-SNAPSHOT</version><br>    <dependencies><br>        <dependency><br>            <groupid>org.apache.hbase</groupid><br>            <artifactid>hbase-client</artifactid><br>            <version>1.2.3</version><br>        </dependency><br>    </dependencies><br></project> </li><li><p>复制hbase集群的hbase-site.xml文件到模块的src/main/resources目录下。</p></li><li><p>-——————————————————————————————</p></li><li><p>Hbase API 类和数据模型之间的对应关系:</p><ul><li><p>\0. 总的对应关系</p><p><img src="https://img.mubu.com/document_image/12d391d8-836e-48de-bf08-53a95e01343c-584150.jpg" alt="img"></p></li><li><ol><li>HBaseAdmin</li></ol><ul><li></li></ul></li></ul></li><li><p>编程实现 代码见 <a href="https://github.com/airpoet/bigdata/blob/master/HBaseDemo/src/test/java/com/rox/text/TestCRUD.java" target="_blank" rel="noopener">https://github.com/airpoet/bigdata/blob/master/HBaseDemo/src/test/java/com/rox/text/TestCRUD.java</a><br>public class TestCRUD {<br>@Test<br>public void put() throws IOException {<br>// 创建 conf 对象<br>Configuration conf = HBaseConfiguration.create();/<em> 通过连接工厂创建连接对 </em>/<br>// 通过连接工厂创建连接对象<br>Connection conn = ConnectionFactory.createConnection(conf);<br>// 通过连接查询 TableName 对象<br>TableName tname = TableName.valueOf(“ns1:t1”);<br>// 获得 table对象<br>Table table = conn.getTable(tname);<br>// 通过bytes 工具类转化字符串为字节数组<br>byte[] bytes = Bytes.toBytes(“row3”);<br>// 创建 put 对象, 传入行号<br>Put put = new Put(bytes);<br>// 创建 列族, 列, value 的 byte 数据<br>byte[] f1 = Bytes.toBytes(“f1”);<br>byte[] id = Bytes.toBytes(“id”);<br>byte[] value = Bytes.toBytes(188);<br>put.addColumn(f1, id, value);<br>// table put 数据<br>table.put(put);<br>//============================<br>// get<br>byte[] rowid = Bytes.toBytes(“row3”);<br>Get get = new Get(rowid);<br>// 得到 res<br>Result res = table.get(get);<br> // 从 res 中取出 value<br>byte[] idvalue = res.getValue(Bytes.toBytes(“f1”),Bytes.toBytes(“id”));<br>System.out.println(Bytes.toInt(idvalue));<br>}<br>}</p></li></ul><h2 id="HBase-写入过程-amp-存储路径"><a href="#HBase-写入过程-amp-存储路径" class="headerlink" title="HBase 写入过程 &amp; 存储路径"></a>HBase 写入过程 &amp; 存储路径</h2><ul><li>WAL<br>write ahead log,写前日志。</li><li>HDFS 上存储路径详解<ul><li><a href="http://cs1:50070/explorer.html#/hbase/data/ns1/t1/97eada11d196f1e134c41e859d338e07/f1/fsfsfgwgfsgfrgfdg" target="_blank" rel="noopener">http://cs1:50070/explorer.html#/hbase/data/ns1/t1/97eada11d196f1e134c41e859d338e07/f1/fsfsfgwgfsgfrgfdg</a><ul><li>data<br>存储数据的目录</li><li>ns1<br>namespace 名称</li><li>t1<br>表名</li><li>97eada11d196f1e134c41e859d338e07<br>region 的编号</li><li>f1<br>列族</li><li>fsfsfgwgfsgfrgfdg<br>HFile: 列族存储的文件, 每一个 hfile 文件对应的是一个列族</li></ul></li><li>meta 数据路径 <a href="http://cs1:50070/explorer.html#/hbase/data/hbase/meta/1588230740/info" target="_blank" rel="noopener">http://cs1:50070/explorer.html#/hbase/data/hbase/meta/1588230740/info</a></li></ul></li></ul><h2 id="HBase-和-Hive-的比较"><a href="#HBase-和-Hive-的比较" class="headerlink" title="HBase 和 Hive 的比较"></a>HBase 和 Hive 的比较</h2><ul><li>相同点<ul><li>HBase 和 Hive 都是架构在 Hadoop 之上，用 HDFS 做底层的数据存储，用 MapReduce 做 数据计算</li></ul></li><li>不同点<ul><li>解决的问题不同<ul><li>Hive 是建立在 Hadoop 之上为了降低 MapReduce 编程复杂度的 ETL 工具。</li><li>HBase 是为了弥补 Hadoop 对实时操作的缺陷</li></ul></li><li>表的架构不同<ul><li>Hive 表是纯逻辑表，因为 Hive 的本身并不能做数据存储和计算，而是完全依赖 Hadoop</li><li>HBase 是物理表，提供了一张超大的内存 Hash 表来存储索引，方便查询</li></ul></li><li>定位 &amp; 访问机制不同<ul><li>Hive 是数据仓库工具，需要全表扫描，就用 Hive，因为 Hive 是文件存储</li><li>HBase 是数据库，需要索引访问，则用 HBase，因为 HBase 是面向列的 NoSQL 数据库</li></ul></li><li>存储模式不同<ul><li>Hive 表中存入数据(文件)时不做校验，属于读模式存储系统</li><li>HBase 表插入数据时，会和 RDBMS 一样做 Schema 校验，所以属于写模式存储系统</li></ul></li><li>是否实时处理(处理效率)不同<ul><li>Hive 不支持单行记录操作，数据处理依靠 MapReduce，操作延时高</li><li>HBase 支持单行记录的 CRUD，并且是实时处理，效率比 Hive 高得多</li></ul></li></ul></li></ul><h2 id="HBase在-HDFS-上的存储路径"><a href="#HBase在-HDFS-上的存储路径" class="headerlink" title="HBase在 HDFS 上的存储路径"></a>HBase在 HDFS 上的存储路径</h2><ul><li>相同列族的数据存放在一个文件中。</li><li>[表数据的存储目录结构构成]<ul><li><a href="hdfs://cs1:8020/hbase/data/${名字空间}/${表名}/${区域名称}/${列族名称}/${文件名}" target="_blank" rel="noopener">hdfs://cs1:8020/hbase/data/${名字空间}/${表名}/${区域名称}/${列族名称}/${文件名}</a></li></ul></li><li>[WAL目录结构构成]<ul><li><a href="hdfs://cs1:8020/hbase/WALs/${区域服务器名称,端口号,时间戳}/" target="_blank" rel="noopener">hdfs://cs1:8020/hbase/WALs/${区域服务器名称,端口号,时间戳}/</a></li></ul></li></ul><h2 id="Client-端-与-HBase-交互过程"><a href="#Client-端-与-HBase-交互过程" class="headerlink" title="Client 端 与 HBase 交互过程"></a>Client 端 与 HBase 交互过程</h2><ul><li><p>HBase 简单集群结构</p><p><img src="https://img.mubu.com/document_image/5c640ea0-d4ff-43ed-94bb-ebebfc35aa62-584150.jpg" alt="img"></p><ul><li>region:是 hbase 中对表进行切割的单元，由 regionserver 负责管理<ul><li>region 分裂是逻辑概念?? :TODO</li></ul></li><li>hamster:hbase 的主节点，负责整个集群的状态感知，负载分配、负责用户表的元数据(schema)管理(可以配置多个用来实现 HA),hmaster 负载压力相对于 hdfs 的 namenode 会小很多</li><li>regionserver:hbase 中真正负责管理 region 的服务器，也就是负责为客户端进行表数据读写 的服务器每一台 regionserver 会管理很多的 region，同一个 regionserver 上面管理的所有的 region 不属于同一张表</li><li>zookeeper:整个 hbase 中的主从节点协调，主节点之间的选举，集群节点之间的上下线感 知„„都是通过 zookeeper 来实现</li><li>HDFS:用来存储 hbase 的系统文件，或者表的 region</li></ul></li><li><p>Hbase 顶层结构图</p><p><img src="https://img.mubu.com/document_image/ebc78e6e-830c-49af-a05f-c79ec1bad8d6-584150.jpg" alt="img"></p></li><li><p>0.hbase集群启动时，master负责分配区域到指定区域服务器。</p></li><li><p>1.联系zk，找出meta表所在rs(regionserver)/hbase/meta-region-server</p></li><li><p>2.定位row key,找到对应region server</p></li><li><p>3.缓存信息在本地。</p></li><li><p>4.联系RegionServer</p></li><li><p>5.HRegionServer负责open HRegion对象，为每个列族创建Store对象，Store包含多个StoreFile实例，他们是对HFile的轻量级封装。每个Store还对应了一个MemStore，用于内存存储数据。</p></li></ul><h2 id="百万数据批量插入"><a href="#百万数据批量插入" class="headerlink" title="百万数据批量插入"></a>百万数据批量插入</h2><ul><li>代码<br>long start = System.currentTimeMillis() ;<br>Configuration conf = HBaseConfiguration.create();<br>Connection conn = ConnectionFactory.createConnection(conf);<br>TableName tname = TableName.valueOf(“ns1:t1”);<br>HTable table = (HTable)conn.getTable(tname);<br>//不要自动清理缓冲区<br>table.setAutoFlush(false);<br>for(int i = 4 ; i &lt; 1000000 ; i ++){<pre><code>Put put = new Put(Bytes.toBytes(&quot;row&quot; + i)) ;//关闭写前日志put.setWriteToWAL(false);put.addColumn(Bytes.toBytes(&quot;f1&quot;),Bytes.toBytes(&quot;id&quot;),Bytes.toBytes(i));put.addColumn(Bytes.toBytes(&quot;f1&quot;),Bytes.toBytes(&quot;name&quot;),Bytes.toBytes(&quot;tom&quot; + i));put.addColumn(Bytes.toBytes(&quot;f1&quot;),Bytes.toBytes(&quot;age&quot;),Bytes.toBytes(i % 100));table.put(put);if(i % 2000 == 0){    table.flushCommits();}</code></pre>}<br>//<br>table.flushCommits();<br>System.out.println(System.currentTimeMillis() - start ); </li></ul><h2 id="HBase-切割文件"><a href="#HBase-切割文件" class="headerlink" title="HBase 切割文件"></a>HBase 切割文件</h2><ul><li>默认10G 进行切割<property><br>        <name>hbase.hregion.max.filesize</name><br>        <value>10737418240</value><br>        <source>hbase-default.xml<br></property> </li></ul><h2 id="HBase-合并文件"><a href="#HBase-合并文件" class="headerlink" title="HBase 合并文件"></a>HBase 合并文件</h2><ul><li>merge_region ‘1f4609ba4e2a9440aedde5d0e7123722’,’48277c27196a5c6e30d5ae679e9ec4f0’<br>merge ‘前一个区域的 encoded’ , ‘后一个区域的 encoded’</li></ul><h2 id="HBase-手动移动文件"><a href="#HBase-手动移动文件" class="headerlink" title="HBase 手动移动文件"></a>HBase 手动移动文件</h2><ul><li>hbase(main):034:0&gt; move ‘8658ac9ea01f80c2cca32693397a1e70’,’cs3,16020,1529980599903’<br>前面是 encoded region name , 后面是服务器名</li></ul><h2 id="预先切割"><a href="#预先切割" class="headerlink" title="预先切割"></a>预先切割</h2><ul><li>创建表时, 预先对表进行切割</li><li>切割线是 rowkey<br>create ‘ns1:t2’,’f1’,SPLITS=&gt;[‘row300’, ‘row600’]</li><li>创建预先切割表之后, 存的值就会到对应的区域中去</li><li>注意: put 一次就是一条数据</li></ul><h2 id="指定版本数的问题"><a href="#指定版本数的问题" class="headerlink" title="指定版本数的问题"></a>指定版本数的问题</h2><ul><li>创建表时指定列族的版本数,该列族的所有列都具有相同数量版本 </li><li>创建表时，指定列族的版本数。<br>$hbase&gt;create ‘ns1:t3’,{NAME=&gt;’f1’,VERSIONS=&gt;3}            </li><li>如果创建表的时候创的是3个版本, 查4个版本, 此时最多也就显示3个<br>$hbase&gt;get ‘ns1:t3’,’row1’,{COLUMN=&gt;’f1’,VERSIONS=&gt;4}<br>COLUMN                  CELL<br> f1:name                timestamp=1530019546709, value=tom3<br> f1:name                timestamp=1530019542978, value=tom2<br> f1:name                timestamp=1530019540164, value=tom1</li></ul><h2 id="原生扫描-expert"><a href="#原生扫描-expert" class="headerlink" title="原生扫描(expert)"></a>原生扫描(expert)</h2><ul><li><p><img src="https://img.mubu.com/document_image/7d75648a-c26c-46c9-9c06-286805d4270b-584150.jpg" alt="img"></p></li><li><p>原生扫描, 包含标记了delete的数据<br>$hbase&gt;scan ‘ns1:t3’,{COLUMN=&gt;’f1’,RAW=&gt;true,VERSIONS=&gt;10}<br>hbase(main):177:0&gt; scan ‘ns1:t3’,{COLUMN=&gt;’f1’,RAW=&gt;true,VERSIONS=&gt;10}<br>ROW                     COLUMN+CELL<br> row1                   column=f1:name, timestamp=1530019546709, value=tom3<br> row1                   column=f1:name, timestamp=1530019542978, value=tom2<br> row1                   column=f1:name, timestamp=1530019540164, type=DeleteColumn<br> row1                   column=f1:name, timestamp=1530019540164, value=tom1</p></li><li><p>删除数据, 标记为删除, 小于该删除时间的数据都作废, flush 后, 用 RAW 似乎还会保留, 用非 RAW 的方式查看的画, 此删除数据, 及时间戳比这个小的数据, 都不会显示.</p></li></ul><p>   $hbase&gt;delete ‘ns1:t3’,’row1’,’f1:name’,148989875645</p><ul><li>这里是否要停掉 hbase 服务再起来看看?</li></ul><ul><li><p>TTL</p><ul><li>time to live ,存活时间</li><li>影响所有的数据，包括没有删除的数据</li><li>超过该时间，原生扫描也扫不到数据</li><li>创建带 TTL参数的表<br>$hbase&gt;create ‘ns1:tx’ , {NAME=&gt;’f1’,TTL=&gt;10,VERSIONS=&gt;10} </li></ul></li><li><p>KEEP_DELETED_CELLS</p><ul><li>删除key之后，数据是否还保留, 未测试<br>$hbase&gt;create ‘ns1:tx’ , {NAME=&gt;’f1’,TTL=&gt;10,VERSIONS,KEEP_DELETED_CELLS=&gt;true}</li><li>TTL 的优先级高于 KEEP_DELETED_CELLS, 如果设置了 TTL, 时间一到就没了, 用 RAW 也找不回来</li></ul></li></ul><h2 id="缓存-和-批处理"><a href="#缓存-和-批处理" class="headerlink" title="缓存 和 批处理"></a>缓存 和 批处理</h2><ul><li><p>开启服务器端扫描器缓存</p><ul><li>表层面(全局), 貌似是默认的? 再看看书上说的<property><br>    <name>hbase.client.scanner.caching</name><br>    <!-- 整数最大值 --><br>    <value>2147483647</value><br>    <source>hbase-default.xml<br></property> </li></ul></li><li><p>操作层面</p></li></ul><p>  注意: 貌似JavaAPI 中默认是没有设置 caching 的</p><ul><li>设置方式<br>scan.setCaching(10);</li><li>操作时间<br>cache row nums : 1000            //632<br>cache row nums : 5000            //423<br>cache row nums : 1                //7359 </li></ul><ul><li><p>扫描器缓存是 面向行级别的</p></li><li><p>批量扫描是 面向列级别的</p><ul><li>控制每次next()服务器端返回的列的个数。</li><li>scan.setBatch(5);<br>每次next返回5列。</li></ul></li></ul><h2 id="过滤器"><a href="#过滤器" class="headerlink" title="过滤器"></a>过滤器</h2><ul><li><p>图示</p><p><img src="https://img.mubu.com/document_image/36f8af62-4775-498f-a399-3e6a7f748382-584150.jpg" alt="img"></p></li><li><p>代码</p><ul><li><a href="https://github.com/airpoet/bigdata/blob/master/HBaseDemo/src/test/java/com/rox/text/TestCRUD.java" target="_blank" rel="noopener">https://github.com/airpoet/bigdata/blob/master/HBaseDemo/src/test/java/com/rox/text/TestCRUD.java</a></li></ul></li></ul><h2 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h2><ul><li>shell 实现<ul><li>$hbase&gt;incr ‘ns1:t8’,’row1’,’f1:click’,1<br>后面可以跟任意数字, 正负,0都可以</li><li>$hbase&gt;get_counter ‘ns1:t8’,’row1’,’f1:click’<br>获取计数器</li></ul></li><li>Java 代码<ul><li><a href="https://github.com/airpoet/bigdata/blob/master/HBaseDemo/src/test/java/com/rox/text/TestCRUD.java" target="_blank" rel="noopener">https://github.com/airpoet/bigdata/blob/master/HBaseDemo/src/test/java/com/rox/text/TestCRUD.java</a></li></ul></li></ul><h2 id="协处理器-coprocessor-TODO-有空再看"><a href="#协处理器-coprocessor-TODO-有空再看" class="headerlink" title="协处理器 coprocessor :TODO 有空再看"></a>协处理器 coprocessor :TODO 有空再看</h2><ul><li><p>批处理的，等价于存储过程或者触发器 </p><p><img src="https://img.mubu.com/document_image/16c4bf5b-b2ea-4908-a6d2-1fc2cf779e22-584150.jpg" alt="img"></p></li><li><p>Observer</p><p><img src="https://img.mubu.com/document_image/78a0ca5e-5ead-4a01-b37c-e5e29f650045-584150.jpg" alt="img"></p><ul><li>RegionObserver        //RegionServer区域观察者</li><li>MasterObserver        //Master节点。</li><li>WAlObserver            //</li></ul></li><li><p>Endpoint</p><ul><li>终端,类似于存储过程。 </li></ul></li></ul><h2 id="HBase-amp-Mapreduce-整合"><a href="#HBase-amp-Mapreduce-整合" class="headerlink" title="HBase &amp; Mapreduce 整合"></a>HBase &amp; Mapreduce 整合</h2><ul><li>运行官方案例<ul><li>[ap@cs1]~/apps/hbase%  export HBASE_HOME=/home/ap/apps/hbase</li><li>[ap@cs1]~/apps/hbase%  export HADOOP_HOME=/home/ap/apps/hadoop</li><li>[ap@cs1]~/apps/hbase% export HADOOP_CLASSPATH=<code>${HBASE_HOME}/bin/hbase mapredcp</code></li></ul></li><li>使用 MapReduce导入本地数据到Hbase<ul><li>apps/hadoop/bin/yarn jar apps/hbase/lib/hbase-server-1.2.6.1.jar importtsv \</li><li>-Dimporttsv.columns=HBASE_ROW_KEY,info:name,info:color fruit \</li><li><a href="hdfs://cs1:8020/input_fruit" target="_blank" rel="noopener">hdfs://cs1:8020/input_fruit</a><br>这是 namenode 的 RPC 地址</li></ul></li></ul><h2 id="HBase-amp-Hive-整合"><a href="#HBase-amp-Hive-整合" class="headerlink" title="HBase &amp; Hive 整合"></a>HBase &amp; Hive 整合</h2><p>1.环境配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HBASE_HOME=/home/ap/apps/hbase</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/home/ap/apps/hadoop</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> HADOOP_CLASSPATH=`<span class="variable">$&#123;HBASE_HOME&#125;</span>/bin/hbase mapredcp`</span><br></pre></td></tr></table></figure><h2 id="剩下的hbase知识点-有空再补充"><a href="#剩下的hbase知识点-有空再补充" class="headerlink" title="剩下的hbase知识点 有空再补充"></a>剩下的hbase知识点 有空再补充</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Hbase概念&quot;&gt;&lt;a href=&quot;#Hbase概念&quot; class=&quot;headerlink&quot; title=&quot;Hbase概念&quot;&gt;&lt;/a&gt;Hbase概念&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;hbase &amp;amp; hdfs 关系图&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;htt
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="HBase" scheme="https://airpoet.github.io/categories/Hadoop/HBase/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="HBase" scheme="https://airpoet.github.io/tags/HBase/"/>
    
  </entry>
  
  <entry>
    <title>i-Zookeeper</title>
    <link href="https://airpoet.github.io/2018/06/23/Hadoop/4-Zookeeper/i-Zookeeper/"/>
    <id>https://airpoet.github.io/2018/06/23/Hadoop/4-Zookeeper/i-Zookeeper/</id>
    <published>2018-06-23T06:00:32.111Z</published>
    <updated>2018-06-30T15:07:11.420Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Zookeeper-的安装-amp-概述"><a href="#1-Zookeeper-的安装-amp-概述" class="headerlink" title="1.Zookeeper 的安装&amp;概述"></a>1.Zookeeper 的安装&amp;概述</h1><h3 id="1-1-Zookeeper-简介"><a href="#1-1-Zookeeper-简介" class="headerlink" title="1.1. Zookeeper 简介"></a>1.1. Zookeeper 简介</h3><p><strong>Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。</strong></p><p>zk提供的服务 </p><ul><li><p>Naming service                //按名称区分集群中的节点. </p></li><li><p>Configuration management    //对加入节点的最新化处理。 </p></li><li>Cluster management            //实时感知集群中节点的增减. </li><li>Leader election                //leader选举 </li><li>Locking and synchronization service    //修改时锁定数据，实现容灾. </li><li>Highly reliable data registry        //节点宕机数据也是可用的。 </li></ul><h3 id="1-2-ZK-的工作机制"><a href="#1-2-ZK-的工作机制" class="headerlink" title="1.2.  ZK 的工作机制"></a>1.2.  ZK 的工作机制</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-23-065529.png" alt="image-20180623145528682"></p><h3 id="1-4-ZK-架构"><a href="#1-4-ZK-架构" class="headerlink" title="1.4. ZK 架构"></a>1.4. ZK 架构</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-23-064232.png" alt="image-20180623144232085"></p><h4 id="1-4-1-名词解释"><a href="#1-4-1-名词解释" class="headerlink" title="1.4.1 名词解释"></a>1.4.1 名词解释</h4><p>   1.Client </p><p>​        从server获取信息，周期性发送数据给server，表示自己还活着。 </p><p>​        client连接时，server回传ack信息。 </p><p>​        如果client没有收到reponse，自动重定向到另一个server. </p><p>​    2.Server </p><p>​        zk集群中的一员，向client提供所有service，回传ack信息给client，表示自己还活着。 </p><p>​    3.ensemble </p><p>​        一组服务器。 </p><p>​        最小节点数是3. </p><p>​    4.Leader </p><p>​        如果连接的节点失败，自定恢复，zk服务启动时，完成leader选举。 </p><p>​    5.Follower </p><p>​        追寻leader指令的节点。 </p><h4 id="1-4-2-整体解释"><a href="#1-4-2-整体解释" class="headerlink" title="1.4.2 整体解释"></a>1.4.2 整体解释</h4><ul><li>1）Zookeeper：一个领导者（leader），多个跟随者（follower）组成的集群。</li><li>2）Leader负责进行投票的发起和决议，更新系统状态</li><li>3）Follower用于接收客户请求并向客户端返回结果，在选举Leader过程中参与投票</li><li>4）集群中只要有半数以上节点存活，Zookeeper集群就能正常服务。</li><li>5）全局数据一致：每个server保存一份相同的数据副本，client无论连接到哪个server，数据都是一致的。</li><li>6）更新请求顺序进行，来自同一个client的更新请求按其发送顺序依次执行。</li><li>7）数据更新原子性，一次数据更新要么成功，要么失败。</li><li>8）实时性，在一定时间范围内，client能读到最新数据。</li></ul><h3 id="1-5-znode"><a href="#1-5-znode" class="headerlink" title="1.5. znode"></a>1.5. znode</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-23-064202.png" alt="image-20180623144202281"></p><p>ZooKeeper数据模型的结构与Unix文件系统很类似，整体上可以看作是一棵树，每个节点称做一个ZNode。每一个ZNode默认能够存储1MB的数据，每个ZNode都可以通过其路径唯一标识 </p><p>zk中的节点，维护了stat，由Version number, Action control list (ACL), Timestamp,Data length.构成. </p><p>data version        //数据写入的过程变化 </p><p> ACL                    //action control list, </p><h3 id="1-6-节点类型"><a href="#1-6-节点类型" class="headerlink" title="1.6. 节点类型"></a>1.6. 节点类型</h3><p>​    1.持久节点 </p><p>​        client结束，还存在。 </p><p>​    2.临时节点 </p><p>​        在client活动时有效，断开自动删除。临时节点不能有子节点。 </p><p>​        leader推选时使用。 </p><p>​    3.序列节点 </p><p>​        在节点名之后附加10个数字，主要用于同步和锁. </p><h3 id="1-7-Session"><a href="#1-7-Session" class="headerlink" title="1.7. Session"></a>1.7. Session</h3><p>​    Session中的请求以FIFO执行，一旦client连接到server，session就建立了。sessionid分配client. </p><p>​    client以固定间隔向server发送心跳，表示session是valid的，zk集群如果在超时时候，没有收到心跳， </p><p>​    判定为client挂了，与此同时，临时节点被删除。 </p><h3 id="1-8-Watches"><a href="#1-8-Watches" class="headerlink" title="1.8. Watches"></a>1.8. Watches</h3><p>​    观察。 </p><p>​    client能够通过watch机制在数据发生变化时收到通知。 </p><p>​    client可以在read 节点时设置观察者。watch机制会发送通知给注册的客户端。 </p><p>​    观察模式只触发一次。 </p><p>​    session过期，watch机制删除了。 </p><h3 id="1-9-zk工作流程"><a href="#1-9-zk工作流程" class="headerlink" title="1.9. zk工作流程"></a>1.9. zk工作流程</h3><p>​    zk集群启动后，client连接到其中的一个节点，这个节点可以是leader，也可以是follower。 </p><p>​    连通后，node分配一个id给client，发送ack信息给client。 </p><p>​    如果客户端没有收到ack，连接到另一个节点。 </p><p>​    client周期性发送心跳信息给节点保证连接不会丢失。 </p><p>​    如果client读取数据，发送请求给node，node读取自己数据库，返回节点数据给client. </p><p>​    如果client想要在 zk 中存储数据，将路径和数据发送给server，server转发给leader。 </p><p>​    leader再补发请求给所有follower。只有大多数(超过半数)节点成功响应，则 </p><p>​    写操作成功。 </p><h3 id="1-10-zk-应用场景"><a href="#1-10-zk-应用场景" class="headerlink" title="1.10 zk 应用场景"></a>1.10 zk 应用场景</h3><h4 id="1-10-1-统一命名服务"><a href="#1-10-1-统一命名服务" class="headerlink" title="1.10.1 统一命名服务"></a>1.10.1 统一命名服务</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-23-073654.png" alt="image-20180623153654235"></p><h4 id="1-10-2-统一配管理"><a href="#1-10-2-统一配管理" class="headerlink" title="1.10.2 统一配管理"></a>1.10.2 统一配管理</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-23-073811.png" alt="image-20180623153811327"></p><h4 id="1-10-3-统一集群管理"><a href="#1-10-3-统一集群管理" class="headerlink" title="1.10.3 统一集群管理"></a>1.10.3 统一集群管理</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-23-073914.png" alt="image-20180623153914499"></p><h4 id="1-10-4-服务器动态上下线"><a href="#1-10-4-服务器动态上下线" class="headerlink" title="1.10.4 服务器动态上下线"></a>1.10.4 服务器动态上下线</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-23-074113.png" alt="image-20180623154113232"></p><h4 id="1-10-5-软负载均衡"><a href="#1-10-5-软负载均衡" class="headerlink" title="1.10.5 软负载均衡"></a>1.10.5 软负载均衡</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-23-074213.png" alt="image-20180623154212560"></p><h1 id="2-Zookeeper-完全分布式的安装-amp-基本使用"><a href="#2-Zookeeper-完全分布式的安装-amp-基本使用" class="headerlink" title="2.Zookeeper 完全分布式的安装&amp;基本使用"></a>2.Zookeeper 完全分布式的安装&amp;基本使用</h1><h3 id="2-1-安装"><a href="#2-1-安装" class="headerlink" title="2.1 安装"></a>2.1 安装</h3><h4 id="2-1-1-高可用安装"><a href="#2-1-1-高可用安装" class="headerlink" title="2.1.1 高可用安装"></a>2.1.1 高可用安装</h4><p><a href="https://airpoet.github.io/2018/06/22/Hadoop/0-Hadoop/Hadoop-HA%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/">PS:  高可用安装见这里</a></p><h4 id="2-2-2-完全分布式安装"><a href="#2-2-2-完全分布式安装" class="headerlink" title="2.2.2 完全分布式安装"></a>2.2.2 完全分布式安装</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">1.挑选3台主机</span><br><span class="line">cs1 ~ cs3</span><br><span class="line">2.每台机器都安装zk &amp; 配置环境变量</span><br><span class="line"></span><br><span class="line">3.配置zk配置文件</span><br><span class="line">cs1 ~ cs3</span><br><span class="line">[/home/ap/apps/zk/conf/zoo.cfg]</span><br><span class="line">----------</span><br><span class="line">tickTime=2000</span><br><span class="line">initLimit=10</span><br><span class="line">syncLimit=5</span><br><span class="line">dataDir=/home/ap/zookeeper</span><br><span class="line">clientPort=2181</span><br><span class="line"></span><br><span class="line">server.1=cs1:2888:3888</span><br><span class="line">server.2=cs2:2888:3888</span><br><span class="line">server.3=cs3:2888:3888</span><br><span class="line">---------</span><br><span class="line"></span><br><span class="line">4.在每台主机的/home/ap/zookeeper中添加myid,内容分别是1,2,3</span><br><span class="line">[cs1]</span><br><span class="line">$&gt;<span class="built_in">echo</span> 1 &gt; /home/ap/zookeeper/myid</span><br><span class="line">[cs2]</span><br><span class="line">$&gt;<span class="built_in">echo</span> 2 &gt; /home/ap/zookeeper/myid</span><br><span class="line">[cs3]</span><br><span class="line">$&gt;<span class="built_in">echo</span> 3 &gt; /home/ap/zookeeper/myid</span><br><span class="line"></span><br><span class="line">5.在cs1~cs3上,启动服务器集群 </span><br><span class="line">$&gt;zkServer.sh start</span><br><span class="line"></span><br><span class="line">6.查看每台服务器的状态</span><br><span class="line">$&gt;zkServer.sh status</span><br><span class="line">* 注意: 如果有3台机器, 只启动一台的话, 会显示如下, 因为根据配置, 有3台机器, 此时只有一台启动, 没有过半数, 整个集群是挂掉的</span><br><span class="line">* 只有启动的机器数量超过配置的半数, zk 集群才有效.</span><br></pre></td></tr></table></figure><h4 id="2-2-3-zoo-cfg-文件中配置参数的含义"><a href="#2-2-3-zoo-cfg-文件中配置参数的含义" class="headerlink" title="2.2.3 zoo.cfg 文件中配置参数的含义"></a>2.2.3 <strong>zoo.cfg 文件中配置参数的含义</strong></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">1）tickTime=2000：通信心跳数</span><br><span class="line">    tickTime：通信心跳数，Zookeeper服务器心跳时间，单位毫秒</span><br><span class="line">    Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。</span><br><span class="line">    它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime)</span><br><span class="line">2）initLimit=10：LF初始通信时限</span><br><span class="line">    initLimit：LF初始通信时限</span><br><span class="line">    集群中的follower跟随者服务器(F)与leader领导者服务器(L)之间初始连接时能容忍的最多心跳数（tickTime的数量），用它来限定集群中的Zookeeper服务器连接到Leader的时限。</span><br><span class="line">    投票选举新leader的初始化时间</span><br><span class="line">    Follower在启动过程中，会从Leader同步所有最新数据，然后确定自己能够对外服务的起始状态。</span><br><span class="line">    Leader允许F在initLimit时间内完成这个工作。</span><br><span class="line">3）syncLimit=5：LF同步通信时限</span><br><span class="line">    syncLimit：LF同步通信时限</span><br><span class="line">    集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，</span><br><span class="line">    Leader认为Follwer死掉，从服务器列表中删除Follwer。</span><br><span class="line">    在运行过程中，Leader负责与ZK集群中所有机器进行通信，例如通过一些心跳检测机制，来检测机器的存活状态。</span><br><span class="line">    如果L发出心跳包在syncLimit之后，还没有从F那收到响应，那么就认为这个F已经不在线了。</span><br><span class="line">4）dataDir：数据文件目录+数据持久化路径</span><br><span class="line">    dataDir：数据文件目录+数据持久化路径</span><br><span class="line">    保存内存数据库快照信息的位置，如果没有其他说明，更新的事务日志也保存到数据库。</span><br><span class="line">5）clientPort=2181：客户端连接端口</span><br><span class="line">    监听客户端连接的端口</span><br><span class="line">6) Server.A=B:C:D。</span><br><span class="line">    A是一个数字，表示这个是第几号服务器；</span><br><span class="line">    B是这个服务器的ip地址；</span><br><span class="line">    C是这个服务器与集群中的Leader服务器交换信息的端口；</span><br><span class="line">    D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。</span><br><span class="line">    <span class="comment"># 集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。</span></span><br></pre></td></tr></table></figure><h3 id="2-2-ZK客户端的连接"><a href="#2-2-ZK客户端的连接" class="headerlink" title="2.2 ZK客户端的连接"></a>2.2 ZK客户端的连接</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">$&gt;zkCli.sh -server cs1:2181    //进入zk命令行</span><br><span class="line"><span class="variable">$zk</span>]<span class="built_in">help</span>                        //查看帮助</span><br><span class="line"><span class="variable">$zk</span>]quit                        //退出</span><br><span class="line"><span class="variable">$zk</span>]create /a tom                 //创建节点</span><br><span class="line"><span class="variable">$zk</span>]get /a                        //查看数据</span><br><span class="line"><span class="variable">$zk</span>]ls /                          //列出节点</span><br><span class="line"><span class="variable">$zk</span>]ls2 /                         //查看当前节点数据并能看到更新次数等数据</span><br><span class="line"><span class="variable">$zk</span>]<span class="built_in">set</span> /a tom                    //设置数据</span><br><span class="line"><span class="variable">$zk</span>]delete /a                     //删除一个节点</span><br><span class="line"><span class="variable">$zk</span>]rmr /a                        //递归删除所有节点。</span><br><span class="line"><span class="variable">$zk</span>]<span class="built_in">stat</span> ///查看节点状态</span><br><span class="line"></span><br><span class="line">注意: </span><br><span class="line">* 创建节点不能递归创建, 目前只能一层一层创建</span><br><span class="line">* 每次创建都的写数据, 只创建目录的话会创建不成功</span><br><span class="line">* 如果 close 后, 并没有退出客户端, 只是访问不到数据, 此时如果想要连接, 可以用connect host:port</span><br><span class="line">================================================================================</span><br><span class="line"></span><br><span class="line">ZK 的帮助文档</span><br><span class="line">----------------</span><br><span class="line"><span class="built_in">help</span>:</span><br><span class="line">ZooKeeper -server host:port cmd args</span><br><span class="line">    <span class="built_in">stat</span> path [watch]</span><br><span class="line">    <span class="built_in">set</span> path data [version]</span><br><span class="line">    ls path [watch]</span><br><span class="line">    delquota [-n|-b] path</span><br><span class="line">    ls2 path [watch]</span><br><span class="line">    setAcl path acl</span><br><span class="line">    setquota -n|-b val path</span><br><span class="line">    <span class="built_in">history</span></span><br><span class="line">    redo cmdno</span><br><span class="line">    printwatches on|off</span><br><span class="line">    delete path [version]</span><br><span class="line">    sync path</span><br><span class="line">    listquota path</span><br><span class="line">    rmr path</span><br><span class="line">    get path [watch]</span><br><span class="line">    create [-s] [-e] path data acl</span><br><span class="line">    addauth scheme auth</span><br><span class="line">    quit</span><br><span class="line">    getAcl path</span><br><span class="line">    close</span><br><span class="line">    connect host:port</span><br></pre></td></tr></table></figure><h3 id="2-3-监听测试"><a href="#2-3-监听测试" class="headerlink" title="2.3 监听测试"></a>2.3 监听测试</h3><p><strong>注意点: 注册的监听只能使用一次, 监听完毕后需要重新注册.</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">=======节点值的变化的监听==========</span><br><span class="line">（1）在cs1主机上注册监听/app1节点数据变化</span><br><span class="line">[zk: localhost:2181(CONNECTED) 26] get /app1 watch</span><br><span class="line"></span><br><span class="line">（2）在cs2主机上修改/app1节点的数据</span><br><span class="line">[zk: localhost:2181(CONNECTED) 5] <span class="built_in">set</span> /app1  777</span><br><span class="line"></span><br><span class="line">（3）观察cs2主机收到数据变化的监听</span><br><span class="line">WATCHER::</span><br><span class="line">WatchedEvent state:SyncConnected <span class="built_in">type</span>:NodeDataChanged path:/app1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">=======节点的子节点变化的监听==========</span><br><span class="line">（1）在cs1主机上注册监听/app1节点的子节点变化</span><br><span class="line">[zk: localhost:2181(CONNECTED) 1] ls /app1 watch</span><br><span class="line">[aa0000000001, server101]</span><br><span class="line"></span><br><span class="line">（2）在cs2主机/app1节点上创建子节点</span><br><span class="line">[zk: localhost:2181(CONNECTED) 6] create /app1/bb 666</span><br><span class="line">Created /app1/bb</span><br><span class="line"></span><br><span class="line">（3）观察cs1主机收到子节点变化的监听</span><br><span class="line">WATCHER::</span><br><span class="line">WatchedEvent state:SyncConnected <span class="built_in">type</span>:NodeChildrenChanged path:/app1</span><br></pre></td></tr></table></figure><h1 id="3-ZK-内部机制"><a href="#3-ZK-内部机制" class="headerlink" title="3. ZK 内部机制"></a>3. ZK 内部机制</h1><h2 id="3-1-选举机制"><a href="#3-1-选举机制" class="headerlink" title="3.1  选举机制"></a>3.1  选举机制</h2><h4 id="3-1-1-zookeeper的选举机制（全新集群paxos）"><a href="#3-1-1-zookeeper的选举机制（全新集群paxos）" class="headerlink" title="3.1.1. zookeeper的选举机制（全新集群paxos）"></a>3.1.1. zookeeper的选举机制（全新集群paxos）</h4><p>以一个简单的例子来说明整个选举的过程.<br> 假设有五台服务器组成的zookeeper集群,它们的id从1-5,同时它们都是最新启动的,也就是没有历史数据,在存放数据量这一点上,都是一样的.假设这些服务器依序启动,来看看会发生什么.<br> 1) 服务器1启动,此时只有它一台服务器启动了,它发出去的报没有任何响应,所以它的选举状态一直是LOOKING状态<br> 2) 服务器2启动,它与最开始启动的服务器1进行通信,互相交换自己的选举结果,由于两者都没有历史数据,所以id值较大的服务器2胜出,但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3),所以服务器1,2还是继续保持LOOKING状态.<br> 3) 服务器3启动,根据前面的理论分析,服务器3成为服务器1,2,3中的老大,而与上面不同的是,此时有三台服务器选举了它,所以它成为了这次选举的leader.<br> 4) 服务器4启动,根据前面的分析,理论上服务器4应该是服务器1,2,3,4中最大的,但是由于前面已经有半数以上的服务器选举了服务器3,所以它只能接收当小弟的命了.<br> 5) 服务器5启动,同4一样,当小弟.</p><h4 id="3-1-2-非全新集群的选举机制-数据恢复"><a href="#3-1-2-非全新集群的选举机制-数据恢复" class="headerlink" title="3.1.2. 非全新集群的选举机制(数据恢复)"></a>3.1.2. 非全新集群的选举机制(数据恢复)</h4><p>那么，初始化的时候，是按照上述的说明进行选举的，但是当zookeeper运行了一段时间之后，有机器down掉，重新选举时，选举过程就相对复杂了。</p><p>需要加入数据id、leader id和逻辑时钟。</p><p>数据id：数据新的id就大，数据每次更新都会更新id。</p><p>Leader id：就是我们配置的myid中的值，每个机器一个。</p><p>逻辑时钟：这个值从0开始递增,每次选举对应一个值,也就是说:  如果在同一次选举中,那么这个值应该是一致的 ;  逻辑时钟值越大,说明这一次选举leader的进程更新.</p><p>选举的标准就变成：</p><p>​                     1、逻辑时钟小的选举结果被忽略，重新投票</p><p>​                     2、统一逻辑时钟后，数据id大的胜出</p><p>​                     3、数据id相同的情况下，leader id大的胜出</p><p>根据这个规则选出leader。</p><h2 id="3-2-stat-的结构"><a href="#3-2-stat-的结构" class="headerlink" title="3.2  stat 的结构"></a>3.2  stat 的结构</h2><ul><li>1）czxid- 引起这个znode创建的zxid，创建节点的事务的zxid<ul><li>每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。<br>事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于zxid2，那么zxid1在zxid2之前发生。</li></ul></li><li>2）ctime - znode被创建的毫秒数(从1970年开始)</li><li>3）mzxid - znode最后更新的zxid</li><li>4）mtime - znode最后修改的毫秒数(从1970年开始)</li><li>5）pZxid-znode最后更新的子节点zxid</li><li>6）cversion - znode子节点变化号，znode子节点修改次数</li><li>7）dataversion - znode数据变化号</li><li>8）aclVersion - znode访问控制列表的变化号</li><li>9）ephemeralOwner- 如果是临时节点，这个是znode拥有者的session id。如果不是临时节点则是0。</li><li>10）dataLength- znode的数据长度</li><li>11）numChildren - znode子节点数量</li></ul><h1 id="4-通过-JavaAPI-访问-ZK"><a href="#4-通过-JavaAPI-访问-ZK" class="headerlink" title="4. 通过 JavaAPI 访问 ZK"></a>4. 通过 JavaAPI 访问 ZK</h1><h3 id="4-1-添加Maven-依赖"><a href="#4-1-添加Maven-依赖" class="headerlink" title="4.1  添加Maven 依赖"></a>4.1  添加Maven 依赖</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">9.1[pom.xml]</span><br><span class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.it18zhang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>ZooKeeperDemo<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.zookeeper<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>zookeeper<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.4.9<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.11<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="4-2-Java-代码"><a href="#4-2-Java-代码" class="headerlink" title="4.2 Java 代码"></a>4.2 Java 代码</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line">* 对应关系:</span><br><span class="line">*  Linux   Java</span><br><span class="line">----------------------</span><br><span class="line">*  ls      getChildren</span><br><span class="line">*  get     getData</span><br><span class="line">*  set     setDAta</span><br><span class="line">*  create  create</span><br><span class="line">˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘˘</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> com.rox.zktest;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.data.ACL;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.data.Stat;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 对应关系:</span></span><br><span class="line"><span class="comment"> *  Linux   Java</span></span><br><span class="line"><span class="comment"> *  ls      getChildren</span></span><br><span class="line"><span class="comment"> *  get     getData</span></span><br><span class="line"><span class="comment"> *  set     setDAta</span></span><br><span class="line"><span class="comment"> *  create  create</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestZK</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">ls</span><span class="params">()</span> <span class="keyword">throws</span> IOException, KeeperException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 放三个就不行, 只能放2个目前来看</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        ZooKeeper zk = <span class="keyword">new</span> ZooKeeper(<span class="string">"cs1:2181,cs2:2181,cs3:2181"</span>, <span class="number">5000</span>, <span class="keyword">null</span>);</span><br><span class="line">        List&lt;String&gt; list = zk.getChildren(<span class="string">"/"</span>, <span class="keyword">null</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (String s : list) &#123;</span><br><span class="line">            System.out.println(s);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">lsAll</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            ls(<span class="string">"/"</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 列出指定 path 下的children</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> path</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">ls</span><span class="params">(String path)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        System.out.println(path);</span><br><span class="line"></span><br><span class="line">        ZooKeeper zk = <span class="keyword">new</span> ZooKeeper(<span class="string">"cs1:2181,cs2:2181,cs3:2181"</span>, <span class="number">5000</span>, <span class="keyword">null</span>);</span><br><span class="line">        List&lt;String&gt; list = zk.getChildren(path, <span class="keyword">null</span>);</span><br><span class="line">        <span class="keyword">if</span> (list == <span class="keyword">null</span> || list.isEmpty()) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (String s : list) &#123;</span><br><span class="line">            <span class="comment">// 先输出 children</span></span><br><span class="line">            <span class="keyword">if</span> (path.equals(<span class="string">"/"</span> )) &#123;</span><br><span class="line">                ls(path + s);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                ls(path + <span class="string">"/"</span> + s);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 设置数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span>  <span class="keyword">void</span>  <span class="title">setData</span><span class="params">()</span> <span class="keyword">throws</span>  Exception </span>&#123;</span><br><span class="line">        ZooKeeper zk = <span class="keyword">new</span> ZooKeeper(<span class="string">"cs1:2181"</span>, <span class="number">5000</span>, <span class="keyword">null</span>);</span><br><span class="line">        zk.setData(<span class="string">"/a"</span>,<span class="string">"tomaslee"</span>.getBytes(),<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 创建临时节点</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span>  <span class="title">createEPHEMERAL</span><span class="params">()</span> <span class="keyword">throws</span>  Exception </span>&#123;</span><br><span class="line">        ZooKeeper zk = <span class="keyword">new</span> ZooKeeper(<span class="string">"cs1:2181"</span>, <span class="number">5000</span>, <span class="keyword">null</span>);</span><br><span class="line">        zk.create(<span class="string">"/c/c1"</span>, <span class="string">"tom"</span>.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE,</span><br><span class="line">                CreateMode.EPHEMERAL);</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"hello"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 创建观察者</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span>  <span class="title">testWatch</span><span class="params">()</span> <span class="keyword">throws</span>  Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> ZooKeeper zk = <span class="keyword">new</span> ZooKeeper(<span class="string">"cs1:2181,cs2:2181,cs3:2181"</span>, <span class="number">5000</span>, <span class="keyword">null</span>);</span><br><span class="line"></span><br><span class="line">        Stat st = <span class="keyword">new</span> Stat();</span><br><span class="line"></span><br><span class="line">        Watcher w = <span class="keyword">null</span>;</span><br><span class="line">        w = <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    System.out.println(<span class="string">"数据改了..."</span>);</span><br><span class="line">                    zk.getData(<span class="string">"/a"</span>,<span class="keyword">this</span>,<span class="keyword">null</span>);</span><br><span class="line">                &#125;<span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">byte</span>[] data = zk.getData(<span class="string">"/a"</span>,w,st);</span><br><span class="line">        System.out.println(<span class="keyword">new</span> String(data));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Zookeeper-的安装-amp-概述&quot;&gt;&lt;a href=&quot;#1-Zookeeper-的安装-amp-概述&quot; class=&quot;headerlink&quot; title=&quot;1.Zookeeper 的安装&amp;amp;概述&quot;&gt;&lt;/a&gt;1.Zookeeper 的安装&amp;amp;
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="Zookeeper" scheme="https://airpoet.github.io/categories/Hadoop/Zookeeper/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="Zookeeper" scheme="https://airpoet.github.io/tags/Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop-HA高可用集群QJM搭建</title>
    <link href="https://airpoet.github.io/2018/06/22/Hadoop/0-Hadoop/Hadoop-HA%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"/>
    <id>https://airpoet.github.io/2018/06/22/Hadoop/0-Hadoop/Hadoop-HA高可用集群搭建/</id>
    <published>2018-06-21T16:38:59.993Z</published>
    <updated>2018-06-30T15:29:15.618Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-此教程默认已经搭建好完全分布式"><a href="#1-此教程默认已经搭建好完全分布式" class="headerlink" title="1.此教程默认已经搭建好完全分布式"></a>1.此教程默认已经搭建好完全分布式</h3><h3 id="2-Zookeeper-集群搭建"><a href="#2-Zookeeper-集群搭建" class="headerlink" title="2. Zookeeper 集群搭建"></a>2. Zookeeper 集群搭建</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">配置完全分布式zk集群</span><br><span class="line">---------------------</span><br><span class="line">    1.挑选3台主机</span><br><span class="line">        cs1 ~ cs3</span><br><span class="line">    2.每台机器都安装zk</span><br><span class="line">        tar</span><br><span class="line">        环境变量</span><br><span class="line"></span><br><span class="line">    3.配置zk配置文件</span><br><span class="line">        cs1 ~ cs3</span><br><span class="line">        [/home/ap/apps/zk/conf/zoo.cfg]</span><br><span class="line">        ...</span><br><span class="line">        dataDir=/home/ap/zookeeper</span><br><span class="line"></span><br><span class="line">    4.在每台主机的/home/centos/zookeeper中添加myid,内容分别是1,2,3</span><br><span class="line">        [cs1]</span><br><span class="line">        $&gt;<span class="built_in">echo</span> 1 &gt; /home/ap/zookeeper/myid</span><br><span class="line">        [cs2]</span><br><span class="line">        $&gt;<span class="built_in">echo</span> 2 &gt; /home/ap/zookeeper/myid</span><br><span class="line">        [cs3]</span><br><span class="line">        $&gt;<span class="built_in">echo</span> 3 &gt; /home/ap/zookeeper/myid</span><br><span class="line"></span><br><span class="line">    5.启动服务器集群 </span><br><span class="line">        $&gt;zkServer.sh start</span><br><span class="line">            </span><br><span class="line">    6.查看每台服务器的状态</span><br><span class="line">        $&gt;zkServer.sh status</span><br><span class="line">* 注意: 如果有3台机器, 只启动一台的话, 会显示如下, 因为根据配置, 有3台机器, 此时只有一台启动, 没有过半数, 整个集群是挂掉的</span><br><span class="line">* 只有启动的机器数量超过配置的半数, zk 集群才有效.</span><br></pre></td></tr></table></figure><h3 id="3-HA-集群搭建"><a href="#3-HA-集群搭建" class="headerlink" title="3.HA 集群搭建"></a>3.HA 集群搭建</h3><h4 id="首先声明"><a href="#首先声明" class="headerlink" title="首先声明"></a>首先声明</h4><p><strong>笔者用的6台主机,  主机名 <code>cs1-cs6</code>,  用户名为<code>ap</code>, 可以对照改为自己的主机名&amp;用户名</strong></p><p>另外, 搭建 HA 不会影响原来的完全分布式, 具体操作会在下面告知.<br><strong>hadoop 安装目录层级结构:</strong></p><ul><li><code>/home/ap/apps/hadoop/etc/hadoop/hdfs-site.xml</code></li></ul><p><strong>data 目录层级结构:</strong></p><ul><li><code>cs1</code>: <code>/home/ap/hadoopdata/namenode/current/edits_00....</code> </li><li><code>cs2</code>: <code>/home/ap/hadoopdata/datanode/current/BP-15...</code></li></ul><p><strong>可以对照参考</strong></p><p><br></p><p><strong>集群结构如下</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-21-164723.png" alt="image-20180622004722436"></p><hr><h4 id="开始搭建"><a href="#开始搭建" class="headerlink" title="开始搭建"></a>开始搭建</h4><p><strong>首先保证 各节点直接的 ssh 免密登录没问题</strong></p><ul><li>如果非生产环境, 可以同时把 <code>.ssh</code>删掉后, 全部重新生成 <code>ssh-keygen</code>,  同时相互发送, 这样操作最简单, 效率最高. </li></ul><p><strong>其次上代码了</strong></p><ol><li>把原本<code>/home/ap/apps/hadoop/etc/hadoop</code>中的 <code>hadoop</code>目录改为<code>full</code>,意思是完全分布式.  </li><li><code>cp -r full ha</code>, 复制一份 full 为 ha, 在这份配置文件中配置 HA</li><li><code>ln -s /home/ap/apps/hadoop/etc/ha  /home/ap/apps/hadoop/etc/hadoop</code>, 用一个软链接hadoop 指向 ha</li><li>配置 <code>/home/ap/apps/hadoop/etc/ha/hdfs-site.xml</code></li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">[hdfs-site.xml]</span><br><span class="line">------------------------------------------------------------------</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定 2个 namenode 的命名空间 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- myucluster下的名称节点两个id --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 配置每个nn的rpc地址。 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cs1:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cs6:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 配置webui端口 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cs1:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cs6:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 名称节点共享编辑目录. --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://cs2:8485;cs3:8485;cs4:8485/mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- java类，client使用它判断哪个节点是激活态。 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 脚本列表或者java类，在容灾保护激活态的nn. --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">                sshfence</span><br><span class="line">                shell(/bin/true)</span><br><span class="line">        <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- ssh免密登陆 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/ap/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 配置 sshfence 隔离机制超时时间(可不配) --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.connect-timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>30000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 配置JN存放edit的本地路径。 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/ap/hadoopdata/journal<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><ol start="5"><li>配置 <code>core-site.xml</code>, 这里给出完整配置 (目前不包括 Hive 配置)</li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[core-site.xml]</span><br><span class="line">------------------------------------------------------------</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!-- 指定 hdfs 的 nameservice 为 mycluster --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定 hadoop 工作目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/ap/hadoopdata<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定zk集群访问地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cs1:2181,cs2:2181,cs3:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><ol start="6"><li>配置<code>mapred-site.xml</code></li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[mapred-site.xml]</span><br><span class="line">-------------------</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!-- 指定 mr 框架为 yarn 方式 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 设置 mapreduce 的历史服务器地址和端口号 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cs1:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- mapreduce 历史服务器的 web 访问地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cs1:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><ol start="6"><li>把<code>/home/ap/apps/hadoop/etc/*</code>发给其他所有节点 2-6<ul><li>注意: 软链接scp 的时候会有问题, 最终保证每个节点跟 cs1一样就可以了,可以每个节点单独修改, 也可以写脚本一起修改</li><li><code>ln -sfT /home/ap/apps/hadoop/etc/ha  /home/ap/apps/hadoop/etc/hadoop</code></li></ul></li><li><strong>部署细节</strong></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">0.在 zk 节点启动 zkServer</span><br><span class="line">cs1-cs3: $&gt;zkServer.sh start</span><br><span class="line"></span><br><span class="line">1.在jn节点分别启动jn进程</span><br><span class="line">$&gt;hadoop-daemon.sh start journalnode</span><br><span class="line"></span><br><span class="line">2.启动jn之后，在两个NN之间进行disk元数据同步</span><br><span class="line">    a)如果是全新集群，先format文件系统,只需要在一个nn上执行。</span><br><span class="line">    [cs1]</span><br><span class="line">    $&gt;hadoop namenode -format</span><br><span class="line"></span><br><span class="line">    b)如果将非HA集群转换成HA集群，复制原NN的metadata到另一个nn.</span><br><span class="line">        1.步骤一</span><br><span class="line">        [cs1]</span><br><span class="line">        $&gt;scp -r /home/centos/hadoop/dfs ap@cs6:/home/centos/hadoop/</span><br><span class="line"></span><br><span class="line">        2.步骤二</span><br><span class="line">        在新的nn(未格式化的nn)上运行一下命令，实现待命状态引导。</span><br><span class="line">        [cs6]</span><br><span class="line">        $&gt;hdfs namenode -bootstrapStandby        //需要cs1为启动状态,提示是否格式化,选择N.</span><br><span class="line"></span><br><span class="line">3.在一个NN上执行以下命令，完成edit日志到jn节点的传输。</span><br><span class="line">$&gt;hdfs namenode -initializeSharedEdits</span><br><span class="line"><span class="comment">#查看cs2,cs3,cs4 这几个 jn 节点是否有edit数据.</span></span><br><span class="line"></span><br><span class="line">4.启动所有节点.</span><br><span class="line">[cs1]</span><br><span class="line">$&gt;hadoop-daemon.sh start namenode        //启动名称节点</span><br><span class="line">$&gt;hadoop-daemons.sh start datanode        //启动所有数据节点</span><br><span class="line"></span><br><span class="line">[2,3,4]</span><br><span class="line">$&gt;hadoop-daemon.sh start journalnode</span><br><span class="line"></span><br><span class="line">[cs6]</span><br><span class="line">$&gt;hadoop-daemon.sh start namenode        //启动名称节点</span><br></pre></td></tr></table></figure><ol start="8"><li><strong>HA 管理</strong></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看web 界面, 是否是2个 standby 状态</span></span><br><span class="line">http://cs1:50070/</span><br><span class="line">http://cs6:50070/</span><br><span class="line"></span><br><span class="line">hdfs haadmin : 查看 ha 帮助</span><br><span class="line">-----------------</span><br><span class="line">$&gt;hdfs haadmin -transitionToActive nn1                //切成激活态</span><br><span class="line">$&gt;hdfs haadmin -transitionToStandby nn1                //切成待命态</span><br><span class="line">$&gt;hdfs haadmin -transitionToActive --forceactive nn2//强行激活</span><br><span class="line">$&gt;hdfs haadmin -failover nn1 nn2                    //模拟容灾演示,从nn1切换到nn2</span><br></pre></td></tr></table></figure><ol start="9"><li><strong>加入 Zookeeper 容灾服务 zkfc</strong></li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 1.部署容灾 --&gt;</span></span><br><span class="line">------------------------------</span><br><span class="line">a.停止所有进程</span><br><span class="line">$&gt;stop-all.sh</span><br><span class="line"></span><br><span class="line">b.配置hdfs-site.xml，启用自动容灾.</span><br><span class="line">[hdfs-site.xml]</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">c.配置core-site.xml，指定zk的连接地址.</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>cs1:2181,cs2:2181,cs3:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">d.分发以上两个文件到所有节点。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 2.登录其中的一台NN(s201),在ZK中初始化HA状态,</span></span><br><span class="line"><span class="comment"> 创建 namenode 的 命名空间节点 mycluster --&gt;</span></span><br><span class="line">------------------------------------</span><br><span class="line">$&gt;hdfs zkfc -formatZK</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 3.启动hdfs进程. --&gt;</span>    </span><br><span class="line">$&gt;start-dfs.sh</span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 4.查看 webUI, 是否有一台自动切换为 active 状态了 --&gt;</span></span><br><span class="line">http://cs1:50070/</span><br><span class="line">http://cs6:50070/</span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 5.测试自动容灾(如果cs6是活跃节点) --&gt;</span>    </span><br><span class="line">$&gt;kill -9  cs6的 namenode进程号</span><br><span class="line">观察 cs1:50070的状态变化</span><br></pre></td></tr></table></figure><ol start="10"><li><strong>配置RM的HA自动容灾</strong></li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line">1.配置yarn-site.xml</span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 是否允许高可用 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- Identifies the cluster. Used by the elector to ensure an RM doesn’t take over as Active for another cluster.  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cluster1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- RM 的 id --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 2台 RM 的宿主 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cs1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cs6<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 配置web界面 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cs1:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cs6:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- zookeeper 地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>cs1:2181,cs2:2181,cs3:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- YARN 集群为 MapReduce 程序提供的 shuffle 服务(原本的)  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- ===========以下是可选的============= --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 开启 YARN 集群的日志聚合功能 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- YARN 集群的聚合日志最长保留时长 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>86400<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 启用自动恢复 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定 resourcemanager 的状态信息存储在 zookeeper 集群上--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.store.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2.使用管理命令</span><br><span class="line"><span class="comment">&lt;!-- 查看状态 --&gt;</span></span><br><span class="line">$&gt;yarn rmadmin -getServiceState rm1</span><br><span class="line"><span class="comment">&lt;!-- 切换状态到standby --&gt;</span></span><br><span class="line">$&gt;yarn rmadmin -transitionToStandby rm1</span><br><span class="line"></span><br><span class="line">3.启动yarn集群</span><br><span class="line">$&gt;start-yarn.sh</span><br><span class="line"></span><br><span class="line">4.hadoop没有启动两个resourcemanager,需要手动启动另外一个</span><br><span class="line">$&gt;yarn-daemon.sh start resourcemanager</span><br><span class="line"></span><br><span class="line">5.查看webUI, 点击 About, 查看 active 或 standby</span><br><span class="line">http://cs1:8088</span><br><span class="line">http://cs6:8088</span><br><span class="line"></span><br><span class="line">6.做容灾模拟.</span><br><span class="line">kill -9 活跃的 RM 端口号</span><br><span class="line"></span><br><span class="line">7.注意: 如果容灾失败, 检查下每台主机时间是否同步</span><br><span class="line">$&gt;sudo ntpdate ntp1.aliyun.com</span><br></pre></td></tr></table></figure><p><strong>至此, 大功告成</strong></p><hr><h3 id="4-HA-集群的启动-关闭"><a href="#4-HA-集群的启动-关闭" class="headerlink" title="4.HA 集群的启动/关闭"></a>4.HA 集群的启动/关闭</h3><h4 id="4-1-HA-的启动"><a href="#4-1-HA-的启动" class="headerlink" title="4.1 HA 的启动"></a>4.1 HA 的启动</h4><p><strong>一:单点启动</strong></p><ol start="0"><li><p>启动 zk 服务 QuorumPeerMain, 否则后面 RM 会起不来,连不上 ZK 服务<br>cs1-cs3: $&gt; <code>zkServer.sh start</code></p></li><li><p>启动 namenode/datanode<br> cs1,cs6: $&gt; <code>hadoop-daemon.sh start namenode</code><br> cs1/cs6: $&gt; <code>hadoop-daemons.sh start datanode</code><br> cs6:     $&gt; <code>hadoop-daemon.sh start namenode</code></p></li></ol><ol start="2"><li>启动 journalnode<br> cs2-cs4: $&gt; <code>hadoop-daemon.sh start journalnode</code></li></ol><ol start="3"><li>启动RM (RM 会自动选出一个 active)<br> cs1,cs6: $&gt; <code>yarn-daemon.sh start resourcemanager</code><br> cs1/cs6: $&gt; <code>yarn-daemons.sh start nodemanager</code></li></ol><ol start="4"><li><p>启动 zk 的 DFSZKFailoverController<br> cs1,cs6: $&gt; <code>hadoop-daemon.sh start zkfc</code></p></li><li><p>此外可以启动MapReduce 的历史任务服务器</p><p>[ap@cs1]$&gt; <code>mr-jobhistory-daemon.sh start historyserver</code></p><p>然后访问配置的 <code>http://cs1:19888/jobhistory</code></p></li></ol><p><strong>二:懒汉启动</strong></p><ol start="0"><li><p>启动 zk 服务 QuorumPeerMain, 否则后面 RM 会起不来,连不上 ZK 服务<br>cs1-cs3: $&gt; <code>zkServer.sh start</code></p></li><li><p>执行启动全部</p><p>cs1: $&gt;  <code>start-all.sh</code>(RM 节点)</p><p>或者, 使用新的启动方式</p><p>cs1: $&gt; <code>start-dfs.sh</code>(任意节点)</p><p>cs1: $&gt; <code>start-yarn.sh</code>(在 RM 节点)</p></li><li><p>另一个 RM 节点不会自己启动,要手动启动</p><p>cs6: $&gt;  <code>yarn-daemon.sh start resourcemanager</code></p></li></ol><p>三: 启动完成后</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">============= cs1 jps =============</span><br><span class="line">5696 QuorumPeerMain</span><br><span class="line">6641 DFSZKFailoverController</span><br><span class="line">6338 NameNode</span><br><span class="line">6756 ResourceManager</span><br><span class="line">6873 Jps</span><br><span class="line">============= cs2 jps =============</span><br><span class="line">10849 Jps</span><br><span class="line">10722 NodeManager</span><br><span class="line">10631 JournalNode</span><br><span class="line">10541 DataNode</span><br><span class="line">127934 QuorumPeerMain</span><br><span class="line">============= cs3 jps =============</span><br><span class="line">630 NodeManager</span><br><span class="line">758 Jps</span><br><span class="line">535 JournalNode</span><br><span class="line">443 DataNode</span><br><span class="line">117503 QuorumPeerMain</span><br><span class="line">============= cs4 jps =============</span><br><span class="line">79589 Jps</span><br><span class="line">79462 NodeManager</span><br><span class="line">79368 JournalNode</span><br><span class="line">79278 DataNode</span><br><span class="line">============= cs5 jps =============</span><br><span class="line">23655 Jps</span><br><span class="line">23529 NodeManager</span><br><span class="line">23423 DataNode</span><br><span class="line">============= cs6 jps =============</span><br><span class="line">35680 Jps</span><br><span class="line">35506 NameNode</span><br><span class="line">35608 DFSZKFailoverController</span><br><span class="line">21455 ResourceManager</span><br></pre></td></tr></table></figure><h4 id="4-2-HA-的关闭"><a href="#4-2-HA-的关闭" class="headerlink" title="4.2 HA 的关闭"></a>4.2 HA 的关闭</h4><p>在 一台NN 上<code>stop-all.sh</code>, 注意 zk 的 server- <code>QuorumPeerMain</code>不会停掉</p><h3 id="5-在集群实现时间同步-root-用户操作"><a href="#5-在集群实现时间同步-root-用户操作" class="headerlink" title="5. 在集群实现时间同步(root 用户操作)"></a>5. 在集群实现时间同步(root 用户操作)</h3><h4 id="思路"><a href="#思路" class="headerlink" title="思路:"></a>思路:</h4><p><strong>时间同步的方式：找一个机器，作为时间服务器，所有的机器与这台集群时间进行定时的同步，比如，每隔十分钟，同步一次时间。</strong> </p><h4 id="5-1-在cs1上修改ntp服务"><a href="#5-1-在cs1上修改ntp服务" class="headerlink" title="5.1 在cs1上修改ntp服务"></a>5.1 在<code>cs1</code>上修改<code>ntp</code>服务</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">1) 检查 ntp 是否安装</span><br><span class="line"><span class="comment">#root&gt; rpm -qa|grep ntp</span></span><br><span class="line">---</span><br><span class="line">ntp-4.2.6p5-10.el6.centos.x86_64</span><br><span class="line">fontpackages-filesystem-1.41-1.1.el6.noarch</span><br><span class="line">ntpdate-4.2.6p5-10.el6.centos.x86_64</span><br><span class="line"><span class="comment"># 出现上面的3个文件, 就是安装了; 如果没有的话, 用 yum 装一下</span></span><br><span class="line"><span class="comment"># root&gt; yum inatall -y ntp</span></span><br><span class="line"></span><br><span class="line">2) 修改 ntp 配置文件</span><br><span class="line"><span class="comment">#root&gt; vi /etc/ntp.conf</span></span><br><span class="line"></span><br><span class="line">修改内容如下</span><br><span class="line">    a）修改1（设置本地网络上的主机不受限制。）</span><br><span class="line">    <span class="comment">#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap</span></span><br><span class="line">    为</span><br><span class="line">    restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap</span><br><span class="line">    b）修改2（设置为不采用公共的服务器）</span><br><span class="line">    server 0.centos.pool.ntp.org iburst</span><br><span class="line">    server 1.centos.pool.ntp.org iburst</span><br><span class="line">    server 2.centos.pool.ntp.org iburst</span><br><span class="line">    server 3.centos.pool.ntp.org iburst</span><br><span class="line">    为</span><br><span class="line">    <span class="comment">#server 0.centos.pool.ntp.org iburst</span></span><br><span class="line">    <span class="comment">#server 1.centos.pool.ntp.org iburst</span></span><br><span class="line">    <span class="comment">#server 2.centos.pool.ntp.org iburst</span></span><br><span class="line">    <span class="comment">#server 3.centos.pool.ntp.org iburst</span></span><br><span class="line">    c）添加3（添加默认的一个内部时钟数据，使用它为局域网用户提供服务。）</span><br><span class="line">    server 127.127.1.0</span><br><span class="line">    fudge 127.127.1.0 stratum 10</span><br><span class="line"></span><br><span class="line">3）修改/etc/sysconfig/ntpd 文件</span><br><span class="line"><span class="comment">#root&gt; vim /etc/sysconfig/ntpd</span></span><br><span class="line">增加内容如下（让硬件时间与系统时间一起同步）</span><br><span class="line">SYNC_HWCLOCK=yes</span><br><span class="line"></span><br><span class="line">4）重新启动ntpd</span><br><span class="line"><span class="comment">#root&gt; service ntpd status</span></span><br><span class="line">ntpd 已停</span><br><span class="line"></span><br><span class="line"><span class="comment">#root&gt; service ntpd start</span></span><br><span class="line">正在启动 ntpd：    </span><br><span class="line"></span><br><span class="line">5) 执行开机启动 ntpd 服务</span><br><span class="line"><span class="comment">#root&gt; chkconfig ntpd on</span></span><br><span class="line">查看 ntpd 服务开机启动的状态</span><br><span class="line"><span class="comment">#root&gt; chkconfig --list ntpd</span></span><br></pre></td></tr></table></figure><h4 id="5-2-其它机器上"><a href="#5-2-其它机器上" class="headerlink" title="5.2 其它机器上"></a>5.2 其它机器上</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1）在其他所有机器配置10分钟与时间服务器同步一次</span><br><span class="line"><span class="comment">#编写定时任务脚本</span></span><br><span class="line"><span class="comment">#root&gt; crontab -e </span></span><br><span class="line">*/10 * * * * /usr/sbin/ntpdate hadoop102</span><br><span class="line"></span><br><span class="line">2) 修改任意机器时间</span><br><span class="line"><span class="comment">#root&gt; date -s "2017-9-11 11:11:11"</span></span><br><span class="line"></span><br><span class="line">3) 十分钟后查看机器时间是否与cs1 同步</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-此教程默认已经搭建好完全分布式&quot;&gt;&lt;a href=&quot;#1-此教程默认已经搭建好完全分布式&quot; class=&quot;headerlink&quot; title=&quot;1.此教程默认已经搭建好完全分布式&quot;&gt;&lt;/a&gt;1.此教程默认已经搭建好完全分布式&lt;/h3&gt;&lt;h3 id=&quot;2-Zoo
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="hadoop" scheme="https://airpoet.github.io/categories/Hadoop/hadoop/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>i-Hive-Practice-1 影评练习</title>
    <link href="https://airpoet.github.io/2018/06/19/Hadoop/3-Hive/i-Hive-Practice-1-%E5%BD%B1%E8%AF%84%E7%BB%83%E4%B9%A0/"/>
    <id>https://airpoet.github.io/2018/06/19/Hadoop/3-Hive/i-Hive-Practice-1-影评练习/</id>
    <published>2018-06-19T11:56:19.763Z</published>
    <updated>2018-06-19T15:33:56.958Z</updated>
    
    <content type="html"><![CDATA[<p><strong>现有如此三份数据：</strong><br><strong>1、users.dat    数据格式为：  2::M::56::16::70072</strong><br><strong>对应字段为：UserID BigInt, Gender String, Age Int, Occupation String, Zipcode String</strong><br><strong>对应字段中文解释：用户id，性别，年龄，职业，邮政编码</strong></p><p><strong>2、movies.dat        数据格式为： 2::Jumanji (1995)::Adventure|Children’s|Fantasy</strong><br><strong>对应字段为：MovieID BigInt, Title String, Genres String</strong><br><strong>对应字段中文解释：电影ID，电影名字，电影类型</strong></p><p><strong>3、ratings.dat        数据格式为：  1::1193::5::978300760</strong><br><strong>对应字段为：UserID BigInt, MovieID BigInt, Rating Double, Timestamped String</strong><br><strong>对应字段中文解释：用户ID，电影ID，评分，评分时间戳</strong></p><p>题目要求：</p><p>数据要求：<br>（1）写shell脚本清洗数据。（hive不支持解析多字节的分隔符，也就是说hive只能解析’:’, 不支持解析’::’，所以用普通方式建表来使用是行不通的，要求对数据做一次简单清洗）<br>（2）使用Hive能解析的方式进行</p><p>Hive要求：<br><strong>（1）正确建表，导入数据（三张表，三份数据），并验证是否正确</strong></p><p><strong>（2）求被评分次数最多的10部电影，并给出评分次数（电影名，评分次数）</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">思路:</span><br><span class="line">1. 分组</span><br><span class="line"><span class="keyword">select</span> movieid, <span class="keyword">count</span>(movieid) rateCount <span class="keyword">from</span> ratings </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> movieid <span class="keyword">limit</span> <span class="number">10</span>;</span><br><span class="line">2. 排序</span><br><span class="line"><span class="keyword">select</span> movieid, <span class="keyword">count</span>(movieid) rateCount <span class="keyword">from</span> ratings </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> movieid <span class="keyword">order</span> <span class="keyword">by</span> rateCount <span class="keyword">desc</span> <span class="keyword">limit</span> <span class="number">10</span>;</span><br><span class="line">3.join</span><br><span class="line"><span class="keyword">select</span> a.movieid mvid, b.Title mvtitle, a.rateCount mvratecount <span class="keyword">from</span> </span><br><span class="line">(<span class="keyword">select</span> movieid, <span class="keyword">count</span>(movieid) rateCount <span class="keyword">from</span> ratings </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> movieid <span class="keyword">order</span> <span class="keyword">by</span> rateCount <span class="keyword">desc</span> <span class="keyword">limit</span> <span class="number">10</span>) a</span><br><span class="line"><span class="keyword">join</span> movies b <span class="keyword">where</span> a.movieid=b.movieid;</span><br><span class="line">==============================</span><br><span class="line"></span><br><span class="line"> 完整的</span><br><span class="line"><span class="comment">------------</span></span><br><span class="line"><span class="keyword">select</span> a.movieid mvid, b.Title mvtitle, a.rateCount mvratecount <span class="keyword">from</span> </span><br><span class="line">(<span class="keyword">select</span> movieid, <span class="keyword">count</span>(movieid) rateCount <span class="keyword">from</span> ratings </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> movieid <span class="keyword">order</span> <span class="keyword">by</span> rateCount <span class="keyword">desc</span> <span class="keyword">limit</span> <span class="number">10</span>) a</span><br><span class="line"><span class="keyword">join</span> movies b <span class="keyword">where</span> a.movieid=b.movieid;</span><br><span class="line"></span><br><span class="line"><span class="comment">--结果---</span></span><br><span class="line">+<span class="comment">-------+----------------------------------------------------+--------------+</span></span><br><span class="line">| mvid  |                      mvtitle                       | mvratecount  |</span><br><span class="line">+<span class="comment">-------+----------------------------------------------------+--------------+</span></span><br><span class="line">| 2858  | American Beauty (1999)                             | 3428         |</span><br><span class="line">| 260   | Star Wars: Episode IV - A New Hope (1977)          | 2991         |</span><br><span class="line">| 1196  | Star Wars: Episode V - The Empire Strikes Back (1980) | 2990         |</span><br><span class="line">| 1210  | Star Wars: Episode VI - Return of the Jedi (1983)  | 2883         |</span><br><span class="line">| 480   | Jurassic Park (1993)                               | 2672         |</span><br><span class="line">| 2028  | Saving Private Ryan (1998)                         | 2653         |</span><br><span class="line">| 589   | Terminator 2: Judgment Day (1991)                  | 2649         |</span><br><span class="line">| 2571  | Matrix, The (1999)                                 | 2590         |</span><br><span class="line">| 1270  | Back to the Future (1985)                          | 2583         |</span><br><span class="line">| 593   | Silence of the Lambs, The (1991)                   | 2578         |</span><br><span class="line">+<span class="comment">-------+----------------------------------------------------+--------------+</span></span><br></pre></td></tr></table></figure><p><strong>（3）分别求男性，女性当中评分最高的10部电影（性别，电影名，影评分）</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># 注意,这里的 avg(r.rating) 是因为group by了 gender,title, 也就是</span><br><span class="line"># 说有 gender,title 分组, 此时可能对应此组的有多个值,</span><br><span class="line"># 这里就是 相同性别评价相同电影的, 评分有多个, 此时就要使用聚合函数,得出唯一值</span><br><span class="line"># 因此就使用了 avg(r.rating)</span><br><span class="line"><span class="keyword">select</span> u.gender, m.title, <span class="keyword">avg</span>(r.rating) rr</span><br><span class="line"><span class="keyword">from</span> ratings r </span><br><span class="line"><span class="keyword">join</span> <span class="keyword">users</span> u <span class="keyword">on</span> r.userid=u.userid </span><br><span class="line"><span class="keyword">join</span> movies m <span class="keyword">on</span> r.movieid=m.movieid</span><br><span class="line"><span class="keyword">where</span> u.gender = <span class="string">'M'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> gender,title</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> rr <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line">+<span class="comment">-----------+--------------------------------------------+------+</span></span><br><span class="line">| u.gender  |                  m.title                   |  rr  |</span><br><span class="line">+<span class="comment">-----------+--------------------------------------------+------+</span></span><br><span class="line">| M         | Schlafes Bruder (Brother of Sleep) (1995)  | 5.0  |</span><br><span class="line">| M         | Small Wonders (1996)                       | 5.0  |</span><br><span class="line">| M         | Lured (1947)                               | 5.0  |</span><br><span class="line">| M         | Bells, The (1926)                          | 5.0  |</span><br><span class="line">| M         | Dangerous Game (1993)                      | 5.0  |</span><br><span class="line">| M         | Baby, The (1973)                           | 5.0  |</span><br><span class="line">| M         | Gate of Heavenly Peace, The (1995)         | 5.0  |</span><br><span class="line">| M         | Follow the Bitch (1998)                    | 5.0  |</span><br><span class="line">| M         | Ulysses (Ulisse) (1954)                    | 5.0  |</span><br><span class="line">| M         | Angela (1995)                              | 5.0  |</span><br><span class="line">+<span class="comment">-----------+--------------------------------------------+------+</span></span><br></pre></td></tr></table></figure><p><strong>（4）求movieid = 2116这部电影各年龄段（因为年龄就只有7个，就按这个7个分就好了）的平均影评（年龄段，影评分）</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 注意: 没有歧义的字段, 可以不用指明是谁的属性</span><br><span class="line"><span class="keyword">select</span> age, <span class="keyword">avg</span>(r.rating) avgrating <span class="keyword">from</span> ratings r </span><br><span class="line"><span class="keyword">join</span> <span class="keyword">users</span> u <span class="keyword">on</span> r.userid=u.userid</span><br><span class="line"><span class="keyword">where</span> movieid=<span class="number">2116</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> age;</span><br></pre></td></tr></table></figure><p><strong>（5）求最喜欢看电影（影评次数最多）的那位女性评最高分的10部电影的平均影评分（观影者，电影名，影评分）</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"># 1.先拿到 影评次数最多的女性(id)</span><br><span class="line">0: jdbc:hive2://cs2:10000&gt; select UserID, count(UserID) count from ratings</span><br><span class="line">. . . . . . . . . . . . .&gt; group by UserID</span><br><span class="line">. . . . . . . . . . . . .&gt; order by count desc</span><br><span class="line">. . . . . . . . . . . . .&gt; limit 1;</span><br><span class="line">+<span class="comment">---------+--------+</span></span><br><span class="line">| userid  | count  |</span><br><span class="line">+<span class="comment">---------+--------+</span></span><br><span class="line">| 4169    | 2314   |</span><br><span class="line">+<span class="comment">---------+--------+</span></span><br><span class="line"></span><br><span class="line"># 2.拿到此人评分最高的10部电影, 此人 userid, 电影 id, 此人评分</span><br><span class="line">##  一定要去重</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">distinct</span>(MovieID) MovieID, Rating <span class="keyword">from</span> ratings <span class="keyword">where</span> UserID=<span class="number">4169</span> </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> Rating <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line">+<span class="comment">----------+---------+</span></span><br><span class="line">| movieid  | rating  |</span><br><span class="line">+<span class="comment">----------+---------+</span></span><br><span class="line">| 78       | 5.0     |</span><br><span class="line">| 73       | 5.0     |</span><br><span class="line">| 72       | 5.0     |</span><br><span class="line">| 58       | 5.0     |</span><br><span class="line">| 55       | 5.0     |</span><br><span class="line">| 50       | 5.0     |</span><br><span class="line">| 41       | 5.0     |</span><br><span class="line">| 36       | 5.0     |</span><br><span class="line">| 25       | 5.0     |</span><br><span class="line">| 17       | 5.0     |</span><br><span class="line">+<span class="comment">----------+---------+</span></span><br><span class="line"></span><br><span class="line">3.这10部电影的(观影者，电影名，平均影评分)</span><br><span class="line">观影者还没写</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> t.MovieID, m.Title, <span class="keyword">avg</span>(r.rating) avgrating <span class="keyword">from</span> topten t </span><br><span class="line"><span class="keyword">join</span> ratings r <span class="keyword">on</span> t.MovieID=r.MovieID</span><br><span class="line"><span class="keyword">join</span> movies m <span class="keyword">on</span> t.MovieID=m.MovieID</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> t.MovieID,m.Title;</span><br><span class="line"></span><br><span class="line">+<span class="comment">------------+-----------------------------------------------+---------------------+</span></span><br><span class="line">| t.movieid  |                    m.title                    |      avgrating      |</span><br><span class="line">+<span class="comment">------------+-----------------------------------------------+---------------------+</span></span><br><span class="line">| 3849       | Spiral Staircase, The (1946)                  | 4.046511627906977   |</span><br><span class="line">| 3870       | Our Town (1940)                               | 3.857142857142857   |</span><br><span class="line">| 3871       | Shane (1953)                                  | 3.839344262295082   |</span><br><span class="line">| 3893       | Nurse Betty (2000)                            | 3.5026833631484795  |</span><br><span class="line">| 3897       | Almost Famous (2000)                          | 4.22635814889336    |</span><br><span class="line">| 3910       | Dancer in the Dark (2000)                     | 3.82                |</span><br><span class="line">| 3927       | Fantastic Voyage (1966)                       | 3.5804597701149423  |</span><br><span class="line">| 3928       | Abbott and Costello Meet Frankenstein (1948)  | 3.441747572815534   |</span><br><span class="line">| 3929       | Bank Dick, The (1940)                         | 3.993197278911565   |</span><br><span class="line">| 3932       | Invisible Man, The (1933)                     | 3.75                |</span><br><span class="line">+<span class="comment">------------+-----------------------------------------------+---------------------+</span></span><br></pre></td></tr></table></figure><p>（6）求好片（评分&gt;=4.0）最多的那个年份的最好看的10部电影<br>（7）求1997年上映的电影中，评分最高的10部Comedy类电影<br>（8）该影评库中各种类型电影中评价最高的5部电影（类型，电影名，平均影评分）<br>（9）各年评分最高的电影类型（年份，类型，影评分）<br>（10）每个地区最高评分的电影名，把结果存入HDFS（地区，电影名，影评分）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;现有如此三份数据：&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;1、users.dat    数据格式为：  2::M::56::16::70072&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;对应字段为：UserID BigInt, Gender String, 
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/categories/Hadoop/Hive/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>IDEA的简单使用</title>
    <link href="https://airpoet.github.io/2018/06/18/Tools/IDEA/IDEA%E7%9A%84%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/"/>
    <id>https://airpoet.github.io/2018/06/18/Tools/IDEA/IDEA的简单使用/</id>
    <published>2018-06-18T11:14:22.768Z</published>
    <updated>2018-06-18T13:44:30.402Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="工具" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="IDEA" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/IDEA/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="工具" scheme="https://airpoet.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="IDEA" scheme="https://airpoet.github.io/tags/IDEA/"/>
    
  </entry>
  
  <entry>
    <title>Hive学习—2</title>
    <link href="https://airpoet.github.io/2018/06/14/Hadoop/3-Hive/Hive%E5%AD%A6%E4%B9%A0-2/"/>
    <id>https://airpoet.github.io/2018/06/14/Hadoop/3-Hive/Hive学习-2/</id>
    <published>2018-06-14T01:04:30.334Z</published>
    <updated>2018-06-14T15:21:20.295Z</updated>
    
    <content type="html"><![CDATA[<h1 id="hive数据类型"><a href="#hive数据类型" class="headerlink" title="hive数据类型"></a>hive数据类型</h1><h3 id="1-原子数据类型"><a href="#1-原子数据类型" class="headerlink" title="1) 原子数据类型"></a>1) 原子数据类型</h3><ul><li>TinyInt：1byte有符号整数</li><li>SmallInt：2byte有符号整数</li><li>Int：4byte有符号整数</li><li>BigInt：8byte有符号整数</li><li>Float：单精度浮点数</li><li>Double：双精度浮点数</li><li>Boolean：布尔类型</li><li>String：字符串</li><li>TimeStamp：整数</li></ul><h3 id="2-复杂数据类型"><a href="#2-复杂数据类型" class="headerlink" title="2) 复杂数据类型"></a>2) 复杂数据类型</h3><ul><li><p>Array\&lt;Type></p><ul><li>由一系列相同数据类型的元素组成</li><li>这些元素可以通过 下标 来访问</li><li>查询时如果查到返回响应值，没查到则返回null<br>建表：<br>create table person(name string,work_locations array\&lt;string>)<br>row format delimited fields terminated by ‘\t’<br>collection items terminated by ‘,’;<br>导入数据：<br>load data local inpath ‘/home/sigeon/person.txt’ into table person;<br>查询<br>Select work_locations[0] from person;</li></ul></li><li><p>Map&lt;KType, VType&gt;</p><ul><li>包含 key-value 键值对</li><li>可以通过 key 来访问元素<br>建表语句：<br>create table score(name string, scores map)<br>row format delimited fields terminated by ‘\t’<br>collection items terminated by ‘,’<br>map keys terminated by ‘:’;<br>导入数据：<br>load data local inpath ‘/home/sigeon/score.txt’ into table score;<br>查询语句：<br>Select scores[‘Chinese’] from score;</li></ul></li><li><p>Struct&lt;Param1:Type1, Param1:Type1, … &gt;</p><ul><li>可以包含不同数据类型的元素<br>类似于c语言中的结构体</li><li>这些元素可以通过 点语法 的方式来得到<br>建表语句：<br>create table course(id int,course struct&lt;name:string, score:int&gt;)<br>row format delimited fields terminated by ‘\t’<br>collection items terminated by ‘,’;<br>导入数据：<br>load data local inpath ‘/ home/sigoen/course.txt’ into table course;<br>查询语句：<br>Select c.course.score from course c;</li></ul></li><li><p>几个分隔符</p></li></ul><p>  指定分隔符要按从外向内的顺序，字段 -&gt; 集合元素 -&gt;map k-v</p><ul><li>ROW FORMAT：指定分隔符的关键字 </li><li>DELIMITED FIELDS TERMINATED BY：字段分隔符 </li><li>COLLECTION ITEMS TERMINATED BY：集合元素分隔符（Array 中的各元素、Struct 中的各元素、 Map 中的 key-value 对之间） </li><li>MAP KEYS TERMINATED BY：Map 中 key 与 value 的分隔符 </li><li>LINES TERMINATED BY：行之间的分隔符</li></ul><h1 id="hive视图"><a href="#hive视图" class="headerlink" title="hive视图"></a>hive视图</h1><ul><li><p>和关系型数据库一样，Hive也提供了视图的功能</p></li><li><p>Hive 的视图和关系型数据库的视图有很大的区别： </p><ul><li>1、只有逻辑视图，没有物化视图； </li><li>2、视图只能查询，不能增删改 (Load|Insert/Update/Delete) 数据； </li><li>3、视图在创建时候，只是保存了一份元数据 (存在TBLS中)，当查询视图的时候，才开始执行视图对应的那些子查询<br>视图元数据只存储了hql语句，而不是执行结果</li></ul></li><li><p>视图操作</p><ul><li><p>创建视图</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">view</span> my_view <span class="keyword">as</span> &lt;<span class="keyword">select</span> * <span class="keyword">from</span> mytable&gt; [<span class="keyword">limit</span> <span class="number">500</span>];</span><br></pre></td></tr></table></figure></li><li><p>查看视图</p><ul><li>show tables;   // 显示所有表和视图</li><li>show views;    //显示所有视图</li><li>desc [formatted] view_name;   // 查看某个具体视图的(详细)信息<br>视图类型：VIRTUAL_VIEW</li></ul></li><li><p>删除视图</p><ul><li>drop view [if exists] view_name</li></ul></li><li><p>使用视图 </p><ul><li>select count(distinct uid) from my_view;</li></ul></li></ul></li></ul><h1 id="hive函数"><a href="#hive函数" class="headerlink" title="hive函数"></a>hive函数</h1><ul><li><p>函数分类</p><ul><li>UDF（自定义函数 User-Defined Function）作用于单个数据行，产生一个数据行作为输出。（数学函数，字 符串函数） </li><li>UDAF（用户定义聚集函数 User-Defined Aggregation Funcation）：接收多个输入数据行，并产 生一个输出数据行。（count，max） </li><li>UDTF（表格生成函数 User-Defined Table Function）：接收一行输入，输出多行（explode）</li></ul></li><li><p>内置函数</p><ul><li><p>查看函数命令</p><ul><li>查看内置函数： show functions; </li><li>显示函数的详细信息： desc function [extended] fun_name;<br>extended：显示扩展信息</li></ul></li><li><p>分类</p><ul><li><p>关系运算</p><ul><li>分类<br>\1. 等值比较: =<br>\2. 等值比较:&lt;=&gt;<br>\3. 不等值比较: &lt;&gt;和!=<br>\4. 小于比较: &lt;<br>\5. 小于等于比较: &lt;=<br>\6. 大于比较: &gt;<br>\7. 大于等于比较: &gt;=<br>\8. 区间比较<br>\9. 空值判断: IS NULL<br>\10. 非空判断: IS NOT NULL<br>\10. LIKE 比较: LIKE<br>\11. JAVA 的 LIKE 操作: RLIKE<br>\12. REGEXP 操作: REGEXP</li></ul></li><li><p>数学运算</p><ul><li>分类<br>\1. 加法操作: +<br>\2. 减法操作: –<br>\3. 乘法操作: *<br>\4. 除法操作: /<br>\5. 取余操作: %<br>\6. 位与操作: &amp;<br>\7. 位或操作: |<br>\8. 位异或操作: ^<br>9．位取反操作: ~</li></ul></li><li><p>逻辑运算</p><ul><li>分类<br>\1. 逻辑与操作: AND 、&amp;&amp;<br>\2. 逻辑或操作: OR 、||<br>\3. 逻辑非操作: NOT、!</li></ul></li><li><p>复合类型构造函数 </p><ul><li>分类<ol><li>array 结构</li><li>map 结构<br>\3. struct 结构<br>\4. named_struct 结构<br>\5. create_union </li></ol></li></ul></li><li><p>复合类型操作符</p><ul><li>\1. 获取 array 中的元素 </li><li>\2. 获取 map 中的元素 </li><li>\3. 获取 struct 中的元素</li></ul></li><li><p>集合操作函数 </p><ul><li><p>\1. map 类型大小：size </p></li><li><p>\2. array 类型大小：size </p></li><li><p>\3. 判断元素数组是否包含元素：array_contains </p></li><li><p>\4. 获取 map 中所有 value 集合 </p></li><li><p>\5. 获取 map 中所有 key 集合 </p></li><li><p>\6. 数组排序</p></li><li><p>\7. 获取素组或map集合的单个元素（k-v对）：<strong>explode()</strong></p><p><strong>当同时查询炸裂字段和普通字段时，需要使用横向虚拟视图：lateral view</strong></p><ul><li>如：select name,addr.city from usr_addr lateral view(address) addr as city;<br>这里一般需要给 查询结果 和 结果字段 起别名，不然没办法获得每个字段，如果只是查询所有的话就不需要了</li></ul></li></ul></li><li><p>类型转换函数</p><ul><li>\1. 二进制转换：binary </li><li>\2. 基础类型之间强制转换：cast </li></ul></li><li><p>数值计算函数</p><ul><li>\1. 取整函数: round </li><li>\2. 指定精度取整函数: round </li><li>\3. 向下取整函数: floor </li><li>\4. 向上取整函数: ceil </li><li>\5. 向上取整函数: ceiling </li><li>\6. 取随机数函数: rand </li><li>\7. 自然指数函数: exp </li><li>\8. 以 10 为底对数函数: log10 </li><li>\9. 以 2 为底对数函数: log2 </li><li>\10. 对数函数: log </li><li>\11. 幂运算函数: pow </li><li>\12. 幂运算函数: power </li><li>\13. 开平方函数: sqrt </li><li>\14. 二进制函数: bin </li><li>\15. 十六进制函数: hex </li><li>\16. 反转十六进制函数: unhex </li><li>\17. 进制转换函数: conv </li><li>\18. 绝对值函数: abs </li><li>\19. 正取余函数: pmod </li><li>\20. 正弦函数: sin </li><li>\21. 反正弦函数: asin </li><li>\22. 余弦函数: cos </li><li>\23. 反余弦函数: acos </li><li>\24. positive 函数: positive </li><li>\25. negative 函数: negative </li></ul></li><li><p>字符串函数</p><ul><li>\1. 字符 ascii 码函数：ascii </li><li>\2. base64 字符串 </li><li>\3. 字符串连接函数：concat </li><li>\4. 带分隔符字符串连接函数：concat_ws </li><li>\5. 数组转换成字符串的函数：concat_ws </li><li>\6. 小数位格式化成字符串函数：format_number </li><li>\7. 字符串截取函数：substr, substring<br>序号从1开始，可以传负数，代表从右开始</li><li>\9. 字符串查找函数：instr<br>找到返回一个正整数，未找到返回0</li><li>\10. 字符串长度函数：length </li><li>\11. 字符串查找函数：locate </li><li>\12. 字符串格式化函数：printf </li><li>\13. 字符串转换成 map 函数：str_to_map </li><li>\14. base64 解码函数：unbase64(string str) </li><li>\15. 字符串转大写函数：upper,ucase </li><li>\16. 字符串转小写函数：lower,lcase </li><li>\17. 去空格函数：trim </li><li>\18. 左边去空格函数：ltrim </li><li>\19. 右边去空格函数：rtrim </li><li>\20. 正则表达式替换函数：regexp_replace </li><li>\21. 正则表达式解析函数：regexp_extract </li><li>\22. URL 解析函数：parse_url </li><li>\23. json 解析函数：get_json_object </li><li>\24. 空格字符串函数：space </li><li>\25. 重复字符串函数：repeat </li><li>\26. 左补足函数：lpad</li><li>\27. 右补足函数：rpad </li><li>\28. 分割字符串函数: split </li><li>\29. 集合查找函数: find_in_set </li><li>\30. 分词函数：sentences </li><li>\31. 分词后统计一起出现频次最高的 TOP-K </li><li>\32. 分词后统计与指定单词一起出现频次最高的 TOP-K </li></ul></li><li><p>日期函数</p><ul><li>\1. UNIX 时间戳转日期函数: from_unixtime </li><li>\2. 获取当前 UNIX 时间戳函数: unix_timestamp </li><li>\3. 日期转 UNIX 时间戳函数: unix_timestamp </li><li>\4. 指定格式日期转 UNIX 时间戳函数: unix_timestamp </li><li>\5. 日期时间转日期函数: to_date </li><li>\6. 日期转年函数: year </li><li>\7. 日期转月函数: month </li><li>\8. 日期转天函数: day </li><li>\9. 日期转小时函数: hour </li><li>\10. 日期转分钟函数: minute </li><li>\11. 日期转秒函数: second </li><li>\12. 日期转周函数: weekofyear </li><li>\13. 日期比较函数: datediff </li><li>\14. 日期增加函数: date_add </li><li>\15. 日期减少函数: date_sub </li></ul></li><li><p>条件函数</p><ul><li>\1. If 函数: if( 条件 ，true返回参数，false返回参数 ) </li><li>\2. 当param1不为null返回param1，否则返回param2：nvl(param1, param2)</li><li>\2. 非空查找函数: coalesce</li><li>\3. 条件判断函数：case</li></ul></li><li><p>混合函数 </p><ul><li>分类<br>\1. 调用 Java 函数：java_method<br>\2. 调用 Java 函数：reflect<br>\3. 字符串的 hash 值：hash</li></ul></li><li><p>XPath 解析 XML 函数 </p><ul><li>分类<br>\1. xpath<br>\2. xpath_string<br>\3. xpath_boolean<br>\4. xpath_short, xpath_int, xpath_long<br>\5. xpath_float, xpath_double, xpath_number </li></ul></li><li><p>汇总统计函数（UDAF）</p><ul><li>\1. 个数统计函数: count </li><li>\2. 总和统计函数: sum </li><li>\3. 平均值统计函数: avg </li><li>\4. 最小值统计函数: min </li><li>\5. 最大值统计函数: max</li><li>\6. 非空集合总体变量函数: var_pop </li><li>\7. 非空集合样本变量函数: var_samp </li><li>\8. 总体标准偏离函数: stddev_pop </li><li>\9. 样本标准偏离函数: stddev_samp </li><li>10．中位数函数: percentile </li><li>\11. 中位数函数: percentile </li><li>\12. 近似中位数函数: percentile_approx </li><li>\13. 近似中位数函数: percentile_approx </li><li>\14. 直方图: histogram_numeric </li><li>\15. 集合去重数：collect_set </li><li>\16. 集合不去重函数：collect_list </li></ul></li><li><p>表格生成函数 Table-Generating Functions (UDTF) </p><ul><li>分类<br>1．数组拆分成多行：explode(array)<br>2．Map 拆分成多行：explode(map) </li></ul></li></ul></li></ul></li><li><p>自定义函数</p><ul><li>\1. 需要继承 org.apache.hadoop.hive.ql.exec.UDF 类，实现一个或多个 evaluate() 方法</li><li>\2. 将hive的jar包放在hive的classpath路径下，进入hive客户端，执行命令：add jar \<jar path="">;</jar></li><li>\3. 检查jar包是否添加成功，执行命令：list jars;</li><li>\4. 给自定义函数 添加别名，并在hive中 注册 该函数，执行命令：create temporary function myfunc as ‘主类全路径名’;<br>myfunc：自定义函数别名<br>该方法创建的是临时函数，当前客户端关闭就没有了，下次重复第3，4步；<br>真实 生产中就是使用该方法！</li><li>\5. 查看hive函数库有没有成功添加，执行命令：show functions;</li><li>\6. 调用函数时通过函数名和参数列表确定调用的是具体哪一个方法</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;hive数据类型&quot;&gt;&lt;a href=&quot;#hive数据类型&quot; class=&quot;headerlink&quot; title=&quot;hive数据类型&quot;&gt;&lt;/a&gt;hive数据类型&lt;/h1&gt;&lt;h3 id=&quot;1-原子数据类型&quot;&gt;&lt;a href=&quot;#1-原子数据类型&quot; class=&quot;head
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/categories/Hadoop/Hive/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>i-Hive-1</title>
    <link href="https://airpoet.github.io/2018/06/13/Hadoop/3-Hive/i-Hive-1/"/>
    <id>https://airpoet.github.io/2018/06/13/Hadoop/3-Hive/i-Hive-1/</id>
    <published>2018-06-13T14:04:40.446Z</published>
    <updated>2018-06-22T13:29:40.732Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Hive-初探"><a href="#1-Hive-初探" class="headerlink" title="1.  Hive 初探"></a>1.  Hive 初探</h1><h3 id="1-1-Hive-的数据存储"><a href="#1-1-Hive-的数据存储" class="headerlink" title="1.1 Hive 的数据存储"></a>1.1 Hive 的数据存储</h3><ul><li>Hive的数据存储基于Hadoop HDFS</li><li>Hive没有专门的数据存储格式</li><li>存储结构主要包括：数据库、文件、表、视图、索引</li><li>Hive默认可以直接加载文本文件（TextFile），还支持SequenceFile、RCFile </li><li>创建表时，指定Hive数据的列分隔符与行分隔符，Hive即可解析数据</li></ul><h3 id="1-2-Hive的系统架构"><a href="#1-2-Hive的系统架构" class="headerlink" title="1.2 Hive的系统架构"></a>1.2 Hive的系统架构</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-13-152054.jpg" alt=""></p><ul><li>用户接口，包括 CLI，JDBC/ODBC，WebUI</li><li>元数据存储，通常是存储在关系数据库如 mysql, derby 中</li><li>解释器、编译器、优化器、执行器</li><li>Hadoop：用 HDFS 进行存储，利用 MapReduce 进行计算</li></ul><h3 id="1-3-Hive的系统架构"><a href="#1-3-Hive的系统架构" class="headerlink" title="1.3 Hive的系统架构"></a>1.3 Hive的系统架构</h3><ul><li>用户接口主要有三个：CLI，JDBC/ODBC和 WebUI<ul><li>CLI，即Shell命令行</li><li>JDBC/ODBC 是 Hive 的Java，与使用传统数据库JDBC的方式类似</li><li>WebGUI是通过浏览器访问 Hive</li></ul></li><li>Hive 将元数据存储在数据库中(metastore)，目前只支持 mysql、derby。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等</li><li>解释器、编译器、优化器完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划（plan）的生成。生成的查询计划存储在 HDFS 中，并在随后由 MapReduce 调用执行</li><li>Hive 的数据存储在 HDFS 中，大部分的查询由 MapReduce 完成（包含 <em> 的查询，比如 select </em> from table 不会生成 MapRedcue 任务</li></ul><h3 id="1-4-Hive的metastore"><a href="#1-4-Hive的metastore" class="headerlink" title="1.4 Hive的metastore"></a>1.4 Hive的metastore</h3><ul><li>metastore是hive元数据的集中存放地。</li><li>metastore默认使用内嵌的derby数据库作为存储引擎</li><li>Derby引擎的缺点：一次只能打开一个会话</li><li>使用Mysql作为外置存储引擎，多用户同时访问 </li></ul><h3 id="1-5-Hive-和-Hadoop-的调用关系"><a href="#1-5-Hive-和-Hadoop-的调用关系" class="headerlink" title="1.5 Hive 和 Hadoop 的调用关系"></a>1.5 Hive 和 Hadoop 的调用关系</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-13-140311.jpg" alt=""></p><blockquote><p>1、提交sql  交给驱动<br>2、驱动编译    解析相关的字段表信息<br>3、去metastore查询相关的信息    返回字段表信息<br>4、编译返回信息 发给驱动<br>5、驱动发送一个执行计划    交给执行引擎<br>6.1、DDLs <strong>对数据库表的操作的, 直接和metastore交互</strong>,<code>create table t1(name string);</code></p><p>6.2、完成job返回数据信息、找<strong>namenode</strong>查数据<br>6.3、<strong>namenode</strong>交互<code>select count(1) from t1;</code><br>7、返回结果信息集</p></blockquote><h3 id="1-6-Hive-参数配置使用"><a href="#1-6-Hive-参数配置使用" class="headerlink" title="1.6 Hive 参数配置使用"></a>1.6 Hive 参数配置使用</h3><table><thead><tr><th>命名空间</th><th>使用权限</th><th>描述</th></tr></thead><tbody><tr><td>hivevar</td><td>可读写</td><td>$   hive -d name=zhangsan;</td></tr><tr><td>hiveconf</td><td>可读写</td><td>\$   hive –hiveconf hive.cli.print.current.db=true;   $   hive –hiveconf hive.cli.print.header=true;</td></tr><tr><td>system</td><td>可读写</td><td>java定义的配置属性，如system:user.name</td></tr><tr><td>env</td><td>只读</td><td>shell环境变量，如env:USER</td></tr></tbody></table><ul><li><p>hivevar </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用场景: 起别名</span></span><br><span class="line">hive -d name=zhangsan  <span class="comment">#传参</span></span><br><span class="line">&gt; create table t2(name string,<span class="variable">$&#123;name&#125;</span> string); <span class="comment">#取参数</span></span><br><span class="line">&gt; desc t2;</span><br><span class="line">---</span><br><span class="line">name                string</span><br><span class="line">zhangsan            string</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>hiveconf :</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  显示当前数据库名称</span></span><br><span class="line">[ap@cs2]~% hive --hiveconf hive.cli.print.current.db=<span class="literal">true</span>;</span><br><span class="line">hive (default)&gt; create database mydb;</span><br><span class="line">hive (default)&gt; use mydb;</span><br><span class="line">hive (mydb)&gt; </span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示表头(字段名)</span></span><br><span class="line">hive --hiveconf hive.cli.print.header=<span class="literal">true</span>;</span><br><span class="line">select * from t2;</span><br><span class="line">t2.namet2.zhangsan</span><br></pre></td></tr></table></figure></li></ul><h3 id="1-7-Hive-的脚本执行"><a href="#1-7-Hive-的脚本执行" class="headerlink" title="1.7 Hive 的脚本执行"></a>1.7 Hive 的脚本执行</h3><ul><li><strong>Hive -e “xx ”</strong><ul><li>e 就是 edit, 在终端打印输出</li></ul></li><li><strong>Hive -e “show tables” &gt;&gt; a.txt</strong><ul><li>可以把执行结果重定向到文件中</li></ul></li><li><strong>Hive -S -e “show tables” &gt;&gt; a.txt</strong><ul><li>-S : silence 安静的执行</li></ul></li><li><strong>hive -f file</strong> <ul><li>hive -f hql ,  hql 是文件, 执行文件</li><li>执行完了之后,  就离开 hive 命令行</li></ul></li><li><strong>hive -i /home/ap/hive-init.sql</strong> <ul><li>执行完了,还在控制台, 可以继续操作</li></ul></li><li><strong>hive&gt;source file</strong><ul><li>source + 文件名  : 直接执行当前目录文件</li><li>source /home/ap/xx.sql;</li></ul></li></ul><h3 id="1-8-hive与依赖环境的交互"><a href="#1-8-hive与依赖环境的交互" class="headerlink" title="1.8 hive与依赖环境的交互"></a>1.8 hive与依赖环境的交互</h3><ul><li><strong>与linux交互命令 ！</strong><ul><li><code>!ls</code></li><li><code>!pwd</code></li></ul></li><li><strong>与hdfs交互命令</strong><ul><li><code>dfs -ls /</code></li><li><code>dfs -mkdir /hive</code></li><li><code>hive (default)&gt; dfs -rm -r /user/hive/warehouse/t5;</code></li></ul></li><li><strong>beeline 与 linux &amp; hdfs 交互</strong><ul><li>!help 查看帮助</li></ul></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-043653.png" alt="image-20180619123652339"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-043558.png" alt="image-20180619123557944"></p><h3 id="1-9-Hive-的-JDBC-模式"><a href="#1-9-Hive-的-JDBC-模式" class="headerlink" title="1.9 Hive 的 JDBC 模式"></a>1.9 Hive 的 JDBC 模式</h3><ul><li><p>JAVA API交互执行方式 </p></li><li><p>hive 远程服务 (端口号1000  0) 启动方式</p><ul><li><code>hive --service hiveserver2</code></li><li><code>org.apache.hive.jdbc.HiveDriver</code></li></ul></li><li><p>在java代码中调用hive的JDBC建立连接</p></li><li><p><strong>用 beeline 连接</strong></p><ul><li><p><strong>方式1: 直接登录</strong></p><ul><li>注意: 这里的<strong>cs2是指的数据库所在的服务器</strong>, 如果mysql 安装在 cs2上, 那么不管在哪台机器上登录beeline , 都这样输入就行了</li></ul><p><code>beeline -u jdbc:hive2://cs2:10000 -n ap</code></p></li><li><p><strong>方式2: 输入用户名密码登录</strong></p><p><code>!connect jdbc:hive2://cs2:10000</code></p></li></ul></li><li><p><strong>beeline注意点:</strong> </p><ul><li>使用 beeline 连接时, 貌似无法与 Linux 目录交互</li><li>当前目录在<code>/home/ap/apps/apache-hive-2.3.2-bin/bin/</code>下</li><li><strong>要传文件的话, 要使用全路径</strong></li></ul></li></ul><h3 id="1-10-SET命令使用"><a href="#1-10-SET命令使用" class="headerlink" title="1.10 SET命令使用"></a>1.10 SET命令使用</h3><ul><li>Hive 控制台set 命令<ul><li>set;    set -v;  显示所有的环境变量</li><li><code>set hive.cli.print.current.db=true;</code></li><li><code>set hive.cli.print.header=true;</code> </li><li><code>set hive.metastore.warehouse.dir=/hive;</code></li></ul></li><li><strong>hive参数初始化配置set命令:</strong><ul><li><strong>~/.hiverc</strong><ul><li>创建此文件, 在此文件中配置初始化命令</li></ul></li><li>补充：<br><strong>hive历史操作命令集</strong><br><strong>~/.hivehistory</strong></li></ul></li></ul><p><br></p><h1 id="2-Hive数据类型"><a href="#2-Hive数据类型" class="headerlink" title="2.  Hive数据类型"></a>2.  Hive数据类型</h1><h3 id="2-1-基本数据类型"><a href="#2-1-基本数据类型" class="headerlink" title="2.1 基本数据类型"></a>2.1 基本数据类型</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-13-163316.jpg" alt=""></p><h3 id="2-2-复合数据类型"><a href="#2-2-复合数据类型" class="headerlink" title="2.2 复合数据类型"></a>2.2 复合数据类型</h3><blockquote><p>创建学生表</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> student(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">INT</span>,</span><br><span class="line">    <span class="keyword">name</span> <span class="keyword">STRING</span>,</span><br><span class="line">    favors <span class="built_in">ARRAY</span>\&lt;<span class="keyword">STRING</span>&gt;,</span><br><span class="line">    scores <span class="keyword">MAP</span>&lt;<span class="keyword">STRING</span>, <span class="built_in">FLOAT</span>&gt;</span><br><span class="line">);</span><br></pre></td></tr></table></figure><table><thead><tr><th>默认分隔符</th><th>描述</th><th>语句</th></tr></thead><tbody><tr><td>\n</td><td>分隔行</td><td>LINES   TERMINATED BY ‘\t’</td></tr><tr><td>^A</td><td>分隔字段(列)，显示编码使用\001</td><td>FIELDS   TERMINATED BY ‘\001’</td></tr><tr><td>^B</td><td>分隔复合类型中的元素，显示编码使用\002</td><td>COLLECTION   ITEMS TERMINATED BY ‘\002’</td></tr><tr><td>^C</td><td>分隔map元素的key和value，显示编码使用\003</td><td>MAP   KEYS TERMINATED BY ‘\003’</td></tr></tbody></table><h4 id="2-2-1-Struct-使用"><a href="#2-2-1-Struct-使用" class="headerlink" title="2.2.1.  Struct 使用"></a>2.2.1.  Struct 使用</h4><p><strong>Structs内部的数据可以通过DOT（.）来存取</strong>，例如，表中一列c的类型为<code>STRUCT{a INT; b INT}</code>，我们可以通过c.a来访问域a</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># 数据</span><br><span class="line">1001,zhangsan:24</span><br><span class="line">1002,lisi:28</span><br><span class="line">1003,wangwu:25</span><br><span class="line"></span><br><span class="line"># 1.创建表</span><br><span class="line">hive&gt; create table student_test(id INT, info struct&lt;name:STRING, age:INT&gt;)ROW FORMAT DELIMITED FIELDS TERMINATED BY ','COLLECTION ITEMS TERMINATED BY ':';</span><br><span class="line"></span><br><span class="line"># 2.加载表</span><br><span class="line">hive&gt; load data local inpath "student_test" into table student_test;</span><br><span class="line"></span><br><span class="line"># 3.顺便设置 显示表头,和当前数据库</span><br><span class="line">hive&gt; set hive.cli.print.header=true;</span><br><span class="line">hive&gt; set hive.cli.print.current.db=true;</span><br><span class="line"></span><br><span class="line"># 4. 展示所有的</span><br><span class="line">hive (default)&gt; select * from student_test;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">student_test.idstudent_test.info</span><br><span class="line">1001&#123;"name":"zhangsan","age":24&#125;</span><br><span class="line">1002&#123;"name":"lisi","age":28&#125;</span><br><span class="line">1003&#123;"name":"wangwu","age":25&#125;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line"></span><br><span class="line"># Struct -结构体-使用 . </span><br><span class="line">hive (default)&gt; select id,info.name,info.age from student_test;</span><br><span class="line">idnameage</span><br><span class="line">1001zhangsan24</span><br><span class="line">1002lisi28</span><br><span class="line">1003wangwu25</span><br></pre></td></tr></table></figure><h4 id="2-2-2-Array-使用"><a href="#2-2-2-Array-使用" class="headerlink" title="2.2.2. Array 使用"></a>2.2.2. Array 使用</h4><p><strong>Array中的数据为相同类型</strong>，例如，假如array A中元素<code>[&#39;a&#39;,&#39;b&#39;,&#39;c’]</code>，则A[1]的值为’b’</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># 原始数据</span><br><span class="line">zhangsan,24:25:27:37</span><br><span class="line">lisi,28:39:23:43</span><br><span class="line">wangwu,25:23:02:54</span><br><span class="line"></span><br><span class="line"># 创建表</span><br><span class="line">hive (default)&gt; create table class_test(name string,student_id_list array&lt;int&gt;) row format delimited fields terminated by ',' collection items terminated by ':';</span><br><span class="line"></span><br><span class="line"># 加载表</span><br><span class="line">hive (default)&gt; load data local inpath "class_test" into table class_test;</span><br><span class="line"></span><br><span class="line"># 查看表</span><br><span class="line">hive (default)&gt; select * from class_test;</span><br><span class="line">OK</span><br><span class="line">class_test.nameclass_test.student_id_list</span><br><span class="line">zhangsan[24,25,27,37]</span><br><span class="line">lisi[28,39,23,43]</span><br><span class="line">wangwu[25,23,2,54]</span><br><span class="line"></span><br><span class="line"># 查看数据中某个元素</span><br><span class="line">hive (default)&gt; select name, student_id_list[0] from class_test where name='zhangsan';</span><br><span class="line">OK</span><br><span class="line">name_c1</span><br><span class="line">zhangsan24</span><br></pre></td></tr></table></figure><h4 id="2-2-3-Map-使用"><a href="#2-2-3-Map-使用" class="headerlink" title="2.2.3. Map 使用"></a>2.2.3. Map 使用</h4><p>访问指定域可以通过[“指定域名称”]进行，例如，一个Map M包含了一个group-&gt;gid的kv对，<strong>gid的值可以通过M[‘group’]来获取</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"># 原始数据</span><br><span class="line">1001job:80,team:60,person:70</span><br><span class="line">1002job:60,team:80,person:80</span><br><span class="line">1003job:90,team:70,person:100</span><br><span class="line"></span><br><span class="line"># 创建表</span><br><span class="line">hive (default)&gt; create table employee(id string,perf map&lt;string,int&gt;) row format delimited fields terminated by '\t' collection items terminated by ',' map keys terminated by ':';</span><br><span class="line"></span><br><span class="line"># 导入</span><br><span class="line">hive (default)&gt; load data local inpath "employee_data" into table employee;</span><br><span class="line"></span><br><span class="line"># 查看</span><br><span class="line">hive (default)&gt; select * from employee;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">employee.idemployee.perf</span><br><span class="line">1001&#123;"job":80,"team":60,"person":70&#125;</span><br><span class="line">1002&#123;"job":60,"team":80,"person":80&#125;</span><br><span class="line">1003&#123;"job":90,"team":70,"person":100&#125;</span><br><span class="line">Time taken: 0.228 seconds, Fetched: 3 row(s)</span><br><span class="line"></span><br><span class="line"># 查看单个</span><br><span class="line">hive (default)&gt; select id,perf['job'],perf['team'],perf['person'] from employee;</span><br><span class="line">OK</span><br><span class="line">id_c1_c2_c3</span><br><span class="line">1001806070</span><br><span class="line">1002608080</span><br><span class="line">10039070100</span><br><span class="line"></span><br><span class="line"># 显示别名</span><br><span class="line">hive (default)&gt; select id,perf['job'] as job,perf['team'] as team,perf['person'] as person from employee;</span><br><span class="line">OK</span><br><span class="line">idjobteamperson</span><br><span class="line">1001806070</span><br><span class="line">1002608080</span><br><span class="line">10039070100</span><br></pre></td></tr></table></figure><p><br></p><h1 id="3-DDL-DML"><a href="#3-DDL-DML" class="headerlink" title="3. DDL , DML"></a>3. DDL , DML</h1><h2 id="3-1-DDL"><a href="#3-1-DDL" class="headerlink" title="3.1 DDL"></a>3.1 DDL</h2><h3 id="3-1-1-数据库定义"><a href="#3-1-1-数据库定义" class="headerlink" title="3.1.1  数据库定义"></a>3.1.1  数据库定义</h3><ul><li><p>默认数据库”default”</p></li><li><p>使用某个数据库 <code>use &lt;数据库名&gt;</code></p></li><li><p>创建一个新库</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span>  </span><br><span class="line">[<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] mydb  </span><br><span class="line">[LOCATION] <span class="string">'/.......'</span>  </span><br><span class="line">[<span class="keyword">COMMENT</span>] <span class="string">'....’;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">hive&gt;SHOW DATABASES;</span></span><br><span class="line"><span class="string">hive&gt;DESCRIBE DATABASE [extended] mydb;</span></span><br><span class="line"><span class="string">hive&gt;DROP DATABASE [IF EXISTS] mydb [CASCADE];</span></span><br></pre></td></tr></table></figure></li><li><p>创建</p><ul><li><code>create database db1;</code></li></ul></li><li><p>删除</p><ul><li><code>drop database if exists db1;</code></li><li><strong>级联删除</strong><ul><li><code>drop database if exists db1 cascade;</code></li></ul></li></ul></li></ul><h3 id="3-1-2-表定义-修改"><a href="#3-1-2-表定义-修改" class="headerlink" title="3.1.2  表定义/修改"></a>3.1.2  表定义/修改</h3><ul><li><p><strong>创建表</strong></p><ul><li><p>hive&gt;CREATE TABLE IF NOT EXISTS t1(…) </p><p>[COMMENT ‘….’] </p><p>[LOCATION ‘…’] </p><ul><li><code>hive (default)&gt; create table t4(name string,age int) row format delimited fields terminated by &quot;\t”;</code></li></ul></li><li><p>hive&gt; SHOW TABLES in mydb;</p><ul><li><code>show tables in mydb ‘’class*“</code> : 查看以 mydb 库中, 以 class 开头的表</li></ul></li><li><p><strong>hive&gt;CREATE TABLE t2 LIKE t1;</strong>   复制表</p><ul><li><strong>只会复制表结构</strong></li><li><code>hive (default)&gt; create table t2 like t1;</code></li><li><code>hive (mydb)&gt; create table t3 like default.employee;</code><ul><li>复制其它库的表</li></ul></li></ul></li><li><p>hive&gt;DESCRIBE t2;</p><ul><li>desc t2;  # 效果一样的</li><li><code>desc extended t1;</code>    # 查看更详细的表信息</li><li><strong><code>hive (default)&gt; desc formatted t1;</code>  # 格式化查看表的详细信息</strong></li></ul></li><li><p>drop  table xxx;</p><ul><li>删除表</li></ul></li><li><p><strong>查看建表语句</strong></p><ul><li><strong>show create table t_table;</strong></li></ul></li></ul></li><li><p><strong>修改表</strong></p><ul><li><strong>重命名表</strong><ul><li><code>ALTER TABLE table_name RENAME TO new_table_name</code></li></ul></li><li><strong>增加/删除 分区</strong><ul><li><code>alter table student_p add partition(part=&#39;a&#39;) partition(part=&#39;b&#39;);</code><ul><li>两个 partition中没有’,’</li></ul></li><li>alter table student drop partition(stat_data=‘ffff’), partition(part=‘a’),partiton(part=‘b’);<ul><li>两个 partition中有’,’</li></ul></li></ul></li><li><strong>增加/更新 列</strong><ul><li><code>ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)</code><ul><li><em>注：ADD<strong>是代表新增一字段，字段位置在所有列后面(partition</strong>列前)<strong>，REPLACE</strong>则是表示替换表中所有字段。</em> </li><li><code>alter table student add  columns (name1 string);</code></li></ul></li><li><code>ALTER TABLE table_name CHANGE c_name new_c_name new_c_type [FIRST | AFTER c_name]</code></li></ul></li></ul></li></ul><h3 id="3-1-3-列定义"><a href="#3-1-3-列定义" class="headerlink" title="3.1.3 列定义"></a>3.1.3 列定义</h3><ul><li><p>修改列的名称、类型、位置、注释</p><ul><li><p><code>ALTER TABLE t3 CHANGE COLUMN old_name new_name String COMMENT &#39;...&#39; AFTER column2;</code></p></li><li><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 修改列名</span><br><span class="line">hive (default)&gt; alter table t1 change column name username string comment 'new name';</span><br><span class="line"># 查看表</span><br><span class="line">hive (default)&gt; desc t1;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">col_namedata_type<span class="keyword">comment</span></span><br><span class="line">username            <span class="keyword">string</span>              <span class="keyword">new</span> <span class="keyword">name</span></span><br><span class="line">age                 <span class="built_in">int</span></span><br><span class="line"><span class="comment">---</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>增加列</p><ul><li><p><code>hive&gt; ALTER TABLE t3 ADD COLUMNS(gender int);</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 查看表结构</span><br><span class="line">hive (default)&gt; desc t3;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">col_namedata_type<span class="keyword">comment</span></span><br><span class="line"><span class="keyword">name</span>                <span class="keyword">string</span></span><br><span class="line"><span class="comment">---</span></span><br><span class="line"># 添加列</span><br><span class="line">hive (<span class="keyword">default</span>)&gt; <span class="keyword">alter</span> <span class="keyword">table</span> t3 <span class="keyword">add</span> <span class="keyword">columns</span>(gender <span class="built_in">int</span>);</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">hive (default)&gt; desc t3;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">col_namedata_type<span class="keyword">comment</span></span><br><span class="line"><span class="keyword">name</span>                <span class="keyword">string</span></span><br><span class="line">gender              <span class="built_in">int</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>删除列  replace </p><ul><li><strong>非常不建议使用</strong>, 会造成数据错乱, 一般采取重新创建一张表的方式.</li></ul></li></ul><h3 id="3-1-4-显示命令"><a href="#3-1-4-显示命令" class="headerlink" title="3.1.4 显示命令"></a>3.1.4 显示命令</h3><blockquote><p>show tables</p><p>show databases</p><p>show partitions</p><p>show functions</p><p>desc extended t_name;</p><p>desc formatted table_name;</p></blockquote><h2 id="3-2-DML"><a href="#3-2-DML" class="headerlink" title="3.2 DML"></a>3.2 DML</h2><h3 id="3-2-1-Load"><a href="#3-2-1-Load" class="headerlink" title="3.2.1 Load"></a>3.2.1 Load</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-17-032955.gif" alt="*">  <strong>语法结构</strong></p><p><code>LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]</code></p><p><strong>说明：</strong></p><ol><li>Load 操作只是单纯的复制/移动操作，将数据文件移动到 Hive 表对应的位置。</li><li>filepath：<ul><li>相对路径，例如：<code>project/data1</code></li><li>绝对路径，例如：<code>/user/hive/project/data1</code></li><li>包含模式的完整 URI，例如：<ul><li><code>hdfs://namenode:9000/user/hive/project/data1</code></li></ul></li></ul></li><li><p>LOCAL关键字</p><ul><li>如果指定了 LOCAL， load 命令会去查找本地文件系统中的 filepath。</li><li>如果没有指定 LOCAL 关键字，则根据inpath中的uri查找文件</li></ul></li><li><p>OVERWRITE 关键字</p><ul><li>如果使用了 OVERWRITE 关键字，则目标表（或者分区）中的内容会被删除，然后再将 filepath 指向的文件/目录中的内容添加到表/分区中。 </li><li>如果目标表（分区）已经有一个文件，并且文件名和 filepath 中的文件名冲突，那么现有的文件会被新文件所替代。 </li></ul></li></ol><h3 id="3-2-2-Insert"><a href="#3-2-2-Insert" class="headerlink" title="3.2.2 Insert"></a>3.2.2 Insert</h3><h4 id="语法结构"><a href="#语法结构" class="headerlink" title="语法结构"></a><strong>语法结构</strong></h4><ul><li><p>普通插入</p><ul><li>INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 …)] select_statement1 FROM from_statement </li></ul></li><li><p>Multiple inserts:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">FROM from_statement </span><br><span class="line">[<span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename1 [<span class="keyword">PARTITION</span> (partcol1=val1, partcol2=val2 ...)] select_statement1 ]</span><br><span class="line">[<span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename2 [<span class="keyword">PARTITION</span> ...] select_statement2] ...</span><br><span class="line"></span><br><span class="line"># 多重插入举例</span><br><span class="line"><span class="keyword">from</span> student</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> student_p <span class="keyword">partition</span>(part=<span class="string">'a'</span>)</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">where</span> <span class="keyword">id</span>&lt;<span class="number">95011</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> student_p <span class="keyword">partition</span>(part=<span class="string">'b'</span>)</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">where</span> <span class="keyword">id</span>&gt;<span class="number">95011</span>;</span><br></pre></td></tr></table></figure></li><li><p>Dynamic partition inserts:</p><ul><li>不指定分区字段, 按照 from 表的分区字段插入</li><li><code>INSERT OVERWRITE TABLE tablename PARTITION (partcol1, partcol2 ...) select_statement FROM from_statement</code></li></ul></li></ul><h4 id="导出表数据"><a href="#导出表数据" class="headerlink" title="导出表数据"></a>导出表数据</h4><p><strong>语法结构</strong></p><ul><li><p><code>INSERT OVERWRITE [LOCAL] DIRECTORY directory1 SELECT ... FROM ...</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 导出到本地</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/home/ap/test/stucent1'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student1;</span><br><span class="line"><span class="comment">--------------------</span></span><br><span class="line">例子:</span><br><span class="line">'查询学生信息，按性别分区，在分区内按年龄有序'</span><br><span class="line">0: jdbc:hive2://cs2:10000&gt; set mapred.reduce.tasks=2;</span><br><span class="line">No rows affected (0.015 seconds)</span><br><span class="line">0: jdbc:hive2://cs2:10000&gt; insert overwrite local directory '/home/ap/ihiveout'</span><br><span class="line">. . . . . . . . . . . . .&gt; select * from student distribute by Sex sort by Sage;</span><br><span class="line"><span class="comment">--------------------</span></span><br><span class="line"></span><br><span class="line"># 导出到 HDFS (仅仅是少了一个 local)</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">directory</span> <span class="string">'/test/stucent1'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student1;</span><br></pre></td></tr></table></figure></li><li><p><strong>multiple inserts:</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">FROM from_statement</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE [<span class="keyword">LOCAL</span>] <span class="keyword">DIRECTORY</span> directory1 select_statement1</span><br><span class="line">[<span class="keyword">INSERT</span> OVERWRITE [<span class="keyword">LOCAL</span>] <span class="keyword">DIRECTORY</span> directory2 select_statement2] ...</span><br></pre></td></tr></table></figure></li></ul><h1 id="4-Hive的数据模型"><a href="#4-Hive的数据模型" class="headerlink" title="4. Hive的数据模型"></a>4. Hive的数据模型</h1><h3 id="4-1-管理表-又称为内部表-受控表"><a href="#4-1-管理表-又称为内部表-受控表" class="headerlink" title="4.1 管理表 - 又称为内部表, 受控表"></a>4.1 管理表 - 又称为内部表, 受控表</h3><h4 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a><strong>基本操作</strong></h4><ul><li>创建数据文件inner_table.dat</li><li>创建表<ul><li><code>hive&gt;create table inner_table (key string);</code></li></ul></li><li><strong>加载数据</strong><ul><li><strong>加载本地数据</strong><ul><li><code>hive&gt;load data local inpath &#39;/root/inner_table.dat&#39; into table inner_table;</code></li></ul></li><li><strong>加载HDFS 上数据</strong><ul><li><code>hive&gt;load data inpath ‘xxx’ into table xxx;</code></li></ul></li><li><strong>区别</strong><ul><li>加载 hdfs 上的数据没有 <strong>local</strong></li><li><strong>加载本地数据是 copy 一份, 加载 hdfs 上的数据是直接移动数据到加载的表目录下– mv</strong></li></ul></li></ul></li><li>查看数据<ul><li><code>select * from inner_table</code></li><li><code>select count(*) from inner_table</code></li></ul></li><li><strong>删除表 <code>drop table inner_table</code></strong></li><li><strong>清空表 <code>truncate table table_name;</code></strong> </li><li><strong>注意: </strong><ul><li><strong>如果创建表的时候, 只指定了目录, 没有指定表名, 删除表的时候, 会把该目录下的所有表全部删掉</strong></li><li><code>hive (mydb)&gt; create table t2(id int)location &#39;/home/t2&#39;;</code></li></ul></li></ul><h4 id="内部表解释"><a href="#内部表解释" class="headerlink" title="内部表解释"></a><strong>内部表解释</strong></h4><ul><li>管理表，也称作<strong>内部表</strong>,受控表<ul><li>所有的 Table 数据（不包括 External Table）<strong>都保存在warehouse这个目录中。</strong></li><li><strong>删除表时，元数据与数据都会被删除</strong></li><li>创建过程和数据加载过程（这两个过程可以在同一个语句中完成），<strong>在加载数据的过程中，实际数据会被移动到数据仓库目录中</strong>；之后<strong>对数据对访问</strong>将会<strong>直接在数据仓库目录中</strong>完成。删除表时，表中的数据和元数据将会被同时删除</li></ul></li></ul><h4 id="内部表转为外部表-外部表转为内部表"><a href="#内部表转为外部表-外部表转为内部表" class="headerlink" title="内部表转为外部表,  外部表转为内部表"></a><strong>内部表转为外部表,  外部表转为内部表</strong></h4><ul><li><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">hive (mydb)&gt; create table t1(id int);</span><br><span class="line"></span><br><span class="line"># manage_table 转换为 外部表 external_table </span><br><span class="line">## 注意: 修改为外部表时, 后面2个都要大写</span><br><span class="line">hive (mydb)&gt; alter table t1 set tblproperties('EXTERNAL'='TRUE');</span><br><span class="line">## 修改为内部表</span><br><span class="line">hive (mydb)&gt; alter table t1 set tblproperties('EXTERNAL'='FALSE');</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 查看t1详细信息</span><br><span class="line">desc formatted t1;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">Location:           hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1</span><br><span class="line">Table Type:         EXTERNAL_TABLE</span><br><span class="line"><span class="comment">-- </span></span><br><span class="line"></span><br><span class="line"># 删除t1</span><br><span class="line">hive (mydb)&gt; drop table t1;</span><br><span class="line"># 此时再查看, 已经没了</span><br><span class="line">hive (mydb)&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">tab_name</span><br><span class="line"></span><br><span class="line"># 但是查看hdfs 路径会发现还在, 因为此表现在已经是外部表, 删除不会删除数据</span><br><span class="line">dfs -ls /user/hive/warehouse/mydb.db/t1</span><br><span class="line"></span><br><span class="line"># 如果此时再创建一个新表 t1, 表结构一样, 则数据会自动加载</span><br></pre></td></tr></table></figure></li></ul><h3 id="4-2-※-外部表"><a href="#4-2-※-外部表" class="headerlink" title="4.2 ※ 外部表"></a>4.2 ※ 外部表</h3><h4 id="4-2-1基本操作"><a href="#4-2-1基本操作" class="headerlink" title="4.2.1基本操作"></a><strong>4.2.1基本操作</strong></h4><ul><li>创建数据文件external_table.dat</li><li>创建表<ul><li><code>hive&gt;create external table external_table1 (key string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39; location &#39;/home/external’;</code></li></ul></li><li>在HDFS创建目录/home/external<ul><li><code>#hadoop fs -put /home/external_table.dat /home/external</code></li><li>在工作中, 一般都这样使用, 把数据上传到 hdfs 中</li></ul></li><li>加载数据<ul><li><code>LOAD DATA  &#39;/home/external_table1.dat&#39; INTO TABLE external_table1;</code></li></ul></li><li>查看数据<ul><li><code>select * from external_table</code></li><li><code>select count(*) from external_table</code></li></ul></li><li>删除表 <ul><li><code>drop table external_table</code></li></ul></li></ul><h4 id="4-2-2-外部表解释"><a href="#4-2-2-外部表解释" class="headerlink" title="4.2.2 外部表解释"></a><strong>4.2.2 外部表解释</strong></h4><ul><li>包含External 的表叫外部表<ul><li>删除外部表只删除metastore的元数据，不删除hdfs中的表数据</li><li>外部表 只有一个过程，加载数据和创建表同时完成，并不会移动到数据仓库目录中，只是与外部数据建立一个链接。当删除一个 外部表 时，仅删除该链接</li><li>指向已经在 HDFS 中存在的数据，可以创建 Partition</li><li>它和 内部表 在元数据的组织上是相同的，而实际数据的存储则有较大的差异</li></ul></li></ul><h4 id="4-2-3-外部表语法"><a href="#4-2-3-外部表语法" class="headerlink" title="4.2.3 外部表语法"></a>4.2.3 外部表语法</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> page_view</span><br><span class="line">( viewTime <span class="built_in">INT</span>, </span><br><span class="line">  userid <span class="built_in">BIGINT</span>,</span><br><span class="line">  page_url <span class="keyword">STRING</span>, </span><br><span class="line"> referrer_url <span class="keyword">STRING</span>, </span><br><span class="line">  ip <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'IP Address of the User'</span>,</span><br><span class="line">  country <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'country of origination‘</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">    COMMENT '</span>This <span class="keyword">is</span> the staging page <span class="keyword">view</span> <span class="keyword">table</span><span class="string">'</span></span><br><span class="line"><span class="string">    ROW FORMAT DELIMITED FIELDS TERMINATED BY '</span>\t<span class="string">' LINES TERMINATED BY '</span>\n<span class="string">'</span></span><br><span class="line"><span class="string">    STORED AS TEXTFILE</span></span><br><span class="line"><span class="string">    LOCATION '</span>hdfs://centos:<span class="number">9000</span>/<span class="keyword">user</span>/<span class="keyword">data</span>/staging/page_view<span class="string">';</span></span><br></pre></td></tr></table></figure><h4 id="4-2-4外部表注意点"><a href="#4-2-4外部表注意点" class="headerlink" title="4.2.4外部表注意点:"></a>4.2.4外部表注意点:</h4><ul><li><p>先创建外部表/内部表, 表名为<code>t3</code>, 再往<code>t3</code>传对应字段的数据, 就可以直接 select 数据了</p></li><li><p><u>删除外部表之后, 原本数据不会删除</u>, 此时<strong>在相同的父路径</strong>创建与被删除表<strong>字段相同&amp;名称相同</strong>的<strong>内部/外部表</strong>, 数据<strong>也会直接加载</strong></p></li><li><p><strong>再看一个操作</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 在 mydb.db 库下创建一个外部表 t5, 指定路径为 '/external/t5'</span><br><span class="line"># 此时在 mydb.db 库的路径下是不存在 t5表的, 而是存在 '/external/t5' 中</span><br><span class="line"># 但是使用 'show tables' 是存在 t5 的</span><br><span class="line">hive (mydb)&gt; create external table t5(id int) location '/external/t5';</span><br><span class="line"># 往此目录传数据, 注意: 此时传过去, intdata 数据存在 t5目录下</span><br><span class="line">[ap@cs2]~% hadoop fs -put intdata /external/t5</span><br><span class="line">[ap@cs2]~% hadoop fs -put intdata /external/t5/i2</span><br><span class="line"># 继续传数据, 查询的时候, 就是传的全部数据'相当于所有的数据都在 t5表中'</span><br><span class="line">[ap@cs2]~% hadoop fs -put intdata /external/t5</span><br><span class="line"># 注意: 如果传到 t5 目录下, 没有指定数据文件名的话, 会默认采用数据的名称文件.</span><br></pre></td></tr></table></figure></li></ul><h3 id="4-3-※-分区表"><a href="#4-3-※-分区表" class="headerlink" title="4.3  ※ 分区表"></a>4.3  ※ 分区表</h3><h4 id="4-3-1-基本概念和操作"><a href="#4-3-1-基本概念和操作" class="headerlink" title="4.3.1 基本概念和操作"></a>4.3.1 基本概念和操作</h4><ul><li>分区可以理解为分类，通过分类把不同类型的数据放到不同的目录下。</li><li>分类的标准就是分区字段，可以一个，也可以多个。</li><li>分区表的意义在于优化查询。查询时尽量利用分区字段。如果不使用分区字段，就会全部扫描。</li></ul><blockquote><p><strong>创建分区表, 指定分区字段</strong></p><p><code>hive&gt;CREATE TABLE t3(...) PARTITIONED BY (province string);</code></p><ul><li>创建表的时候, 指定分区字段 key<code>province</code></li></ul><p><strong>为分区字段添加一个值</strong></p><p><code>hive&gt;ALTER TABLE t3 ADD [IF NOT EXISTS] PARTITION(...) LOCATION &#39;...’;</code></p><ul><li><code>alter table t3 add if not exists partition(province=&#39;hubei&#39;) partition(province=&#39;shanghai&#39;);</code></li><li><code>alter table t3 add if not exists partition(province=&#39;jiangsu&#39;);</code></li><li>可为此分区字段添加多个值,  为 province 添加 hubei, hunan….</li></ul><p><strong>查看表的分区字段&amp;值</strong></p><p><code>hive&gt;SHOW PARTITIONS t3 [partition (province=&#39;beijing&#39;)];</code></p><p><strong>删除分区</strong></p><p><code>hive&gt;ALTER TABLE t3 DROP PARTITION(province=‘beijing’.);</code></p><ul><li>这里是删除北京的分区 (如果是内部表, 会连数据一起删除)</li></ul><p><strong>设置表不能被删除/查询</strong>  ——– 这里报语法错误, :TODO</p><ul><li>防止分区被删除:<code>alter table student_p partition (part=&#39;aa&#39;) enable no_drop;</code></li><li>防止分区被查询:<code>alter table student_p partition (part=&#39;aa&#39;) enable offline;</code></li><li>enable 和 disable 是反向操作</li></ul><p><strong>其它一些相关命令</strong></p><p>SHOW TABLES; # 查看所有的表</p><p>SHOW TABLES ‘<em>TMP</em>‘; #支持模糊查询</p><p><code>SHOW PARTITIONS TMP_TABLE;</code> #查看表有哪些分区</p><p>DESC TMP_TABLE; #查看表结构</p></blockquote><h4 id="4-3-2-创建分区表完整语法"><a href="#4-3-2-创建分区表完整语法" class="headerlink" title="4.3.2 创建分区表完整语法"></a>4.3.2 创建分区表完整语法</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> tmp_table #表名</span><br><span class="line">(</span><br><span class="line">title   <span class="keyword">string</span>, # 字段名称 字段类型</span><br><span class="line">minimum_bid     <span class="keyword">double</span>,</span><br><span class="line">quantity        <span class="built_in">bigint</span>,</span><br><span class="line">have_invoice    <span class="built_in">bigint</span></span><br><span class="line">)<span class="keyword">COMMENT</span> <span class="string">'注释：XXX'</span> #表注释</span><br><span class="line"> PARTITIONED <span class="keyword">BY</span>(pt <span class="keyword">STRING</span>) #分区表字段（如果你文件非常之大的话，采用分区表可以快过滤出按分区字段划分的数据）</span><br><span class="line"> <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> </span><br><span class="line">   <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\001'</span>   # 字段是用什么分割开的</span><br><span class="line">STORED AS SEQUENCEFILE; #用哪种方式存储数据，SEQUENCEFILE是hadoop自带的文件压缩格式</span><br></pre></td></tr></table></figure><h4 id="4-3-3-分区表注意点-错误点"><a href="#4-3-3-分区表注意点-错误点" class="headerlink" title="4.3.3 分区表注意点(错误点)"></a>4.3.3 分区表注意点(错误点)</h4><ul><li><p><strong>1) 分区表在 load 数据的时候, 得指定分区, 否则会报错</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 错误1: 分区表在 load 数据的时候, 得指定分区</span><br><span class="line">hive (mydb)&gt; load data local inpath '~/ihivedata/intdata' into table t6;</span><br><span class="line">FAILED: SemanticException [Error 10062]: Need to specify partition columns because the destination table is partitioned</span><br><span class="line"></span><br><span class="line"># 错误2: 导入本地数据的时候, 'path'是从当前所在路径开始的</span><br><span class="line">hive (mydb)&gt; load data local inpath '~/ihivedata/intdata' into table t6 partition(class='job1');</span><br><span class="line">FAILED: SemanticException Line 1:23 Invalid path ''&lt;sub&gt;/ihivedata/intdata'': No files matching path file:/home/ap/&lt;/sub&gt;/ihivedata/intdata</span><br><span class="line"></span><br><span class="line"># 这里就正确了</span><br><span class="line">hive (mydb)&gt; load data local inpath 'ihivedata/intdata' into table t6 partition(class='job1');</span><br><span class="line">Loading data to table mydb.t6 partition (class=job1)</span><br><span class="line">OK</span><br></pre></td></tr></table></figure><ul><li><p><strong>本质原因:</strong> </p><ul><li><strong>分区表的分区, 就是在 hdfs 上, 原表的文件夹下面创建了一个子文件夹, 文件夹名就是分区名.</strong></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-14-113618.png" alt="image-20180614193618027"></p><ul><li><strong>从本地 load 数据:</strong> <code>hive (mydb)&gt; load data local inpath &#39;ihivedata/intdata&#39; into table t6 partition(class=&#39;job1&#39;);</code></li><li><strong>load 数据指定分区之后, 会直接 load 到数据文件夹里面</strong></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-14-113550.png" alt="image-20180614193549709"></p></li><li><p><strong>2) 没有添加分区时, 直接往不存在的分区导入数据, 分区会自动创建</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 直接往不存在的分区load数据, 分区会自动创建</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'ihivedata/intdata'</span> <span class="keyword">into</span> <span class="keyword">table</span> t6 <span class="keyword">partition</span>(<span class="keyword">class</span>=<span class="string">'job110'</span>);</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p><strong>3) 手动在表中创建分区(文件夹), 并直接向此文件夹中导入数据</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"># 直接创建目录</span><br><span class="line">hive (mydb)&gt; dfs -mkdir /user/hive/warehouse/mydb.db/t6/class=job120;</span><br><span class="line"></span><br><span class="line"># 直接从 hadoop 端传数据</span><br><span class="line">hadoop fs -put ihivedata/intdata /user/hive/warehouse/mydb.db/t6/class=job120</span><br><span class="line"></span><br><span class="line"># 此时再 show partitions t6; 会发现并没有此分区</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">partition</span><br><span class="line">class=job1</span><br><span class="line">class=job110</span><br><span class="line">class=job2</span><br><span class="line">class=job3</span><br><span class="line">class=job4</span><br><span class="line"></span><br><span class="line"># 此时就需要手动'激活'此分区, 加入了就有了</span><br><span class="line">hive (mydb)&gt; alter table t6 add partition(class='job120');</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">hive (mydb)&gt; show partitions t6;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">partition</span><br><span class="line">class=job1</span><br><span class="line">class=job110</span><br><span class="line">class=job120</span><br><span class="line">class=job2</span><br><span class="line">class=job3</span><br><span class="line">class=job4</span><br><span class="line"></span><br><span class="line"># 查看分区信息</span><br><span class="line">hive (mydb)&gt; select * from t6 where class='job110';</span><br><span class="line">OK</span><br><span class="line">t6.idt6.class</span><br><span class="line">1job110</span><br><span class="line">2job110</span><br><span class="line">3job110</span><br><span class="line">4job110</span><br><span class="line">5job110</span><br></pre></td></tr></table></figure></li></ul><h4 id="4-3-4-复合分区"><a href="#4-3-4-复合分区" class="headerlink" title="4.3.4 复合分区"></a>4.3.4 复合分区</h4><h5 id="基本操作-1"><a href="#基本操作-1" class="headerlink" title="基本操作"></a>基本操作</h5><ul><li>创建数据文件<code>partition_table.dat</code></li><li>创建表<ul><li><code>create table t7(name string,age int)partitioned by(class string,city string)row format delimited fields terminated by &#39;\t&#39; stored as TEXTFILE;</code></li></ul></li><li><strong>在 Hive 下加载数据到分区</strong><ul><li><code>load data local inpath &#39;ihivedata/partidata&#39; into table t7 partition(class=&#39;job1&#39;,city=&#39;beijing&#39;);</code></li><li><code>load data local inpath &#39;ihivedata/partidata&#39; into table t7 partition(class=&#39;job1&#39;,city=&#39;shanghai&#39;);</code></li><li><code>load data local inpath &#39;ihivedata/partidata&#39; into table t7 partition(class=&#39;job2&#39;,city=&#39;ss&#39;);</code></li><li><strong>注意: 多级分区其实就是多级目录</strong><ul><li>越靠近左边, 目录层级越高; </li><li>越靠近右边, 目录层级越低; </li></ul></li><li><strong>load 数据到多级分区, load层级必须和整个层级数量相同</strong><ul><li>也就是说, 如果<strong>分区有2层</strong>, <strong>传数据</strong>的时候, 也<strong>必须传2层分区</strong>, 并且<strong>层级顺序必须一致</strong></li></ul></li></ul></li><li><strong>从Linux 本地直接导数据到分区</strong><ul><li><strong>可以直接在 hadoop UI 页面, 查看路径, 然后直接传到此路径中</strong></li><li><code>hadoop fs -put ihivedata/partidata /user/hive/warehouse/mydb.db/t7/class=job1/city=beijing/p2</code></li></ul></li><li>查看数据<ul><li><code>select * from partition_table</code></li><li><code>select count(*) from partition_table</code></li></ul></li><li>删除表<ul><li><code>drop table partition_table</code></li></ul></li><li>工作中 用的最多的是 <strong>外部表 + 分区表</strong> </li></ul><h3 id="4-4-桶表-主要用于抽样查询"><a href="#4-4-桶表-主要用于抽样查询" class="headerlink" title="4.4 桶表 - 主要用于抽样查询"></a>4.4 桶表 - 主要用于抽样查询</h3><h4 id="桶表的基本操作"><a href="#桶表的基本操作" class="headerlink" title="桶表的基本操作"></a>桶表的基本操作</h4><ul><li><strong>创建桶表完整过程</strong></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#创建分桶表</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> stu_buck;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu_buck(Sno <span class="built_in">int</span>,Sname <span class="keyword">string</span>,Sex <span class="keyword">string</span>,Sage <span class="built_in">int</span>,Sdept <span class="keyword">string</span>)</span><br><span class="line">clustered <span class="keyword">by</span>(Sno) </span><br><span class="line">sorted <span class="keyword">by</span>(Sno <span class="keyword">DESC</span>)</span><br><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#设置变量,设置分桶为true, 设置reduce数量是分桶的数量个数</span><br><span class="line"><span class="keyword">set</span> hive.enforce.bucketing = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">4</span>;</span><br><span class="line"></span><br><span class="line">#开始往创建的分通表插入数据(插入数据需要是已分桶, 且排序的)</span><br><span class="line">#可以使用distribute by(sno) sort by(sno asc)   或是排序和分桶的字段相同的时候使用Cluster by(字段)</span><br><span class="line">#注意使用cluster by  就等同于分桶+排序(sort)</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_buck</span><br><span class="line"><span class="keyword">select</span> Sno,Sname,Sex,Sage,Sdept <span class="keyword">from</span> student <span class="keyword">distribute</span> <span class="keyword">by</span>(Sno) <span class="keyword">sort</span> <span class="keyword">by</span>(Sno <span class="keyword">asc</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> stu_buck</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student <span class="keyword">distribute</span> <span class="keyword">by</span>(Sno) <span class="keyword">sort</span> <span class="keyword">by</span>(Sno <span class="keyword">asc</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> stu_buck</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student cluster <span class="keyword">by</span>(Sno);</span><br><span class="line"><span class="comment">-----</span></span><br><span class="line">以上3者效果一样的</span><br></pre></td></tr></table></figure><ul><li><p><strong>保存select查询结果的几种方式：</strong></p><ul><li><p>将查询结果保存到一张新的hive表中</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_tmp</span><br><span class="line"><span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t_p;</span><br></pre></td></tr></table></figure></li></ul></li></ul><ol start="2"><li><p>将查询结果保存到一张已经存在的hive表中</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span>  <span class="keyword">table</span> t_tmp</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t_p;</span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>将查询结果保存到指定的文件目录（可以是本地，也可以是hdfs）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/home/hadoop/test'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t_p;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">directory</span> <span class="string">'/aaa/test'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t_p;</span><br></pre></td></tr></table></figure></li></ol><ul><li><p><strong>数据加载到桶表时，会对字段取hash值，然后与桶的数量取模。把数据放到对应的文件中。</strong></p><ul><li>所以<strong>顺序是打乱的</strong>, 不是原始 t1的数据顺序</li></ul></li><li><p><strong>查看数据</strong></p><ul><li><p>可以直接select * 查看全部</p></li><li><p>也可以直接单独查看每个桶的数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hive (mydb)&gt; dfs -cat /user/hive/warehouse/mydb.db/bucket_table/000000_0;</span><br><span class="line">4</span><br><span class="line">hive (mydb)&gt; dfs -cat /user/hive/warehouse/mydb.db/bucket_table/000001_0;</span><br><span class="line">5</span><br><span class="line">1</span><br><span class="line">hive (mydb)&gt; dfs -cat /user/hive/warehouse/mydb.db/bucket_table/000002_0;</span><br><span class="line">2</span><br><span class="line">hive (mydb)&gt; dfs -cat /user/hive/warehouse/mydb.db/bucket_table/000003_0;</span><br><span class="line">3</span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>修改桶的个数</strong></p><ul><li><code>alter table bucket_table clustered by (id) sorted by(id) into 10 buckets;</code><ul><li>但是这样修改之后, 生成的是原来的 copy, 并且里面的数据也很奇怪, 不知道是按照什么来执行的? <strong>:TODO</strong></li></ul></li></ul></li><li><p><strong>注意：</strong></p><ul><li><strong>物理上，每个桶就是表(或分区）目录里的一个文件</strong></li><li>一个作业产生的<strong>桶(输出文件)和reduce任务个数相同</strong></li></ul></li><li><p><strong>桶表工作中容易遇到的错误</strong></p><ul><li><p><strong>向桶表中插入其它表查出的数据的时候,  必须指定字段名</strong>, 否则会报字段不匹配.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">FAILED: SemanticException [Error 10044]: Line 1:12 Cannot <span class="keyword">insert</span> <span class="keyword">into</span> target <span class="keyword">table</span> because <span class="keyword">column</span> <span class="built_in">number</span>/types <span class="keyword">are</span> different <span class="string">'bucket_table'</span>: <span class="keyword">Table</span> insclause<span class="number">-0</span> has <span class="number">1</span> <span class="keyword">columns</span>, but <span class="keyword">query</span> has <span class="number">2</span> columns.</span><br><span class="line"></span><br><span class="line"># 应该是这样</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> bucket_table <span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> t6;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h4 id="※-桶表的抽样查询"><a href="#※-桶表的抽样查询" class="headerlink" title="※ 桶表的抽样查询"></a>※ 桶表的抽样查询</h4><ul><li>桶表的抽样查询<ul><li>select * from bucket_table tablesample<strong>(bucket 1 out of 4 on id);</strong></li></ul></li><li>tablesample是抽样语句<ul><li>语法解析：<strong>TABLESAMPLE(BUCKET x OUT OF y)</strong></li><li><strong>y必须是table总bucket数的<u>倍数</u>或者<u>因子</u>。</strong></li><li>hive根据y的大小，决定抽样的比例。</li><li><strong>例如</strong>，table总共分了64份，当y=32时，抽取(64/32=)2个bucket的数据，当y=128时，抽取(64/128=)1/2个bucket的数据。x表示从哪个bucket开始抽取。</li><li><strong>例如</strong>，table总bucket数为32，tablesample(bucket 3 out of 16)，表示总共抽取（32/16=）2个bucket的数据，分别为第3个bucket和第（3+16=）19个bucket的数据。</li></ul></li></ul><h1 id="5-Hive-视图的操作"><a href="#5-Hive-视图的操作" class="headerlink" title="5. Hive 视图的操作"></a>5. Hive 视图的操作</h1><ul><li>使用视图可以<strong>降低查询的复杂度</strong></li><li><strong>视图的创建</strong><ul><li><strong>create view</strong> v1 <strong>AS</strong> <u>select  t1.name from t1</u>;</li></ul></li><li><strong>视图的删除</strong><ul><li><strong>drop view</strong> if exists v1;</li></ul></li></ul><h1 id="6-Hive-索引的操作"><a href="#6-Hive-索引的操作" class="headerlink" title="6. Hive 索引的操作"></a>6. Hive 索引的操作</h1><ul><li><p><strong>创建索引</strong></p><ul><li><code>create index t1_index on table t1(id) as &#39;org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler&#39; with deferred rebuild in table t1_index_table;</code></li><li><code>t1_index</code>: <strong>索引名称</strong></li><li>as: 指定索引器</li><li><code>t1_index_table</code>: <strong>要创建的索引表</strong></li></ul></li><li><p><strong>显示索引</strong></p><ul><li><code>show formatted index on t1;</code></li></ul></li><li><p><strong>重建索引</strong></p><ul><li><p><strong>alter index</strong> t1_index <strong>on</strong> t1 <strong>rebuild</strong>;</p></li><li><p>重建完索引之后, 查看 t1_index_table 这张表, 就存了t1表文件具体的位置, 最后一列<code>t1_index_table._offsets</code>是 <strong>索引的偏移量, 类似于指针,  偏移量是索引的精髓</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (mydb)&gt; select * from t1_index_table;</span><br><span class="line">OK</span><br><span class="line">t1_index_table.idt1_index_table._bucketnamet1_index_table._offsets</span><br><span class="line">1hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata[0]</span><br><span class="line">2hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata[2]</span><br><span class="line">3hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata[4]</span><br><span class="line">4hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata[6]</span><br><span class="line">5hdfs://cs1:9000/user/hive/warehouse/mydb.db/t1/intdata[8]</span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>分区字段本质上其实就是索引</strong></p></li></ul><h1 id="7-装载数据"><a href="#7-装载数据" class="headerlink" title="7. 装载数据"></a>7. 装载数据</h1><h3 id="7-1-普通装载数据"><a href="#7-1-普通装载数据" class="headerlink" title="7.1 普通装载数据:"></a><strong>7.1 普通装载数据:</strong></h3><ul><li><strong>从本地</strong>  put</li><li><p><strong>从 hive</strong> cp</p></li><li><p><strong>从文件中装载数据</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;LOAD DATA [LOCAL] INPATH '...' [OVERWRITE] INTO TABLE t2 [PARTITION (province='beijing')];</span><br></pre></td></tr></table></figure></li><li><p><strong>通过查询表装载数据</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 方式1</span><br><span class="line">hive&gt;INSERT OVERWRITE TABLE t2 PARTITION (province='beijing') SELECT * FROM xxx WHERE xxx;</span><br><span class="line"></span><br><span class="line"># 方式2</span><br><span class="line">hive&gt;FROM t4 </span><br><span class="line"> <span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> t3 <span class="keyword">PARTITION</span> (...) <span class="keyword">SELECT</span> ...WHERE... </span><br><span class="line"> <span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> t3 <span class="keyword">PARTITION</span> (...) <span class="keyword">SELECT</span> ...WHERE... </span><br><span class="line"> <span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> t3 <span class="keyword">PARTITION</span> (...) <span class="keyword">SELECT</span> ...WHERE...;</span><br><span class="line"> </span><br><span class="line"> # 方式3</span><br><span class="line"> 直接插入数据, 也会转化为文件的形式, 存在表的目录下</span><br><span class="line">- <span class="keyword">insert</span> <span class="keyword">into</span> table_name  <span class="keyword">values</span>(xxx); </span><br><span class="line"></span><br><span class="line"># 方式4</span><br><span class="line"> 直接传文件</span><br><span class="line"> - <span class="keyword">load</span> <span class="keyword">data</span> (<span class="keyword">local</span>) inpath ‘ xxx’ <span class="keyword">into</span> <span class="keyword">table</span> t_1;</span><br></pre></td></tr></table></figure></li></ul><h3 id="7-2动态装载数据"><a href="#7-2动态装载数据" class="headerlink" title="7.2动态装载数据"></a>7.2动态装载数据</h3><ul><li><p>不开启动态装载时</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;INSERT OVERWRITE TABLE t3 PARTITION(province='bj', city='bj') </span><br><span class="line"><span class="keyword">SELECT</span> t.province, t.city <span class="keyword">FROM</span> temp t <span class="keyword">WHERE</span> t.province=<span class="string">'bj'</span>;</span><br></pre></td></tr></table></figure></li><li><p><strong>开启动态分区支持</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;set hive.exec.dynamic.partition=true;</span><br><span class="line">hive&gt;set hive.exec.dynamic.partition.mode=nostrict;</span><br><span class="line">hive&gt;set hive.exec.max.dynamic.partitions.pernode=1000;</span><br></pre></td></tr></table></figure></li><li><p><strong>把 t6 表的所有的字段 (包括分区字段) 加载进 t9 对应的分区</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (mydb)&gt; insert overwrite table t9 partition(class) select id,class from t6;</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>单语句建表并同时装载数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;CREATE TABLE t4 AS SELECT ....</span><br></pre></td></tr></table></figure></li></ul><h1 id="8-导出数据"><a href="#8-导出数据" class="headerlink" title="8. 导出数据"></a>8. 导出数据</h1><ul><li><strong>在hdfs之间复制文件(夹)</strong><ul><li><code>hadoop fs -cp source destination</code></li><li><code>hive&gt; dfs -cp source destination</code></li><li>案例: <code>hive&gt;  dfs -get  /user/hive/warehouse/mydb.db/t9  /root/t9;</code><ul><li>从 hdfs 复制到本地</li></ul></li></ul></li><li>使用DIRECTORY<ul><li><code>hive&gt;INSERT OVERWRITE 【LOCAL】 DIRECTORY &#39;...&#39; SELECT ...FROM...WHERE ...;</code></li><li>案例:通过查询导出到 t9, 走的 MapReduce<ul><li>导到到 hdfs:  <code>insert overwrite directory &quot;/home/t9&quot; select * from t9;</code></li><li>导出到本地: <code>insert overwrite local directory &quot;/home/ap/t9&quot; select * from t9;</code></li></ul></li></ul></li></ul><h1 id="9-读模式-amp-写模式"><a href="#9-读模式-amp-写模式" class="headerlink" title="9. 读模式&amp;写模式"></a>9. 读模式&amp;写模式</h1><ul><li>RDBMS是写模式</li><li>Hive是读模式</li></ul><h1 id="10-完整建表语句语法"><a href="#10-完整建表语句语法" class="headerlink" title="10. 完整建表语句语法"></a>10. 完整建表语句语法</h1><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db_name.]table_name</span><br><span class="line">  [(col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)]</span><br><span class="line">    [<span class="keyword">COMMENT</span> table_comment]</span><br><span class="line">    [PARTITIONED <span class="keyword">BY</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)]</span><br><span class="line">    [CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) </span><br><span class="line">    [SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span>|<span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS]</span><br><span class="line">    [SKEWED <span class="keyword">BY</span> (col_name, col_name, ...) <span class="keyword">ON</span> ([(col_value, col_value, ...), ...|col_value, col_value, ...]) </span><br><span class="line">    [<span class="keyword">STORED</span> <span class="keyword">AS</span> DIRECTORIES]   ]</span><br><span class="line">  [ [<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format] </span><br><span class="line">    [<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format] | <span class="keyword">STORED</span> <span class="keyword">BY</span> <span class="string">'storage.handler.class.name'</span> [<span class="keyword">WITH</span> SERDEPROPERTIES (...)]   ]</span><br><span class="line">  [LOCATION hdfs_path]</span><br><span class="line">  [TBLPROPERTIES (property_name=property_value, ...)]     </span><br><span class="line">  [<span class="keyword">AS</span> select_statement]  (Note: <span class="keyword">not</span> supported <span class="keyword">when</span> creating <span class="keyword">external</span> tables.)</span><br></pre></td></tr></table></figure><h1 id="11-文件格式"><a href="#11-文件格式" class="headerlink" title="11. 文件格式"></a>11. 文件格式</h1><ul><li>TextFile</li><li>SequenceFile</li><li>RCFile</li><li>ORC</li></ul><h3 id="11-1-默认存储格式就是-TextFile"><a href="#11-1-默认存储格式就是-TextFile" class="headerlink" title="11.1 默认存储格式就是 TextFile"></a>11.1 默认存储格式就是 TextFile</h3><ul><li>存储空间消耗比较大，</li><li>并且压缩的text 无法分割和合并</li><li>查询的效率最低,可以直接存储，</li><li>加载数据的速度最高</li></ul><h3 id="11-2-使用SequenceFile存储"><a href="#11-2-使用SequenceFile存储" class="headerlink" title="11.2 使用SequenceFile存储"></a>11.2 使用SequenceFile存储</h3><ul><li>存储空间消耗大</li><li>压缩的文件可以分割和合并 </li><li>查询效率高</li><li>需要通过text文件转化来加载</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test2(<span class="keyword">str</span> <span class="keyword">STRING</span>) <span class="keyword">STORED</span> <span class="keyword">AS</span> SEQUENCEFILE;  </span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> hive.exec.compress.output=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> mapred.output.compress=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> mapred.output.compression.codec=com.hadoop.compression.lzo.LzoCodec;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> io.seqfile.compression.type=<span class="keyword">BLOCK</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> io.compression.codecs=com.hadoop.compression.lzo.LzoCodec;</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> test2 <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> test1;</span><br></pre></td></tr></table></figure><p><strong>注意点: SequenceFile 类型的表, 不能直接导入数据文件,  只能通过从他表查询</strong></p><ul><li><p><code>insert overwrite table t2 select * from t1;</code></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 查看此 'SequenceFile' 表</span><br><span class="line">hive (db2)&gt; dfs -cat /user/hive/warehouse/db2.db/t2/000000_0 ;</span><br><span class="line">SEQ"org.apache.hadoop.io.BytesWritableorg.apache.hadoop.io.Text*org.apache.hadoop.io.compress.DefaultCodec���/*&lt;bb�m�?x�c453x�c464x�c475x�c486x�c497hive (db2)&gt;</span><br></pre></td></tr></table></figure></li></ul><h3 id="11-3-使用RCFile存储"><a href="#11-3-使用RCFile存储" class="headerlink" title="11.3 使用RCFile存储"></a>11.3 使用RCFile存储</h3><p><strong>RCFILE是一种行列存储相结合的存储方式。首先，其将数据按行分块，保证同一个record在一个块上，避免读一个记录需要读取多个block。其次，块数据列式存储，有利于数据压缩和快速的列存取。</strong></p><ul><li>rcfile 存储空间最小</li><li>查询的效率最高</li><li>需要通过text文件转化来加载</li><li>加载的速度最低</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test3(<span class="keyword">str</span> <span class="keyword">STRING</span>)  <span class="keyword">STORED</span> <span class="keyword">AS</span> RCFILE;  </span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> hive.exec.compress.output=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> mapred.output.compress=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> mapred.output.compression.codec=com.hadoop.compression.lzo.LzoCodec;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> io.compression.codecs=com.hadoop.compression.lzo.LzoCodec;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> test3 <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> test1;</span><br></pre></td></tr></table></figure><p><strong>注意点:  RCFile 也只能从其它表导入数据</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (db2)&gt; dfs -cat /user/hive/warehouse/db2.db/t3/000000_0;</span><br><span class="line">RCF*org.apache.hadoop.io.compress.DefaultCodechive.io.rcfile.column.number1</span><br></pre></td></tr></table></figure><h3 id="11-4-使用ORC存储-最好的一种格式"><a href="#11-4-使用ORC存储-最好的一种格式" class="headerlink" title="11.4 使用ORC存储(最好的一种格式)"></a>11.4 使用ORC存储(最好的一种格式)</h3><p><strong>是一种针对 RCFile 优化的格式</strong></p><p>主要特点: <strong>压缩, 索引, 单文件输出</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-15-070816.jpg" alt=""></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t1_orc(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span> <span class="keyword">stored</span> <span class="keyword">as</span> orc </span><br><span class="line"></span><br><span class="line">tblproperties(<span class="string">"orc.compress"</span>=<span class="string">"ZLIB"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> ... [<span class="keyword">PARTITION</span> partition_spec] <span class="keyword">SET</span> FILEFORMAT ORC;</span><br><span class="line"></span><br><span class="line"># 也可以改为其它的, 修改的语法就是这样</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> t1 <span class="keyword">set</span> fileformat textfile;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SET</span> hive.default.fileformat=Orc;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> t1_orc <span class="keyword">select</span> * <span class="keyword">from</span> t1;</span><br></pre></td></tr></table></figure><p><strong>注意点:  </strong></p><ul><li><strong>ORC 也只能从其它表导入数据</strong></li><li><strong>占用空间大, 一个 block 有256M, 之前2种都是128M</strong></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive (db2)&gt; dfs -cat /user/hive/warehouse/db2.db/t4/000000_0;</span><br><span class="line">ORC</span><br><span class="line">P+</span><br><span class="line">P�6�b�``���ь@�H�</span><br><span class="line">                       1q01-</span><br><span class="line">P</span><br><span class="line">PK</span><br><span class="line"># ("</span><br><span class="line">       id0P:</span><br><span class="line">P@�;��"</span><br><span class="line">       (0��ORChive</span><br></pre></td></tr></table></figure><h1 id="12-序列化-amp-反序列化-Hive-SerDe"><a href="#12-序列化-amp-反序列化-Hive-SerDe" class="headerlink" title="12. 序列化 &amp; 反序列化 (Hive SerDe)"></a>12. 序列化 &amp; 反序列化 (Hive SerDe)</h1><h3 id="12-1-SerDe"><a href="#12-1-SerDe" class="headerlink" title="12.1 SerDe"></a>12.1 SerDe</h3><ul><li><strong>What is a SerDe?</strong><ul><li>SerDe 是 “Serializer and Deserializer.”的缩写</li><li>Hive 使用 SerDe和FileFormat进行行内容的读写.</li></ul></li><li><strong>Hive序列化流程</strong><ul><li><strong>从 HDFS 上读入文件 (反序列化)</strong><ul><li><code>HDFS文件 --&gt; InputFileFormat --&gt; &lt;key, value&gt; --&gt; Deserializer --&gt; 行对象</code></li></ul></li><li><strong>写出到 HDFS (序列化)</strong><ul><li><code>行对象 --&gt; Serializer --&gt; &lt;key, value&gt; --&gt; OutputFileFormat --&gt; HDFS文件</code></li></ul></li><li>注意: <strong>数据全部存在在value中，key内容无意义</strong></li></ul></li></ul><ul><li><p><strong>Hive 使用如下FileFormat 类读写 HDFS files:</strong></p><ul><li>TextInputFormat/HiveIgnoreKeyTextOutputFormat: 读写普通HDFS文本文件.</li><li>SequenceFileInputFormat/SequenceFileOutputFormat: 读写SequenceFile格式的HDFS文件</li><li>….</li></ul></li></ul><ul><li><p><strong>Hive 使用如下SerDe 类(反)序列化数据:</strong></p><ul><li>MetadataTypedColumnsetSerDe: 读写csv、tsv文件和默认格式文件</li><li>ThriftSerDe: 读写Thrift 序列化后的对象.</li><li>DynamicSerDe: 读写Thrift序列化后的对象, 不过不需要解读schema中的ddl.</li></ul></li></ul><h3 id="12-2-使用CSV-Serde"><a href="#12-2-使用CSV-Serde" class="headerlink" title="12.2 使用CSV Serde"></a>12.2 使用CSV Serde</h3><p>CSV格式的文件也称为逗号分隔值（Comma-Separated Values，CSV，有时也称为字符分隔值，因为分隔字符也可以不是逗号。在本文中的CSV格式的数据就不是简单的逗号分割的），其文件以纯文本形式存储表格数据（数字和文本）。CSV文件由任意数目的记录组成，记录间以某种换行符分隔；每条记录由字段组成，字段间的分隔符是其它字符或字符串，最常见的是逗号或制表符。通常，所有记录都有完全相同的字段序列。<br>默认的分隔符是</p><blockquote><p>DEFAULT_ESCAPE_CHARACTER \<br>DEFAULT_QUOTE_CHARACTER  “     —如果没有，则不需要指定<br>DEFAULT_SEPARATOR  , </p></blockquote><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> csv_table(a <span class="keyword">string</span>, b <span class="keyword">string</span>) <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> SERDE <span class="string">'org.apache.hadoop.hive.serde2.OpenCSVSerde'</span> <span class="keyword">WITH</span> SERDEPROPERTIES (<span class="string">"separatorChar"</span>=<span class="string">"\t"</span>, <span class="string">"quoteChar"</span>=<span class="string">"'"</span>, <span class="string">"escapeChar"</span>=<span class="string">"\\"</span>)  <span class="keyword">STORED</span> <span class="keyword">AS</span> TEXTFILE;</span><br><span class="line"></span><br><span class="line"># separatorChar：分隔符</span><br><span class="line"># quoteChar：引号符</span><br><span class="line"># escapeChar：转义符</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt;&gt;  :TODO 创建表没成功, 用到时再说</span><br></pre></td></tr></table></figure><h1 id="13-Lateral-View-语法"><a href="#13-Lateral-View-语法" class="headerlink" title="13. Lateral View 语法"></a>13. Lateral View 语法</h1><p>lateral view用于和split, explode等UDTF一起使用，它能够<strong>将一行数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合</strong>。lateral view首先为原始表的每行调用UDTF，UTDF会把一行拆分成一或者多行，lateral view再把结果组合，产生一个<strong>支持别名表的虚拟表</strong>。</p><p><strong>lateral: 侧面, 横切面</strong></p><p><strong>Lateral View: 切面表</strong></p><ul><li><p>创建表</p><ul><li><strong>create table t8(name string,nums array\&lt;int>)row format delimited fields terminated by “\t” COLLECTION ITEMS TERMINATED BY ‘:’;</strong></li></ul></li><li><p>数据切割</p><ul><li><strong>SELECT name,new_num FROM t8 LATERAL VIEW explode(nums) num AS new_num;</strong></li><li><code>select name,id from class_test lateral view explode(student_id_list) list as id;</code></li><li><strong>注意: as 前面的 <code>list</code> 貌似是可以随表起名的</strong></li></ul></li><li><p>效果演示</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from class_test;</span><br><span class="line">OK</span><br><span class="line">class_test.nameclass_test.student_id_list</span><br><span class="line">zhangsan[24,25,27,37]</span><br><span class="line">lisi[28,39,23,43]</span><br><span class="line">wangwu[25,23,2,54]</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">hive (default)&gt; select name,id from class_test lateral view explode(student_id_list) list as id;</span><br><span class="line">OK</span><br><span class="line">nameid</span><br><span class="line">zhangsan24</span><br><span class="line">zhangsan25</span><br><span class="line">zhangsan27</span><br><span class="line">zhangsan37</span><br><span class="line">lisi28</span><br><span class="line">lisi39</span><br><span class="line">lisi23</span><br><span class="line">lisi43</span><br><span class="line">wangwu25</span><br><span class="line">wangwu23</span><br><span class="line">wangwu2</span><br><span class="line">wangwu54</span><br></pre></td></tr></table></figure></li></ul><h1 id="14-Hive的高级函数"><a href="#14-Hive的高级函数" class="headerlink" title="14. Hive的高级函数"></a>14. Hive的高级函数</h1><h3 id="14-1-简介"><a href="#14-1-简介" class="headerlink" title="14.1 简介"></a>14.1 简介</h3><ul><li>简单查询<ul><li>select … from…where…</li></ul></li><li><strong>使用各种函数</strong><ul><li><strong>hive&gt;show functions;</strong><ul><li>展示所有函数</li></ul></li><li><strong>hive&gt;describe function  xxx;</strong><ul><li>详细描述函数用法</li></ul></li></ul></li><li>LIMIT语句</li><li>列别名</li><li>嵌套select语句</li></ul><h3 id="14-2-高级函数分类"><a href="#14-2-高级函数分类" class="headerlink" title="14.2 高级函数分类"></a>14.2 高级函数分类</h3><ul><li><p>标准函数</p><ul><li>reverse()</li><li>upper()</li></ul></li><li><p>聚合函数</p><ul><li><p>avg()</p></li><li><p>sum()</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">avg</span>(<span class="keyword">id</span>) <span class="keyword">from</span> t3;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">sum</span>(<span class="keyword">id</span>) <span class="keyword">from</span> t3;</span><br><span class="line"></span><br><span class="line"># 最简答的嵌套</span><br><span class="line">hive (mydb)&gt; select t.id from (select * from t1 where id &gt; 3)t;</span><br><span class="line"></span><br><span class="line"># 最简单的 group by</span><br><span class="line"></span><br><span class="line"># if else 的效果</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, </span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> </span><br><span class="line"><span class="keyword">when</span> <span class="keyword">id</span> &lt;= <span class="number">2</span> </span><br><span class="line"><span class="keyword">then</span> <span class="string">'low'</span></span><br><span class="line"><span class="keyword">when</span> <span class="keyword">id</span>&gt;=<span class="number">3</span> <span class="keyword">and</span> <span class="keyword">id</span> &lt;<span class="number">4</span> </span><br><span class="line"><span class="keyword">then</span> <span class="string">'middle'</span></span><br><span class="line"><span class="keyword">when</span> <span class="keyword">id</span>&gt;=<span class="number">4</span> <span class="keyword">and</span> <span class="keyword">id</span> &lt;<span class="number">5</span> </span><br><span class="line"><span class="keyword">then</span> <span class="string">'high'</span></span><br><span class="line"><span class="keyword">else</span> </span><br><span class="line"><span class="string">'very high'</span></span><br><span class="line"><span class="keyword">end</span> </span><br><span class="line"></span><br><span class="line"><span class="keyword">as</span> </span><br><span class="line">id_highly </span><br><span class="line"><span class="keyword">from</span> t1;</span><br><span class="line"></span><br><span class="line"><span class="comment">----执行结果----</span></span><br><span class="line">idid_highly</span><br><span class="line">1low</span><br><span class="line">2low</span><br><span class="line">3middle</span><br><span class="line">4high</span><br><span class="line">5very high</span><br><span class="line"></span><br><span class="line"><span class="comment">--------------------</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">sid</span>,</span><br><span class="line"><span class="keyword">case</span> course</span><br><span class="line"><span class="keyword">when</span> <span class="string">'yuwen'</span> <span class="keyword">then</span> score</span><br><span class="line"><span class="keyword">else</span> <span class="string">'0'</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">(别名) yuwen</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> course</span><br><span class="line"><span class="keyword">when</span> <span class="string">'shuxue'</span> <span class="keyword">then</span> score</span><br><span class="line"><span class="keyword">else</span> <span class="string">'0'</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">(别名) shuxue</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># <span class="keyword">cast</span> 转换函数, 大概是这么用</span><br><span class="line">hive (mydb)&gt; <span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> t1 <span class="keyword">where</span> <span class="keyword">cast</span>(<span class="keyword">id</span> <span class="keyword">AS</span> <span class="built_in">FLOAT</span>) &lt;<span class="number">3.0</span>;</span><br><span class="line">OK</span><br><span class="line">id</span><br><span class="line">1</span><br><span class="line">2</span><br></pre></td></tr></table></figure><pre><code>- ![image-20180619150824031](http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-070824.png)</code></pre></li></ul></li></ul><p><strong>首先当前不存在的列补0, 然后按照学号分组求和</strong></p><p>​    </p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-070624.png" alt="image-20180619150623836"></p><ul><li>然后按照 sid 分组求和/求最大值, 就可以了</li><li>同一列不同的放在不同的列上, 常用的方法</li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-071819.png" alt="image-20180619151819409"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-072342.png" alt="image-20180619152341523"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-072352.png" alt="image-20180619152351866"></p><hr><p>面试题4</p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-075926.png" alt="image-20180619155925871"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-080134.png" alt="image-20180619160134008"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-080605.png" alt="image-20180619160604750"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-080942.png" alt="image-20180619160941777"></p><p>array_contains()</p><p>desc function array_contains()</p><ul><li><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-081706.jpg?ff=fh3" alt=""></li></ul><ul><li><p>自定义函数</p><ul><li>UDF</li></ul></li></ul><hr><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-082423.png" alt="image-20180619162423281"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-084428.png" alt="image-20180619164428063"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-19-084655.png" alt="image-20180619164654514"></p><h1 id="15-Hive-性能调优"><a href="#15-Hive-性能调优" class="headerlink" title="15. Hive 性能调优"></a>15. Hive 性能调优</h1><h3 id="15-1-什么时候可以避免执行MapReduce？"><a href="#15-1-什么时候可以避免执行MapReduce？" class="headerlink" title="15.1 什么时候可以避免执行MapReduce？"></a>15.1 什么时候可以避免执行MapReduce？</h3><ul><li><p><code>select *  or  select field1,field2</code></p></li><li><p><code>limite 10</code></p></li><li><p>where语句中只有分区字段</p></li><li><p>使用本地<code>set hive.exec.mode.local.auto=true;</code></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t3;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,age <span class="keyword">from</span> t3;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,age <span class="keyword">from</span> t3 <span class="keyword">limit</span> <span class="number">2</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,age <span class="keyword">from</span> t3 <span class="keyword">where</span> age=<span class="number">25</span>;</span><br><span class="line"># 当 where 是本地字段(列中的字段), 是不走 MR的</span><br></pre></td></tr></table></figure></li><li><p>group by语句：</p><ul><li>通常和聚合函数一起使用，按照一个或者多个列对结果进行分组，然后对每组执行聚合操作</li></ul></li><li>having语句：<ul><li>限制结果的输出</li></ul></li><li>hive将查询转化为MapReduce执行，hive的优化可以转化为mapreduce的优化！</li></ul><h3 id="15-2-hive是如何将查询转化为MapReduce的？-EXPLAIN的使用"><a href="#15-2-hive是如何将查询转化为MapReduce的？-EXPLAIN的使用" class="headerlink" title="15.2 hive是如何将查询转化为MapReduce的？-EXPLAIN的使用"></a><strong>15.2 hive是如何将查询转化为MapReduce的？-EXPLAIN的使用</strong></h3><ul><li>hive对sql的查询计划信息解析</li><li><p>EXPLAIN SELECT COUNT(1) FROM T1;</p></li><li><p><strong>EXPLAIN EXTENDED</strong> </p><ul><li><p><strong>显示详细扩展查询计划信息</strong></p></li><li><p><strong><code>EXPLAIN EXTENDED SELECT COUNT(1) FROM T1;</code></strong></p></li><li><p>为啥我的 explain extended 只有固定的几行?</p><ul><li><strong>因为这个 count 没有调动 MR, 用 sum 就会启用 MR, 会出现长长的 log</strong></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">---不会启用 MR</span></span><br><span class="line">hive (mydb)&gt; explain EXTENDED select count(1) from t9;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line"><span class="keyword">Explain</span></span><br><span class="line">STAGE DEPENDENCIES:</span><br><span class="line">  Stage<span class="number">-0</span> <span class="keyword">is</span> a root stage</span><br><span class="line">  ...</span><br><span class="line"><span class="comment">-----------------------------</span></span><br><span class="line"><span class="comment">---这里会启用 MR</span></span><br><span class="line">hive (mydb)&gt; <span class="keyword">explain</span> <span class="keyword">EXTENDED</span> <span class="keyword">select</span> <span class="keyword">sum</span>(<span class="keyword">id</span>) <span class="keyword">from</span> t9;</span><br><span class="line"><span class="comment">---</span></span><br><span class="line"><span class="keyword">Explain</span></span><br><span class="line">STAGE DEPENDENCIES:</span><br><span class="line">  Stage<span class="number">-1</span> <span class="keyword">is</span> a root stage</span><br><span class="line">  Stage<span class="number">-0</span> depends <span class="keyword">on</span> stages: Stage<span class="number">-1</span></span><br><span class="line"></span><br><span class="line">STAGE PLANS:</span><br><span class="line">  Stage: Stage<span class="number">-1</span></span><br><span class="line">  ....</span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="15-3-性能调优"><a href="#15-3-性能调优" class="headerlink" title="15.3 性能调优"></a>15.3 性能调优</h3><h4 id="15-3-1-本地mr"><a href="#15-3-1-本地mr" class="headerlink" title="15.3.1 本地mr"></a>15.3.1 本地mr</h4><ul><li><p><strong>本地模式设置方式：</strong></p><ul><li><code>set mapred.job.tracker=local;</code><ul><li>mapper 的本地模式, <strong>只有在开发中才会使用</strong></li></ul></li><li><strong><code>set hive.exec.mode.local.auto=true;</code></strong><ul><li><strong>Hive 的执行模式</strong></li><li><strong>可以用在生产中</strong>, 因为是自动模式, 可根据参数变化</li><li><strong>设置这里才会转成 local  hadoop</strong></li><li>按照这里设定的规则<code>hive.exec.mode.local.auto.input.files.max</code></li></ul></li><li>测试 select 1 from wlan limit 5;</li></ul></li><li><p><strong>下面两个参数是local mr中常用的控制参数:</strong></p><ul><li><p><code>hive.exec.mode.local.auto.inputbytes.max</code>默认134217728</p><ul><li><strong>设置local mr的最大输入数据量</strong>,当输入数据量小于这个值的时候会采用local  mr的方式</li></ul></li><li><p><strong><code>hive.exec.mode.local.auto.input.files.max</code>默认是4</strong></p><ul><li><p>设置local mr的最大输入文件个数,当输入文件个数小于这个值的时候会采用local mr的方式</p></li><li><p>大于此数时, 就不会转化为 local hadoop</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Cannot run job locally: Number of Input Files (= 6) is larger than hive.exec.mode.local.auto.input.files.max(= 4)</span><br></pre></td></tr></table></figure></li><li><p><strong>可以这样修改local mr的最大输入文件个数值, 主要在调试阶段使用</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (mydb)&gt; set hive.exec.mode.local.auto.input.files.max=8;</span><br><span class="line"></span><br><span class="line"># 这样设置了之后, 只要文件数&lt;=8, 就会在本地运行</span><br><span class="line">Job running in-process (local Hadoop)</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul><h4 id="15-3-2-开启并行计算"><a href="#15-3-2-开启并行计算" class="headerlink" title="15.3.2 开启并行计算"></a>15.3.2 开启并行计算</h4><ul><li>开启并行计算,增加集群的利用率<ul><li><code>set hive.exec.parallel=true;</code></li></ul></li></ul><h4 id="15-3-3-设置严格模式"><a href="#15-3-3-设置严格模式" class="headerlink" title="15.3.3 设置严格模式"></a>15.3.3 设置严格模式</h4><ul><li><strong>设置严格模式</strong><ul><li><code>set hive.mapred.mode=strict;</code></li></ul></li><li>设置非严格模式<ul><li><code>set hive.mapred.mode=nonstrict;</code></li></ul></li><li><strong>strict可以禁止三种类型的查询：</strong><ul><li>强制分区表的where条件过滤</li><li>Order by语句必须使用limit<ul><li><code>hive (mydb)&gt; select id from t9 where class=&#39;job110&#39; order by id limit 3;</code></li></ul></li><li>限制笛卡尔积查询</li></ul></li></ul><h4 id="15-3-4-调整mapper和reducer的数量"><a href="#15-3-4-调整mapper和reducer的数量" class="headerlink" title="15.3.4 调整mapper和reducer的数量"></a>15.3.4 调整mapper和reducer的数量</h4><ul><li>调整mapper和reducer的数量<ul><li>太多map导致启动产生过多开销</li><li><code>marpred.min.split.size</code></li><li>按照输入数据量大小确定reducer数目,<ul><li>`set mapred.reduce.tasks=  默认3</li><li>dfs -count  /分区目录/* </li><li>hive.exec.reducers.max设置阻止资源过度消耗</li></ul></li></ul></li><li>JVM重用<ul><li>小文件多或task多的业务场景</li><li>set mapred.job.reuse.jvm.num.task=10</li><li>会一直占用task槽</li></ul></li></ul><h4 id="15-3-5-排序方面的优化"><a href="#15-3-5-排序方面的优化" class="headerlink" title="15.3.5 排序方面的优化"></a>15.3.5 排序方面的优化</h4><ul><li><p><strong>order by</strong> 语句：是<strong>全局</strong>排序, 用的比较多</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 加个 desc 就是倒序排序</span><br><span class="line">hive (mydb)&gt; select id from bucket_table order by id desc limit 10;</span><br><span class="line">Automatically selecting local only mode for query</span><br></pre></td></tr></table></figure><p> <strong>sort by</strong> 语句：      是<strong>单reduce</strong>排序</p><ul><li>一般与 <code>distribute by</code>结合使用</li></ul></li><li><p><strong>distribute by</strong>语句：类似于分桶，<strong>根据指定的字段</strong>将<strong>数据分到不同的 reducer</strong>，且分发算法是 <strong>hash 散列</strong></p><ul><li><p><strong>与 sort by 结合用的比较多</strong>, <strong>在每个分区内有序</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 注意此处 distribute 的作用</span><br><span class="line">hive (mydb)&gt; select id from bucket_table distribute by id sort by id desc limit 10;</span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>cluster by</strong>语句：</p><ul><li><code>select * from t9 cluster by id;</code></li><li>可以确保类似的数据的分发到同一个reduce task中，并且保证数据有序, 防止所有的数据分发到同一个reduce上，导致整体的job时间延长</li></ul></li><li><p><strong>cluster by语句的等价语句：</strong></p><ul><li><em>如果分桶和sort字段是同一个时，此时，cluster by = distribute by + sort by</em> </li><li><code>distribute by Word sort by Word ASC</code></li></ul></li></ul><h4 id="15-3-6-Map-side聚合"><a href="#15-3-6-Map-side聚合" class="headerlink" title="15.3.6 Map-side聚合"></a>15.3.6 Map-side聚合</h4><p><strong>可以直接在 <code>.hiverc</code>中配置</strong></p><ul><li><code>set hive.map.aggr=true;</code></li><li>这个设置可以将顶层的聚合操作放在Map阶段执行，从而减轻清洗阶段数据传输和Reduce阶段的执行时间，提升总体性能。:TODO 不太懂</li><li>缺点：该设置会消耗更多的内存。</li><li>执行select count(1) from wlan;</li></ul><h4 id="15-3-6-Join-优化"><a href="#15-3-6-Join-优化" class="headerlink" title="15.3.6 Join 优化"></a>15.3.6 Join 优化</h4><ul><li>优先过滤后再进行 Join 操作，最大限度的减少参与 join 的数据量</li><li>小表 join 大表，最好启动 mapjoin<ul><li>在使用写有 Join 操作的查询语句时有一条原则:应该将条目少的表/子查询放在 Join 操作 符的左边。 </li></ul></li><li>Join on 的条件相同的话，最好放入同一个 job，并且 join 表的排列顺序从小到大</li></ul><blockquote><p>在编写 Join 查询语句时，如果确定是由于 join 出现的数据倾斜，那么请做如下设置:</p><p>set hive.skewjoin.key=100000; // 这个是 join 的键对应的记录条数超过这个值则会进行</p><p>分拆，值根据具体数据量设置</p><p>set hive.optimize.skewjoin=true; // 如果是 join 过程出现倾斜应该设置为 true</p></blockquote><h1 id="16-表连接-只支持等值连接"><a href="#16-表连接-只支持等值连接" class="headerlink" title="16. 表连接  (只支持等值连接)"></a>16. 表连接  (只支持等值连接)</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-16-071623.jpg" alt=""></p><h3 id="16-1-简介"><a href="#16-1-简介" class="headerlink" title="16.1 简介"></a>16.1 简介</h3><ul><li><p>INNER JOIN</p><ul><li>两张表中都有，且两表符合连接条件</li><li>select t1.name,t1.age,t9.age from t9 join t1 on t1.name=t9.name;</li></ul></li><li><p><strong>LEFT OUTER JOIN</strong></p><ul><li>左表中符合where条件出现，右表可以为空</li><li><strong>从左表返回所有的行(字段), 右表没有匹配where 条件的话返回null</strong></li></ul></li><li><p>RIGHT OUTER JOIN</p><ul><li>右表中符合where条件出现，左表可以为空</li></ul></li><li><p>FULL OUTER JOIN</p><ul><li>返回所有表符合where条件的所有记录，没有NULL替代</li></ul></li><li><p><strong>LEFT SEMI-JOIN</strong></p><ul><li><p>左表中符合右表on条件出现，右表不出现</p></li><li><p>Hive 当前<strong>没有实现 IN/EXISTS 子查询，可以用 LEFT SEMI JOIN 重写子查询语句</strong>。LEFT SEMI JOIN 的限制是， JOIN 子句中右边的表只能在 ON 子句中设置过滤条件，在 WHERE 子句、SELECT 子句或其他地方过滤都不行。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> a.key, a.value <span class="keyword">FROM</span> a</span><br><span class="line"><span class="keyword">WHERE</span> a.key <span class="keyword">in</span> (<span class="keyword">SELECT</span> b.key <span class="keyword">FROM</span> B);</span><br><span class="line"></span><br><span class="line"># 可以被重写为：</span><br><span class="line"><span class="keyword">SELECT</span> a.key, a.val <span class="keyword">FROM</span> a <span class="keyword">LEFT</span> SEMI <span class="keyword">JOIN</span> b <span class="keyword">on</span> (a.key = b.key)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>笛卡尔积</p><ul><li>是m x n的结果</li></ul></li><li><p><strong>map-side JOIN</strong></p><ul><li>只有一张小表，在mapper的时候将小表完全放在内存中</li><li>select /<em>+ mapjoin(t9) </em>/t1.name,t1.age from t9 JOIN t1on t1.name=t9.name;</li></ul></li></ul><h3 id="16-2-代码测试"><a href="#16-2-代码测试" class="headerlink" title="16.2  代码测试"></a>16.2  代码测试</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line">关于hive中的各种join</span><br><span class="line"></span><br><span class="line">准备数据</span><br><span class="line">1,a</span><br><span class="line">2,b</span><br><span class="line">3,c</span><br><span class="line">4,d</span><br><span class="line">7,y</span><br><span class="line">8,u</span><br><span class="line"></span><br><span class="line">2,bb</span><br><span class="line">3,cc</span><br><span class="line">7,yy</span><br><span class="line">9,pp</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">建表：</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> a(<span class="keyword">id</span> <span class="built_in">int</span>,<span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> b(<span class="keyword">id</span> <span class="built_in">int</span>,<span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br><span class="line"></span><br><span class="line">导入数据：</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/a.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> a;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/b.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> b;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">实验：</span><br><span class="line">** inner join</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> a <span class="keyword">inner</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.id=b.id;</span><br><span class="line">+<span class="comment">-------+---------+-------+---------+--+</span></span><br><span class="line">| a.id  | a.name  | b.id  | b.name  |</span><br><span class="line">+<span class="comment">-------+---------+-------+---------+--+</span></span><br><span class="line">| 2     | b       | 2     | bb      |</span><br><span class="line">| 3     | c       | 3     | cc      |</span><br><span class="line">| 7     | y       | 7     | yy      |</span><br><span class="line">+<span class="comment">-------+---------+-------+---------+--+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**left join</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> a <span class="keyword">left</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.id=b.id;</span><br><span class="line">+<span class="comment">-------+---------+-------+---------+--+</span></span><br><span class="line">| a.id  | a.name  | b.id  | b.name  |</span><br><span class="line">+<span class="comment">-------+---------+-------+---------+--+</span></span><br><span class="line">| 1     | a       | NULL  | NULL    |</span><br><span class="line">| 2     | b       | 2     | bb      |</span><br><span class="line">| 3     | c       | 3     | cc      |</span><br><span class="line">| 4     | d       | NULL  | NULL    |</span><br><span class="line">| 7     | y       | 7     | yy      |</span><br><span class="line">| 8     | u       | NULL  | NULL    |</span><br><span class="line">+<span class="comment">-------+---------+-------+---------+--+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**right join</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> a <span class="keyword">right</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.id=b.id;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> a <span class="keyword">full</span> <span class="keyword">outer</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.id=b.id;</span><br><span class="line">+<span class="comment">-------+---------+-------+---------+--+</span></span><br><span class="line">| a.id  | a.name  | b.id  | b.name  |</span><br><span class="line">+<span class="comment">-------+---------+-------+---------+--+</span></span><br><span class="line">| 1     | a       | NULL  | NULL    |</span><br><span class="line">| 2     | b       | 2     | bb      |</span><br><span class="line">| 3     | c       | 3     | cc      |</span><br><span class="line">| 4     | d       | NULL  | NULL    |</span><br><span class="line">| 7     | y       | 7     | yy      |</span><br><span class="line">| 8     | u       | NULL  | NULL    |</span><br><span class="line">| NULL  | NULL    | 9     | pp      |</span><br><span class="line">+<span class="comment">-------+---------+-------+---------+--+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> a <span class="keyword">left</span> semi <span class="keyword">join</span> b <span class="keyword">on</span> a.id = b.id;</span><br><span class="line">+<span class="comment">-------+---------+--+</span></span><br><span class="line">| a.id  | a.name  |</span><br><span class="line">+<span class="comment">-------+---------+--+</span></span><br><span class="line">| 2     | b       |</span><br><span class="line">| 3     | c       |</span><br><span class="line">| 7     | y       |</span><br><span class="line">+<span class="comment">-------+---------+--+</span></span><br></pre></td></tr></table></figure><p><br></p><h1 id="17-Hive自定义函数-amp-Transform"><a href="#17-Hive自定义函数-amp-Transform" class="headerlink" title="17.  Hive自定义函数 &amp; Transform"></a>17.  Hive自定义函数 &amp; Transform</h1><h3 id="17-1-自定义函数类别"><a href="#17-1-自定义函数类别" class="headerlink" title="17.1 自定义函数类别"></a>17.1 自定义函数类别</h3><ul><li><strong>UDF</strong>  <ul><li>作用于单个数据行，产生一个数据行作为输出。（数学函数，字符串函数）</li></ul></li><li><strong>UDAF</strong><ul><li>(用户定义聚集函数)：接收多个输入数据行，并产生一个输出数据行。（count，max）</li></ul></li></ul><h3 id="17-2-UDF-开发实例"><a href="#17-2-UDF-开发实例" class="headerlink" title="17.2 UDF 开发实例"></a>17.2 UDF 开发实例</h3><h4 id="17-2-1-简单入门"><a href="#17-2-1-简单入门" class="headerlink" title="17.2.1 简单入门"></a>17.2.1 简单入门</h4><ul><li><p>先开发一个java类，继承UDF，并重载evaluate方法 </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.hive.udf;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ToLowerCase</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 必须是 public</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">evaluate</span><span class="params">(String field)</span> </span>&#123;</span><br><span class="line">String res = field.toLowerCase();</span><br><span class="line"><span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>打成jar包上传到服务器 </p></li><li><p>将jar包添加到hive的classpath </p><ul><li><code>hive&gt;add JAR /home/ap/udf.jar;</code></li></ul></li><li><p>创建临时函数与开发好的java class关联 </p><ul><li><code>Hive&gt;create temporary function tolowercase as &#39;com.rox.hive.udf.ToLowerCase&#39;;</code></li></ul></li><li><p>即可在hql中使用自定义的函数strip  </p><ul><li><code>select tolowercase(name) from t_1..</code></li></ul></li></ul><h4 id="17-2-2-稍稍复杂"><a href="#17-2-2-稍稍复杂" class="headerlink" title="17.2.2 稍稍复杂"></a>17.2.2 稍稍复杂</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"># 需求: 通过一些手机号判断手机区域</span><br><span class="line">1364535532,10</span><br><span class="line">1374535532,42</span><br><span class="line">1384535532,34</span><br><span class="line">1364535532,45</span><br><span class="line">1384535532,22</span><br><span class="line">136-beijing</span><br><span class="line">137-shanghai</span><br><span class="line">138-guangzhou</span><br><span class="line"><span class="comment">-----</span></span><br><span class="line"></span><br><span class="line">## 1.编写 UDF</span><br><span class="line">public static HashMap&lt;String, String&gt; provinceMap = new HashMap&lt;String, String&gt;();</span><br><span class="line">static &#123;</span><br><span class="line">provinceMap.put("136", "beijing");</span><br><span class="line">provinceMap.put("136", "shanghai");</span><br><span class="line">provinceMap.put("136", "guangzhou");</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public String evaluate(String phoneNum) &#123;</span><br><span class="line">return provinceMap.get(phoneNum.substring(0, 3)) == null ? "huoxing" : provinceMap.get(phoneNum.substring(0, 3));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">## 2.打包上传, 添加到 classpath, 创建临时函数</span><br><span class="line"></span><br><span class="line">## 3.创建表,加载数据</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> flow_t(pnum <span class="keyword">string</span>,flow <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/ap/ihivedata/flow.tmp'</span> <span class="keyword">into</span> <span class="keyword">table</span> flow_t;</span><br><span class="line"></span><br><span class="line">## 4.使用</span><br><span class="line">0: jdbc:hive2://cs2:10000&gt; select pnum,tolow(pnum),flow from flow_t;</span><br><span class="line">+<span class="comment">-------------+------------+-------+</span></span><br><span class="line">|    pnum     |    _c1     | flow  |</span><br><span class="line">+<span class="comment">-------------+------------+-------+</span></span><br><span class="line">| 1364535532  | guangzhou  | 10    |</span><br><span class="line">| 1374535532  | huoxing    | 42    |</span><br><span class="line">| 1384535532  | huoxing    | 34    |</span><br><span class="line">| 1364535532  | guangzhou  | 45    |</span><br><span class="line">| 1384535532  | huoxing    | 22    |</span><br><span class="line">+<span class="comment">-------------+------------+-------+</span></span><br></pre></td></tr></table></figure><h4 id="17-2-3-有些复杂"><a href="#17-2-3-有些复杂" class="headerlink" title="17.2.3  有些复杂"></a>17.2.3  有些复杂</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"># 解析 json 数据表 rating.json</span><br><span class="line">## 1. 写 udf</span><br><span class="line"></span><br><span class="line"><span class="comment">// com.rox.json.MovieRateBean </span></span><br><span class="line"><span class="keyword">package</span> com.rox.json;</span><br><span class="line"><span class="keyword">import</span> lombok.Getter;</span><br><span class="line"><span class="keyword">import</span> lombok.Setter;</span><br><span class="line"><span class="meta">@Getter</span></span><br><span class="line"><span class="meta">@Setter</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MovieRateBean</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> String movie;</span><br><span class="line"><span class="keyword">private</span> String rate;</span><br><span class="line"><span class="keyword">private</span> String timeStamp;</span><br><span class="line"><span class="keyword">private</span> String uid;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> movie + <span class="string">"\t"</span> + rate + <span class="string">"\t"</span></span><br><span class="line">+ timeStamp + <span class="string">"\t"</span> + uid;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// com.rox.json.JsonParser </span></span><br><span class="line"><span class="keyword">package</span> com.rox.json;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"><span class="keyword">import</span> com.fasterxml.jackson.databind.ObjectMapper;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JsonParser</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">evaluate</span><span class="params">(String jsonLine)</span> </span>&#123;</span><br><span class="line">ObjectMapper objectMapper = <span class="keyword">new</span> ObjectMapper();</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">MovieRateBean bean = objectMapper.readValue(jsonLine, MovieRateBean.class);</span><br><span class="line"><span class="keyword">return</span> bean.toString();</span><br><span class="line">&#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="string">""</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">====================================================</span><br><span class="line">## 2.打包上传, 添加到 classpath, 创建临时函数, 检查是否成功</span><br><span class="line">show functions;</span><br><span class="line"></span><br><span class="line">## 3.创建表,加载数据</span><br><span class="line"><span class="function">create table <span class="title">t_json</span><span class="params">(line string)</span> row format delimited</span>;</span><br><span class="line"></span><br><span class="line">load data local inpath <span class="string">'/home/ap/ihivedata/flow.log'</span> into table t_json;</span><br><span class="line"></span><br><span class="line">## 4.检查数据</span><br><span class="line">select * from t_json limit <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line">## 5.调用函数</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="comment">//cs2:10000&gt; select jsonparser(line)parsedline  from t_json limit 10;</span></span><br><span class="line">+---------------------+</span><br><span class="line">|     parsedline      |</span><br><span class="line">+---------------------+</span><br><span class="line">| <span class="number">1193</span><span class="number">5</span><span class="number">978300760</span><span class="number">1</span>  |</span><br><span class="line">| <span class="number">661</span><span class="number">3</span><span class="number">978302109</span><span class="number">1</span>   |</span><br><span class="line">| <span class="number">914</span><span class="number">3</span><span class="number">978301968</span><span class="number">1</span>   |</span><br><span class="line">| <span class="number">3408</span><span class="number">4</span><span class="number">978300275</span><span class="number">1</span>  |</span><br><span class="line">| <span class="number">2355</span><span class="number">5</span><span class="number">978824291</span><span class="number">1</span>  |</span><br><span class="line">| <span class="number">1197</span><span class="number">3</span><span class="number">978302268</span><span class="number">1</span>  |</span><br><span class="line">| <span class="number">1287</span><span class="number">5</span><span class="number">978302039</span><span class="number">1</span>  |</span><br><span class="line">| <span class="number">2804</span><span class="number">5</span><span class="number">978300719</span><span class="number">1</span>  |</span><br><span class="line">| <span class="number">594</span><span class="number">4</span><span class="number">978302268</span><span class="number">1</span>   |</span><br><span class="line">| <span class="number">919</span><span class="number">4</span><span class="number">978301368</span><span class="number">1</span>   |</span><br><span class="line">+---------------------+</span><br><span class="line"><span class="comment">// 但是这样只是把每一行解析出来了,</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">## 6.删除原来的表    </span><br><span class="line">drop table <span class="keyword">if</span> exists t_rating;</span><br><span class="line"></span><br><span class="line">## 7.重新创建一个表</span><br><span class="line"><span class="function">create table <span class="title">t_rating</span><span class="params">(movieid string,rate <span class="keyword">int</span>,timestring string,uid string)</span></span></span><br><span class="line"><span class="function">row format delimited fields terminated by '\t'</span>;</span><br><span class="line"></span><br><span class="line">## 8.根据查出来的每一行, 按照 '\t'分割, 然后再插入到表中</span><br><span class="line">create table  t_rating as</span><br><span class="line">select </span><br><span class="line">split(jsonparser(line),<span class="string">'\t'</span>)[<span class="number">0</span>]as movieid, split(jsonparser(line),<span class="string">'\t'</span>)[<span class="number">1</span>] as rate, split(jsonparser(line),<span class="string">'\t'</span>)[<span class="number">2</span>] as timestring, split(jsonparser(line),<span class="string">'\t'</span>)[<span class="number">3</span>] as uid   </span><br><span class="line">from t_json limit <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 但是执行结果会报错,不知道为啥,难道是 java 代码的问题? :TODO</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">-------</span><br><span class="line">## 9.内置json函数</span><br><span class="line"><span class="function">select <span class="title">get_json_object</span><span class="params">(line,<span class="string">'$.movie'</span>)</span> as moive,<span class="title">get_json_object</span><span class="params">(line,<span class="string">'$.rate'</span>)</span> as rate  from rat_json limit 10</span>;</span><br></pre></td></tr></table></figure><h4 id="17-3-Transform实现"><a href="#17-3-Transform实现" class="headerlink" title="17.3 Transform实现"></a>17.3 Transform实现</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"># 1、先加载rating.json文件到hive的一个原始表 t_json</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> rat_json(line <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/ap/rating.json'</span> <span class="keyword">into</span> <span class="keyword">table</span> t_json;</span><br><span class="line"></span><br><span class="line">2、需要解析json数据成四个字段，插入一张新的表 t_rating</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> t_rating;</span><br><span class="line"># 创建表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_rating(movieid <span class="keyword">string</span>,rate <span class="built_in">int</span>,timestring <span class="keyword">string</span>,uid <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"># 插入, 也可以直接创建  create table xx as + select...</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> t_rating</span><br><span class="line"><span class="keyword">select</span> get_json_object(line,<span class="string">'$.movie'</span>) <span class="keyword">as</span> moiveid, get_json_object(line,<span class="string">'$.rate'</span>) <span class="keyword">as</span> rate, get_json_object(line,<span class="string">'$.timeStamp'</span>) <span class="keyword">as</span> timestring,</span><br><span class="line">get_json_object(line,<span class="string">'$.uid'</span>) <span class="keyword">as</span> uid </span><br><span class="line"><span class="keyword">from</span> t_json;</span><br><span class="line"></span><br><span class="line">3. 写一个 python 脚本</span><br><span class="line">vi weekday_mapper.py</span><br><span class="line"></span><br><span class="line">#!/bin/python</span><br><span class="line">import sys</span><br><span class="line">import datetime</span><br><span class="line"></span><br><span class="line">for line in sys.stdin:</span><br><span class="line">  line = line.strip()</span><br><span class="line">  movieid, rating, timestring,userid = line.split('\t')</span><br><span class="line">  weekday = datetime.datetime.fromtimestamp(float(timestring)).isoweekday()</span><br><span class="line">  print '\t'.join([movieid, rating, str(weekday),userid])</span><br><span class="line">  </span><br><span class="line">4. 保存文件, 然后将文件加入 hive 的 classpath</span><br><span class="line">hive&gt;add FILE /home/hadoop/weekday_mapper.py;</span><br><span class="line"></span><br><span class="line">5. 此时可以直接创建新表</span><br><span class="line">hive&gt;create TABLE u_data_new as</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  TRANSFORM (movieid, rate, timestring,uid)</span><br><span class="line">  <span class="keyword">USING</span> <span class="string">'python weekday_mapper.py'</span></span><br><span class="line">  <span class="keyword">AS</span> (movieid, rate, <span class="keyword">weekday</span>,uid)</span><br><span class="line"><span class="keyword">FROM</span> t_rating;</span><br><span class="line"></span><br><span class="line">6. 查询结果</span><br><span class="line">## distinct看看有多少个不重复的数字</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">distinct</span>(<span class="keyword">weekday</span>) <span class="keyword">from</span> u_data_new <span class="keyword">limit</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure><h1 id="18-案例"><a href="#18-案例" class="headerlink" title="18. 案例"></a>18. 案例</h1><h3 id="18-1-广告推送-用户画像的介绍"><a href="#18-1-广告推送-用户画像的介绍" class="headerlink" title="18.1 广告推送-用户画像的介绍"></a>18.1 广告推送-用户画像的介绍</h3><p><strong>一个广告推送平台的项目结构示意图</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-17-071714.png" alt="image-20180617151714445"></p><h3 id="18-2-累计报表实现套路-面试"><a href="#18-2-累计报表实现套路-面试" class="headerlink" title="18.2 累计报表实现套路(面试)"></a>18.2 累计报表实现套路(面试)</h3><p><strong>题:  求每个月的累计访问次数</strong></p><p><strong>此处的访问次数可以换成工资,等等..</strong></p><p><strong>有如下访客访问次数统计表 t_access_times</strong></p><table><thead><tr><th>访客</th><th>月份</th><th>访问次数</th></tr></thead><tbody><tr><td>A</td><td>2015-01</td><td>5</td></tr><tr><td>A</td><td>2015-01</td><td>15</td></tr><tr><td>B</td><td>2015-01</td><td>5</td></tr><tr><td>A</td><td>2015-01</td><td>8</td></tr><tr><td>B</td><td>2015-01</td><td>25</td></tr><tr><td>A</td><td>2015-01</td><td>5</td></tr><tr><td>A</td><td>2015-02</td><td>4</td></tr><tr><td>A</td><td>2015-02</td><td>6</td></tr><tr><td>B</td><td>2015-02</td><td>10</td></tr><tr><td>B</td><td>2015-02</td><td>5</td></tr><tr><td>……</td><td>……</td><td>……</td></tr></tbody></table><p><strong>需要输出报表：t_access_times_accumulate</strong></p><table><thead><tr><th>访客</th><th>月份</th><th>月访问总计</th><th>累计访问总计</th></tr></thead><tbody><tr><td>A</td><td>2015-01</td><td>33</td><td>33</td></tr><tr><td>A</td><td>2015-02</td><td>10</td><td>43</td></tr><tr><td>…….</td><td>…….</td><td>…….</td><td>…….</td></tr><tr><td>B</td><td>2015-01</td><td>30</td><td>30</td></tr><tr><td>B</td><td>2015-02</td><td>15</td><td>45</td></tr><tr><td>…….</td><td>…….</td><td>…….</td><td>…….</td></tr></tbody></table><h5 id="解题代码"><a href="#解题代码" class="headerlink" title="解题代码"></a>解题代码</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 准备数据</span><br><span class="line">vi t_access_times</span><br><span class="line"></span><br><span class="line">A,2015-01,5</span><br><span class="line">A,2015-01,15</span><br><span class="line">B,2015-01,5</span><br><span class="line">A,2015-01,8</span><br><span class="line">B,2015-01,254</span><br><span class="line">A,2015-01,5</span><br><span class="line">A,2015-02,4</span><br><span class="line">A,2015-02,6</span><br><span class="line">B,2015-02,10</span><br><span class="line">B,2015-02,5</span><br><span class="line"><span class="comment">----</span></span><br><span class="line"></span><br><span class="line"># 建表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_access_times(username <span class="keyword">string</span>,<span class="keyword">month</span> <span class="keyword">string</span>,salary <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br><span class="line"></span><br><span class="line"># 加载数据</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/ap/t_access_times'</span> <span class="keyword">into</span> <span class="keyword">table</span> t_access_times;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">1、第一步，先求个用户的月总金额</span><br><span class="line"><span class="keyword">select</span> username,<span class="keyword">month</span>,<span class="keyword">sum</span>(salary) <span class="keyword">as</span> salary <span class="keyword">from</span> t_access_times <span class="keyword">group</span> <span class="keyword">by</span> username,<span class="keyword">month</span>;</span><br><span class="line"></span><br><span class="line">+<span class="comment">-----------+----------+---------+--+</span></span><br><span class="line">| username  |  month   | salary  |</span><br><span class="line">+<span class="comment">-----------+----------+---------+--+</span></span><br><span class="line">| A         | 2015-01  | 33      |</span><br><span class="line">| A         | 2015-02  | 10      |</span><br><span class="line">| B         | 2015-01  | 30      |</span><br><span class="line">| B         | 2015-02  | 15      |</span><br><span class="line">+<span class="comment">-----------+----------+---------+--+</span></span><br><span class="line"></span><br><span class="line">2、第二步，将月总金额表 自己连接 自己连接</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> aa <span class="keyword">as</span></span><br><span class="line">(<span class="keyword">select</span> username,<span class="keyword">month</span>,<span class="keyword">sum</span>(salary) <span class="keyword">as</span> salary <span class="keyword">from</span> t_access_times <span class="keyword">group</span> <span class="keyword">by</span> username,<span class="keyword">month</span>) A </span><br><span class="line"><span class="keyword">inner</span> <span class="keyword">join</span> </span><br><span class="line">(<span class="keyword">select</span> username,<span class="keyword">month</span>,<span class="keyword">sum</span>(salary) <span class="keyword">as</span> salary <span class="keyword">from</span> t_access_times <span class="keyword">group</span> <span class="keyword">by</span> username,<span class="keyword">month</span>) B</span><br><span class="line">A.username=B.username</span><br><span class="line"></span><br><span class="line">+<span class="comment">-------------+----------+-----------+-------------+----------+-----------+--+</span></span><br><span class="line">| a.username  | a.month  | a.salary  | b.username  | b.month  | b.salary  |</span><br><span class="line">+<span class="comment">-------------+----------+-----------+-------------+----------+-----------+--+</span></span><br><span class="line">| A           | <span class="number">2015</span><span class="number">-01</span>  | <span class="number">33</span>        | A           | <span class="number">2015</span><span class="number">-01</span>  | <span class="number">33</span>        |</span><br><span class="line">| A           | <span class="number">2015</span><span class="number">-01</span>  | <span class="number">33</span>        | A           | <span class="number">2015</span><span class="number">-02</span>  | <span class="number">10</span>        |</span><br><span class="line">| A           | <span class="number">2015</span><span class="number">-02</span>  | <span class="number">10</span>        | A           | <span class="number">2015</span><span class="number">-01</span>  | <span class="number">33</span>        |</span><br><span class="line">| A           | <span class="number">2015</span><span class="number">-02</span>  | <span class="number">10</span>        | A           | <span class="number">2015</span><span class="number">-02</span>  | <span class="number">10</span>        |</span><br><span class="line">| B           | <span class="number">2015</span><span class="number">-01</span>  | <span class="number">30</span>        | B           | <span class="number">2015</span><span class="number">-01</span>  | <span class="number">30</span>        |</span><br><span class="line">| B           | <span class="number">2015</span><span class="number">-01</span>  | <span class="number">30</span>        | B           | <span class="number">2015</span><span class="number">-02</span>  | <span class="number">15</span>        |</span><br><span class="line">| B           | <span class="number">2015</span><span class="number">-02</span>  | <span class="number">15</span>        | B           | <span class="number">2015</span><span class="number">-01</span>  | <span class="number">30</span>        |</span><br><span class="line">| B           | <span class="number">2015</span><span class="number">-02</span>  | <span class="number">15</span>        | B           | <span class="number">2015</span><span class="number">-02</span>  | <span class="number">15</span>        |</span><br><span class="line">+<span class="comment">-------------+----------+-----------+-------------+----------+-----------+--+</span></span><br><span class="line"></span><br><span class="line"><span class="number">3</span>、第三步，从上一步的结果中</span><br><span class="line">进行分组查询，分组的字段是a.username a.month</span><br><span class="line">求月累计值：  将b.month &lt;= a.month的所有b.salary求和即可</span><br><span class="line"><span class="keyword">select</span> A.username,A.month,<span class="keyword">max</span>(A.salary) <span class="keyword">as</span> salary,<span class="keyword">sum</span>(B.salary) <span class="keyword">as</span> accumulate</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">(<span class="keyword">select</span> username,<span class="keyword">month</span>,<span class="keyword">sum</span>(salary) <span class="keyword">as</span> salary <span class="keyword">from</span> t_access_times <span class="keyword">group</span> <span class="keyword">by</span> username,<span class="keyword">month</span>) A </span><br><span class="line"><span class="keyword">inner</span> <span class="keyword">join</span> </span><br><span class="line">(<span class="keyword">select</span> username,<span class="keyword">month</span>,<span class="keyword">sum</span>(salary) <span class="keyword">as</span> salary <span class="keyword">from</span> t_access_times <span class="keyword">group</span> <span class="keyword">by</span> username,<span class="keyword">month</span>) B</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">A.username=B.username</span><br><span class="line"><span class="keyword">where</span> B.month &lt;= A.month</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> A.username,A.month</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> A.username,A.month;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">+<span class="comment">-------------+----------+---------+-------------+</span></span><br><span class="line">| a.username  | a.month  | salary  | accumulate  |</span><br><span class="line">+<span class="comment">-------------+----------+---------+-------------+</span></span><br><span class="line">| A           | 2015-01  | 33      | 33          |</span><br><span class="line">| A           | 2015-02  | 10      | 43          |</span><br><span class="line">| B           | 2015-01  | 259     | 259         |</span><br><span class="line">| B           | 2015-02  | 15      | 274         |</span><br><span class="line">+<span class="comment">-------------+----------+---------+-------------+</span></span><br></pre></td></tr></table></figure><h3 id="18-3-待做项目"><a href="#18-3-待做项目" class="headerlink" title="18.3 待做项目"></a>18.3 待做项目</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-17-082606.png" alt="image-20180617162606289"></p><h1 id="19-注意点"><a href="#19-注意点" class="headerlink" title="19.注意点"></a>19.注意点</h1><ol><li><strong>使用聚合函数, 后面一定要分组(group by xxx),  group by 会自动去重</strong><ul><li>如果 sql 语句中有 group by, 那么 select 后面必须有 group by 的字段, 或聚合函数</li></ul></li><li><strong>使用order by 排序某个字段时, 必须在 select中出现此字段, 否则会找不到 :TODO</strong><ul><li>待验证</li></ul></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Hive-初探&quot;&gt;&lt;a href=&quot;#1-Hive-初探&quot; class=&quot;headerlink&quot; title=&quot;1.  Hive 初探&quot;&gt;&lt;/a&gt;1.  Hive 初探&lt;/h1&gt;&lt;h3 id=&quot;1-1-Hive-的数据存储&quot;&gt;&lt;a href=&quot;#1-1-Hiv
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/categories/Hadoop/Hive/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce-简单总结</title>
    <link href="https://airpoet.github.io/2018/06/12/Hadoop/2-MapReduce/MapReduce-%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93/"/>
    <id>https://airpoet.github.io/2018/06/12/Hadoop/2-MapReduce/MapReduce-简单总结/</id>
    <published>2018-06-12T07:03:29.967Z</published>
    <updated>2018-06-17T09:27:08.821Z</updated>
    
    <content type="html"><![CDATA[<p>MapReduce在编程的时候，基本上一个固化的模式，没有太多可灵活改变的地方，<strong>除了以下几处</strong>：</p><ul><li><h4 id="输入数据接口：InputFormat"><a href="#输入数据接口：InputFormat" class="headerlink" title="输入数据接口：InputFormat"></a><strong>输入数据接口：InputFormat</strong></h4><ul><li>FileInputFormat  (文件类型数据读取的通用抽象类)  </li><li>DBInputFormat （数据库数据读取的通用抽象类）</li><li><strong>默认使用的实现类是： TextInputFormat</strong>    <ul><li>job.setInputFormatClass(TextInputFormat.class)</li><li>TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回</li></ul></li></ul></li><li><h4 id="逻辑处理接口：-Mapper"><a href="#逻辑处理接口：-Mapper" class="headerlink" title="逻辑处理接口： Mapper"></a><strong>逻辑处理接口： Mapper</strong></h4><ul><li>完全需要用户自己去实现其中  <code>map()   setup()   clean()</code></li></ul></li></ul><ul><li><h4 id="map输出的结果在shuffle阶段会被partition以及sort，此处有两个接口可自定义："><a href="#map输出的结果在shuffle阶段会被partition以及sort，此处有两个接口可自定义：" class="headerlink" title="map输出的结果在shuffle阶段会被partition以及sort，此处有两个接口可自定义："></a><strong>map输出的结果在shuffle阶段会被partition以及sort</strong>，此处有两个接口可自定义：</h4><ul><li><strong>Partitioner</strong><ul><li>有默认实现 <code>HashPartitioner</code>，逻辑是根据<strong>key</strong>和<strong>numReduces</strong>来返回一个分区号:<code>key.hashCode()&amp;Integer.MAXVALUE % numReduces</code></li><li>通常情况下，用默认的这个HashPartitioner就可以，如果业务上有特别的需求，可以自定义</li><li>继承 <code>Partitioner</code>, 重写<code>getPartition</code>方法, 具体见<a href="https://airpoet.github.io/2018/06/07/Hadoop/Study/2-MapReduce/MapReduce%E7%AC%94%E8%AE%B0-2/#%E5%85%B8%E5%9E%8B%E7%9A%84-Partitioner-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0">这里</a></li></ul></li><li><strong>Comparable</strong><ul><li>当我们用自定义的对象作为key来输出时，就必须要实现<code>WritableComparable</code>接口，<strong>override</strong>其中的<code>compareTo()</code>方法, 具体见<a href="https://airpoet.github.io/2018/06/07/Hadoop/Study/2-MapReduce/MapReduce%E7%AC%94%E8%AE%B0-2/#%E5%85%B8%E5%9E%8B%E7%9A%84-Partitioner-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0">这里</a></li></ul></li></ul></li></ul><ul><li><h4 id="reduce端的数据分组比较接口-：-Groupingcomparator"><a href="#reduce端的数据分组比较接口-：-Groupingcomparator" class="headerlink" title="reduce端的数据分组比较接口 ： Groupingcomparator"></a><strong>reduce端的数据分组比较接口 ： Groupingcomparator</strong></h4><ul><li>reduceTask拿到输入数据（一个partition的所有数据）后，首先需要对数据进行分组，其分组的默认原则是key相同，然后对每一组kv数据调用一次reduce()方法，并且将这一组kv中的第一个kv的key作为参数传给reduce的key，将这一组数据的value的迭代器传给reduce()的values参数</li><li>利用上述这个机制，我们可以<strong>实现一个高效的分组取最大值的逻辑：</strong><ul><li>自定义一个bean对象用来封装我们的数据，然后改写其compareTo方法产生倒序排序的效果</li><li>然后自定义一个Groupingcomparator，将bean对象的分组逻辑改成按照我们的业务分组id来分组（比如订单号）</li><li>这样，我们要取的最大值就是reduce()方法中传进来key</li></ul></li></ul></li></ul><hr><ul><li><h4 id="逻辑处理接口：Reducer"><a href="#逻辑处理接口：Reducer" class="headerlink" title="逻辑处理接口：Reducer"></a><strong>逻辑处理接口：Reducer</strong></h4><ul><li>完全需要用户自己去实现其中  <code>reduce()   setup()   clean()</code></li></ul></li><li><h4 id="输出数据接口：-OutputFormat"><a href="#输出数据接口：-OutputFormat" class="headerlink" title="输出数据接口： OutputFormat"></a><strong>输出数据接口： OutputFormat</strong></h4><ul><li>有一系列子类  FileOutputformat  DBoutputFormat  …..</li><li>默认实现类是TextOutputFormat</li><li>功能逻辑是：  <strong>将每一个KV对向目标文本文件中输出为一行</strong></li></ul><p>​    </p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;MapReduce在编程的时候，基本上一个固化的模式，没有太多可灵活改变的地方，&lt;strong&gt;除了以下几处&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;h4 id=&quot;输入数据接口：InputFormat&quot;&gt;&lt;a href=&quot;#输入数据接口：InputFormat&quot; c
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>Hive学习-1-安装</title>
    <link href="https://airpoet.github.io/2018/06/12/Hadoop/3-Hive/Hive%E5%AD%A6%E4%B9%A0-1/"/>
    <id>https://airpoet.github.io/2018/06/12/Hadoop/3-Hive/Hive学习-1/</id>
    <published>2018-06-12T03:24:46.733Z</published>
    <updated>2018-06-28T13:55:35.306Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hive简介"><a href="#Hive简介" class="headerlink" title="Hive简介"></a>Hive简介</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-12-034055.png" alt="image-20180612114055129"></p><h3 id="Hive基本情况"><a href="#Hive基本情况" class="headerlink" title="Hive基本情况"></a>Hive基本情况</h3><p><strong>Hive</strong> 是建立在 <strong>Hadoop</strong> 上的数据仓库基础构架</p><ul><li>由facebook开源，最初用于解决海量结构化的日志数据统 计问题;<ul><li><strong>ETL</strong>(<strong>Extraction-Transformation-Loading</strong>)工具</li></ul></li><li>构建在Hadoop之上的数据仓库;<ul><li>数据计算使用<strong>MR</strong>，数据存储使用<strong>HDFS</strong></li><li>数据库&amp;数据仓库 的区别：<ul><li>概念上<ul><li><strong>数据库</strong>：用于管理精细化数据，一般情况下用于存储结果数据，分库分表进行存储</li><li><strong>数据仓库</strong>：<strong>存储、查询、分析大规模数据</strong> 。更像一个打包的过程，里面存储的数据没有细化区分，粒度较粗</li></ul></li><li>用途上：<ul><li>数据库：OLTP，on line Transation Processing 联机事务处理，增删改</li><li>数据仓库：OLAP，on line analysis Processing 联机事务分析处理，查询，hive不支持删除、修改。 支持插入。</li></ul></li><li>使用上：<ul><li>数据库：标准sql,  hbase: 非标准sql</li><li>数据仓库：方言版的sql， HQL</li></ul></li><li>模式上：<ul><li>数据库：写模式</li><li>数据仓库：读模式</li></ul></li></ul></li></ul></li><li>可以将结构化的数据映射成一张数据库表<ul><li>结构化数据映射成二维表</li><li><strong>将文本中每一行数据映射为数据库的每一条数据</strong></li><li><strong>将文本中每一列数据映射为hive的表字段</strong></li></ul></li><li>提供HQL 查询功能<ul><li>hive query language， 方言版sql</li></ul></li><li><strong>底层数据是存储在HDFS上</strong><ul><li>hive上建的表仅仅相当于<strong>对hdfs上的结构化数据进行映射管理</strong></li><li>hive仅仅是一个管理数据的作用，而<strong>不会存储数据</strong></li><li>hive想要管理hdfs上的数据，就要建立一个关联关系，关联hive上的表和hdfs上的数据路径</li><li>数据是依赖于一个元数据库</li><li>元数据库采用的是关系型数据库， 真实生产中一般使用mysql为hive的元数据库，hive内置默认的元数据库是 derby</li><li><strong>元数据：</strong> <strong>HCalalog</strong> <ul><li>hive中的表和hdfs的<strong>映射关系</strong>，以及<strong>hive表属性</strong>（内部表，外部表，视图）和<strong>字段信息</strong></li><li>元数据一旦修饰，hive的所有映射关系等都没了，就无法使用了</li><li>可与<strong>Pig</strong>、<strong>Presto</strong>等共享 </li></ul></li></ul></li><li>Hive的是<strong>构建在Hadoop之上的数据仓库</strong><ul><li>数据计算使用<strong>MR</strong>，数据存储使用<strong>HDFS</strong> </li></ul></li><li>通常用于进行离线数据处理(采用MapReduce) </li><li><strong>可认为是一个HQL—-&gt;MR的语言翻译器</strong> </li></ul><h3 id="Hive优缺点："><a href="#Hive优缺点：" class="headerlink" title="Hive优缺点："></a>Hive优缺点：</h3><ul><li><p>优点：</p><ul><li>简单，容易上手 <ul><li>提供了类<strong>SQL</strong>查询语言<strong>HQL</strong> </li></ul></li><li>为超大数据集设计的计算/扩展能力 <ul><li><strong>MR</strong>作为计算引擎，<strong>HDFS</strong>作为存储系统 </li></ul></li><li>统一的元数据管理(HCalalog) <ul><li>可与<strong>Pig</strong>、<strong>Presto</strong>等共享 </li></ul></li></ul></li><li><p><strong>缺点</strong></p><ul><li><p><strong>不支持 删除 &amp; 修改</strong>  delete&amp;update，<strong>不支持事务</strong></p><ul><li>因为是基于HDFS</li><li>hive做的最多的是查询</li></ul></li><li><p>Hive的HQL表达的能力有限 </p><ul><li>迭代式算法无法表达 </li><li>有些复杂运算用<strong>HQL</strong>不易表达 </li></ul></li><li><p><strong>Hive效率较低，查询延时高</strong></p><ul><li><strong>Hive</strong>自动生成<strong>MapReduce</strong>作业，通常不够智能 </li><li><strong>HQL</strong>调优困难，粒度较粗 </li><li>可控性差 </li></ul></li></ul></li></ul><h3 id="Hive与传统关系型数据库-RDBMS）对比"><a href="#Hive与传统关系型数据库-RDBMS）对比" class="headerlink" title="Hive与传统关系型数据库(RDBMS）对比"></a>Hive与传统关系型数据库(RDBMS）对比</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-12-082659.png" alt="image-20180612162659151"><br></p><h1 id="Hive的架构"><a href="#Hive的架构" class="headerlink" title="Hive的架构"></a>Hive的架构</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-12-061321.png" alt="image-20180612141321130"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-12-061413.png" alt="image-20180612141413285"></p><ul><li><p><strong>1) 用户接口</strong></p><ul><li><p>CLI：Command Line Interface，即Shell终端命令行，使用 Hive 命令行与 Hive 进行交互，最常用（学习，调试，生产），包括两种运行方式：</p><ul><li><p>hive命令方式：前提必须在hive安装节点上执行</p></li><li><p>hiveserver2方式：hive安装节点将hive启动为一个后台进程，客户机进行连接（类似于启动了一个hive服务端）</p></li></ul></li></ul></li></ul><pre><code>真实生产中常用！- 1) 修改配置文件，允许远程连接;  第一：修改 hdfs-site.xml，加入一条配置信息，启用 webhdfs；  第二：修改 core-site.xml，加入两条配置信息，设置 hadoop的代理用户。- 2) 启动服务进程  - 前台启动：hiveserver2  - 后台启动    - 记录日志：nohup hiveserver2 1&gt;/home/sigeon/hiveserver.log 2&gt;/home/sigeon/hiveserver.err &amp;      0：标准日志输入   1：标准日志输出   2：错误日志输出      如果我没有配置日志的输出路径，日志会生成在当前工作目录，默认的日志名称叫做：      nohup.xxx    - 不记录日志：nohup hiveserver2 1&gt;/dev/null 2&gt;/dev/null &amp;     - [补充：]      - nohup命令：no hang up的缩写，即不挂起，可以在你退出帐户/关闭终端之后继续运行相应的进程。      - 语法：nohup &lt;command&gt; &amp;- 3) 开启beenline客户端并连接：  - 方法一：     - beenline，开启beenline客户端;    - !connect jdbc:&lt;hive2://master:10000&gt;，回车。然后输入用户名和密码，这个用户名是安装 hadoop 集群的用户名  - 方法二：    - beeline -u jdbc:&lt;hive2://master:10000&gt; -n sigeon</code></pre><ul><li><p>JDBC/ODBC：是 Hive 的基于 JDBC 操作提供的客户端，用户（开发员，运维人员）通过 这连接至 Hive server 服务 </p></li><li><p>Web UI：通过浏览器访问 Hive，基本不会使用</p></li></ul><ul><li><p>2) 元数据库：保存元数据，一般会选用关系型数据库（如mysql，Hive 和 MySQL 之间通过 MetaStore 服务交互）</p></li><li><p>3) Thrift服务：Facebook 开发的一个软件框架，可以用来进行可扩展且跨语言的服务的开发， Hive 集成了该服务，能让不同的编程语言调用 Hive 的接口</p></li><li><p>4) 驱动Driver</p><ul><li>a. 解释器：解释器的作用是将 HiveSQL 语句转换为抽象语法树（AST） </li><li>b. 编译器：编译器是将语法树编译为逻辑执行计划 </li><li>c. 优化器：优化器是对逻辑执行计划进行优化 </li><li>d. 执行器：执行器是调用底层的运行框架执行逻辑执行计划 </li></ul></li></ul><p><br></p><h1 id="Hive的数据组织格式"><a href="#Hive的数据组织格式" class="headerlink" title="Hive的数据组织格式"></a>Hive的数据组织格式</h1><ul><li><p>1)库：database</p></li><li><p>2) 表</p><ul><li><p>a. 内部表（管理表：managed_table）</p></li><li><p>b. 外部表（external_table）</p></li><li><p>内部表和外部表区别：</p><ul><li>内部表和外部表是两个相对的概念，不可能有一个表同时是内部表又是外部表；</li><li>内部表删除表的时候会删除原始数据和元数据，而外部表删除表的时候只会删除元数据不会删除原始数据</li><li>一般情况下存储公共数据的表存放为外部表</li><li>大多数情况，他们的区别不明显。如果数据的所有处理都在 Hive 中进行，那么倾向于 选择内部表；但是如果 Hive 和其他工具要针对相同的数据集进行处理，外部表更合适。 </li><li>使用外部表访问存储在 HDFS 上的初始数据，然后通过 Hive 转换数据并存到内部表中，使用外部表的场景是针对一个数据集有多个不同的 Schema。 </li></ul></li><li><p>c. 分区表</p><ul><li>不同于hadoop中的分区，分区表是人为划分的</li><li>hive最终存储海量数据，海量数据查询一定注意避免全表扫描</li><li>查询的时候为了提升我们的查询性能，出现了分区表</li><li>将数据按照用户的业务存储到不同的目录下，在进行数据查询时只会对指定分区下的数据进行扫描<br>一般情况下生产中用日期作为分区字段</li></ul></li><li><p>d. 分桶表</p><ul><li><p>类似于hadoop中的分区，是由程序决定的，只能指定桶的个数（分区的个数）</p></li><li><p>根据hash算法将余数不同的输出到不同的文件中</p></li><li><p>作用：</p><ul><li>1）提升join的性能<br>思考这个问题：select <a href="http://a.id" target="_blank" rel="noopener">a.id</a>,<a href="http://a.name" target="_blank" rel="noopener">a.name</a>,b.addr from a join b on <a href="http://a.id" target="_blank" rel="noopener">a.id</a> = <a href="http://b.id" target="_blank" rel="noopener">b.id</a>;如果 a 表和 b 表已经是 分桶表，而且分桶的字段是 id 字段做这个 join 操作时，还需要全表做笛卡尔积</li><li>2）提升数据样本的抽取效率，直接拿一个桶中的数据作为样本数据</li></ul></li><li><p>分区表和分桶表的区别： </p><p><img src="/var/folders/6l/blvdbwd53hqglz0f09n3sd5c0000gn/T/abnerworks.Typora/image-20180612143606401.png" alt="image-20180612143606401"></p><ul><li>Hive 数据表可以根据某些字段进行分区操作，细化数据管理，可以让部分查询更快。</li><li>同时表和分区也可以进一步被划分为 Buckets，分桶表的原理和 MapReduce 编程中的 HashPartitioner 的原理类似 </li><li>分区和分桶都是细化数据管理，但是分区表是手动添加区分，由于 Hive 是读模式，所以对添加进分区的数据不做模式校验</li><li>分桶表中的数据是按照某些分桶字段进行 hash 散列 形成的多个文件，所以数据的准确性也高很多</li></ul></li></ul></li></ul></li></ul><p><br></p><ul><li><p><strong>3)视图：</strong></p><ul><li>hive中的视图仅仅相当于一个sql语句的别名</li><li><strong>在hive中仅仅存在逻辑视图，不存在物理视图</strong><ul><li>物理视图：讲sql语句的执行结果存在视图中</li><li>逻辑视图： <strong>仅仅是对查询结果的引用</strong></li></ul></li></ul></li><li><p><strong>4)数据存储：</strong></p><ul><li>原始数据中存在HDFS</li><li>元数据存在mysql</li></ul></li></ul><p><br></p><h1 id="Hive安装"><a href="#Hive安装" class="headerlink" title="Hive安装"></a>Hive安装</h1><p><strong>装hive其实不难，主要是安装mysql，解决mysql的权限问题</strong></p><h2 id="MySql安装"><a href="#MySql安装" class="headerlink" title="MySql安装"></a>MySql安装</h2><h3 id="RPM-安装MySQl："><a href="#RPM-安装MySQl：" class="headerlink" title="RPM 安装MySQl："></a>RPM 安装MySQl：</h3><ol><li><p>检查以前是否装过 MySQL </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -qa|grep -i mysql</span><br></pre></td></tr></table></figure></li><li><p>发现有的话就都卸载 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rpm -e --nodeps mysql-libs-5.1.73-5.el6_6.x86_64</span><br><span class="line">rpm -e --nodeps ....</span><br></pre></td></tr></table></figure></li><li><p>删除老版本 mysql 的开发头文件和库 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rm -rf /usr/lib64/mysql</span><br><span class="line"><span class="comment"># 在搜索 my.cnf 文件，有的话就删掉</span></span><br></pre></td></tr></table></figure></li><li><p>上传mysql 安装包到 Linux中，解压</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tar -xvf mysql-5.6.26-1.linux_glibc2.5.x86_64.rpm-bundle.tar</span><br><span class="line"><span class="comment"># 解压出来有这些文件</span></span><br><span class="line">MySQL-devel-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br><span class="line">MySQL-embedded-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br><span class="line">MySQL-server-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br><span class="line">MySQL-shared-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br><span class="line">MySQL-shared-compat-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br><span class="line">MySQL-test-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br></pre></td></tr></table></figure></li><li><p>安装 server &amp; client </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># server</span></span><br><span class="line">rpm -ivh MySQL-server-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br><span class="line"><span class="comment"># client</span></span><br><span class="line">rpm -ivh MySQL-client-5.6.26-1.linux_glibc2.5.x86_64.rpm</span><br></pre></td></tr></table></figure></li><li><p>启动Mysql</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service mysql start</span><br></pre></td></tr></table></figure></li><li><p>登录Mysql并改密码，等</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"># 初始密码在这个文件中</span><br><span class="line">cat /root/.mysql_sercert </span><br><span class="line"></span><br><span class="line"># 登录</span><br><span class="line">mysql -uroot -pxxxxx</span><br><span class="line"></span><br><span class="line"># 删除除了`%`之外的其他所有host</span><br><span class="line">use mysql;</span><br><span class="line">select host,user,password from user;</span><br><span class="line">delete from user where host in (&apos;localhost&apos;, &apos;127.0.0.1&apos;,&apos;::1&apos;, ...)</span><br><span class="line"></span><br><span class="line"># 修改密码 </span><br><span class="line">UPDATE user SET Password = PASSWORD(&apos;psd&apos;) WHERE user = &apos;root&apos;;</span><br><span class="line"></span><br><span class="line"># 为`%` 和 `*` 添加远程登录权限 </span><br><span class="line">#注意： 前面的 mysql 登录用户名， 123 是登录密码</span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;psd&apos; WITH GRANT OPTION;</span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;*&apos; IDENTIFIED BY &apos;psd&apos; WITH GRANT OPTION;</span><br><span class="line">FLUSH PRIVILEGES;</span><br><span class="line"></span><br><span class="line"># 退出登录</span><br><span class="line">exit;</span><br><span class="line"></span><br><span class="line"># 此时可以再登录试试</span><br><span class="line">mysql -uroot -ppsd</span><br><span class="line"></span><br><span class="line">======================================</span><br><span class="line"></span><br><span class="line"># 修改字符集为 utf-8</span><br><span class="line"># 新建一个文件</span><br><span class="line">vi /etc/my.cnf  </span><br><span class="line"># 添加以下内容</span><br><span class="line">[client]</span><br><span class="line">default-character-set=utf8</span><br><span class="line"></span><br><span class="line">[mysql]</span><br><span class="line">default-character-set=utf8</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line">character-set-server=utf8</span><br><span class="line"></span><br><span class="line"># 重启mysql</span><br><span class="line">sudo service mysql restart</span><br><span class="line"></span><br><span class="line">======================================</span><br><span class="line"></span><br><span class="line"># 忘记密码，修改密码的方法</span><br><span class="line"># 停止mysql服务的运行</span><br><span class="line">service mysql stop</span><br><span class="line"></span><br><span class="line"># 跳过授权表访问</span><br><span class="line">mysqld_safe --user=mysql --skip-grant-tables --skip-networking &amp; </span><br><span class="line"></span><br><span class="line"># 登录mysql</span><br><span class="line">mysql -u root mysql </span><br><span class="line"></span><br><span class="line"># 接下来可以修改密码了</span><br><span class="line">  ##在mysql5.7以下的版本如下：</span><br><span class="line">mysql&gt; UPDATE user SET Password=PASSWORD(&apos;newpassword&apos;) where USER=&apos;root’；</span><br><span class="line">  ##在mysql5.7版本如下：</span><br><span class="line">update mysql.user set authentication_string=password(&apos;newpassword&apos;) ;</span><br><span class="line"></span><br><span class="line"># 修改完了重启</span><br><span class="line">service mysql restart</span><br><span class="line"></span><br><span class="line">======================================</span><br></pre></td></tr></table></figure></li></ol><h4 id="MySql的其它错误"><a href="#MySql的其它错误" class="headerlink" title="MySql的其它错误"></a>MySql的其它错误</h4><p>在运行<code>schematool -dbType mysql -initSchema</code>手动初始化元数据库的时候</p><ul><li><p>报了一个log4j重复加载的问题</p><ul><li>解决：可以不用理睬</li></ul></li><li><p>链接mysql 密码过期问题 <code>Your password has expired.</code></p><ul><li><p>解决： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">mysql -uroot -p123</span><br><span class="line"></span><br><span class="line">mysql&gt; use mysql</span><br><span class="line">Reading table information for completion of table and column names</span><br><span class="line">You can turn off this feature to get a quicker startup with -A</span><br><span class="line"></span><br><span class="line">Database changed</span><br><span class="line">mysql&gt; select host,user,password_expired from user;</span><br><span class="line">+-----------+------+------------------+</span><br><span class="line">| host      | user | password_expired |</span><br><span class="line">+-----------+------+------------------+</span><br><span class="line">| %         | root | N                |</span><br><span class="line">| cs1       | root | Y                |</span><br><span class="line">| 127.0.0.1 | root | Y                |</span><br><span class="line">| ::1       | root | Y                |</span><br><span class="line">+-----------+------+------------------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br><span class="line">-------------</span><br><span class="line">mysql&gt; update user set password_expired=&apos;N&apos; where host=&apos;::1&apos;;</span><br><span class="line">Query OK, 1 row affected (0.01 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class="line"></span><br><span class="line">mysql&gt; update user set password_expired=&apos;N&apos; where host=&apos;cs1&apos;;</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class="line"></span><br><span class="line">mysql&gt; update user set password_expired=&apos;N&apos; where host=&apos;127.0.0.1&apos;;</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class="line"></span><br><span class="line">mysql&gt; FLUSH PRIVILEGES;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select host,user,password_expired from user;</span><br><span class="line">+-----------+------+------------------+</span><br><span class="line">| host      | user | password_expired |</span><br><span class="line">+-----------+------+------------------+</span><br><span class="line">| %         | root | N                |</span><br><span class="line">| cs1       | root | N                |</span><br><span class="line">| 127.0.0.1 | root | N                |</span><br><span class="line">| ::1       | root | N                |</span><br><span class="line">+-----------+------+------------------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mysql&gt; exit</span><br><span class="line">Bye</span><br><span class="line">[ap@cs1]~/apps/hive% sudo service mysql restart</span><br><span class="line"></span><br><span class="line"># 再重新初始化就好了</span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="Yum安装-Mysql"><a href="#Yum安装-Mysql" class="headerlink" title="Yum安装 Mysql"></a>Yum安装 Mysql</h3><blockquote><p><strong>因为笔者没装过，所以这部分暂且不表</strong></p></blockquote><hr><h2 id="step2-安装Hive"><a href="#step2-安装Hive" class="headerlink" title="step2-安装Hive"></a>step2-安装Hive</h2><p>MySql安装好之后， 安装Hive就很简答了</p><h4 id="注意-Hive-是操作-Mysql-的数据库-一定要记得把-mysql-驱动文件放到-hive-下的-lib-中"><a href="#注意-Hive-是操作-Mysql-的数据库-一定要记得把-mysql-驱动文件放到-hive-下的-lib-中" class="headerlink" title="注意: Hive 是操作 Mysql 的数据库, 一定要记得把 mysql 驱动文件放到 hive 下的 lib 中!!!"></a>注意: Hive 是操作 Mysql 的数据库, 一定要记得把 mysql 驱动文件放到 hive 下的 lib 中!!!</h4><h4 id="启动beeline-前-要把-hiveserver2开起来-当然-hdfs-mysql-都要启动起来"><a href="#启动beeline-前-要把-hiveserver2开起来-当然-hdfs-mysql-都要启动起来" class="headerlink" title="启动beeline 前, 要把 hiveserver2开起来, 当然, hdfs, mysql 都要启动起来"></a>启动beeline 前, 要把 hiveserver2开起来, 当然, hdfs, mysql 都要启动起来</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.上传到Linux</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.解压安装包到安装目录</span></span><br><span class="line">tar -zxvf apache-hive-2.3.2-bin.tar.gz -C ~/apps/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.把MySQL驱动包(mysql-connector-java-5.1.40-bin.jar)放置在hive 的根路径下的 lib 目录，此处是 ~/apps/apache-hive-2.3.2-bin/lib</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.修改配置文件</span></span><br><span class="line"><span class="built_in">cd</span> ~/apps/apache-hive-2.3.2-bin/conf</span><br><span class="line"><span class="comment"># 新建一个 hive-site.xml</span></span><br><span class="line">touch hive-site.xml</span><br><span class="line">vi hive-site.xml</span><br><span class="line"></span><br><span class="line">+++++++++++++++++++++++++++++++++++++++++++添加如下内容++++++++++++</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">&lt;value&gt;jdbc:mysql://localhost:3306/hivedb?createDatabaseIfNotExist=<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">&lt;description&gt;JDBC connect string <span class="keyword">for</span> a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;!-- 注意:如果mysql和hive 不在同一个服务器节点,需要使用mysql节点的 hostname 或 ip --&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">&lt;description&gt;Driver class name <span class="keyword">for</span> a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">&lt;value&gt;root&lt;/value&gt;</span><br><span class="line">&lt;description&gt;username to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">&lt;value&gt;123&lt;/value&gt;</span><br><span class="line">&lt;description&gt;password to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line">+++++++++++++++++++++++++++++++++++++++++++添加如下内容+++++++++++</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">&lt;description&gt;hive default warehouse, <span class="keyword">if</span> nessecory, change it</span><br><span class="line"></span><br><span class="line">&lt;!-- 这个是配置hive在HDFS上 db 的路存储径的，不配默认默认就是上述路径 --&gt;</span><br><span class="line">&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">++++++++++++++++++++++++++</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.配置环境变量 &amp; source</span></span><br><span class="line"><span class="comment">## 注意：ap是我的用户家目录，换上自己的用户家目录</span></span><br><span class="line"><span class="built_in">export</span> HIVE_HOME=/home/ap/apps/apache-hive-2.3.2-bin </span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HIVE_HOME</span>/bin</span><br><span class="line"><span class="comment"># 用bash的找 .bash_profile， 配置所有环境变量</span></span><br><span class="line"><span class="built_in">source</span> ~/.zshrc  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 6.此时基本安装完成了，验证Hive安装</span></span><br><span class="line">hive --helo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7.重点来了！ 初始化元数据库</span></span><br><span class="line">schematool -dbType mysql -initSchema</span><br><span class="line">&gt; 这里可能会遇到很多错误！！ </span><br><span class="line">&gt; 但是如果前面按照我的方法装的, 应该就问题不大了</span><br><span class="line">&gt; 主要是 mysql 连接权限的问题！！</span><br><span class="line"><span class="comment"># 一定会出现的2个</span></span><br><span class="line">1&gt; 找不到 hive命令的一长串环境变量 (不用理会)</span><br><span class="line">2&gt; 两个log4j，jar包重复的问题 (不用理会)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 8.初始化完成后，可以看下数据库中有没有 hivedb 这个库，成功的话是会有的</span></span><br><span class="line"><span class="comment">## 启动hive</span></span><br><span class="line">hive --service cli  </span><br><span class="line">&gt;hive: show databases;</span><br><span class="line"><span class="comment"># 如果能显示数据库，就没啥问题了</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ❤️9.Hive的使用方式之  HiveServer2/beeline</span></span><br><span class="line">此处需要修改 hadoop 的配置文件</span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.1.首先关闭hdfs &amp; yarn服务 &amp; RunJar(hive服务)，修改hadoop配置文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.2.修改 hadoop 集群的 hdfs-site.xml 配置文件:加入一条配置信息，表示启用 webhdfs</span></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;</span><br><span class="line">&lt;value&gt;<span class="literal">true</span>&lt;/value&gt; </span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.3.修改 hadoop 集群的 core-site.xml 配置文件:加入两条配置信息:表示设置 hadoop 的代理用户</span></span><br><span class="line"><span class="comment">## 注意： 此处的ap是配置 hadoop 的用户名</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.proxyuser.ap.hosts&lt;/name&gt;</span><br><span class="line">&lt;value&gt;*&lt;/value&gt; </span><br><span class="line">&lt;!-- 表示任意节点使用 hadoop 集群的代理用户 hadoop 都能访问 hdfs 集群 --&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt; </span><br><span class="line">&lt;name&gt;hadoop.proxyuser.ap.groups&lt;/name&gt; </span><br><span class="line">&lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;!-- 表示代理用户的组所属 --&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"> 注意 </span><br><span class="line">修改完成后， 发给hadoop集群其他主机</span><br><span class="line">scp xxx.xx  xxx.ss  cs2:<span class="variable">$PWD</span></span><br><span class="line">scp xxx.xx  xxx.ss  cs3:<span class="variable">$PWD</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.4.重启hdfs &amp; yarn服务</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.5.启动 hiveserver2 服务</span></span><br><span class="line"><span class="comment">## 后台启动：</span></span><br><span class="line">nohup hiveserver2 1&gt;/home/ap/hiveserver.log 2&gt;/home/ap/hiveserver.err &amp; </span><br><span class="line"><span class="comment"># 或者:</span></span><br><span class="line">nohup hiveserver2 1&gt;/dev/null 2&gt;/dev/null &amp;</span><br><span class="line"><span class="comment"># 或者:</span></span><br><span class="line">nohup hiveserver2 &gt;/dev/null 2&gt;&amp;1 &amp;</span><br><span class="line"><span class="comment"># 以上 3 个命令是等价的，第一个表示记录日志，第二个和第三个表示不记录日志</span></span><br><span class="line"></span><br><span class="line">注意： nohup 可以在你退出帐户/关闭终端之后继续运行相应的进程。 nohup 就是不挂起的意思(no hang up)。</span><br><span class="line">该命令的一般形式为:nohup <span class="built_in">command</span> &amp;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.6 启动 beeline 客户端去连接</span></span><br><span class="line">方式1：执行命令:</span><br><span class="line">beeline -u jdbc:hive2://cs2:10000 -n ap</span><br><span class="line">-u : 指定元数据库的链接信息 -n : 指定用户名和密码</span><br><span class="line"></span><br><span class="line">方式2：</span><br><span class="line">先执行 </span><br><span class="line">beeline</span><br><span class="line">然后按图所示输入:</span><br><span class="line"><span class="comment"># 此处的cs2是只安装hive的hostname</span></span><br><span class="line">!connect jdbc:hive2://cs2:10000 按回车，然后输入用户名，密码，这个 用户名就是安装 hadoop 集群的用户名和密码</span><br></pre></td></tr></table></figure><h4 id="登录beeline-bee"><a href="#登录beeline-bee" class="headerlink" title="登录beeline  :bee:"></a>登录beeline  :bee:</h4><p><strong>方式1: 直接登录</strong></p><p>登录前要开启 RunJar 进程, 也就是 开启hiveserver2</p><p><code>nohup hiveserver2 1&gt;/home/ap/hiveserver.log 2&gt;/home/ap/hiveserver.err &amp;</code></p><p><code>beeline -u jdbc:hive2://cs2:10000 -n ap</code></p><p><strong>方式2: 输入用户名密码登录</strong></p><p><code>!connect jdbc:hive2://cs2:10000</code></p><h4 id="登录-hive"><a href="#登录-hive" class="headerlink" title="登录 hive"></a>登录 hive</h4><p><code>hive</code></p><h4 id="PS-Linux环境变量失效"><a href="#PS-Linux环境变量失效" class="headerlink" title="PS: Linux环境变量失效"></a>PS: Linux环境变量失效</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 就是直接把环境变量设置为/bin:/usr/bin，因为常用的命令都在/bin这个文件夹中。</span></span><br><span class="line">PATH=/bin:/usr/bin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 接下来修改 .bashr_profile 或则 .zshrc中的内容即可， 修改完了重新source</span></span><br></pre></td></tr></table></figure><h1 id="Hive-–-DDL"><a href="#Hive-–-DDL" class="headerlink" title="Hive – DDL"></a>Hive – DDL</h1><h3 id="库的操作"><a href="#库的操作" class="headerlink" title="库的操作"></a>库的操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">  库的操作 </span><br><span class="line">============================================================================</span><br><span class="line"></span><br><span class="line">#  建库 </span><br><span class="line">CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment]</span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[WITH DBPROPERTIES (property_name=property_value, ...)];</span><br><span class="line"></span><br><span class="line">1、创建普通库</span><br><span class="line">create database dbname;</span><br><span class="line"></span><br><span class="line">2、创建库的时候检查存与否</span><br><span class="line">create databse if not exists dbname;</span><br><span class="line"></span><br><span class="line">3、创建库的时候带注释</span><br><span class="line">create database if not exists dbname comment &apos;create my db named dbname&apos;;</span><br><span class="line"></span><br><span class="line">4、查看创建库的详细语句</span><br><span class="line">show create database mydb;</span><br><span class="line"></span><br><span class="line">#  查看库 </span><br><span class="line">1、查看有哪些数据库 </span><br><span class="line">show databases;</span><br><span class="line"></span><br><span class="line">2、显示数据库的详细属性信息</span><br><span class="line">语法:desc database [extended] dbname; </span><br><span class="line">示例:desc database extended myhive;</span><br><span class="line"></span><br><span class="line">3、查看正在使用哪个库 </span><br><span class="line">select current_database();</span><br><span class="line"></span><br><span class="line">4、查看创建库的详细语句 </span><br><span class="line">show create database mydb;</span><br><span class="line"></span><br><span class="line">5、查看以xx开头的库</span><br><span class="line">show databases like &apos;s*&apos;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#  删除库 </span><br><span class="line">删除库操作:</span><br><span class="line">drop database dbname;</span><br><span class="line">drop database if exists dbname;</span><br><span class="line"></span><br><span class="line">默认情况下，hive 不允许删除包含表的数据库，有两种解决办法:</span><br><span class="line">1、 手动删除库下所有表，然后删除库</span><br><span class="line">2、 使用 cascade 关键字</span><br><span class="line">drop database if exists dbname cascade;</span><br><span class="line"></span><br><span class="line">    默认情况下就是 restrict（严格模式）, 后2行效果一样</span><br><span class="line">drop database if exists myhive </span><br><span class="line">drop database if exists myhive restrict</span><br><span class="line"></span><br><span class="line">#  切换库 </span><br><span class="line">切换库操作:</span><br><span class="line">- 语法:</span><br><span class="line">use database_name </span><br><span class="line">- 实例:</span><br><span class="line">use myhive;</span><br></pre></td></tr></table></figure><h3 id="表的操作"><a href="#表的操作" class="headerlink" title="表的操作"></a>表的操作</h3><h4 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h4><h5 id="建表语法"><a href="#建表语法" class="headerlink" title="建表语法"></a><strong>建表语法</strong></h5><ul><li><code>CREATE [EXTERNAL] TABLE [IF NOT EXISTS] &lt;table_name&gt;</code><br>创建内部表；添加EXTERNAL参数会创建外部表</li><li><code>(col_name data_type [COMMENT col_comment], ...)</code><br>添加字段和字段描述</li><li><code>[COMMENT table_comment]</code><br>添加表描述</li><li><code>[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]</code><br>指定分区字段和字段描述，分区字段不能为建表字段！</li><li><code>[CLUSTERED BY (col_name, col_name, ...)]  SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]</code><ul><li>指定分桶字段，分桶字段必须为建表字段！</li><li>指定排序字段，此字段也必须为建表字段，指定的是分桶内的排序</li><li>指定分桶个数（hash后再取模）</li></ul></li><li><code>[ROW FORMAT row_format]</code><br>指定分隔符，row_format 格式：<pre><code>列分隔符：`delimited fields terminated by ‘x&apos;`行分隔符：`line terminated by ‘x&apos;`</code></pre></li><li><code>[STORED AS file_format]</code><br>指定存储格式<pre><code>textfile：文本格式，默认rcfile：行列结合格式parquet：压缩格式</code></pre></li><li><code>[LOCATION hdfs_path]</code><br>指定表在hsfs上的存储路径，不指定的话就按配置的路径存储，如果也没指定就在hive默认的路经 <code>/user/hive/warehouse</code></li></ul><h5 id="建表代码"><a href="#建表代码" class="headerlink" title="建表代码"></a>建表代码</h5><ul><li>a. 创建内部表<ul><li><code>create table mytable (id int, name string) row format delimited fields terminated by &#39;,&#39; stored as textfile;</code></li></ul></li><li>b. 创建外部表<ul><li><code>create  external table mytable2 (id int, name string) row format delimited  fields terminated by &#39;,&#39; location &#39;/user/hive/warehouse/mytable2&#39;;</code></li></ul></li><li>c. 创建分区表<ul><li><code>create  table table3(id int, name string) partitioned by(sex string) row format  delimited fields terminated by &#39;,&#39; stored as textfile;</code></li><li>插入分区数据：<code>load data local inpath &#39;/root/hivedata/mingxing.txt&#39; overwrite into table mytable3 partition(sex=&#39;girl’);</code></li><li>查询表分区： <code>show partitions mytable3</code></li></ul></li><li>d. 创建分桶表<ul><li><code>create  table stu_buck(Sno int,Sname string,Sex string,Sage int,Sdept string)  clustered by(Sno) sorted by(Sno DESC) into 4 buckets row format  delimited fields terminated by &#39;,’;</code></li></ul></li><li>e. 复制表<ul><li><code>create [external] table [if not exists] new_table like table_name;</code></li></ul></li><li>f. 查询表<ul><li><code>create table table_a as select * from  teble_b;</code></li></ul></li></ul><h4 id="查看表"><a href="#查看表" class="headerlink" title="查看表"></a>查看表</h4><ul><li><code>desc &lt;table_name&gt;</code>：显示表的字段信息</li><li><code>desc formatted &lt;table_name&gt;</code>：格式化显示表的详细信息</li><li><code>desc extended &lt;table_name&gt;</code>：显示表的详细信息</li></ul><h4 id="修改表"><a href="#修改表" class="headerlink" title="修改表"></a>修改表</h4><ul><li>重命名<ul><li><code>ALTER TABLE old_name RENAME TO new_name</code></li></ul></li><li>修改属性<ul><li><code>ALTER TABLE table_name SET TBLPROPERTIES (&#39;comment&#39; = &#39;my new students table’);</code></li><li>不支持修改表名，和表的数据存储目录</li></ul></li><li>增加/修改/替换字段<ul><li><code>ALTER TABLE table_name ADD COLUMNS (col_spec [, col_spec ...])</code><br>新增的字段位置在所有列后面 ( partition 列前 )</li><li><code>ALTER TABLE table_name CHANGE c_name new_c_name new_c_type [FIRST | AFTER c_name]</code><br>注意修改字段时，类型只能由小类型转为大类型，不让回报错；（在hive1.2.2中并没有此限制）</li><li><code>ALTER TABLE table_name REPLACE COLUMNS (col_spec [, col_spec ...])</code><br>REPLACE 表示替换表中所有字段</li></ul></li><li>添加/删除分区<ul><li>添加分区：<code>ALTER TABLE table_name ADD [IF NOT EXISTS]  PARTITION (partition_col = col_value1 [ ... ] ) [LOCATION &#39;location1’]</code></li><li>删除分区：<code>ALTER TABLE table_name DROP [IF EXISTS] PARTITION (partition_col = col_value1 [ ... ] )</code></li><li>修改分区路径：<code>ALTER TABLE student_p PARTITION (part=&#39;bb&#39;) SET LOCATION &#39;/myhive_bbbbb’;</code></li><li><strong>[补充：]</strong><ul><li>1、 防止分区被删除：alter table student_p partition (part=’aa’) enable no_drop;</li><li>2、 防止分区被查询：alter table student_p partition (part=’aa’) enable offline;<br>enable 和 disable 是反向操作 </li></ul></li></ul></li></ul><h4 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h4><ul><li><code>drop table if exists &lt;table_name&gt;;</code></li></ul><h4 id="清空表"><a href="#清空表" class="headerlink" title="清空表"></a>清空表</h4><p>会保留表结构</p><ul><li><code>truncate table table_name;</code></li><li><code>truncate table table_name partition(city=&#39;beijing’);</code></li></ul><h4 id="其他辅助命令"><a href="#其他辅助命令" class="headerlink" title="其他辅助命令"></a>其他辅助命令</h4><ul><li>a. 查看数据库列表<ul><li><code>show databases;</code></li><li><code>show databases like &#39;my*&#39;;</code></li></ul></li><li>b. 查看数据表<ul><li><code>show tables;</code></li><li><code>show tables in db_name;</code></li></ul></li><li>c. 查看数据表的建表语句<ul><li><code>show create table table_name;</code></li></ul></li><li>d. 查看 hive 函数列表<ul><li><code>show functions;</code></li></ul></li><li>e. 查看 hive 表的分区<ul><li><code>show partitions table_name;</code></li><li><code>show partitions table_name partition(city=&#39;beijing&#39;)</code></li></ul></li><li>f. 查看表的详细信息（元数据信息） <ul><li><code>desc table_name;</code></li><li><code>desc extended table_name;</code></li></ul></li><li>g. 查看数据库的详细属性信息<ul><li><code>desc formatted table_name;</code></li><li><code>desc database db_name; desc database extended db_name;</code></li></ul></li><li>h. 清空数据表<ul><li><code>truncate table table_name;</code></li></ul></li></ul><p><br></p><h1 id="Hive-–-DML"><a href="#Hive-–-DML" class="headerlink" title="Hive – DML"></a>Hive – DML</h1><h3 id="装载数据"><a href="#装载数据" class="headerlink" title="装载数据"></a>装载数据</h3><ul><li><code>LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE table_name [PARTITION (partcol1=val1, partcol2=val2 ...)]</code></li><li>注意：<ul><li>LOAD 操作只是单纯的 复制（本地文件）或者 移动（hdfs文件，一般是公共数据，需要建立外部表）操作，将数据文件移动到 Hive 表对应的位置</li><li>如果指定了 LOCAL 就去本地文件系统中查找，否则按 inpath 中的 uri 在 hdfs 上查找</li><li>inpath 子句中的文件路径下，不能再有文件夹</li><li>如果使用了 OVERWRITE 关键字，则目标表（或者分区）中的内容会被删除，然后再将 filepath 指向的文件/目录中的内容添加到表/分区中。 </li><li>如果目标表（分区）已经有一个文件，并且文件名和 filepath 中的文件名冲突，那么现有的文件会被新文件所替代。???不是自动重命名为xxx_copy_1</li></ul></li></ul><h3 id="插入数据"><a href="#插入数据" class="headerlink" title="插入数据"></a>插入数据</h3><ul><li><p>a. 单条插入</p><ul><li><code>INSERT INTO TABLE table_name VALUES(value1, value2, ...);</code></li></ul></li><li><p>b. 单重插入</p><ul><li><code>INSERT INTO TABLE table_name [PARTITION (partcol1=val1, ...)] &lt;select_statement1 FROM from_statement&gt;</code></li></ul></li><li><p>c. 多重插入</p><ul><li>FROM from_statement<br>从基表中按不同的字段查询得到的结果分别插入不同的 hive 表<br>只会扫描一次基表，提高查询性能</li><li><code>INSERT INTO TABLE table_name1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 [WHERE where_statement]</code></li><li><code>INSERT INTO TABLE table_name2 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement2 [WHERE where_statement]</code></li><li>[ … ];</li></ul></li><li><p>d. 分区插入</p><ul><li><p>分区插入有两种：一种是静态分区，另一种是动态分区。</p></li><li><p>如果混合使用静态分区和动态分区， 则静态分区必须出现在动态分区之前。</p></li><li><p>静态分区</p><ul><li>A)、创建静态分区表 </li><li>B)、从查询结果中导入数据（单重插入）<br>记得加载数据前要添加分区，然后指定要加载到那个分区</li><li>C)、查看插入结果 </li></ul></li><li><p>动态分区</p><ul><li><p>静态分区添加数据前需要指定分区，当分区个数不确定的时候就很不方便了，这个时候可以使用动态分区</p></li><li><p>重要且常用，尤其是按照日期分区时！</p></li><li><p>A)、创建分区表 </p></li><li><p>B)、参数设置</p></li></ul></li></ul></li></ul><pre><code>  hive-1.2版本  -  set hive.exec.dynamic.partition=true;   //动态分区开启状态，默认开启  -  set hive.exec.dynamic.partition.mode=nonstrict;   //动态分区执行模式，默认&quot;strict&quot;，在这种模式 下要求至少有一列分区字段是静态的，这有助于阻止因设计错误导致查询产生大量的分区  - \# 可选设置项    如果这些参数被更改了又想还原，则执行一次 reset 命令即可    - set hive.exec.max.dynamic.partitions.pernode=100;   //每个节点生成动态分区最大个数     - set hive.exec.max.dynamic.partitions=1000;   //生成动态分区最大个数，如果自动分区数大于这个参数，将会报错     - set hive.exec.max.created.files=100000;   //一个任务最多可以创建的文件数目     - set dfs.datanode.max.xcievers=4096;   //限定一次最多打开的文件数 set     - hive.error.on.empty.partition=false;   //表示当有空分区产生时，是否抛出异常- C)、动态数据插入   - 单个分区字段    - insert into table test2 partition (age) select name,address,school,age from students;   - 多个分区字段    多重分区中目录结构是按照分区字段顺序进行划分的    - insert into table student_ptn2 partition(department, age) select id, name, sex, department,age from students;      分区字段都是动态的    - insert into table student_ptn2 partition(city=&apos;sa&apos;, zipcode) select id, name, sex, age, department, department as zipcode from students;      第一个分区字段时静态的，第二字department字段动态的，重命名为zipcode？  - [注意：]    - 查询语句 select 查询出来的动态分区 age 和 zipcode 必须放在 最后，和分区字段对应，不然结果会出错- D)、查看插入结果  - select * from student_ptn2 where city=&apos;sa&apos; and zipcode=&apos;MA&apos;;</code></pre><ul><li><p>e. 分桶插入</p><ul><li>A)、创建分桶表</li><li>B)、从查询结果中导入数据<br>只能使用insert方式</li><li>C)、查看插入结果 </li><li># 几个命令<ul><li>set hive.exec.reducers.bytes.per.reducer=<number>  // 设置每个reducer的吞吐量，单位byte，默认256M</number></li><li>set hive.exec.reducers.max=<number>  //reduceTask最多执行个数，默认1009</number></li><li>set mapreduce.job.reduces=<number>   //设置reducetask实际运行数，默认-1，代表没有设置，即reducetask默认数为1</number></li><li>set hive.exec.mode.local.auto=true //设置hive本地模式</li></ul></li></ul></li></ul><h3 id="导出数据（了解）"><a href="#导出数据（了解）" class="headerlink" title="导出数据（了解）"></a>导出数据（了解）</h3><ul><li>单模式导出<ul><li><code>INSERT OVERWRITE [LOCAL] DIRECTORY directory1 &lt;select_statement&gt;;</code></li></ul></li><li>多模式导出<ul><li><code>FROM from_statement</code></li><li><code>INSERT OVERWRITE [LOCAL] DIRECTORY directory1 &lt;select_statement1&gt;</code></li><li><code>[INSERT OVERWRITE [LOCAL] DIRECTORY directory2 &lt;select_statement2&gt;] ...</code></li></ul></li></ul><h3 id="查询数据"><a href="#查询数据" class="headerlink" title="查询数据"></a>查询数据</h3><ul><li>Hive 中的 SELECT 基础语法和标准 SQL 语法基本一致，支持 WHERE、DISTINCT、GROUP BY、 ORDER BY、HAVING、LIMIT、子查询等；<ul><li>1、<code>select * from db.table1</code><br>虽然可以，但是要尽量避免 select * 这样的全表扫描操作，效率太低又费时</li><li>2、<code>select count(distinct uid) from db.table1</code></li><li>3、支持 select、union all、join（left、right、full join）、like、where、having、各种聚合函数、 支持 json 解析 </li><li>4、UDF/ UDAF/UDTF <ul><li>UDF：User Defined Function，自定义函数，一对一</li><li>UDAF：User Defined Aggregate Function，自定义聚合函数，多对一，如sum()，count()</li><li>UDTF ：User Defined Table Function，自定义表函数，一对多，如explode()</li></ul></li><li>5、不支持 update 和 delete </li><li>6、hive 虽然支持 in/exists（老版本是不支持的），但是 hive 推荐使用 semi join 的方式来代替 实现，而且效率更高。 <ul><li>半连接<ul><li>左半连接：left semi join，以左表为基表，右表有的，只显示左表相应记录（即一半）</li><li>右半连接：right semi join，与左半连接相反</li></ul></li><li>内连接：inner join，两表中都有的才会连接</li><li>外连接<ul><li>左外连接：left outer join，坐标为基表，右表有的会关联，右表没有的以null表示；左表没有右表有的不会关联</li><li>右外连接：right outer join，与左外连接相反</li><li>全外连接：full outer join，两表合并</li></ul></li></ul></li><li>7、支持 case … when …</li></ul></li><li><strong>语法结构</strong><ul><li><code>SELECT [ALL | DISTINCT] select_ condition, select_ condition, ...</code></li><li>FROM table_name a </li><li>[JOIN table_other b ON <a href="http://a.id" target="_blank" rel="noopener">a.id</a> = <a href="http://b.id" target="_blank" rel="noopener">b.id</a>]<br>表连接</li><li>[WHERE where_condition]<br>过滤条件</li><li>[GROUP BY col_list [HAVING condition]]<br>分组条件</li><li>[CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY col_list | ORDER BY col_list] [DESC]]<br>排序条件：<br>\1. order by：全局排序，默认升序，DESC表示降序。只有一个 reduce task 的结果，比如文<br>件名是 000000_0，会导致当输入规模较大时，需要较长的计算时间。<br>\2.  sort by：局部排序，其在数据进入 reducer 前完成排序。因此，如果用 sort by 进行排序，并且设置  mapred.reduce.tasks &gt; 1，则 sort by 只保证每个 reducer 的输出有序，不保证全局有序。<br>\3. distribute by：类似于分桶，根据指定的字段将数据分到不同的 reducer，且分发算法是 hash 散列<br>\4. cluster by：除了具有 Distribute by 的功能外，还会对该字段进行排序。<br>注意：如果 distribute 和 sort 字段是同一个时，cluster by = distribute by + sort by；<br>如果分桶字段和排序字段不一样，那么就不能使用 clustered by </li><li>[LIMIT number]<br>显示结果的前几个记录</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hive简介&quot;&gt;&lt;a href=&quot;#Hive简介&quot; class=&quot;headerlink&quot; title=&quot;Hive简介&quot;&gt;&lt;/a&gt;Hive简介&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/study/2018-
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/categories/Hadoop/Hive/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="Hive" scheme="https://airpoet.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>偶然发现的好歌</title>
    <link href="https://airpoet.github.io/2018/06/11/Songs/%E5%81%B6%E7%84%B6%E5%8F%91%E7%8E%B0%E7%9A%84%E5%A5%BD%E6%AD%8C/"/>
    <id>https://airpoet.github.io/2018/06/11/Songs/偶然发现的好歌/</id>
    <published>2018-06-11T11:04:05.646Z</published>
    <updated>2018-06-11T11:32:43.834Z</updated>
    
    <content type="html"><![CDATA[ <blockquote class="blockquote-center"><h1 id="Dealbreaker"><a href="#Dealbreaker" class="headerlink" title="Dealbreaker"></a>Dealbreaker</h1><ul><li>专辑：<a href="orpheus://orpheus/pub/app.html#/m/album/?id=1715512" target="_blank" rel="noopener">Chesapeake</a></li><li>歌手：<a href="orpheus://orpheus/pub/app.html#/m/artist/?id=72720" target="_blank" rel="noopener">Rachael Yamagata</a></li><li>链接：<a href="http://music.163.com/#/m/song?id=18733198" target="_blank" rel="noopener">http://music.163.com/#/m/song?id=18733198</a></li></ul><p><br></p><h1 id="You-Won’t-Let-Me"><a href="#You-Won’t-Let-Me" class="headerlink" title="You Won’t Let Me"></a>You Won’t Let Me</h1><ul><li>专辑：<a href="orpheus://orpheus/pub/app.html#/m/album/?id=1715512" target="_blank" rel="noopener">Chesapeake</a></li><li>歌手：<a href="orpheus://orpheus/pub/app.html#/m/artist/?id=72720" target="_blank" rel="noopener">Rachael Yamagata</a></li><li>链接：<a href="http://music.163.com/#/song?id=18733192" target="_blank" rel="noopener">http://music.163.com/#/song?id=18733192</a></li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      
      
         &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h1 id=&quot;Dealbreaker&quot;&gt;&lt;a href=&quot;#Dealbreaker&quot; class=&quot;headerlink&quot; title=&quot;Dealbreaker&quot;&gt;&lt;/a&gt;Dealbreaker&lt;/h
      
    
    </summary>
    
      <category term="文艺" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/"/>
    
      <category term="Songs" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/Songs/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="文艺" scheme="https://airpoet.github.io/tags/%E6%96%87%E8%89%BA/"/>
    
      <category term="Songs" scheme="https://airpoet.github.io/tags/Songs/"/>
    
  </entry>
  
  <entry>
    <title>_HDFS应用场景&amp;原理&amp;基本架构及使用方法</title>
    <link href="https://airpoet.github.io/2018/06/11/Hadoop/1-HDFS/HDFS%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF&amp;%E5%8E%9F%E7%90%86&amp;%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84%E5%8F%8A%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <id>https://airpoet.github.io/2018/06/11/Hadoop/1-HDFS/HDFS应用场景&amp;原理&amp;基本架构及使用方法/</id>
    <published>2018-06-11T00:01:43.146Z</published>
    <updated>2018-06-11T16:27:43.347Z</updated>
    
    <content type="html"><![CDATA[<h1 id="HDFS基本架构和原理"><a href="#HDFS基本架构和原理" class="headerlink" title="HDFS基本架构和原理"></a>HDFS基本架构和原理</h1><h2 id="HDFS设计思想"><a href="#HDFS设计思想" class="headerlink" title="HDFS设计思想"></a>HDFS设计思想</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-142401.png" alt="image-20180611222401493"></p><h2 id="HDFS架构"><a href="#HDFS架构" class="headerlink" title="HDFS架构"></a>HDFS架构</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-142442.png" alt="image-20180611222442066"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-142505.png" alt="image-20180611222505133"></p><h2 id="HDFS数据块（block）"><a href="#HDFS数据块（block）" class="headerlink" title="HDFS数据块（block）"></a>HDFS数据块（block）</h2><ul><li><strong>注意： Hadoop2.x，block默认大小是128MB</strong></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-142646.png" alt="image-20180611222645888"></p><h2 id="HDFS写流程"><a href="#HDFS写流程" class="headerlink" title="HDFS写流程"></a>HDFS写流程</h2><ul><li>创建Distributed FileSystem类 </li><li>询问 NameNode 要写的文件对否存在</li><li>不存在就写入到 FSDataOutputStream 流中</li><li>流写出去到一个 DataNode</li><li>…</li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-143228.png" alt="image-20180611223228053"></p><h2 id="HDFS读流程"><a href="#HDFS读流程" class="headerlink" title="HDFS读流程"></a>HDFS读流程</h2><ul><li>客户端向 NameNode 询问 block 的位置</li><li>按照客户端按照拿到的位置，向不同的DataNode 请求数据</li><li>……</li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-143510.png" alt="image-20180611223510239"></p><h2 id="HDFS典型物理拓扑"><a href="#HDFS典型物理拓扑" class="headerlink" title="HDFS典型物理拓扑"></a>HDFS典型物理拓扑</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-143914.png" alt="image-20180611223914168"></p><h2 id="HDFS副本放置策略"><a href="#HDFS副本放置策略" class="headerlink" title="HDFS副本放置策略"></a>HDFS副本放置策略</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-143940.png" alt="image-20180611223939952"></p><h2 id="HDFS可靠性策略"><a href="#HDFS可靠性策略" class="headerlink" title="HDFS可靠性策略"></a>HDFS可靠性策略</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-144153.png" alt="image-20180611224152799"></p><h2 id="HDFS不适合存储小文件"><a href="#HDFS不适合存储小文件" class="headerlink" title="HDFS不适合存储小文件"></a>HDFS不适合存储小文件</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-144344.png" alt="image-20180611224344481"></p><p><br></p><h1 id="HDFS程序设计"><a href="#HDFS程序设计" class="headerlink" title="HDFS程序设计"></a>HDFS程序设计</h1><h2 id="HDFS访问方式"><a href="#HDFS访问方式" class="headerlink" title="HDFS访问方式"></a>HDFS访问方式</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-144935.png" alt="image-20180611224934785"></p><h2 id="HDFS-Shell命令"><a href="#HDFS-Shell命令" class="headerlink" title="HDFS Shell命令"></a>HDFS Shell命令</h2><h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145132.png" alt="image-20180611225131966"></p><h3 id="HDFS-Shell命令—文件操作命令"><a href="#HDFS-Shell命令—文件操作命令" class="headerlink" title="HDFS Shell命令—文件操作命令"></a>HDFS Shell命令—文件操作命令</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145219.png" alt="image-20180611225219211"></p><h3 id="HDFS-Shell命令—文件操作命令-1"><a href="#HDFS-Shell命令—文件操作命令-1" class="headerlink" title="HDFS Shell命令—文件操作命令"></a>HDFS Shell命令—文件操作命令</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145604.png" alt="image-20180611225604218"></p><h3 id="HDFS-Shell命令—管理命令"><a href="#HDFS-Shell命令—管理命令" class="headerlink" title="HDFS Shell命令—管理命令"></a>HDFS Shell命令—管理命令</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145728.png" alt="image-20180611225727910"></p><h3 id="HDFS-Shell命令—管理脚本"><a href="#HDFS-Shell命令—管理脚本" class="headerlink" title="HDFS Shell命令—管理脚本"></a>HDFS Shell命令—管理脚本</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145803.png" alt="image-20180611225802894"></p><h3 id="HDFS-Shell命令—文件管理命令fsck"><a href="#HDFS-Shell命令—文件管理命令fsck" class="headerlink" title="HDFS Shell命令—文件管理命令fsck"></a>HDFS Shell命令—文件管理命令fsck</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145932.png" alt="image-20180611225931880"></p><ul><li><strong>查看帮助</strong></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-145915.png" alt="image-20180611225914482"></p><ul><li><strong>用法示例</strong></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-150253.png" alt="image-20180611230252655"></p><h3 id="HDFS-Shell命令—数据均衡器balancer"><a href="#HDFS-Shell命令—数据均衡器balancer" class="headerlink" title="HDFS Shell命令—数据均衡器balancer"></a>HDFS Shell命令—数据均衡器balancer</h3><ul><li><strong>一般设置10% —— 15% 就差不多了</strong></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-150614.png" alt="image-20180611230613854"></p><h3 id="HDFS-Shell命令—设置目录份额"><a href="#HDFS-Shell命令—设置目录份额" class="headerlink" title="HDFS Shell命令—设置目录份额"></a>HDFS Shell命令—设置目录份额</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-150704.png" alt="image-20180611230703142"></p><h3 id="※-HDFS-Shell命令—增加-移除节点-※"><a href="#※-HDFS-Shell命令—增加-移除节点-※" class="headerlink" title="※ HDFS Shell命令—增加/移除节点 ※"></a>※ HDFS Shell命令—增加/移除节点 ※</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-150741.png" alt="image-20180611230741126"></p><h2 id="HDFS-Java"><a href="#HDFS-Java" class="headerlink" title="HDFS Java"></a>HDFS Java</h2><h3 id="API介绍"><a href="#API介绍" class="headerlink" title="API介绍"></a>API介绍</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-150953.png" alt="image-20180611230953359"></p><h3 id="HDFS-Java程序举例"><a href="#HDFS-Java程序举例" class="headerlink" title="HDFS Java程序举例"></a>HDFS Java程序举例</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-151048.png" alt="image-20180611231047354"></p><h2 id="HDFS-多语言API—借助thrift"><a href="#HDFS-多语言API—借助thrift" class="headerlink" title="HDFS 多语言API—借助thrift"></a>HDFS 多语言API—借助thrift</h2><p><a href="http://thrift.apache.org/" target="_blank" rel="noopener">也是Apache的顶级项目</a></p><h3 id="thrift执行流程"><a href="#thrift执行流程" class="headerlink" title="thrift执行流程"></a>thrift执行流程</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-151149.png" alt="image-20180611231148955"></p><h3 id="hadoopfs-thrift接口定义"><a href="#hadoopfs-thrift接口定义" class="headerlink" title="hadoopfs.thrift接口定义"></a>hadoopfs.thrift接口定义</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-151634.png" alt="image-20180611231634046"></p><h3 id="PHP语言访问HDFS"><a href="#PHP语言访问HDFS" class="headerlink" title="PHP语言访问HDFS"></a>PHP语言访问HDFS</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-151654.png" alt="image-20180611231653750"></p><h3 id="Python语言访问HDFS"><a href="#Python语言访问HDFS" class="headerlink" title="Python语言访问HDFS"></a>Python语言访问HDFS</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-152052.png" alt="image-20180611232052633"></p><hr><h1 id="Hadoop-2-0新特性"><a href="#Hadoop-2-0新特性" class="headerlink" title="Hadoop 2.0新特性"></a>Hadoop 2.0新特性</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-152948.png" alt="image-20180611232948514"></p><h2 id="HA-高可用-与Federation-联邦"><a href="#HA-高可用-与Federation-联邦" class="headerlink" title="HA(高可用)与Federation(联邦)"></a>HA(高可用)与Federation(联邦)</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153058.png" alt="image-20180611233058385"></p><h2 id="异构层级存储结构"><a href="#异构层级存储结构" class="headerlink" title="异构层级存储结构"></a>异构层级存储结构</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153117.png" alt="image-20180611233117295"></p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153206.png" alt="image-20180611233206158"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153228.png" alt="image-20180611233227447"></p><h2 id="HDFS-ACL"><a href="#HDFS-ACL" class="headerlink" title="HDFS ACL"></a>HDFS ACL</h2><h3 id="背景：现有权限管理的局限性"><a href="#背景：现有权限管理的局限性" class="headerlink" title="背景：现有权限管理的局限性"></a>背景：现有权限管理的局限性</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153255.png" alt="image-20180611233255196"></p><h3 id="基于POSIX-ACL的实现"><a href="#基于POSIX-ACL的实现" class="headerlink" title="基于POSIX ACL的实现"></a>基于POSIX ACL的实现</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153310.png" alt="image-20180611233310082"></p><h2 id="HDFS快照"><a href="#HDFS快照" class="headerlink" title="HDFS快照"></a>HDFS快照</h2><h3 id="背景-1"><a href="#背景-1" class="headerlink" title="背景"></a>背景</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153338.png" alt="image-20180611233337709"></p><h3 id="基本使用方法"><a href="#基本使用方法" class="headerlink" title="基本使用方法"></a>基本使用方法</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153358.png" alt="image-20180611233357356"></p><h2 id="HDFS缓存"><a href="#HDFS缓存" class="headerlink" title="HDFS缓存"></a>HDFS缓存</h2><h3 id="背景-2"><a href="#背景-2" class="headerlink" title="背景"></a>背景</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153423.png" alt="image-20180611233423131"></p><h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153446.png" alt="image-20180611233446072"></p><h3 id="实现情况"><a href="#实现情况" class="headerlink" title="实现情况"></a>实现情况</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153502.png" alt="image-20180611233502379"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-153521.png" alt="image-20180611233521502"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;HDFS基本架构和原理&quot;&gt;&lt;a href=&quot;#HDFS基本架构和原理&quot; class=&quot;headerlink&quot; title=&quot;HDFS基本架构和原理&quot;&gt;&lt;/a&gt;HDFS基本架构和原理&lt;/h1&gt;&lt;h2 id=&quot;HDFS设计思想&quot;&gt;&lt;a href=&quot;#HDFS设计思想&quot;
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="HDFS" scheme="https://airpoet.github.io/categories/Hadoop/HDFS/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="HDFS" scheme="https://airpoet.github.io/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>关于hexo的时序图插件 hexo-filter-sequence 的巨坑</title>
    <link href="https://airpoet.github.io/2018/06/10/Tools/Hexo/%E5%85%B3%E4%BA%8Ehexo%E7%9A%84%E6%97%B6%E5%BA%8F%E5%9B%BE%E6%8F%92%E4%BB%B6-hexo-filter-sequence-%E7%9A%84%E5%B7%A8%E5%9D%91/"/>
    <id>https://airpoet.github.io/2018/06/10/Tools/Hexo/关于hexo的时序图插件-hexo-filter-sequence-的巨坑/</id>
    <published>2018-06-10T13:13:02.149Z</published>
    <updated>2018-06-10T13:19:32.909Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在写代码的过程中，发现时序图还是挺管用的，简洁明了，于是想用一用。</p><p>结果发到站上，不显示。</p><p>在网上查了下，发现是hexo所使用的Markdown渲染语法不支持。</p><p>这里吐槽下，这里渲染的确实烂，作者为啥不改改..</p><p><br></p><p>于是开始找解决方案，发现大多数都推荐了一个叫<code>hexo-filter-sequence</code>的插件，故安装之。</p><p>结果死活还是不行。</p><p>装了其它的几个flow图，却可以显示。</p><p><strong>当flow图和sequence图同时存在的时候，sequence可以显示，但是sequence单独存在时，死活不显示。</strong></p><p>难不成有啥依赖？必须先使用flow才能使用sequence？逻辑上不应该啊！</p><p>但是事实却是这样！</p><p><br></p><p>网上有一篇大神说要改源码才行，开始没注意，觉得应该不会吧，这么多人使用，源码还会有问题？？</p><p><strong>仔细一看源码，妈的！！心里千万匹草泥马呼啸而过！</strong></p><p><strong>把初始化 sequence，写成了初始化 flow！！！</strong></p><p><strong>把 flow 改成 sequence， 再把 js CDN源换成国内的！</strong></p><p><strong>可以了！！</strong></p><p>再仔细一看，发现最后一次更新是在1年前！</p><p>坑爹的作者，浪费了我至少3-5个小时！！</p><p><br></p><hr><h2 id="下面为部分摘抄"><a href="#下面为部分摘抄" class="headerlink" title="下面为部分摘抄"></a>下面为部分摘抄</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p><a href="https://github.com/bubkoo/hexo-filter-sequence" target="_blank" rel="noopener">hexo-filter-sequence</a> 插件:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install --save hexo-filter-sequence</span><br></pre></td></tr></table></figure><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>站点配置文件 <code>_config.yml</code> 中增加如下配置:</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sequence:</span><br><span class="line">  webfont: https:<span class="comment">//cdn.bootcss.com/webfont/1.6.28/webfontloader.js</span></span><br><span class="line">  raphael: https:<span class="comment">//cdn.bootcss.com/raphael/2.2.7/raphael.min.js</span></span><br><span class="line">  underscore: https:<span class="comment">//cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js</span></span><br><span class="line">  sequence: https:<span class="comment">//cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js</span></span><br><span class="line">  css: # optional, the url for css, such as hand drawn theme </span><br><span class="line">  options: </span><br><span class="line">    theme: simple</span><br><span class="line">    css_class:</span><br></pre></td></tr></table></figure><h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><p>源码修改后才能正常使用，进入插件目录作如下修改：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// index.js</span></span><br><span class="line"><span class="keyword">var</span> assign = <span class="built_in">require</span>(<span class="string">'deep-assign'</span>);</span><br><span class="line"><span class="keyword">var</span> renderer = <span class="built_in">require</span>(<span class="string">'./lib/renderer'</span>);</span><br><span class="line"></span><br><span class="line">hexo.config.sequence = assign(&#123;</span><br><span class="line">  webfont: <span class="string">'https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js'</span>,</span><br><span class="line">  raphael: <span class="string">'https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js'</span>,</span><br><span class="line">  underscore: <span class="string">'https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js'</span>,</span><br><span class="line">  sequence: <span class="string">'https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js'</span>,</span><br><span class="line">  css: <span class="string">''</span>,</span><br><span class="line">  options: &#123;</span><br><span class="line">    theme: <span class="string">'simple'</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;, hexo.config.sequence);</span><br><span class="line"></span><br><span class="line">hexo.extend.filter.register(<span class="string">'before_post_render'</span>, renderer.render, <span class="number">9</span>);</span><br></pre></td></tr></table></figure><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// lib/renderer.js, 25 行</span></span><br><span class="line"><span class="keyword">if</span> (sequences.length) &#123;</span><br><span class="line">      <span class="keyword">var</span> config = <span class="keyword">this</span>.config.sequence;</span><br><span class="line">      <span class="comment">// resources</span></span><br><span class="line">      data.content += <span class="string">'&lt;script src="'</span> + config.webfont + <span class="string">'"&gt;&lt;/script&gt;'</span>;</span><br><span class="line">      data.content += <span class="string">'&lt;script src="'</span> + config.raphael + <span class="string">'"&gt;&lt;/script&gt;'</span>;</span><br><span class="line">      data.content += <span class="string">'&lt;script src="'</span> + config.underscore + <span class="string">'"&gt;&lt;/script&gt;'</span>;</span><br><span class="line">      data.content += <span class="string">'&lt;script src="'</span> + config.sequence + <span class="string">'"&gt;&lt;/script&gt;'</span>;</span><br><span class="line">      ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>新建代码块，增加如下内容：</p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-10-131910.png" alt="image-20180610211909603"></p><div id="sequence-0"></div><p><a href="http://wewelove.github.io/fcoder/2017/09/06/markdown-sequence/index.html" target="_blank" rel="noopener">详情参考</a></p><p><script src="https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js"></script><script src="https://cdn.bootcss.com/snap.svg/0.5.1/snap.svg-min.js"></script><script src="https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js"></script><script src="https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js"></script><script src="https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js"></script><textarea id="sequence-0-code" style="display: none">Alice->Bob: Hello Bob, how are you?Note right of Bob: Bob thinksBob-->Alice: I am good thanks!</textarea><textarea id="sequence-0-options" style="display: none">{"theme":"simple"}</textarea><script>  var code = document.getElementById("sequence-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value));  var diagram = Diagram.parse(code);  diagram.drawSVG("sequence-0", options);</script></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在写代码的过程中，发现时序图还是挺管用的，简洁明了，于是想用一用。&lt;/p&gt;
&lt;p&gt;结果发到站上，不显示。&lt;/p&gt;
&lt;p&gt;在网上查了下，发现是
      
    
    </summary>
    
      <category term="工具" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="hexo" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/hexo/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="工具" scheme="https://airpoet.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="hexo" scheme="https://airpoet.github.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce-分组浅探</title>
    <link href="https://airpoet.github.io/2018/06/10/Hadoop/2-MapReduce/MapReduce-%E5%88%86%E7%BB%84%E6%B5%85%E6%8E%A2/"/>
    <id>https://airpoet.github.io/2018/06/10/Hadoop/2-MapReduce/MapReduce-分组浅探/</id>
    <published>2018-06-10T08:49:24.337Z</published>
    <updated>2018-06-11T11:46:23.533Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>最近学分组的时候，一直弄不明白，总感觉一头雾水，通过近3个小时的断点调试和log输出。 大概了解了一些表象，先记录。</p><p>案例是这个</p><blockquote><p>求出每门课程参考学生成绩最高平均分的学生的信息：</p><p>课程，姓名和平均分，详细见<a href="https://airpoet.github.io/2018/06/09/Hadoop/Study/MapReduce%E7%AC%94%E8%AE%B0-%E7%BB%83%E4%B9%A0/"><sup>MapReduce笔记-练习第二题第3小题</sup></a></p><p>数据格式是这样的：</p><blockquote><p>第一个是课程名称，总共四个课程，computer，math，english，algorithm，</p><p>第二个是学生姓名，后面是每次考试的分数</p><p><em>math,huangxiaoming,85,75,85,99,66,88,75,91</em></p><p><em>english,huanglei,85,75,85,99,66,88,75,91</em></p><p>… </p></blockquote></blockquote><p><br></p><p><br></p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><div id="sequence-0"></div><h4 id="执行流程结论"><a href="#执行流程结论" class="headerlink" title="执行流程结论"></a><strong>执行流程结论</strong></h4><ul><li>map每读一行就 write 到 context 一次，按照指定的<code>key</code>进行分发</li><li><p>map 把所有的数据都读完了之后，大概执行到<code>67%</code>的时候，开始进入 <code>CustomBean</code>，执行<code>CustomBean</code>的<code>compareTo()</code>方法，会按照自己写的规则一条一条数据比较</p></li><li><p>上述都比较完毕之后，map阶段就结束了，此时来到了 reduce阶段，但是是到了<code>67%</code>了</p></li><li>到了reduce阶段，直接进入了<strong>MyGroup</strong>中自定义的<strong>compare</strong>方法。</li><li>MyGroup的<code>compare()</code>方法，如果返回<strong>非0</strong>， 就会进入 reduce 方法<strong>写出到context</strong></li></ul><h4 id="MyGroup进入Reduce的条件是"><a href="#MyGroup进入Reduce的条件是" class="headerlink" title="MyGroup进入Reduce的条件是"></a><strong>MyGroup进入Reduce的条件是</strong></h4><ul><li>MyReduce中，如果compare的结果不等于0，也就是比较的2者不相同， 此时就进入Reduce， 写出到上下文</li><li>如果相同，会一直往下读，直到读到不同的， 此时写出读到上下文。</li><li>因为MyGroup会在Reduce阶段执行，而<code>CustomBean</code>中的<code>compareTo()</code>是在map阶段执行，所以需要在<code>CustomBean</code>中就把组排好序，此时<strong>分组功能</strong>才能正常运作</li></ul><h4 id="指定分组类MyGroup和不指定的区别"><a href="#指定分组类MyGroup和不指定的区别" class="headerlink" title="指定分组类MyGroup和不指定的区别"></a>指定分组类MyGroup和不指定的区别</h4><p><em>指定与不指定是指：在Driver类中，是否加上<code>job.setGroupingComparatorClass(MyGrouper.class);</code>这一句。</em></p><ul><li><strong>指定分组类</strong>：<ul><li>会按照分组类中，自定义的<code>compare()</code>方法比较，相同的为一组，分完一组就进入一次reduce方法</li></ul></li><li><strong>不指定分组类：（目前存疑）</strong><ul><li>是否是按照key进行分组</li><li>如果是自定义类为key，是否是按照此key中值相同的分为一组</li><li>如果是hadoop内置类，是否是按照此类的值分组（Text-String的值，IntWritable-int值等..）</li><li><strong>依然是走得以上这套分组逻辑，一组的数据读完才进入到Reduce阶段做归并</strong></li></ul></li></ul><p><br></p><p><br></p><h3 id="Log信息"><a href="#Log信息" class="headerlink" title="Log信息"></a>Log信息</h3><h5 id="CustomBean中没有进行分组-组内排序的log"><a href="#CustomBean中没有进行分组-组内排序的log" class="headerlink" title="CustomBean中没有进行分组, 组内排序的log"></a><code>CustomBean</code>中没有进行<code>分组</code>, <code>组内排序</code>的log</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ==================MyGroup中compare()方法=======================</span></span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// ======================reduce中的执行log==========================</span></span><br><span class="line"></span><br><span class="line">==================第<span class="number">1</span>次进入reduce</span><br><span class="line">computerhuangjiaju<span class="number">83.2</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerliutao<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">2</span>次进入reduce</span><br><span class="line">mathhuangxiaoming<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">3</span>次进入reduce</span><br><span class="line">englishhuanglei<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">4</span>次进入reduce</span><br><span class="line">mathhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">5</span>次进入reduce</span><br><span class="line">algorithmhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmliutao<span class="number">82.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">6</span>次进入reduce</span><br><span class="line">computerhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">7</span>次进入reduce</span><br><span class="line">englishliuyifei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">8</span>次进入reduce</span><br><span class="line">algorithmhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">9</span>次进入reduce</span><br><span class="line">mathhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">10</span>次进入reduce</span><br><span class="line">algorithmhuangzitao<span class="number">72.75</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">11</span>次进入reduce</span><br><span class="line">mathliujialing<span class="number">72.75</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">12</span>次进入reduce</span><br><span class="line">computerhuangzitao<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">13</span>次进入reduce</span><br><span class="line">englishhuangxiaoming<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">14</span>次进入reduce</span><br><span class="line">mathwangbaoqiang<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">15</span>次进入reduce</span><br><span class="line">computerhuangxiaoming<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">16</span>次进入reduce</span><br><span class="line">mathxuzheng<span class="number">69.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">17</span>次进入reduce</span><br><span class="line">englishzhaobenshan<span class="number">69.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">18</span>次进入reduce</span><br><span class="line">computerhuangbo<span class="number">65.25</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerxuzheng<span class="number">65.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">19</span>次进入reduce</span><br><span class="line">englishzhouqi<span class="number">64.18181818181819</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">20</span>次进入reduce</span><br><span class="line">computerliujialing<span class="number">64.11111111111111</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">21</span>次进入reduce</span><br><span class="line">algorithmliuyifei<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">22</span>次进入reduce</span><br><span class="line">englishliujialing<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">23</span>次进入reduce</span><br><span class="line">computerliuyifei<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">24</span>次进入reduce</span><br><span class="line">englishliuyifei<span class="number">59.57142857142857</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">25</span>次进入reduce</span><br><span class="line">mathliutao<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">26</span>次进入reduce</span><br><span class="line">algorithmhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">27</span>次进入reduce</span><br><span class="line">computerhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">28</span>次进入reduce</span><br><span class="line">englishhuangbo<span class="number">55.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br></pre></td></tr></table></figure><h5 id="CustomBean中做了分组-amp-组内排序-的"><a href="#CustomBean中做了分组-amp-组内排序-的" class="headerlink" title="CustomBean中做了分组&amp;组内排序 的"></a><code>CustomBean</code>中做了<code>分组&amp;组内排序</code> 的</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// **Bean中相同的组进行分数降序， 组进行字典排序，此时MyGroup中执行的**</span></span><br><span class="line"></span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line"></span><br><span class="line"><span class="comment">// ======================reduce中执行==========================</span></span><br><span class="line"></span><br><span class="line">==================第<span class="number">1</span>次进入reduce</span><br><span class="line">algorithmhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmliutao<span class="number">82.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmhuangzitao<span class="number">72.75</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmliuyifei<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">===================离开reduce</span><br><span class="line">==================第<span class="number">2</span>次进入reduce</span><br><span class="line">computerhuangjiaju<span class="number">83.2</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerliutao<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerhuangzitao<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerhuangxiaoming<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerhuangbo<span class="number">65.25</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerxuzheng<span class="number">65.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerliujialing<span class="number">64.11111111111111</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerliuyifei<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">===================离开reduce</span><br><span class="line">==================第<span class="number">3</span>次进入reduce</span><br><span class="line">englishhuanglei<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishliuyifei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishhuangxiaoming<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishzhaobenshan<span class="number">69.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishzhouqi<span class="number">64.18181818181819</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishliujialing<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishliuyifei<span class="number">59.57142857142857</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishhuangbo<span class="number">55.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">===================离开reduce</span><br><span class="line">==================第<span class="number">4</span>次进入reduce</span><br><span class="line">mathhuangxiaoming<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathliujialing<span class="number">72.75</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathwangbaoqiang<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathxuzheng<span class="number">69.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathliutao<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">===================离开reduce</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//  如果只取一个每次values的第一个的话 </span></span><br><span class="line"></span><br><span class="line">algorithmhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">==================第<span class="number">1</span>次进入reduce</span><br><span class="line">computerhuangjiaju<span class="number">83.2</span></span><br><span class="line">==================第<span class="number">2</span>次进入reduce</span><br><span class="line">englishhuanglei<span class="number">83.0</span></span><br><span class="line">==================第<span class="number">3</span>次进入reduce</span><br><span class="line">mathhuangxiaoming<span class="number">83.0</span></span><br><span class="line">==================第<span class="number">4</span>次进入reduce</span><br></pre></td></tr></table></figure><p><br></p><p><br></p><h3 id="其它疑点"><a href="#其它疑点" class="headerlink" title="其它疑点"></a>其它疑点</h3><ul><li>通过log可以看出来， MyGroup是在读到了不同的数据，才会进入到 reduce 类中写出；</li><li>但是 通过 <strong>断点调试</strong>时， 现象是，第一次读到了2个相同的，就去reduce去写出了；</li><li>后面再读的就不做写出操作，直到下一次读到2个相同的，再在reduce中写出。</li></ul><p><script src="https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js"></script><script src="https://cdn.bootcss.com/snap.svg/0.5.1/snap.svg-min.js"></script><script src="https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js"></script><script src="https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js"></script><script src="https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js"></script><textarea id="sequence-0-code" style="display: none">title: 执行流程时序图 Mapper(map)->ScoreBean: k:ScoreBean(courseName,avgScore)Mapper(map)->ScoreBean: v:Text(stuName)Mapper(ScoreBean)->Reducer(MyGroup): course按字典升序Mapper(ScoreBean)->Reducer(MyGroup): course内成绩降序Reducer(MyGroup)->Reducer(reduce): 根据自定义的分组规则按组输出Reducer(MyGroup)->Reducer(reduce): 一组只调用reduce一次Reducer(reduce)-->Reducer(reduce):</textarea><textarea id="sequence-0-options" style="display: none">{"theme":"simple"}</textarea><script>  var code = document.getElementById("sequence-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value));  var diagram = Diagram.parse(code);  diagram.drawSVG("sequence-0", options);</script></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;最近学分组的时候，一直弄不明白，总感觉一头雾水，通过近3个小时的断点调试和log输出。 大概了解了一些表象，先记录。&lt;/p&gt;
&lt;p&gt;案例是这
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>▍开始</title>
    <link href="https://airpoet.github.io/2018/06/10/Poetry/%E5%BC%80%E5%A7%8B/"/>
    <id>https://airpoet.github.io/2018/06/10/Poetry/开始/</id>
    <published>2018-06-09T17:03:10.001Z</published>
    <updated>2018-06-09T17:19:40.887Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-09-601528563678_.pic.jpg" alt=""></p><p>月亮落下一两片羽毛在田野上。</p><p>黑暗中的麦子聆听着。</p><p>快静下来。</p><p>快。</p><p>就在那儿，月亮的孩子们正试着</p><p>挥动翅膀。</p><p>在两棵树之间，身材修长的女子抬起面庞，</p><p>美丽的剪影。接着，她步入空中，接着，</p><p>她完全消失在空中。</p><p>我独自站在一棵接骨木旁，不敢呼吸，</p><p>也不敢动。</p><p>我聆听着。</p><p>麦子向后靠着自己的黑暗，</p><p>而我靠着我的。</p><p><br></p><p><strong>作者 / [美国] 詹姆斯·赖特</strong></p><p>翻译 / 张文武</p><hr><h3 id="▍Beginning"><a href="#▍Beginning" class="headerlink" title="▍Beginning"></a><strong>▍Beginning</strong></h3><p><br></p><p>The moon drops one or two feathers into the fields.</p><p>The dark wheat listens.</p><p>Be still.</p><p>Now.</p><p>There they are, the moon’s young, trying</p><p>Their wings.</p><p>Between trees, a slender woman lifts up the lovely shadow</p><p>Of her face, and now she steps into the air, now she is gone</p><p>Wholly, into the air.</p><p>I stand alone by an elder tree, I do not dare breathe</p><p>Or move.</p><p>I listen.</p><p>The wheat leans back toward its own darkness,</p><p>And I lean toward mine.</p><p><br></p><p><strong>Author / James Wright</strong></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-09-601528563678_.pic.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;月
      
    
    </summary>
    
      <category term="文艺" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/"/>
    
      <category term="诗歌" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/%E8%AF%97%E6%AD%8C/"/>
    
    
      <category term="诗歌" scheme="https://airpoet.github.io/tags/%E8%AF%97%E6%AD%8C/"/>
    
  </entry>
  
  <entry>
    <title>Markdown高阶语法</title>
    <link href="https://airpoet.github.io/2018/06/10/Tools/Markdown/Markdown%E9%AB%98%E9%98%B6%E8%AF%AD%E6%B3%95/"/>
    <id>https://airpoet.github.io/2018/06/10/Tools/Markdown/Markdown高阶语法/</id>
    <published>2018-06-09T16:49:50.407Z</published>
    <updated>2018-06-10T13:21:24.745Z</updated>
    
    <content type="html"><![CDATA[<h3 id="时序图的写法"><a href="#时序图的写法" class="headerlink" title="时序图的写法"></a>时序图的写法</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-09-165900.png" alt="image-20180610005859980"></p><div id="sequence-0"></div><h3 id="流程图的写法"><a href="#流程图的写法" class="headerlink" title="流程图的写法"></a>流程图的写法</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-09-165930.png" alt="image-20180610005930019"></p><div id="flowchart-0" class="flow-chart"></div><h3 id="类图的写法"><a href="#类图的写法" class="headerlink" title="类图的写法"></a>类图的写法</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-09-165948.png" alt="image-20180610005948130"></p><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLl2jz4qiA4Wjud98pKi12WC0"></p><p><script src="https://cdnjs.cloudflare.com/ajax/libs/raphael/2.2.7/raphael.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/flowchart/1.6.5/flowchart.min.js"></script><textarea id="flowchart-0-code" style="display: none">st=>start: Start|past:>http://www.google.com[blank]e=>end: End:>http://www.google.comop1=>operation: My Operation|pastop2=>operation: Stuff|currentsub1=>subroutine: My Subroutine|invalidcond=>condition: Yesor No?|approved:>http://www.google.comc2=>condition: Good idea|rejectedio=>inputoutput: catch something...|requestst->op1(right)->condcond(yes, right)->c2cond(no)->sub1(left)->op1c2(yes)->io->ec2(no)->op2->e</textarea><textarea id="flowchart-0-options" style="display: none">{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("flowchart-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value));  var diagram = flowchart.parse(code);  diagram.drawSVG("flowchart-0", options);</script><script src="https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js"></script><script src="https://cdn.bootcss.com/snap.svg/0.5.1/snap.svg-min.js"></script><script src="https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js"></script><script src="https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js"></script><script src="https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js"></script><textarea id="sequence-0-code" style="display: none">mapper->Bob: Hello Bob, how are you?Note right of Bob: Bob thinksBob-->reducer: I am good thanks!ad u?reducer->out: I'm fine tooout->me: ok, you winme-->Bob: nono, not</textarea><textarea id="sequence-0-options" style="display: none">{"theme":"simple"}</textarea><script>  var code = document.getElementById("sequence-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value));  var diagram = Diagram.parse(code);  diagram.drawSVG("sequence-0", options);</script></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;时序图的写法&quot;&gt;&lt;a href=&quot;#时序图的写法&quot; class=&quot;headerlink&quot; title=&quot;时序图的写法&quot;&gt;&lt;/a&gt;时序图的写法&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/study/2018-
      
    
    </summary>
    
      <category term="工具" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="Markdown" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/Markdown/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="工具" scheme="https://airpoet.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="Markdown" scheme="https://airpoet.github.io/tags/Markdown/"/>
    
  </entry>
  
  <entry>
    <title>About Sublime Text3</title>
    <link href="https://airpoet.github.io/2018/06/09/Tools/Sublime/%E5%AE%89%E8%A3%85%E4%B8%BB%E9%A2%98/"/>
    <id>https://airpoet.github.io/2018/06/09/Tools/Sublime/安装主题/</id>
    <published>2018-06-09T04:43:17.189Z</published>
    <updated>2018-06-10T01:09:27.890Z</updated>
    
    <content type="html"><![CDATA[<h3 id="主题"><a href="#主题" class="headerlink" title="主题"></a>主题</h3><p><a href="https://scotch.io/bar-talk/best-sublime-text-3-themes-of-2015-and-2016" target="_blank" rel="noopener">详情参见这个网站</a></p><h3 id="详细操作"><a href="#详细操作" class="headerlink" title="详细操作"></a>详细操作</h3><p><a href="http://zh.lucida.me/blog/sublime-text-complete-guide/" target="_blank" rel="noopener">见此站</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;主题&quot;&gt;&lt;a href=&quot;#主题&quot; class=&quot;headerlink&quot; title=&quot;主题&quot;&gt;&lt;/a&gt;主题&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://scotch.io/bar-talk/best-sublime-text-3-themes-of-2015
      
    
    </summary>
    
      <category term="工具" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="Sublime" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/Sublime/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="工具" scheme="https://airpoet.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="Sublime" scheme="https://airpoet.github.io/tags/Sublime/"/>
    
  </entry>
  
</feed>
