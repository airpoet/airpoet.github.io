<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>A.P的文艺杂谈</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://airpoet.github.io/"/>
  <updated>2018-06-11T11:32:43.834Z</updated>
  <id>https://airpoet.github.io/</id>
  
  <author>
    <name>airpoet</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>偶然发现的好歌</title>
    <link href="https://airpoet.github.io/2018/06/11/Songs/%E5%81%B6%E7%84%B6%E5%8F%91%E7%8E%B0%E7%9A%84%E5%A5%BD%E6%AD%8C/"/>
    <id>https://airpoet.github.io/2018/06/11/Songs/偶然发现的好歌/</id>
    <published>2018-06-11T11:04:05.646Z</published>
    <updated>2018-06-11T11:32:43.834Z</updated>
    
    <content type="html"><![CDATA[ <blockquote class="blockquote-center"><h1 id="Dealbreaker"><a href="#Dealbreaker" class="headerlink" title="Dealbreaker"></a>Dealbreaker</h1><ul><li>专辑：<a href="orpheus://orpheus/pub/app.html#/m/album/?id=1715512" target="_blank" rel="noopener">Chesapeake</a></li><li>歌手：<a href="orpheus://orpheus/pub/app.html#/m/artist/?id=72720" target="_blank" rel="noopener">Rachael Yamagata</a></li><li>链接：<a href="http://music.163.com/#/m/song?id=18733198" target="_blank" rel="noopener">http://music.163.com/#/m/song?id=18733198</a></li></ul><p><br></p><h1 id="You-Won’t-Let-Me"><a href="#You-Won’t-Let-Me" class="headerlink" title="You Won’t Let Me"></a>You Won’t Let Me</h1><ul><li>专辑：<a href="orpheus://orpheus/pub/app.html#/m/album/?id=1715512" target="_blank" rel="noopener">Chesapeake</a></li><li>歌手：<a href="orpheus://orpheus/pub/app.html#/m/artist/?id=72720" target="_blank" rel="noopener">Rachael Yamagata</a></li><li>链接：<a href="http://music.163.com/#/song?id=18733192" target="_blank" rel="noopener">http://music.163.com/#/song?id=18733192</a></li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      
      
         &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h1 id=&quot;Dealbreaker&quot;&gt;&lt;a href=&quot;#Dealbreaker&quot; class=&quot;headerlink&quot; title=&quot;Dealbreaker&quot;&gt;&lt;/a&gt;Dealbreaker&lt;/h
      
    
    </summary>
    
      <category term="文艺" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/"/>
    
      <category term="Songs" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/Songs/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="文艺" scheme="https://airpoet.github.io/tags/%E6%96%87%E8%89%BA/"/>
    
      <category term="Songs" scheme="https://airpoet.github.io/tags/Songs/"/>
    
  </entry>
  
  <entry>
    <title>关于hexo的时序图插件 hexo-filter-sequence 的巨坑</title>
    <link href="https://airpoet.github.io/2018/06/10/Tools/Hexo/%E5%85%B3%E4%BA%8Ehexo%E7%9A%84%E6%97%B6%E5%BA%8F%E5%9B%BE%E6%8F%92%E4%BB%B6-hexo-filter-sequence-%E7%9A%84%E5%B7%A8%E5%9D%91/"/>
    <id>https://airpoet.github.io/2018/06/10/Tools/Hexo/关于hexo的时序图插件-hexo-filter-sequence-的巨坑/</id>
    <published>2018-06-10T13:13:02.149Z</published>
    <updated>2018-06-10T13:19:32.909Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在写代码的过程中，发现时序图还是挺管用的，简洁明了，于是想用一用。</p><p>结果发到站上，不显示。</p><p>在网上查了下，发现是hexo所使用的Markdown渲染语法不支持。</p><p>这里吐槽下，这里渲染的确实烂，作者为啥不改改..</p><p><br></p><p>于是开始找解决方案，发现大多数都推荐了一个叫<code>hexo-filter-sequence</code>的插件，故安装之。</p><p>结果死活还是不行。</p><p>装了其它的几个flow图，却可以显示。</p><p><strong>当flow图和sequence图同时存在的时候，sequence可以显示，但是sequence单独存在时，死活不显示。</strong></p><p>难不成有啥依赖？必须先使用flow才能使用sequence？逻辑上不应该啊！</p><p>但是事实却是这样！</p><p><br></p><p>网上有一篇大神说要改源码才行，开始没注意，觉得应该不会吧，这么多人使用，源码还会有问题？？</p><p><strong>仔细一看源码，妈的！！心里千万匹草泥马呼啸而过！</strong></p><p><strong>把初始化 sequence，写成了初始化 flow！！！</strong></p><p><strong>把 flow 改成 sequence， 再把 js CDN源换成国内的！</strong></p><p><strong>可以了！！</strong></p><p>再仔细一看，发现最后一次更新是在1年前！</p><p>坑爹的作者，浪费了我至少3-5个小时！！</p><p><br></p><hr><h2 id="下面为部分摘抄"><a href="#下面为部分摘抄" class="headerlink" title="下面为部分摘抄"></a>下面为部分摘抄</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p><a href="https://github.com/bubkoo/hexo-filter-sequence" target="_blank" rel="noopener">hexo-filter-sequence</a> 插件:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install --save hexo-filter-sequence</span><br></pre></td></tr></table></figure><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>站点配置文件 <code>_config.yml</code> 中增加如下配置:</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sequence:</span><br><span class="line">  webfont: https:<span class="comment">//cdn.bootcss.com/webfont/1.6.28/webfontloader.js</span></span><br><span class="line">  raphael: https:<span class="comment">//cdn.bootcss.com/raphael/2.2.7/raphael.min.js</span></span><br><span class="line">  underscore: https:<span class="comment">//cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js</span></span><br><span class="line">  sequence: https:<span class="comment">//cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js</span></span><br><span class="line">  css: # optional, the url for css, such as hand drawn theme </span><br><span class="line">  options: </span><br><span class="line">    theme: simple</span><br><span class="line">    css_class:</span><br></pre></td></tr></table></figure><h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><p>源码修改后才能正常使用，进入插件目录作如下修改：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// index.js</span></span><br><span class="line"><span class="keyword">var</span> assign = <span class="built_in">require</span>(<span class="string">'deep-assign'</span>);</span><br><span class="line"><span class="keyword">var</span> renderer = <span class="built_in">require</span>(<span class="string">'./lib/renderer'</span>);</span><br><span class="line"></span><br><span class="line">hexo.config.sequence = assign(&#123;</span><br><span class="line">  webfont: <span class="string">'https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js'</span>,</span><br><span class="line">  raphael: <span class="string">'https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js'</span>,</span><br><span class="line">  underscore: <span class="string">'https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js'</span>,</span><br><span class="line">  sequence: <span class="string">'https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js'</span>,</span><br><span class="line">  css: <span class="string">''</span>,</span><br><span class="line">  options: &#123;</span><br><span class="line">    theme: <span class="string">'simple'</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;, hexo.config.sequence);</span><br><span class="line"></span><br><span class="line">hexo.extend.filter.register(<span class="string">'before_post_render'</span>, renderer.render, <span class="number">9</span>);</span><br></pre></td></tr></table></figure><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// lib/renderer.js, 25 行</span></span><br><span class="line"><span class="keyword">if</span> (sequences.length) &#123;</span><br><span class="line">      <span class="keyword">var</span> config = <span class="keyword">this</span>.config.sequence;</span><br><span class="line">      <span class="comment">// resources</span></span><br><span class="line">      data.content += <span class="string">'&lt;script src="'</span> + config.webfont + <span class="string">'"&gt;&lt;/script&gt;'</span>;</span><br><span class="line">      data.content += <span class="string">'&lt;script src="'</span> + config.raphael + <span class="string">'"&gt;&lt;/script&gt;'</span>;</span><br><span class="line">      data.content += <span class="string">'&lt;script src="'</span> + config.underscore + <span class="string">'"&gt;&lt;/script&gt;'</span>;</span><br><span class="line">      data.content += <span class="string">'&lt;script src="'</span> + config.sequence + <span class="string">'"&gt;&lt;/script&gt;'</span>;</span><br><span class="line">      ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>新建代码块，增加如下内容：</p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-10-131910.png" alt="image-20180610211909603"></p><div id="sequence-0"></div><p><a href="http://wewelove.github.io/fcoder/2017/09/06/markdown-sequence/index.html" target="_blank" rel="noopener">详情参考</a></p><p><script src="https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js"></script><script src="https://cdn.bootcss.com/snap.svg/0.5.1/snap.svg-min.js"></script><script src="https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js"></script><script src="https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js"></script><script src="https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js"></script><textarea id="sequence-0-code" style="display: none">Alice->Bob: Hello Bob, how are you?Note right of Bob: Bob thinksBob-->Alice: I am good thanks!</textarea><textarea id="sequence-0-options" style="display: none">{"theme":"simple"}</textarea><script>  var code = document.getElementById("sequence-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value));  var diagram = Diagram.parse(code);  diagram.drawSVG("sequence-0", options);</script></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在写代码的过程中，发现时序图还是挺管用的，简洁明了，于是想用一用。&lt;/p&gt;
&lt;p&gt;结果发到站上，不显示。&lt;/p&gt;
&lt;p&gt;在网上查了下，发现是
      
    
    </summary>
    
      <category term="工具" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="hexo" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/hexo/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="工具" scheme="https://airpoet.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="hexo" scheme="https://airpoet.github.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce-分组浅探</title>
    <link href="https://airpoet.github.io/2018/06/10/Hadoop/Study/2-MapReduce/MapReduce-%E5%88%86%E7%BB%84%E6%B5%85%E6%8E%A2/"/>
    <id>https://airpoet.github.io/2018/06/10/Hadoop/Study/2-MapReduce/MapReduce-分组浅探/</id>
    <published>2018-06-10T08:49:24.337Z</published>
    <updated>2018-06-11T11:46:23.533Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>最近学分组的时候，一直弄不明白，总感觉一头雾水，通过近3个小时的断点调试和log输出。 大概了解了一些表象，先记录。</p><p>案例是这个</p><blockquote><p>求出每门课程参考学生成绩最高平均分的学生的信息：</p><p>课程，姓名和平均分，详细见<a href="https://airpoet.github.io/2018/06/09/Hadoop/Study/MapReduce%E7%AC%94%E8%AE%B0-%E7%BB%83%E4%B9%A0/"><sup>MapReduce笔记-练习第二题第3小题</sup></a></p><p>数据格式是这样的：</p><blockquote><p>第一个是课程名称，总共四个课程，computer，math，english，algorithm，</p><p>第二个是学生姓名，后面是每次考试的分数</p><p><em>math,huangxiaoming,85,75,85,99,66,88,75,91</em></p><p><em>english,huanglei,85,75,85,99,66,88,75,91</em></p><p>… </p></blockquote></blockquote><p><br></p><p><br></p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><div id="sequence-0"></div><h4 id="执行流程结论"><a href="#执行流程结论" class="headerlink" title="执行流程结论"></a><strong>执行流程结论</strong></h4><ul><li>map每读一行就 write 到 context 一次，按照指定的<code>key</code>进行分发</li><li><p>map 把所有的数据都读完了之后，大概执行到<code>67%</code>的时候，开始进入 <code>CustomBean</code>，执行<code>CustomBean</code>的<code>compareTo()</code>方法，会按照自己写的规则一条一条数据比较</p></li><li><p>上述都比较完毕之后，map阶段就结束了，此时来到了 reduce阶段，但是是到了<code>67%</code>了</p></li><li>到了reduce阶段，直接进入了<strong>MyGroup</strong>中自定义的<strong>compare</strong>方法。</li><li>MyGroup的<code>compare()</code>方法，如果返回<strong>非0</strong>， 就会进入 reduce 方法<strong>写出到context</strong></li></ul><h4 id="MyGroup进入Reduce的条件是"><a href="#MyGroup进入Reduce的条件是" class="headerlink" title="MyGroup进入Reduce的条件是"></a><strong>MyGroup进入Reduce的条件是</strong></h4><ul><li>MyReduce中，如果compare的结果不等于0，也就是比较的2者不相同， 此时就进入Reduce， 写出到上下文</li><li>如果相同，会一直往下读，直到读到不同的， 此时写出读到上下文。</li><li>因为MyGroup会在Reduce阶段执行，而<code>CustomBean</code>中的<code>compareTo()</code>是在map阶段执行，所以需要在<code>CustomBean</code>中就把组排好序，此时<strong>分组功能</strong>才能正常运作</li></ul><h4 id="指定分组类MyGroup和不指定的区别"><a href="#指定分组类MyGroup和不指定的区别" class="headerlink" title="指定分组类MyGroup和不指定的区别"></a>指定分组类MyGroup和不指定的区别</h4><p><em>指定与不指定是指：在Driver类中，是否加上<code>job.setGroupingComparatorClass(MyGrouper.class);</code>这一句。</em></p><ul><li><strong>指定分组类</strong>：<ul><li>会按照分组类中，自定义的<code>compare()</code>方法比较，相同的为一组，分完一组就进入一次reduce方法</li></ul></li><li><strong>不指定分组类：（目前存疑）</strong><ul><li>是否是按照key进行分组</li><li>如果是自定义类为key，是否是按照此key中值相同的分为一组</li><li>如果是hadoop内置类，是否是按照此类的值分组（Text-String的值，IntWritable-int值等..）</li><li><strong>依然是走得以上这套分组逻辑，一组的数据读完才进入到Reduce阶段做归并</strong></li></ul></li></ul><p><br></p><p><br></p><h3 id="Log信息"><a href="#Log信息" class="headerlink" title="Log信息"></a>Log信息</h3><h5 id="CustomBean中没有进行分组-组内排序的log"><a href="#CustomBean中没有进行分组-组内排序的log" class="headerlink" title="CustomBean中没有进行分组, 组内排序的log"></a><code>CustomBean</code>中没有进行<code>分组</code>, <code>组内排序</code>的log</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ==================MyGroup中compare()方法=======================</span></span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// ======================reduce中的执行log==========================</span></span><br><span class="line"></span><br><span class="line">==================第<span class="number">1</span>次进入reduce</span><br><span class="line">computerhuangjiaju<span class="number">83.2</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerliutao<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">2</span>次进入reduce</span><br><span class="line">mathhuangxiaoming<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">3</span>次进入reduce</span><br><span class="line">englishhuanglei<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">4</span>次进入reduce</span><br><span class="line">mathhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">5</span>次进入reduce</span><br><span class="line">algorithmhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmliutao<span class="number">82.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">6</span>次进入reduce</span><br><span class="line">computerhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">7</span>次进入reduce</span><br><span class="line">englishliuyifei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">8</span>次进入reduce</span><br><span class="line">algorithmhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">9</span>次进入reduce</span><br><span class="line">mathhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">10</span>次进入reduce</span><br><span class="line">algorithmhuangzitao<span class="number">72.75</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">11</span>次进入reduce</span><br><span class="line">mathliujialing<span class="number">72.75</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">12</span>次进入reduce</span><br><span class="line">computerhuangzitao<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">13</span>次进入reduce</span><br><span class="line">englishhuangxiaoming<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">14</span>次进入reduce</span><br><span class="line">mathwangbaoqiang<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">15</span>次进入reduce</span><br><span class="line">computerhuangxiaoming<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">16</span>次进入reduce</span><br><span class="line">mathxuzheng<span class="number">69.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">17</span>次进入reduce</span><br><span class="line">englishzhaobenshan<span class="number">69.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">18</span>次进入reduce</span><br><span class="line">computerhuangbo<span class="number">65.25</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerxuzheng<span class="number">65.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">19</span>次进入reduce</span><br><span class="line">englishzhouqi<span class="number">64.18181818181819</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">20</span>次进入reduce</span><br><span class="line">computerliujialing<span class="number">64.11111111111111</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">21</span>次进入reduce</span><br><span class="line">algorithmliuyifei<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">22</span>次进入reduce</span><br><span class="line">englishliujialing<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">23</span>次进入reduce</span><br><span class="line">computerliuyifei<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">24</span>次进入reduce</span><br><span class="line">englishliuyifei<span class="number">59.57142857142857</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">25</span>次进入reduce</span><br><span class="line">mathliutao<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">26</span>次进入reduce</span><br><span class="line">algorithmhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">27</span>次进入reduce</span><br><span class="line">computerhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">==================第<span class="number">28</span>次进入reduce</span><br><span class="line">englishhuangbo<span class="number">55.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br></pre></td></tr></table></figure><h5 id="CustomBean中做了分组-amp-组内排序-的"><a href="#CustomBean中做了分组-amp-组内排序-的" class="headerlink" title="CustomBean中做了分组&amp;组内排序 的"></a><code>CustomBean</code>中做了<code>分组&amp;组内排序</code> 的</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// **Bean中相同的组进行分数降序， 组进行字典排序，此时MyGroup中执行的**</span></span><br><span class="line"></span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---algorithm</span><br><span class="line">algorithm---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---computer</span><br><span class="line">computer---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---english</span><br><span class="line">english---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line">math---MyGroup中比较---math</span><br><span class="line"></span><br><span class="line"><span class="comment">// ======================reduce中执行==========================</span></span><br><span class="line"></span><br><span class="line">==================第<span class="number">1</span>次进入reduce</span><br><span class="line">algorithmhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmliutao<span class="number">82.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmhuangzitao<span class="number">72.75</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmliuyifei<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">algorithmhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">===================离开reduce</span><br><span class="line">==================第<span class="number">2</span>次进入reduce</span><br><span class="line">computerhuangjiaju<span class="number">83.2</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerliutao<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerhuangzitao<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerhuangxiaoming<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerhuangbo<span class="number">65.25</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerxuzheng<span class="number">65.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerliujialing<span class="number">64.11111111111111</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerliuyifei<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">computerhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">===================离开reduce</span><br><span class="line">==================第<span class="number">3</span>次进入reduce</span><br><span class="line">englishhuanglei<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishliuyifei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishhuangxiaoming<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishzhaobenshan<span class="number">69.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishzhouqi<span class="number">64.18181818181819</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishliujialing<span class="number">62.142857142857146</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishliuyifei<span class="number">59.57142857142857</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishhuangdatou<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">englishhuangbo<span class="number">55.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">===================离开reduce</span><br><span class="line">==================第<span class="number">4</span>次进入reduce</span><br><span class="line">mathhuangxiaoming<span class="number">83.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathliujialing<span class="number">72.75</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathwangbaoqiang<span class="number">72.42857142857143</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathxuzheng<span class="number">69.28571428571429</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">mathliutao<span class="number">56.0</span></span><br><span class="line">---------in <span class="keyword">for</span> write------</span><br><span class="line">===================离开reduce</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//  如果只取一个每次values的第一个的话 </span></span><br><span class="line"></span><br><span class="line">algorithmhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">==================第<span class="number">1</span>次进入reduce</span><br><span class="line">computerhuangjiaju<span class="number">83.2</span></span><br><span class="line">==================第<span class="number">2</span>次进入reduce</span><br><span class="line">englishhuanglei<span class="number">83.0</span></span><br><span class="line">==================第<span class="number">3</span>次进入reduce</span><br><span class="line">mathhuangxiaoming<span class="number">83.0</span></span><br><span class="line">==================第<span class="number">4</span>次进入reduce</span><br></pre></td></tr></table></figure><p><br></p><p><br></p><h3 id="其它疑点"><a href="#其它疑点" class="headerlink" title="其它疑点"></a>其它疑点</h3><ul><li>通过log可以看出来， MyGroup是在读到了不同的数据，才会进入到 reduce 类中写出；</li><li>但是 通过 <strong>断点调试</strong>时， 现象是，第一次读到了2个相同的，就去reduce去写出了；</li><li>后面再读的就不做写出操作，直到下一次读到2个相同的，再在reduce中写出。</li></ul><p><script src="https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js"></script><script src="https://cdn.bootcss.com/snap.svg/0.5.1/snap.svg-min.js"></script><script src="https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js"></script><script src="https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js"></script><script src="https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js"></script><textarea id="sequence-0-code" style="display: none">title: 执行流程时序图 Mapper(map)->ScoreBean: k:ScoreBean(courseName,avgScore)Mapper(map)->ScoreBean: v:Text(stuName)Mapper(ScoreBean)->Reducer(MyGroup): course按字典升序Mapper(ScoreBean)->Reducer(MyGroup): course内成绩降序Reducer(MyGroup)->Reducer(reduce): 根据自定义的分组规则按组输出Reducer(MyGroup)->Reducer(reduce): 一组只调用reduce一次Reducer(reduce)-->Reducer(reduce):</textarea><textarea id="sequence-0-options" style="display: none">{"theme":"simple"}</textarea><script>  var code = document.getElementById("sequence-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value));  var diagram = Diagram.parse(code);  diagram.drawSVG("sequence-0", options);</script></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;最近学分组的时候，一直弄不明白，总感觉一头雾水，通过近3个小时的断点调试和log输出。 大概了解了一些表象，先记录。&lt;/p&gt;
&lt;p&gt;案例是这
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>▍开始</title>
    <link href="https://airpoet.github.io/2018/06/10/Poetry/%E5%BC%80%E5%A7%8B/"/>
    <id>https://airpoet.github.io/2018/06/10/Poetry/开始/</id>
    <published>2018-06-09T17:03:10.001Z</published>
    <updated>2018-06-09T17:19:40.887Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-09-601528563678_.pic.jpg" alt=""></p><p>月亮落下一两片羽毛在田野上。</p><p>黑暗中的麦子聆听着。</p><p>快静下来。</p><p>快。</p><p>就在那儿，月亮的孩子们正试着</p><p>挥动翅膀。</p><p>在两棵树之间，身材修长的女子抬起面庞，</p><p>美丽的剪影。接着，她步入空中，接着，</p><p>她完全消失在空中。</p><p>我独自站在一棵接骨木旁，不敢呼吸，</p><p>也不敢动。</p><p>我聆听着。</p><p>麦子向后靠着自己的黑暗，</p><p>而我靠着我的。</p><p><br></p><p><strong>作者 / [美国] 詹姆斯·赖特</strong></p><p>翻译 / 张文武</p><hr><h3 id="▍Beginning"><a href="#▍Beginning" class="headerlink" title="▍Beginning"></a><strong>▍Beginning</strong></h3><p><br></p><p>The moon drops one or two feathers into the fields.</p><p>The dark wheat listens.</p><p>Be still.</p><p>Now.</p><p>There they are, the moon’s young, trying</p><p>Their wings.</p><p>Between trees, a slender woman lifts up the lovely shadow</p><p>Of her face, and now she steps into the air, now she is gone</p><p>Wholly, into the air.</p><p>I stand alone by an elder tree, I do not dare breathe</p><p>Or move.</p><p>I listen.</p><p>The wheat leans back toward its own darkness,</p><p>And I lean toward mine.</p><p><br></p><p><strong>Author / James Wright</strong></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-09-601528563678_.pic.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;月
      
    
    </summary>
    
      <category term="文艺" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/"/>
    
      <category term="诗歌" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/%E8%AF%97%E6%AD%8C/"/>
    
    
      <category term="诗歌" scheme="https://airpoet.github.io/tags/%E8%AF%97%E6%AD%8C/"/>
    
  </entry>
  
  <entry>
    <title>Markdown高阶语法</title>
    <link href="https://airpoet.github.io/2018/06/10/Tools/Markdown/Markdown%E9%AB%98%E9%98%B6%E8%AF%AD%E6%B3%95/"/>
    <id>https://airpoet.github.io/2018/06/10/Tools/Markdown/Markdown高阶语法/</id>
    <published>2018-06-09T16:49:50.407Z</published>
    <updated>2018-06-10T13:21:24.745Z</updated>
    
    <content type="html"><![CDATA[<h3 id="时序图的写法"><a href="#时序图的写法" class="headerlink" title="时序图的写法"></a>时序图的写法</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-09-165900.png" alt="image-20180610005859980"></p><div id="sequence-0"></div><h3 id="流程图的写法"><a href="#流程图的写法" class="headerlink" title="流程图的写法"></a>流程图的写法</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-09-165930.png" alt="image-20180610005930019"></p><div id="flowchart-0" class="flow-chart"></div><h3 id="类图的写法"><a href="#类图的写法" class="headerlink" title="类图的写法"></a>类图的写法</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-09-165948.png" alt="image-20180610005948130"></p><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLl2jz4qiA4Wjud98pKi12WC0"></p><p><script src="https://cdnjs.cloudflare.com/ajax/libs/raphael/2.2.7/raphael.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/flowchart/1.6.5/flowchart.min.js"></script><textarea id="flowchart-0-code" style="display: none">st=>start: Start|past:>http://www.google.com[blank]e=>end: End:>http://www.google.comop1=>operation: My Operation|pastop2=>operation: Stuff|currentsub1=>subroutine: My Subroutine|invalidcond=>condition: Yesor No?|approved:>http://www.google.comc2=>condition: Good idea|rejectedio=>inputoutput: catch something...|requestst->op1(right)->condcond(yes, right)->c2cond(no)->sub1(left)->op1c2(yes)->io->ec2(no)->op2->e</textarea><textarea id="flowchart-0-options" style="display: none">{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("flowchart-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value));  var diagram = flowchart.parse(code);  diagram.drawSVG("flowchart-0", options);</script><script src="https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js"></script><script src="https://cdn.bootcss.com/snap.svg/0.5.1/snap.svg-min.js"></script><script src="https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js"></script><script src="https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js"></script><script src="https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js"></script><textarea id="sequence-0-code" style="display: none">mapper->Bob: Hello Bob, how are you?Note right of Bob: Bob thinksBob-->reducer: I am good thanks!ad u?reducer->out: I'm fine tooout->me: ok, you winme-->Bob: nono, not</textarea><textarea id="sequence-0-options" style="display: none">{"theme":"simple"}</textarea><script>  var code = document.getElementById("sequence-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value));  var diagram = Diagram.parse(code);  diagram.drawSVG("sequence-0", options);</script></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;时序图的写法&quot;&gt;&lt;a href=&quot;#时序图的写法&quot; class=&quot;headerlink&quot; title=&quot;时序图的写法&quot;&gt;&lt;/a&gt;时序图的写法&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/study/2018-
      
    
    </summary>
    
      <category term="工具" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="Markdown" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/Markdown/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="工具" scheme="https://airpoet.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="Markdown" scheme="https://airpoet.github.io/tags/Markdown/"/>
    
  </entry>
  
  <entry>
    <title>About Sublime Text3</title>
    <link href="https://airpoet.github.io/2018/06/09/Tools/Sublime/%E5%AE%89%E8%A3%85%E4%B8%BB%E9%A2%98/"/>
    <id>https://airpoet.github.io/2018/06/09/Tools/Sublime/安装主题/</id>
    <published>2018-06-09T04:43:17.189Z</published>
    <updated>2018-06-10T01:09:27.890Z</updated>
    
    <content type="html"><![CDATA[<h3 id="主题"><a href="#主题" class="headerlink" title="主题"></a>主题</h3><p><a href="https://scotch.io/bar-talk/best-sublime-text-3-themes-of-2015-and-2016" target="_blank" rel="noopener">详情参见这个网站</a></p><h3 id="详细操作"><a href="#详细操作" class="headerlink" title="详细操作"></a>详细操作</h3><p><a href="http://zh.lucida.me/blog/sublime-text-complete-guide/" target="_blank" rel="noopener">见此站</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;主题&quot;&gt;&lt;a href=&quot;#主题&quot; class=&quot;headerlink&quot; title=&quot;主题&quot;&gt;&lt;/a&gt;主题&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://scotch.io/bar-talk/best-sublime-text-3-themes-of-2015
      
    
    </summary>
    
      <category term="工具" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="Sublime" scheme="https://airpoet.github.io/categories/%E5%B7%A5%E5%85%B7/Sublime/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="工具" scheme="https://airpoet.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="Sublime" scheme="https://airpoet.github.io/tags/Sublime/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce笔记-Bug汇总</title>
    <link href="https://airpoet.github.io/2018/06/09/Hadoop/Study/2-MapReduce/MapReduce-Bug%E6%B1%87%E6%80%BB/"/>
    <id>https://airpoet.github.io/2018/06/09/Hadoop/Study/2-MapReduce/MapReduce-Bug汇总/</id>
    <published>2018-06-09T03:11:48.470Z</published>
    <updated>2018-06-11T11:46:16.724Z</updated>
    
    <content type="html"><![CDATA[<h4 id="1、reduce-输出路径必须是新创建的。不能已经存在"><a href="#1、reduce-输出路径必须是新创建的。不能已经存在" class="headerlink" title="1、reduce 输出路径必须是新创建的。不能已经存在"></a>1、reduce 输出路径必须是新创建的。不能已经存在</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread <span class="string">"main"</span> org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs:<span class="comment">//cs1:9000/flowout01 already exists</span></span><br></pre></td></tr></table></figure><h4 id="2、在初始化-job-的时候，-没有传-conf-，-导致后面一直找不到文件，-因为不知道到哪里去找"><a href="#2、在初始化-job-的时候，-没有传-conf-，-导致后面一直找不到文件，-因为不知道到哪里去找" class="headerlink" title="2、在初始化 job 的时候， 没有传 conf ， 导致后面一直找不到文件， 因为不知道到哪里去找"></a>2、在初始化 job 的时候， 没有传 <code>conf</code> ， 导致后面一直找不到文件， 因为不知道到哪里去找</h4><h4 id="3、Text导包倒错-导的时候要注意"><a href="#3、Text导包倒错-导的时候要注意" class="headerlink" title="3、Text导包倒错, 导的时候要注意"></a>3、Text导包倒错, 导的时候要注意</h4><p><strong>应该是这个 <code>import org.apache.hadoop.io.Text;</code></strong></p><h4 id="4、进行字符串拼接的时候，把-StringBuilder-写到了-reduce-方法外，-这样导致-sb-会越来越多，当然，也可以每次拼接完了清空"><a href="#4、进行字符串拼接的时候，把-StringBuilder-写到了-reduce-方法外，-这样导致-sb-会越来越多，当然，也可以每次拼接完了清空" class="headerlink" title="4、进行字符串拼接的时候，把 StringBuilder 写到了 reduce 方法外， 这样导致 sb 会越来越多，当然，也可以每次拼接完了清空"></a>4、进行字符串拼接的时候，把 <strong>StringBuilder 写到了 reduce 方法外</strong>， 这样导致 sb 会越来越多，当然，也可以每次拼接完了清空</h4><p>类似于这样</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">AF,I,O,K,G,D,C,H,B</span><br><span class="line">BF,I,O,K,G,D,C,H,B,E,J,F,A</span><br><span class="line">CF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F</span><br><span class="line">DF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L</span><br><span class="line">EF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H</span><br><span class="line">FF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G</span><br><span class="line">GF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M</span><br><span class="line">HF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O</span><br><span class="line">IF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C</span><br><span class="line">JF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O</span><br><span class="line">KF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,B</span><br><span class="line">LF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,B,D,E</span><br><span class="line">MF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,B,D,E,E,F</span><br><span class="line">OF,I,O,K,G,D,C,H,B,E,J,F,A,B,E,K,A,H,G,F,H,C,G,F,E,A,K,L,A,B,L,G,M,F,D,H,C,M,L,A,D,G,M,O,O,C,O,O,B,D,E,E,F,A,H,I,J,F</span><br></pre></td></tr></table></figure><h4 id="4、mapreduce执行错误Mapper-错误"><a href="#4、mapreduce执行错误Mapper-错误" class="headerlink" title="4、mapreduce执行错误Mapper.\错误"></a>4、mapreduce执行错误Mapper.\<init>错误</init></h4><ul><li>Mapper &amp; Reducer 写成内部类的时候，有没有加上 <code>static</code></li><li>Bean类有没有无参构造</li></ul><h4 id="5、排序过程中，自定义了排序类，bean类的-compareTo-方法，只写了按照分数大小排序。"><a href="#5、排序过程中，自定义了排序类，bean类的-compareTo-方法，只写了按照分数大小排序。" class="headerlink" title="5、排序过程中，自定义了排序类，bean类的 compareTo()方法，只写了按照分数大小排序。"></a>5、排序过程中，自定义了排序类，bean类的 <code>compareTo()</code>方法，只写了按照分数大小排序。</h4><p><u>会出现如下错误： 课程并没有分组</u></p><p>没有在相同的一组课程中比较分数， 而是比较的所有的分数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">computerhuangjiaju<span class="number">83.2</span></span><br><span class="line">mathhuangxiaoming<span class="number">83.0</span></span><br><span class="line">englishhuanglei<span class="number">83.0</span></span><br><span class="line">mathhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">algorithmhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">computerhuanglei<span class="number">74.42857142857143</span></span><br><span class="line">englishliuyifei<span class="number">74.42857142857143</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><p><strong>此时应该在Bean对象中做如下事情</strong></p><ul><li>相同课程的按照分数降序排序</li><li>课程名按照自然（升序）排序</li><li><strong>换言之，就是CustomBean 对象要输出的数据是 <code>组名升序排序，组内按成绩降序排序</code></strong></li><li><a href="https://airpoet.github.io/2018/06/10/Hadoop/Study/MapReduce-%E5%88%86%E7%BB%84%E6%B5%85%E6%8E%A2/#more">具体分析参阅</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;1、reduce-输出路径必须是新创建的。不能已经存在&quot;&gt;&lt;a href=&quot;#1、reduce-输出路径必须是新创建的。不能已经存在&quot; class=&quot;headerlink&quot; title=&quot;1、reduce 输出路径必须是新创建的。不能已经存在&quot;&gt;&lt;/a&gt;1、red
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce笔记-练习</title>
    <link href="https://airpoet.github.io/2018/06/09/Hadoop/Study/2-MapReduce/MapReduce-%E7%BB%83%E4%B9%A0/"/>
    <id>https://airpoet.github.io/2018/06/09/Hadoop/Study/2-MapReduce/MapReduce-练习/</id>
    <published>2018-06-09T03:11:16.234Z</published>
    <updated>2018-06-11T14:15:19.807Z</updated>
    
    <content type="html"><![CDATA[<h2 id="求微博共同粉丝"><a href="#求微博共同粉丝" class="headerlink" title="求微博共同粉丝"></a>求微博共同粉丝</h2><h3 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h3><p><strong>涉及知识点： 多 Job 串联</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">A:B,C,D,F,E,O</span><br><span class="line">B:A,C,E,K</span><br><span class="line">C:F,A,D,I</span><br><span class="line">D:A,E,F,L</span><br><span class="line">E:B,C,D,M,L</span><br><span class="line">F:A,B,C,D,E,O,M</span><br><span class="line">G:A,C,D,E,F</span><br><span class="line">H:A,C,D,E,O</span><br><span class="line">I:A,O</span><br><span class="line">J:B,O</span><br><span class="line">K:A,C,D</span><br><span class="line">L:D,E,F</span><br><span class="line">M:E,F,G</span><br><span class="line">O:A,H,I,J,K</span><br></pre></td></tr></table></figure><blockquote><p>以上是数据：<br>A:B,C,D,F,E,O<br>表示：A用户 关注B,C,D,E,F,O</p><blockquote><p>求所有两两用户之间的共同关注对象</p></blockquote></blockquote><h3 id="答案："><a href="#答案：" class="headerlink" title="答案："></a>答案：</h3><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLd3EpytDptDBp2jsIStDvt98pKi1IW80"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._01_多Job串联;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CommonFansDemo</span> </span>&#123;</span><br><span class="line"><span class="meta">@SuppressWarnings</span>(<span class="string">"deprecation"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// Job 逻辑</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 HDFS 相关的参数</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// 新建一个 job1</span></span><br><span class="line">Job job1 = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置 Jar 包所在路径</span></span><br><span class="line">job1.setJarByClass(CommonFansDemo.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 mapper 类和 reducer 类</span></span><br><span class="line">job1.setMapperClass(MyMapper_Step1.class);</span><br><span class="line">job1.setReducerClass(MyReducer_Step1.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 maptask 的输出类型</span></span><br><span class="line">job1.setMapOutputKeyClass(Text.class);</span><br><span class="line">job1.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定最终的输出类型(reduce存在时,就是指 ReduceTask 的输出类型)</span></span><br><span class="line">job1.setOutputKeyClass(Text.class);</span><br><span class="line">job1.setOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定该 MapReduce 程序数据的输入输出路径</span></span><br><span class="line">FileInputFormat.setInputPaths(job1, <span class="keyword">new</span> Path(<span class="string">"/in/commonfriend"</span>));</span><br><span class="line">FileOutputFormat.setOutputPath(job1, <span class="keyword">new</span> Path(<span class="string">"/out/job1"</span>));</span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// 新建一个 job2</span></span><br><span class="line">Job job2 = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置 Jar 包所在路径</span></span><br><span class="line">job2.setJarByClass(CommonFansDemo.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 mapper 类和 reducer 类</span></span><br><span class="line">job2.setMapperClass(MyMapper_Step2.class);</span><br><span class="line">job2.setReducerClass(MyReducer_Step2.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 maptask 的输出类型</span></span><br><span class="line">job2.setMapOutputKeyClass(Text.class);</span><br><span class="line">job2.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定最终的输出类型(reduce存在时,就是指 ReduceTask 的输出类型)</span></span><br><span class="line">job2.setOutputKeyClass(Text.class);</span><br><span class="line">job2.setOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定该 MapReduce 程序数据的输入输出路径</span></span><br><span class="line">FileInputFormat.setInputPaths(job2, <span class="keyword">new</span> Path(<span class="string">"/out/job1"</span>));</span><br><span class="line">FileOutputFormat.setOutputPath(job2, <span class="keyword">new</span> Path(<span class="string">"/out/job2"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将多个 job 当做一个组中的 job 提交, 参数名是组名</span></span><br><span class="line"><span class="comment"> * 注意: JobControl 是实现了 Runnable 接口的 </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">JobControl jControl = <span class="keyword">new</span> JobControl(<span class="string">"common_friend"</span>);</span><br><span class="line"><span class="comment">// 将原生的 job携带配置 转换为可控的 job</span></span><br><span class="line">ControlledJob aJob = <span class="keyword">new</span> ControlledJob(job1.getConfiguration());</span><br><span class="line">ControlledJob bJob = <span class="keyword">new</span> ControlledJob(job2.getConfiguration());</span><br><span class="line"><span class="comment">// 添加依赖关系</span></span><br><span class="line">bJob.addDependingJob(aJob);</span><br><span class="line"><span class="comment">// 添加 job 到组中</span></span><br><span class="line">jControl.addJob(aJob);</span><br><span class="line">jControl.addJob(bJob);</span><br><span class="line"><span class="comment">// 启动一个线程</span></span><br><span class="line">Thread jobThread = <span class="keyword">new</span> Thread(jControl);</span><br><span class="line">jobThread.start();</span><br><span class="line"><span class="keyword">while</span> (!jControl.allFinished()) &#123;</span><br><span class="line">Thread.sleep(<span class="number">500</span>);</span><br><span class="line">&#125;</span><br><span class="line">jobThread.stop();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper_Step1</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">String[] user_attentions;</span><br><span class="line">String[] attentions;</span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value,</span></span></span><br><span class="line"><span class="function"><span class="params">Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">user_attentions = value.toString().split(<span class="string">":"</span>);</span><br><span class="line">attentions = user_attentions[<span class="number">1</span>].trim().split(<span class="string">","</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (String att : attentions) &#123;</span><br><span class="line">k.set(att);</span><br><span class="line">v.set(user_attentions[<span class="number">0</span>].trim());</span><br><span class="line">context.write(k, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 将两两粉丝(普通用户)拼接起来, 格式a-f:c =&gt; a,b 都共同关注了 c</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> *  AF,I,O,K,G,D,C,H,B</span></span><br><span class="line"><span class="comment">BE,J,F,A</span></span><br><span class="line"><span class="comment">CB,E,K,A,H,G,F</span></span><br><span class="line"><span class="comment">DH,C,G,F,E,A,K,L</span></span><br><span class="line"><span class="comment">EA,B,L,G,M,F,D,H</span></span><br><span class="line"><span class="comment">FC,M,L,A,D,G</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper_Step2</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">String[] attenion_users;</span><br><span class="line">String[] users;</span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value,</span></span></span><br><span class="line"><span class="function"><span class="params">Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">attenion_users = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">users = attenion_users[<span class="number">1</span>].trim().split(<span class="string">","</span>);</span><br><span class="line"><span class="keyword">for</span> (String u1 : users) &#123;</span><br><span class="line"><span class="keyword">for</span> (String u2 : users) &#123;</span><br><span class="line"><span class="keyword">if</span> (u1.compareTo(u2) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">String users = u1 + <span class="string">"-"</span> + u2;</span><br><span class="line">k.set(users);</span><br><span class="line">v.set(attenion_users[<span class="number">0</span>].trim());</span><br><span class="line">context.write(k, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> *需要统计的是, 某人拥有的全部粉丝</span></span><br><span class="line"><span class="comment"> *  key: 传过来的 key</span></span><br><span class="line"><span class="comment"> *  value:  用,分割 </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer_Step1</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Reducer&lt;Text, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注意: 这里 sb 不能写在外面,会不断的拼接</span></span><br><span class="line">StringBuilder sb = <span class="keyword">new</span> StringBuilder();</span><br><span class="line"><span class="keyword">for</span> (Text v : values) &#123;</span><br><span class="line">sb.append(v.toString()).append(<span class="string">","</span>);</span><br><span class="line">&#125;</span><br><span class="line">k.set(key);</span><br><span class="line">v.set(sb.substring(<span class="number">0</span>, sb.length() - <span class="number">1</span>));</span><br><span class="line">context.write(k, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 拿到的数据: a-b c</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer_Step2</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Reducer&lt;Text, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">StringBuilder sb = <span class="keyword">new</span> StringBuilder();</span><br><span class="line"><span class="keyword">for</span> (Text attention : values) &#123;</span><br><span class="line">sb.append(attention.toString()).append(<span class="string">","</span>);</span><br><span class="line">&#125;</span><br><span class="line">k.set(key);</span><br><span class="line">v.set(sb.substring(<span class="number">0</span>, sb.length() - <span class="number">1</span>));</span><br><span class="line">context.write(k, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// job1的输出</span></span><br><span class="line">AF,I,O,K,G,D,C,H,B</span><br><span class="line">BE,J,F,A</span><br><span class="line">CB,E,K,A,H,G,F</span><br><span class="line">DH,C,G,F,E,A,K,L</span><br><span class="line">EA,B,L,G,M,F,D,H</span><br><span class="line">FC,M,L,A,D,G</span><br><span class="line">GM</span><br><span class="line">HO</span><br><span class="line">IO,C</span><br><span class="line">JO</span><br><span class="line">KO,B</span><br><span class="line">LD,E</span><br><span class="line">ME,F</span><br><span class="line">OA,H,I,J,F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// job2的输出</span></span><br><span class="line">A-BE,C</span><br><span class="line">A-CD,F</span><br><span class="line">A-DF,E</span><br><span class="line">A-EC,D,B</span><br><span class="line">A-FO,B,E,D,C</span><br><span class="line">A-GE,F,D,C</span><br><span class="line">A-HO,E,D,C</span><br><span class="line">A-IO</span><br><span class="line">A-JB,O</span><br><span class="line">A-KD,C</span><br><span class="line">A-LD,F,E</span><br><span class="line">A-ME,F</span><br><span class="line">B-CA</span><br><span class="line">B-DE,A</span><br><span class="line">B-EC</span><br><span class="line">B-FA,E,C</span><br><span class="line">B-GC,A,E</span><br><span class="line">B-HA,E,C</span><br><span class="line">B-IA</span><br><span class="line">B-KC,A</span><br><span class="line">B-LE</span><br><span class="line">B-ME</span><br><span class="line">B-OA,K</span><br><span class="line">C-DA,F</span><br><span class="line">C-ED</span><br><span class="line">C-FD,A</span><br><span class="line">C-GF,A,D</span><br><span class="line">C-HD,A</span><br><span class="line">C-IA</span><br><span class="line">C-KA,D</span><br><span class="line">C-LF,D</span><br><span class="line">C-MF</span><br><span class="line">C-OI,A</span><br><span class="line">D-EL</span><br><span class="line">D-FE,A</span><br><span class="line">D-GA,F,E</span><br><span class="line">D-HE,A</span><br><span class="line">D-IA</span><br><span class="line">D-KA</span><br><span class="line">D-LF,E</span><br><span class="line">D-MF,E</span><br><span class="line">D-OA</span><br><span class="line">E-FC,B,M,D</span><br><span class="line">E-GC,D</span><br><span class="line">E-HC,D</span><br><span class="line">E-JB</span><br><span class="line">E-KD,C</span><br><span class="line">E-LD</span><br><span class="line">F-GA,D,C,E</span><br><span class="line">F-HA,E,C,D,O</span><br><span class="line">F-IO,A</span><br><span class="line">F-JO,B</span><br><span class="line">F-KC,A,D</span><br><span class="line">F-LE,D</span><br><span class="line">F-ME</span><br><span class="line">F-OA</span><br><span class="line">G-HA,C,D,E</span><br><span class="line">G-IA</span><br><span class="line">G-KC,A,D</span><br><span class="line">G-LD,E,F</span><br><span class="line">G-MF,E</span><br><span class="line">G-OA</span><br><span class="line">H-IO,A</span><br><span class="line">H-JO</span><br><span class="line">H-KA,D,C</span><br><span class="line">H-LE,D</span><br><span class="line">H-ME</span><br><span class="line">H-OA</span><br><span class="line">I-JO</span><br><span class="line">I-KA</span><br><span class="line">I-OA</span><br><span class="line">K-LD</span><br><span class="line">K-OA</span><br><span class="line">L-MF,E</span><br></pre></td></tr></table></figure><h2 id="求学生成绩"><a href="#求学生成绩" class="headerlink" title="求学生成绩"></a>求学生成绩</h2><h3 id="题目-1"><a href="#题目-1" class="headerlink" title="题目"></a>题目</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">computer,huangxiaoming,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span></span><br><span class="line">computer,xuzheng,<span class="number">54</span>,<span class="number">52</span>,<span class="number">86</span>,<span class="number">91</span>,<span class="number">42</span></span><br><span class="line">computer,huangbo,<span class="number">85</span>,<span class="number">42</span>,<span class="number">96</span>,<span class="number">38</span></span><br><span class="line">english,zhaobenshan,<span class="number">54</span>,<span class="number">52</span>,<span class="number">86</span>,<span class="number">91</span>,<span class="number">42</span>,<span class="number">85</span>,<span class="number">75</span></span><br><span class="line">english,liuyifei,<span class="number">85</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">21</span>,<span class="number">85</span>,<span class="number">96</span>,<span class="number">14</span></span><br><span class="line">algorithm,liuyifei,<span class="number">75</span>,<span class="number">85</span>,<span class="number">62</span>,<span class="number">48</span>,<span class="number">54</span>,<span class="number">96</span>,<span class="number">15</span></span><br><span class="line">computer,huangjiaju,<span class="number">85</span>,<span class="number">75</span>,<span class="number">86</span>,<span class="number">85</span>,<span class="number">85</span></span><br><span class="line">english,liuyifei,<span class="number">76</span>,<span class="number">95</span>,<span class="number">86</span>,<span class="number">74</span>,<span class="number">68</span>,<span class="number">74</span>,<span class="number">48</span></span><br><span class="line">english,huangdatou,<span class="number">48</span>,<span class="number">58</span>,<span class="number">67</span>,<span class="number">86</span>,<span class="number">15</span>,<span class="number">33</span>,<span class="number">85</span></span><br><span class="line">algorithm,huanglei,<span class="number">76</span>,<span class="number">95</span>,<span class="number">86</span>,<span class="number">74</span>,<span class="number">68</span>,<span class="number">74</span>,<span class="number">48</span></span><br><span class="line">algorithm,huangjiaju,<span class="number">85</span>,<span class="number">75</span>,<span class="number">86</span>,<span class="number">85</span>,<span class="number">85</span>,<span class="number">74</span>,<span class="number">86</span></span><br><span class="line">computer,huangdatou,<span class="number">48</span>,<span class="number">58</span>,<span class="number">67</span>,<span class="number">86</span>,<span class="number">15</span>,<span class="number">33</span>,<span class="number">85</span></span><br><span class="line">english,zhouqi,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span>,<span class="number">75</span>,<span class="number">55</span>,<span class="number">47</span>,<span class="number">22</span></span><br><span class="line">english,huangbo,<span class="number">85</span>,<span class="number">42</span>,<span class="number">96</span>,<span class="number">38</span>,<span class="number">55</span>,<span class="number">47</span>,<span class="number">22</span></span><br><span class="line">algorithm,liutao,<span class="number">85</span>,<span class="number">75</span>,<span class="number">85</span>,<span class="number">99</span>,<span class="number">66</span></span><br><span class="line">computer,huangzitao,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span></span><br><span class="line">math,wangbaoqiang,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span></span><br><span class="line">computer,liujialing,<span class="number">85</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">21</span>,<span class="number">85</span>,<span class="number">96</span>,<span class="number">14</span>,<span class="number">74</span>,<span class="number">86</span></span><br><span class="line">computer,liuyifei,<span class="number">75</span>,<span class="number">85</span>,<span class="number">62</span>,<span class="number">48</span>,<span class="number">54</span>,<span class="number">96</span>,<span class="number">15</span></span><br><span class="line">computer,liutao,<span class="number">85</span>,<span class="number">75</span>,<span class="number">85</span>,<span class="number">99</span>,<span class="number">66</span>,<span class="number">88</span>,<span class="number">75</span>,<span class="number">91</span></span><br><span class="line">computer,huanglei,<span class="number">76</span>,<span class="number">95</span>,<span class="number">86</span>,<span class="number">74</span>,<span class="number">68</span>,<span class="number">74</span>,<span class="number">48</span></span><br><span class="line">english,liujialing,<span class="number">75</span>,<span class="number">85</span>,<span class="number">62</span>,<span class="number">48</span>,<span class="number">54</span>,<span class="number">96</span>,<span class="number">15</span></span><br><span class="line">math,huanglei,<span class="number">76</span>,<span class="number">95</span>,<span class="number">86</span>,<span class="number">74</span>,<span class="number">68</span>,<span class="number">74</span>,<span class="number">48</span></span><br><span class="line">math,huangjiaju,<span class="number">85</span>,<span class="number">75</span>,<span class="number">86</span>,<span class="number">85</span>,<span class="number">85</span>,<span class="number">74</span>,<span class="number">86</span></span><br><span class="line">math,liutao,<span class="number">48</span>,<span class="number">58</span>,<span class="number">67</span>,<span class="number">86</span>,<span class="number">15</span>,<span class="number">33</span>,<span class="number">85</span></span><br><span class="line">english,huanglei,<span class="number">85</span>,<span class="number">75</span>,<span class="number">85</span>,<span class="number">99</span>,<span class="number">66</span>,<span class="number">88</span>,<span class="number">75</span>,<span class="number">91</span></span><br><span class="line">math,xuzheng,<span class="number">54</span>,<span class="number">52</span>,<span class="number">86</span>,<span class="number">91</span>,<span class="number">42</span>,<span class="number">85</span>,<span class="number">75</span></span><br><span class="line">math,huangxiaoming,<span class="number">85</span>,<span class="number">75</span>,<span class="number">85</span>,<span class="number">99</span>,<span class="number">66</span>,<span class="number">88</span>,<span class="number">75</span>,<span class="number">91</span></span><br><span class="line">math,liujialing,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span>,<span class="number">75</span></span><br><span class="line">english,huangxiaoming,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span></span><br><span class="line">algorithm,huangdatou,<span class="number">48</span>,<span class="number">58</span>,<span class="number">67</span>,<span class="number">86</span>,<span class="number">15</span>,<span class="number">33</span>,<span class="number">85</span></span><br><span class="line">algorithm,huangzitao,<span class="number">85</span>,<span class="number">86</span>,<span class="number">41</span>,<span class="number">75</span>,<span class="number">93</span>,<span class="number">42</span>,<span class="number">85</span>,<span class="number">75</span></span><br></pre></td></tr></table></figure><blockquote><p>一、数据解释</p><p>数据字段个数不固定：<br>第一个是课程名称，总共四个课程，computer，math，english，algorithm，<br>第二个是学生姓名，后面是每次考试的分数</p><p>二、统计需求：<br>1、统计每门课程的参加考试人数和课程平均分</p><p>2、统计每门课程参考学生的平均分，并且按课程存入不同的结果文件，要求一门课程一个结果文件</p><p>3、求出每门课程参考学生成绩最高平均分的学生的信息：课程，姓名和平均分</p></blockquote><h3 id="答案"><a href="#答案" class="headerlink" title="答案"></a>答案</h3><h4 id="第1小题"><a href="#第1小题" class="headerlink" title="第1小题"></a>第1小题</h4><p><strong>统计每门课程的参考人数和课程平均分</strong></p><p><strong>涉及知识点: 去重， 自定义类</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//  ScoreBean </span></span><br><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._02_分组组件;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.AllArgsConstructor;</span><br><span class="line"><span class="keyword">import</span> lombok.Getter;</span><br><span class="line"><span class="keyword">import</span> lombok.NoArgsConstructor;</span><br><span class="line"><span class="keyword">import</span> lombok.Setter;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Getter</span></span><br><span class="line"><span class="meta">@Setter</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ScoreBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">ScoreBean</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> String courseName;</span><br><span class="line"><span class="keyword">private</span> String stuName; </span><br><span class="line"><span class="keyword">private</span> Double score;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">out.writeUTF(courseName);</span><br><span class="line">out.writeUTF(stuName);</span><br><span class="line">out.writeDouble(score);</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.courseName = in.readUTF();</span><br><span class="line"><span class="keyword">this</span>.stuName = in.readUTF();</span><br><span class="line"><span class="keyword">this</span>.score = in.readDouble();</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 如果是相同课程, 按照分数降序排列的</span></span><br><span class="line"><span class="comment"> * 如果是不同课程, 按照课程名称升序排列</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(ScoreBean o)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 测试一下只写按分数降序排序</span></span><br><span class="line"><span class="comment">//return o.getScore().compareTo(this.getScore());</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*// 首先分组(只在相同的组内进行比较)</span></span><br><span class="line"><span class="comment">int nameRes = this.getCourseName().compareTo(o.getCourseName());</span></span><br><span class="line"><span class="comment">if (nameRes == 0) &#123;</span></span><br><span class="line"><span class="comment">// 课程相同的时候才进行降序排序</span></span><br><span class="line"><span class="comment">int scoreRes = </span></span><br><span class="line"><span class="comment">return scoreRes;</span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment">return nameRes;*/</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString1</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> stuName + <span class="string">"\t"</span> + score;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> courseName + <span class="string">"\t"</span> + stuName</span><br><span class="line">+ <span class="string">"\t"</span> + score;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ScoreBean</span><span class="params">(String stuName, Double score)</span> </span>&#123;</span><br><span class="line"><span class="keyword">super</span>();</span><br><span class="line"><span class="keyword">this</span>.stuName = stuName;</span><br><span class="line"><span class="keyword">this</span>.score = score;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//  ScorePlusDemo1 </span></span><br><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._02_分组组件;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.HashSet;</span><br><span class="line"><span class="keyword">import</span> java.util.Set;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ScorePlusDemo1</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line">job.setJarByClass(ScorePlusDemo1.class);</span><br><span class="line"></span><br><span class="line">job.setMapperClass(MyMapper.class);</span><br><span class="line">job.setReducerClass(MyReducer.class);</span><br><span class="line"></span><br><span class="line">job.setMapOutputKeyClass(Text.class);</span><br><span class="line">job.setMapOutputValueClass(ScoreBean.class);</span><br><span class="line"></span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">String inP = <span class="string">"/in/newScoreIn"</span>;</span><br><span class="line">String outP = <span class="string">"/out/ans1"</span>;</span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(inP));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(outP));</span><br><span class="line"></span><br><span class="line">        Path mypath = <span class="keyword">new</span> Path(outP);</span><br><span class="line">        FileSystem hdfs = mypath.getFileSystem(conf);</span><br><span class="line">        <span class="keyword">if</span> (hdfs.isDirectory(mypath)) &#123;</span><br><span class="line">            hdfs.delete(mypath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">Boolean waitForComp = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">System.exit(waitForComp?<span class="number">0</span>:<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">ScoreBean</span>&gt;  </span>&#123;</span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">// 1.截取</span></span><br><span class="line">String[] datas = value.toString().trim().split(<span class="string">","</span>);</span><br><span class="line">String courseName = datas[<span class="number">0</span>].trim();</span><br><span class="line">String stuName = datas[<span class="number">1</span>].trim();</span><br><span class="line"><span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;datas.length; i++) &#123;</span><br><span class="line">sum += Integer.parseInt(datas[i]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">double</span> avgScore = sum/(datas.length-<span class="number">2</span>);</span><br><span class="line">ScoreBean sb = <span class="keyword">new</span> ScoreBean(courseName, stuName, avgScore);</span><br><span class="line">k.set(courseName);</span><br><span class="line">context.write(k, sb);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">ScoreBean</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;ScoreBean&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Reducer&lt;Text, ScoreBean, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">Set&lt;String&gt; stuNames = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line"><span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (ScoreBean sb : values) &#123;</span><br><span class="line">stuNames.add(sb.getStuName());</span><br><span class="line">count ++;</span><br><span class="line">sum += sb.getScore();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">int</span> size = stuNames.size();</span><br><span class="line">String val = size + <span class="string">"\t"</span> + (<span class="keyword">double</span>)sum/count;</span><br><span class="line">v.set(val);</span><br><span class="line">context.write(key, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行结果 </span></span><br><span class="line">algorithm<span class="number">6</span><span class="number">71.33333333333333</span></span><br><span class="line">computer<span class="number">10</span><span class="number">69.6</span></span><br><span class="line">english<span class="number">8</span><span class="number">66.0</span></span><br><span class="line">math<span class="number">7</span><span class="number">72.57142857142857</span></span><br></pre></td></tr></table></figure><h4 id="第2小题"><a href="#第2小题" class="headerlink" title="第2小题"></a>第2小题</h4><p><strong>统计每门课程参考学生的平均分，并且按课程存入不同的结果文件，要求一门课程一个结果文件</strong></p><p><strong>涉及知识点： 分区, 字符串组合key， Partitioner</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._02_分组组件;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.DoubleWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 注意: 此题因为数据中有2条 course 和 stuName相同的数据(english liuyifei), 所以必须再在reduce中继续去重一下, 再计算一下平均分</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 否则, 可以不用写reduce, 因为Mapper中已经把逻辑处理完了,可以直接输出</span></span><br><span class="line"><span class="comment"> </span></span><br><span class="line"><span class="comment"> * 最终输出: </span></span><br><span class="line"><span class="comment"> * computer liuyifei 43</span></span><br><span class="line"><span class="comment"> * computer huanglei 63</span></span><br><span class="line"><span class="comment"> * math liutao   64</span></span><br><span class="line"><span class="comment"> * ...</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ScorePlusDemo2</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定HDFS相关参数</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//  创建/配置 Job</span></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Jar包类型</span></span><br><span class="line">job.setJarByClass(ScorePlusDemo2.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Map Reduce执行类</span></span><br><span class="line">job.setMapperClass(MyMapper.class);</span><br><span class="line">job.setReducerClass(MyReducer.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Map输出类</span></span><br><span class="line">job.setMapOutputKeyClass(Text.class);</span><br><span class="line">job.setMapOutputValueClass(DoubleWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Reduce输出类</span></span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(DoubleWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//  设置分区 </span></span><br><span class="line">job.setPartitionerClass(MyPartition.class);</span><br><span class="line">job.setNumReduceTasks(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置输入 输出路径</span></span><br><span class="line">String inP = <span class="string">"/in/newScoreIn"</span>;</span><br><span class="line">String outP = <span class="string">"/out/scorePlus2"</span>;</span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(inP));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(outP));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置如果存在路径就删除</span></span><br><span class="line">        Path mypath = <span class="keyword">new</span> Path(outP);</span><br><span class="line">        FileSystem hdfs = mypath.getFileSystem(conf);</span><br><span class="line">        <span class="keyword">if</span> (hdfs.isDirectory(mypath)) &#123;</span><br><span class="line">            hdfs.delete(mypath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//  执行job</span></span><br><span class="line">        <span class="keyword">boolean</span> waitForCompletion = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(waitForCompletion?<span class="number">0</span>:-<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line">    </span><br><span class="line">===============================================================</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">DoubleWritable</span>&gt; </span>&#123;</span><br><span class="line"><span class="comment">// 把 课程+学生 作为 key</span></span><br><span class="line"></span><br><span class="line">Text k = <span class="keyword">new</span> Text();  <span class="comment">//只有输出String类型的, 才需要在这里设置Text</span></span><br><span class="line">DoubleWritable v = <span class="keyword">new</span> DoubleWritable();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">String[] datas = value.toString().trim().split(<span class="string">","</span>);</span><br><span class="line">String kStr = datas[<span class="number">0</span>].trim() + <span class="string">"\t"</span> + datas[<span class="number">1</span>].trim();</span><br><span class="line"><span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>; i &lt; datas.length; i++) &#123;</span><br><span class="line">sum += Integer.parseInt(datas[i]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">double</span> avg = sum / (datas.length - <span class="number">2</span>);</span><br><span class="line">k.set(kStr);</span><br><span class="line">v.set(avg);</span><br><span class="line">context.write(k, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">===============================================================</span><br><span class="line">    </span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 注意: 此题因为数据中有2条 course 和 stuName相同的数据, 所以必须再在reduce中</span></span><br><span class="line"><span class="comment"> * 继续去重一下, 再计算一下平均分</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 否则, 可以不用写reduce, 因为Mapper中已经把逻辑处理完了,可以直接输出</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">DoubleWritable</span>, <span class="title">Text</span>, <span class="title">DoubleWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">DoubleWritable v = <span class="keyword">new</span> DoubleWritable();</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;DoubleWritable&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Reducer&lt;Text, DoubleWritable, Text, DoubleWritable&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 考虑到有 课程, 学生名相同, 后面的数据不同的情况, 这里再做一个平均求和</span></span><br><span class="line"><span class="comment"> * 可以验证打印下</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">Double sum = <span class="number">0.0</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (DoubleWritable avg : values) &#123;</span><br><span class="line"><span class="keyword">if</span> (count &gt; <span class="number">0</span>) &#123;</span><br><span class="line"><span class="comment">// 有key完全相同的情况才会进到这里</span></span><br><span class="line">System.out.println(<span class="string">"这是第"</span> +count +<span class="string">"次, 说明课程和姓名有相同的两条数据\n课程姓名是: "</span>+key.toString());</span><br><span class="line">&#125;</span><br><span class="line">sum += avg.get();</span><br><span class="line">count ++;</span><br><span class="line">&#125;</span><br><span class="line">Double finAvg = sum/count;</span><br><span class="line">v.set(finAvg);</span><br><span class="line">context.write(key, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">===============================================================</span><br><span class="line">===============================================================</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 继承 Partitioner, 实现自定义分区</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyPartition</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">DoubleWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> HashMap&lt;String, Integer&gt; courseMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line">courseMap.put(<span class="string">"algorithm"</span>, <span class="number">0</span>);</span><br><span class="line">courseMap.put(<span class="string">"computer"</span>, <span class="number">1</span>);</span><br><span class="line">courseMap.put(<span class="string">"english"</span>, <span class="number">2</span>);</span><br><span class="line">courseMap.put(<span class="string">"math"</span>, <span class="number">3</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text key, DoubleWritable value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 取出Map输出的key中的前半部分--courseName</span></span><br><span class="line">Integer code = courseMap.get(key.toString().trim().split(<span class="string">"\t"</span>)[<span class="number">0</span>]);</span><br><span class="line"><span class="keyword">if</span> (code != <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="keyword">return</span> code;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="number">5</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">===============================================================</span><br><span class="line"> ===============================================================</span><br><span class="line">    </span><br><span class="line"> 执行结果 </span><br><span class="line">algorithmhuangdatou<span class="number">56.0</span></span><br><span class="line">algorithmhuangjiaju<span class="number">82.0</span></span><br><span class="line">algorithmhuanglei<span class="number">74.0</span></span><br><span class="line">algorithmhuangzitao<span class="number">72.0</span></span><br><span class="line">algorithmliutao<span class="number">82.0</span></span><br><span class="line">algorithmliuyifei<span class="number">62.0</span></span><br><span class="line">----------</span><br><span class="line">computerhuangbo<span class="number">65.0</span></span><br><span class="line">computerhuangdatou<span class="number">56.0</span></span><br><span class="line">computerhuangjiaju<span class="number">83.0</span></span><br><span class="line">computerhuanglei<span class="number">74.0</span></span><br><span class="line">computerhuangxiaoming<span class="number">72.0</span></span><br><span class="line">computerhuangzitao<span class="number">72.0</span></span><br><span class="line">computerliujialing<span class="number">64.0</span></span><br><span class="line">computerliutao<span class="number">83.0</span></span><br><span class="line">computerliuyifei<span class="number">62.0</span></span><br><span class="line">computerxuzheng<span class="number">65.0</span></span><br><span class="line">---------</span><br><span class="line">englishhuangbo<span class="number">55.0</span></span><br><span class="line">englishhuangdatou<span class="number">56.0</span></span><br><span class="line">englishhuanglei<span class="number">83.0</span></span><br><span class="line">englishhuangxiaoming<span class="number">72.0</span></span><br><span class="line">englishliujialing<span class="number">62.0</span></span><br><span class="line">englishliuyifei<span class="number">66.5</span></span><br><span class="line">englishzhaobenshan<span class="number">69.0</span></span><br><span class="line">englishzhouqi<span class="number">64.0</span></span><br><span class="line">------------</span><br><span class="line">mathhuangjiaju<span class="number">82.0</span></span><br><span class="line">mathhuanglei<span class="number">74.0</span></span><br><span class="line">mathhuangxiaoming<span class="number">83.0</span></span><br><span class="line">mathliujialing<span class="number">72.0</span></span><br><span class="line">mathliutao<span class="number">56.0</span></span><br><span class="line">mathwangbaoqiang<span class="number">72.0</span></span><br><span class="line">mathxuzheng<span class="number">69.0</span></span><br></pre></td></tr></table></figure><h4 id="第3小题"><a href="#第3小题" class="headerlink" title="第3小题"></a>第3小题</h4><p>求出 <strong><em>每门课程</em></strong><sup>①</sup>参与考试的学生成绩 <strong><em>最高平局分<sup>②</sup></em></strong>   的学生的信息：<u>课程，姓名和平均分</u></p><p><br></p><p><strong>解题思路：</strong> </p><ul><li>通过题意得出<strong>2个结论</strong><ul><li><strong>课程要分组</strong></li><li><strong>平均分要排序</strong></li></ul></li><li>排序的话，交给<strong>key</strong>来做无疑是最好的，因为<strong>MapReduce</strong>会<strong>自动</strong>对<strong>key</strong>进行<strong>分组&amp;排序</strong></li><li>因此可以把 <code>课程&amp;平均分</code> 作为一个<strong>联合key</strong></li><li>为了操作方便，可以<strong>封装到一个对象</strong>中去： <strong>ScoreBean</strong></li><li><strong>分组和排序</strong>需要在 <strong>ScoreBean</strong>重写的<strong><code>compareTo()</code>方法中完成</strong></li><li>因为最后结果是求<strong>每门课程</strong>的<strong>最高平均分</strong>，因此需要对课程进行分组。</li><li>此时原本的默认分组（以Bean对象整体分组）就不管用了，需要<strong>自定义分组</strong></li><li>自定义分组要<strong>继承<code>WritableComparator</code></strong>，重写<code>compare()</code>方法，指定分组的规则。</li><li><strong>ScoreBean</strong>先按照组别进行排序，到<strong>reduce</strong>中时，已经是按照组，排好的数据，<strong>MyGroup</strong> 会把相同的比较结果放到同一个组中，分发到<strong>reduce</strong>.</li><li><strong>reduce</strong>中，只需要取出每组的第一个元素输出到上下文即可</li></ul><p><br></p><p><strong>图示</strong></p><div id="sequence-0"></div><p><br></p><p><strong>涉及知识点： mr中key的作用，自定义对象的用法，自定义分组，mr的执行流程</strong></p><ul><li>利用“班级和平均分”作为 key，可以将 map 阶段读取到的所有学生成绩数据按照班级 和成绩排倒序，发送到 reduce</li><li>在 reduce 端利用 GroupingComparator 将班级相同的 kv 聚合成组，然后取第一个即是最 大值</li></ul><p><br></p><h5 id="先贴个结论："><a href="#先贴个结论：" class="headerlink" title="先贴个结论："></a><strong>先贴个结论：</strong></h5><p><strong>执行流程结论</strong></p><ul><li>map每读一行就 write 到 context 一次，按照指定的<code>key</code>进行分发</li><li><p>map 把所有的数据都读完了之后，大概执行到<code>67%</code>的时候，开始进入 <code>CustomBean</code>，执行<code>CustomBean</code>的<code>compareTo()</code>方法，会按照自己写的规则一条一条数据比较</p></li><li><p>上述都比较完毕之后，map阶段就结束了，此时来到了 reduce阶段，但是是到了<code>67%</code>了</p></li><li>到了reduce阶段，直接进入了<strong>MyGroup</strong>中自定义的<strong>compare</strong>方法。</li><li>MyGroup的<code>compare()</code>方法，如果返回<strong>非0</strong>， 就会进入 reduce 方法<strong>写出到context</strong></li></ul><p><strong>MyGroup进入Reduce的条件是</strong></p><ul><li>MyReduce中，如果compare的结果不等于0，也就是比较的2者不相同， 此时就进入Reduce， 写出到上下文</li><li>如果相同，会一直往下读，直到读到不同的， 此时写出读到上下文。</li><li>因为MyGroup会在Reduce阶段执行，而<code>CustomBean</code>中的<code>compareTo()</code>是在map阶段执行，所以需要在<code>CustomBean</code>中就把组排好序，此时<strong>分组功能</strong>才能<strong>正常运作</strong></li></ul><p><strong>指定分组类MyGroup和不指定的区别</strong></p><p><u>指定与不指定是指：在Driver类中，是否加上<code>job.setGroupingComparatorClass(MyGrouper.class);</code>这一句。</u></p><ul><li><strong>指定分组类</strong>：<ul><li>会按照分组类中，自定义的<code>compare()</code>方法比较，相同的为一组，分完一组就进入一次reduce方法</li></ul></li><li><strong>不指定分组类：（目前存疑）</strong><ul><li>是否是按照key进行分组</li><li>如果是自定义类为key，是否是按照此key中值相同的分为一组</li><li>如果是hadoop内置类，是否是按照此类的值分组（Text-String的值，IntWritable-int值等..）</li><li><strong>依然是走得以上这套分组逻辑，一组的数据读完才进入到Reduce阶段做归并</strong></li></ul></li></ul><p><br></p><h5 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ScoreBean2 </span></span><br><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._02_分组组件;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.Getter;</span><br><span class="line"><span class="keyword">import</span> lombok.Setter;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Getter</span></span><br><span class="line"><span class="meta">@Setter</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ScoreBean2</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">ScoreBean2</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> String courseName;</span><br><span class="line"><span class="keyword">private</span> Double score;</span><br><span class="line">    </span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">out.writeUTF(courseName);</span><br><span class="line">out.writeDouble(score);</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.courseName = in.readUTF();</span><br><span class="line"><span class="keyword">this</span>.score = in.readDouble();</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 如果是相同课程, 按照分数降序排列的</span></span><br><span class="line"><span class="comment"> * 如果是不同课程, 按照课程名称升序排列</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(ScoreBean2 o)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 测试一下只写按分数降序排序</span></span><br><span class="line"><span class="comment">//return o.getScore().compareTo(this.getScore());</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 首先分组(只在相同的组内进行比较)</span></span><br><span class="line"><span class="keyword">int</span> nameRes = <span class="keyword">this</span>.getCourseName().compareTo(o.getCourseName());</span><br><span class="line"><span class="keyword">if</span> (nameRes == <span class="number">0</span>) &#123;</span><br><span class="line"><span class="comment">// 课程相同的时候才进行降序排序</span></span><br><span class="line"><span class="keyword">int</span> scoreRes = o.getScore().compareTo(<span class="keyword">this</span>.getScore());</span><br><span class="line"><span class="keyword">return</span> scoreRes;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> nameRes;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 实际上ScoreBean中是包含所有的参数的, 这里的输出可以自己设置</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> courseName + <span class="string">"\t"</span> + score;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ScoreBean2</span><span class="params">(String courseName, Double score)</span> </span>&#123;</span><br><span class="line"><span class="keyword">super</span>();</span><br><span class="line"><span class="keyword">this</span>.courseName = courseName;</span><br><span class="line"><span class="keyword">this</span>.score = score;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ScoreBean2</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">super</span>();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// ScorePlusDemo3 </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._02_分组组件;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparator;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ScorePlusDemo3</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line"> main     </span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line">job.setJarByClass(ScorePlusDemo3.class);</span><br><span class="line"></span><br><span class="line">job.setMapperClass(MyMapper.class);</span><br><span class="line">job.setReducerClass(MyReducer.class);</span><br><span class="line"></span><br><span class="line">job.setMapOutputKeyClass(ScoreBean2.class);</span><br><span class="line">job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">job.setGroupingComparatorClass(MyGrouper.class);</span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">String outP = <span class="string">"/out/scorePlus3"</span>;</span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"/in/newScoreIn"</span>));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(outP));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果输出目录存在,就先删除</span></span><br><span class="line">Path myPath = <span class="keyword">new</span> Path(outP);</span><br><span class="line">FileSystem fs = myPath.getFileSystem(conf);</span><br><span class="line"><span class="keyword">if</span> (fs.isDirectory(myPath)) &#123;</span><br><span class="line">fs.delete(myPath, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">boolean</span> waitForCompletion = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">System.exit(waitForCompletion ? <span class="number">0</span> : -<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"> Mapper </span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 输出: key: course</span></span><br><span class="line"><span class="comment"> *     value: score ...</span></span><br><span class="line"><span class="comment"> * 思路:</span></span><br><span class="line"><span class="comment"> * 1.不同课程要分开展示, 以 课程+分数 作为key, 在mapper中完成排序 </span></span><br><span class="line"><span class="comment"> * 2.在reduce中按照 MyGrouper 完成分组</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span></span></span><br><span class="line"><span class="class"><span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">ScoreBean2</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> String[] datas;</span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">datas = value.toString().trim().split(<span class="string">","</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>; i &lt; datas.length; i++) &#123;</span><br><span class="line">sum += Integer.parseInt(datas[i]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">double</span> avg = (<span class="keyword">double</span>) sum / (datas.length - <span class="number">2</span>);</span><br><span class="line">ScoreBean2 sb = <span class="keyword">new</span> ScoreBean2(datas[<span class="number">0</span>].trim(), avg);</span><br><span class="line"></span><br><span class="line">v.set(datas[<span class="number">1</span>].trim());</span><br><span class="line">context.write(sb, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"> Redecer     </span><br><span class="line"><span class="keyword">static</span> <span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span></span></span><br><span class="line"><span class="class"><span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">ScoreBean2</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line"><span class="keyword">int</span> count = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(ScoreBean2 key, Iterable&lt;Text&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 如果没有其它问题</span></span><br><span class="line"><span class="comment"> * 此时是按照课程分好组了, 同一个课程的所有学生都过来了, 并且学生成绩是排好的,</span></span><br><span class="line"><span class="comment"> * 如果此时求最大值, 只需要取出第一个即可 </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">// 进来一次只取第一个</span></span><br><span class="line">Text name = values.iterator().next();</span><br><span class="line">k.set(key.getCourseName() + <span class="string">"\t"</span> + name.toString() + <span class="string">"\t"</span></span><br><span class="line">+ key.getScore());</span><br><span class="line">context.write(k, NullWritable.get());</span><br><span class="line">context.write(<span class="keyword">new</span> Text(<span class="string">"==================第"</span>+count+<span class="string">"次进入reduce"</span>), NullWritable.get());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/*context.write(new Text("==================第"+count+"次进入reduce"), NullWritable.get());</span></span><br><span class="line"><span class="comment">for (Text name : values) &#123;</span></span><br><span class="line"><span class="comment">k.set(key.getCourseName() + "\t" + name.toString() + "\t"</span></span><br><span class="line"><span class="comment">+ key.getScore());</span></span><br><span class="line"><span class="comment">context.write(k, NullWritable.get());</span></span><br><span class="line"><span class="comment">context.write(new Text("---------in for write------"), NullWritable.get());</span></span><br><span class="line"><span class="comment">&#125;*/</span></span><br><span class="line">count++;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"> MyGrouper </span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 自定义分组  需要继承一个类WritableComparator</span></span><br><span class="line"><span class="comment"> * 重写compare方法</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyGrouper</span> <span class="keyword">extends</span> <span class="title">WritableComparator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// WritableComparator 此方法的默认无参构造是不会创建对象的, 需要自己重写</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">MyGrouper</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="comment">// 中间省去的参数是 Configuration, 如果为空, 会创建一个新的</span></span><br><span class="line"><span class="keyword">super</span>(ScoreBean2.class, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 此处比较的是2个  WritableComparable 对象, 需要强转一下具体的类对象</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@SuppressWarnings</span>(<span class="string">"rawtypes"</span>)</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> </span>&#123;</span><br><span class="line">ScoreBean2 aBean = (ScoreBean2) a;</span><br><span class="line">ScoreBean2 bBean = (ScoreBean2) b;</span><br><span class="line"><span class="comment">// 返回分组规则</span></span><br><span class="line">System.out.println(aBean.getCourseName()+<span class="string">"---MyGroup中比较---"</span>+(bBean.getCourseName()));</span><br><span class="line"><span class="keyword">return</span> aBean.getCourseName().compareTo(bBean.getCourseName());</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">================================================================================</span><br><span class="line"> 执行结果 </span><br><span class="line">================================================================================</span><br><span class="line"></span><br><span class="line">algorithmhuangjiaju<span class="number">82.28571428571429</span></span><br><span class="line">==================第<span class="number">1</span>次进入reduce</span><br><span class="line">computerhuangjiaju<span class="number">83.2</span></span><br><span class="line">==================第<span class="number">2</span>次进入reduce</span><br><span class="line">englishhuanglei<span class="number">83.0</span></span><br><span class="line">==================第<span class="number">3</span>次进入reduce</span><br><span class="line">mathhuangxiaoming<span class="number">83.0</span></span><br><span class="line">==================第<span class="number">4</span>次进入reduce</span><br></pre></td></tr></table></figure><h2 id="MR实现两个表的数据关联Join"><a href="#MR实现两个表的数据关联Join" class="headerlink" title="MR实现两个表的数据关联Join"></a>MR实现两个表的数据关联<code>Join</code></h2><h3 id="题目-2"><a href="#题目-2" class="headerlink" title="题目"></a>题目</h3><blockquote><p>订单数据表t_order：  flag=0<br>id    date    pid    amount<br>1001    20150710    P0001    2<br>1002    20150710    P0001    3<br>1003    20150710    P0002    3<br>Id:数据记录id<br>Date   日期<br>Pid     商品id<br>Amount    库存数量</p><p>6.商品信息表t_product   flag=1<br>pid    name    category_id    price<br>P0001    小米5    C01    2000<br>P0002    锤子T1    C01    3500</p><p>mr实现两个表的数据关联<br>id   pid    date    amount    name    category_id     price</p></blockquote><p><br></p><h3 id="答案1-Reducer-端-实现-Join"><a href="#答案1-Reducer-端-实现-Join" class="headerlink" title="答案1 : Reducer 端 实现 Join"></a>答案1 : Reducer 端 实现 <code>Join</code></h3><h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><ul><li><p>map端</p><ul><li><p>读取到当前路径下，所有文件的切片信息， 根据文件名判断是那张表</p><ul><li><p>在setup中，从文件切片中获取到文件名</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取读取到的切片相关信息,一个切片对应一个 maptask</span></span><br><span class="line">InputSplit inputSplit = context.getInputSplit();</span><br><span class="line"><span class="comment">// 转换为文件切片</span></span><br><span class="line">FileSplit fs = (FileSplit)inputSplit;</span><br><span class="line"><span class="comment">// 获取文件名</span></span><br><span class="line">filename = fs.getPath().getName();</span><br></pre></td></tr></table></figure></li><li><p>这里总共会获得2个文件名（指定目录存了2个指定文件），一个文件名对应一个切片</p></li></ul></li><li><p>关联字段作为key， 其它的作为value，在value前面加上当前文件的名称标记</p></li></ul></li><li><p>reduce端</p><ul><li>通过标记区分两张表，把读取到的信息，分别存入2个list中</li><li>遍历大的表，与小表进行拼接（小表的相同pid记录只会有一条）</li><li>拼接完成后即可写出</li></ul></li></ul><h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._03_join2表的数据关联;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReduceJoinDemo</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 指定HDFS相关参数</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//  创建/配置 Job</span></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Jar包类型</span></span><br><span class="line">job.setJarByClass(ReduceJoinDemo.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Map Reduce执行类</span></span><br><span class="line">job.setMapperClass(MyMapper.class);</span><br><span class="line">job.setReducerClass(MyReducer.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Map输出类</span></span><br><span class="line">job.setMapOutputKeyClass(Text.class);</span><br><span class="line">job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Reduce输出类</span></span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置输入 输出路径</span></span><br><span class="line">String inP = <span class="string">"/in/joindemo"</span>;</span><br><span class="line">String outP = <span class="string">"/out/joinout1"</span>;</span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(inP));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(outP));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置如果存在路径就删除</span></span><br><span class="line">        Path mypath = <span class="keyword">new</span> Path(outP);</span><br><span class="line">        FileSystem hdfs = mypath.getFileSystem(conf);</span><br><span class="line">        <span class="keyword">if</span> (hdfs.isDirectory(mypath)) &#123;</span><br><span class="line">            hdfs.delete(mypath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//  执行job</span></span><br><span class="line">        <span class="keyword">boolean</span> waitForCompletion = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(waitForCompletion?<span class="number">0</span>:-<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 思路: 读取2个表中的数据,进行标记发送</span></span><br><span class="line"><span class="comment"> * key: 两表需要关联的字段</span></span><br><span class="line"><span class="comment"> * value: 其它值, 需要标记， 标记数据的来源</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * **核心： 关联条件**</span></span><br><span class="line"><span class="comment">- 想要在 reduce 端完成 join， 要在 reduce 端可以同时接收到两个表中的数据</span></span><br><span class="line"><span class="comment">- 要保证在 Map 端进行读文件的时候， 读到2个表的数据， 并且需要对2个表的数据进行区分</span></span><br><span class="line"><span class="comment">- 将2个表放在同一个目录下</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">解决: </span></span><br><span class="line"><span class="comment">mapper 开始执行时, 在setup方法中, 从上下文中取到文件名, 根据文件名打标记</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">String filename = <span class="string">""</span>;</span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line">Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">// 获取读取到的切片相关信息,一个切片对应一个 maptask</span></span><br><span class="line">InputSplit inputSplit = context.getInputSplit();</span><br><span class="line"><span class="comment">// 转换为文件切片</span></span><br><span class="line">FileSplit fs = (FileSplit)inputSplit;</span><br><span class="line"><span class="comment">// 获取文件名</span></span><br><span class="line">filename = fs.getPath().getName();</span><br><span class="line">System.out.println(<span class="string">"本次获取到的文件名为-----"</span>+filename);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value,</span></span></span><br><span class="line"><span class="function"><span class="params">Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">// 解析出来每一行内容, 打标记, 发送</span></span><br><span class="line">String[] infos = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (filename.equals(<span class="string">"order"</span>)) &#123;</span><br><span class="line">k.set(infos[<span class="number">2</span>]);</span><br><span class="line"><span class="comment">// 设置标记前缀为 OR</span></span><br><span class="line">v.set(<span class="string">"OR"</span>+infos[<span class="number">0</span>]+<span class="string">"\t"</span>+infos[<span class="number">1</span>]+<span class="string">"\t"</span>+infos[<span class="number">3</span>]);</span><br><span class="line">&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">k.set(infos[<span class="number">0</span>]);</span><br><span class="line"><span class="comment">// 设置标记前缀为 PR</span></span><br><span class="line">v.set(<span class="string">"PR"</span>+infos[<span class="number">1</span>]+<span class="string">"\t"</span>+infos[<span class="number">2</span>]+<span class="string">"\t"</span>+infos[<span class="number">3</span>]);</span><br><span class="line">&#125;</span><br><span class="line">context.write(k, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Reducer&lt;Text, Text, Text, NullWritable&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 首先明确 product 和 order 是 一对多的关系</span></span><br><span class="line"><span class="comment"> * 根据前缀不同,取到2个不同的表存进2个容器中</span></span><br><span class="line"><span class="comment"> * 遍历多的表, 与一进行拼接</span></span><br><span class="line"><span class="comment"> * 最后写出到上下文</span></span><br><span class="line"><span class="comment"> * 最终的输出格式 id   pid    date    amount    name    category_id     price</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">// 因为每次遍历到不同的pid, 都会走进来一次, list也会有新的输出,所以必须定义在里面,每次进来都要初始化</span></span><br><span class="line">List&lt;String&gt; productList =<span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">List&lt;String&gt; orderList =<span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (Text v : values) &#123;</span><br><span class="line">String vStr = v.toString();</span><br><span class="line"><span class="keyword">if</span> (vStr.startsWith(<span class="string">"OR"</span>)) &#123;</span><br><span class="line">orderList.add(vStr.substring(<span class="number">2</span>));</span><br><span class="line">&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">productList.add(vStr.substring(<span class="number">2</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 此时2个list添加完了本次 相同的 key(pid) 的所有商品</span></span><br><span class="line"><span class="comment">// 遍历多的进行拼接</span></span><br><span class="line"><span class="keyword">for</span> (String or : orderList) &#123;</span><br><span class="line"><span class="comment">// 相同的 pid的 product 只有一个, productList中的数量是1</span></span><br><span class="line"><span class="comment">// 但是相同pid 的 订单 可能有多个</span></span><br><span class="line">String res =  key.toString() + <span class="string">"\t"</span> + or + productList.get(<span class="number">0</span>);</span><br><span class="line">k.set(res);</span><br><span class="line">context.write(k, NullWritable.get());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><br></p><h3 id="※-答案2-：-Mapper-端实现-Join-※"><a href="#※-答案2-：-Mapper-端实现-Join-※" class="headerlink" title="※ 答案2 ： Mapper 端实现 Join  ※"></a>※ 答案2 ： Mapper 端实现 <code>Join</code>  ※</h3><h4 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h4><ul><li>创建job的时候,把小表加入缓存 在map的setup中, </li><li>读取缓存中的数据, 存入一个成员变量 map中<ul><li>map方法中,只需要读一个表, 然后根据关联条件(关联key: pid)消除笛卡尔集,进行拼接</li><li>map直接输出, 甚至都不需要reduce</li></ul></li></ul><h4 id="注意点"><a href="#注意点" class="headerlink" title="注意点:"></a>注意点:</h4><ul><li><p>需要达成jar包运行, 直接用Eclipse会找不到缓存</p></li><li><p><strong>jar包执行方法</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果代码内部指定了输入输出路径，后面的/in，/out参数可以不加</span></span><br><span class="line">hadoop jar xxxx.jar com.rox.xxx.xxxx(主方法)  /<span class="keyword">in</span>/xx   /out/xx</span><br></pre></td></tr></table></figure></li><li><p>如果没有Reduce方法</p><ul><li><p>main方法中，设置map的写出key，value,应该用 <code>setOutputKeyClass</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//// 设置Map输出类 (因为这里没有Reduce, 所以这里是最终输出,一定要注意!!!)//////////</span></span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(NullWritable.class);</span><br></pre></td></tr></table></figure></li><li><p>要设置reduce task 的个数为0 </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setNumReduceTasks(<span class="number">0</span>);</span><br></pre></td></tr></table></figure></li><li><p>把小文件加载到缓存中的方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">////////////// 将小文件加载到缓存  </span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"/in/joindemo/product"</span>));</span><br></pre></td></tr></table></figure></li></ul></li></ul><p>  ​    </p><h4 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr3._03_join;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.FileReader;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MapJoinDemo</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 指定HDFS相关参数</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//  创建/配置 Job</span></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Jar包类型:这里千万别写错了</span></span><br><span class="line">job.setJarByClass(MapJoinDemo.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Map Reduce执行类</span></span><br><span class="line">job.setMapperClass(MyMapper.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">///////////// 设置Map输出类 (因为这里没有Reduce, 所以这里是最终输出,一定要注意!!!)//////////</span></span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">////////////// 设置reduce执行个数为0</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">////////////// 将小文件加载到缓存  </span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"/in/joindemo/product"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置输入 输出路径</span></span><br><span class="line">String inP = <span class="string">"/in/joindemo/order"</span>;</span><br><span class="line">String outP = <span class="string">"/out/joinout2"</span>;</span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(inP));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(outP));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置如果存在路径就删除</span></span><br><span class="line">        Path mypath = <span class="keyword">new</span> Path(outP);</span><br><span class="line">        FileSystem hdfs = mypath.getFileSystem(conf);</span><br><span class="line">        <span class="keyword">if</span> (hdfs.isDirectory(mypath)) &#123;</span><br><span class="line">            hdfs.delete(mypath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//  执行job</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="keyword">true</span>)?<span class="number">0</span>:-<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> shixuanji</span></span><br><span class="line"><span class="comment"> * 思路: </span></span><br><span class="line"><span class="comment"> * 创建job的时候,把小表加入缓存</span></span><br><span class="line"><span class="comment"> * 在map的setup中, 读取缓存中的数据, 存入一个成员变量 map中</span></span><br><span class="line"><span class="comment"> * map方法中,只需要读一个表, 然后根据关联条件(关联key: pid)消除笛卡尔集,进行拼接</span></span><br><span class="line"><span class="comment"> * 直接输出, 甚至都不需要reduce</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 注意点: </span></span><br><span class="line"><span class="comment"> * 需要达成jar包运行, 直接用Eclipse会找不到缓存</span></span><br><span class="line"><span class="comment"> * 格式: hadoop jar包本地路径 jar包主方法全限定名 hadoop输入  hadoop输出</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建装载小表的map, key存储 关联键, value存其它</span></span><br><span class="line">Map&lt;String, String&gt; proMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">// 获取缓存中存储的小表 (一般是 一对多中的 一), 因为只存了1个,所以直接取第0个</span></span><br><span class="line">Path path = context.getLocalCacheFiles()[<span class="number">0</span>];</span><br><span class="line">String pString = path.toString();</span><br><span class="line"><span class="comment">// 开启in流, BufferedReader 逐行读取文件</span></span><br><span class="line">BufferedReader br = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> FileReader(pString));</span><br><span class="line">String line = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">while</span> ((line = br.readLine()) != <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="comment">// 成功读取一行</span></span><br><span class="line">String[] infos = line.split(<span class="string">"\t"</span>);</span><br><span class="line"><span class="comment">// 存进proMap</span></span><br><span class="line">proMap.put(infos[<span class="number">0</span>],</span><br><span class="line">infos[<span class="number">1</span>] + <span class="string">"\t"</span> + infos[<span class="number">2</span>] + <span class="string">"\t"</span> + infos[<span class="number">3</span>]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//br.close();</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 直接从路径读取大文件</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">String[] infos = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">String pid = infos[<span class="number">2</span>];</span><br><span class="line"><span class="comment">//进行关联   pid到map中匹配   如果包含  证明匹配上了</span></span><br><span class="line"><span class="comment">// 艹, 这里pid之前加了 "", 妈的,当然找不到啦!!!</span></span><br><span class="line"><span class="keyword">if</span> (proMap.containsKey(pid)) &#123;</span><br><span class="line">String res = value.toString() + <span class="string">"\t"</span> + proMap.get(pid);</span><br><span class="line">k.set(res);</span><br><span class="line">context.write(k, NullWritable.get());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><script src="https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js"></script><script src="https://cdn.bootcss.com/snap.svg/0.5.1/snap.svg-min.js"></script><script src="https://cdn.bootcss.com/raphael/2.2.7/raphael.min.js"></script><script src="https://cdn.bootcss.com/underscore.js/1.8.3/underscore-min.js"></script><script src="https://cdn.bootcss.com/js-sequence-diagrams/1.0.6/sequence-diagram-min.js"></script><textarea id="sequence-0-code" style="display: none">title: 执行流程时序图 Mapper(map)->ScoreBean: k:ScoreBean(courseName,avgScore)Mapper(map)->ScoreBean: v:Text(stuName)Mapper(ScoreBean)->Reducer(MyGroup): course按字典升序Mapper(ScoreBean)->Reducer(MyGroup): course内成绩降序Reducer(MyGroup)->Reducer(reduce): 根据自定义的分组规则按组输出Reducer(MyGroup)->Reducer(reduce): 一组只调用reduce一次</textarea><textarea id="sequence-0-options" style="display: none">{"theme":"simple"}</textarea><script>  var code = document.getElementById("sequence-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value));  var diagram = Diagram.parse(code);  diagram.drawSVG("sequence-0", options);</script></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;求微博共同粉丝&quot;&gt;&lt;a href=&quot;#求微博共同粉丝&quot; class=&quot;headerlink&quot; title=&quot;求微博共同粉丝&quot;&gt;&lt;/a&gt;求微博共同粉丝&lt;/h2&gt;&lt;h3 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce笔记-3</title>
    <link href="https://airpoet.github.io/2018/06/08/Hadoop/Study/2-MapReduce/MapReduce%E7%AC%94%E8%AE%B0-3/"/>
    <id>https://airpoet.github.io/2018/06/08/Hadoop/Study/2-MapReduce/MapReduce笔记-3/</id>
    <published>2018-06-08T01:23:27.815Z</published>
    <updated>2018-06-11T14:13:56.738Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-多-Job-串联"><a href="#1-多-Job-串联" class="headerlink" title="1.多 Job 串联"></a>1.多 Job 串联</h2><h3 id="1-概念"><a href="#1-概念" class="headerlink" title="1.概念"></a>1.概念</h3><p>当程序中有多个 Job， 并且多个 job 之间相互依赖， a ， job 需要依赖另一个b，job 的执行结果时候， 此时需要使用多 job 串联</p><h3 id="2-涉及到昨天的微博求共同粉丝题目"><a href="#2-涉及到昨天的微博求共同粉丝题目" class="headerlink" title="2. 涉及到昨天的微博求共同粉丝题目"></a>2. 涉及到昨天的微博求共同粉丝题目</h3><blockquote><p>A:B,C,D,F,E,O<br>B:A,C,E,K<br>C:F,A,D,I<br>D:A,E,F,L<br>E:B,C,D,M,L<br>F:A,B,C,D,E,O,M<br>G:A,C,D,E,F<br>H:A,C,D,E,O<br>I:A,O<br>J:B,O<br>K:A,C,D<br>L:D,E,F<br>M:E,F,G<br>O:A,H,I,J,K</p><p>以上是数据：<br>A:B,C,D,F,E,O<br>表示：A用户 关注B,C,D,E,F,O</p><p>求所有两两用户之间的共同关注对象</p></blockquote><h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><ul><li>要写2个MapReduce， 开启2个job</li><li>后一个job依赖于前一个的执行结果</li><li>后一个job的输入文件路径，就是前一个job的输出路径</li><li>2个job需要添加依赖</li></ul><h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><p><a href="https://airpoet.github.io/2018/06/09/Hadoop/Study/2-MapReduce/MapReduce-%E7%BB%83%E4%B9%A0/#more">参考练习-第一题-求微博共同好友</a></p><h4 id="多-Job-串联部分代码"><a href="#多-Job-串联部分代码" class="headerlink" title="多 Job 串联部分代码"></a><strong>多 Job 串联部分代码</strong></h4><ul><li><p>基本的写到一起， job1， job2</p></li><li><p>用<code>JobControl</code>对象管理多 job， 会将多个 job 当做一个组中的 job 提交， 参数指的是组名， 随意起</p></li><li><p>原生的 job 要转为可控制的 job</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建 JobControl 组</span></span><br><span class="line">JobControl jc = <span class="keyword">new</span> JobControl(<span class="string">"common_friend"</span>);</span><br><span class="line"><span class="comment">// job 拿好配置， 加入 ControlledJob 管理, 变成可控制的 job</span></span><br><span class="line">ControlledJob ajob = <span class="keyword">new</span> ControlledJob(job1.getConfiguration());</span><br><span class="line">ControlledJob bjob = <span class="keyword">new</span> ControlledJob(job2.getConfiguration());</span><br><span class="line"><span class="comment">// 添加依赖关系</span></span><br><span class="line">bjob.addDependingJob(ajob);  </span><br><span class="line"><span class="comment">//  添加 job进 JC</span></span><br><span class="line">jc.addJob(ajob);</span><br><span class="line">jc.addJob(bjob);</span><br><span class="line"><span class="comment">// 启动线程</span></span><br><span class="line">Thread jobControlTread = <span class="keyword">new</span> Thread(jc);</span><br><span class="line">jobControlTread.start();</span><br><span class="line"><span class="comment">// 在线程完成之后关闭</span></span><br><span class="line"><span class="keyword">while</span>(!jc.allFinished()) &#123;</span><br><span class="line">Thread.sleep(<span class="number">500</span>);</span><br><span class="line">&#125;</span><br><span class="line">jobControl.stop();</span><br></pre></td></tr></table></figure></li></ul><hr><h2 id="2-分组组件"><a href="#2-分组组件" class="headerlink" title="2. 分组组件"></a>2. 分组组件</h2><p><strong>map–分组–reduce</strong>  </p><p>reduce 接收到的数据是按照 map 输出的 key 进行分组的, 分组的时候按照 key 相同的时候为一组,  默认都实现了 <code>WritableComparable</code>接口， 其中的 compareTo（）方法返回为0的时候 默认为一组， 返回不为0， 则分到下一组</p><p><br></p><p><strong>自定义分组使用场景：</strong> 默认的数据分组不能满足需求</p><blockquote><p>一、数据解释</p><p>数据字段个数不固定：<br>第一个是课程名称，总共四个课程，computer，math，english，algorithm，<br>第二个是学生姓名，后面是每次考试的分数</p><p>二、统计需求：<br>1、统计每门课程的参考人数和课程平均分</p><p>2、统计每门课程参考学生的平均分，并且按课程存入不同的结果文件，要求一门课程一个结果文件</p><p>3、求出每门课程参考学生成绩最高平均分的学生的信息：课程，姓名和平均分</p></blockquote><p><strong>第三题： 要求就是分组求最大值， 两件事情： 分组， 排序（shuffle）</strong></p><p><strong>总结：</strong> </p><p>1、利用“班级和平均分”作为 key，可以将 map 阶段读取到的所有学生成绩数据按照班级 和成绩排倒序，发送到 reduce</p><p>2、在 reduce 端利用 GroupingComparator 将班级相同的 kv 聚合成组，然后取第一个即是最 大值</p><h4 id="具体参考："><a href="#具体参考：" class="headerlink" title="具体参考："></a>具体参考：</h4><p><a href="https://airpoet.github.io/2018/06/09/Hadoop/Study/2-MapReduce/MapReduce-%E7%BB%83%E4%B9%A0/#more">练习-求学生成绩-第三小题</a></p><hr><h2 id="3-Reduce-中的2个坑"><a href="#3-Reduce-中的2个坑" class="headerlink" title="3. Reduce 中的2个坑"></a>3. Reduce 中的2个坑</h2><h3 id="坑1"><a href="#坑1" class="headerlink" title="坑1"></a>坑1</h3><p><strong>Iterable\<text>只能循环遍历一次</text></strong></p><p>迭代器每次循环遍历完成， 指针都会移动到最后一个</p><p>系统类型，没事</p><p>自定义类型 ，有问题？</p><h3 id="坑2"><a href="#坑2" class="headerlink" title="坑2"></a>坑2</h3><p><strong>迭代器中所有对象公用同一个地址</strong></p><hr><h2 id="4-Reduce-端的-Join"><a href="#4-Reduce-端的-Join" class="headerlink" title="4. Reduce 端的 Join"></a>4. Reduce 端的 Join</h2><p><strong>牺牲效率换执行</strong></p><p>思路： </p><p><strong>核心： 关联条件</strong></p><ul><li>想要在 reduce 端完成 join， 要在 reduce 端可以同时接收到两个表中的数据</li><li>要保证在 Map 端进行读文件的时候， 读到2个表的数据， 并且需要对2个表的数据进行区分</li><li>将2个表放在同一个目录下</li></ul><h4 id="Map-端"><a href="#Map-端" class="headerlink" title="Map 端"></a>Map 端</h4><ul><li><strong>读取两个表中的数据， 进行切分、发送</strong></li><li>key ： 公共字段–关联字段–<code>pid</code></li><li>value： 剩下的字段， 标记数据的来源表</li></ul><h4 id="Reduce-端"><a href="#Reduce-端" class="headerlink" title="Reduce 端"></a>Reduce 端</h4><ul><li>通过编辑分离出2个表的数据</li><li>分别存到2个容器中（ArrayList）</li><li>遍历大表，拼接小表</li></ul><h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><p><a href="https://airpoet.github.io/2018/06/09/Hadoop/Study/2-MapReduce/MapReduce-%E7%BB%83%E4%B9%A0/#more">参考练习-第三题MR实现2个表之间的Join</a></p><h3 id="缺陷"><a href="#缺陷" class="headerlink" title="缺陷"></a>缺陷</h3><p><strong>1. ReduceTask 的并行度问题：</strong></p><ul><li>建议0.95*datanode 的个数</li><li>并行度不高， 性能不高</li></ul><p><strong>2. 容器性能</strong></p><ul><li>list 等， 不提倡， reduce 接收的数据， 可能会很大</li></ul><p><strong>3. ReduceTask 容易产生数据倾斜</strong></p><ul><li>假设我们设置多个 ReduceTask， 根据分区规则， 默认 hash</li><li>以 key关联条件分，  ReduceTask数据倾斜， 每个 ReduceTask 分工不均， 非常影响性能，没有合理的利用集群资源</li><li>在真实的生产中一定要尽量的避免数据倾斜</li><li>最好的做法：将分区设计的足够完美，难度比较大</li><li>因此，ReduceTask 一般不会完成 John工作</li><li><strong>放在 Map 端完成就不会有这个问题了</strong></li></ul><hr><h5 id="补充：Mapper-中的源码分析"><a href="#补充：Mapper-中的源码分析" class="headerlink" title="补充：Mapper 中的源码分析"></a>补充：Mapper 中的源码分析</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 在 maptask 执行之前调用一次， 一个 maptask 只会调用一次。setup 中通常会帮 map 中初始化一些变量和资源， 比如数据库的连接等。</span></span><br><span class="line">    <span class="comment">// 主要目的：减少资源的初始化次数而提升程序的性能</span></span><br><span class="line">    setup(context);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 获取文件是否还有下一行， 一行只调用一次</span></span><br><span class="line">      <span class="keyword">while</span> (context.nextKeyValue()) &#123;</span><br><span class="line">        map(context.getCurrentKey(), context.getCurrentValue(), context);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">       <span class="comment">// maptask 任务执行完成之后会调用一次，一个 maptask 只会调用一次</span></span><br><span class="line">       <span class="comment">// 帮 map 处理一些善后工作， 比如：资源的关闭</span></span><br><span class="line">      cleanup(context);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><hr><h2 id="5-Map-端的-Join"><a href="#5-Map-端的-Join" class="headerlink" title="5. Map 端的 Join"></a>5. Map 端的 Join</h2><p><strong>注意点：这种方式只能通过 Jar 包上传的方式，直接用 Eclipse 会找不到缓存</strong> </p><p>为了提升 Map 端 Join 性能， 我们的策略是， <strong>将小表的数据加载到每个运行的 MapTask 的内存中</strong>。</p><p>如果小表被加载到了内存中， 我们<strong>每次在 Map 端只需要读取大表，当读取到大表的每一行数据，可以直接和内存中的小表进行关联。</strong></p><p>这个时候，<strong>只需要 Map 就可以完成 Join 操作了</strong>。</p><p><br></p><h3 id="1-如何将小表加入到内存中？"><a href="#1-如何将小表加入到内存中？" class="headerlink" title="1. 如何将小表加入到内存中？"></a>1. 如何将小表加入到内存中？</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将指定路径文件加载到缓存中</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"/xxx"</span>));</span><br></pre></td></tr></table></figure><h3 id="2-Map-端怎样读取缓存中的数据"><a href="#2-Map-端怎样读取缓存中的数据" class="headerlink" title="2. Map 端怎样读取缓存中的数据"></a>2. Map 端怎样读取缓存中的数据</h3><p>想要在 Java 中使用缓存中的数据，缓存中的数据必须封装到 Java 的容器中</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取缓存文件</span></span><br><span class="line">context.getLocalCacheFiles()[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h3 id="3-代码"><a href="#3-代码" class="headerlink" title="3. 代码"></a>3. 代码</h3><p><a href="https://airpoet.github.io/2018/06/09/Hadoop/Study/2-MapReduce/MapReduce-%E7%BB%83%E4%B9%A0/#more">参考练习-第3题</a></p><p><strong>代码注意点：</strong> </p><p><strong>setup</strong>：从缓存读取一文件（多对一的一）到 HashMap</p><p><strong>main 方法中注意点</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 指定文件加入缓存</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"/xxx"</span>)); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果没有ReduceTask， 要设置为0</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">0</span>);</span><br></pre></td></tr></table></figure><hr><h2 id="6-对比"><a href="#6-对比" class="headerlink" title="6. 对比"></a>6. 对比</h2><p><strong>MapJoin 的方式： 大 &amp; 小表</strong></p><p>因为有一个表需要加载到内存中，注定加载到内存中的表不能过大（hive 中默认是256M）</p><p><strong>大表 &amp; 大表 如何设计</strong></p><ul><li>ReduceJoin ： 解决数据倾斜的问题，合理设计分区。 —很难做到</li><li>将其中一个<strong>大表进行切分</strong>，切分成小表， <strong>最终执行 大表 &amp; 小表</strong></li></ul><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul><li><p>并行度高，不存在数据倾斜的问题，运行效率高</p></li><li><p><strong>优先选择MapJoin</strong></p></li></ul><hr><h2 id="arrow-forward-7-排序算法-（待整理：TODO）"><a href="#arrow-forward-7-排序算法-（待整理：TODO）" class="headerlink" title=":arrow_forward: 7. 排序算法 （待整理：TODO）"></a>:arrow_forward: 7. 排序算法 （待整理：TODO）</h2><h3 id="1-快速排序"><a href="#1-快速排序" class="headerlink" title="1. 快速排序"></a>1. 快速排序</h3><p><strong>边界值始终是不变的。</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-08-083614.png" alt="image-20180608163614370"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-08-083842.png" alt="image-20180608163841417"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-08-084004.png" alt="image-20180608164004311"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-08-084128.png" alt="image-20180608164127167"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-08-084319.png" alt="image-20180608164318335"></p><h3 id="2-归并排序"><a href="#2-归并排序" class="headerlink" title="2. 归并排序"></a>2. 归并排序</h3><p>一般情况针对<strong>有序的</strong>，<strong>多个</strong>， <strong>小</strong>数据集</p><blockquote><p>应用场景：想到了多个Reduce 任务产生的多个文件的合并</p></blockquote><h4 id="1-归并排序前传：-合并多个数组"><a href="#1-归并排序前传：-合并多个数组" class="headerlink" title="1. 归并排序前传： 合并多个数组"></a>1. 归并排序前传： 合并多个数组</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-08-085031.png" alt="image-20180608165031396"></p><h4 id="2-归并排序-之-一个大数据集"><a href="#2-归并排序-之-一个大数据集" class="headerlink" title="2. 归并排序 之 一个大数据集"></a>2. 归并排序 之 一个大数据集</h4><h5 id="—归———"><a href="#—归———" class="headerlink" title="—归———-"></a>—归———-</h5><p><strong>切分成单个的数据集</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-08-085652.png" alt="image-20180608165651868"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-08-085727.png" alt="image-20180608165727484"></p><h5 id="—-并———"><a href="#—-并———" class="headerlink" title="—-并———"></a>—-并———</h5><ol><li>两两相并， 并成新的数组， 小的先放入数组， 再放大的</li><li>新的数组再不断执行 上述的 <strong>合并多个数组</strong></li></ol><hr><h2 id="8-※-Shuffle-过程-※"><a href="#8-※-Shuffle-过程-※" class="headerlink" title="8.  ※ Shuffle 过程 ※"></a>8.  ※ Shuffle 过程 ※</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-024417.png" alt="image-20180611104416430"></p><h3 id="1、Map端的Shuffle"><a href="#1、Map端的Shuffle" class="headerlink" title="1、Map端的Shuffle"></a>1、Map端的Shuffle</h3><p><strong>环形缓冲区</strong></p><p><strong>类： mapreduce.task.io.sort.mb</strong>  </p><ul><li>map读取数据后，并非直接写到磁盘，而是<strong>先写到一个缓冲区–环形缓冲区</strong></li><li>默认大小：100M</li><li>阈值：0.8</li><li>当达到阈值之后，环形缓冲区会触发写入磁盘的操作，80M写完后，会清空。</li><li><strong>一个MapTask对应一个环形缓冲区， 一个MapTask默认128M</strong></li><li>一个<strong>MapTask</strong>可以分发到多个<strong>ReduceTask</strong>，看自己怎么设置的</li></ul><p><strong>底层</strong></p><ul><li><strong>存储的是一个字节数组</strong></li><li><strong>MapOutputBuffer类</strong><ul><li><strong>equator</strong>   // marks origin of meta/serialization </li><li>赤道：用于区分<strong>元数据</strong>和<strong>原始数据</strong>, 元数据&amp;原始数据 <strong>以赤道为界</strong> <strong>往左右同时写</strong><ul><li>原始数据： maptask输出的数据  hello,1</li><li>元数据：用于记录原始数据的数据</li></ul></li><li>左边到头了，转到右边最后一个，继续往左边写</li></ul></li><li><strong>为什么要进行存储元数据：</strong><ul><li>shuffle过程中会进行排序</li><li>原始数据会按照key排序，key的长度不一致，很麻烦</li><li>使用元数据可以解决这个问题</li></ul></li><li><strong>元数据包含</strong><ul><li>原始数据中key的起始位置</li><li>原始数据中的value的起始位置</li><li>value的长度</li><li><strong>分区信息：这条数据属于哪个分区</strong><ul><li>写的时候会调用分区组件</li><li>如果没有定义，就调用默认的，没有设置的话，就只有一个reducetask</li><li>定义了，就调用<strong>自定义的分区</strong>，自己重写的 <strong>Partition</strong></li></ul></li></ul></li><li><strong>元数据长度</strong><ul><li>16字节 * 4 （上述4条数据，一条16字节）</li><li>每一条元数据占用的空间大小是一样的，在进行排序交换位置的时候，可以方便的交换元数据</li></ul></li><li><strong>默认大小100M， 阈值80M</strong><ul><li>便于继续进行数据写入</li><li>如果在写出到磁盘过程中，剩下的20M写满了，就会进入阻塞状态，直到80M的空间释放完成</li></ul></li><li>开启<strong>溢写之前</strong>会对 <strong>环形缓冲区的数据</strong> 进行<strong>排序</strong><ul><li>先根据<strong>分区排序</strong>，相同的分区会排在一起（按照<strong>元数据排</strong>）</li><li>再对<strong>key排序</strong>，相同的key会在一起（按照<strong>原始数据排</strong>，再<strong>调整元数据位置</strong>）</li><li>内部排序采用的 <strong>快速排序</strong></li></ul></li><li><strong>元数据&amp; 原始数据 到了0.8阈值之后</strong><ul><li>开启<strong>溢写进程</strong>，每个分区有一个溢写大文件<ul><li>一个split就是一个MapTask（128M）， 写入缓冲区（100M-80M</li><li><strong>写满一个80M，就开始写出到本地磁盘</strong>， <code>file.out</code>，再写下一个<code>file.out</code>,可能有多个，因为在map程序中可能还会拼接其它的东西</li><li>将这多个 小 file.out,  按照<strong>分区，key</strong> 进行<strong>归并排序到本地磁盘中的一个大的 file.out</strong><ul><li>多个溢写小文件最后进行 merge <strong>（归并排序）</strong></li></ul></li><li>此<code>file.out</code>是按照分区，key排序了的，<strong>至此，等待reduce线程来抓取了</strong>？？ </li></ul></li></ul></li></ul><h3 id="2、Reducer端的Shuffle"><a href="#2、Reducer端的Shuffle" class="headerlink" title="2、Reducer端的Shuffle"></a>2、Reducer端的Shuffle</h3><ul><li>reduceTask 根据自己的分区号，去各个 mapTask 机器上取相应的结果分区数据</li><li>reduceTask 会取到同一个分区的来自不同 mapTask 的结果文件，reduceTask 会将这些文 件再进行合并（归并排序）</li><li>合并成大文件后，shuffle 的过程也就结束了，后面进入 reduceTask 的逻辑运算过程（从 文件中取出一个一个的键值对 group，调用用户自定义的 reduce()方法）<ul><li>group是在shuffle过程中吗？</li></ul></li></ul><p><strong>Combiner组件调用时机：</strong></p><ul><li>环形缓冲区 溢写 到磁盘的时候 的时候，调用一次</li><li>磁盘中的多个溢写文件进行归并的时候，也会调用</li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-030348.png" alt="image-20180611110348500"></p><h2 id="9、源码跟踪-TODO"><a href="#9、源码跟踪-TODO" class="headerlink" title="9、源码跟踪(:TODO)"></a>9、源码跟踪(:TODO)</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-034909.png" alt="image-20180611114909081"></p><hr><p><br></p><h2 id="10、自定义输入"><a href="#10、自定义输入" class="headerlink" title="10、自定义输入"></a>10、自定义输入</h2><h4 id="需求："><a href="#需求：" class="headerlink" title="需求："></a>需求：</h4><p>对小文件的处理，在业务处理之前，在 HDFS 上使用 MapReduce 程序对小文件进行合并。</p><h4 id="核心实现思路："><a href="#核心实现思路：" class="headerlink" title="核心实现思路："></a><strong>核心实现思路：</strong></h4><ol><li><p>编写自定义的 InputFormat。</p></li><li><p>改写 RecordReader，实现一次 maptask 读取一个小文件的完整内容封装了一个 KV 对。</p></li><li><p>在 Driver 类中一定要设置使用自定义的 InputFormat：</p><p><code>job.setInputFormatClass(WholeFileInputFormat.class)</code></p></li></ol><h4 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h4><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLWZFoCz9TSlCIVNCAoWjSSiloaqiuN98pKi1AW40"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr4._02_inputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.JobContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordReader;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WholeFileInputFormat</span> <span class="keyword">extends</span> <span class="title">FileInputFormat</span>&lt;<span class="title">NullWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 设置每个小文件不可分片,保证一个小文件生成一个key-v键值对</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">isSplitable</span><span class="params">(JobContext context, Path filename)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> RecordReader&lt;NullWritable, Text&gt; <span class="title">createRecordReader</span><span class="params">(InputSplit split,</span></span></span><br><span class="line"><span class="function"><span class="params">TaskAttemptContext context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">WholeFileRecordReader reader = <span class="keyword">new</span> WholeFileRecordReader();</span><br><span class="line">reader.initialize(split, context);</span><br><span class="line"><span class="keyword">return</span> reader;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLWZFoCz9TSlCIVNCAoWjSSiloaqiuN98pKi1AW40"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr4._02_inputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordReader;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WholeFileRecordReader</span> <span class="keyword">extends</span> <span class="title">RecordReader</span>&lt;<span class="title">NullWritable</span>, <span class="title">Text</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> FileSplit fileSplit;</span><br><span class="line"><span class="keyword">private</span> Configuration conf;</span><br><span class="line"><span class="keyword">private</span> Text value = <span class="keyword">new</span> Text(); </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">boolean</span> processed = <span class="keyword">false</span>;  <span class="comment">// 标识文件是否读取完成</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 初始化方法</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(InputSplit split, TaskAttemptContext context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.fileSplit=(FileSplit)split;</span><br><span class="line"><span class="keyword">this</span>.conf = context.getConfiguration();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">nextKeyValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (!processed) &#123;</span><br><span class="line"><span class="keyword">byte</span>[] contents = <span class="keyword">new</span> <span class="keyword">byte</span>[(<span class="keyword">int</span>)fileSplit.getLength()];</span><br><span class="line">Path file = fileSplit.getPath();</span><br><span class="line">FileSystem fs = file.getFileSystem(conf);</span><br><span class="line">FSDataInputStream in = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">in = fs.open(file);</span><br><span class="line"><span class="comment">// 把输入流上的数据全部读取到contents字节数组中</span></span><br><span class="line">IOUtils.readFully(in, contents, <span class="number">0</span>, contents.length);</span><br><span class="line"><span class="comment">// 把读取到的数据设置到value里</span></span><br><span class="line">value.set(contents,<span class="number">0</span>,contents.length);</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">IOUtils.closeStream(in);</span><br><span class="line">&#125;</span><br><span class="line">processed = <span class="keyword">true</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> NullWritable <span class="title">getCurrentKey</span><span class="params">()</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="keyword">return</span> NullWritable.get();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Text <span class="title">getCurrentValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="keyword">return</span> value;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">float</span> <span class="title">getProgress</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="keyword">return</span> processed ? <span class="number">1.0f</span> : <span class="number">0.0f</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLWZEJSp9SSlCIIrspiyhIoqg2Sbtoapt3U9oICrB0Ie30000"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.mr4._02_inputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SmallFilesConvertToBigMR</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">int</span> exitCode = ToolRunner.run(<span class="keyword">new</span> SmallFilesConvertToBigMR(), args);</span><br><span class="line">System.exit(exitCode);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line"></span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line">Job job = Job.getInstance(conf, <span class="string">"combine small files to bigfile"</span>);</span><br><span class="line"></span><br><span class="line">job.setJarByClass(SmallFilesConvertToBigMR.class);</span><br><span class="line"></span><br><span class="line">job.setMapOutputKeyClass(Text.class);</span><br><span class="line"></span><br><span class="line">job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">job.setMapperClass(SmallFilesConvertToBigMRMapper.class);</span><br><span class="line"></span><br><span class="line">job.setReducerClass(SmallFilesConvertToBigMRReducer.class);</span><br><span class="line"></span><br><span class="line">job.setOutputKeyClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">job.setOutputValueClass(Text.class);</span><br><span class="line"><span class="comment">////////</span></span><br><span class="line">job.setInputFormatClass(WholeFileInputFormat.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// job.setOutputFormatClass(SequenceFileOutputFormat.class);</span></span><br><span class="line"></span><br><span class="line">Path input = <span class="keyword">new</span> Path(<span class="string">"/in/joindemo"</span>);</span><br><span class="line"></span><br><span class="line">Path output = <span class="keyword">new</span> Path(<span class="string">"/out/bigfile"</span>);</span><br><span class="line"></span><br><span class="line">FileInputFormat.setInputPaths(job, input);</span><br><span class="line"></span><br><span class="line">FileSystem fs = FileSystem.get(conf);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (fs.exists(output)) &#123;</span><br><span class="line"></span><br><span class="line">fs.delete(output, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">FileOutputFormat.setOutputPath(job, output);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> status = job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> status;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SmallFilesConvertToBigMRMapper</span></span></span><br><span class="line"><span class="class"><span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">NullWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> Text filenameKey;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">Mapper&lt;NullWritable, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">InputSplit split = context.getInputSplit();</span><br><span class="line">Path path = ((FileSplit) split).getPath();</span><br><span class="line">filenameKey = <span class="keyword">new</span> Text(path.toString());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(NullWritable key, Text value,</span></span></span><br><span class="line"><span class="function"><span class="params">Mapper&lt;NullWritable, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">context.write(filenameKey, value);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SmallFilesConvertToBigMRReducer</span></span></span><br><span class="line"><span class="class"><span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text filename, Iterable&lt;Text&gt; bytes,</span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">context.write(NullWritable.get(), bytes.iterator().next());</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><br></p><h2 id="11、自定义输出"><a href="#11、自定义输出" class="headerlink" title="11、自定义输出"></a>11、自定义输出</h2><p><strong>TextOutputFormat</strong></p><p><strong>main文件中，写输出路径是为了存放运行成功的标识文件</strong></p><h3 id="需求：-1"><a href="#需求：-1" class="headerlink" title="需求："></a>需求：</h3><p>在一个 MapReduce 程序中根据数据的不同输出两类结果到不同目录，这 类灵活的输出需求可以通过自定义 OutputFormat 来实现 </p><h3 id="实现："><a href="#实现：" class="headerlink" title="实现："></a>实现：</h3><p>自定义 OutputFormat，改写其中的 RecordWriter，改写具体输出数据的方法 write() </p><p>以课程考试考试成绩（/in/newScoreIn）为例，现要求把参考次数&gt;=7 的输出到一个文件 /output/out1，然后剩下的不合格的参考输出到另外一个文件/output/out2 </p><h4 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h4><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLl0jpId9BCX9zIyjAIWjyGtYSaZDIm5A0m00"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.outputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MultipleOutputMR</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 指定HDFS相关参数</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://cs1:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"ap"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//  创建/配置 Job</span></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Jar包类型:这里千万别写错了</span></span><br><span class="line">job.setJarByClass(MultipleOutputMR.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Map Reduce执行类</span></span><br><span class="line">job.setMapperClass(MultipleOutputMRMapper.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Map输出类</span></span><br><span class="line">job.setMapOutputKeyClass(Text.class);</span><br><span class="line">job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">////////////// 设置reduce执行个数为0</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">///////////// 设置MapOutputFormatClass</span></span><br><span class="line">job.setOutputFormatClass(MyOutputFormat.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置输入 输出路径</span></span><br><span class="line">String inP = <span class="string">"/in/newScoreIn"</span>;</span><br><span class="line">String outP = <span class="string">"/out/myoutformat/mulWriteSuc"</span>;</span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(inP));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(outP));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置如果存在路径就删除</span></span><br><span class="line">        Path mypath = <span class="keyword">new</span> Path(outP);</span><br><span class="line">        FileSystem hdfs = mypath.getFileSystem(conf);</span><br><span class="line">        <span class="keyword">if</span> (hdfs.exists(mypath)) &#123;</span><br><span class="line">            hdfs.delete(mypath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//  执行job</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="keyword">true</span>)?<span class="number">0</span>:-<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MultipleOutputMRMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value,</span></span></span><br><span class="line"><span class="function"><span class="params">Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">// 参考次数&gt;7次 算合格</span></span><br><span class="line">String[] splits = value.toString().split(<span class="string">","</span>);</span><br><span class="line"><span class="keyword">if</span> (splits.length &gt; <span class="number">9</span>) &#123;</span><br><span class="line">context.write(<span class="keyword">new</span> Text(<span class="string">"1::"</span>+value.toString()), NullWritable.get());</span><br><span class="line">&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">context.write(<span class="keyword">new</span> Text(<span class="string">"2::"</span>+value.toString()), NullWritable.get());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLl2jz2yjAIWjSSiloaqiuN98pKi1IW80"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.outputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyOutputFormat</span> <span class="keyword">extends</span> <span class="title">FileOutputFormat</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> RecordWriter&lt;Text, NullWritable&gt; <span class="title">getRecordWriter</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">TaskAttemptContext job)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">Configuration configuration = job.getConfiguration();</span><br><span class="line">FileSystem fs = FileSystem.get(configuration);</span><br><span class="line"></span><br><span class="line">Path p1 = <span class="keyword">new</span> Path(<span class="string">"/out/myoutformat/out1"</span>);</span><br><span class="line">Path p2 = <span class="keyword">new</span> Path(<span class="string">"/out/myoutformat/out2"</span>);</span><br><span class="line"></span><br><span class="line">FSDataOutputStream out1 = fs.create(p1);</span><br><span class="line">FSDataOutputStream out2 = fs.create(p2);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> MyRecordWriter(out1,out2);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://www.plantuml.com/plantuml/svg/SoWkIImgAStDuKhEIImkLl2j34fDpYzA2I_AB4ajud98pKi1IW80"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.mapreduce.outputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyRecordWriter</span> <span class="keyword">extends</span> <span class="title">RecordWriter</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">FSDataOutputStream fsout = <span class="keyword">null</span>;</span><br><span class="line">FSDataOutputStream fsout1 = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">MyRecordWriter</span><span class="params">(FSDataOutputStream fsout, FSDataOutputStream fsout1)</span> </span>&#123;</span><br><span class="line"><span class="keyword">super</span>();</span><br><span class="line"><span class="keyword">this</span>.fsout = fsout;</span><br><span class="line"><span class="keyword">this</span>.fsout1 = fsout1;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Text key, NullWritable value)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"> String[] strs = key.toString().split(<span class="string">"::"</span>);</span><br><span class="line"> <span class="keyword">if</span> (strs[<span class="number">0</span>].equals(<span class="string">"1"</span>)) &#123;</span><br><span class="line">fsout.write((strs[<span class="number">1</span>]+<span class="string">"\n"</span>).getBytes());</span><br><span class="line">&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">fsout1.write((strs[<span class="number">1</span>]+<span class="string">"\n"</span>).getBytes());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(TaskAttemptContext context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">IOUtils.closeStream(fsout);</span><br><span class="line">IOUtils.closeStream(fsout1);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="12、Yarn"><a href="#12、Yarn" class="headerlink" title="12、Yarn"></a>12、Yarn</h2><h3 id="1、Yarn图示简介"><a href="#1、Yarn图示简介" class="headerlink" title="1、Yarn图示简介"></a>1、Yarn图示简介</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-081744.png" alt="image-20180611161743746"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-081803.png" alt="image-20180611161802886"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-081827.png" alt="image-20180611161827469"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-081902.png" alt="image-20180611161901720"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-081927.png" alt="image-20180611161927641"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-081949.png" alt="image-20180611161948431"></p><h3 id="2、JPS进程观察"><a href="#2、JPS进程观察" class="headerlink" title="2、JPS进程观察"></a>2、JPS进程观察</h3><p>运行程序的时候： </p><ul><li>runjar —— 运行的java Jar包的进程</li><li>MRAppMaster —— </li></ul><h4 id="在Hadoop1-x时，-只有两个主要组件：hdfs（文件存储），-MapReduce（计算）"><a href="#在Hadoop1-x时，-只有两个主要组件：hdfs（文件存储），-MapReduce（计算）" class="headerlink" title="在Hadoop1.x时， 只有两个主要组件：hdfs（文件存储）， MapReduce（计算）"></a><strong>在Hadoop1.x时， 只有两个主要组件：hdfs（文件存储）， MapReduce（计算）</strong></h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-082801.png" alt="image-20180611162800671"></p><p><strong>所有的计算相关的全部放在MapReduce上</strong></p><ul><li><strong>JobTracker</strong>: 整个计算程序的老大<ul><li>资源调度：随机调度</li><li>监控程序运行的状态，启动运行程序</li><li>存在单点故障问题</li></ul></li><li><strong>TaskTracker</strong>：负责计算程序的执行<ul><li>强行的将计算资源分成2部分<ul><li>MapSlot</li><li>ReduceSlot</li></ul></li><li>每一部分资源只能跑对应的任务</li></ul></li><li><strong>缺陷：</strong><ul><li>单点故障</li><li>资源调度随机，会造成资源浪费</li><li>JobTracker的运行压力过大</li></ul></li></ul><p><br></p><h4 id="Hadoop2-x：-分理出Yarn，专门负责集群的资源管理和调度"><a href="#Hadoop2-x：-分理出Yarn，专门负责集群的资源管理和调度" class="headerlink" title="Hadoop2.x： 分理出Yarn，专门负责集群的资源管理和调度"></a><strong>Hadoop2.x： 分理出Yarn，专门负责集群的资源管理和调度</strong></h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-11-082010.png" alt="image-20180611162010610"></p><h5 id="Yarn的进程："><a href="#Yarn的进程：" class="headerlink" title="Yarn的进程："></a>Yarn的进程：</h5><p><strong>ResourceManager:</strong></p><ul><li><p>整个资源调度的老大</p><ul><li>接受hadoop客户端的请求</li><li>接受NodeManager 的状态报告， NM的资源状态和存活状态</li><li>资源调度，整个计算程序的资源调度，调度的运行资源和节点</li></ul></li></ul><ul><li><p><strong>内部组件</strong>： </p><ul><li><strong>ASM</strong>——ApplicationsManager<ul><li>所有应用程序的管理者，负责调度应用程序</li></ul></li><li><strong>Scheduler</strong>——调度器概念<ul><li>调度的是什么时候执行哪个计算程序</li><li><strong>调度器：</strong><ul><li><strong>FIFO</strong>: first in first out<ul><li>先提交的先执行，后提交的后执行</li><li>内部维护一个队列</li></ul></li><li><strong>FAIR</strong>: 公平调度器<ul><li>大家平分资源运行</li><li>假设刚开始只有一个任务，占资源100%，此时又来了一个任务，这是进行资源平分，每人50%</li><li>内部也是维护一个队列</li></ul></li><li><strong>CAPICITPY</strong>: 可以按需进行配置，使用资源<ul><li>内部<strong>可维护多个队列</strong>，多个队列之间可以进行资源分配</li><li>例如：分配两个队列<ul><li>队列1：60%</li><li>队列2：40%</li><li>每个队列中都是执行FIFO的</li></ul></li></ul></li></ul></li></ul></li></ul></li></ul><p><strong>NodeManager：</strong></p><ul><li>负责真正的提供资源，运行计算程序<ul><li>接受ResourceManager的命令</li><li>提供资源运行计算程序</li></ul></li></ul><p><strong><u>MRAppMaster</u></strong>: </p><ul><li>单个计算程序的老大, 类似于项目经理<ul><li>负责帮助当前计算程序向ResourceManager申请资源</li><li>负责启动 MapTask 和 ReduceTask 任务</li></ul></li></ul><p><u><strong>Container:</strong></u></p><ul><li>抽象资源容器，封装这一定的cpu，io 和网络资源（逻辑概念）</li><li>是运行MapTask，ReduceTask等的<strong>运行资源单位</strong></li><li>1个split —— 1个MapTask (ReduceTask) —— 1个Container —— 显示为YarnChild，底层运行的资源单位就是<code>Container</code></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-多-Job-串联&quot;&gt;&lt;a href=&quot;#1-多-Job-串联&quot; class=&quot;headerlink&quot; title=&quot;1.多 Job 串联&quot;&gt;&lt;/a&gt;1.多 Job 串联&lt;/h2&gt;&lt;h3 id=&quot;1-概念&quot;&gt;&lt;a href=&quot;#1-概念&quot; class=&quot;head
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/"/>
    
      <category term="学习笔记" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce笔记-2 三大组件-Partitioner分区,sort排序,Combiner局部分区</title>
    <link href="https://airpoet.github.io/2018/06/07/Hadoop/Study/2-MapReduce/MapReduce%E7%AC%94%E8%AE%B0-2/"/>
    <id>https://airpoet.github.io/2018/06/07/Hadoop/Study/2-MapReduce/MapReduce笔记-2/</id>
    <published>2018-06-07T03:27:55.684Z</published>
    <updated>2018-06-11T11:46:14.096Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Combiner-组件"><a href="#1-Combiner-组件" class="headerlink" title="1.  Combiner 组件"></a>1.  Combiner 组件</h2><h3 id="1-产生缘由："><a href="#1-产生缘由：" class="headerlink" title="1. 产生缘由："></a>1. 产生缘由：</h3><p>Combiner 是 MapReduce 程序中 Mapper 和 Reducer 之外的一种组件，它的作用是在 maptask 之后给 maptask 的结果进行局部汇总，以减轻 reducetask 的计算负载，减少网络传输</p><p><strong>Combiner 组件的作用：</strong></p><ul><li>减少 reduce 端的数据量</li><li>减少 shuffle 过程的数据量</li><li>在 map 端做了一次合并，提高分布式计算程序的整体性能</li></ul><p><strong>Combiner 组件帮 reduce 分担压力， 因此其业务逻辑和 reduce 中的业务逻辑相似</strong></p><h3 id="2-自定义-Combiner-组件："><a href="#2-自定义-Combiner-组件：" class="headerlink" title="2.自定义 Combiner 组件："></a>2.自定义 Combiner 组件：</h3><p>默认情况下没有 Combiner 组件，Combiner 作用时间点 — map–combiner–reduce</p><ol><li><p>继承 Reduce 类</p><ul><li>public class  MyCombiner extends Reducer&lt;前两个： map 的输出， 后两个： reduce 的输入&gt;{}</li><li>我们在写 MapReduce 程序的时候， map 的输出就是 reduce 的输入</li><li>也就是说， 这个 MyCombiner() 的前两个泛型和后两个泛型的类型一致</li></ul></li><li><p>重写 reduce 方法</p><ul><li><p><strong>Combiner 本质上相当于 在 map 端进行了一次 reduce 操作， 通常情况下直接使用 reducer 的类作为 Combiner 的类，不再单独写 Combiner 代码逻辑</strong></p></li><li><p><strong>在 Job 中加上<code>job.setCombinerClass(WorldcountReduce.class)</code>， 就会调用 Combiner</strong></p></li></ul></li></ol><ol start="3"><li><p><strong>Combiner 使用原则</strong></p><ul><li>有或没有都<strong>不能影响业务逻辑</strong>，都<strong>不能影响最终结果</strong>。比如累加，最大值等，求平均值就不能用。</li></ul></li></ol><hr><h2 id="2、MapReduce-中的序列化"><a href="#2、MapReduce-中的序列化" class="headerlink" title="2、MapReduce 中的序列化"></a>2、MapReduce 中的序列化</h2><h3 id="2-1、概述"><a href="#2-1、概述" class="headerlink" title="2.1、概述"></a>2.1、概述</h3><p><strong>Java</strong> 的<strong>序列化</strong>是一个<strong>重量级序列化框架（Serializable）</strong>，一个对象被序列化后，会附带很多额 外的信息（各种校验信息，header，继承体系等），<strong>不便于在网络中高效传输</strong>；</p><p><strong>Hadoop 自己开发了一套序列化机制</strong>（参与序列化的对象的类都要<strong>实现 Writable 接口</strong>），精简，高效</p><h4 id="Java-基本类型-amp-Hadoop-类型对照表"><a href="#Java-基本类型-amp-Hadoop-类型对照表" class="headerlink" title="Java 基本类型 &amp; Hadoop 类型对照表"></a>Java 基本类型 &amp; Hadoop 类型对照表</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Java &amp; Hadoop类型参照</span></span><br><span class="line">hadoop数据类型      &lt;------------&gt;  java数据类型:  </span><br><span class="line">布尔型：  </span><br><span class="line">BooleanWritable     &lt;------------&gt;  <span class="keyword">boolean</span>  </span><br><span class="line">整型：  </span><br><span class="line">ByteWritable        &lt;------------&gt;  <span class="keyword">byte</span>  </span><br><span class="line">ShortWritable       &lt;------------&gt;  <span class="keyword">short</span>  </span><br><span class="line">IntWritable         &lt;------------&gt;  <span class="keyword">int</span>  </span><br><span class="line">LongWritable        &lt;------------&gt;  <span class="keyword">long</span>  </span><br><span class="line">浮点型：  </span><br><span class="line">FloatWritable       &lt;------------&gt;  <span class="keyword">float</span>  </span><br><span class="line">DoubleWritable      &lt;------------&gt;  <span class="keyword">double</span>  </span><br><span class="line">字符串（文本）：  </span><br><span class="line">Text                &lt;------------&gt;  String  </span><br><span class="line">数组：  </span><br><span class="line">ArrayWritable       &lt;------------&gt;  Array  </span><br><span class="line">map集合：  </span><br><span class="line">MapWritable         &lt;------------&gt;  map</span><br></pre></td></tr></table></figure><h3 id="2-2、自定义对象实现-MapReduce-框架的序列化"><a href="#2-2、自定义对象实现-MapReduce-框架的序列化" class="headerlink" title="2.2、自定义对象实现 MapReduce 框架的序列化"></a>2.2、自定义对象实现 MapReduce 框架的序列化</h3><p><strong>要实现<code>WritableComparable</code>接口</strong>，因为 MapReduce 框架中的 shuffle 过程一定会对 key 进行排序</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//序列化方法</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    out.writeUTF(phone);</span><br><span class="line">    out.writeLong(upfFlow);</span><br><span class="line">    out.writeLong(downFlow);</span><br><span class="line">    out.writeLong(sumFlow);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//反序列化方法</span></span><br><span class="line"><span class="comment">//注意： 字段的反序列化顺序与序列化时的顺序保持一致,並且类型也一致</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.phone = in.readUTF();</span><br><span class="line"><span class="keyword">this</span>.upfFlow = in.readLong();</span><br><span class="line"><span class="keyword">this</span>.downFlow = in.readLong();</span><br><span class="line"><span class="keyword">this</span>.sumFlow = in.readLong();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h2 id="3-MapReduce中的Sort-–TODO。。"><a href="#3-MapReduce中的Sort-–TODO。。" class="headerlink" title="3. MapReduce中的Sort –TODO。。"></a>3. MapReduce中的Sort –TODO。。</h2><p>MapTask –&gt; ReduceTask 之间， 框架<strong>默认</strong>添加了排序</p><p>排序的规则是<strong>按照Map 端输出的 key 的字典顺序进行排序</strong></p><h5 id="1、-如果没有重写-WritableComparable-时"><a href="#1、-如果没有重写-WritableComparable-时" class="headerlink" title="1、 如果没有重写 WritableComparable 时"></a>1、 如果没有重写 WritableComparable 时</h5><p> 按单词统计中词频出现的此处进行排序， 按照出现的次数， 从低到高</p><p>如果想要对词频进行排序， 那么词频应该放在 map 输出 key 的位置</p><p><strong>代码实现</strong>：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> Map </span><br><span class="line"><span class="comment">//词频为 key， 其它为 value</span></span><br><span class="line"></span><br><span class="line"> Reduce </span><br><span class="line"><span class="comment">// 将 map 输入的结果反转(k,v 换位置), 输出最终结果</span></span><br><span class="line"><span class="comment">// 最后输出还是按照左边词, 右边次数</span></span><br><span class="line"><span class="comment">// ps： 如果倒序排的时候, map 的时候发的时候 加上-,  reduce 发的时候, 再加上-, 转成 IntWritable</span></span><br></pre></td></tr></table></figure><h5 id="2、自定义排序要实现WritableComparable接口"><a href="#2、自定义排序要实现WritableComparable接口" class="headerlink" title="2、自定义排序要实现WritableComparable接口"></a>2、<strong>自定义排序要实现<code>WritableComparable</code>接口</strong></h5><ul><li><strong>自定义的类必须放在 key 的位置</strong></li><li><strong>实现<code>WritableComparable</code>接口</strong>， 重写 <code>compareTo()</code>方法</li><li>待扩展…</li></ul><blockquote><p> 作业： <strong>增强需求： 按照总流量排序， 总流量相同时， 按照手机号码排序</strong></p></blockquote><hr><h2 id="4、MapReduce-中的数据分发组件-Partitioner（分区）"><a href="#4、MapReduce-中的数据分发组件-Partitioner（分区）" class="headerlink" title="4、MapReduce 中的数据分发组件 Partitioner（分区）"></a>4、MapReduce 中的数据分发组件 <code>Partitioner（分区）</code></h2><p><strong>需求：</strong> 根据归属地<strong>输出</strong>流量统计数据<strong>结果到不同文件</strong>，以便于在查询统计结果时可以定位到 省级范围进行</p><p><strong>思路</strong>：MapReduce 中会将 map 输出的 kv 对，按照相同 key 分组，然后分发给不同的 reducetask</p><p><strong>执行时机</strong>: <strong>在Map输出 kv 对之后, 所携带的 k,v 参数，跟 Map 输出相同</strong></p><p><br></p><h4 id="MapReduce-默认的分发规则为："><a href="#MapReduce-默认的分发规则为：" class="headerlink" title="MapReduce 默认的分发规则为："></a><strong>MapReduce 默认的分发规则为</strong>：</h4><p><strong>根据 <code>key</code> 的 <code>hashcode%reducetask</code> 数来分发</strong>，所以：<strong>如果要按照我们自 己的需求进行分组，则需要改写数据分发（分区）组件 Partitioner</strong></p><p><br></p><h4 id="Partition重点总结："><a href="#Partition重点总结：" class="headerlink" title="Partition重点总结："></a>Partition重点总结：</h4><ul><li><p><strong>Partition 的 key value, 就是Mapper输出的key value</strong></p><p><code>public abstract int getPartition(KEY key, VALUE value, int numPartitions);</code></p><p><strong>输入是Map的结果对&lt;key, value&gt;和Reducer的数目，输出则是分配的Reducer（整数编号）</strong>。<strong>就是指定Mappr输出的键值对到哪一个reducer上去</strong>。系统缺省的Partitioner是HashPartitioner，它以key的Hash值对Reducer的数目取模，得到对应的Reducer。<strong>这样保证如果有相同的key值，肯定被分配到同一个reducre上。如果有N个reducer，编号就为0,1,2,3……(N-1)</strong>。</p></li><li><p>MapReduce 中会将 map 输出的 kv 对，按照相同 key 分组，然后分发给不同的 reducetask 默认的分发规则为:根据 key 的 hashcode%reducetask 数来分发，所以:如果要按照我们自 己的需求进行分组，则需要改写数据分发(分组)组件 Partitioner, 自定义一个 CustomPartitioner 继承抽象类:Partitioner</p></li><li><strong>因此， Partitioner 的执行时机， 是在Map输出 kv 对之后</strong></li></ul><h5 id="Partitioner-实现过程"><a href="#Partitioner-实现过程" class="headerlink" title="Partitioner 实现过程"></a><strong>Partitioner 实现过程</strong></h5><ol><li>先分析一下具体的业务逻辑，确定大概有多少个分区</li><li>首先书写一个类，它要<strong>继承 <code>org.apache.hadoop.mapreduce.Partitioner</code>这个抽象类</strong></li><li><strong>重写<code>public int getPartition</code>这个方法，根据具体逻辑，读数据库或者配置返回相同的数字</strong></li><li><strong>在<code>main</code>方法中设置<code>Partioner</code>的类，<code>job.setPartitionerClass(DataPartitioner.class)</code>;</strong></li><li><strong>设置<code>Reducer</code>的数量，<code>job.setNumReduceTasks(6)</code>;</strong></li></ol><h4 id="典型的-Partitioner-代码实现"><a href="#典型的-Partitioner-代码实现" class="headerlink" title="典型的 Partitioner 代码实现"></a>典型的 Partitioner 代码实现</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProvincePartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> HashMap&lt;String, Integer&gt; provincMap = <span class="keyword">new</span> HashMap&lt;String, Integer&gt;();</span><br><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line">provincMap.put(<span class="string">"138"</span>, <span class="number">0</span>);</span><br><span class="line">provincMap.put(<span class="string">"139"</span>, <span class="number">1</span>);</span><br><span class="line">provincMap.put(<span class="string">"136"</span>, <span class="number">2</span>);</span><br><span class="line">provincMap.put(<span class="string">"137"</span>, <span class="number">3</span>);</span><br><span class="line">provincMap.put(<span class="string">"135"</span>, <span class="number">4</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text key, FlowBean value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">Integer code = provincMap.get(key.toString().substring(<span class="number">0</span>, <span class="number">3</span>));</span><br><span class="line"><span class="keyword">if</span> (code != <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="keyword">return</span> code;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="number">5</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h2 id="5、全局计数器"><a href="#5、全局计数器" class="headerlink" title="5、全局计数器"></a>5、全局计数器</h2><h3 id="1-框架内置计数器："><a href="#1-框架内置计数器：" class="headerlink" title="1.  框架内置计数器："></a>1.  框架内置计数器：</h3><ul><li>Hadoop内置的计数器，主要用来记录作业的执行情况</li><li>内置计数器包括 <strong>MapReduce框架计数器</strong>（Map-Reduce Framework）<ul><li><strong>文件系统计数器（FielSystemCounters）</strong></li><li><strong>作业计数器（Job Counters）</strong></li><li><strong>文件输入格式计数器（File Output Format Counters）</strong></li><li><strong>文件输出格式计数器（File Input Format Counters)</strong></li></ul></li><li>计数器由相关的task进行维护，定期传递给tasktracker，再由tasktracker传给jobtracker；</li><li>最终的作业计数器实际上是有jobtracker维护，所以计数器可以被全局汇总，同时也不必在整个网络中传递</li><li>只有当一个作业执行成功后，最终的计数器的值才是完整可靠的；</li></ul><h3 id="2-自定义的计数器"><a href="#2-自定义的计数器" class="headerlink" title="2. 自定义的计数器"></a>2. 自定义的计数器</h3><h5 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h5><ul><li>用来统计运行过程中的进度和状态， 类似于 job 运行的一个报告、日志</li><li>要将数据处理过程中遇到的不合规数据行进行全局计数，类似这 种需求可以借助 MapReduce 框架中提供的全局计数器来实现</li><li><strong>计数器的值可以在mapper或reducer中增加</strong></li></ul><p><strong>使用方式</strong></p><ol><li><p>定义枚举类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">enum</span> Temperature&#123;  </span><br><span class="line">MISSING,  </span><br><span class="line">TOTAL  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>在map或者reduce中使用计数器 </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.自定义计数器</span></span><br><span class="line">Counter counter = context.getCounter(Temperature.TOTAL);  </span><br><span class="line"><span class="comment">// 2.为计数器赋初始值</span></span><br><span class="line">counter.setValue(<span class="keyword">long</span> value);</span><br><span class="line"><span class="comment">// 3.计数器工作</span></span><br><span class="line">counter.increment(<span class="keyword">long</span> incr);</span><br></pre></td></tr></table></figure></li><li><p>获取计数器</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Counters counters=job.getCounters(); </span><br><span class="line">Counter counter=counters.findCounter(LOG_PROCESSOR_COUNTER.BAD_RECORDS_LONG);<span class="comment">// 查找枚举计数器，假如Enum的变量为BAD_RECORDS_LONG </span></span><br><span class="line"><span class="keyword">long</span> value=counter.getValue();<span class="comment">//获取计数值</span></span><br></pre></td></tr></table></figure></li></ol><h4 id="计数器使用完整代码"><a href="#计数器使用完整代码" class="headerlink" title="计数器使用完整代码"></a>计数器使用完整代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** </span></span><br><span class="line"><span class="comment">* <span class="doctag">@Description</span> 假如一个文件，规范的格式是3个字段，“\t”作为分隔符，其中有2条异常数据，一条数据是只有2个字段，一条数据是有4个字段</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyCounter</span> </span>&#123;</span><br><span class="line">    <span class="comment">// \t键</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String TAB_SEPARATOR = <span class="string">"\t"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyCounterMap</span> <span class="keyword">extends</span></span></span><br><span class="line"><span class="class">            <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">        <span class="comment">// 定义枚举对象</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">enum</span> LOG_PROCESSOR_COUNTER &#123;</span><br><span class="line">            BAD_RECORDS_LONG, BAD_RECORDS_SHORT</span><br><span class="line">        &#125;;</span><br><span class="line">        </span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            String arr_value[] = value.toString().split(TAB_SEPARATOR);</span><br><span class="line">            <span class="keyword">if</span> (arr_value.length &gt; <span class="number">3</span>) &#123;</span><br><span class="line">                <span class="comment">/* 自定义计数器 */</span></span><br><span class="line">                context.getCounter(<span class="string">"ErrorCounter"</span>, <span class="string">"toolong"</span>).increment(<span class="number">1</span>);</span><br><span class="line">                <span class="comment">/* 枚举计数器 */</span></span><br><span class="line">                context.getCounter(LOG_PROCESSOR_COUNTER.BAD_RECORDS_LONG).increment(<span class="number">1</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (arr_value.length &lt; <span class="number">3</span>) &#123;</span><br><span class="line">                <span class="comment">// 自定义计数器</span></span><br><span class="line">                context.getCounter(<span class="string">"ErrorCounter"</span>, <span class="string">"tooshort"</span>).increment(<span class="number">1</span>);</span><br><span class="line">                <span class="comment">// 枚举计数器</span></span><br><span class="line">                context.getCounter(LOG_PROCESSOR_COUNTER.BAD_RECORDS_SHORT).increment(<span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@SuppressWarnings</span>(<span class="string">"deprecation"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException </span>&#123;</span><br><span class="line">        String[] args0 = &#123; </span><br><span class="line">                <span class="string">"hdfs://hadoop2:9000/buaa/counter/counter.txt"</span>,</span><br><span class="line">                <span class="string">"hdfs://hadoop2:9000/buaa/counter/out/"</span> </span><br><span class="line">            &#125;;</span><br><span class="line">        <span class="comment">// 读取配置文件</span></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 如果输出目录存在，则删除</span></span><br><span class="line">        Path mypath = <span class="keyword">new</span> Path(args0[<span class="number">1</span>]);</span><br><span class="line">        FileSystem hdfs = mypath.getFileSystem(conf);</span><br><span class="line">        <span class="keyword">if</span> (hdfs.isDirectory(mypath)) &#123;</span><br><span class="line">            hdfs.delete(mypath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 新建一个任务</span></span><br><span class="line">        Job job = <span class="keyword">new</span> Job(conf, <span class="string">"MyCounter"</span>);</span><br><span class="line">        <span class="comment">// 主类</span></span><br><span class="line">        job.setJarByClass(MyCounter.class);</span><br><span class="line">        <span class="comment">// Mapper</span></span><br><span class="line">        job.setMapperClass(MyCounterMap.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 输入目录</span></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(args0[<span class="number">0</span>]));</span><br><span class="line">        <span class="comment">// 输出目录</span></span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args0[<span class="number">1</span>]));</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 提交任务，并退出</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>注意点：在没有 ReduceTask 的时候，  <code>job.setNumReduceTasks(0);</code></strong></p><p><a href="https://blog.csdn.net/qq_35732963/article/details/53358033" target="_blank" rel="noopener">关于计数器，详情可参考</a></p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-Combiner-组件&quot;&gt;&lt;a href=&quot;#1-Combiner-组件&quot; class=&quot;headerlink&quot; title=&quot;1.  Combiner 组件&quot;&gt;&lt;/a&gt;1.  Combiner 组件&lt;/h2&gt;&lt;h3 id=&quot;1-产生缘由：&quot;&gt;&lt;a href=
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce笔记-1  WordCount, MapReduce运行机制</title>
    <link href="https://airpoet.github.io/2018/06/06/Hadoop/Study/2-MapReduce/MapReduce%E7%AC%94%E8%AE%B0-1/"/>
    <id>https://airpoet.github.io/2018/06/06/Hadoop/Study/2-MapReduce/MapReduce笔记-1/</id>
    <published>2018-06-06T09:41:22.111Z</published>
    <updated>2018-06-11T11:46:15.452Z</updated>
    
    <content type="html"><![CDATA[<p>参考链接:</p><p><a href="https://mubu.com/doc/254d__SRSn" target="_blank" rel="noopener">hdfs 笔记</a> </p><p><a href="https://mubu.com/doc/1BuHQkjk0G" target="_blank" rel="noopener">mapreduce 笔记</a></p><h1 id="1、MapReduce-入门"><a href="#1、MapReduce-入门" class="headerlink" title="1、MapReduce 入门"></a>1、MapReduce 入门</h1><h3 id="1-1、MapReduce概念"><a href="#1-1、MapReduce概念" class="headerlink" title="1.1、MapReduce概念"></a>1.1、MapReduce概念</h3><p><strong>hadoop 的四大组件：</strong></p><ul><li><strong>HDFS</strong>：分布式存储系统</li><li><strong>MapReduce</strong>：分布式计算系统</li><li><strong>YARN</strong>：hadoop 的资源调度系统</li><li><strong>Common</strong>：以上三大组件的底层支撑组件，主要提供基础工具包和 RPC 框架等</li></ul><p>MapReduce 是一个分布式运算程序的编程框架，是用户开发“<strong>基于 Hadoop 的数据分析应用</strong>” 的核心框架</p><p>MapReduce <strong>核心功能</strong> ：<strong>将用户编写的业务逻辑代码</strong>和<strong>自带默认组件</strong>整合成一个完整的<strong>分布 式运算程序</strong>，<strong>并发运行</strong>在一个 Hadoop <strong>集群</strong>上</p><h3 id="1-2、为什么需要-MapReduce？"><a href="#1-2、为什么需要-MapReduce？" class="headerlink" title="1.2、为什么需要 MapReduce？"></a>1.2、为什么需要 MapReduce？</h3><p>引入 MapReduce 框架后，<strong>开发人员</strong>可以将绝大部分工作<strong>集中在业务逻辑</strong>的开发上，而将 <strong>分布式计算中的复杂性交由框架来处理</strong></p><p>Hadoop 当中的 <strong>MapReduce</strong> <strong>分布式程序运算框架</strong>的<strong>整体结构</strong>如下：</p><blockquote><p><strong>MRAppMaster</strong>：MapReduce Application Master，分配任务，协调任务的运行</p><p><strong>MapTask</strong>：阶段并发任，负责 mapper 阶段的任务处理</p><p>YARNChild</p><p><strong>ReduceTask</strong>：阶段汇总任务，负责 reducer 阶段的任务处理</p><p>YARNChild</p></blockquote><h3 id="1-3、MapReduce-的编写规范"><a href="#1-3、MapReduce-的编写规范" class="headerlink" title="1.3、MapReduce 的编写规范"></a>1.3、MapReduce 的编写规范</h3><p>MapReduce 程序编写规范：</p><ol><li>用户编写的程序分成<strong>三个部分</strong>：<strong>Mapper，Reducer，Driver</strong>(提交运行 MR 程序的客户端)</li><li>Mapper 的输入数据是 KV 对的形式（KV 的类型可自定义）</li><li>Mapper 的输出数据是 KV 对的形式（KV 的类型可自定义）</li><li>Mapper 中的业务逻辑写在 map()方法中</li><li>map()方法（maptask 进程）对每一个&lt;K,V&gt;调用一次</li><li>Reducer 的输入数据类型对应 Mapper 的输出数据类型，也是 KV 对的形式</li><li>Reducer 的业务逻辑写在 reduce()方法中</li><li>Reducetask 进程对每一组相同 k 的&lt;K,V&gt;组调用一次 reduce()方法</li><li>用户自定义的 Mapper 和 Reducer 都要继承各自的父类</li><li>整个程序需要一个 Drvier 来进行提交，提交的是一个描述了各种必要信息的 job 对象</li></ol><h3 id="1-4、WordCount-程序"><a href="#1-4、WordCount-程序" class="headerlink" title="1.4、WordCount 程序"></a>1.4、WordCount 程序</h3><h4 id="1、业务逻辑"><a href="#1、业务逻辑" class="headerlink" title="1、业务逻辑"></a>1、业务逻辑</h4><ol><li><p>maptask阶段处理每个数据分块的单词统计分析，思路是每遇到一个单词则把其转换成一个 key-value对，比如单词  hello，就转换成&lt;’hello’,1&gt;发送给 reducetask去汇总</p></li><li><p>reducetask阶段将接受  maptask的结果，来做汇总计数</p></li></ol><h4 id="2、具体代码实现"><a href="#2、具体代码实现" class="headerlink" title="2、具体代码实现"></a>2、具体代码实现</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"> Map </span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">// 计算任务代码：切割单词，输出每个单词计 1 的 key-value 对</span></span><br><span class="line">String[] words = value.toString().split(<span class="string">" "</span>);</span><br><span class="line"><span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">context.write(<span class="keyword">new</span> Text(word), <span class="keyword">new</span> IntWritable(<span class="number">1</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> Reduce </span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">// 汇总计算代码：对每个 key 相同的一组 key-value 做汇总统计</span></span><br><span class="line"><span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (IntWritable v : values) &#123;</span><br><span class="line">sum += v.get();</span><br><span class="line">&#125;</span><br><span class="line">context.write(key, <span class="keyword">new</span> IntWritable(sum));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> main </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 指定 hdfs 相关的参数</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://hadoop02:9000"</span>);</span><br><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"hadoop"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 新建一个 job 任务</span></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置 jar 包所在路径</span></span><br><span class="line">job.setJarByClass(WordCountMR.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 mapper 类和 reducer 类</span></span><br><span class="line">job.setMapperClass(WordCountMapper.class);</span><br><span class="line">job.setReducerClass(WordCountReducer.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 maptask 的输出类型</span></span><br><span class="line">job.setMapOutputKeyClass(Text.class);</span><br><span class="line">job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 reducetask 的输出类型</span></span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定该 mapreduce 程序数据的输入和输出路径</span></span><br><span class="line">Path inputPath = <span class="keyword">new</span> Path(<span class="string">"/wordcount/input"</span>);</span><br><span class="line">Path outputPath = <span class="keyword">new</span> Path(<span class="string">"/wordcount/output"</span>);</span><br><span class="line">FileInputFormat.setInputPaths(job, inputPath);</span><br><span class="line">FileOutputFormat.setOutputPath(job, outputPath);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 最后提交任务</span></span><br><span class="line"><span class="keyword">boolean</span> waitForCompletion = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">System.exit(waitForCompletion?<span class="number">0</span>:<span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="2、MapReduce-程序的核心运行机制"><a href="#2、MapReduce-程序的核心运行机制" class="headerlink" title="2、MapReduce 程序的核心运行机制"></a>2、MapReduce 程序的核心运行机制</h1><h3 id="2-1、概述"><a href="#2-1、概述" class="headerlink" title="2.1、概述"></a>2.1、概述</h3><p>一个完整的 MapReduce 程序在分布式运行时有两类实例进程：</p><ol><li>MRAppMaster：负责整个程序的过程调度及状态协调</li><li>Yarnchild：负责 map 阶段的整个数据处理流程</li><li>Yarnchild：负责 reduce 阶段的整个数据处理流程</li></ol><p>以上两个阶段 MapTask 和 ReduceTask 的进程都是 YarnChild ， 并不是说这 MapTask 和 ReduceTask 就跑在同一个 YarnChild 进行里</p><h3 id="2-2、MapReduce-程序的运行流程"><a href="#2-2、MapReduce-程序的运行流程" class="headerlink" title="2.2、MapReduce 程序的运行流程"></a>2.2、MapReduce 程序的运行流程</h3><ol><li>一个 mr 程序启动的时候，最先启动的是 MRAppMaster，MRAppMaster 启动后根据本次 job 的描述信息，计算出需要的 maptask 实例数量，然后向集群申请机器启动相应数量的 maptask 进程</li><li>maptask 进程启动之后，根据给定的数据切片(哪个文件的哪个偏移量范围)范围进行数 据处理，主体流程为：<ul><li>利用客户指定的 InputFormat 来获取 RecordReader 读取数据，形成输入 KV 对</li><li>将输入 KV 对传递给客户定义的 map()方法，做逻辑运算，并将 map()方法输出的 KV 对收 集到缓存</li><li>将缓存中的 KV 对按照 K 分区排序后不断溢写到磁盘文件</li></ul></li><li>MRAppMaster 监控到所有 maptask 进程任务完成之后（真实情况是，某些 maptask 进 程处理完成后，就会开始启动 reducetask 去已完成的 maptask 处 fetch 数据），会根据客户指 定的参数启动相应数量的 reducetask 进程，并告知 reducetask 进程要处理的数据范围（数据 分区）</li><li>Reducetask 进程启动之后，根据 MRAppMaster 告知的待处理数据所在位置，从若干台 maptask 运行所在机器上获取到若干个 maptask 输出结果文件，并在本地进行重新<strong>归并排序</strong>， 然后按照相同 key 的 KV 为一个组，调用客户定义的 reduce()方法进行逻辑运算，并收集运 算输出的结果 KV，然后调用客户指定的 OutputFormat 将结果数据输出到外部存储</li></ol><h3 id="2-3、MapTask-并行度决定机制"><a href="#2-3、MapTask-并行度决定机制" class="headerlink" title="2.3、MapTask 并行度决定机制"></a>2.3、MapTask 并行度决定机制</h3><p><strong>将待处理数据执行逻辑切片（即按照一个特定切片大小，将待处理数据划分成逻辑上的多 个 split），然后每一个 split 分配一个 mapTask 并行实例处理</strong>。</p><p>这段逻辑及形成的切片规划描述文件，是由 FileInputFormat 实现类的 getSplits()方法完成的。 该方法返回的是 List<inputsplit>，InputSplit 封装了每一个逻辑切片的信息，包括长度和位置 信息，而 getSplits()方法返回一组 InputSplit。</inputsplit></p><h3 id="2-4、切片机制"><a href="#2-4、切片机制" class="headerlink" title="2.4、切片机制"></a>2.4、切片机制</h3><h4 id="FileInputFormat-中默认的切片机制"><a href="#FileInputFormat-中默认的切片机制" class="headerlink" title="FileInputFormat 中默认的切片机制"></a><strong>FileInputFormat 中默认的切片机制</strong></h4><ol><li>简单地按照文件的内容长度进行切片</li><li>切片大小，默认等于 block 大小</li><li>切片时不考虑数据集整体，而是逐个针对每一个文件单独切片</li></ol><blockquote><p>比如待处理数据有两个文件：</p><p>File1.txt    200M </p><p>File2.txt    100M </p><p>经过 getSplits()方法处理之后，形成的切片信息是：</p><p>File1.txt-split1    0-128M</p><p>File1.txt-split2    129M-200M</p><p>File2.txt-split1    0-100M</p></blockquote><h4 id="FileInputFormat-中切片的大小的参数配置"><a href="#FileInputFormat-中切片的大小的参数配置" class="headerlink" title="FileInputFormat 中切片的大小的参数配置"></a>FileInputFormat 中切片的大小的参数配置</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 通过分析源码，在 FileInputFormat 中，计算切片大小的逻辑：</span></span><br><span class="line"><span class="keyword">long</span> splitSize = computeSplitSize(blockSize, minSize, maxSize)，翻译一下就是求这三个值的中 间值</span><br><span class="line"></span><br><span class="line"><span class="comment">// 切片主要由这几个值来运算决定：</span></span><br><span class="line">blocksize：默认是 <span class="number">128</span>M，可通过 dfs.blocksize 修改</span><br><span class="line">minSize：默认是 <span class="number">1</span>，可通过 mapreduce.input.fileinputformat.split.minsize 修改</span><br><span class="line">maxsize：默认是 Long.MaxValue，可通过 mapreduce.input.fileinputformat.split.maxsize 修改</span><br><span class="line"></span><br><span class="line"><span class="comment">//因此，如果 maxsize 调的比 blocksize 小，则切片会小于 blocksize;  如果 minsize 调的比 blocksize 大，则切片会大于 blocksize</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 但是，不论怎么调参数，都不能让多个小文件“划入”一个 split</span></span><br></pre></td></tr></table></figure><h3 id="2-5、MapTask-并行度经验之谈"><a href="#2-5、MapTask-并行度经验之谈" class="headerlink" title="2.5、MapTask 并行度经验之谈"></a>2.5、MapTask 并行度经验之谈</h3><p>如果硬件配置为 2*12core + 64G，恰当的 map 并行度是大约每个节点 20-100 个 map，最好 每个 map 的执行时间至少一分钟。</p><ol><li>如果 job 的每个 map 或者 reduce task 的运行时间都只有 30-40 秒钟，那么就减少该 job 的 map 或者 reduce 数，每一个 task(map|reduce)的 setup 和加入到调度器中进行调度，这个 中间的过程可能都要花费几秒钟，所以如果每个 task 都非常快就跑完了，就会在 task 的开 始和结束的时候浪费太多的时间。</li></ol><p><strong>配置 task 的 JVM 重用</strong>可以改善该问题：</p><ul><li><strong>mapred.job.reuse.jvm.num.tasks</strong>，默认是 1，表示一个 JVM 上最多可以顺序执行的 task 数目（属于同一个 Job）是 1。也就是说一个 task 启一个 JVM。</li><li>这个值可以在 <strong>mapred-site.xml</strong> 中进行更改，当设置成多个，就意味着这多个 task 运行在同一个 JVM 上，但不是同时执行， 是排队顺序执行</li></ul><ol start="2"><li>如果 input 的<strong>文件非常的大</strong>，比如 1TB，<strong>可以考虑</strong>将 hdfs 上的每个 <strong>blocksize 设大</strong>，比如 设成 256MB 或者 512MB</li></ol><h3 id="2-6、ReduceTask-并行度决定机制"><a href="#2-6、ReduceTask-并行度决定机制" class="headerlink" title="2.6、ReduceTask 并行度决定机制"></a>2.6、ReduceTask 并行度决定机制</h3><p><strong>reducetask</strong> 的并行度同样影响整个 job 的执行并发度和执行效率，但与 maptask 的并发数由 切片数决定不同，Reducetask 数量的决定是可以直接手动设置：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置 ReduceTask 的并行度</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">4</span>);</span><br></pre></td></tr></table></figure><p>默认值是 1，</p><p>手动设置为 4，表示运行 4 个 reduceTask，</p><p>设置为 0，表示不运行 reduceTask 任务，也就是没有 reducer 阶段，只有 mapper 阶段</p><p><br></p><p>如果<strong>数据分布不均匀</strong>，就有<strong>可能在 reduce 阶段产生数据倾斜</strong></p><p>注意：reducetask 数量并不是任意设置，还要<strong>考虑业务逻辑需求</strong>，有些情况下，需要计算<strong>全局汇总结果</strong>，就<strong>只能有 1 个 reducetask</strong></p><p>尽量不要运行太多的 reducetask。对大多数 job 来说，最好 rduce 的个数最多和集群中的 reduce 持平，或者比集群的 reduce slots 小。这个对于小集群而言，尤其重要。</p><p><strong>最好的ReduceTask 个数是：datanode 个数 *  0.75~0.95 左右</strong></p><hr><h1 id="3-昨日复习"><a href="#3-昨日复习" class="headerlink" title="3. 昨日复习"></a>3. 昨日复习</h1><p>1.MapReduce 的 wc 编程</p><ul><li>手写代码<ul><li>Mapper</li><li>Reducer</li><li>Driver</li></ul></li></ul><p>2.MapTask 的并行度</p><ul><li>在程序执行的时候运行的 maptask 的总个数</li></ul><p>3.ReduceTask的并行度问题</p><ul><li>ReduceTask 的并行度设置依赖于自己传入的参数</li><li>一般经验： ReduceTask 的个数应该 = datanode 的阶段数 * （0.75~0.95）</li><li>ReduceTask 在设置的时候的并行度有一定的瓶颈</li><li>分区： 决定 ReduceTask 中的数据怎么分配的<ul><li>默认分区方式</li><li>自定义分区</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;参考链接:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://mubu.com/doc/254d__SRSn&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;hdfs 笔记&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://mubu.com/doc/1
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/categories/Hadoop/MapReduce/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MapReduce" scheme="https://airpoet.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>▍Do Not Go Gentle into That Good Night</title>
    <link href="https://airpoet.github.io/2018/06/04/Poetry/%E4%B8%8D%E8%A6%81%E6%B8%A9%E5%92%8C%E7%9A%84%E8%B5%B0%E8%BF%9B%E9%82%A3%E4%B8%AA%E8%89%AF%E5%A4%9C/"/>
    <id>https://airpoet.github.io/2018/06/04/Poetry/不要温和的走进那个良夜/</id>
    <published>2018-06-03T16:38:57.809Z</published>
    <updated>2018-06-09T17:19:23.133Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-03-521528043568_.pic.jpg" alt=""></p><p> <br></p><p>Do not go gentle into that good night,</p><p>Old age should burn and rave at close of the day;</p><p>Rage, rage against the dying of the light.</p><p><br></p><p>Though wise men at their end know dark is right,</p><p>Because their words had forked no lightning they</p><p>Do not go gentle into that good night.</p><p> <br></p><p>Good men, the last wave by, crying how bright</p><p>Their frail deeds might have danced in a green bay,</p><p>Rage, rage against the dying of the light.</p><p><br></p><p>Wild men, who caught and sang the sun in flight,</p><p>And learn, too late, they grieved it on its way,</p><p>Do not go gentle into that good night.</p><p><br></p><p>Grave men, near death, who see with blinding sight</p><p>Blind eyes could blaze like meteors and be gay,</p><p>Rage, rage against the dying of the light.</p><p><br></p><p>And you, my father, there on the sad height,</p><p>Curse, bless, me now with your fierce tears, I pray.</p><p>Do not go gentle into that good night.</p><p>Rage, rage against the dying of the light.</p><p><br></p><hr><p>  <br></p><p>《不要温和地走进那个良夜》 -巫宁坤译本</p><p>​    <br></p><p>不要温和地走进那个良夜，</p><p>老年应当在日暮时燃烧咆哮；</p><p>怒斥，怒斥光明的消逝。</p><p>  <br></p><p>虽然智慧的人临终时懂得黑暗有理，</p><p>因为他们的话没有进发出闪电，他们</p><p>也并不温和地走进那个良夜。</p><p><br>  </p><p>善良的人，当最后一浪过去，高呼他们脆弱的善行</p><p>可能曾会多么光辉地在绿色的海湾里舞蹈，</p><p>怒斥，怒斥光明的消逝。</p><p>狂暴的人抓住并歌唱过翱翔的太阳，</p><p>懂得，但为时太晚，他们使太阳在途中悲伤，</p><p>也并不温和地走进那个良夜。</p><p>  <br></p><p>严肃的人，接近死亡，用炫目的视觉看出</p><p>失明的跟睛可以像流星一样闪耀欢欣，</p><p>怒斥，恕斥光明的消逝。</p><p>  <br></p><p>您啊，我的父亲，在那悲哀的高处。</p><p>现在用您的热泪诅咒我，祝福我吧。我求您</p><p>不要温和地走进那个良夜。</p><p>怒斥，怒斥光明的消逝</p><p><br></p><hr><p>《不要温顺地走入那长夜》 -和菜头译本</p><p><br></p><p>白日将尽，暮年仍应燃烧咆哮</p><p>狂怒吧，狂怒吧！</p><p>对抗着光明渐逝</p><p><br></p><p>虽然智者深知</p><p>人之将死，黑暗自有其时</p><p>只因他们所言未曾裂天如电</p><p>他们不要温顺地走入那长夜</p><p><br></p><p>随着最后一浪，善人在哭喊</p><p>哭喊那脆弱的善行</p><p>它本应何其欢快</p><p>在绿色峡湾里起舞</p><p>狂怒吧，狂怒吧！</p><p>对抗着光明渐逝。</p><p><br></p><p>狂人曾抓住飞驰的太阳</p><p>放声歌唱</p><p>太晚，他们才感到其中的伤感</p><p>不要温顺地走进那长夜</p><p><br></p><p>严肃的人行将死去时</p><p>用那渐渐失神的目光去看</p><p>盲瞳却如流星璀璨，欢欣溢满</p><p>狂怒吧，狂怒吧！</p><p>对抗着光明渐逝</p><p><br></p><p>还有你啊，我的父亲，远在悲伤的高地</p><p>我恳请你现在</p><p>就让你诅咒，你的祝福</p><p>随着热泪落下</p><p>不要温顺地走进那长夜</p><p>狂怒吧，狂怒吧！</p><p>对抗这光明渐逝</p><p><br></p><hr><p>​       《 绝不向黑夜请安》  -高晓松译本</p><p>绝不向黑夜请安</p><p>老朽请于白日尽头涅槃</p><p>咆哮于光之消散</p><p><br></p><p>先哲虽败于幽暗</p><p>诗歌终不能将苍穹点燃</p><p>绝不向黑夜请安</p><p><br></p><p>贤者舞蹈于碧湾</p><p>为惊涛淹没的善行哭喊</p><p>咆哮于光之消散</p><p><br></p><p>狂者如夸父逐日</p><p>高歌中顿觉迟来的伤感</p><p>绝不向黑夜请安</p><p><br></p><p>逝者于临终迷幻</p><p>盲瞳怒放出流星的灿烂</p><p>咆哮于光之消散</p><p><br></p><p>那么您，我垂垂将死的父亲</p><p>请掬最后一捧热泪降临</p><p>请诅咒，请保佑</p><p>我祈愿，绝不向</p><p>黑夜请安，咆哮</p><p>于光之消散</p><p><br> </p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-03-521528043568_.pic.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt; 
      
    
    </summary>
    
      <category term="文艺" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/"/>
    
      <category term="诗歌" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/%E8%AF%97%E6%AD%8C/"/>
    
    
      <category term="诗歌" scheme="https://airpoet.github.io/tags/%E8%AF%97%E6%AD%8C/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop学习笔记-3 HDFS 原理剖析</title>
    <link href="https://airpoet.github.io/2018/06/03/Hadoop/Study/1-HDFS/HDFS-2-%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90/"/>
    <id>https://airpoet.github.io/2018/06/03/Hadoop/Study/1-HDFS/HDFS-2-原理剖析/</id>
    <published>2018-06-03T09:12:34.396Z</published>
    <updated>2018-06-11T11:45:23.066Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-HDFS体系结构"><a href="#1-HDFS体系结构" class="headerlink" title="1.  HDFS体系结构"></a>1.  HDFS体系结构</h2><p>主从。。。</p><p><br></p><h2 id="2-NameNode"><a href="#2-NameNode" class="headerlink" title="2.NameNode"></a>2.NameNode</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><ul><li><p>[x] 是整个文件系统的管理节点。它维护着整个文件系统的文件目录树，文件/目录的元信息和每个文件对应的数据块列表。接收用户的操作请求。 </p></li><li><p>[x] <strong>在<code>hdfs-site.xml</code>中的<code>dfs.namenode.name.dir</code>属性</strong> </p></li><li><p>[x] 文件包括： </p><ul><li><p>[x] 文件包括:</p><p><strong>①fsimage</strong>:元数据镜像文件。存储某一时段<code>NameNode</code>内存元数据信息。</p><p><strong>②edits</strong>:操作日志文件。</p><p><strong>③fstime</strong>:保存最近一次<code>checkpoint</code>的时间</p><p><strong>以上这些文件是保存在linux的文件系统中。</strong></p></li></ul></li></ul><h3 id="查看-fsimage-和-edits的内容"><a href="#查看-fsimage-和-edits的内容" class="headerlink" title="查看 fsimage 和 edits的内容"></a>查看 <code>fsimage</code> 和 <code>edits</code>的内容</h3><ol><li><p>查看 <code>NameNode中</code> <code>fsimage</code> 的内容 </p><ul><li><p>查看 <strong>fsimage镜像文件</strong>内容<code>Usage: bin/hdfs oiv [OPTIONS] -i INPUTFILE -o  OUTPUTFILE</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以知道数据存在那个哪个 fsimage 镜像中</span></span><br><span class="line">------------------------------------</span><br><span class="line"><span class="comment"># 使用离线的查看器 输出到网页查看</span></span><br><span class="line">oiv -i hadoopdata/namenode/current/fsimage_0000000000000000250 -o 0000000000000000250</span><br><span class="line"></span><br><span class="line"><span class="comment"># 出现这样的提示</span></span><br><span class="line">INFO offlineImageViewer.WebImageViewer: WebImageViewer started. Listening on /127.0.0.1:5978. Press Ctrl+C to stop the viewer.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 另起一个窗口查看</span></span><br><span class="line">hadoop fs -ls -R webhdfs://127.0.0.1:5978</span><br><span class="line">------------------------------------</span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以导出到 xml 文件</span></span><br><span class="line">bin/hdfs oiv -p XML -i  tmp/dfs/name/current/fsimage_0000000000000000055 -o fsimage.xml</span><br></pre></td></tr></table></figure></li></ul></li></ol><ol start="2"><li><p>查看<code>edits</code>文件， 也可以导出到 xml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看edtis内容</span></span><br><span class="line">bin/hdfs oev -i tmp/dfs/name/current/edits_0000000000000000057-0000000000000000186   -o edits.xml</span><br></pre></td></tr></table></figure></li></ol><p><br></p><h2 id="3-Datanode"><a href="#3-Datanode" class="headerlink" title="3.  Datanode"></a>3.  Datanode</h2><p><strong>提供真实文件数据的存储服务</strong></p><ul><li><p><strong>Datanode 节点的数据切块存储位置</strong></p><ul><li><p><code>~/hadoopdata/datanode/current/BP-1070660005-192.168.170.131-1527865615372/current/finalized/subdir0/subdir0</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[ap@cs2]~/hadoopdata/datanode/current/BP-1070660005-192.168.170.131-1527865615372/current/finalized/subdir0/subdir0% ll</span><br><span class="line">总用量 213340</span><br><span class="line">-rw-r--r-- 1 ap ap 134217728 6月   2 13:35 blk_1073741842</span><br><span class="line">-rw-r--r-- 1 ap ap   1048583 6月   2 13:35 blk_1073741842_1018.meta</span><br><span class="line">-rw-r--r-- 1 ap ap  82527955 6月   2 13:35 blk_1073741843</span><br><span class="line">-rw-r--r-- 1 ap ap    644759 6月   2 13:35 blk_1073741843_1019.meta</span><br><span class="line">-rw-r--r-- 1 ap ap        13 6月   3 02:12 blk_1073741850</span><br><span class="line">-rw-r--r-- 1 ap ap        11 6月   3 02:12 blk_1073741850_1028.meta</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p>文件块（block）：最基本的存储单位。对于文件内容而言，一个文件的长度大小是size，那么从文件的０偏移开始，按照固定的大小，顺序对文件进行划分并编号，划分好的每一个块称一个Block。HDFS默认Block大小是128MB，以一个256MB文件，共有256/128=2个Block.</p></li><li><p>不同于普通文件系统的是，<strong>HDFS中，如果一个文件小于一个数据块的大小，并不占用整个数据块存储空间, 按文件大小的实际容量存储</strong></p></li><li><p>Replication。多复本。默认是三个。</p><ul><li><code>hdfs-site.xml</code>的<code>dfs.replication</code>属性 </li><li>手动设置某个文件的副本数为3个<ul><li><code>bin/hdfs dfs -setrep 3 /a.txt</code></li></ul></li></ul></li></ul><h2 id="4-数据存储：-写文件解析"><a href="#4-数据存储：-写文件解析" class="headerlink" title="4.  数据存储： 写文件解析"></a>4.  数据存储： 写文件解析</h2><ul><li><p>[x] <strong>疑点</strong>： HDFS client上传数据到HDFS时，会<strong>首先在本地缓存数据</strong>，当<strong>数据达到一个block大小时，请求NameNode分配一个block。</strong>NameNode会把block所在的DataNode的地址告诉HDFS client。HDFS client会直接和DataNode通信，把数据写到DataNode节点一个block文件中。 </p><blockquote><p>问题： 如果一直写的数据都没有达到一个 block 大小， 那怎么存储？？</p></blockquote></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-03-125641.png" alt="image-20180603205640605"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-03-125412.png" alt="image-20180603205412264"></p><h3 id="写文件的过程："><a href="#写文件的过程：" class="headerlink" title="写文件的过程："></a>写文件的过程：</h3><ol><li>首先调用FileSystem对象的open方法，其实是一个DistributedFileSystem的实例</li><li>DistributedFileSystem通过rpc获得文件的第一批个block的locations，同一block按照重复数会返回多个locations，这些locations按照hadoop拓扑结构排序，距离客户端近的排在前面.</li><li>前两步会返回一个FSDataInputStream对象，该对象会被封装成DFSInputStream对象，DFSInputStream可以方便的管理datanode和namenode数据流。客户端调用read方法，DFSInputStream最会找出离客户端最近的datanode并连接。</li><li>数据从datanode源源不断的流向客户端。</li><li>如果第一块的数据读完了，就会关闭指向第一块的datanode连接，接着读取下一块。这些操作对客户端来说是透明的，客户端的角度看来只是读一个持续不断的流。</li><li>如果第一批block都读完了，DFSInputStream就会去namenode拿下一批blocks的location，然后继续读，如果所有的块都读完，这时就会关闭掉所有的流。</li></ol><blockquote><p> 如果在读数据的时候，DFSInputStream和datanode的通讯发生异常，就会尝试正在读的block的排第二近的datanode,并且会记录哪个datanode发生错误，剩余的blocks读的时候就会直接跳过该datanode。DFSInputStream也会检查block数据校验和，如果发现一个坏的block,就会先报告到namenode节点，然后DFSInputStream在其他的datanode上读该block的镜像</p></blockquote><blockquote><p>该设计的方向就是客户端直接连接datanode来检索数据并且namenode来负责为每一个block提供最优的datanode，namenode仅仅处理block location的请求，这些信息都加载在namenode的内存中，hdfs通过datanode集群可以承受大量客户端的并发访问。</p></blockquote><p><br></p><h2 id="5-数据存储：-读文件解析"><a href="#5-数据存储：-读文件解析" class="headerlink" title="5.  数据存储： 读文件解析"></a>5.  数据存储： 读文件解析</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-03-125556.png" alt="image-20180603205556077"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-03-125203.png" alt="image-20180603205203151"></p><h3 id="读文件的过程"><a href="#读文件的过程" class="headerlink" title="读文件的过程"></a>读文件的过程</h3><ol><li>首先调用FileSystem对象的open方法，其实是一个DistributedFileSystem的实例</li><li>DistributedFileSystem通过rpc获得文件的第一批个block的locations，同一block按照重复数会返回多个locations，这些locations按照hadoop拓扑结构排序，距离客户端近的排在前面.</li><li>前两步会返回一个FSDataInputStream对象，该对象会被封装成DFSInputStream对象，DFSInputStream可以方便的管理datanode和namenode数据流。客户端调用read方法，DFSInputStream最会找出离客户端最近的datanode并连接。</li><li>数据从datanode源源不断的流向客户端。</li><li>如果第一块的数据读完了，就会关闭指向第一块的datanode连接，接着读取下一块。这些操作对客户端来说是透明的，客户端的角度看来只是读一个持续不断的流。</li><li>如果第一批block都读完了，DFSInputStream就会去namenode拿下一批blocks的location，然后继续读，如果所有的块都读完，这时就会关闭掉所有的流。</li></ol><blockquote><p> 如果在读数据的时候，DFSInputStream和datanode的通讯发生异常，就会尝试正在读的block的排第二近的datanode,并且会记录哪个datanode发生错误，剩余的blocks读的时候就会直接跳过该datanode。DFSInputStream也会检查block数据校验和，如果发现一个坏的block,就会先报告到namenode节点，然后DFSInputStream在其他的datanode上读该block的镜像</p></blockquote><blockquote><p>该设计的方向就是客户端直接连接datanode来检索数据并且namenode来负责为每一个block提供最优的datanode，namenode仅仅处理block location的请求，这些信息都加载在namenode的内存中，hdfs通过datanode集群可以承受大量客户端的并发访问。</p></blockquote><p><br></p><h2 id="6-Hadoop-Archives-（HAR-files）"><a href="#6-Hadoop-Archives-（HAR-files）" class="headerlink" title="6.Hadoop Archives （HAR files）"></a>6.Hadoop Archives （HAR files）</h2><p><strong>Hadoop Archives (HAR files)</strong>是在0.18.0版本中引入的，它的出现就是为了<strong>缓解大量小文件消耗namenode内存</strong>的问题。HAR文件是<strong>通过在HDFS上构建一个层次化的文件系统</strong>来工作。一个HAR文件是<strong>通过hadoop的archive命令来创建</strong>，而这个命令<strong>实 际上也是运行了一个MapReduce任务来将小文件打包成HAR</strong>。对于client端来说，使用HAR文件没有任何影响。所有的原始文件都 （using har://URL）。但在HDFS端它内部的文件数减少了。</p><p>通过HAR来读取一个文件并不会比直接从HDFS中读取文件高效，而且实际上可能还会稍微低效一点，因为对每一个HAR文件的访问都需要完成两层 index文件的读取和文件本身数据的读取。并且尽管HAR文件可以被用来作为MapReduce job的input，但是并没有特殊的方法来使maps将HAR文件中打包的文件当作一个HDFS文件处理。</p><p><strong>打包出来的 har 文件在<code>xxx.har/part-0</code>  中， contentz-size 跟原来的文件总大小一样</strong></p><p><strong>创建文件</strong> <code>hadoop archive -archiveName xxx.har -p  /src  /dest</code><br><strong>查看内容</strong> <code>hadoop fs -lsr har:///dest/xxx.har</code> 可以原封不动的显示出来</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打包成 har</span></span><br><span class="line">hadoop archive -archiveName test.har -p /user/<span class="built_in">test</span> /</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看har 文件</span></span><br><span class="line">[ap@cs1]~% hadoop fs -count /test.har/part-0</span><br><span class="line">           0(目录数)            1(文件数)         72(文件大小)    /test.har/part-0 (文件名)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看打包前文件</span></span><br><span class="line">[ap@cs1]~% hadoop fs -count /user/<span class="built_in">test</span></span><br><span class="line">           1            2                 72 /user/<span class="built_in">test</span></span><br><span class="line">           </span><br><span class="line"><span class="comment"># 查看 har 文件， 把打包前的原本文件都显示出来了</span></span><br><span class="line">[ap@cs1]~% hadoop fs -ls -R har:///test.har</span><br><span class="line">-rw-r--r--   3 ap supergroup         50 2018-06-03 04:24 har:///test.har/a.txt</span><br><span class="line">-rw-r--r--   3 ap supergroup         22 2018-06-03 04:24 har:///test.har/b.txt</span><br></pre></td></tr></table></figure><p><strong>注意点：</strong></p><ul><li><strong>存储层面：</strong>为了解决小文件过多导致的 Namenode 压力过大问题， 把很多小文件打包成一个 har 文件。<ul><li><strong>使用层面：</strong> 但是实际处理的时候， 还是会还原出原本的小文件进行处理， 不会把 har 文件当成一个 HDFS 文件处理。 </li><li><strong>HDFS</strong> 上不支持 tar， <strong>只支持 har打包</strong></li></ul></li></ul><h2 id="7-HDFS-的-HA"><a href="#7-HDFS-的-HA" class="headerlink" title="7.HDFS 的 HA"></a>7.HDFS 的 HA</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-HDFS体系结构&quot;&gt;&lt;a href=&quot;#1-HDFS体系结构&quot; class=&quot;headerlink&quot; title=&quot;1.  HDFS体系结构&quot;&gt;&lt;/a&gt;1.  HDFS体系结构&lt;/h2&gt;&lt;p&gt;主从。。。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&quot;2-Na
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="HDFS" scheme="https://airpoet.github.io/categories/Hadoop/HDFS/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="HDFS" scheme="https://airpoet.github.io/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>▍回答</title>
    <link href="https://airpoet.github.io/2018/06/03/Poetry/%E5%9B%9E%E7%AD%94/"/>
    <id>https://airpoet.github.io/2018/06/03/Poetry/回答/</id>
    <published>2018-06-03T06:40:41.275Z</published>
    <updated>2018-06-09T17:12:20.524Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-03-511528043539_.pic.jpg" alt=""></p><p><br></p><p>卑鄙是卑鄙者的通行证，<br>高尚是高尚者的墓志铭，<br>看吧，在那镀金的天空中，<br>飘满了死者弯曲的倒影。</p><p><br></p><p>冰川纪过去了，<br>为什么到处都是冰凌？<br>好望角发现了，<br>为什么死海里千帆相竞？</p><p><br></p><p>我来到这个世界上，<br>只带着纸、绳索和身影，<br>为了在审判之前，<br>宣读那些被判决的声音。</p><p><br></p><p>告诉你吧，世界<br>我–不–相–信！<br>纵使你脚下有一千名挑战者，<br>那就把我算作第一千零一名。</p><p><br></p><p>我不相信天是蓝的，<br>我不相信雷的回声，<br>我不相信梦是假的，<br>我不相信死无报应。</p><p><br></p><p>如果海洋注定要决堤，<br>就让所有的苦水都注入我心中，<br>如果陆地注定要上升，<br>就让人类重新选择生存的峰顶。</p><p><br></p><p>新的转机和闪闪星斗，<br>正在缀满没有遮拦的天空。<br>那是五千年的象形文字，<br>那是未来人们凝视的眼睛。</p><p><br></p><p>作者 / 北岛</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-03-511528043539_.pic.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;
      
    
    </summary>
    
      <category term="文艺" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/"/>
    
      <category term="诗歌" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/%E8%AF%97%E6%AD%8C/"/>
    
    
      <category term="诗歌" scheme="https://airpoet.github.io/tags/%E8%AF%97%E6%AD%8C/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop学习笔记-2 HDFS基础入门</title>
    <link href="https://airpoet.github.io/2018/06/02/Hadoop/Study/1-HDFS/HDFS-1-%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8/"/>
    <id>https://airpoet.github.io/2018/06/02/Hadoop/Study/1-HDFS/HDFS-1-基础入门/</id>
    <published>2018-06-02T07:07:33.222Z</published>
    <updated>2018-06-11T11:45:18.181Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hadoop的核心组件-之-HDFS"><a href="#Hadoop的核心组件-之-HDFS" class="headerlink" title="Hadoop的核心组件 之 HDFS"></a>Hadoop的核心组件 之 HDFS</h1><h2 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h2><h3 id="1-HDFS是什么"><a href="#1-HDFS是什么" class="headerlink" title="1. HDFS是什么:"></a>1. HDFS是什么:</h3><ul><li>分布式文件系统</li></ul><h3 id="2-HDFS-设计思想"><a href="#2-HDFS-设计思想" class="headerlink" title="2. HDFS 设计思想"></a>2. HDFS 设计思想</h3><ul><li>分而治之,  切分存储, 当一个文件过大的时候, 一个节点存储不了, 采用切分存储</li><li><strong>分块存储</strong>: 每一个块叫做 block<ul><li>如果一个8T的数据, 这个怎么分合适???<ul><li>设置分块的时候要<strong>考虑</strong>一个事情 : <strong>负载均衡</strong></li><li>块的大小不能太大, 太大会造成负载不均衡</li><li><strong>hadoop2.x</strong> 中默认的切分的<strong>块的大小</strong>是: <strong>128M</strong>, 1.x中默认的是64M</li><li>如果一个文件<strong>不足128M, 也会单独存一个快</strong>, <strong>快的大小就是存储数据的实际大小</strong></li></ul></li><li>这个分块存储思想中, 如果一个块的存储节点宕机了, 这个时候, 数据的安全性得不到保证了</li></ul></li><li>HDFS中默认块的存储采用<strong>备份机制</strong><ul><li><strong>默认的备份个数是3个</strong>(总共存的, 存到datanode上的, namenode不存),  之前自己配的是2个, 所有备份相同地位是相同的.</li><li>相同的数据块的备份一定存储在不同的节点上</li><li>如果节点总共2个,  <code>dfs.replication=3</code> 副本个数是3个,  实际存储2个, 另一个进行<strong>记账</strong>,  当集群节点个数大于3个时, 会复制这个副本, 最终达到3个</li><li>假设集群中的节点4个, 副本3个, 有一个副本的机器宕机了, 这个时候发现副本的个数 小于 设定的个数,  就会进行<strong>复制, 达到3个副本</strong>,</li><li>如果 <em>这个时候</em>, 刚才宕机的节点又恢复了, 这个时候集群副本个数为4了,  集群会等待一段时间, 如果发现还是4个, 就会<strong>删除</strong>一个副本, <strong>达到3个</strong>(<u>设定值</u>)</li><li>备份越多越好吗?<ul><li>理论上副本数越多, 数据安全性越高</li><li>但是副本数越多, 会占用过多的存储资源, 会造成集群的维护变得越困难<ul><li>100 个节点, 50个副本,  在这50个副本中, 随时都有可能宕机, hdfs就需要维护副本</li></ul></li><li>一般情况下, 3个就可以了</li><li>hadoop是基于廉价的pc机设计的, 会造成机器随时可能宕机</li></ul></li></ul></li><li>HDFS的<strong>目录结构</strong><ul><li>hdfs的目录结构与linux 操作系统类似,  以 <code>/</code>为跟节点,  我们将这个目录🌲称为<code>抽象目录树</code></li><li>因为<strong>hdfs的目录结构</strong>代表的是所有数据节点的<strong>抽象出来的目录</strong>,  不代表任何一个节点<ul><li>hdfs:  /hadoop.zip   500M  被分成4个块存储</li><li>hdfs中存储的数据块 是有编号的, blk_1,  blk_2, blk_3,  blk_4</li><li>/spark.zip  300M  3个块, blk_5  blk_6  blk_7</li><li>底层存储的时候, 每一个block都有一个唯一的id</li><li>hdfs的数据底层存储的时候吗, 还是存在真正的物理节点上.</li></ul></li></ul></li></ul><h3 id="2-HDFS-的整体结构"><a href="#2-HDFS-的整体结构" class="headerlink" title="2. HDFS 的整体结构"></a>2. HDFS 的整体结构</h3><p><strong>主从结构:  一个主节点,  多个从节点</strong></p><h4 id="namenode"><a href="#namenode" class="headerlink" title="namenode:"></a>namenode:</h4><ul><li>用于存储元数据, 包括: <ul><li>抽象目录树</li><li>存储数据和block的对应关系</li><li>block存储的位置</li></ul></li><li>处理客户端的读写请求<ul><li>读: 下载</li><li>写: 上传</li></ul></li></ul><h4 id="datanode"><a href="#datanode" class="headerlink" title="datanode"></a>datanode</h4><ul><li>负责真正的数据存储, 存储数据的block</li><li>真正处理读写</li></ul><h4 id="secondarynamenode"><a href="#secondarynamenode" class="headerlink" title="secondarynamenode:"></a>secondarynamenode:</h4><ul><li>冷备份节点: 助理<ul><li>当namenode宕机的时候,  secondarynamenode不能主动切换为 namenode, 但是 secondarynamenode中存储的数据与namenode相同.</li></ul></li><li>主要作用: <ul><li>namenode宕机的时候, 帮助namenode恢复</li><li>帮助namenode做一些事情, 分担namenode的压力</li></ul></li></ul><h3 id="3-HDFS优缺点"><a href="#3-HDFS优缺点" class="headerlink" title="3. HDFS优缺点:"></a>3. HDFS优缺点:</h3><ul><li><p>优点: </p><ol><li><p>可构建在廉价机器上, 成本低,  通过多副本提高可靠性, 提供了容错和恢复机制</p></li><li><p>高容错性</p><ul><li>容错性: 数据访问上, 一个节点数据丢失, 不影响整体的数据访问</li><li>数据自动保存多个副本, 副本丢失后, 自动恢复, 最终恢复到用户配置的副本个数</li></ul></li><li><p>适合批处理, 适合离线数据处理</p><ul><li>移动计算而非数据, 数据位置暴露给计算框架</li></ul></li><li><p>适合大数据处理</p><ul><li>GB,  TB 甚至 PB 级数据, 百万规模以上的文件数量, 10k+ 节点规模</li></ul></li><li><p>流式文件访问, 不支持数据修改, hdfs用于数据存储</p><ul><li>一次性写入, 多次读取, 保证数据一致性</li></ul></li></ol></li></ul><ul><li><p><strong>缺点:</strong> </p><ol><li>不支持低延迟的数据访问, 不支持 <strong>实时/近实时</strong> 数据访问, 因为涉及到多轮<code>RPC</code>调用<ol><li>向 NameNode 寻址..</li><li>拿到地址后， 向 DataNode 请求数据..</li></ol></li><li><strong>不擅长存储大量的小文件</strong>–kb级别的<ul><li>寻址时间可能大于读取数据的时间, 不划算<ul><li>进行数据访问的时候先找元数据</li><li>元数据是和block对应的, 1个block块对应一条元数据</li><li>1000w个1kb的文件, 存了1000w个块 — 1000w元数据</li><li>在进行数据访问的时候可能花了 1s 的时间, 总体上不划算</li></ul></li><li>这样会造成元数据存储量过大, 增加namenode的压力<ul><li>在hdfs中一般情况下, 一条元数据大小 150byte 左右</li><li>1000w条元数据 — 1000w * 150,  1.5G左右</li></ul></li></ul></li><li><strong>不支持文件内容修改</strong>, 仅<strong>仅支持</strong>文件<strong>末尾追加</strong> <code>append</code>， 一个文件同时只能有一个写者，<strong>不支持并发操作</strong></li></ol></li></ul><h3 id="4-HDFS-的-常用命令"><a href="#4-HDFS-的-常用命令" class="headerlink" title="==4. HDFS 的 常用命令:=="></a>==4. HDFS 的 常用命令:==</h3><p><strong>HDFS归根结底就是一个文件系统,  类似于 linux,  需要用命令来操作</strong></p><h4 id="1-hapdoop-fs-命令"><a href="#1-hapdoop-fs-命令" class="headerlink" title="1. hapdoop fs 命令"></a>1. <code>hapdoop fs</code> 命令</h4><blockquote><p><code>hadoop fs</code> /  <code>hdfs dfs</code> 效果是一样的</p><p>在hadoop中查看, 只有绝对路径的访问方式</p></blockquote><ol><li><p>查看帮助</p><ul><li><code>hadoop fs -help</code> 查看所有 <code>hadoop fs</code>的帮助</li><li><code>hadoop fs -help ls</code> 查看 <code>fs</code>下的 <code>ls</code>的帮助</li></ul></li><li><p><strong>列出根目录</strong>: <code>hadoop fs -ls /</code></p><ul><li><code>hadoop fs -ls -R /</code> 递归展示</li><li><code>hadoop fs -ls -R -h /</code>友好展示， 展示文件大小单位</li><li><strong>如果不指定目录， 会默认找当前用户xx对应的<code>/user/xx</code>的目录</strong></li></ul></li><li><p><strong>递归创建 -mkdir -p</strong>:  </p><ul><li><code>hadoop fs -mkdir -p /aa/bb/cc/dd</code></li><li>不加 -p  为普通创建</li></ul></li><li><p><strong>创建空文件<code>-touchz</code></strong></p><ul><li>类似于 Linux 下的 touch</li></ul></li><li><p><strong>上传  put</strong>: <code>[-put [-f][-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]</code></p><ul><li>上传一个: <code>hadoop fs -put hadoop-2.7.6.tar.gz /ss</code></li><li>上传多个: <code>hadoop fs -put aa.txt bb.txt /ss</code></li></ul></li><li><p><strong>下载 get</strong></p><ul><li><code>hadoop fs -get hdfs路径 本地路径</code></li></ul></li><li><p><strong>合并下载 getmerge</strong></p><ul><li><code>hadoop fs -getmerge /ss/aa.txt /ss/bb.txt /home/ap/cc.txt</code></li><li>会将最后一个路径之前8的当做需要合并的文件, 最后一个路径中指定的文件就是合并生成的文件</li></ul></li><li><p><strong>查看文件内容 cat</strong></p><ul><li><code>-cat</code> 查看文件内容</li><li><code>-text</code>也是类似</li></ul></li><li><p><strong>删除文件  rm</strong></p><ul><li><sub></sub>rm -rf (错误的)<sub></sub></li><li>rm -r(递归)  -f(强制)</li><li>文件<code>hadoop fs -rm -f /ss/aa.txt</code></li><li>文件夹 <code>hadoop fs -rm -f -r /aa</code></li></ul></li><li><p><strong>mv 修改名字, 移动</strong></p><ul><li><p>移动的文件<strong>从 hdfs 到 hdfs</strong></p></li><li><p><code>hadoop fs -mv  ..  ..</code></p></li></ul></li><li><p><strong>cp 复制</strong></p><ul><li><code>hadoop fs -cp /hdfsfile /hdfsfile</code>: 从 hdfs 复制到 hdfs<ul><li>参数 <strong>-p</strong>  ： 复制后<strong>保持文件的原本属性</strong>, 时间戳， 权限等<ul><li><code>Passing -p preserves status [topax] (timestamps, ownership, permission, ACLs, XAttr).</code></li></ul></li><li>参数 <strong>-f</strong>   :   已有同名文件的话， 直接<strong>覆盖</strong></li></ul></li></ul></li><li><p><strong>在末尾追加</strong>: <code></code>-appendToFile 本地文件  hdfs文件` </p><ul><li>将本地文件bb.txt 追加到 htfd的 /aa/aa.txt 上<ul><li><code>hadoop fs -appendToFile aa.txt /ss/bb.txt</code></li></ul></li><li>从命令行追加 , 但是不知道怎么结束， 先存疑？？<ul><li><code></code>hadoop fs -appendToFile - /a.txt`</li></ul></li><li>这个追加是在原始块的末尾追加的. 会改变集群上的文件</li><li>如果超过128M才会进行切分,  但这个命令<em>一般不会使用</em></li></ul></li><li><p><strong>查看文件，文件夹数量  count</strong></p><ul><li><code>DIR_COUNT FILE_COUNT CONTENT_SIZE FILE_NAME</code></li><li><p><code>8            3         176.5 K     /tmp</code></p></li><li><p><code>hadoop fs -count -h /tmp</code>:  -h 是友好展示</p></li><li><code>hdfs dfs -count -h /tmp</code>:  与上面效果一样</li><li><code>hdfs dfs -count -q -h /tmp</code>:  查看文件配额， 具体看 help</li></ul></li><li><p><strong><code>du</code>：</strong> <strong>展示文件大小</strong>， 如果参数是文件夹， 则展示文件夹下文件的大小</p><ul><li><code>hadoop fs -du -h  /tmp</code></li><li><code>hadoop fs -du -s -h  /tmp</code>: s 应该是  sum 的意思， 展示所有文件大小的总和</li></ul></li><li><p><strong>展示文件最后1kb内容</strong><code>-tail</code></p><ul><li><p>Show the last 1KB of the file.</p></li><li><p><code></code> hadoop fs -tail /dd.txt`</p></li><li><code>-f  Shows appended data as the file grows.</code></li><li>应用场景： <strong>监控日志</strong></li></ul></li><li><p><strong>修改文件权限 chmod</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 直接使用十进制数字修改 </span></span><br><span class="line">[ap@cs2]~/<span class="built_in">test</span>% hadoop fs -ls /</span><br><span class="line">drwxr-xr-x   - ap supergroup          0 2018-06-01 08:55 /aa</span><br><span class="line"><span class="comment"># -R：  /aa 目录下所有的文件递归修改权限</span></span><br><span class="line">[ap@cs2]~/<span class="built_in">test</span>% hadoop fs -chmod -R 777 /aa</span><br><span class="line">[ap@cs2]~/<span class="built_in">test</span>% hadoop fs -ls /</span><br><span class="line">drwxrwxrwx   - ap supergroup          0 2018-06-01 08:55 /aa</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 针对用户组修改，注意，修改2个不同组权限， 用，隔开</span></span><br><span class="line">hadoop fs -chmod u+x,g+x /a.txt </span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 最常用的文件权限， 是  644(-rw-r--r--) 和 755(-rwxr-xr-x) </span></span><br><span class="line">文件创建默认就是644</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. u+x 与 +x 的区别</span></span><br><span class="line">前者指定加在哪组用户上，  后者是所有组都加</span><br></pre></td></tr></table></figure></li><li><p><strong>修改用户权限 chown</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -chown -R 用户名:组名  hdfs目录/文件</span><br></pre></td></tr></table></figure></li></ol><hr><h4 id="2-hdfs-dfsadmin命令"><a href="#2-hdfs-dfsadmin命令" class="headerlink" title="2. hdfs dfsadmin命令"></a>2. <strong><code>hdfs dfsadmin</code>命令</strong></h4><p> 管理员对当前节点的一些操作</p><ul><li><strong><code>hdfs dfsadmin -report</code>  报告当前的一些状态</strong><ul><li>-live  活跃的   </li><li>-dead  死的</li><li>-decommissioning  退役的</li></ul></li><li>**<code>hdfs dfsadmin -safemode</code> 安全模式<ul><li>系统刚启动的时候， 会有30秒的安全模式开启状态， 过了30秒就关了</li><li>enter 进入</li><li>leave 离开</li><li>get  查看</li></ul></li><li><strong><code>hdfs dfsadmin</code> 设置配额</strong><ul><li><code>-setQuota</code> ： 配额是限定的文件&amp;文件夹的数量<ul><li>A quota of 1 would force the directory to remain empty.</li><li>空文件本身算一个文件</li><li><code>bin/hdfs dfsadmin -setQuota 10 lisi</code></li></ul></li><li><code>-clrQuota</code></li><li><code>-setSpaceQuota</code>： 空间配额限定的是大小<ul><li><code>bin/hdfs dfsadmin -setSpaceQuota 4k /lisi/</code></li></ul></li><li><code>-clrSpaceQuota</code></li><li><code>hdfs dfs -count -q -h /user</code>:  加上 -q 是查看配额</li></ul></li></ul><hr><h4 id="3-httpFS访问"><a href="#3-httpFS访问" class="headerlink" title="3.  httpFS访问"></a>3.  httpFS访问</h4><p>使用 REST 的形式， 可以在浏览器上直接访问集群， 可以在非 Linux 平台访问</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 编辑文件httpfs-env.sh</span><br><span class="line"># 打开此句注释, 使用内嵌的 tomcat</span><br><span class="line">export HTTPFS_HTTP_PORT=14000</span><br><span class="line"></span><br><span class="line"># 编辑文件core-site.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">编辑文件hdfs-site.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.webhdfs.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">重新启动namenode，执行 sbin/httpfs.sh start</span><br><span class="line"># 执行命令</span><br><span class="line">curl -i "http://cs1:14000/webhdfs/v1?user.name=root&amp;op=LISTSTATUS"</span><br></pre></td></tr></table></figure><p><a href="http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/WebHDFS.html" target="_blank" rel="noopener">更多命令参考</a></p><hr><p><strong>相关知识点</strong></p><ul><li><p>这些命令<strong>在集群中的任何节点都可以做</strong>, hdfs文件系统中, 看到的目录结构只是一个抽象目录, 实际存储在集群中的节点上</p><ul><li>aa.txt ,  大小150M,   <code>hadoop fs -put aa.txt /</code></li><li>会在根目录下看到 /aa.txt,  但是 aa.txt 真实存储的时候, 会先进行分块, 分2块, 进行存储, 假设集群中5个存储节点,  这2个块存储在哪个节点,  由namenode进行分配</li><li>图形界面点进去, 可以看到存储的块</li></ul></li><li><p><strong>Linux的权限管理命令</strong>: </p><ol><li>修改 文件/文件夹 权限的 <code>chmod</code>: <ol><li>可读: r ,  =4        </li><li>可写: w,  =2</li><li>可执行: x,  =1<ol><li>最大权限是7</li><li>-rw-rw-r–</li><li>文件属性 d:目录  -:文件  l:链接</li><li>第一组: 本用户,  第二组: 本组用户,  第三组: 其它用户</li></ol></li></ol></li></ol><ul><li>chmod  711 </li><li>改一个文件夹下所有文件权限为711 <code>chmod -R 711 目录</code></li></ul><ol><li>修改文件所属用户和组 <code>chown</code><ul><li><code>chown -R root:root  ss/</code>把ss的文件夹全部改成root用户和root组</li></ul></li></ol></li></ul><hr><h3 id="5、Eclipse查看Hadoop文件信息"><a href="#5、Eclipse查看Hadoop文件信息" class="headerlink" title="5、Eclipse查看Hadoop文件信息"></a>5、Eclipse查看Hadoop文件信息</h3><p><a href="https://app.yinxiang.com/shard/s37/nl/7399077/9f4966ae-da19-48aa-960b-0a6081ab7ff2/" target="_blank" rel="noopener">详情可以查看</a></p><p><a href="https://app.yinxiang.com/shard/s37/nl/7399077/2668e8e5-f7a1-48e3-a5d5-a7fcf9076c1f/" target="_blank" rel="noopener">其中可能遇到的bug，参见</a></p><p>其中， Eclipse端无法直接删除文件的问题，似乎可以通过在<code>hdfs-site.xml</code> 中修改访问权限来实现， <strong>还未尝试</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><hr><h3 id="6-通过-Java-API的方式操作-HDFS"><a href="#6-通过-Java-API的方式操作-HDFS" class="headerlink" title="6. 通过 Java API的方式操作 HDFS"></a>6. 通过 Java API的方式操作 HDFS</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hadoop的核心组件-之-HDFS&quot;&gt;&lt;a href=&quot;#Hadoop的核心组件-之-HDFS&quot; class=&quot;headerlink&quot; title=&quot;Hadoop的核心组件 之 HDFS&quot;&gt;&lt;/a&gt;Hadoop的核心组件 之 HDFS&lt;/h1&gt;&lt;h2 id=&quot;H
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="HDFS" scheme="https://airpoet.github.io/categories/Hadoop/HDFS/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="HDFS" scheme="https://airpoet.github.io/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>HashMap的实现原理</title>
    <link href="https://airpoet.github.io/2018/06/02/Java/HashMap%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"/>
    <id>https://airpoet.github.io/2018/06/02/Java/HashMap的实现原理/</id>
    <published>2018-06-02T02:25:19.841Z</published>
    <updated>2018-06-02T03:22:52.059Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-什么是HashMap"><a href="#1-什么是HashMap" class="headerlink" title="1.什么是HashMap"></a>1.什么是HashMap</h1><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-02-024247.jpg" alt=""></p><p><strong>Hash</strong>: 散列将一个任意的长度通过某种(hash函数)算法转换成一个固定值</p><p><strong>Map</strong>: 地图,  (x,y)存储</p><p>底层就是一个数组结构, 数组中的每一项又是一个链表, 当新建一个HashMap的时候, 就会初始化一个数组</p><p><strong>总结</strong>: 通过 hash 出来值, 然后通过值定位到某个 map, 然后value 存储到这个 map中, <strong>value只不过是 key 的附属.</strong></p><hr><h1 id="2-源码分析"><a href="#2-源码分析" class="headerlink" title="2.源码分析"></a>2.源码分析</h1><h3 id="先给出结论"><a href="#先给出结论" class="headerlink" title="先给出结论"></a>先给出结论</h3><h4 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构:"></a>数据结构:</h4><ul><li>底层是数组</li><li><code>Entry</code> 就是数组中的元素</li><li>每个<code>Map.Entry</code>其实就是一个<code>key-value</code>对,  它持有一个指向下一个元素的引用<code>Entry&lt;K,V&gt; next;</code>, 这就构成了链表</li></ul><h4 id="存取实现"><a href="#存取实现" class="headerlink" title="存取实现:"></a>存取实现:</h4><ul><li><strong>存储<code>put</code></strong> :<ul><li><strong>过程</strong><ul><li>先根据 <code>key</code> 的 <code>hashCode</code> 重新计算 <code>hash</code> 值, 根据 <code>hash</code> 值得到这个元素在数组中的位置(下标)</li><li>如果数组该位置上已经存放有其他元素了,  那么在这个位置上的元素将以链表的形式存放, 新加入的放在链头, 最先加入的放在链尾.  </li><li>如果数组该位置上没有元素, 就直接将该元素放到数组中的该位置上.</li></ul></li><li><strong>注意点</strong><ul><li>当系统决定存储 <code>HashMap</code> 中的 <code>key-value</code> 对时，完全没有考虑 <code>Entry</code> 中的 <code>value</code>，仅仅只是根据 <code>key</code>来计算并决定每个<code>Entry</code>的存储位置。当系统决定了 <code>key</code>的存储位置之后，<code>value</code>随之保存在那里即可。</li><li>对于于任意给定的对象，只要它的 <code>hashCode()</code>返回值相同，那么程序调用 <code>hash(int h)</code>方法所计算得到的<code>hash</code> 码值总是相同的。</li><li>本质上就是<strong>把 <code>hash</code> 值对数组长度取模运算</strong>， 这样一来，<strong>元素的分布相对来说是比较均匀的</strong></li><li>但是系统是用的<strong>位运算</strong>， 方法更巧妙， 消耗更小</li></ul></li></ul></li><li><strong>读取<code>get</code></strong><ul><li><strong>过程</strong><ul><li>首先计算 <code>key</code> 的 <code>hashCode</code>，找到数组中对应位置的某一元素，然后通过<code>key</code> 的 <code>equals</code> 方法在对应位置的链表中找到需要的元素。</li></ul></li></ul></li></ul><h4 id="存储实现总结"><a href="#存储实现总结" class="headerlink" title="存储实现总结:"></a>存储实现总结:</h4><ul><li><p><strong>HashMap</strong> 在底层将 <strong>key-value</strong> 当成一个<u>整体</u>进行处理，这个整体就是一个 <strong>Entry</strong> 对象。</p></li><li><p><strong>HashMap</strong> 底层采用一个 <strong>Entry[]</strong> 数组来保存所有的 <strong>key- value</strong> 对，当需要<strong><em>存储</em></strong>一个 <strong>Entry</strong> 对象时，会根据 <strong>hash</strong> 算法来<u>决定其在数组中的存储位置</u>，在根据 <strong>equals</strong> 方法<u>决定其在该数组位置上的链表中的存储位置</u>；</p></li><li>当需要<strong><em>取出</em></strong>一个 <strong>Entry</strong> 时，也会根据 <strong>hash</strong> 算法<u>找到其在数组中的存储位置</u>，再根据 <strong>equals</strong> 方法<u>从该位置上的链表中取出</u>该 <strong>Entry</strong>。</li></ul><hr><h1 id="3-HashMap-的性能参数"><a href="#3-HashMap-的性能参数" class="headerlink" title="3.  HashMap 的性能参数"></a>3.  HashMap 的性能参数</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 桶表默认容量 16,  必须是 2 的倍数， 便于后面的 位运算</span></span><br><span class="line"><span class="comment"> * 控制hashcode 不超16范围, a.hashcode = xx % 16 (hashcode 取模 桶个数)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> DEFAULT_INITIAL_CAPACITY = <span class="number">1</span> &lt;&lt; <span class="number">4</span>; <span class="comment">// aka 16</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * MUST be a power of two &lt;= 1&lt;&lt;30.</span></span><br><span class="line"><span class="comment"> * 桶表最大 2^30</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MAXIMUM_CAPACITY = <span class="number">1</span> &lt;&lt; <span class="number">30</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 扩容因子（负载因子）: 0.75</span></span><br><span class="line"><span class="comment"> * 扩容: 每次2倍</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">float</span> DEFAULT_LOAD_FACTOR = <span class="number">0.75f</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 链表: hash算法值相同的时候, 会把值相同的放在一个链表上, 链表上的元素个数</span></span><br><span class="line"><span class="comment"> * 超过8个时, 链表转化为二叉树, 提升查询效率</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> TREEIFY_THRESHOLD = <span class="number">8</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 小于6个， 又变回链表</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> UNTREEIFY_THRESHOLD = <span class="number">6</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The smallest table capacity for which bins may be treeified.</span></span><br><span class="line"><span class="comment"> * (Otherwise the table is resized if too many nodes in a bin.)</span></span><br><span class="line"><span class="comment"> * Should be at least 4 * TREEIFY_THRESHOLD to avoid conflicts</span></span><br><span class="line"><span class="comment"> * between resizing and treeification thresholds.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MIN_TREEIFY_CAPACITY = <span class="number">64</span>;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-什么是HashMap&quot;&gt;&lt;a href=&quot;#1-什么是HashMap&quot; class=&quot;headerlink&quot; title=&quot;1.什么是HashMap&quot;&gt;&lt;/a&gt;1.什么是HashMap&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt
      
    
    </summary>
    
      <category term="Java" scheme="https://airpoet.github.io/categories/Java/"/>
    
      <category term="知识点" scheme="https://airpoet.github.io/categories/Java/%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Java" scheme="https://airpoet.github.io/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>▍无题</title>
    <link href="https://airpoet.github.io/2018/06/02/Poetry/%E6%97%A0%E9%A2%98/"/>
    <id>https://airpoet.github.io/2018/06/02/Poetry/无题/</id>
    <published>2018-06-01T17:10:33.894Z</published>
    <updated>2018-06-09T17:12:12.202Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-03-501528043523_.pic.jpg" alt=""></p><p><br>在淮海中路看油画，要把天空调弱</p><p>让油菜花暗下去，看眉式清秀之人离开身体</p><p>穿过街巷中涂抹过的人群，悄悄投了水。  </p><p><br>在淮海中路1411号，春光遮蔽了暗疾</p><p>鸟鸣带来逼仄和飞行感，一个人的身体像麻绳</p><p>裸露在新鲜空气中，骨头开裂出花朵。  </p><p><br>眼底的云又白又黑，膝盖的青色愈爱愈深</p><p>穿过死后潭水的寂静，背部长出的鱼鳞</p><p>一年比一年薄，月亮一日比一日旧。  </p><p><br>与春风交换身体，与素不相识之人抱头痛哭</p><p>与我，许下再死一次的诺言，这么多年了</p><p>她说，我爱你依旧，胜过画中人。 </p><p> <br></p><p>作者 / 隐居的事</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;&lt;img src=&quot;http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-03-501528043523_.pic.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;
      
    
    </summary>
    
      <category term="文艺" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/"/>
    
      <category term="诗歌" scheme="https://airpoet.github.io/categories/%E6%96%87%E8%89%BA/%E8%AF%97%E6%AD%8C/"/>
    
    
      <category term="诗歌" scheme="https://airpoet.github.io/tags/%E8%AF%97%E6%AD%8C/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop相关书籍</title>
    <link href="https://airpoet.github.io/2018/06/02/Hadoop/Study/0-Hadoop/Hadoop%E7%9B%B8%E5%85%B3%E4%B9%A6%E7%B1%8D/"/>
    <id>https://airpoet.github.io/2018/06/02/Hadoop/Study/0-Hadoop/Hadoop相关书籍/</id>
    <published>2018-06-01T16:20:33.728Z</published>
    <updated>2018-06-11T11:41:32.528Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hadoop框架体系相关书籍推荐"><a href="#Hadoop框架体系相关书籍推荐" class="headerlink" title="Hadoop框架体系相关书籍推荐"></a>Hadoop框架体系相关书籍推荐</h1><h2 id="1-Hadoop权威指南-第四版"><a href="#1-Hadoop权威指南-第四版" class="headerlink" title="1. [Hadoop权威指南] 第四版"></a>1. [Hadoop权威指南] 第四版</h2><blockquote><p> <strong>顾名思义, 很权威</strong>  </p></blockquote><p>🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘</p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-01-162303.jpg" alt=""></p><p>🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘🐘</p><h2 id="2-HBase权威指南"><a href="#2-HBase权威指南" class="headerlink" title="2. [HBase权威指南]"></a>2. [HBase权威指南]</h2><blockquote><p> 祝你🐴到成功</p></blockquote><p>🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴</p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-01-163622.jpg" alt=""></p><p>🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴🐴</p><h2 id="3-Hive编程指南"><a href="#3-Hive编程指南" class="headerlink" title="3. [Hive编程指南]"></a>3. [Hive编程指南]</h2><blockquote><p>小蜜蜂, 嗡嗡嗡.</p></blockquote><p>🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝</p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-01-163954.jpg" alt=""></p><p>🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝🐝</p><h2 id="4-zookeeper分布式过程协同技术详解"><a href="#4-zookeeper分布式过程协同技术详解" class="headerlink" title="4. [zookeeper分布式过程协同技术详解]"></a>4. [zookeeper分布式过程协同技术详解]</h2><blockquote><p>这是个啥动物?? 🐱??</p></blockquote><p>🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱</p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-01-164208.jpg" alt=""></p><p>🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱🐱</p><hr><h1 id="以上书籍下载地址"><a href="#以上书籍下载地址" class="headerlink" title="以上书籍下载地址"></a>以上书籍下载地址</h1><h5 id="有效期至-2018年06月08日"><a href="#有效期至-2018年06月08日" class="headerlink" title="有效期至 2018年06月08日"></a><strong>有效期至 2018年06月08日</strong></h5><p><a href="https://pan.baidu.com/s/1s8YK-Xbjpd738inPLSczRA" target="_blank" rel="noopener">百度网盘下载</a> </p><p>密码:  <code>tm0c</code></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hadoop框架体系相关书籍推荐&quot;&gt;&lt;a href=&quot;#Hadoop框架体系相关书籍推荐&quot; class=&quot;headerlink&quot; title=&quot;Hadoop框架体系相关书籍推荐&quot;&gt;&lt;/a&gt;Hadoop框架体系相关书籍推荐&lt;/h1&gt;&lt;h2 id=&quot;1-Hadoop权
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="Books" scheme="https://airpoet.github.io/categories/Hadoop/Books/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="Books" scheme="https://airpoet.github.io/tags/Books/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop学习笔记-1 简介&amp;安装</title>
    <link href="https://airpoet.github.io/2018/05/31/Hadoop/Study/0-Hadoop/Hadoop%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-%E7%AE%80%E4%BB%8B&amp;%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    <id>https://airpoet.github.io/2018/05/31/Hadoop/Study/0-Hadoop/Hadoop学习笔记-1-简介&amp;环境搭建/</id>
    <published>2018-05-31T02:08:21.006Z</published>
    <updated>2018-06-11T11:43:56.661Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><h3 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h3><h4 id="1-有一个很大的-4T-的文件-文件中存储的是ip-每行存储一个-要求求出出现此处最多的那个ip"><a href="#1-有一个很大的-4T-的文件-文件中存储的是ip-每行存储一个-要求求出出现此处最多的那个ip" class="headerlink" title="1. 有一个很大的(4T)的文件,  文件中存储的是ip, 每行存储一个, 要求求出出现此处最多的那个ip"></a>1. 有一个很大的(4T)的文件,  文件中存储的是ip, 每行存储一个, 要求求出出现此处最多的那个ip</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">如果这个文件是小文件：</span><br><span class="line">io流+集合</span><br><span class="line">实现思路：</span><br><span class="line">创建一个流进行文件读取</span><br><span class="line">读取出来的数据存储到map集合中  key：ip    value:次数</span><br><span class="line">统计逻辑：</span><br><span class="line">判断读取的ip是否已经存在在map中</span><br><span class="line">存在：取出value+<span class="number">1</span></span><br><span class="line">不存在：将ip作为key   <span class="number">1</span>作为value</span><br><span class="line">怎么求ip出现次数最多的</span><br><span class="line">遍历map    遍历key，取出value找出最大值  value最大的key就是要找的ip</span><br><span class="line">我的文件足够大：大到一台机器装不下     </span><br><span class="line">数组   集合  变量------&gt;基于内存的</span><br><span class="line">怎么办？？？？？</span><br><span class="line">服务器的<span class="number">2</span>T</span><br><span class="line"><span class="number">1</span>.在最早的时候我们的思维模式就是纵向扩展，增加单个节点的性能    <span class="number">8</span>T</span><br><span class="line">摩尔定律：硬件性能<span class="number">18</span>-<span class="number">24</span>个月会提升一倍</span><br><span class="line"><span class="number">4</span>T------<span class="number">2</span>h</span><br><span class="line"><span class="number">4</span>T------<span class="number">1</span>h    前提是数据量不发生改变</span><br><span class="line">但是往往数据量的变化速度远远大于服务器性能的提升速度</span><br><span class="line">经过<span class="number">18</span>个月</span><br><span class="line">服务器性能提升了一倍</span><br><span class="line">数据量------提升了<span class="number">10</span>倍</span><br><span class="line"><span class="number">4</span>T------<span class="number">2</span>h</span><br><span class="line"><span class="number">40</span>t-----<span class="number">20</span>h   目前只需要<span class="number">10</span>h</span><br><span class="line">纵向扩展不可行？</span><br><span class="line">横向扩展：如果一台机器处理不了数据   使用多台机器</span><br><span class="line"><span class="number">4</span>T------<span class="number">2</span>h</span><br><span class="line"><span class="number">4</span>t----<span class="number">4</span>个机器----<span class="number">0.5</span>小时</span><br><span class="line">分而治之的思想：</span><br><span class="line">一个机器计算性能有限  这个时候可以使用多台机器共同计算  每台机器承担一部分计算量</span><br><span class="line">最终实现：</span><br><span class="line"><span class="number">1</span>.先将这个足够大的文件进行切分  切分成了多个小文件</span><br><span class="line"><span class="number">2</span>.将多个小文件分发给多个机器进行统计每个ip出现的次数   每个求出出现次数最多的ip</span><br><span class="line"><span class="number">3</span>.合并求出最终的最大值</span><br></pre></td></tr></table></figure><hr><h4 id="2-有两个很大的文件-两个文件中存储的都是url-求出两个文件中相同的url"><a href="#2-有两个很大的文件-两个文件中存储的都是url-求出两个文件中相同的url" class="headerlink" title="2. 有两个很大的文件, 两个文件中存储的都是url,  求出两个文件中相同的url"></a>2. 有两个很大的文件, 两个文件中存储的都是url,  求出两个文件中相同的url</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">如果文件是小文件：</span><br><span class="line">io流+集合（set）</span><br><span class="line">实现逻辑：</span><br><span class="line"><span class="number">1</span>.先创建两个文件读取流，用来读取两个文件</span><br><span class="line"><span class="number">2</span>.创建两个集合set1    set2</span><br><span class="line"><span class="number">3</span>.进行文件读取并分别set1   set2中</span><br><span class="line"><span class="number">4</span>.循环遍历其中一个set1，判断set1中取出的每个url是否在set2中  set2.contains(url)</span><br><span class="line">大文件的时候怎么办？</span><br><span class="line">我们也采用分而治之的思想：将两个大文件都进行切分，每个大文件都切成多个小文件</span><br><span class="line">一个大任务=<span class="number">4</span>*<span class="number">4</span>个小任务</span><br><span class="line">这样虽然可以达到目的但是效率太低？怎么办？</span><br><span class="line">排序，切分（规则同一）   最终将任务减少到<span class="number">4</span>个</span><br><span class="line">但是大文件排序仍然是一个非常消耗性能的事情，如果不需要排序就可做到这个效果尽量不要排序</span><br><span class="line">怎么办？</span><br><span class="line">hash算法的目的----》给每一个对象生成一个“唯一”的hash值<span class="number">0</span>-Integer_MAX</span><br><span class="line">是否可以运用hash算法解决这个问题</span><br><span class="line">url.hashCode()%分段的个数   两个文件分段规则一定相同吗？</span><br><span class="line">url.hashCode()肯定一样</span><br><span class="line">分段个数一定相同吗？可以不一样  如果不一样的话  必须成倍数关系</span><br><span class="line">最终的解决方案：</span><br><span class="line">分而治之+分段规则</span><br><span class="line">分段：分区</span><br></pre></td></tr></table></figure><hr><h4 id="3-有一个很小的文件-存储的都是url-每行一个-怎样快速判断给定的一个url是否在这个文件中"><a href="#3-有一个很小的文件-存储的都是url-每行一个-怎样快速判断给定的一个url是否在这个文件中" class="headerlink" title="3. 有一个很小的文件, 存储的都是url, 每行一个, 怎样快速判断给定的一个url是否在这个文件中"></a>3. 有一个很小的文件, 存储的都是url, 每行一个, 怎样快速判断给定的一个url是否在这个文件中</h4><p><strong>小文件</strong>:  IO + 集合(set)</p><ol><li>创建io 和 集合</li><li>进行文件读取放在 set集合中</li><li>set.contains(url) ==&gt;  true:存在,  false: 不存在</li></ol><p><strong>大文件:</strong>  </p><p>思路1:  用hashCode() 进行分区, 然后用要查找的 url 取模定位 </p><ul><li>但是这样定位到了还是要一个个找</li></ul><p><strong>思路2:</strong> </p><ul><li>数组的查询性能比较高, 数组可以通过下标</li><li><p><strong>基数排序</strong></p><ol><li>数组的索引代表的是数据的原始值, 数组中存储的值, 是原始值出现的次数</li><li>放到对应下标的位置,  值只存出现的次数</li><li>如果数组中对应的下表存储的值为0, 代表此下标的值没有出现过, 就不需要输出</li><li><strong>缺点:</strong> <ul><li>数据范围过大时, 数组长度不好创建</li><li>数组的类型不好确定</li><li>如果数据比较分散时, 会造成资源浪费</li></ul></li><li><u>练习: 写一个基数排序, 随机生成的20个数, 运用基数排序排序</u></li></ol></li></ul><ul><li><p>对于本题</p><ul><li>不需要统计次数, 存在标记为1, 不存在就是0</li><li>所以存的时候最好用boolean存,  用位数组 <code>bit[]</code></li><li>可以设计多个hash算法, 用来校验某一种hashCode相同的情况</li><li><strong>影响误判率3要素</strong>: hash算法个数 k - 数据量n - 数组长度 m</li><li><strong>布隆过滤器 </strong>公式: k = 0.7*(m/n), 此时的误判率最小</li></ul></li></ul><hr><h1 id="大数据基本介绍"><a href="#大数据基本介绍" class="headerlink" title="大数据基本介绍"></a>大数据基本介绍</h1><h3 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h3><ul><li>数据就是数值，也就是我们通过观察、实验或计算得出的结果。数据有很多种，最简单的就是数字。</li><li>数据也可以是文字、图像、声音等。数据可以用于科学研究、设计、查证等。</li></ul><h3 id="结构划分"><a href="#结构划分" class="headerlink" title="结构划分:"></a>结构划分:</h3><ul><li>结构化<ul><li>半结构化</li><li>非结构化</li></ul></li></ul><h3 id="大数据特点-4V"><a href="#大数据特点-4V" class="headerlink" title="大数据特点:  4V"></a>大数据特点:  4V</h3><ol><li><h5 id="数据量大"><a href="#数据量大" class="headerlink" title="数据量大"></a>数据量大</h5><blockquote><p>1 Byte =8 bit<br>1 KB = 1,024 Bytes = 8192 bit<br>1 MB = 1,024 KB = 1,048,576 Bytes<br>1 GB = 1,024 MB = 1,048,576 KB<br>1 TB = 1,024 GB = 1,048,576 MB   （普通用户数据级别）<br>1 PB = 1,024 TB = 1,048,576 GB（企业级数据级别）<br>1 EB = 1,024 PB = 1,048,576 TB<br>1 ZB = 1,024 EB = 1,048,576 PB（全球数据总量级别）</p></blockquote></li><li><p>数据增长速度快</p></li><li><p>数据种类多</p><ul><li>文字 图片  音频 视频..</li></ul></li><li>数据的价值密度低  整体价值高</li></ol><h3 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h3><ol><li>公司中的自己的业务数据  淘宝  京东</li><li>第三方</li><li>爬虫  爬数据</li></ol><h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h3><ol><li>缺失数据的处理<ul><li>考虑缺失数据是否影响整体的业务逻辑  不影响 删除</li><li>如果是和钱相关的数据    —-慎重   不能轻易删除  </li></ul></li><li>敏感数据<ul><li>脱敏处理 – 加密</li></ul></li></ol><h3 id="数据价值"><a href="#数据价值" class="headerlink" title="数据价值"></a>数据价值</h3><ul><li>人物画像<ul><li>根据根据用户数据给用户做一个全方位的分析画像   属性：  人脉  消费水平   性格特点<ul><li>….</li></ul></li></ul></li></ul><hr><h1 id="几个概念"><a href="#几个概念" class="headerlink" title="几个概念"></a>几个概念</h1><h3 id="集群"><a href="#集群" class="headerlink" title="集群:"></a>集群:</h3><ul><li>多个机器共同协作完成同一个任务, 每一个机器叫做节点,  多个机器共同组成的群体叫做集群</li><li>集群中的每个节点之间通过局域网或其他方式通讯</li></ul><h3 id="分布式"><a href="#分布式" class="headerlink" title="分布式:"></a>分布式:</h3><ul><li>分而治之 ,  一个任务呗分成多个子任务模块, 每个任务跑在不同的节点上</li><li>原来一个人干的事情, 现在大家分工劳动</li><li>分布式的文件系统 ,  分布式数据库,  分布式计算系统</li></ul><h3 id="负载均衡-Nginx"><a href="#负载均衡-Nginx" class="headerlink" title="负载均衡: Nginx"></a>负载均衡: Nginx</h3><ul><li>每个节点分配到的任务基本均衡</li><li>负载均衡是跟每个节点自身的配置等匹配的</li><li>不存在绝对的均衡</li></ul><hr><h1 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h1><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><ul><li>一个分布式的开源框架</li><li>支持成千上万的节点, 每个节点依靠本地的计算和存储</li><li>在应用层面提供高可用性</li><li>将硬件错误看成一个常态</li></ul><h3 id="Hadoop的模块"><a href="#Hadoop的模块" class="headerlink" title="Hadoop的模块"></a>Hadoop的模块</h3><ul><li><strong>Common</strong> <ul><li>支持其他 Hadoop 模块的公共实用程序</li><li>封装: 工具类, RPC框架</li></ul></li><li><strong>HDFS</strong><ul><li>Hadoop的分布式文件系统, 负责海量数据的存储</li><li>将文件切分成指定大小的数据块并以多副本的存储形式存储在多个机器上</li><li>数据切分, 多副本, 容错等操作对用户是透明的</li><li><strong>架构</strong>: 主从架构 ( Java进程)<ul><li>主: namenode  一个</li><li>从: datenode   多个</li><li>助理: SecondaryNamenode 分担主进程的压力</li></ul></li></ul></li><li><strong>YARN</strong><ul><li>集群的资源调度框架,  负责集群的资源管理</li><li>架构: 主从架构<ul><li>主: ResourceManager  – 负责统筹资源</li><li>从: NodeManager</li></ul></li></ul></li><li><strong>MapReduce</strong><ul><li>分布式计算框架,  有计算任务的时候才会有响应的进程</li></ul></li></ul><h3 id="Hadoop的搭建"><a href="#Hadoop的搭建" class="headerlink" title="Hadoop的搭建"></a>Hadoop的搭建</h3><h4 id="搭建前的准备"><a href="#搭建前的准备" class="headerlink" title="搭建前的准备"></a>搭建前的准备</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">搭建准备：</span><br><span class="line">1）ip配置</span><br><span class="line"></span><br><span class="line">2）主机名   vi /etc/sysconfig/network</span><br><span class="line"></span><br><span class="line">3）主机映射</span><br><span class="line"></span><br><span class="line">4）关闭防火墙和sellinux</span><br><span class="line">    service iptables stop</span><br><span class="line">    vi /etc/selinux/config</span><br><span class="line">    SELINUX=disabled</span><br><span class="line"></span><br><span class="line">5）将系统的启动级别改为3</span><br><span class="line">vi /etc/inittab</span><br><span class="line"></span><br><span class="line">6）创建普通用户，并为普通用户添加sudolers权限</span><br><span class="line">    创建用户：useradd 用户名</span><br><span class="line">    passwd 用户名</span><br><span class="line">    vi /etc/sudoers</span><br><span class="line">    hadoop  ALL=(ALL)       ALL</span><br><span class="line"></span><br><span class="line">7）配置免密登录</span><br><span class="line">   先切换到普通用户</span><br><span class="line">   1）生成秘钥</span><br><span class="line">   ssh-keygen</span><br><span class="line">   2)发送秘钥</span><br><span class="line">   ssh-copy-id hadoop(主机名)</span><br><span class="line">   验证：ssh hadoop</span><br><span class="line">   </span><br><span class="line">8）安装jdk</span><br><span class="line">    卸载jdk：</span><br><span class="line">    rpm -qa|grep jdk</span><br><span class="line">    rpm -e java-1.7.0-openjdk-1.7.0.99-2.6.5.1.el6.x86_64 --nodeps</span><br><span class="line"></span><br><span class="line">9）时间同步   伪分布式不需要   分布式需要，必须做</span><br><span class="line"></span><br><span class="line">10)选择安装版本：</span><br><span class="line">    不选太陈旧的版本也不选最新的版本</span><br><span class="line">    2.7.6</span><br><span class="line">    </span><br><span class="line">11)安装</span><br><span class="line">    一定切换用户    普通用户</span><br></pre></td></tr></table></figure><h4 id="方式1-伪分布式"><a href="#方式1-伪分布式" class="headerlink" title="方式1:  伪分布式"></a>方式1:  伪分布式</h4><p><strong>所有进程全部运行在同一个节点上</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">1）上传</span><br><span class="line">2）解压</span><br><span class="line">3）修改配置文件</span><br><span class="line">配置文件的目录：HADOOP_HOME/etc/hadoop</span><br><span class="line">    需要修改6个配置文件：</span><br><span class="line">    1）hadoop-env.sh</span><br><span class="line">    <span class="built_in">export</span> JAVA_HOME=/home/hadoop/jdk1.8.0_73/</span><br><span class="line">    </span><br><span class="line">    2)core-site.xml</span><br><span class="line">    核心配置文件</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://hadoop:9000&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    3)hdfs-site.xml</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        </span><br><span class="line">     4)yarn-site.xml</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">     </span><br><span class="line">     5)mapred-site.xml</span><br><span class="line">     &lt;property&gt;</span><br><span class="line">         &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">   </span><br><span class="line">                                      </span><br><span class="line"> 6)slaves   配置的是从节点的信息</span><br><span class="line"> </span><br><span class="line"> 7)配置环境变量</span><br><span class="line"> <span class="built_in">export</span> JAVA_HOME=/home/hadoop/jdk1.8.0_73</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/home/hadoop/hadoop-2.7.6</span><br><span class="line"><span class="built_in">export</span>  PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br><span class="line"><span class="built_in">source</span> /etc/rpofile</span><br><span class="line">验证  hadoop  version</span><br><span class="line"></span><br><span class="line">8)先进行格式化</span><br><span class="line">hadoop namenode -format</span><br><span class="line">9)启动</span><br><span class="line">        start-all.sh</span><br><span class="line">        不建议   建议以下命令：</span><br><span class="line">        start-dfs.sh</span><br><span class="line">        start-yarn.sh</span><br><span class="line"></span><br><span class="line">10）验证</span><br><span class="line">        jps   6个进程</span><br><span class="line">        3909 Jps</span><br><span class="line">        3736 ResourceManager</span><br><span class="line">        3401 DataNode</span><br><span class="line">        3306 NameNode</span><br><span class="line">        3836 NodeManager</span><br><span class="line">        3597 SecondaryNameNode</span><br><span class="line"></span><br><span class="line">页面：</span><br><span class="line">        hdfs：namenode的ip：50070</span><br><span class="line">        yarn:resourcemanager的ip：8088</span><br></pre></td></tr></table></figure><hr><h4 id="方式2-完全分布式"><a href="#方式2-完全分布式" class="headerlink" title="方式2:  完全分布式"></a>方式2:  完全分布式</h4><p><a href="https://app.yinxiang.com/shard/s37/nl/7399077/c3f4c3b9-249e-42cf-87e4-fb5db3bb37e2/" target="_blank" rel="noopener">参考文档</a></p><p><strong>各个节点的安装的普通用户名必须相同  密码也得相同, 每个节点都需要操作</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">搭建准备：</span><br><span class="line"><span class="number">1</span>）ip配置</span><br><span class="line"><span class="number">2</span>）主机名   vi /etc/sysconfig/network</span><br><span class="line"><span class="number">3</span>）主机映射</span><br><span class="line"><span class="number">4</span>）关闭防火墙和sellinux</span><br><span class="line">service iptables stop</span><br><span class="line">vi /etc/selinux/config</span><br><span class="line">SELINUX=disabled</span><br><span class="line"><span class="number">5</span>）将系统的启动级别改为<span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="number">6</span>）创建普通用户，并为普通用户添加sudolers权限</span><br><span class="line">创建用户：useradd 用户名</span><br><span class="line"> passwd 用户名</span><br><span class="line"> vi /etc/sudoers</span><br><span class="line"> hadoop  ALL=(ALL)       ALL</span><br><span class="line"><span class="number">7</span>）配置免密登录</span><br><span class="line">先切换到普通用户</span><br><span class="line">每台机器都需要执行下面的操作</span><br><span class="line">各个节点之间都做一下</span><br><span class="line">    a. 生成秘钥 ssh-keygen</span><br><span class="line">    b. 发送秘钥  ssh-copy-<span class="function">id <span class="title">hadoop</span><span class="params">(主机名)</span></span></span><br><span class="line"><span class="function">c. 验证：各个节点之间都需要做相互验证</span></span><br><span class="line"><span class="function">ssh hadoop01</span></span><br><span class="line"><span class="function">ssh hadoop02</span></span><br><span class="line"><span class="function">ssh hadoop03</span></span><br><span class="line"><span class="function">8）安装jdk</span></span><br><span class="line"><span class="function">卸载jdk：</span></span><br><span class="line"><span class="function">rpm -qa|grep jdk</span></span><br><span class="line"><span class="function">rpm -e java-1.7.0-openjdk-1.7.0.99-2.6.5.1.el6.x86_64 --nodeps</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">9）时间同步   伪分布式不需要   分布式需要，必须做</span></span><br><span class="line"><span class="function">1)不能联网的时候    手动指定   date -s 时间  或者手动搭建一个时间服务器</span></span><br><span class="line"><span class="function">2）能联网的时候    找一个公网中的公用的时间服务器  所有节点的时间和公网中的时间服务器保持一致</span></span><br><span class="line"><span class="function">ntpdate 公网的时间服务器地址</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">完全分布式必须要做  每个节点都需要执行</span></span><br><span class="line"><span class="function">10) 选择安装版本：</span></span><br><span class="line"><span class="function">不选太陈旧的版本也不选最新的版本</span></span><br><span class="line"><span class="function">2.7.6</span></span><br><span class="line"><span class="function">11) 安装</span></span><br><span class="line"><span class="function">一定切换用户    普通用户</span></span><br><span class="line"><span class="function">先在一个节点上执行所有的修改  在远程发送到其他节点</span></span><br><span class="line"><span class="function">                1）上传</span></span><br><span class="line"><span class="function">                2）解压d</span></span><br><span class="line"><span class="function">                3）配置环境变量</span></span><br><span class="line"><span class="function">                4）修改配置文件</span></span><br><span class="line"><span class="function">                    6个配置文件</span></span><br><span class="line"><span class="function">                    集群规划</span></span><br><span class="line"><span class="function">                    ....</span></span><br><span class="line"><span class="function">                    ....</span></span><br><span class="line"><span class="function">                    </span></span><br><span class="line"><span class="function">                5)远程发送</span></span><br><span class="line"><span class="function">                scp -r hadoop-2.7.6 hadoop02:$PWD</span></span><br><span class="line"><span class="function">                scp -r hadoop-2.7.6 hadoop03:$PWD</span></span><br><span class="line"><span class="function">                </span></span><br><span class="line"><span class="function">                scp -r /home/ap/apps/hadoop-2.7.6/etc/hadoop ap@cs1:/home/ap/apps/hadoop-2.7.6/etc</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">                远程发送/etc/pofile</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">                执行source /etc/pofile</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">                6）进行格式化     必须在namenode的节点（hdfs的主节点）</span></span><br><span class="line"><span class="function">                hadoop namenode -format</span></span><br><span class="line"><span class="function">            不配置目录默认/tmp  临时目录   可以随时回收的</span></span><br><span class="line"><span class="function">                7)启动</span></span><br><span class="line"><span class="function">                启动hdfs    start-dfs.sh  在任意节点都可以</span></span><br><span class="line"><span class="function">                启动yarn     start-yarn.sh   在yarn的主节点执行</span></span><br><span class="line"><span class="function">                jps命令查看</span></span><br><span class="line"><span class="function">                网页：</span></span><br><span class="line"><span class="function">                hdfs:     hadoop01:50070</span></span><br><span class="line"><span class="function">                yarn      hadoop03:8088</span></span><br><span class="line"><span class="function">                    </span></span><br><span class="line"><span class="function">                8) 去掉警告（在/etc/profile或者 .bash_profile 或者 .zshrc中添加）</span></span><br><span class="line"><span class="function">export HADOOP_HOME_WARN_SUPPRESS</span>=<span class="number">1</span></span><br><span class="line">                    </span><br><span class="line">                    </span><br><span class="line">测试yarn集群是否启动成功 (提交MapReduce例子程序试跑)</span><br><span class="line">ls apps/hadoop-<span class="number">2.7</span>.6/share/hadoop/mapreduce</span><br><span class="line">bin/hadoop jar hadoop-mapreduce-examples-<span class="number">2.6</span>.5.jar  pi <span class="number">5</span> <span class="number">5</span></span><br></pre></td></tr></table></figure><h3 id="可能遇到的错误"><a href="#可能遇到的错误" class="headerlink" title="可能遇到的错误"></a>可能遇到的错误</h3><h4 id="搭建过程中"><a href="#搭建过程中" class="headerlink" title="搭建过程中"></a>搭建过程中</h4><ol><li>主机找不到<ol><li>/etc/sysconfig/network     /etc/hosts</li><li>重启机器</li></ol></li><li>何时化的时候报错<ul><li>配置文件错误,  根据错误去相应文件进行调整, 修改完毕后, 重新格式化直到格式化成功</li></ul></li></ol><h4 id="启动过程中"><a href="#启动过程中" class="headerlink" title="启动过程中"></a>启动过程中</h4><h4 id="某些进程启动不了"><a href="#某些进程启动不了" class="headerlink" title="某些进程启动不了"></a>某些进程启动不了</h4><h5 id="措辞1-暴力"><a href="#措辞1-暴力" class="headerlink" title="措辞1: 暴力"></a>措辞1: 暴力</h5><ul><li>全部关闭集群重新启动<ul><li>stop-dfs.sh     在任意节点执行</li><li>stop-yarn.sh  在yarn的主节点启动</li><li>重新启动, 直接启动就可以了</li><li>start-dfs.sh</li><li>start-yarn.sh</li></ul></li></ul><h5 id="措施2-单独启动某些进程"><a href="#措施2-单独启动某些进程" class="headerlink" title="措施2: 单独启动某些进程"></a>措施2: 单独启动某些进程</h5><p>单独启动<code>hdfs</code>的相关进程</p><ul><li>hadoop-daemon.sh  start  hdfs 过程</li><li>hadoop-daemon.sh  start namenode</li><li>hadoop-daemon.sh start secondarynamenode</li></ul><p>单独启动<code>yarn</code>的相关命令</p><ul><li>yarn-daemon.sh start yarn 的相关过程</li><li>yarn-daemon.sh start resourcemanager</li></ul><h3 id="搭建过程中的注意事项"><a href="#搭建过程中的注意事项" class="headerlink" title="==搭建过程中的注意事项=="></a>==搭建过程中的注意事项==</h3><ol><li><p>集群的<strong>只能成功的格式化一次,</strong>  不成功的要一直到格式化成功, <strong>成功后就不能再次格式化</strong></p><ul><li><p><strong>格式化的过程中</strong>: 创建出来<strong>namenode</strong>存储的相关目录</p><ul><li><p>version文件: 记录仪集群的版本信息的,  每格式化一次, 就会产生一个新的版本信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">namespaceID=1163449973</span><br><span class="line">clusterID=CID-47f10077-2aef-4df6-a364-1a735515a100   <span class="comment">#记录集群的版本信息的</span></span><br><span class="line">cTime=0</span><br><span class="line">storageType=NAME_NODE</span><br><span class="line">blockpoolID=BP-1527239677-192.168.75.162-1527817150436</span><br><span class="line">layoutVersion=-63</span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>启动hdfs的时候</strong>:  生成<strong>datanode</strong>的相关数据信息</p><ul><li><p>version: 记录datanode 相关版本的信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clusterID=CID-47f10077-2aef-4df6-a364-1a735515a100</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ol><ul><li>两个文件中的<code>clusterID</code>相同的时候, datanode 才会认为是同一个集群的</li></ul><ol start="2"><li><p><strong>想要重复格式化, 分3步走</strong></p><ol><li><p>停止所有服务</p></li><li><p>删除 namenode 的数据目录</p><blockquote><p><code>rm -rf /home/ap/data/hadoopdata/name</code></p></blockquote></li><li><p>删除 datanode 的数据目录</p><blockquote><p><code>rm -rf /home/ap/data/hadoopdata/data</code></p></blockquote></li><li><p>此时才可以重新格式化,  否则会造成 datanode 启动不了, 注意, 关闭防火墙, 关闭vpn</p></li><li><p>也可以一步到位</p><blockquote><p><code>rm -rf /home/ap/data</code> </p><p>再重新格式化</p></blockquote></li></ol></li></ol><ol start="3"><li><p>集群搭建过程中<strong>环境变量的配置</strong>(jdk. hadoop…)</p><ul><li>在Linux中修改环境变量的地方有3个<ul><li>/etc/profile  系统环境变量, 针对所有用户的</li><li>~/.bashrc     用户环境变量</li><li>~/.bash_profile  用户环境变量</li></ul></li><li>这3个配置文件的加载顺序<ul><li>/etc/profile  &gt; .bashrc  &gt;   .bash_profile</li></ul></li><li>生效: 最后一次加载的生效</li></ul></li></ol><ol start="4"><li><p><strong>时间同步</strong>问题</p><ul><li>只要是完全分布式的, 多个节点之间一定要做时间同步,</li><li>目的:<ul><li>要和 北京/上海 时间保持一致?    no</li><li>集群背部各个节点之间时间保持一致  yes</li></ul></li><li>why ? <ul><li>集群内部各个节点之间需要通信, 尤其是datanode 和 namenode之间, 他们之间的通信依靠<strong>心跳机制</strong>, 他们之间的心跳存在一个时间控制, 这个时间是 <strong>630s</strong>, 他们之前需要做时间同步</li></ul></li></ul></li></ol><hr><h1 id="集群的安装模式"><a href="#集群的安装模式" class="headerlink" title="集群的安装模式"></a>集群的安装模式</h1><h3 id="1-单机模式"><a href="#1-单机模式" class="headerlink" title="1. 单机模式"></a>1. 单机模式</h3><ul><li>直接解压的方式, 什么都不配置, 并且在一个节点上</li><li>没有分布式文件系统, 所有的文件都是来自本地, 只能对本地的文件进行读写</li><li>几乎没人用, 测试时偶尔会用</li></ul><h3 id="2-伪分布式"><a href="#2-伪分布式" class="headerlink" title="2. 伪分布式"></a>2. 伪分布式</h3><ul><li>可以看做跑在一个节点上的完全分布式</li><li>有分布式文件系统, 只不过这个文件系统只有一个节点</li></ul><h3 id="3-完全分布式"><a href="#3-完全分布式" class="headerlink" title="==3. 完全分布式=="></a>==3. 完全分布式==</h3><p><strong><a href="https://app.yinxiang.com/shard/s37/nl/7399077/c3f4c3b9-249e-42cf-87e4-fb5db3bb37e2/" target="_blank" rel="noopener">参考文档</a></strong> </p><p><strong>规划</strong></p><p><strong>目前疑点:</strong>  NodeManager是根据什么配置到每台机器上的??</p><p>根据表征, 可能是根据 slave文件</p><table><thead><tr><th>主机名 / IP</th><th>HDFS</th><th>YARN</th></tr></thead><tbody><tr><td>cts1 / 192.168.56.131</td><td>NameNode</td><td>空的</td></tr><tr><td>cts2 / 192.168.56.132</td><td>DataNode</td><td>NodeManager</td></tr><tr><td>cts3 / 192.168.56.133</td><td>DataNode + Secondary NameNode</td><td>NodeManager</td></tr><tr><td>cts4 / 192.168.56.134</td><td>DataNode</td><td>NodeManager +ResourceManager</td></tr></tbody></table><ul><li>hdfs 为例 : 在宏观看就是一个大的节点, 后台采用的硬件配置是三天机器的硬件配置之和, 但是对用户来讲完全感觉不到</li><li>在完全分布式中, 有主节点, 有从节点</li><li>主节点 namenode只有一个, 从节点有多个, 真实生产中, namenode会单独做一个节点</li><li>如果集群中namenode宕机,  整个集群还可以使用吗?  不可以<ul><li>namenode: 主要作用存储<strong>元数据</strong> (管理数据的数据, 存储的就是datanode存储数据的描述)</li><li>datanode: 负责集群中真正处理数据存存储的</li></ul></li><li>如果namenode 宕机, 集群无法使用, 这也是完全分布式的一大缺点, 存在单点故障问题</li><li>一般生产中不太使用,  学习, 测试, 节点个数比较少的时候, 有时候也会使用这种模式</li><li>节点数目越多, namenode宕机的可能性越大, 压力太大</li><li>助理secondarynamenode: 只是一个助理, 只是分担namenode的压力, 但是不能代替</li><li>架构:<ul><li>一主多从</li></ul></li></ul><h3 id="4-高可用"><a href="#4-高可用" class="headerlink" title="==4. 高可用=="></a>==4. 高可用==</h3><ul><li>概念: 集群可以持续对外提供服务,  做到 7*24 小时不间断</li><li>依赖于zookeeper,  搭建放在 zookeeper课程之后</li><li>集群架构: <ul><li>双主多从</li></ul></li><li>有<strong>2个 namenode</strong>, 但是在同一时间只能有一个是 活跃的 namenode, 我们把这个活跃的namenode 成为 active 的, 另外一个是处理热备份状态,  我们将这个节点叫 <code>standby</code>, 但是<strong>2个主节点</strong>存储的<strong>元数据</strong>是一模一样的, 当 <code>active namenode</code>宕机的时候, standby的namenode 可以立马切换为 active 的namenode, 对外提供服务, 就可以做到 <strong>集群持续对外提供服务</strong>的功能</li><li>如果过一段时间, 宕机的 namenode 又活过来了, 宕机的 namenode 只能是变成 standby 的</li><li><strong>缺陷</strong>: 在同一时间中, 集群中只有一个active 的 namenode,  也就是说 <strong>集群中有主节点能力的节点 只有一个</strong>, 如果集群中, 节点个数过多(1000) 的时候, 会造成namenode的崩溃, namenode存储的是元数据,  元数据过多的时候, 会造成namenode的崩溃(<strong>两个都崩溃</strong>),  没有真正的分担namenode 的压力</li><li><strong>实际生产多使用高可用</strong></li></ul><h3 id="5-联邦机制"><a href="#5-联邦机制" class="headerlink" title="5. 联邦机制"></a>5. 联邦机制</h3><ul><li>同一个集群中可以有多个主节点, 这些主节点的地位是一样的.</li><li><strong>同一时间, 可以有多个活跃的 namenode</strong></li><li>这些 namenode 共同使用集群中所有的 datanode, 每个namenode 只负责管理集群中的 datanode上的一部分数据</li><li>一般超大集群搭建的时候:  <strong>联邦 + 高可用</strong></li><li><strong>超大集群使用</strong></li><li>每个namenode进行数据管理靠的Block Pool ID相同 </li><li>不同的namenode管理的数据Block Pool ID  不同</li></ul><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;引入&quot;&gt;&lt;a href=&quot;#引入&quot; class=&quot;headerlink&quot; title=&quot;引入&quot;&gt;&lt;/a&gt;引入&lt;/h1&gt;&lt;h3 id=&quot;面试题&quot;&gt;&lt;a href=&quot;#面试题&quot; class=&quot;headerlink&quot; title=&quot;面试题&quot;&gt;&lt;/a&gt;面试题&lt;/h3&gt;&lt;h
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="hadoop" scheme="https://airpoet.github.io/categories/Hadoop/hadoop/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop体系概览</title>
    <link href="https://airpoet.github.io/2018/05/31/Hadoop/Study/0-Hadoop/Hadoop%E4%BD%93%E7%B3%BB%E6%A6%82%E8%A7%88/"/>
    <id>https://airpoet.github.io/2018/05/31/Hadoop/Study/0-Hadoop/Hadoop体系概览/</id>
    <published>2018-05-30T16:46:03.290Z</published>
    <updated>2018-06-11T11:44:15.034Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hadoop核心组件"><a href="#Hadoop核心组件" class="headerlink" title="Hadoop核心组件"></a>Hadoop核心组件</h1><h3 id="整体直观概览"><a href="#整体直观概览" class="headerlink" title="整体直观概览"></a>整体直观概览</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-10-231704.png" alt="image-20180611071704138"></p><h2 id="1-分布式文件系统HDFS"><a href="#1-分布式文件系统HDFS" class="headerlink" title="1. 分布式文件系统HDFS"></a>1. 分布式文件系统HDFS</h2><h3 id="1-1-基本概念"><a href="#1-1-基本概念" class="headerlink" title="1.1 基本概念"></a>1.1 基本概念</h3><ul><li>将文件切分成指定大小的数据块并以多副本的存储形式存储在多个机器上</li><li>数据切分, 多副本, 容错等操作对用户是透明的</li></ul><h3 id="1-2-图示"><a href="#1-2-图示" class="headerlink" title="1.2 图示"></a>1.2 图示</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-05-30-165029.png" alt="image-20180531005029275"></p><h3 id="1-3-HDFS架构"><a href="#1-3-HDFS架构" class="headerlink" title="1.3  HDFS架构"></a>1.3  HDFS架构</h3><ul><li><strong>Datanode 定期向 Namenode 发 Hearbeat</strong></li><li><strong>元数据信息： 多份备份</strong></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-02-064845.jpg" alt=""></p><h3 id="1-4-HDFS的-IO-操作"><a href="#1-4-HDFS的-IO-操作" class="headerlink" title="1.4 HDFS的 IO 操作"></a>1.4 HDFS的 IO 操作</h3><ul><li>上面的是<strong>读</strong><ul><li>客户端先向 NameNode 寻址</li><li>然后再找 DataNode 拿数据</li></ul></li><li>下面的是<strong>写</strong><ul><li><strong>HDFS</strong> 不支持修改， 没有 leader 角色， <strong>不支持并发写</strong>， 只能支持非并发的追加</li><li>HBase 支持并发写和修改</li></ul></li><li><strong>删除</strong>： 删除的是<strong>元数据</strong>（索引信息）<ul><li>Datanode 会定期向 Namenode <strong>发送心跳</strong>， <strong>同步信息</strong>， 当 Namenode 发现 Datanode 上没有自己存储的信息时，就会把这部分信息删除掉。</li></ul></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-02-065117.png" alt="image-20180602145116939"></p><h3 id="1-5-HDFS-副本存放策略"><a href="#1-5-HDFS-副本存放策略" class="headerlink" title="1.5 HDFS 副本存放策略"></a>1.5 HDFS 副本存放策略</h3><ul><li><strong>复制因子为3时的 存放策略</strong><ul><li>如果写入者在一个 datanode 上， 则把一份拷贝放在本地机器上， 否则随机放到一个 datande 上</li><li>另一个副本放在不同的（远程）机架的节点上， 最后一个副本存放在同一个机架的不同节点上</li><li>这一策略削减了机架间的写入流量，通常提高了写入性能</li></ul></li><li>复制因子大于3， 则随机确定第4个和其它的副本位置，同时将每个拷贝的数目保持在上限以下(基本上是<code>(副本数 - 1) / racks + 2</code>)。</li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-06-02-093751.png" alt="image-20180602173750873"></p><h2 id="2-资源调度系统-YARN"><a href="#2-资源调度系统-YARN" class="headerlink" title="2. 资源调度系统 YARN"></a>2. 资源调度系统 YARN</h2><h3 id="2-1-基本概念"><a href="#2-1-基本概念" class="headerlink" title="2.1 基本概念"></a>2.1 基本概念</h3><ul><li>YARN:  Yet Another Resource Negotiator</li><li>负责整个集群资源的管理和调度</li><li>YAEN特点: 扩展性 &amp; 容错性 &amp; 多框架资源统一调度</li></ul><h3 id="2-2-图示"><a href="#2-2-图示" class="headerlink" title="2.2 图示"></a>2.2 图示</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-05-30-165444.png" alt="image-20180531005443543"></p><h2 id="3-分布式计算框架-MapReduce"><a href="#3-分布式计算框架-MapReduce" class="headerlink" title="3. 分布式计算框架 MapReduce"></a>3. 分布式计算框架 MapReduce</h2><h3 id="3-1-基本概念"><a href="#3-1-基本概念" class="headerlink" title="3.1 基本概念"></a>3.1 基本概念</h3><ul><li>源于Google的MapReduce论文, 论文发表于2004年12月</li><li>MapReduce是Google MapReduce的克隆版</li><li>MapReduce的特点: 扩展性 &amp; 容错性 &amp; 海量数据离线处理</li></ul><h3 id="3-2-图示"><a href="#3-2-图示" class="headerlink" title="3.2 图示"></a>3.2 图示</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-05-30-165821.png" alt="image-20180531005821191"></p><hr><h1 id="Hadoop优势"><a href="#Hadoop优势" class="headerlink" title="Hadoop优势"></a>Hadoop优势</h1><h3 id="1-高可靠性"><a href="#1-高可靠性" class="headerlink" title="1.  高可靠性"></a>1.  高可靠性</h3><ul><li>数据存储: 数据块多副本</li><li>数据计算: 重新调度作业计算</li></ul><h3 id="2-高扩展性"><a href="#2-高扩展性" class="headerlink" title="2.  高扩展性"></a>2.  高扩展性</h3><ul><li>存储/计算资源不够时, 可以横向的线性扩展机器</li><li>一个集群中可以包含数以千计的节点</li></ul><h3 id="3-其它"><a href="#3-其它" class="headerlink" title="3.  其它"></a>3.  其它</h3><ul><li>存储在廉价机器上, 降低成本</li><li>成熟的生态圈</li></ul><hr><h1 id="Hadoop的发展史"><a href="#Hadoop的发展史" class="headerlink" title="Hadoop的发展史"></a>Hadoop的发展史</h1><p><a href="http://www.infoq.com/cn/articles/hadoop-ten-years-interpretation-and-development-forecast" target="_blank" rel="noopener">见文章: Hadoop十年解读与发展预测</a></p><p><a href="http://hadoop.apache.org/" target="_blank" rel="noopener">Hadoop官网</a></p><hr><h1 id="Hadoop生态系统"><a href="#Hadoop生态系统" class="headerlink" title="Hadoop生态系统"></a>Hadoop生态系统</h1><h3 id="1-图示"><a href="#1-图示" class="headerlink" title="1. 图示"></a>1. 图示</h3><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-05-30-171338.png" alt="image-20180531011337987"></p><h3 id="2-特点"><a href="#2-特点" class="headerlink" title="2. 特点"></a>2. 特点</h3><ul><li>开源,  社区活跃</li><li>囊括了大数据处理的方方面面</li><li>成熟的生态圈</li></ul><hr><h1 id="Hadoop常用发行版及选型"><a href="#Hadoop常用发行版及选型" class="headerlink" title="Hadoop常用发行版及选型"></a>Hadoop常用发行版及选型</h1><ul><li>Apache Hadoop</li><li><strong>CDH : Cloudera Distributed Hadoop</strong>  （国内用的比较多）</li><li>HDP : Hortonworks Data Platform  </li></ul><blockquote><p><strong>使用:</strong></p><p>CDH使用占比 60-70</p><p>hadoop:  hadoop-2.6.0-cdh5.7.0  </p><p>hive : hive-1.1.0-cdh5.7.0</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hadoop核心组件&quot;&gt;&lt;a href=&quot;#Hadoop核心组件&quot; class=&quot;headerlink&quot; title=&quot;Hadoop核心组件&quot;&gt;&lt;/a&gt;Hadoop核心组件&lt;/h1&gt;&lt;h3 id=&quot;整体直观概览&quot;&gt;&lt;a href=&quot;#整体直观概览&quot; class=&quot;
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="hadoop" scheme="https://airpoet.github.io/categories/Hadoop/hadoop/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="hadoop" scheme="https://airpoet.github.io/tags/hadoop/"/>
    
  </entry>
  
</feed>
