<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>A.P的文艺杂谈</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://airpoet.github.io/"/>
  <updated>2018-07-30T03:09:19.161Z</updated>
  <id>https://airpoet.github.io/</id>
  
  <author>
    <name>airpoet</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark重点解析(三) =&gt; Spark SQL</title>
    <link href="https://airpoet.github.io/2018/07/27/Spark/Spark%E9%87%8D%E7%82%B9%E8%A7%A3%E6%9E%90(%E4%B8%89)-=-Spark-SQL/"/>
    <id>https://airpoet.github.io/2018/07/27/Spark/Spark重点解析(三)-=-Spark-SQL/</id>
    <published>2018-07-27T06:32:27.283Z</published>
    <updated>2018-07-30T03:09:19.161Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一-SparkSQL-的前世今生"><a href="#一-SparkSQL-的前世今生" class="headerlink" title="一. SparkSQL 的前世今生"></a>一. SparkSQL 的前世今生</h1><ul><li>Hive =&gt; MapReduce =&gt; HDFS</li><li>Shark =&gt; 使用 Hive 的 SQL 解析引擎 =&gt; RDD =&gt; 通过Hive 的metadata表去操作 HDFS</li><li>SparkSQL =&gt; <strong>使用自己SQL 解析引擎</strong> =&gt; RDD =&gt; 通过Hive 的metadata表去操作 HDFS</li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-27-064024.jpg" alt=""></p><hr><h1 id="二-SparkSession"><a href="#二-SparkSession" class="headerlink" title="二. SparkSession"></a>二. SparkSession</h1><p>在 Spark 的早期版本(<strong>1.x 版本)</strong>中，SparkContext 是 Spark 的主要切入点，由于 RDD 是主要的 API，我 们通过 sparkContext 来创建和操作 RDD。对于每个其他的 API，我们需要使用不同的 context。 例如：</p><p>SparkContext      =&gt; 创建 RDD</p><p>StreamingContext =&gt; 创建 Streaming</p><p>SQLContext          =&gt; 创建 SQL</p><p>HiveContext          =&gt; 创建 Hive</p><p><strong>从 Spark 2.0开始, 引入 <code>SparkSession</code></strong></p><p>—- 为用户提供一个统一的切入点使用 Spark 各项功能</p><p>—- 允许用户通过它调用 DataFrame 和 Dataset 相关 API 来编写程序</p><p>—- 减少了用户需要了解的一些概念，可以很容易的与 Spark 进行交互</p><p>—- 与 Spark 交互之时不需要显示的创建 SparkConf、SparkContext 以及 SQlContext，这些对象已经封闭在 SparkSession 中</p><p>—- SparkSession 提供对 Hive 特征的内部支持：用 HiveQL 写 SQL 语句，访问 Hive UDFs，从 Hive 表中读取数据。</p><hr><h1 id="三-DataFrame"><a href="#三-DataFrame" class="headerlink" title="三. DataFrame"></a>三. DataFrame</h1><p><strong>注意</strong>: 这里的操作都是基于1.x 版本,  与2.x 的区别就是2.x 统一了操作入口为 SparkSession</p><p><a href="https://airpoet.github.io/2018/07/20/Spark/i-Spark-2/#more">2.x 的操作代码见我的另一篇 bolg</a></p><blockquote><p>从 json 文件读取为 dataframe, 使用spark 的 api 调用</p></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SQLContext</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataFrameOperation</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//create  datarame</span></span><br><span class="line">    <span class="comment">//首先创建程序入口</span></span><br><span class="line">    <span class="keyword">val</span> conf=<span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"DataFrameOperation"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc=<span class="keyword">new</span> <span class="type">SparkContext</span>(conf);</span><br><span class="line">    <span class="comment">//create sqlcontext</span></span><br><span class="line">    <span class="keyword">val</span> sqlContext=<span class="keyword">new</span> <span class="type">SQLContext</span>(sc);</span><br><span class="line">    <span class="keyword">val</span> df=sqlContext.read.json(<span class="string">"hdfs://mycluster/user/ap/sparkdatas/people"</span>)</span><br><span class="line">    df.show();  </span><br><span class="line">    </span><br><span class="line">    <span class="comment">//print schema</span></span><br><span class="line">    df.printSchema()</span><br><span class="line">    <span class="comment">//name age</span></span><br><span class="line">    df.select(<span class="string">"name"</span>).show();</span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    df.select(df(<span class="string">"name"</span>),df(<span class="string">"age"</span>)+<span class="number">1</span>).show()</span><br><span class="line">    <span class="comment">//where</span></span><br><span class="line">    df.filter(df(<span class="string">"age"</span>) &gt; <span class="number">21</span>).show()</span><br><span class="line">    <span class="comment">//groupby</span></span><br><span class="line">    df.groupBy(<span class="string">"age"</span>).count().show();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="四-RDD-转为-DataFrame"><a href="#四-RDD-转为-DataFrame" class="headerlink" title="四. RDD 转为 DataFrame"></a>四. RDD 转为 DataFrame</h1><h2 id="1-使用对象-RDD-反射-Reflection-的方式"><a href="#1-使用对象-RDD-反射-Reflection-的方式" class="headerlink" title="1.使用对象 RDD 反射 Reflection 的方式"></a>1.使用对象 RDD 反射 Reflection 的方式</h2><blockquote><p>Scala 方式</p></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">步骤: </span><br><span class="line"><span class="number">1</span>) 构造一个封装了指定对象的<span class="type">RDD</span></span><br><span class="line"><span class="number">2</span>) 引入sparksession的隐式转换</span><br><span class="line"><span class="number">3</span>) 调用 rdd.toDF()</span><br><span class="line">=====================================================================</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SQLContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">age:<span class="type">Int</span>,name:<span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">RDD2DataFrameReflection</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">     <span class="comment">//create  datarame</span></span><br><span class="line">    <span class="comment">//首先创建程序入口</span></span><br><span class="line">    <span class="keyword">val</span> conf=<span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"RDD2DataFrameReflection"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc=<span class="keyword">new</span> <span class="type">SparkContext</span>(conf);</span><br><span class="line">  </span><br><span class="line">    <span class="comment">//create sqlcontext</span></span><br><span class="line">    <span class="keyword">val</span> sqlContext=<span class="keyword">new</span> <span class="type">SQLContext</span>(sc);</span><br><span class="line">    <span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line">    <span class="keyword">val</span> personRDD= sc.textFile(<span class="string">"hdfs://mycluster/user/ap/sparkdatas/peopletxt/people.txt"</span>, <span class="number">2</span>)</span><br><span class="line">    .map &#123; line =&gt; line.split(<span class="string">","</span>) &#125;.map &#123; p =&gt; <span class="type">Person</span>(p(<span class="number">1</span>).trim().toInt,p(<span class="number">0</span>)) &#125;</span><br><span class="line">    <span class="keyword">val</span> personDF=personRDD.toDF()</span><br><span class="line">   <span class="comment">// 或者 personDF.createOrReplaceTempView("person") / createGlobalTempView / createTempView</span></span><br><span class="line">    personDF.registerTempTable(<span class="string">"person"</span>)</span><br><span class="line">    <span class="keyword">val</span> personDataframe=sqlContext.sql(<span class="string">"select name,age from person where age &gt; 13 and age &lt;= 19"</span>)</span><br><span class="line">    personDataframe.rdd.foreach &#123; row =&gt; println(row.getString(<span class="number">0</span>)+<span class="string">"  "</span>+row.getString(<span class="number">1</span>)) &#125;</span><br><span class="line">    personDataframe.rdd.saveAsTextFile(<span class="string">"hdfs://hadoop1:9000/reflectionresult/"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Java 方式</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.DataFrame;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SQLContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RDD2DataFrameReflection</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">conf.setAppName(<span class="string">"RDD2DataFrameReflection"</span>);</span><br><span class="line">JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">SQLContext sqlContext = <span class="keyword">new</span> SQLContext(sc);</span><br><span class="line">JavaRDD&lt;Person&gt; PersonRDD = sc.textFile(<span class="string">"hdfs://hadoop1:9000/examples/src/main/resources/people.txt"</span>)</span><br><span class="line">.map(<span class="keyword">new</span> Function&lt;String, Person&gt;() &#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> Person <span class="title">call</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">String[] strs = line.split(<span class="string">","</span>);</span><br><span class="line">String name=strs[<span class="number">0</span>];</span><br><span class="line"><span class="keyword">int</span> age=Integer.parseInt(strs[<span class="number">1</span>].trim());</span><br><span class="line">Person person=<span class="keyword">new</span> Person(age,name);</span><br><span class="line"><span class="keyword">return</span> person;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">DataFrame personDF = sqlContext.createDataFrame(PersonRDD, Person.class);</span><br><span class="line">personDF.registerTempTable(<span class="string">"person"</span>);</span><br><span class="line"></span><br><span class="line">DataFrame resultperson = sqlContext.sql(<span class="string">"select name,age from person where age &gt; 13 and age &lt;= 19"</span>);</span><br><span class="line">resultperson.javaRDD().foreach(<span class="keyword">new</span> VoidFunction&lt;Row&gt;() &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Row row)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">//把每一条数据都看成是一个row  row(0)=name  row(1)=age </span></span><br><span class="line">System.out.println(<span class="string">"name"</span>+row.getString(<span class="number">0</span>));</span><br><span class="line">System.out.println(<span class="string">"age"</span>+row.getInt(<span class="number">1</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">resultperson.javaRDD().saveAsTextFile(<span class="string">"hdfs://hadoop1:9000/reflectionresult"</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-使用构造StructType方式"><a href="#2-使用构造StructType方式" class="headerlink" title="2.  使用构造StructType方式"></a>2.  使用构造StructType方式</h2><blockquote><p>scala</p></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">要点: </span><br><span class="line"><span class="number">1</span>) 构造 <span class="type">StructType</span></span><br><span class="line"><span class="number">2</span>) 构造 rowRDD </span><br><span class="line"><span class="keyword">val</span> rowRDD = studentRDD.map(p =&gt; <span class="type">Row</span>(p(<span class="number">0</span>).toInt, p(<span class="number">1</span>).trim, p(<span class="number">2</span>).trim, p(<span class="number">3</span>).toInt, p(<span class="number">4</span>).trim))</span><br><span class="line"><span class="number">3</span>) 构造 <span class="type">DataFrame</span></span><br><span class="line">df = sparksession.createDataFrame(rowRDD, schema) </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SQLContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.<span class="type">StructType</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.<span class="type">StructField</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.<span class="type">StringType</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RDD2DataFrameProgrammatically</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> conf=<span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"RDD2DataFrameProgrammatically"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc=<span class="keyword">new</span> <span class="type">SparkContext</span>(conf);</span><br><span class="line">  </span><br><span class="line">    <span class="comment">//create sqlcontext</span></span><br><span class="line">    <span class="keyword">val</span> sqlContext=<span class="keyword">new</span> <span class="type">SQLContext</span>(sc);</span><br><span class="line">    <span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line">    <span class="keyword">val</span> personRDD= sc.textFile(<span class="string">"hdfs://hadoop1:9000/examples/src/main/resources/people.txt"</span>, <span class="number">1</span>)</span><br><span class="line">      </span><br><span class="line"><span class="comment">////////////////////////////////////////////////////////////////////////</span></span><br><span class="line">    <span class="comment">//create schema</span></span><br><span class="line">    <span class="keyword">val</span> schemaString=<span class="string">"nameage"</span>;</span><br><span class="line">    <span class="keyword">val</span> schema=<span class="type">StructType</span>(</span><br><span class="line">        schemaString.split(<span class="string">"\t"</span>).map &#123; fieldsName =&gt; <span class="type">StructField</span>(fieldsName,<span class="type">StringType</span>,<span class="literal">true</span>) &#125;</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">/**  参数</span></span><br><span class="line"><span class="comment">            case class StructField(</span></span><br><span class="line"><span class="comment">            name: String,</span></span><br><span class="line"><span class="comment">            dataType: DataType,</span></span><br><span class="line"><span class="comment">            nullable: Boolean = true,</span></span><br><span class="line"><span class="comment">            metadata: Metadata = Metadata.empty)</span></span><br><span class="line"><span class="comment">    构造方式:</span></span><br><span class="line"><span class="comment">    val schema = StructType(</span></span><br><span class="line"><span class="comment">              List(</span></span><br><span class="line"><span class="comment">                StructField("id", IntegerType, true),</span></span><br><span class="line"><span class="comment">                StructField("name", StringType, true),</span></span><br><span class="line"><span class="comment">                StructField("sex", StringType, true),</span></span><br><span class="line"><span class="comment">                StructField("age", IntegerType, true),</span></span><br><span class="line"><span class="comment">                StructField("department", StringType, true)</span></span><br><span class="line"><span class="comment">              )</span></span><br><span class="line"><span class="comment">)</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">     <span class="comment">//create rowrdd</span></span><br><span class="line">    <span class="keyword">val</span> rowRDD=personRDD.map &#123; line =&gt; line.split(<span class="string">","</span>) &#125;.map &#123; p =&gt; <span class="type">Row</span>(p(<span class="number">0</span>),p(<span class="number">1</span>)) &#125;</span><br><span class="line">    <span class="keyword">val</span> personDF=sqlContext.createDataFrame(rowRDD, schema)</span><br><span class="line">      </span><br><span class="line"><span class="comment">////////////////////////////////////////////////////////////////////////      </span></span><br><span class="line">    personDF.registerTempTable(<span class="string">"person"</span>);</span><br><span class="line">     <span class="keyword">val</span> personDataframe=sqlContext.sql(<span class="string">"select name,age from person where age &gt; 13 and age &lt;= 19"</span>)</span><br><span class="line">    </span><br><span class="line">     personDataframe.rdd.foreach &#123; row =&gt; println(row.getString(<span class="number">0</span>)+<span class="string">"=&gt;  "</span>+row.getString(<span class="number">1</span>)) &#125;</span><br><span class="line">    personDataframe.rdd.saveAsTextFile(<span class="string">"hdfs://hadoop1:9000/RDD2DataFrameProgrammatically/"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Java</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.DataFrame;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.RowFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SQLContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DataTypes;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructField;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructType;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RDD2DataFrameProgrammactically</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">conf.setAppName(<span class="string">"RDD2DataFrameProgrammactically"</span>);</span><br><span class="line">JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">SQLContext sqlContext = <span class="keyword">new</span> SQLContext(sc);</span><br><span class="line">JavaRDD&lt;String&gt; personRDD = sc.textFile(<span class="string">"hdfs://hadoop1:9000/examples/src/main/resources/people.txt"</span>);</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 这里的schemaString 是从数据库里面动态获取从来的</span></span><br><span class="line"><span class="comment"> * 在实际的开发中我们需要写另外的代码去获取</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">String schemaString=<span class="string">"nameage"</span>;</span><br><span class="line"><span class="comment">//create  schema</span></span><br><span class="line">ArrayList&lt;StructField&gt; list = <span class="keyword">new</span> ArrayList&lt;StructField&gt;();</span><br><span class="line"><span class="keyword">for</span>(String str:schemaString.split(<span class="string">"\t"</span>))&#123;</span><br><span class="line">list.add(DataTypes.createStructField(str, DataTypes.StringType, <span class="keyword">true</span>));</span><br><span class="line">&#125;</span><br><span class="line">StructType schema = DataTypes.createStructType(list);</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需要将RDD转换为一个JavaRDD《Row》</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">JavaRDD&lt;Row&gt; rowRDD = personRDD.map(<span class="keyword">new</span> Function&lt;String, Row&gt;() &#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> Row <span class="title">call</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">String[] fields = line.split(<span class="string">","</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> RowFactory.create(fields[<span class="number">0</span>],fields[<span class="number">1</span>]);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">DataFrame personDF = sqlContext.createDataFrame(rowRDD, schema);</span><br><span class="line">personDF.registerTempTable(<span class="string">"person"</span>);</span><br><span class="line"></span><br><span class="line">DataFrame resultperson = sqlContext.sql(<span class="string">"select name,age from person where age &gt; 13 and age &lt;= 19"</span>);</span><br><span class="line">resultperson.javaRDD().foreach(<span class="keyword">new</span> VoidFunction&lt;Row&gt;() &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Row row)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">//把每一条数据都看成是一个row  row(0)=name  row(1)=age </span></span><br><span class="line">System.out.println(<span class="string">"name"</span>+row.getString(<span class="number">0</span>));</span><br><span class="line">System.out.println(<span class="string">"age"</span>+row.getInt(<span class="number">1</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">resultperson.javaRDD().saveAsTextFile(<span class="string">"hdfs://hadoop1:9000/reflectionresult"</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3-DataSet-amp-DataFrame-amp-RDD"><a href="#3-DataSet-amp-DataFrame-amp-RDD" class="headerlink" title="3.DataSet &amp; DataFrame &amp; RDD"></a>3.DataSet &amp; DataFrame &amp; RDD</h2><p><strong>RDD 仅表示数据集，RDD 没有元数据，也就是说没有字段语义定义</strong></p><p><strong>DataFrame = RDD+Schema = SchemaRDD</strong></p><p><strong>Schema 是就是元数据，是语义描述信息。</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-27-084726.png" alt="image-20180727164726341"></p><p><strong>DataFrame 是一种特殊类型的 Dataset，DataSet[Row] = DataFrame</strong></p><p><strong>DataFrame 的数据类型统一是 Row,  缺点: </strong></p><ul><li>Row 不能直接操作 domain 对象</li><li>函数风格编程，没有面向对象风格的 API</li></ul><p>所以，Spark SQL 引入了 Dataset，扩展了 DataFrame API，提供了编译时类型检查，面向对象风格的 API。</p><ul><li>Dataset 可以和 DataFrame、RDD 相互转换。DataFrame=Dataset[Row]</li><li>可见 DataFrame 是一种特殊的 Dataset。</li></ul><p><strong>既然 Spark SQL 提供了 SQL 访问方式，那为什么还需要 DataFrame 和 Dataset 的 API 呢？</strong></p><ul><li>SQL 的表达能力却是有限的</li><li>DataFrame 和 Dataset 可以采用更加通用的语言（Scala 或 Python）来表达用户的 查询请求。</li><li>DataFrame / Dataset 的面向对象语法, 可以更快捕捉错误，因为 SQL 是运行时捕获异常，而 Dataset 是 编译时检查错误。</li></ul><p><strong>DataFrame &amp; Dataset 的部分 api</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-27-085823.png" alt="image-20180727165823013"></p><h4 id="总的来讲"><a href="#总的来讲" class="headerlink" title="总的来讲"></a>总的来讲</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataFream</span>=<span class="type">DataSet</span>[row]  <span class="comment">//弱类型</span></span><br><span class="line"><span class="type">DataFream</span>=<span class="type">Untyped</span> <span class="type">Dataset</span></span><br><span class="line"><span class="type">DataSet</span>=<span class="type">Untyped</span> <span class="type">Dataset</span> +typed <span class="type">Dataset</span></span><br></pre></td></tr></table></figure><h4 id="举个🌰"><a href="#举个🌰" class="headerlink" title="举个🌰"></a>举个🌰</h4><blockquote><p><strong>SparkCore: RDD</strong></p></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">defined</span> <span class="title">class</span> <span class="title">Person</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">val</span> <span class="title">rdd</span> </span>= sc.textFile(<span class="string">"/user/ap/sparkdatas/peopletxt/people.txt"</span>).map(line =&gt; line.split(<span class="string">","</span>)).map(x =&gt; <span class="type">Person</span>(x(<span class="number">0</span>),x(<span class="number">1</span>).trim.toLong))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Person</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">14</span>] at map at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"><span class="comment">// 这里得到的是 RDD[Person] 类型</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect</span><br><span class="line">res11: <span class="type">Array</span>[<span class="type">Person</span>] = <span class="type">Array</span>(<span class="type">Person</span>(<span class="type">Michael</span>,<span class="number">29</span>), <span class="type">Person</span>(<span class="type">Andy</span>,<span class="number">30</span>), <span class="type">Person</span>(<span class="type">Justin</span>,<span class="number">19</span>))</span><br></pre></td></tr></table></figure><blockquote><p><strong>SparkSQL: DataFrame</strong> </p></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">"/user/ap/sparkdatas/people"</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line"><span class="comment">// Row  =&gt;相当于 数据库的表里面的一行数据  DataFrame=DataSet[Row]</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>SparkSQL: DataSet</strong> </p></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// as[Person] 把 Json 读出来的 DataFrame 直接反射到 Person类上, 转为 DataSet  !! 还有这种骚操作??</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> personDS = spark.read.json(<span class="string">"/user/ap/sparkdatas/people"</span>).as[<span class="type">Person</span>]</span><br><span class="line"><span class="comment">// </span></span><br><span class="line">personDS: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Person</span>] = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 再把 dataSet 转为 dataFrame</span></span><br><span class="line"><span class="keyword">val</span> personDS2 = personDS.toDF </span><br><span class="line">res13: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br></pre></td></tr></table></figure><h4 id="来张图"><a href="#来张图" class="headerlink" title="来张图!"></a>来张图!</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-27-145640.png" alt="image-20180727225640182"></p><hr><h1 id="五-数据源之load-和-Save"><a href="#五-数据源之load-和-Save" class="headerlink" title="五. 数据源之load 和 Save"></a>五. 数据源之load 和 Save</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SQLContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hdfs.server.namenode.<span class="type">SafeMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SaveMode</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DatasourceLoadAndSave</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf=<span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"RDD2DataFrameReflection"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc=<span class="keyword">new</span> <span class="type">SparkContext</span>(conf);</span><br><span class="line">    <span class="keyword">val</span> sqlContext=<span class="keyword">new</span> <span class="type">SQLContext</span>(sc);</span><br><span class="line">  <span class="comment">//  sqlContext.read.json("person.json");</span></span><br><span class="line">    <span class="comment">//sparksql默认支持的是parquet文件格式</span></span><br><span class="line">  <span class="keyword">val</span> df=  sqlContext.read.load(<span class="string">"examples/src/main/resources/users.parquet"</span>);</span><br><span class="line">  <span class="comment">//  sqlContext.read.format("json").load("person.json")</span></span><br><span class="line">  <span class="comment">//  sqlContext.read.format("parquet").load("users.parquet")</span></span><br><span class="line">  <span class="comment">//df保存的时候，如果不指定保存的文件格式，默认就是parquet</span></span><br><span class="line">    df.select(<span class="string">"name"</span>, <span class="string">"age"</span>).write.save(<span class="string">"namesAndAges.parquet"</span>);</span><br><span class="line">  <span class="comment">//  df.select("name", "age").write.json("user.json")</span></span><br><span class="line">  <span class="comment">//   df.select("name", "age").write.format("json").save("user.json")</span></span><br><span class="line">    df.select(<span class="string">"name"</span>, <span class="string">"age"</span>).write.format(<span class="string">"parquet"</span>).save(<span class="string">"user.parquet"</span>)</span><br><span class="line">    </span><br><span class="line">    df.select(<span class="string">"name"</span>, <span class="string">"age"</span>).write.mode(<span class="type">SaveMode</span>.<span class="type">ErrorIfExists</span>).format(<span class="string">"json"</span>).save(<span class="string">"hdfs://hadoop1:9000/user.json"</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 保存结果文件的时候的策略</span></span><br><span class="line"><span class="comment">     * if data/table already exists,</span></span><br><span class="line"><span class="comment">       Append,    // contents of the DataFrame are expected to be appended to existing data</span></span><br><span class="line"><span class="comment">       Overwrite,   // existing data is expected to be overwritten</span></span><br><span class="line"><span class="comment">       ErrorIfExists,   // an exception is expected to be thrown</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="六-数据源之parquet-files-合并"><a href="#六-数据源之parquet-files-合并" class="headerlink" title="六. 数据源之parquet  files 合并"></a>六. 数据源之parquet  files 合并</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df1 = sc.makeRDD(<span class="number">1</span> to <span class="number">5</span>).map(i =&gt; (i, i * <span class="number">2</span>)).toDF(<span class="string">"single"</span>, <span class="string">"double"</span>)</span><br><span class="line">df1.write.parquet(<span class="string">"/user/ap/test_table/key=1"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df2 = sc.makeRDD(<span class="number">6</span> to <span class="number">10</span>).map(i =&gt; (i, i * <span class="number">3</span>)).toDF(<span class="string">"single"</span>, <span class="string">"triple"</span>)</span><br><span class="line">df2.write.parquet(<span class="string">"/user/ap/test_table/key=2"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//合并两个dataframe, 我们期望的就是合并出来应该是三个属性分别是single double triple</span></span><br><span class="line"><span class="comment">//实际上最后还会有一个分区就是keys</span></span><br><span class="line"><span class="keyword">val</span> df3 = sqlContext.read.option(<span class="string">"mergeSchema"</span>, <span class="string">"true"</span>).parquet(<span class="string">"/user/ap/test_table"</span>)</span><br><span class="line"></span><br><span class="line">df3.printSchema()</span><br><span class="line">df3.show()</span><br></pre></td></tr></table></figure><h1 id="七-数据源之-MySQL"><a href="#七-数据源之-MySQL" class="headerlink" title="七. 数据源之 MySQL"></a>七. 数据源之 MySQL</h1><p><a href="https://airpoet.github.io/2018/07/20/Spark/i-Spark-2/#more">在 idea 中访问 mysql 的, 看这里</a></p><p><strong>注意: 启动 Spark Shell，必须指定 mysql 连接驱动 jar 包</strong></p><blockquote><p>启动本机的单进程 Shell</p></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark-shell \</span><br><span class="line">--jars $<span class="type">SPARK_HOME</span>/jars/mysql-connector-java<span class="number">-5.1</span><span class="number">.40</span>-bin.jar \</span><br><span class="line">--driver-<span class="class"><span class="keyword">class</span><span class="title">-path</span> <span class="title">$SPARK_HOME/jars/mysql-connector-java-5</span>.1.40<span class="title">-bin</span>.<span class="title">jar</span></span></span><br></pre></td></tr></table></figure><blockquote><p>连接 Spark 集群的 shell</p></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">spark-shell \</span><br><span class="line">--driver-<span class="class"><span class="keyword">class</span><span class="title">-path</span> <span class="title">$SPARK_HOME/jars/mysql-connector-java-5</span>.1.40<span class="title">-bin</span>.<span class="title">jar</span> <span class="title">\</span></span></span><br><span class="line"><span class="class"><span class="title">--master</span> <span class="title">yarn</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">查询</span></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">val</span> <span class="title">jdbcDF</span> </span>= spark.read.format(<span class="string">"jdbc"</span>).option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://cs2:3306/test"</span>).option(<span class="string">"dbtable"</span>, <span class="string">"student"</span>).option(<span class="string">"user"</span>, <span class="string">"root"</span>).option(<span class="string">"password"</span>, <span class="string">"123"</span>).option(<span class="string">"driver"</span>,<span class="string">"com.mysql.jdbc.Driver"</span>).load();</span><br><span class="line"></span><br><span class="line">jdbcDF: org.apache.spark.sql.<span class="type">DataFrame</span> = [id: int, name: string ... <span class="number">2</span> more fields]</span><br><span class="line"></span><br><span class="line">scala&gt; jdbcDF.show</span><br><span class="line">+---+------------+--------+-----+</span><br><span class="line">| id|        name|  course|score|</span><br><span class="line">+---+------------+--------+-----+</span><br><span class="line">|  <span class="number">1</span>|     huangbo|    math|   <span class="number">81</span>|</span><br><span class="line">|  <span class="number">2</span>|     huangbo| englist|   <span class="number">87</span>|</span><br><span class="line">|  <span class="number">3</span>|     huangbo|computer|   <span class="number">67</span>|</span><br><span class="line">|  <span class="number">4</span>|     xuzheng|    math|   <span class="number">89</span>|</span><br><span class="line">|  <span class="number">5</span>|     xuzheng| english|   <span class="number">92</span>|</span><br><span class="line">|  <span class="number">6</span>|     xuzheng|computer|   <span class="number">83</span>|</span><br><span class="line">|  <span class="number">7</span>|wangbaoqiang|    math|   <span class="number">78</span>|</span><br><span class="line">|  <span class="number">8</span>|wangbaoqiang| english|   <span class="number">88</span>|</span><br><span class="line">|  <span class="number">9</span>|wangbaoqiang|computer|   <span class="number">90</span>|</span><br><span class="line">| <span class="number">10</span>|    dengchao|    math|   <span class="number">88</span>|</span><br><span class="line">+---+------------+--------+-----+</span><br></pre></td></tr></table></figure><blockquote><p>通过 Spark-submit 的方式</p></blockquote><p>编写代码</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//通过读取文件创建 RDD</span></span><br><span class="line"><span class="keyword">val</span> studentRDD = sc.textFile(args(<span class="number">0</span>)).map(_.split(<span class="string">","</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">//通过 StructType 直接指定每个字段的 schema</span></span><br><span class="line"><span class="keyword">val</span> schema = <span class="type">StructType</span>(</span><br><span class="line">  <span class="type">List</span>(</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"id"</span>, <span class="type">IntegerType</span>, <span class="literal">true</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"name"</span>, <span class="type">StringType</span>, <span class="literal">true</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"sex"</span>, <span class="type">StringType</span>, <span class="literal">true</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"age"</span>, <span class="type">IntegerType</span>, <span class="literal">true</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"department"</span>, <span class="type">StringType</span>, <span class="literal">true</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 RDD 映射到 rowRDD</span></span><br><span class="line"><span class="keyword">val</span> rowRDD = studentRDD.map(p =&gt; <span class="type">Row</span>(p(<span class="number">0</span>).toInt, p(<span class="number">1</span>).trim, p(<span class="number">2</span>).trim, p(<span class="number">3</span>).toInt,</span><br><span class="line">  p(<span class="number">4</span>).trim))</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 schema 信息应用到 rowRDD 上</span></span><br><span class="line"><span class="keyword">val</span> studentDataFrame = sqlContext.createDataFrame(rowRDD, schema)</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建 Properties 存储数据库相关属性</span></span><br><span class="line"><span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">prop.put(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">prop.put(<span class="string">"password"</span>, <span class="string">"root"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//将数据追加到数据库</span></span><br><span class="line">studentDataFrame.write.mode(<span class="string">"append"</span>).jdbc(<span class="string">"jdbc:mysql://hadoop02:3306/spider"</span>,</span><br><span class="line">  <span class="string">"student"</span>, prop)</span><br><span class="line"></span><br><span class="line"><span class="comment">//停止 SparkContext</span></span><br><span class="line">sc.stop()</span><br></pre></td></tr></table></figure><p>准备数据：student.txt 存储在 HDFS 上的/student 目录中</p><p>给项目打成 jar 包，上传到客户端</p><p>提交任务给 Spark 集群：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$<span class="type">SPARK_HOME</span>/bin/spark-submit \</span><br><span class="line">--<span class="class"><span class="keyword">class</span> <span class="title">com</span>.<span class="title">mazh</span>.<span class="title">spark</span>.<span class="title">sql</span>.<span class="title">SparkSQL_JDBC</span> <span class="title">\</span></span></span><br><span class="line"><span class="class"><span class="title">--master</span> <span class="title">yarn</span> <span class="title">\</span></span></span><br><span class="line"><span class="class"><span class="title">--driver-class-path</span> <span class="title">$SPARK_HOME/jars/mysql-connector-java-5</span>.1.40<span class="title">-bin</span>.<span class="title">jar</span> <span class="title">\</span></span></span><br><span class="line"><span class="class"><span class="title">/home/hadoop/Spark_WordCount-1</span>.0<span class="title">-SNAPSHOT</span>.<span class="title">jar</span> <span class="title">\</span></span></span><br><span class="line"><span class="class"><span class="title">hdfs</span></span>:<span class="comment">//mycluster/user/ap/sparkdatas/student.txt</span></span><br></pre></td></tr></table></figure><h1 id="八-数据源之json"><a href="#八-数据源之json" class="headerlink" title="八. 数据源之json"></a>八. 数据源之json</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 方式 1</span></span><br><span class="line"><span class="keyword">val</span> df1 = sparkSession.read.json(<span class="string">"file://.."</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 方式 2</span></span><br><span class="line"><span class="keyword">val</span> df2 = sparkSession.read.format(<span class="string">"json"</span>).load(<span class="string">"hdfs://mycluster/..."</span>)</span><br></pre></td></tr></table></figure><h1 id="九-数据源之-Hive"><a href="#九-数据源之-Hive" class="headerlink" title="九. 数据源之 Hive"></a>九. 数据源之 Hive</h1><p><a href="https://airpoet.github.io/2018/07/20/Spark/i-Spark-2/">其它可以参考我的另一篇文章</a></p><p>与之前的自己构造数据, 或者读<code>json</code>数据等,得到 <code>DataFrame</code> , 然后把此 <code>df.createTempView(“tmpView”)</code></p><p>然后再对此 <code>tmpView</code> 使用 sql 语句查询不一样</p><p><strong>直接用 <code>sparkSession.sql(“...”)</code> 操作的对象默认就是 hive</strong> </p><p><strong>==&gt; 访问 hive 的元数据库(mysql),  把 hive 底层的执行引擎 <code>MapReduce</code> 换成了<code>SparkCore</code></strong></p><h1 id="十-数据源之-HBase"><a href="#十-数据源之-HBase" class="headerlink" title="十. 数据源之 HBase"></a>十. 数据源之 HBase</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一-SparkSQL-的前世今生&quot;&gt;&lt;a href=&quot;#一-SparkSQL-的前世今生&quot; class=&quot;headerlink&quot; title=&quot;一. SparkSQL 的前世今生&quot;&gt;&lt;/a&gt;一. SparkSQL 的前世今生&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;Hive =
      
    
    </summary>
    
      <category term="Spark" scheme="https://airpoet.github.io/categories/Spark/"/>
    
      <category term="精讲" scheme="https://airpoet.github.io/categories/Spark/%E7%B2%BE%E8%AE%B2/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Spark" scheme="https://airpoet.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://airpoet.github.io/2018/07/27/Spark/Spark%E9%87%8D%E7%82%B9%E8%A7%A3%E6%9E%90(%E4%BA%8C)%20=%3E%20Spark%E8%B0%83%E4%BC%98/"/>
    <id>https://airpoet.github.io/2018/07/27/Spark/Spark重点解析(二) =&gt; Spark调优/</id>
    <published>2018-07-26T17:47:32.984Z</published>
    <updated>2018-07-30T01:37:36.309Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Spark整套调优方案主要分为开发调优、资源调优、数据倾斜调优、shuffle调优几个部分。</p><p><strong>开发调优和资源调优</strong>是所有Spark作业都需要注意和遵循的一些基本原则，<strong>是高性能Spark作业的基础</strong>；</p><p><strong>数据倾斜调优</strong>，主要讲解了一套完整的用来<strong>解决Spark作业数据倾斜的解决方案</strong>；</p><p><strong>shuffle调优</strong>，主要讲解了如何<strong>对Spark作业的shuffle运行过程以及细节进行调优</strong>。</p><p><strong>本文主要参考美团点评技术博客之</strong></p><ul><li><a href="https://tech.meituan.com/spark_tuning_basic.html" target="_blank" rel="noopener">Spark性能优化指南——基础篇</a></li><li><a href="https://tech.meituan.com/spark_tuning_pro.html" target="_blank" rel="noopener">Spark性能优化指南——高级篇</a></li></ul><hr><h1 id="一-开发调优"><a href="#一-开发调优" class="headerlink" title="一. 开发调优"></a>一. 开发调优</h1><h2 id="调优概述"><a href="#调优概述" class="headerlink" title="调优概述"></a>调优概述</h2><p>Spark性能优化的第一步，就是要在开发Spark作业的过程中注意和应用一些性能优化的基本原则。开发调优，就是要让大家了解以下一些Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化等。在开发过程中，时时刻刻都应该注意以上原则，并将这些原则根据具体的业务以及实际的应用场景，灵活地运用到自己的Spark作业中。</p><h2 id="原则一：避免创建重复的RDD"><a href="#原则一：避免创建重复的RDD" class="headerlink" title="原则一：避免创建重复的RDD"></a>原则一：避免创建重复的RDD</h2><p>通常来说，我们在开发一个Spark作业时，首先是基于某个数据源（比如Hive表或HDFS文件）创建一个初始的RDD；接着对这个RDD执行某个算子操作，然后得到下一个RDD；以此类推，循环往复，直到计算出最终我们需要的结果。在这个过程中，多个RDD会通过不同的算子操作（比如map、reduce等）串起来，这个“RDD串”，就是RDD lineage，也就是“RDD的血缘关系链”。</p><p>我们在开发过程中要注意：对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。</p><p>一些Spark初学者在刚开始开发Spark作业时，或者是有经验的工程师在开发RDD lineage极其冗长的Spark作业时，可能会忘了自己之前对于某一份数据已经创建过一个RDD了，从而导致对于同一份数据，创建了多个RDD。这就意味着，我们的Spark作业会进行多次重复计算来创建多个代表相同数据的RDD，进而增加了作业的性能开销。</p><h3 id="一个简单的例子"><a href="#一个简单的例子" class="headerlink" title="一个简单的例子"></a>一个简单的例子</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。也就是说，需要对一份数据执行两次算子操作。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。</span></span><br><span class="line"><span class="comment">// 这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，然后分别对每个RDD都执行了一个算子操作。</span></span><br><span class="line"><span class="comment">// 这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd2.reduce(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。</span></span><br><span class="line"><span class="comment">// 这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，然后对这一个RDD执行了多次算子操作。</span></span><br><span class="line"><span class="comment">// 但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。</span></span><br><span class="line"><span class="comment">// 要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，才能保证一个RDD被多次使用时只被计算一次。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure><h2 id="原则二：尽可能复用同一个RDD"><a href="#原则二：尽可能复用同一个RDD" class="headerlink" title="原则二：尽可能复用同一个RDD"></a>原则二：尽可能复用同一个RDD</h2><p>除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数。</p><h3 id="一个简单的例子-1"><a href="#一个简单的例子-1" class="headerlink" title="一个简单的例子"></a>一个简单的例子</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 错误的做法。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 有一个&lt;Long, String&gt;格式的RDD，即rdd1。</span></span><br><span class="line"><span class="comment">// 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; rdd1 = ...</span><br><span class="line"><span class="type">JavaRDD</span>&lt;<span class="type">String</span>&gt; rdd2 = rdd1.map(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 分别对rdd1和rdd2执行了不同的算子操作。</span></span><br><span class="line">rdd1.reduceByKey(...)</span><br><span class="line">rdd2.map(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的做法。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。</span></span><br><span class="line"><span class="comment">// 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 其实在这种情况下完全可以复用同一个RDD。</span></span><br><span class="line"><span class="comment">// 我们可以使用rdd1，既做reduceByKey操作，也做map操作。</span></span><br><span class="line"><span class="comment">// 在进行第二个map操作时，只使用每个数据的tuple._2，也就是rdd1中的value值，即可。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; rdd1 = ...</span><br><span class="line">rdd1.reduceByKey(...)</span><br><span class="line">rdd1.map(tuple._2...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。</span></span><br><span class="line"><span class="comment">// 但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，rdd1实际上还是会被计算两次。</span></span><br><span class="line"><span class="comment">// 因此还需要配合“原则三：对多次使用的RDD进行持久化”进行使用，才能保证一个RDD被多次使用时只被计算一次。</span></span><br></pre></td></tr></table></figure><h2 id="原则三：对多次使用的RDD进行持久化"><a href="#原则三：对多次使用的RDD进行持久化" class="headerlink" title="原则三：对多次使用的RDD进行持久化"></a>原则三：对多次使用的RDD进行持久化</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-29-140950.png" alt="image-20180729220949680"></p><p>当你在Spark代码中多次对一个RDD做了算子操作后，恭喜，你已经实现Spark作业第一步的优化了，也就是尽可能复用RDD。此时就该在这个基础之上，进行第二步优化了，也就是要保证对一个RDD执行多次算子操作时，这个RDD本身仅仅被计算一次。</p><p>Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。</p><p>因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。</p><h3 id="对多次使用的RDD进行持久化的代码示例"><a href="#对多次使用的RDD进行持久化的代码示例" class="headerlink" title="对多次使用的RDD进行持久化的代码示例"></a>对多次使用的RDD进行持久化的代码示例</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的做法。</span></span><br><span class="line"><span class="comment">// cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。</span></span><br><span class="line"><span class="comment">// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。</span></span><br><span class="line"><span class="comment">// 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>).cache()</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。</span></span><br><span class="line"><span class="comment">// 比如说，StorageLevel.MEMORY_AND_DISK_SER表示，内存充足时优先持久化到内存中，内存不充足时持久化到磁盘文件中。</span></span><br><span class="line"><span class="comment">// 而且其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个partition都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。</span></span><br><span class="line"><span class="comment">// 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，从而发生频繁GC。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>).persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure><p>对于persist()方法而言，我们可以根据不同的业务场景选择不同的持久化级别。</p><h3 id="Spark的持久化级别"><a href="#Spark的持久化级别" class="headerlink" title="Spark的持久化级别"></a>Spark的持久化级别</h3><table><thead><tr><th>持久化级别</th><th>含义解释</th></tr></thead><tbody><tr><td>MEMORY_ONLY</td><td>使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。</td></tr><tr><td>MEMORY_AND_DISK</td><td>使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。</td></tr><tr><td>MEMORY_ONLY_SER</td><td>基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td></tr><tr><td>MEMORY_AND_DISK_SER</td><td>基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td></tr><tr><td>DISK_ONLY</td><td>使用未序列化的Java对象格式，将数据全部写入磁盘文件中。</td></tr><tr><td>MEMORY_ONLY_2, MEMORY_AND_DISK_2, 等等.</td><td>对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。</td></tr></tbody></table><h3 id="如何选择一种最合适的持久化策略"><a href="#如何选择一种最合适的持久化策略" class="headerlink" title="如何选择一种最合适的持久化策略"></a>如何选择一种最合适的持久化策略</h3><ul><li>默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。</li><li>如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。</li><li>如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。</li><li>通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。</li></ul><h2 id="原则四：尽量避免使用shuffle类算子"><a href="#原则四：尽量避免使用shuffle类算子" class="headerlink" title="原则四：尽量避免使用shuffle类算子"></a>原则四：尽量避免使用shuffle类算子</h2><p>如果有可能的话，要尽量避免使用shuffle类算子。因为<strong>Spark作业运行过程中，最消耗性能的地方就是shuffle过程</strong>。shuffle过程，简单来说，就是<strong>将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作</strong>。比如reduceByKey、join等算子，都会触发shuffle操作。</p><p>shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。<strong>磁盘IO和网络数据传输也是shuffle性能较差的主要原因。</strong></p><p>因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。</p><h3 id="Broadcast与map进行join代码示例-※"><a href="#Broadcast与map进行join代码示例-※" class="headerlink" title="Broadcast与map进行join代码示例 ※"></a>Broadcast与map进行join代码示例 ※</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 传统的join操作会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.join(rdd2)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Broadcast+map的join操作，不会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 使用Broadcast将一个数据量较小的RDD作为广播变量。</span></span><br><span class="line"><span class="keyword">val</span> rdd2Data = rdd2.collect()</span><br><span class="line"><span class="keyword">val</span> rdd2DataBroadcast = sc.broadcast(rdd2Data)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。</span></span><br><span class="line"><span class="comment">// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。</span></span><br><span class="line"><span class="comment">// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.map(rdd2DataBroadcast...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。</span></span><br><span class="line"><span class="comment">// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。</span></span><br></pre></td></tr></table></figure><h2 id="原则五：使用map-side预聚合的shuffle操作"><a href="#原则五：使用map-side预聚合的shuffle操作" class="headerlink" title="原则五：使用map-side预聚合的shuffle操作"></a>原则五：使用map-side预聚合的shuffle操作</h2><p>如果因为业务需要，一定要使用<strong>shuffle操作</strong>，无法用map类的算子来替代，那么<strong>尽量使用可以map-side预聚合的算子</strong>。</p><p>所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，<strong>在可能的情况下，建议使用<code>reduceByKey</code>或者<code>aggregateByKey</code>算子来替代掉<code>groupByKey</code>算子</strong>。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而<strong><code>groupByKey</code>算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差</strong>。</p><p>比如如下两幅图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。<img src="https://tech.meituan.com/img/spark-tuning/group-by-key-wordcount.png" alt="groupByKey实现wordcount原理"></p><p><img src="https://tech.meituan.com/img/spark-tuning/reduce-by-key-wordcount.png" alt="reduceByKey实现wordcount原理"></p><h2 id="原则六：使用高性能的算子"><a href="#原则六：使用高性能的算子" class="headerlink" title="原则六：使用高性能的算子"></a>原则六：使用高性能的算子</h2><p>除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。</p><h3 id="1-使用reduceByKey-aggregateByKey替代groupByKey"><a href="#1-使用reduceByKey-aggregateByKey替代groupByKey" class="headerlink" title="1. 使用reduceByKey/aggregateByKey替代groupByKey"></a>1. 使用reduceByKey/aggregateByKey替代groupByKey</h3><p>详情见“原则五：使用map-side预聚合的shuffle操作”。</p><h3 id="2-使用mapPartitions替代普通map"><a href="#2-使用mapPartitions替代普通map" class="headerlink" title="2. 使用mapPartitions替代普通map"></a>2. 使用mapPartitions替代普通map</h3><p>mapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！</p><h3 id="3-使用foreachPartitions替代foreach"><a href="#3-使用foreachPartitions替代foreach" class="headerlink" title="3. 使用foreachPartitions替代foreach"></a>3. 使用foreachPartitions替代foreach</h3><p>原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。</p><h3 id="4-使用filter之后进行coalesce操作"><a href="#4-使用filter之后进行coalesce操作" class="headerlink" title="4. 使用filter之后进行coalesce操作"></a>4. 使用filter之后进行coalesce操作</h3><p>通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。</p><h3 id="5-使用repartitionAndSortWithinPartitions替代repartition与sort类操作"><a href="#5-使用repartitionAndSortWithinPartitions替代repartition与sort类操作" class="headerlink" title="5. 使用repartitionAndSortWithinPartitions替代repartition与sort类操作"></a>5. 使用repartitionAndSortWithinPartitions替代repartition与sort类操作</h3><p>repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。</p><h2 id="原则七：广播大变量"><a href="#原则七：广播大变量" class="headerlink" title="原则七：广播大变量"></a>原则七：广播大变量</h2><p>有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。</p><p>在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。</p><p>因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。</p><h3 id="广播大变量的代码示例"><a href="#广播大变量的代码示例" class="headerlink" title="广播大变量的代码示例"></a>广播大变量的代码示例</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 以下代码在算子函数中，使用了外部的变量。</span></span><br><span class="line"><span class="comment">// 此时没有做任何特殊操作，每个task都会有一份list1的副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line">rdd1.map(list1...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 以下代码将list1封装成了Broadcast类型的广播变量。</span></span><br><span class="line"><span class="comment">// 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。</span></span><br><span class="line"><span class="comment">// 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。</span></span><br><span class="line"><span class="comment">// 每个Executor内存中，就只会驻留一份广播变量副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line"><span class="keyword">val</span> list1Broadcast = sc.broadcast(list1)</span><br><span class="line">rdd1.map(list1Broadcast...)</span><br></pre></td></tr></table></figure><h2 id="原则八：使用Kryo优化序列化性能"><a href="#原则八：使用Kryo优化序列化性能" class="headerlink" title="原则八：使用Kryo优化序列化性能"></a>原则八：使用Kryo优化序列化性能</h2><p><strong>在Spark中，主要有三个地方涉及到了序列化：</strong></p><ul><li>在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。</li><li>将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。</li><li>使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。</li></ul><p>对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。</p><p><strong>以下是使用<code>Kryo</code>的代码示例</strong>，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等, 避免会使用全限定名, 这样就会有额外的开销）：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建SparkConf对象。</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(...).setAppName(...)</span><br><span class="line"><span class="comment">// 设置序列化器为KryoSerializer。</span></span><br><span class="line">conf.set(<span class="string">"spark.serializer"</span>, <span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</span><br><span class="line"><span class="comment">// 注册要序列化的自定义类型。</span></span><br><span class="line">conf.registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">MyClass1</span>], classOf[<span class="type">MyClass2</span>]))</span><br></pre></td></tr></table></figure><h2 id="原则九：优化数据结构"><a href="#原则九：优化数据结构" class="headerlink" title="原则九：优化数据结构"></a>原则九：优化数据结构</h2><p>Java中，有三种类型比较耗费内存：</p><ul><li><strong>对象</strong>，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。</li><li><strong>字符串</strong>，每个字符串内部都有一个字符数组以及长度等额外信息。</li><li><strong>集合类型</strong>，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。</li></ul><p>因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用<strong>字符串替代对象</strong>，使用<strong>原始类型（比如Int、Long）替代字符串</strong>，使用<strong>数组替代集合</strong>类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。</p><p>但是在笔者的编码实践中发现，要做到该原则其实并不容易。因为我们同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于我们的编码难度以及代码可维护性，也是一个极大的挑战。因此笔者建议，<strong>在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性。</strong></p><p>主要就是优化你的算子函数，内部使用到的局部数据，或者是算子函数外部的数据。都可以进行数据结构的优化。优化之后，都会减少其对内存的消耗和占用。 </p><p><strong>通常企业级应用中的做法是: </strong></p><ul><li>对于<strong>HashMap、List</strong>这种数据，统一<strong>用String拼接成特殊格式的字符串</strong>，比如Map&lt;Integer, Person&gt; persons = new HashMap&lt;Integer, Person&gt;()。可以优化为，特殊的字符串格式：id:name,address|id:name,address…。 </li><li><strong>避免使用多层嵌套的对象结构</strong>。比如说，public class Teacher { private List<student> students = new ArrayList\&lt;Student>() }。就是非常不好的例子。因为Teacher类的内部又嵌套了大量的小Student对象。对于上述例子，也完全可以使用特殊的字符串来进行数据的存储。比如，<strong>用json字符串</strong>来存储数据，就是一个很好的选择。 <code>{&quot;teacherId&quot;: 1, &quot;teacherName&quot;: &quot;leo&quot;, students:[{&quot;studentId&quot;: 1, &quot;studentName&quot;: &quot;tom&quot;},{&quot;studentId&quot;:2, &quot;studentName&quot;:&quot;marry&quot;}]}</code></student></li><li><strong>尽量使用int替代String</strong> ,在spark应用中，<strong>id就不要用常用的uuid了</strong>，因为无法转成int，就<strong>用自增的int类型的id即可</strong>。（sdfsdfdf-234242342-sdfsfsfdfd） </li></ul><h2 id="原则十-GroupByKey-和-ReduceByKey的使用"><a href="#原则十-GroupByKey-和-ReduceByKey的使用" class="headerlink" title="原则十: GroupByKey 和 ReduceByKey的使用"></a>原则十: GroupByKey 和 ReduceByKey的使用</h2><blockquote><p>GroupByKey</p></blockquote><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-30-groupByKey%E5%8E%9F%E7%90%86.png" alt=""></p><blockquote><p>ReduceByKey</p></blockquote><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-30-reduceByKey%E5%8E%9F%E7%90%86.png" alt=""></p><hr><h1 id="二-资源调优"><a href="#二-资源调优" class="headerlink" title="二. 资源调优"></a>二. 资源调优</h1><h2 id="调优概述-1"><a href="#调优概述-1" class="headerlink" title="调优概述"></a>调优概述</h2><p>在开发完Spark作业之后，就该为作业配置合适的资源了。<strong>Spark的资源参数，基本都可以在spark-submit命令中作为参数设置。</strong>很多Spark初学者，通常不知道该设置哪些必要的参数，以及如何设置这些参数，最后就只能胡乱设置，甚至压根儿不设置。资源参数设置的不合理，可能会导致没有充分利用集群资源，作业运行会极其缓慢；或者设置的资源过大，队列没有足够的资源来提供，进而导致各种异常。总之，无论是哪种情况，都会导致Spark作业的运行效率低下，甚至根本无法运行。因此我们<strong>必须对Spark作业的资源使用原理有一个清晰的认识</strong>，<strong>并知道在Spark作业运行过程中，有哪些资源参数是可以设置的，以及如何设置合适的参数值。</strong></p><h2 id="Spark作业基本运行原理"><a href="#Spark作业基本运行原理" class="headerlink" title="Spark作业基本运行原理"></a>Spark作业基本运行原理</h2><p><img src="https://tech.meituan.com/img/spark-tuning/spark-base-mech.png" alt="Spark基本运行原理"></p><p>详细原理见上图。我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而<strong>Driver进程要做的第一件事情，就是向集群管理器</strong>（可以是Spark Standalone集群，也可以是其他的资源管理集群，美团•大众点评使用的是YARN作为资源管理集群）<strong>申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。</strong></p><p>在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。<strong>一个stage的所有task都执行完毕之后，<u>会在各个节点本地的磁盘文件中写入计算中间结果</u>，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。</strong>如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。</p><p><strong>Spark是根据shuffle类算子来进行stage的划分</strong>。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。</p><p>当我们在代码中执行了<strong>cache/persist等持久化操作</strong>时，根据我们选择的持久化级别的不同，<strong>每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中</strong>。</p><p>因此<strong>Executor的内存主要分为三块</strong>：第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD持久化时使用，默认占Executor总内存的60%。(这里<strong>指的是静态内存分配</strong>, 详情见下文) </p><p>task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。<strong>如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。</strong></p><p>以上就是Spark作业的基本运行原理的说明，大家可以结合上图来理解。理解作业基本原理，是我们进行资源参数调优的基本前提。</p><h2 id="数据本地化"><a href="#数据本地化" class="headerlink" title="数据本地化"></a>数据本地化</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-29-171030.jpg" alt=""></p><p>移动代码到其他节点，会比移动数据到代码所在的节点上去，速度要快得多，因为代码比较小。Spark也正是基于这个数据本地化的原则来构建task调度算法的。 </p><p>数据本地化，指的是，数据离计算它的代码有多近。<strong>基于数据距离代码的距离，有几种数据本地化级别：</strong> </p><ol><li><code>PROCESS_LOCAL</code>：数据和计算它的代码在同一个JVM进程中。</li><li><code>NODE_LOCAL</code>：数据和计算它的代码在一个节点上，但是不在一个进程中，比如在不同的executor进程中，或者是数据在HDFS文件的block中。</li><li><code>NO_PREF</code>：数据从哪里过来，性能都是一样的。</li><li><code>RACK_LOCAL</code>：数据和计算它的代码在一个机架上。</li><li><code>ANY</code>：数据可能在任意地方，比如其他网络环境内，或者其他机架上。</li></ol><p>Spark倾向于使用最好的本地化级别来调度task，但是这是不可能的。如果没有任何未处理的数据在空闲的executor上，那么Spark就会放低本地化级别。这时有两个选择：第一，等待，直到executor上的cpu释放出来，那么就分配task过去；第二，立即在任意一个executor上启动一个task。 </p><p>Spark默认会等待一会儿，来期望task要处理的数据所在的节点上的executor空闲出一个cpu，从而将task分配过去。只要超过了时间，那么Spark就会将task分配到其他任意一个空闲的executor上。 </p><p>可以设置参数，<code>spark.locality</code>系列参数，来调节Spark等待task可以进行数据本地化的时间。<code>spark.locality.wait</code>（3000毫秒）、spark.locality.wait.node、spark.locality.wait.process、spark.locality.wait.rack。 </p><h2 id="内存空间分配"><a href="#内存空间分配" class="headerlink" title="内存空间分配"></a>内存空间分配</h2><h3 id="1-静态内存管理"><a href="#1-静态内存管理" class="headerlink" title="1.静态内存管理"></a>1.静态内存管理</h3><p>在 Spark 最初采用的静态内存管理机制下，存储内存、执行内存和其他内存的大小在 Spark 应用程序运行期间均为固定的，但用户可以应用程序启动前进行配置</p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-28-083345.png" alt="image-20180728163345199"></p><p><strong>可用的存储内存</strong><code>StorageMemory =systemMaxMemory * spark.storage.memoryFraction * spark.storage.safetyFraction = 43%</code></p><p><strong>可用的执行内存</strong> = <code>systemMaxMemory * spark.shuffle.memoryFraction * spark.shuffle.safetyFraction = 16%</code></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-28-083901.png" alt="image-20180728163901027"></p><p>由于新的内存管理机制的出现，这种方式 使用，出于兼容旧版本的应用程序的目的，Spark 仍然保留了它的实现。</p><h3 id="2-统一内存管理"><a href="#2-统一内存管理" class="headerlink" title="2.统一内存管理"></a>2.统一内存管理</h3><p><strong>Spark-1.6 之后引入的统一内存管理机制，与静态内存管理的区别在于存储内存和执行内存 共享同一块空间，可以动态占用对方的空闲区域</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-28-084013.png" alt="image-20180728164012590"></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-28-084035.png" alt="image-20180728164035249"></p><p><strong>其中最重要的优化在于动态占用机制</strong></p><ol><li>设定基本的存储内存和执行内存区域（spark.memory.storageFraction参数), 该设定确定了双方各自拥有的空间的范围</li><li>双方的空间都不足时，则存储到硬盘；若己方空间不足而对方空余时，可借用对方的空 间;（存储空间不足是指不足以放下一个完整的 Block）</li><li><strong><code>执行内存</code>的空间被<code>存储内存</code>占用后，可让<code>存储内存</code>将占用的部分转存到硬盘，然后<code>归还</code>借用的空间</strong></li><li><strong><code>存储内存</code>的空间被<code>执行内存</code>占用后，<code>无法让执行内存</code>“归还”，因为需要考虑 Shuffle 过程中的很多 因素，实现起来较为复杂</strong></li></ol><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-28-090452.png" alt="image-20180728170452408"></p><h3 id="垃圾回收-JVM-优化"><a href="#垃圾回收-JVM-优化" class="headerlink" title="垃圾回收, JVM 优化"></a>垃圾回收, JVM 优化</h3><blockquote><p>GC 对性能的影响</p></blockquote><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-29-GC%E5%AF%B9Spark%E6%80%A7%E8%83%BD%E5%BD%B1%E5%93%8D%E7%9A%84%E5%8E%9F%E7%90%86.png" alt=""></p><p>对于垃圾回收的性能问题，首先要做的就是，使用更高效的数据结构，比如array和string；其次就是在持久化rdd时，使用序列化的持久化级别，而且用Kryo序列化类库，这样，每个partition就只是一个对象——一个字节数组。 </p><p>我们可以<strong>对垃圾回收进行监测</strong>，包括多久进行一次垃圾回收，以及每次垃圾回收耗费的时间。只要在spark-submit脚本中，增加一个配置即可，–conf “spark.executor.extraJavaOptions=-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps”。 这里虽然会打印出Java虚拟机的垃圾回收的相关信息，但是是输出到了worker上的日志中，而不是driver的日志中。 </p><p>也完全可以通过<strong>SparkUI</strong>（4040端口）来观察每个stage的垃圾回收的情况。 </p><blockquote><p><strong>调节executor内存比例</strong></p></blockquote><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-29-%E8%B0%83%E8%8A%82executor%E5%86%85%E5%AD%98%E6%AF%94%E4%BE%8B.png" alt=""></p><p>默认情况下，Spark使用每个executor 60%的内存空间来缓存RDD，那么在task执行期间创建的对象，只有40%的内存空间来存放。 </p><p>在上述情况下，如果发现垃圾回收频繁发生。那么就需要对那个比例进行调优，使用new SparkConf().set(“spark.storage.memoryFraction”, “0.5”)即可，可以将RDD缓存占用空间的比例降低，从而给更多的空间让task创建的对象进行使用。 </p><p>因此，<strong>对于RDD持久化，完全可以使用Kryo序列化，加上降低其executor内存占比的方式，来减少其内存消耗</strong>。给task提供更多的内存，从而避免task的执行频繁触发垃圾回收。 </p><h3 id="高级垃圾回收调优"><a href="#高级垃圾回收调优" class="headerlink" title="高级垃圾回收调优"></a>高级垃圾回收调优</h3><blockquote><p>JVM minor gc与full gc原理</p></blockquote><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-29-JVM%20minor%20gc%E4%B8%8Efull%20gc%E5%8E%9F%E7%90%86.png" alt=""></p><p>Java堆空间被划分成了两块空间，一个是年轻代，一个是老年代。年轻代放的是短时间存活的对象，老年代放的是长时间存活的对象。年轻代又被划分了三块空间，Eden、Survivor1、Survivor2。</p><p>首先，Eden区域和Survivor1区域用于存放对象，Survivor2区域备用。创建的对象，首先放入Eden区域和Survivor1区域，如果Eden区域满了，那么就会触发一次Minor GC，进行年轻代的垃圾回收。Eden和Survivor1区域中存活的对象，会被移动到Survivor2区域中。然后Survivor1和Survivor2的角色调换，Survivor1变成了备用</p><p>如果一个对象，在年轻代中，撑过了多次垃圾回收，都没有被回收掉，那么会被认为是长时间存活的，此时就会被移入老年代。此外，如果在将Eden和Survivor1中的存活对象，尝试放入Survivor2中时，发现Survivor2放满了，那么会直接放入老年代。此时就出现了，短时间存活的对象，进入老年代的问题。</p><p>如果老年代的空间满了，那么就会触发Full GC，进行老年代的垃圾回收操作。</p><p>Spark中，垃圾回收调优的目标就是，只有真正长时间存活的对象，才能进入老年代，短时间存活的对象，只能呆在年轻代。不能因为某个Survivor区域空间不够，在Minor GC时，就进入了老年代。从而造成短时间存活的对象，长期呆在老年代中占据了空间，而且Full GC时要回收大量的短时间存活的对象，导致Full GC速度缓慢。</p><p><strong>如果发现，在task执行期间，大量full gc发生了，那么说明，年轻代的Eden区域，给的空间不够大。此时可以执行一些操作来优化垃圾回收行为：</strong></p><p>1、包括降低spark.storage.memoryFraction的比例，给年轻代更多的空间，来存放短时间存活的对象；</p><p>2、给Eden区域分配更大的空间，使用-Xmn即可，通常建议给Eden区域，预计大小的4/3；</p><p>3、如果使用的是HDFS文件，那么很好估计Eden区域大小，如果每个executor有4个task，然后每个hdfs压缩块解压缩后大小是3倍，此外每个hdfs块的大小是64M，那么Eden区域的预计大小就是：4 <em> 3 </em> 64MB，然后呢，再通过-Xmn参数，将Eden区域大小设置为4 <em> 3 </em> 64 * 4/3。</p><p>其实啊，根据经验来看，<strong>对于垃圾回收的调优，尽量就是说，调节executor内存的比例就可以了。</strong>因为jvm的调优是非常复杂和敏感的。除非是，真的到了万不得已的地方，然后呢，自己本身又对jvm相关的技术很了解，那么此时进行eden区域的调节，调优，是可以的。</p><p><strong>一些高级的参数</strong></p><p>-XX:SurvivorRatio=4：如果值为4，那么就是两个Survivor跟Eden的比例是2:4，也就是说每个Survivor占据的年轻代的比例是1/6，所以，你其实也可以尝试调大Survivor区域的大小。</p><p>-XX:NewRatio=4：调节新生代和老年代的比例</p><h2 id="资源参数调优"><a href="#资源参数调优" class="headerlink" title="资源参数调优"></a>资源参数调优</h2><p>了解完了Spark作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓的Spark资源参数调优，其实主要就是对Spark运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升Spark作业的执行性能。以下参数就是Spark中主要的资源参数，每个参数都对应着作业运行原理中的某个部分，我们同时也给出了一个调优的参考值。</p><h3 id="num-executors"><a href="#num-executors" class="headerlink" title="num-executors"></a>num-executors</h3><ul><li>参数说明：<u>该参数用于设置Spark作业总共要用多少个Executor进程来执行。</u>Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。</li><li><strong>参数调优建议</strong>：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。</li></ul><h3 id="executor-memory"><a href="#executor-memory" class="headerlink" title="executor-memory"></a>executor-memory</h3><ul><li>参数说明：<u>该参数用于设置每个Executor进程的内存。</u>Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。</li><li><strong>参数调优建议</strong>：每个Executor进程的内存设置4G<sub>8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/3</sub>1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。</li></ul><h3 id="executor-cores"><a href="#executor-cores" class="headerlink" title="executor-cores"></a>executor-cores</h3><ul><li>参数说明：<u>该参数用于设置每个Executor进程的CPU core数量。</u>这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。</li><li><strong>参数调优建议</strong>：Executor的CPU core数量设置为2<sub>4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，<strong>如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3</strong></sub>1/2左右比较合适，也是避免影响其他同学的作业运行。</li></ul><h3 id="driver-memory"><a href="#driver-memory" class="headerlink" title="driver-memory"></a>driver-memory</h3><ul><li>参数说明：<u>该参数用于设置Driver进程的内存。</u></li><li><strong>参数调优建议</strong>：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。</li></ul><h3 id="spark-default-parallelism-※-task-并行度"><a href="#spark-default-parallelism-※-task-并行度" class="headerlink" title="spark.default.parallelism ※  task 并行度"></a>spark.default.parallelism ※  task 并行度</h3><ul><li><strong>参数说明</strong>：<strong><u>该参数用于设置<code>每个stage的默认task数量</code>。</u>**</strong>这个参数极为重要<strong>，如果不设置可能会直接影响你的Spark作业性能。 </strong>提高并行度.**</li><li><strong>参数调优建议</strong>：Spark作业的默认task数量为500<sub>1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），<em>如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃</em>。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此<strong>Spark官网建议的设置原则是，设置该参数为<code>num-executors * executor-cores</code>的<code>2&lt;/sub&gt;3</code>倍较为合适</strong>，比如<strong>所有Executor的总CPU core</strong>数量为300个，那么设置1000个task是可以的，此时<u>可以充分地利用Spark集群的资源</u>。</sub></li></ul><h3 id="spark-storage-memoryFraction"><a href="#spark-storage-memoryFraction" class="headerlink" title="spark.storage.memoryFraction"></a>spark.<u>storage</u>.memoryFraction</h3><ul><li>参数说明：<u>该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6</u>。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。</li><li><strong>参数调优建议</strong>：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li></ul><h3 id="spark-shuffle-memoryFraction"><a href="#spark-shuffle-memoryFraction" class="headerlink" title="spark.shuffle.memoryFraction"></a>spark.<u>shuffle</u>.memoryFraction</h3><ul><li>参数说明：<u>该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。</u>也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。</li><li><strong>参数调优建议</strong>：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li></ul><p><strong>资源参数的调优，没有一个固定的值，需要同学们根据自己的实际情况（包括Spark作业中的shuffle操作数量、RDD持久化操作数量以及spark web ui中显示的作业gc情况），同时参考本篇文章中给出的原理以及调优建议，合理地设置上述参数。</strong></p><h2 id="资源参数参考示例"><a href="#资源参数参考示例" class="headerlink" title="资源参数参考示例"></a>资源参数参考示例</h2><p>以下是一份spark-submit命令的示例，大家可以参考一下，并根据自己的实际情况进行调节：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --master yarn-cluster \</span><br><span class="line">  --num-executors 100 \</span><br><span class="line">  --executor-memory 6G \</span><br><span class="line">  --executor-cores 4 \</span><br><span class="line">  --driver-memory 1G \</span><br><span class="line">  --conf spark.default.parallelism=1000 \</span><br><span class="line">  --conf spark.storage.memoryFraction=0.5 \</span><br><span class="line">  --conf spark.shuffle.memoryFraction=0.3 \</span><br></pre></td></tr></table></figure><hr><h1 id="三-数据倾斜调优"><a href="#三-数据倾斜调优" class="headerlink" title="三. 数据倾斜调优"></a>三. 数据倾斜调优</h1><h2 id="调优概述-2"><a href="#调优概述-2" class="headerlink" title="调优概述"></a>调优概述</h2><p>有的时候，我们可能会遇到大数据计算中一个最棘手的问题——数据倾斜，此时Spark作业的性能会比期望差很多。数据倾斜调优，就是使用各种技术方案解决不同类型的数据倾斜问题，以保证Spark作业的性能。</p><h2 id="数据倾斜发生时的现象"><a href="#数据倾斜发生时的现象" class="headerlink" title="数据倾斜发生时的现象"></a>数据倾斜发生时的现象</h2><ul><li>绝大多数task执行得都非常快，但个别task执行极慢。比如，总共有1000个task，997个task都在1分钟之内执行完了，但是剩余两三个task却要一两个小时。这种情况很常见。</li><li>原本能够正常执行的Spark作业，某天突然报出OOM（内存溢出）异常，观察异常栈，是我们写的业务代码造成的。这种情况比较少见。</li></ul><h2 id="数据倾斜发生的原理"><a href="#数据倾斜发生的原理" class="headerlink" title="数据倾斜发生的原理"></a>数据倾斜发生的原理</h2><p>数据倾斜的原理很简单：在进行shuffle的时候，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理，比如按照key进行聚合或join等操作。此时如果某个key对应的数据量特别大的话，就会发生数据倾斜。比如大部分key对应10条数据，但是个别key却对应了100万条数据，那么大部分task可能就只会分配到10条数据，然后1秒钟就运行完了；但是个别task可能分配到了100万数据，要运行一两个小时。因此，整个Spark作业的运行进度是由运行时间最长的那个task决定的。</p><p>因此出现数据倾斜的时候，Spark作业看起来会运行得非常缓慢，甚至可能因为某个task处理的数据量过大导致内存溢出。</p><p>下图就是一个很清晰的例子：hello这个key，在三个节点上对应了总共7条数据，这些数据都会被拉取到同一个task中进行处理；而world和you这两个key分别才对应1条数据，所以另外两个task只要分别处理1条数据即可。此时第一个task的运行时间可能是另外两个task的7倍，而<strong>整个stage的运行速度也由运行最慢的那个task所决定</strong>。</p><p><img src="https://tech.meituan.com/img/spark-tuning/skwed-mech.png" alt="数据倾斜原理"></p><h2 id="如何定位导致数据倾斜的代码"><a href="#如何定位导致数据倾斜的代码" class="headerlink" title="如何定位导致数据倾斜的代码"></a>如何定位导致数据倾斜的代码</h2><p>数据倾斜只会发生在shuffle过程中。这里给大家罗列一些常用的并且可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。</p><h3 id="某个task执行特别慢的情况"><a href="#某个task执行特别慢的情况" class="headerlink" title="某个task执行特别慢的情况"></a>某个task执行特别慢的情况</h3><p>首先要看的，就是数据倾斜发生在第几个stage中。</p><p>如果是用yarn-client模式提交，那么本地是直接可以看到log的，可以在log中找到当前运行到了第几个stage；如果是用yarn-cluster模式提交，则可以通过Spark Web UI来查看当前运行到了第几个stage。此外，无论是使用yarn-client模式还是yarn-cluster模式，我们都可以在Spark Web UI上深入看一下当前这个stage各个task分配的数据量，从而进一步确定是不是task分配的数据不均匀导致了数据倾斜。</p><p>比如下图中，倒数第三列显示了每个task的运行时间。明显可以看到，有的task运行特别快，只需要几秒钟就可以运行完；而有的task运行特别慢，需要几分钟才能运行完，此时单从运行时间上看就已经能够确定发生数据倾斜了。此外，倒数第一列显示了每个task处理的数据量，明显可以看到，运行时间特别短的task只需要处理几百KB的数据即可，而运行时间特别长的task需要处理几千KB的数据，处理的数据量差了10倍。此时更加能够确定是发生了数据倾斜。</p><p><img src="https://tech.meituan.com/img/spark-tuning/shuffle-skwed-web-ui-demo.png" alt="img"></p><p>知道数据倾斜发生在哪一个stage之后，接着我们就需要根据stage划分原理，推算出来发生倾斜的那个stage对应代码中的哪一部分，这部分代码中肯定会有一个shuffle类算子。精准推算stage与代码的对应关系，需要对Spark的源码有深入的理解，这里我们可以介绍一个相对简单实用的推算方法：只要看到Spark代码中出现了一个shuffle类算子或者是Spark SQL的SQL语句中出现了会导致shuffle的语句（比如group by语句），那么就可以判定，以那个地方为界限划分出了前后两个stage。</p><p>这里我们就以Spark最基础的入门程序——单词计数来举例，如何用最简单的方法大致推算出一个stage对应的代码。如下示例，在整个代码中，只有一个reduceByKey是会发生shuffle的算子，因此就可以认为，以这个算子为界限，会划分出前后两个stage。</p><ul><li><strong>stage0</strong>，主要是执行从textFile到map操作，以及执行shuffle write操作。<strong>shuffle write</strong>操作，我们可以简单理解为对pairs RDD中的数据进行分区操作，每个task处理的数据中，相同的key会写入同一个磁盘文件内。</li><li><strong>stage1</strong>，主要是执行从reduceByKey到collect操作，stage1的各个task一开始运行，就会首先执行<strong>shuffle read</strong>操作。执行shuffle read操作的task，会从stage0的各个task所在节点拉取属于自己处理的那些key，然后对同一个key进行全局性的聚合或join等操作，在这里就是对key的value值进行累加。stage1在执行完reduceByKey算子之后，就计算出了最终的wordCounts RDD，然后会执行collect算子，将所有数据拉取到Driver上，供我们遍历和打印输出。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> lines = sc.textFile(<span class="string">"hdfs://..."</span>)</span><br><span class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"><span class="keyword">val</span> pairs = words.map((_, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> wordCounts = pairs.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">wordCounts.collect().foreach(println(_))</span><br></pre></td></tr></table></figure><p>通过对单词计数程序的分析，希望能够让大家了解最基本的stage划分的原理，以及stage划分后shuffle操作是如何在两个stage的边界处执行的。然后我们就知道如何快速定位出发生数据倾斜的stage对应代码的哪一个部分了。比如我们在Spark Web UI或者本地log中发现，stage1的某几个task执行得特别慢，判定stage1出现了数据倾斜，那么就可以回到代码中定位出stage1主要包括了reduceByKey这个shuffle类算子，此时基本就可以确定是由reduceByKey算子导致的数据倾斜问题。比如某个单词出现了100万次，其他单词才出现10次，那么stage1的某个task就要处理100万数据，整个stage的速度就会被这个task拖慢。</p><h3 id="某个task莫名其妙内存溢出的情况"><a href="#某个task莫名其妙内存溢出的情况" class="headerlink" title="某个task莫名其妙内存溢出的情况"></a>某个task莫名其妙内存溢出的情况</h3><p>这种情况下去定位出问题的代码就比较容易了。我们建议直接看yarn-client模式下本地log的异常栈，或者是通过YARN查看yarn-cluster模式下的log中的异常栈。一般来说，通过异常栈信息就可以定位到你的代码中哪一行发生了内存溢出。然后在那行代码附近找找，一般也会有shuffle类算子，此时很可能就是这个算子导致了数据倾斜。</p><p>但是大家要注意的是，不能单纯靠偶然的内存溢出就判定发生了数据倾斜。因为自己编写的代码的bug，以及偶然出现的数据异常，也可能会导致内存溢出。因此还是要按照上面所讲的方法，通过Spark Web UI查看报错的那个stage的各个task的运行时间以及分配的数据量，才能确定是否是由于数据倾斜才导致了这次内存溢出。</p><h2 id="查看导致数据倾斜的key的数据分布情况"><a href="#查看导致数据倾斜的key的数据分布情况" class="headerlink" title="查看导致数据倾斜的key的数据分布情况"></a>查看导致数据倾斜的key的数据分布情况</h2><p>知道了数据倾斜发生在哪里之后，通常需要分析一下那个执行了shuffle操作并且导致了数据倾斜的RDD/Hive表，查看一下其中key的分布情况。这主要是为之后选择哪一种技术方案提供依据。针对不同的key分布与不同的shuffle算子组合起来的各种情况，可能需要选择不同的技术方案来解决。</p><p>此时根据你执行操作的情况不同，可以有很多种<strong>查看key分布的方式</strong>：</p><ol><li>如果<strong>是Spark SQL中的group by、join语句导致的数据倾斜</strong>，那么就<strong>查询一下SQL中使用的表的key分布情况</strong>。</li><li>如果是对<strong>Spark RDD执行shuffle算子导致的数据倾斜</strong>，那么<strong>可以在Spark作业中加入查看key分布的代码</strong>，比如RDD.countByKey()。然后对统计出来的各个key出现的次数，collect/take到客户端打印一下，就可以看到key的分布情况。</li></ol><p>举例来说，对于上面所说的单词计数程序，如果确定了是stage1的reduceByKey算子导致了数据倾斜，那么就应该看看进行reduceByKey操作的RDD中的key分布情况，在这个例子中指的就是pairs RDD。如下示例，我们可以先<strong>对pairs采样10%的样本数据，然后使用countByKey算子统计出每个key出现的次数，最后在客户端遍历和打印样本数据中各个key的出现次数。</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sampledPairs = pairs.sample(<span class="literal">false</span>, <span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">val</span> sampledWordCounts = sampledPairs.countByKey()</span><br><span class="line">sampledWordCounts.foreach(println(_))</span><br></pre></td></tr></table></figure><h2 id="数据倾斜的解决方案"><a href="#数据倾斜的解决方案" class="headerlink" title="数据倾斜的解决方案"></a>数据倾斜的解决方案</h2><h3 id="解决方案一：使用Hive-ETL预处理数据"><a href="#解决方案一：使用Hive-ETL预处理数据" class="headerlink" title="解决方案一：使用Hive ETL预处理数据"></a>解决方案一：使用Hive ETL预处理数据</h3><p><strong>方案适用场景：</strong>导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀（比如某个key对应了100万数据，其他key才对应了10条数据），而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。</p><p><strong>方案实现思路：</strong>此时可以评估一下，是否可以通过Hive来进行数据预处理（即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行join），然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。</p><p><strong>方案实现原理：</strong>这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。我们只是把数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已。</p><p><strong>方案优点：</strong>实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。</p><p><strong>方案缺点：</strong>治标不治本，Hive ETL中还是会发生数据倾斜。</p><p><strong>方案实践经验：</strong>在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。</p><p><strong>项目实践经验：</strong>在美团·点评的交互式用户行为分析系统中使用了这种方案，该系统主要是允许用户通过Java Web系统提交数据分析统计任务，后端通过Java提交Spark作业进行数据分析统计。要求Spark作业速度必须要快，尽量在10分钟以内，否则速度太慢，用户体验会很差。所以我们将有些Spark作业的shuffle操作提前到了Hive ETL中，从而让Spark直接使用预处理的Hive中间表，尽可能地减少Spark的shuffle操作，大幅度提升了性能，将部分作业的性能提升了6倍以上。</p><h3 id="解决方案二：过滤少数导致倾斜的key"><a href="#解决方案二：过滤少数导致倾斜的key" class="headerlink" title="解决方案二：过滤少数导致倾斜的key"></a>解决方案二：过滤少数导致倾斜的key</h3><p><strong>方案适用场景：</strong>如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。</p><p><strong>方案实现思路：</strong>如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。</p><p><strong>方案实现原理：</strong>将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。</p><p><strong>方案优点：</strong>实现简单，而且效果也很好，可以完全规避掉数据倾斜。</p><p><strong>方案缺点：</strong>适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。</p><p><strong>方案实践经验：</strong>在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。</p><h3 id="解决方案三：提高shuffle操作的并行度"><a href="#解决方案三：提高shuffle操作的并行度" class="headerlink" title="解决方案三：提高shuffle操作的并行度"></a>解决方案三：提高shuffle操作的并行度</h3><p><strong>方案适用场景：</strong>如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。</p><p><strong>方案实现思路：</strong>在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。</p><p><strong>方案实现原理：</strong>增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。具体原理如下图所示。</p><p><strong>方案优点：</strong>实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。</p><p><strong>方案缺点：</strong>只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。</p><p><strong>方案实践经验：</strong>该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用嘴简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。</p><p> <img src="https://tech.meituan.com/img/spark-tuning/shuffle-skwed-add-partition.png" alt="img"></p><h3 id="解决方案四：两阶段聚合（局部聚合-全局聚合）"><a href="#解决方案四：两阶段聚合（局部聚合-全局聚合）" class="headerlink" title="解决方案四：两阶段聚合（局部聚合+全局聚合）"></a>解决方案四：两阶段聚合（局部聚合+全局聚合）</h3><p><strong>方案适用场景：</strong>对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。</p><p><strong>方案实现思路：</strong>这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。</p><p><strong>方案实现原理：</strong>将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。</p><p><strong>方案优点：</strong>对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。</p><p><strong>方案缺点：</strong>仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。</p><p><img src="https://tech.meituan.com/img/spark-tuning/shuffle-skwed-two-phase-aggr.png" alt="img"></p><blockquote><p>Java 版</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 第一步，给RDD中的每个key都打上一个随机前缀。</span></span><br><span class="line">JavaPairRDD&lt;String, Long&gt; randomPrefixRdd = rdd.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> PairFunction&lt;Tuple2&lt;Long,Long&gt;, String, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Long&gt; <span class="title">call</span><span class="params">(Tuple2&lt;Long, Long&gt; tuple)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                Random random = <span class="keyword">new</span> Random();</span><br><span class="line">                <span class="keyword">int</span> prefix = random.nextInt(<span class="number">10</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, Long&gt;(prefix + <span class="string">"_"</span> + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第二步，对打上随机前缀的key进行局部聚合。</span></span><br><span class="line">JavaPairRDD&lt;String, Long&gt; localAggrRdd = randomPrefixRdd.reduceByKey(</span><br><span class="line">        <span class="keyword">new</span> Function2&lt;Long, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Long <span class="title">call</span><span class="params">(Long v1, Long v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第三步，去除RDD中每个key的随机前缀。</span></span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; removedRandomPrefixRdd = localAggrRdd.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> PairFunction&lt;Tuple2&lt;String,Long&gt;, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title">call</span><span class="params">(Tuple2&lt;String, Long&gt; tuple)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">long</span> originalKey = Long.valueOf(tuple._1.split(<span class="string">"_"</span>)[<span class="number">1</span>]);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;Long, Long&gt;(originalKey, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第四步，对去除了随机前缀的RDD进行全局聚合。</span></span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; globalAggrRdd = removedRandomPrefixRdd.reduceByKey(</span><br><span class="line">        <span class="keyword">new</span> Function2&lt;Long, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Long <span class="title">call</span><span class="params">(Long v1, Long v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure><blockquote><p> scala 版</p></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sc.textFile(<span class="string">"hdfs://mycluster/user/ap/a.txt"</span>,<span class="number">4</span>).flatMap(_.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>)).map(t =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> w1 = t._1; </span><br><span class="line">  <span class="keyword">import</span> scala.util.<span class="type">Random</span>;</span><br><span class="line">  <span class="keyword">val</span> n = <span class="type">Random</span>.nextInt(<span class="number">100</span>); </span><br><span class="line">  (w1 + <span class="string">"_"</span> + n, t._2);</span><br><span class="line">&#125;).reduceByKey(_ + _,<span class="number">4</span>).map(t =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> w2 = t._1; </span><br><span class="line">  <span class="keyword">val</span> count = t._2; </span><br><span class="line">  <span class="keyword">val</span> w3 = w2.split(<span class="string">"_"</span>)(<span class="number">0</span>); </span><br><span class="line">  (w3, count);</span><br><span class="line">&#125;).reduceByKey(_ + _,<span class="number">4</span>).saveAsTextFile(<span class="string">"/user/ap/scala/DataSkew"</span>)</span><br></pre></td></tr></table></figure><h3 id="解决方案五：将reduce-join转为map-join"><a href="#解决方案五：将reduce-join转为map-join" class="headerlink" title="解决方案五：将reduce join转为map join"></a>解决方案五：将reduce join转为map join</h3><p><strong>方案适用场景：</strong>在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。</p><p><strong>方案实现思路：</strong>不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。</p><p><strong>方案实现原理：</strong>普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。</p><p><strong>方案优点：</strong>对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。</p><p><strong>方案缺点：</strong>适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。</p><h3 id="解决方案五：将reduce-join转为map-join-1"><a href="#解决方案五：将reduce-join转为map-join-1" class="headerlink" title="解决方案五：将reduce join转为map join"></a>解决方案五：将reduce join转为map join</h3><p><strong>方案适用场景：</strong>在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。</p><p><strong>方案实现思路：</strong>不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。</p><p><strong>方案实现原理：</strong>普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。</p><p><strong>方案优点：</strong>对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。</p><p><strong>方案缺点：</strong>适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。</p><p> <img src="https://tech.meituan.com/img/spark-tuning/shuffle-skwed-map-join.png" alt="img"></p><blockquote><p><strong>通过广播变量, 避免掉 shuffle</strong></p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先将数据量比较小的RDD的数据，collect到Driver中来。</span></span><br><span class="line">List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1.collect()</span><br><span class="line"><span class="comment">// 然后使用Spark的广播功能，将小RDD的数据转换成广播变量，这样每个Executor就只有一份RDD的数据。</span></span><br><span class="line"><span class="comment">// 可以尽可能节省内存空间，并且减少网络传输性能开销。</span></span><br><span class="line"><span class="keyword">final</span> Broadcast&lt;List&lt;Tuple2&lt;Long, Row&gt;&gt;&gt; rdd1DataBroadcast = sc.broadcast(rdd1Data);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 对另外一个RDD执行map类操作，而不再是join类操作。</span></span><br><span class="line">JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRdd = rdd2.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> PairFunction&lt;Tuple2&lt;Long,String&gt;, String, Tuple2&lt;String, Row&gt;&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, String&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="comment">// 在算子函数中，通过广播变量，获取到本地Executor中的rdd1数据。</span></span><br><span class="line">                List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1DataBroadcast.value();</span><br><span class="line">                <span class="comment">// 可以将rdd1的数据转换为一个Map，便于后面进行join操作。</span></span><br><span class="line">                Map&lt;Long, Row&gt; rdd1DataMap = <span class="keyword">new</span> HashMap&lt;Long, Row&gt;();</span><br><span class="line">                <span class="keyword">for</span>(Tuple2&lt;Long, Row&gt; data : rdd1Data) &#123;</span><br><span class="line">                    rdd1DataMap.put(data._1, data._2);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 获取当前RDD数据的key以及value。</span></span><br><span class="line">                String key = tuple._1;</span><br><span class="line">                String value = tuple._2;</span><br><span class="line">                <span class="comment">// 从rdd1数据Map中，根据key获取到可以join到的数据。</span></span><br><span class="line">                Row rdd1Value = rdd1DataMap.get(key);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, String&gt;(key, <span class="keyword">new</span> Tuple2&lt;String, Row&gt;(value, rdd1Value));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这里得提示一下。</span></span><br><span class="line">-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=</span><br><span class="line"><span class="comment">// 上面的做法，仅仅适用于rdd1中的key没有重复，全部是唯一的场景。</span></span><br><span class="line"><span class="comment">// 如果rdd1中有多个相同的key，那么就得用flatMap类的操作，在进行join的时候不能用map，而是得遍历rdd1所有数据进行join。</span></span><br><span class="line"><span class="comment">// rdd2中每条数据都可能会返回多条join后的数据。</span></span><br><span class="line">-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=</span><br></pre></td></tr></table></figure><h3 id="解决方案六：采样倾斜key并分拆join操作"><a href="#解决方案六：采样倾斜key并分拆join操作" class="headerlink" title="解决方案六：采样倾斜key并分拆join操作"></a>解决方案六：采样倾斜key并分拆join操作</h3><p><strong>方案适用场景：</strong>两个RDD/Hive表进行join的时候，如果数据量都比较大，无法采用“解决方案五”，那么此时可以看一下两个RDD/Hive表中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD/Hive表中的少数几个key的数据量过大，而另一个RDD/Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。</p><p><strong>方案实现思路：</strong></p><ul><li>对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。</li><li>然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。</li><li>接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。</li><li>再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。</li><li>而另外两个普通的RDD就照常join即可。</li><li>最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。</li></ul><p><strong>方案实现原理：</strong>对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。具体原理见下图。</p><p><strong>方案优点：</strong>对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。</p><p><strong>方案缺点：</strong>如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。</p><p> <img src="https://tech.meituan.com/img/spark-tuning/shuffle-skwed-sample-expand.png" alt="img"></p><blockquote><p><strong>这Java 代码可真恶心, 有空改成 scala</strong></p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先从包含了少数几个导致数据倾斜key的rdd1中，采样10%的样本数据。</span></span><br><span class="line">JavaPairRDD&lt;Long, String&gt; sampledRDD = rdd1.sample(<span class="keyword">false</span>, <span class="number">0.1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 对样本数据RDD统计出每个key的出现次数，并按出现次数降序排序。</span></span><br><span class="line"><span class="comment">// 对降序排序后的数据，取出top 1或者top 100的数据，也就是key最多的前n个数据。</span></span><br><span class="line"><span class="comment">// 具体取出多少个数据量最多的key，由大家自己决定，我们这里就取1个作为示范。</span></span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; mappedSampledRDD = sampledRDD.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> PairFunction&lt;Tuple2&lt;Long,String&gt;, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;Long, Long&gt;(tuple._1, <span class="number">1L</span>);</span><br><span class="line">            &#125;     </span><br><span class="line">        &#125;);</span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; countedSampledRDD = mappedSampledRDD.reduceByKey(</span><br><span class="line">        <span class="keyword">new</span> Function2&lt;Long, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Long <span class="title">call</span><span class="params">(Long v1, Long v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; reversedSampledRDD = countedSampledRDD.mapToPair( </span><br><span class="line">        <span class="keyword">new</span> PairFunction&lt;Tuple2&lt;Long,Long&gt;, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title">call</span><span class="params">(Tuple2&lt;Long, Long&gt; tuple)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;Long, Long&gt;(tuple._2, tuple._1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 倾斜量最大的 key</span></span><br><span class="line"><span class="keyword">final</span> Long skewedUserid = reversedSampledRDD.sortByKey(<span class="keyword">false</span>).take(<span class="number">1</span>).get(<span class="number">0</span>)._2;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从rdd1中分拆出导致数据倾斜的key，形成独立的RDD。</span></span><br><span class="line">JavaPairRDD&lt;Long, String&gt; skewedRDD = rdd1.filter(</span><br><span class="line">        <span class="keyword">new</span> Function&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Boolean <span class="title">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"><span class="comment">// 从rdd1中分拆出不导致数据倾斜的普通key，形成独立的RDD。</span></span><br><span class="line">JavaPairRDD&lt;Long, String&gt; commonRDD = rdd1.filter(</span><br><span class="line">        <span class="keyword">new</span> Function&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Boolean <span class="title">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> !tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125; </span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// rdd2，就是那个所有key的分布相对较为均匀的rdd。</span></span><br><span class="line"><span class="comment">// 这里将rdd2中，前面获取到的key对应的数据，过滤出来，分拆成单独的rdd，并对rdd中的数据使用flatMap算子都扩容100倍。</span></span><br><span class="line"><span class="comment">// 对扩容的每条数据，都打上0～100的前缀。</span></span><br><span class="line">JavaPairRDD&lt;String, Row&gt; skewedRdd2 = rdd2.filter(</span><br><span class="line">         <span class="keyword">new</span> Function&lt;Tuple2&lt;Long,Row&gt;, Boolean&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Boolean <span class="title">call</span><span class="params">(Tuple2&lt;Long, Row&gt; tuple)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).flatMapToPair(<span class="keyword">new</span> PairFlatMapFunction&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Iterable&lt;Tuple2&lt;String, Row&gt;&gt; call(</span><br><span class="line">                    Tuple2&lt;Long, Row&gt; tuple) <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                Random random = <span class="keyword">new</span> Random();</span><br><span class="line">                List&lt;Tuple2&lt;String, Row&gt;&gt; list = <span class="keyword">new</span> ArrayList&lt;Tuple2&lt;String, Row&gt;&gt;();</span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">                    list.add(<span class="keyword">new</span> Tuple2&lt;String, Row&gt;(i + <span class="string">"_"</span> + tuple._1, tuple._2));</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> list;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将rdd1中分拆出来的导致倾斜的key的独立rdd，每条数据都打上100以内的随机前缀。</span></span><br><span class="line"><span class="comment">// 然后将这个rdd1中分拆出来的独立rdd，与上面rdd2中分拆出来的独立rdd，进行join。</span></span><br><span class="line">JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD1 = skewedRDD.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> PairFunction&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, String&gt; <span class="title">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                Random random = <span class="keyword">new</span> Random();</span><br><span class="line">                <span class="keyword">int</span> prefix = random.nextInt(<span class="number">100</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, String&gt;(prefix + <span class="string">"_"</span> + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        .join(skewedRdd2)</span><br><span class="line">        .mapToPair(<span class="keyword">new</span> PairFunction&lt;Tuple2&lt;String,Tuple2&lt;String,Row&gt;&gt;, Long, Tuple2&lt;String, Row&gt;&gt;() &#123;</span><br><span class="line">                        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="keyword">public</span> Tuple2&lt;Long, Tuple2&lt;String, Row&gt;&gt; call(</span><br><span class="line">                            Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; tuple)</span><br><span class="line">                            <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                            <span class="keyword">long</span> key = Long.valueOf(tuple._1.split(<span class="string">"_"</span>)[<span class="number">1</span>]);</span><br><span class="line">                            <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;Long, Tuple2&lt;String, Row&gt;&gt;(key, tuple._2);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将rdd1中分拆出来的包含普通key的独立rdd，直接与rdd2进行join。</span></span><br><span class="line">JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD2 = commonRDD.join(rdd2);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将倾斜key join后的结果与普通key join后的结果，uinon起来。</span></span><br><span class="line"><span class="comment">// 就是最终的join结果。</span></span><br><span class="line">JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD = joinedRDD1.union(joinedRDD2);</span><br></pre></td></tr></table></figure><h3 id="解决方案七：使用随机前缀和扩容RDD进行join"><a href="#解决方案七：使用随机前缀和扩容RDD进行join" class="headerlink" title="解决方案七：使用随机前缀和扩容RDD进行join"></a>解决方案七：使用随机前缀和扩容RDD进行join</h3><p><strong>方案适用场景：</strong>如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用最后一种方案来解决问题了。</p><p><strong>方案实现思路：</strong></p><ul><li>该方案的实现思路基本和“解决方案六”类似，首先查看RDD/Hive表中的数据分布情况，找到那个造成数据倾斜的RDD/Hive表，比如有多个key都对应了超过1万条数据。</li><li>然后将该RDD的每条数据都打上一个n以内的随机前缀。</li><li>同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。</li><li>最后将两个处理后的RDD进行join即可。</li></ul><p><strong>方案实现原理：</strong>将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到多个task中去处理，而不是让一个task处理大量的相同key。该方案与“解决方案六”的不同之处就在于，上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。</p><p><strong>方案优点：</strong>对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。</p><p><strong>方案缺点：</strong>该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。</p><p><strong>方案实践经验：</strong>曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先将其中一个key分布相对较为均匀的RDD膨胀100倍。</span></span><br><span class="line">JavaPairRDD&lt;String, Row&gt; expandedRDD = rdd1.flatMapToPair(</span><br><span class="line">        <span class="keyword">new</span> PairFlatMapFunction&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Iterable&lt;Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, Row&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                List&lt;Tuple2&lt;String, Row&gt;&gt; list = <span class="keyword">new</span> ArrayList&lt;Tuple2&lt;String, Row&gt;&gt;();</span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">                    list.add(<span class="keyword">new</span> Tuple2&lt;String, Row&gt;(<span class="number">0</span> + <span class="string">"_"</span> + tuple._1, tuple._2));</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> list;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 其次，将另一个有数据倾斜key的RDD，每条数据都打上100以内的随机前缀。</span></span><br><span class="line">JavaPairRDD&lt;String, String&gt; mappedRDD = rdd2.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> PairFunction&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, String&gt; <span class="title">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                Random random = <span class="keyword">new</span> Random();</span><br><span class="line">                <span class="keyword">int</span> prefix = random.nextInt(<span class="number">100</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, String&gt;(prefix + <span class="string">"_"</span> + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将两个处理后的RDD进行join即可。</span></span><br><span class="line">JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRDD = mappedRDD.join(expandedRDD);</span><br></pre></td></tr></table></figure><h3 id="解决方案八：多种方案组合使用"><a href="#解决方案八：多种方案组合使用" class="headerlink" title="解决方案八：多种方案组合使用"></a>解决方案八：多种方案组合使用</h3><p>在实践中发现，很多情况下，如果只是处理较为简单的数据倾斜场景，那么使用上述方案中的某一种基本就可以解决。但是如果要处理一个较为复杂的数据倾斜场景，那么可能需要将多种方案组合起来使用。比如说，我们针对出现了多个数据倾斜环节的Spark作业，可以先运用解决方案一和二，预处理一部分数据，并过滤一部分数据来缓解；其次可以对某些shuffle操作提升并行度，优化其性能；最后还可以针对不同的聚合或join操作，选择一种方案来优化其性能。大家需要对这些方案的思路和原理都透彻理解之后，在实践中根据各种不同的情况，灵活运用多种方案，来解决自己的数据倾斜问题。</p><hr><h1 id="四-shuffle-调优"><a href="#四-shuffle-调优" class="headerlink" title="四. shuffle 调优"></a>四. shuffle 调优</h1><h2 id="调优概述-3"><a href="#调优概述-3" class="headerlink" title="调优概述"></a>调优概述</h2><p>大多数Spark作业的性能主要就是消耗在了shuffle环节，因为该环节包含了大量的磁盘IO、序列化、网络数据传输等操作。因此，如果要让作业的性能更上一层楼，就有必要对shuffle过程进行调优。但是也必须提醒大家的是，影响一个Spark作业性能的因素，主要还是代码开发、资源参数以及数据倾斜，shuffle调优只能在整个Spark的性能调优中占到一小部分而已。因此大家务必把握住调优的基本原则，千万不要舍本逐末。下面我们就给大家详细讲解shuffle的原理，以及相关参数的说明，同时给出各个参数的调优建议。</p><h2 id="ShuffleManager发展概述"><a href="#ShuffleManager发展概述" class="headerlink" title="ShuffleManager发展概述"></a>ShuffleManager发展概述</h2><p> 在Spark的源码中，<strong>负责shuffle过程的执行、计算和处理的组件主要就是<code>ShuffleManager</code></strong>，也即shuffle管理器。而随着Spark的版本的发展，ShuffleManager也在不断迭代，变得越来越先进。</p><p>在<strong>Spark 1.2以前，默认的shuffle计算引擎是<code>HashShuffleManager</code>。</strong>该ShuffleManager而HashShuffleManager有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。</p><p>因此在<strong>Spark 1.2以后的版本中，默认的ShuffleManager改成了<code>SortShuffleManager</code>。</strong><u>SortShuffleManager</u>相较于HashShuffleManager来说，有了一定的改进。主要就在于，<u>每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并（merge）成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的shuffle read task拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。</u></p><p>下面我们详细分析一下HashShuffleManager和SortShuffleManager的原理。</p><h2 id="HashShuffleManager运行原理"><a href="#HashShuffleManager运行原理" class="headerlink" title="HashShuffleManager运行原理"></a>HashShuffleManager运行原理</h2><h3 id="未经优化的HashShuffleManager"><a href="#未经优化的HashShuffleManager" class="headerlink" title="未经优化的HashShuffleManager"></a>未经优化的HashShuffleManager</h3><p>下图说明了未经优化的HashShuffleManager的原理。这里我们先明确一个假设前提：每个Executor只有1个CPU core，也就是说，无论这个Executor上分配多少个task线程，同一时间都只能执行一个task线程。</p><p>我们先从<code>shuffle write</code>开始说起。shuffle write阶段，主要就是在一个stage结束计算之后，为了下一个stage可以执行shuffle类的算子（比如reduceByKey），而将每个task处理的数据按key进行“分类”。所谓“分类”，就是对相同的key执行hash算法，从而将相同key都写入同一个磁盘文件中，而每一个磁盘文件都只属于下游stage的一个task。在将数据写入磁盘之前，会先将数据写入内存缓冲中，当内存缓冲填满之后，才会溢写到磁盘文件中去。</p><p>那么<strong>每个执行shuffle write的task</strong>，要为下一个stage创建多少个磁盘文件呢？很简单，<strong>下一个stage的task有多少个，当前stage的每个task就要创建多少份磁盘文件</strong>。 如果单纯按照 key 来 hash 的话, 有多少不同的 key, ,每个 task 就会创建多少份磁盘文件.</p><p>比如下一个stage总共有100个task，那么当前stage的每个task都要创建100份磁盘文件。如果当前stage有50个task，总共有10个Executor，每个Executor执行5个Task，那么每个Executor上总共就要创建500个磁盘文件，所有Executor上会创建5000个磁盘文件。由此可见，未经优化的shuffle write操作所产生的磁盘文件的数量是极其惊人的。</p><p>接着我们来说说<code>shuffle read</code>。shuffle read，通常就是一个stage刚开始时要做的事情。此时该stage的每一个task就需要将上一个stage的计算结果中的所有相同key，从各个节点上通过网络都拉取到自己所在的节点上，然后进行key的聚合或连接等操作。由于shuffle write的过程中，task给下游stage的每个task都创建了一个磁盘文件，因此shuffle read的过程中，每个task只要从上游stage的所有task所在节点上，拉取属于自己的那一个磁盘文件即可。</p><p>shuffle read的拉取过程是一边拉取一边进行聚合的。每个shuffle read task都会有一个自己的buffer缓冲，每次都只能拉取与buffer缓冲相同大小的数据，然后通过内存中的一个Map进行聚合等操作。聚合完一批数据后，再拉取下一批数据，并放到buffer缓冲中进行聚合操作。以此类推，直到最后将所有数据到拉取完，并得到最终的结果。</p><p> <img src="https://tech.meituan.com/img/spark-tuning/hash-shuffle-common.png" alt="img"></p><h3 id="优化后的HashShuffleManager"><a href="#优化后的HashShuffleManager" class="headerlink" title="优化后的HashShuffleManager"></a>优化后的HashShuffleManager</h3><p>下图说明了优化后的<code>HashShuffleManager</code>的原理。这里说的<strong>优化，是指我们可以设置一个参数</strong>，<strong><code>spark.shuffle.consolidateFiles</code>。</strong>该参数默认值为false，将其<strong>设置为true即可开启优化机制</strong>。通常来说，如果我们使用HashShuffleManager，那么都建议开启这个选项。</p><p>开启consolidate机制之后，在shuffle write过程中，task就不是为下游stage的每个task创建一个磁盘文件了。此时会出现shuffleFileGroup的概念，<strong>每个<code>shuffleFileGroup</code>会对应一批磁盘文件，磁盘文件的数量与下游stage的task数量是相同的。</strong>一个Executor上有多少个CPU core，就可以并行执行多少个task。而第一批并行执行的每个task都会创建一个shuffleFileGroup，并将数据写入对应的磁盘文件内。</p><p><strong>当Executor的CPU core执行完一批task，接着执行下一批task时，下一批task就会复用之前已有的shuffleFileGroup，包括其中的磁盘文件。</strong>也就是说, <strong>同一个Executor的所有 task 中, 总共有多少不同的 key, 就会创建多少个磁盘文件, 这些磁盘文件都在同一个shuffleFileGroup中.</strong> 而不会出现, 相同的 Executor 的不同 task 中, 如果存在相同的 key, 还会各自创建自己的磁盘文件的情况. </p><p>也就是说，此时task会将数据写入已有的磁盘文件中，而不会写入新的磁盘文件中。因此，consolidate机制允许不同的task复用同一批磁盘文件，这样就可以有效将多个task的磁盘文件进行一定程度上的合并，从而大幅度减少磁盘文件的数量，进而提升shuffle write的性能。</p><p>假设第二个stage有100个task，第一个stage有50个task，总共还是有10个Executor，每个Executor执行5个task。那么原本使用未经优化的HashShuffleManager时，每个Executor会产生500个磁盘文件，所有Executor会产生5000个磁盘文件的。但是此时经过优化之后，每个Executor创建的磁盘文件的数量的计算公式为：CPU core的数量 * 下一个stage的task数量。也就是说，每个Executor此时只会创建100个磁盘文件，所有Executor只会创建1000个磁盘文件。</p><p> <img src="https://tech.meituan.com/img/spark-tuning/hash-shuffle-consolidate.png" alt="img"></p><h2 id="SortShuffleManager运行原理"><a href="#SortShuffleManager运行原理" class="headerlink" title="SortShuffleManager运行原理"></a>SortShuffleManager运行原理</h2><p><code>SortShuffleManager</code>的运行机制主要分成两种，一种是普通运行机制，另一种是bypass运行机制。<strong>当shuffle read task的数量小于等于spark.shuffle.sort.bypassMergeThreshold参数的值时（默认为200），就会启用bypass机制。</strong></p><h3 id="普通运行机制"><a href="#普通运行机制" class="headerlink" title="普通运行机制"></a>普通运行机制</h3><p>下图说明了普通的<code>SortShuffleManager</code>的原理。在该模式下，<strong>数据会先写入一个内存数据结构中，此时根据不同的shuffle算子，可能选用不同的数据结构</strong>。如果是<code>reduceByKey</code>这种聚合类的shuffle算子，那么会选用<code>Map数据结构</code>，一边通过Map进行聚合，一边写入内存；如果是<code>join</code>这种普通的shuffle算子，那么会<code>选用Array数据结构</code>，<strong>直接写入内存</strong>。接着，<u>每写一条数据进入内存数据结构之后，就会判断一下，是否达到了某个临界阈值</u>。如果<strong>达到临界阈值</strong>的话，那么就会尝试<strong>将内存数据结构中的数据溢写到磁盘，然后清空内存数据结构</strong>。</p><p><strong>在溢写到磁盘文件之前，会先对内存数据结构中已有的数据, 根据key, 把 key 对应的 value 进行排序</strong>。排序过后，会分批将数据写入磁盘文件。默认的batch数量是10000条(10000个 key, 一个 key 一条)，也就是说，排序好的数据，会以每批1万条数据的形式分批写入磁盘文件。写入磁盘文件是通过Java的BufferedOutputStream实现的。BufferedOutputStream是Java的缓冲输出流，首先会将数据缓冲在内存中，当内存缓冲满溢之后再一次写入磁盘文件中，这样可以减少磁盘IO次数，提升性能。</p><p>一个task将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写操作，也就会产生多个临时文件。最后会将之前所有的临时磁盘文件都进行合并，这就是merge过程，此时会将之前所有临时磁盘文件中的数据读取出来，然后依次写入最终的磁盘文件之中。此外，<strong>由于一个task就只对应一个磁盘文件，也就意味着该task为下游stage的task准备的数据都在这一个文件中</strong>，因此<strong>还会单独写一份索引文件，其中标识了下游各个task的数据在文件中的start offset与end offset。</strong></p><p>SortShuffleManager由于有一个磁盘文件merge的过程，因此大大减少了文件数量。比如第一个stage有50个task，总共有10个Executor，每个Executor执行5个task，而第二个stage有100个task。由于<strong>每个task最终只有一个磁盘文件</strong>，因此此时每个Executor上只有5个磁盘文件，所有Executor只有50个磁盘文件。</p><p><strong>注意:</strong> </p><ul><li>排序是指 key 的 value 排序</li><li>batch 写入磁盘是指, 默认 10000 个 (k,v)写一次.</li><li>是否会 hash? :TODO</li></ul><p><img src="https://tech.meituan.com/img/spark-tuning/sort-shuffle-common.png" alt="img"></p><h3 id="bypass运行机制"><a href="#bypass运行机制" class="headerlink" title="bypass运行机制"></a>bypass运行机制</h3><p>下图说明了bypass SortShuffleManager的原理。bypass运行机制的触发条件如下：</p><ul><li>shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值。</li><li><strong>其实bypass 就是一个高级的参数, 设置后, 如果小于此阈值, key 对应的 value 就不会进行排序</strong></li><li>不是聚合类的shuffle算子（比如reduceByKey）。</li></ul><p>此时task会为每个下游task都创建一个临时磁盘文件，并将数据按key进行hash然后根据key的hash值，将key写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件的。最后，同样会将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。</p><p>该过程的磁盘写机制其实跟未经优化的HashShuffleManager是一模一样的，因为都要创建数量惊人的磁盘文件，只是在最后会做一个磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的HashShuffleManager来说，shuffle read的性能会更好。</p><p>而该机制与普通SortShuffleManager运行机制的不同在于：第一，磁盘写机制不同；第二，不会进行排序。也就是说，启用该机制的最大好处在于，shuffle write过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销。</p><p><img src="https://tech.meituan.com/img/spark-tuning/sort-shuffle-bypass.png" alt="img"></p><h2 id="shuffle相关参数调优"><a href="#shuffle相关参数调优" class="headerlink" title="shuffle相关参数调优"></a>shuffle相关参数调优</h2><p>以下是Shffule过程中的一些主要参数，这里详细讲解了各个参数的功能、默认值以及基于实践经验给出的调优建议。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">Sparkconf</span>()</span><br><span class="line">conf.set(<span class="string">"spark.shuffle.manager"</span>,<span class="string">"hash"</span>)</span><br><span class="line">conf.set(<span class="string">"spark.shuffle.consolidateFiles"</span>,<span class="literal">true</span>)</span><br><span class="line">....</span><br><span class="line"><span class="comment">// 类似于这样设置</span></span><br></pre></td></tr></table></figure><h3 id="spark-shuffle-file-buffer"><a href="#spark-shuffle-file-buffer" class="headerlink" title="spark.shuffle.file.buffer"></a>spark.shuffle.file.buffer</h3><ul><li><strong>默认值：32k</strong></li><li>参数说明：该参数<strong>用于设置<code>shuffle write task</code>的<code>BufferedOutputStream</code>的<code>buffer</code>缓冲大小</strong>。将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘。</li><li>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少<code>shuffle write</code>过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。<strong>在实践中发现，合理调节该参数，性能会有1%~5%的提升。</strong></li></ul><h3 id="spark-reducer-maxSizeInFlight"><a href="#spark-reducer-maxSizeInFlight" class="headerlink" title="spark.reducer.maxSizeInFlight"></a>spark.reducer.maxSizeInFlight</h3><ul><li><strong>默认值：48m</strong></li><li>参数说明：该参数用于设置<strong>shuffle read task的buffer缓冲大小</strong>，而这个buffer缓冲决定了每次能够拉取多少数据。</li><li>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</li></ul><h3 id="spark-shuffle-io-maxRetries"><a href="#spark-shuffle-io-maxRetries" class="headerlink" title="spark.shuffle.io.maxRetries"></a>spark.shuffle.io.maxRetries</h3><ul><li><strong>默认值：3</strong></li><li>参数说明：shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了<strong>可以重试的最大次数</strong>。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。</li><li>调优建议：对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，<strong>对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。</strong></li></ul><h3 id="spark-shuffle-io-retryWait"><a href="#spark-shuffle-io-retryWait" class="headerlink" title="spark.shuffle.io.retryWait"></a>spark.shuffle.io.retryWait</h3><ul><li><strong>默认值：5s</strong></li><li>参数说明：具体解释同上，该参数代表了<strong>每次重试拉取数据的等待间隔，默认是5s。</strong></li><li>调优建议：<strong>建议加大间隔时长（比如60s），以增加shuffle操作的稳定性。</strong></li></ul><h3 id="spark-shuffle-memoryFraction-1"><a href="#spark-shuffle-memoryFraction-1" class="headerlink" title="spark.shuffle.memoryFraction"></a>spark.shuffle.memoryFraction</h3><ul><li><strong>默认值：0.2</strong></li><li>参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。</li><li>调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很少使用持久化操作，建议调高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，<strong>合理调节该参数可以将性能提升10%左右。</strong></li></ul><h3 id="spark-shuffle-manager"><a href="#spark-shuffle-manager" class="headerlink" title="spark.shuffle.manager"></a>spark.shuffle.manager</h3><ul><li><strong>默认值：sort</strong></li><li>参数说明：该参数<strong>用于设置ShuffleManager的类型</strong>。<strong><code>Spark 1.5</code>以后，有三个可选项：<code>hash</code>、<code>sort</code>和<code>tungsten-sort</code>。</strong><code>HashShuffleManager</code>是<code>Spark 1.2</code>以前的默认选项，但是<code>Spark 1.2</code><strong>以及之后</strong>的版本<strong>默认都是<code>SortShuffleManager</code>了</strong>。<code>tungsten-sort</code>与sort类似，但是使用了tungsten计划中的<strong>堆外内存管理机制</strong>，内存使用效率更高。</li><li>调优建议：由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中<strong>需要该排序机制</strong>的话，则<strong>使用默认的SortShuffleManager就可以</strong>；而如果你的业务逻辑<strong>不需要对数据进行排序</strong>，那么建议参考后面的几个参数调优，<strong>通过bypass机制或优化的HashShuffleManager来避免排序操作</strong>，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug。</li></ul><h3 id="spark-shuffle-sort-bypassMergeThreshold"><a href="#spark-shuffle-sort-bypassMergeThreshold" class="headerlink" title="spark.shuffle.sort.bypassMergeThreshold"></a>spark.shuffle.sort.bypassMergeThreshold</h3><ul><li><strong>默认值：200</strong></li><li>参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。</li><li>调优建议：当你<strong>使用<code>SortShuffleManager</code>时，如果的确<code>不需要</code>排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量</strong>。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。</li></ul><h3 id="spark-shuffle-consolidateFiles"><a href="#spark-shuffle-consolidateFiles" class="headerlink" title="spark.shuffle.consolidateFiles"></a>spark.shuffle.consolidateFiles</h3><ul><li><strong>默认值：false</strong></li><li>参数说明：<strong>如果使用HashShuffleManager，该参数有效。</strong>如果设置为<strong>true</strong>，那么就会<strong>开启consolidate机制</strong>，会<strong>大幅度合并shuffle write的输出文件</strong>，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。</li><li>调优建议：如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还<strong>可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。</strong>在实践中尝试过，发现<strong>其性能比开启了bypass机制的SortShuffleManager要高出10%~30%。</strong></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;Spark整套调优方案主要分为开发调优、资源调优、数据倾斜调优、shuffle调优几个部分。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;开发调优和资源调
      
    
    </summary>
    
      <category term="Spark" scheme="https://airpoet.github.io/categories/Spark/"/>
    
      <category term="精讲" scheme="https://airpoet.github.io/categories/Spark/%E7%B2%BE%E8%AE%B2/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Spark" scheme="https://airpoet.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark重点解析(一) =&gt; SparkCore</title>
    <link href="https://airpoet.github.io/2018/07/25/Spark/Spark%E9%87%8D%E7%82%B9%E8%A7%A3%E6%9E%90(%E4%B8%80)%20=%3E%20SparkCore/"/>
    <id>https://airpoet.github.io/2018/07/25/Spark/Spark重点解析(一) =&gt; SparkCore/</id>
    <published>2018-07-25T06:58:04.056Z</published>
    <updated>2018-07-28T03:23:35.440Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一-Spark-与-MapReduce-区别"><a href="#一-Spark-与-MapReduce-区别" class="headerlink" title="一. Spark 与 MapReduce 区别"></a>一. Spark 与 MapReduce 区别</h1><p><strong>Apache Spark™</strong> is a fast and general engine for large-scale data processing.</p><p><strong>与mapreduce比较 </strong>：</p><ul><li><p>Spark大多数执行过程是基于内存的迭代</p></li><li><p>MapReduce 的 优点, SparkCore 都有</p></li><li><p>Hive 能做的操作, SparkSQL 都能做, 可以写 SQL 语句转换为 SparkCore 代码</p></li><li><p>Spark Streaming 提供近实时流</p></li><li><p>超过80个类似于 map, reduce 这样的操作</p></li><li><p>可以在<a href="http://www.alluxio.org/" target="_blank" rel="noopener">Tachyon</a>（基于内存的分布式的文件系统 (HDFS 是基于磁盘)) 上运行Spark, 会更快</p></li></ul><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-25-070107.png" alt="image-20180725150106695"></p><hr><h1 id="二-什么是RDD"><a href="#二-什么是RDD" class="headerlink" title="二. 什么是RDD"></a>二. 什么是RDD</h1><p>1、RDD是Spark提供的核心<strong>抽象</strong>，全称为Resillient Distributed Dataset，即<strong>弹性</strong>分布式数据集。</p><p>2、RDD在抽象上来说是一种元素集合，包含了数据。它是<strong>被分区的</strong>，分为多个分区，每个分区分布在集群中的不同节点上，从而让RDD中的数据可以被并行操作。（分布式数据集）</p><ul><li>如果读取的文件本来就存在于3个分区, 这些操作会并行操作, 如何并行操作?  :TODO</li><li>如果存在于3个分区, 手动规定了2个分区, 那么是如何工作的 ? :TODO</li></ul><p>3、RDD通常<strong>通过Hadoop上的文件</strong>，即HDFS文件或者Hive表，来进行<strong>创建</strong>；有时也可以<strong>通过应用程序中的集合来创建</strong>。</p><ul><li><strong>从文件系统读取</strong>: local 或 HDFS <code>sc.textFile(&quot;/Users/shixuanji/Documents/a.txt&quot;,2);</code></li><li>Hive 表: : TODO</li><li>并行化的方式创建(多用于测试): <code>val rdd = sc.makeRDD(1 to 10)</code>  或者 <code>val rdd = sc.parallelize(arr);</code></li></ul><p>4、RDD最重要的特性就是，提供了<strong>容错性</strong>，可以自动从节点失败中恢复过来。即如果某个节点上的RDD partition，因为节点故障，导致数据丢了，那么<strong>RDD会自动通过自己的数据来源重新计算该partition</strong>。这一切对使用者是透明的。</p><p>5、RDD的数据<strong>默认</strong>情况下存<strong>放在内存</strong>中的，但是在<strong>内存资源不足时</strong>，Spark<strong>会自动将RDD数据写入磁盘</strong>。（弹性 == 灵活）</p><blockquote><p><strong>下图中画橙色框的都是 RDD </strong></p></blockquote><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-25-070716.png" alt="image-20180725150715950"></p><h4 id="源码中的注释说明"><a href="#源码中的注释说明" class="headerlink" title="源码中的注释说明"></a><strong>源码中的注释说明</strong></h4><p>1、A list of partitions：一组分片（Partition），即数据集的基本组成单位</p><p>2、A function for computing each split：一个计算每个分区的函数</p><p>3、A list of dependencies on other RDDs：RDD 之间的依赖关系</p><ul><li>NarrowDependency   完全依赖, 窄依赖</li><li>ShuffleDependency   部分依赖, ‘’宽依赖’’</li></ul><p>4、Optionally, a Partitioner for <strong>key-value RDDs</strong> (e.g. to say that the RDD is hash-partitioned)：</p><ul><li>Partitioner: 自定义分区使用</li></ul><p>5、Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)：一个列表，存储存取每个 Partition 的优先位置（preferred location）。</p><h4 id="Spark-中其它重要概念"><a href="#Spark-中其它重要概念" class="headerlink" title="Spark 中其它重要概念"></a>Spark 中其它重要概念</h4><p><strong>Cluster Manager</strong>：Spark 的集群管理器，主要负责资源的分配与管理。集群管理器分配的资 源属于一级分配，它将各个 Worker 上的内存、CPU 等资源分配给应用程序，但是并不负责 对 Executor 的资源分配。目前，Standalone、YARN、Mesos、K8S，EC2 等都可以作为 Spark 的集群管理器。</p><p><strong>Master</strong>：Spark 集群的主节点。</p><p><strong>Worker</strong>：Spark 集群的工作节点。对 Spark 应用程序来说，由集群管理器分配得到资源的 Worker 节点主要负责以下工作：创建 Executor，将资源和任务进一步分配给 Executor，同步 资源信息给 Cluster Manager。</p><p><strong>Executor</strong>：执行计算任务的一些进程。主要负责任务的执行以及与 Worker、Driver Application 的信息同步。</p><p><strong>Driver Appication：</strong>客户端驱动程序，也可以理解为客户端应用程序，用于将任务程序转换 为 RDD 和 DAG，并与 Cluster Manager 进行通信与调度。</p><p><strong>关系</strong>:</p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-26-093416.png" alt="image-20180726173416513"></p><hr><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-26-093433.png" alt="image-20180726173433286"></p><p>1、用户使用 SparkContext 提供的 API（常用的有 textFile、sequenceFile、runJob、stop 等） 编写 Driver Application 程序。此外 SQLContext、HiveContext 及 StreamingContext 对 SparkContext 进行封装，并提供了 SQL、Hive 及流式计算相关的 API。</p><p>2、使用 SparkContext 提交的用户应用程序，首先会使用 BlockManager 和 BroadcastManager 将任务的 Hadoop 配置进行广播。然后由 DAGScheduler 将任务转换为 RDD 并组织成 DAG， DAG 还将被划分为不同的 Stage。<strong>最后由 TaskScheduler 借助 ActorSystem 将任务提交给集群 管理器（Cluster Manager）。</strong></p><p>3、集群管理器（ClusterManager）给任务分配资源，即将具体任务分配到 Worker 上，Worker 创建 Executor 来处理任务的运行。Standalone、YARN、Mesos、EC2 等都可以作为 Spark 的集 群管理器。</p><p><strong>注意:</strong>  如果是 –deploy-mode <strong>client</strong> 模式, <strong>client 就是 Driver</strong>,   –deploy-mode <strong>cluster</strong> 模式, <strong>Driver 是由集群分配的一台 worker节点</strong></p><h1 id="三-Spark-的架构-standalone"><a href="#三-Spark-的架构-standalone" class="headerlink" title="三. Spark 的架构(standalone)"></a>三. Spark 的架构(standalone)</h1><p>涉及到的名词: <strong>Driver</strong>, <strong>Master</strong>, <strong>Worker</strong>, <strong>Executor</strong> , <strong>Task</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-25-075654.png" alt="image-20180725155654170"></p><hr><h1 id="四-Spark-任务提交"><a href="#四-Spark-任务提交" class="headerlink" title="四. Spark 任务提交"></a>四. Spark 任务提交</h1><p>参考官网 <a href="http://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/submitting-applications.html</a></p><p><strong>Client</strong>模式</p><p>不指定<code>deploy-mode</code> ,默认就是<strong>client</strong>模式，也就是<strong>哪一台服务器提交</strong>spark代码，那么<strong>哪一台就是driver服务器</strong>。</p><p><strong>Cluster</strong>模式</p><p>需要指定<code>deploy-mode</code>，driver服务器并不是提交代码的那一台服务器，而是在提交代码的时候，在<strong>worker</strong>主机上，<strong>随机挑选一台作为driver服务器</strong>，那么<u>如果提交10个应用，那么就有可能10台driver服务器</u>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run application locally on 8 cores</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master <span class="built_in">local</span>[8] \</span><br><span class="line">  /path/to/examples.jar \</span><br><span class="line">  100</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run on a Spark standalone cluster in client deploy mode</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master spark://207.184.161.138:7077 \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --total-executor-cores 100 \</span><br><span class="line">  /path/to/examples.jar \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run on a Spark standalone cluster in cluster deploy mode with supervise</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master spark://207.184.161.138:7077 \</span><br><span class="line">  --deploy-mode cluster \</span><br><span class="line">  --supervise \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --total-executor-cores 100 \</span><br><span class="line">  /path/to/examples.jar \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run on a YARN cluster</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=XXX</span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master yarn \</span><br><span class="line">  --deploy-mode cluster \  <span class="comment"># can be client for client mode</span></span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --num-executors 50 \</span><br><span class="line">  /path/to/examples.jar \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run a Python application on a Spark standalone cluster</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --master spark://207.184.161.138:7077 \</span><br><span class="line">  examples/src/main/python/pi.py \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run on a Mesos cluster in cluster deploy mode with supervise</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master mesos://207.184.161.138:7077 \</span><br><span class="line">  --deploy-mode cluster \</span><br><span class="line">  --supervise \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --total-executor-cores 100 \</span><br><span class="line">  http://path/to/examples.jar \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run on a Kubernetes cluster in cluster deploy mode</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master k8s://xx.yy.zz.ww:443 \</span><br><span class="line">  --deploy-mode cluster \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --num-executors 50 \</span><br><span class="line">  http://path/to/examples.jar \</span><br><span class="line">  1000</span><br></pre></td></tr></table></figure><hr><h1 id="五-Transformation和action原理"><a href="#五-Transformation和action原理" class="headerlink" title="五. Transformation和action原理"></a>五. Transformation和action原理</h1><p><strong>Spark支持两种RDD操作：<code>transformation</code>和<code>action</code></strong>。</p><ul><li><code>transformation</code>操作会<strong>针对已有的RDD创建一个新的RDD</strong>；</li><li>而<code>action</code>则主要是对RDD进行最后的操作，比如遍历、reduce、保存到文件等，并可以<strong>返回结果给Driver程序</strong>。</li></ul><p>例如，map就是一种transformation操作，它用于将已有RDD的每个元素传入一个自定义的函数，并获取一个新的元素，然后将所有的新元素组成一个新的RDD。而reduce就是一种action操作，它用于对RDD中的所有元素进行聚合操作，并获取一个最终的结果，然后返回给Driver程序。</p><p><strong>transformation的特点就是lazy特性</strong>。lazy特性指的是，如果一个spark应用中只定义了transformation操作，那么即使你执行该应用，这些操作也不会执行。也就是说，transformation是不会触发spark程序的执行的，它们只是记录了对RDD所做的操作，但是不会自发的执行。只有当transformation之后，接着执行了一个action操作，那么所有的transformation才会执行。<strong>Spark通过这种lazy特性，来进行底层的spark应用执行的优化，<u>避免产生过多中间结果</u>。</strong></p><p><strong>action操作执行，会触发一个<code>spark job</code>的运行，从而触发这个action之前所有的transformation的执行。</strong>这是action的特性。</p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-25-084407.png" alt="image-20180725164407028"></p><hr><h1 id="六-Transformation-和-Action-算子"><a href="#六-Transformation-和-Action-算子" class="headerlink" title="六. Transformation 和 Action 算子"></a>六. Transformation 和 Action 算子</h1><h2 id="1-Transformation-算子"><a href="#1-Transformation-算子" class="headerlink" title="1.Transformation 算子"></a>1.Transformation 算子</h2><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/java/TransfromationOperation.java" target="_blank" rel="noopener">Java 版代码, 见这里 &amp; 上层目录</a> </p><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/scala/TransformationOperations.scala" target="_blank" rel="noopener">Scala 版代码, 见这里 &amp; 上层目录</a></p><p><strong>或 ../</strong></p><h4 id="map"><a href="#map" class="headerlink" title="map:"></a>map:</h4><p>对调用map的RDD数据集中的每个element都使用func，然后返回一个新的RDD,这个返回的数据集是分布式的数据集</p><h4 id="filter"><a href="#filter" class="headerlink" title="filter:"></a>filter:</h4><p>对调用filter的RDD数据集中的每个元素都使用func，然后返回一个包含使func为true的元素构成的RDD</p><h4 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap:"></a>flatMap:</h4><p>和map差不多，但是flatMap生成的是多个结果</p><h4 id="groupByKey-reduceByKey-sortByKey"><a href="#groupByKey-reduceByKey-sortByKey" class="headerlink" title="groupByKey, reduceByKey, sortByKey :"></a>groupByKey, reduceByKey, sortByKey :</h4><p>凡是这种.. ByKey 的, 必须传入一个对偶元祖,  Java中是 JavaPairRdd</p><h4 id="cogroup-与-join-与-union-区别"><a href="#cogroup-与-join-与-union-区别" class="headerlink" title="cogroup 与 join 与 union 区别:"></a>cogroup 与 join 与 union 区别:</h4><p><a href="http://lxw1234.com/archives/2015/07/384.htm" target="_blank" rel="noopener">详情可参考</a></p><ul><li><strong>cogroup</strong><ul><li>相当于SQL中的<strong>全外关联full outer join</strong>，返回左右RDD中的记录，<strong>关联不上的为空。</strong></li><li>return:   JavaPairRDD[K, (JIterable[V], JIterable[W])</li><li>RDD的value是一个Pair的实例,这个实例包含两个Iterable的值, <code>V</code>表示的是RDD1中相同KEY的值, <code>W</code>表示的是RDD2中相同key的值. </li></ul></li><li><strong>join</strong><ul><li>相当于SQL中的<strong>内关联join</strong>，<strong>只返回两个RDD根据K可以关联上的结果</strong>，join只能用于两个RDD之间的关联，如果要多个RDD关联，多关联几次即可。</li></ul></li><li><strong>union</strong><ul><li>求rdd并集，但是不去重</li></ul></li></ul><h4 id="Intersection，Distinct，Cartesian"><a href="#Intersection，Distinct，Cartesian" class="headerlink" title="Intersection，Distinct，Cartesian"></a>Intersection，Distinct，Cartesian</h4><ul><li><strong>intersection</strong> <ul><li>intersection <strong>求交集</strong>,提取两个rdd中都含有的元素。</li><li>Returns a new RDD that contains the intersection of elements in the source dataset and the argument.</li></ul></li><li><strong>Distinct</strong> (独特的,有区别的)<ul><li>去重 </li><li>Return a new RDD containing the distinct elements in this RDD.</li></ul></li><li><strong>Cartesian</strong> (笛卡尔积)<ul><li>笛卡尔积, 全连接, 前后集合个数为a,b,  a x b  种组合</li></ul></li></ul><h4 id="mapPartition，reparation，coalesce"><a href="#mapPartition，reparation，coalesce" class="headerlink" title="mapPartition，reparation，coalesce"></a>mapPartition，reparation，coalesce</h4><ul><li><p><strong>mapPartition</strong></p><ul><li>该函数和map函数类似，只不过映射函数的参数<strong>由RDD中的每一个元素</strong>变成了<strong><code>RDD</code>中每一个分区的迭代器</strong>。如果在映射的过程中需要频繁创建额外的对象，使用mapPartitions要比map高效的多。 </li><li>比如，将RDD中的所有数据通过JDBC连接写入数据库，如果使用map函数，可能要为每一个元素都创建一个<code>connection</code>，这样开销很大，如果使用<code>mapPartitions</code>，那么只需要针对每一个分区建立一个<code>connection</code>。 </li></ul></li><li><p><strong>coalesce</strong> </p><ul><li><strong>coalesce: 只能用于减少分区的数量</strong>，而且<strong>可以选择不发生shuffle</strong> 其实说白了他就是<strong>合并分区</strong></li><li><strong>repartition:可以增加分区，也可以减少分区</strong>，<strong>必须会发生shuffle</strong>，相当于就是进行<strong>重新分区</strong></li></ul></li><li><p><strong>reparation</strong></p><ul><li>reparition是<code>coalesce shuffle</code>为<code>true</code>的简易实现 </li></ul></li></ul><h4 id="sample-和-aggregateByKey"><a href="#sample-和-aggregateByKey" class="headerlink" title="sample   和  aggregateByKey"></a>sample   和  aggregateByKey</h4><ul><li><p><strong>sample</strong></p><ul><li><p>对RDD中的集合内元素进行采样，第一个参数withReplacement是true表示有放回取样，false表示无放回。第二个参数表示比例 </p></li><li><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *- @param withReplacement can elements be sampled multiple times (replaced when sampled out)</span></span><br><span class="line"><span class="comment">    @param fraction expected size of the sample as a fraction of this RDD's size</span></span><br><span class="line"><span class="comment">       seed  最好不要动</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span></span>(</span><br><span class="line">      withReplacement: <span class="type">Boolean</span>,</span><br><span class="line">      fraction: <span class="type">Double</span>,</span><br><span class="line">      seed: <span class="type">Long</span> = <span class="type">Utils</span>.random.nextLong): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p><strong>aggregateByKey</strong></p><ul><li><code>aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])</code>            //按照key进行聚合 </li><li>其实<code>reduceBykey</code> 就是<code>aggregateByKey</code>的简化版。  </li><li><code>aggregateByKey</code>多提供了一个函数 seqOp 类似于Mapreduce的combine操作（就在map端执行reduce的操作）</li></ul></li></ul><h4 id="mapPartitionsWithIndex-和-repartitionAndSortWithinPartitions"><a href="#mapPartitionsWithIndex-和-repartitionAndSortWithinPartitions" class="headerlink" title="mapPartitionsWithIndex 和 repartitionAndSortWithinPartitions"></a>mapPartitionsWithIndex 和 repartitionAndSortWithinPartitions</h4><ul><li><strong>mapPartitionsWithIndex</strong> <ul><li>说白了就是可以打印出当前所在分区数</li></ul></li><li><strong>repartitionAndSortWithinPartitions</strong><ul><li><strong>该方法依据partitioner对RDD进行分区，并且在每个结果分区中按key进行排序；通过对比sortByKey发现，这种方式比先分区，然后在每个分区中进行排序效率高，这是因为它可以将排序融入到shuffle阶段。</strong> </li></ul></li></ul><h2 id="2-Action-算子"><a href="#2-Action-算子" class="headerlink" title="2. Action 算子"></a>2. Action 算子</h2><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/java/ActionOperations.java" target="_blank" rel="noopener"><strong>Java 代码见这里</strong></a></p><h4 id="reduce"><a href="#reduce" class="headerlink" title="reduce();"></a>reduce();</h4><ul><li>def reduce(f: JFunction2[T, T, T]): T = rdd.reduce(f)</li></ul><h4 id="collect"><a href="#collect" class="headerlink" title="collect();"></a>collect();</h4><ul><li>Return an array that contains all of the elements in this RDD.</li><li>this method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver’s memory.</li></ul><h4 id="take-n"><a href="#take-n" class="headerlink" title="take(n);"></a>take(n);</h4><ul><li>Take the first num elements of the RDD. This currently scans the partitions <em>one by one</em>, so it will be slow if a lot of partitions are required. In that case, use collect() to get the  whole RDD instead.</li><li>this method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver’s memory.</li></ul><h4 id="count"><a href="#count" class="headerlink" title="count();"></a>count();</h4><ul><li>Return the number of elements in the RDD.</li></ul><h4 id="takeOrdered"><a href="#takeOrdered" class="headerlink" title="takeOrdered();"></a>takeOrdered();</h4><ul><li>Returns the <strong>first k (smallest)</strong> elements from this RDD using the  <strong>natural ordering</strong> for T while maintain the order.</li><li>top(n): 自然排序后,最大的前 n</li></ul><h4 id="saveAsTextFile"><a href="#saveAsTextFile" class="headerlink" title="saveAsTextFile();"></a>saveAsTextFile();</h4><ul><li>Save this RDD as a text file, using string representations of elements.</li></ul><h4 id="countByKey"><a href="#countByKey" class="headerlink" title="countByKey();"></a>countByKey();</h4><ul><li>return  map&lt;String, Integer&gt;, key 为 key, value 为 key 的数量</li></ul><h4 id="takeSample"><a href="#takeSample" class="headerlink" title="takeSample();"></a>takeSample();</h4><ul><li>withReplacement：元素可以多次(重复)抽样(在抽样时替换) 如果为 false, 在抽样数 &gt; 样本数时, 只能返回样本数的样本</li><li>num：返回的样本的大小</li><li>seed：随机数生成器的种子</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeSample</span></span>(</span><br><span class="line">    withReplacement: <span class="type">Boolean</span>,</span><br><span class="line">    num: <span class="type">Int</span>,</span><br><span class="line">    seed: <span class="type">Long</span> = <span class="type">Utils</span>.random.nextLong): <span class="type">Array</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure><hr><h1 id="七-RDD-的持久化"><a href="#七-RDD-的持久化" class="headerlink" title="七. RDD 的持久化"></a>七. RDD 的持久化</h1><p>将数据通过操作持久化（或缓存）在内存中是Spark的重要能力之一。当你缓存了一个RDD，每个节点都缓存了RDD的所有分区。这样就可以在内存中进行计算。这样可以使以后在RDD上的动作更快（通常可以提高10倍）。</p><p>你可以对希望缓存的RDD通过使用persist或cache方法进行标记。它通过动作操作第一次在RDD上进行计算后，它就会被缓存在节点上的内存中。Spark的缓存具有容错性，如果RDD的某一分区丢失，它会自动使用最初创建RDD时的转换操作进行重新计算。</p><p>另外，RDD可以被持久化成不同的级别。比如，可以允许你存储在磁盘，内存，甚至是序列化的<strong>Java</strong>对象（节省空间），备份在不同的节点上，或者存储在基于内存的文件系统Tachyon上。通过向persist()方法传递StorageLevel对象来设置。cache方法是使用默认级别<code>StorageLevel.MEMORY_ONLY</code>的方法。</p><h4 id="选持久化方案建议："><a href="#选持久化方案建议：" class="headerlink" title="选持久化方案建议："></a><strong>选持久化方案建议：</strong></h4><ol><li>优先选择MEMORY_ONLY，如果可以用内存缓存所有的数据，那么也就意味着我的计算是纯内存的计算，速度当然快。 </li><li>如果MEMORY_ONLY 缓存不了所有的数据，MEMORY_ONLY_SER 把数据实现序列化然后进行存储。这样也是纯内存操作，速度也快，只不过需要耗费一点cpu资源需要反序列化。 </li><li>也可以选用带_2这种方式, 此方式会存2份, 一份存在本地, 另一份会存到另外的节点。恢复速度的时候可以使用备份。</li><li>能不能使用DISK的，就不使用DISK，有时候从磁盘读，还不如从新计算一次。 </li></ol><h4 id="关于tachyon"><a href="#关于tachyon" class="headerlink" title="关于tachyon "></a>关于<strong><a href="http://www.alluxio.org/" target="_blank" rel="noopener">tachyon</a> </strong></h4><p>Spark2.0开始就不把tachyon(现在成为alluxio)集成在自身内部了, 依然可以直接用</p><p><strong>基于内存的分布式文件系统</strong> </p><p><strong>出现原因</strong>:</p><ul><li>spark运行以 JVM为基础，所以spark的任务会把数据存入JVM的堆中，随着计算的迭代，JVM堆中存放的数据量迅速增大，对于spark而言，spark的计算引擎和存储引擎处在同一个JVM中，所以会有重复的GC方面的开销。这样就增大了系统的延时。</li><li>当JVM崩溃时，缓存在JVM堆中的数据也会消失，这个时候spark不得不根据RDD的血缘关系重新计算数据。</li><li>如果spark需要其他的框架的共享数据，比如就是hadoop的Mapreduce，这个时候就必须通过第三方来共享，比如借助HDFS，那么这样的话，就需要额外的开销，借助的是HDFS，那么就需要磁盘IO的开销。</li><li>因为我们基于内存的分布式计算框架有以上的问题，那么就促使了内存分布式文件系统的诞生，比如tachyon。</li></ul><p><strong>Tachyon可以解决spark的什么问题呢？</strong></p><p>如果我们把数据存放到tachyon上面：</p><ul><li><strong>减少Spark GC的开销。</strong> </li><li><strong>当spark 的JVM崩溃的时候，存放在tachyon上的数据不受影响。</strong> </li><li><strong>spark如果要想跟被的计算工具共享数据，只要通过tachyon的Client就可以做到了。并且延迟远低于HDFS等系统。</strong> </li></ul><hr><h1 id="八-广播变量-和-累加器"><a href="#八-广播变量-和-累加器" class="headerlink" title="八. 广播变量 和  累加器"></a>八. 广播变量 和  累加器</h1><h2 id="1-广播变量"><a href="#1-广播变量" class="headerlink" title="1. 广播变量"></a>1. 广播变量</h2><p>每个 executor 拥有一份， 这个 executor 启动的 task 会共享这个变量</p><p>使用了广播变量之后, executor 中所有的 task 都会共享此变量, 否则每个 task 都会发一份</p><p>在 Driver 端可以修改广播变量的值，在 Executor 端无法修改广播变量的值。</p><p><strong>使用:</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义</span></span><br><span class="line"><span class="keyword">val</span> a = <span class="number">3</span></span><br><span class="line"><span class="keyword">val</span> broadcast = sc.broadcast(a)</span><br><span class="line"><span class="comment">// 获取</span></span><br><span class="line"><span class="keyword">val</span> c = broadcast.value</span><br></pre></td></tr></table></figure><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-26-090621.png" alt="image-20180726170621192"></p><h2 id="2-累加器"><a href="#2-累加器" class="headerlink" title="2. 累加器"></a>2. 累加器</h2><p><strong>使用场景</strong>: 异常监控，调试，记录符合某特性的数据的数目等</p><p><strong>如果一个变量不被声明为一个累加器</strong>，那么它将在 被改变时不会在 driver 端进行全局汇总，即在<strong>分布式运行时每个 task 运行的只是原始变量的一个副本</strong>，并不能改变原始变量的值</p><p>但是当这个变量被声明为<strong>累加器</strong>后，<strong>该变量就会有分布式计数的功能。</strong></p><p>累加器<strong>在 Driver 端定义赋初始值</strong>，累加器只能在 <strong>Driver 端读取最后的值</strong>，在 <strong>Excutor 端更 新</strong>。</p><p><strong>使用:</strong> </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义</span></span><br><span class="line"><span class="keyword">val</span> a = sc.longAccumulator(<span class="number">0</span>)</span><br><span class="line"><span class="comment">// 获取</span></span><br><span class="line"><span class="keyword">val</span> b = a.value</span><br></pre></td></tr></table></figure><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-26-090907.png" alt="image-20180726170906622"></p><hr><h1 id="九-Spark-on-Yarn-模式"><a href="#九-Spark-on-Yarn-模式" class="headerlink" title="九. Spark on Yarn 模式"></a>九. Spark on Yarn 模式</h1><p><strong>配置</strong>: 只需要在<code>conf/spark-env.sh</code> 中配置<code>export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</code> 就可以了</p><p><strong>使用YARN模式的时候，不需要启动master和worker了。</strong> </p><p><strong>只需要启动HDFS和YARN即可。</strong> </p><p><strong>与 <code>Standalone</code>主要区别是</strong>: <code>spark-submit</code>后面的参数中<code>--master</code>后面的不是 spark://….,   而是 <code>yarn</code>,  这样:<code>--master yarn</code></p><h4 id="–deploy-mode-client"><a href="#–deploy-mode-client" class="headerlink" title="–deploy-mode client"></a>–deploy-mode client</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-26-165721.png" alt="image-20180727005721062"></p><h4 id="–deploy-mode-cluster"><a href="#–deploy-mode-cluster" class="headerlink" title="–deploy-mode cluster"></a>–deploy-mode cluster</h4><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-26-165858.png" alt="image-20180727005857420"></p><h1 id="十-宽窄依赖"><a href="#十-宽窄依赖" class="headerlink" title="十. 宽窄依赖"></a>十. 宽窄依赖</h1><p><strong>窄依赖是指父RDD的每个分区都只被子RDD一个分区使用。</strong>(独生, NarrowDependency)</p><p><strong>宽依赖就是指父RDD的分区被多个子RDD的分区所依赖。</strong> (超生, ShuffleDependency)</p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-26-170442.png" alt="image-20180727010441959"></p><h1 id="十一-Stage-划分"><a href="#十一-Stage-划分" class="headerlink" title="十一. Stage 划分"></a>十一. Stage 划分</h1><p>开发完一个应用以后，把应用提交到集群，那么这个应用就叫做Application</p><p>这个应用里面我们开发了好多代码，这些代码里面凡是遇到一个action操作，就会产生一个job任务。</p><p>也就意味着，一个Application有一个或者一个以上的job任务。</p><p>然后这些job任务划分为不同stage去执行，stage里面就是运行不同的task任务。</p><p>遇到一个 shuffle 算子, 就会从中间分开, 划分为2个 stage</p><p>Task计算的就是分区上面的数据。</p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-26-172557.png" alt="image-20180727012556575"></p><h1 id="十二-Spark-任务调度"><a href="#十二-Spark-任务调度" class="headerlink" title="十二. Spark 任务调度"></a><strong>十二. Spark 任务调度</strong></h1><p><strong>Shuffle 机制见下一篇</strong></p><h2 id="1-简版"><a href="#1-简版" class="headerlink" title="1.简版"></a>1.简版</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-26-173025.png" alt="image-20180727013025763"></p><h2 id="2-完整版"><a href="#2-完整版" class="headerlink" title="2.完整版"></a>2.完整版</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-26-spark%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%E5%9B%BE%E8%A7%A3.png" alt=""></p><p><strong>注意:</strong> </p><ul><li>AppClient / clientActor：在 Standalone 模式下的实现是 <code>StandaloneAppClient</code> 类</li><li>dirverActor:  :TODO</li></ul><h1 id="十三-TopN-案例"><a href="#十三-TopN-案例" class="headerlink" title="十三. TopN 案例"></a>十三. TopN 案例</h1><p><strong>对一个文件里面的单词进行单词计数，然后取前3个出现次数最多的三个单词。</strong> </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TopN</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">     <span class="keyword">val</span> conf=<span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"TopN"</span>)</span><br><span class="line">     <span class="keyword">val</span> sc=<span class="keyword">new</span> <span class="type">SparkContext</span>(conf);</span><br><span class="line">     <span class="keyword">val</span> file=sc.textFile(<span class="string">"file://..."</span>)</span><br><span class="line">     <span class="keyword">val</span> topN=file.flatMap &#123; line =&gt; line.split(<span class="string">"\t"</span>) &#125;</span><br><span class="line">     .map &#123; word =&gt; (word,<span class="number">1</span>) &#125;</span><br><span class="line">     .reduceByKey(_+_)  <span class="comment">//key word  value:count</span></span><br><span class="line">     .map(tuple =&gt;(tuple._2,tuple._1))<span class="comment">// swap k,v</span></span><br><span class="line">     .sortByKey(<span class="literal">false</span>)   <span class="comment">// sort the count</span></span><br><span class="line">     .take(<span class="number">3</span>)  <span class="comment">// take top 3</span></span><br><span class="line">     <span class="keyword">for</span>( i &lt;- topN)&#123;</span><br><span class="line">       println(i._2 + <span class="string">"  出现次数："</span>+i._1);</span><br><span class="line">     &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="十四-网站访问日志分析"><a href="#十四-网站访问日志分析" class="headerlink" title="十四. 网站访问日志分析"></a>十四. 网站访问日志分析</h1><h3 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h3><p><strong>需求一 ：</strong></p><p>The average, min, and max content size of responses returned from the server.</p><p><strong>需求二： </strong></p><p>A count of response code’s returned.</p><p><strong>需求三： </strong></p><p>All IPAddresses that have accessed this server more than N times.</p><p><strong>需求四：</strong></p><p>The top endpoints requested by count.  TopN 找出被访问次数最多的地址的前三个</p><h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">log.txt</span><br><span class="line">------------</span><br><span class="line">10.0.0.153<span class="comment">#-#-#[12/Mar/2004:12:23:18-0800]#"GET /cgi-bin/mailgraph.cgi/mailgraph_3_err.png HTTP/1.1"#200#5554</span></span><br><span class="line">10.0.0.153<span class="comment">#-#-#[12/Mar/2004:12:23:40-0800]#"GET /dccstats/index.html HTTP/1.1"#304#2000</span></span><br><span class="line">10.0.0.153<span class="comment">#-#-#[12/Mar/2004:12:23:41-0800]#"GET /dccstats/stats-spam.1day.png HTTP/1.1"#200#2964</span></span><br></pre></td></tr></table></figure><h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ApacheAccesslog</span></span></span><br><span class="line"><span class="class"><span class="title">-----------------------</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">ApacheAccesslog</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    ipAddress:<span class="type">String</span>, // ip地址</span></span></span><br><span class="line"><span class="class"><span class="params">    clientIndentd:<span class="type">String</span>, //标识符</span></span></span><br><span class="line"><span class="class"><span class="params">    userId:<span class="type">String</span> ,//用户<span class="type">ID</span></span></span></span><br><span class="line"><span class="class"><span class="params">    dateTime:<span class="type">String</span> ,//时间</span></span></span><br><span class="line"><span class="class"><span class="params">    method:<span class="type">String</span> ,//请求方式</span></span></span><br><span class="line"><span class="class"><span class="params">    endPoint:<span class="type">String</span> ,//目标地址</span></span></span><br><span class="line"><span class="class"><span class="params">    protocol:<span class="type">String</span> ,//协议</span></span></span><br><span class="line"><span class="class"><span class="params">    responseCode:<span class="type">Int</span> ,//网页请求响应类型</span></span></span><br><span class="line"><span class="class"><span class="params">    contenSize:<span class="type">Long</span>   //内容长度</span></span></span><br><span class="line"><span class="class"><span class="params">    </span></span></span><br><span class="line"><span class="class"><span class="params">    </span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">ApacheAccesslog</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">parseLog</span></span>(log:<span class="type">String</span>):<span class="type">ApacheAccesslog</span>=&#123;</span><br><span class="line">  <span class="keyword">val</span> logArray= log.split(<span class="string">"#"</span>);</span><br><span class="line">  <span class="keyword">val</span> url=logArray(<span class="number">4</span>).split(<span class="string">" "</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="type">ApacheAccesslog</span>(logArray(<span class="number">0</span>),logArray(<span class="number">1</span>),logArray(<span class="number">2</span>),logArray(<span class="number">3</span>),url(<span class="number">0</span>),url(<span class="number">1</span>),url(<span class="number">2</span>),logArray(<span class="number">5</span>).toInt,logArray(<span class="number">6</span>).toLong);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogAnalyer</span></span></span><br><span class="line"><span class="class"><span class="title">--------------------</span></span></span><br><span class="line"><span class="class"><span class="title">import</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">SparkConf</span></span></span><br><span class="line"><span class="class"><span class="title">import</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">SparkContext</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">LogAnalyer</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> conf=<span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"LogAnalyer"</span>)</span><br><span class="line">     <span class="keyword">val</span> sc=<span class="keyword">new</span> <span class="type">SparkContext</span>(conf);</span><br><span class="line">     <span class="keyword">val</span> logsRDD=sc.textFile(<span class="string">"file://...."</span>)</span><br><span class="line">          .map &#123; line =&gt; <span class="type">ApacheAccesslog</span>.parseLog(line) &#125;</span><br><span class="line">          .cache()</span><br><span class="line">     <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 需求一 </span></span><br><span class="line"><span class="comment">      * The average, min, and max content size of responses returned from the server.</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">     <span class="keyword">val</span> contextSize=logsRDD.map &#123; log =&gt; log.contenSize &#125;</span><br><span class="line">        <span class="comment">// get max</span></span><br><span class="line">      <span class="keyword">val</span> maxSize=contextSize.max()</span><br><span class="line">       <span class="comment">//get min</span></span><br><span class="line">      <span class="keyword">val</span> minSize=contextSize.min()</span><br><span class="line">      <span class="comment">// total / count</span></span><br><span class="line">      <span class="comment">//get average</span></span><br><span class="line">      <span class="keyword">val</span> averageSize=contextSize.reduce(_+_)/contextSize.count()</span><br><span class="line">      println(<span class="string">"=============================需求一-=============================="</span>);</span><br><span class="line">      println(<span class="string">"最大值："</span>+maxSize  + <span class="string">"  最小值："</span>+minSize + <span class="string">"   平均值："</span>+averageSize);</span><br><span class="line">     <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 需求二</span></span><br><span class="line"><span class="comment">      * A count of response code's returned.</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">        println(<span class="string">"=============================需求二-=============================="</span>);</span><br><span class="line">     logsRDD.map &#123; log =&gt; (log.responseCode,<span class="number">1</span>) &#125;</span><br><span class="line">     .reduceByKey(_+_)</span><br><span class="line">     .foreach(result =&gt; println(<span class="string">" 响应状态："</span>+result._1 + <span class="string">"  出现的次数："</span>+result._2))</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 需求三</span></span><br><span class="line"><span class="comment">      * All IPAddresses that have accessed this server more than N times.</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">      println(<span class="string">"=============================需求三-=============================="</span>);</span><br><span class="line">     <span class="keyword">val</span> result= logsRDD.map &#123; log =&gt;( log.ipAddress,<span class="number">1</span>) &#125;</span><br><span class="line">     .reduceByKey(_+_)</span><br><span class="line">     .filter(result =&gt; result._2 &gt; <span class="number">1</span>)   <span class="comment">// &gt; 10000 </span></span><br><span class="line">     .take(<span class="number">2</span>)  <span class="comment">//  &gt;  10</span></span><br><span class="line">     <span class="keyword">for</span>( tuple &lt;- result)&#123;</span><br><span class="line">       println(<span class="string">"ip : "</span>+tuple._1 + <span class="string">"  出现的次数："</span>+tuple._2);</span><br><span class="line">     &#125;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 需求四</span></span><br><span class="line"><span class="comment">      * The top endpoints requested by count.  TopN</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">      println(<span class="string">"=============================需求四-=============================="</span>);</span><br><span class="line">     <span class="keyword">val</span> topN=logsRDD.map &#123; log =&gt; (log.endPoint,<span class="number">1</span>) &#125;</span><br><span class="line">     .reduceByKey(_+_)</span><br><span class="line">     .map(result =&gt; (result._2,result._1))</span><br><span class="line">     .sortByKey(<span class="literal">false</span>)</span><br><span class="line">     .take(<span class="number">2</span>)</span><br><span class="line">     <span class="keyword">for</span>(tuple &lt;- topN)&#123;</span><br><span class="line">        println(<span class="string">"目标地址 : "</span>+tuple._2 + <span class="string">"  出现的次数："</span>+tuple._1);</span><br><span class="line">     &#125;</span><br><span class="line">     logsRDD.unpersist(<span class="literal">true</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一-Spark-与-MapReduce-区别&quot;&gt;&lt;a href=&quot;#一-Spark-与-MapReduce-区别&quot; class=&quot;headerlink&quot; title=&quot;一. Spark 与 MapReduce 区别&quot;&gt;&lt;/a&gt;一. Spark 与 MapReduc
      
    
    </summary>
    
      <category term="Spark" scheme="https://airpoet.github.io/categories/Spark/"/>
    
      <category term="精讲" scheme="https://airpoet.github.io/categories/Spark/%E7%B2%BE%E8%AE%B2/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Spark" scheme="https://airpoet.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>i-Spark-5</title>
    <link href="https://airpoet.github.io/2018/07/25/Spark/i-Spark-5/"/>
    <id>https://airpoet.github.io/2018/07/25/Spark/i-Spark-5/</id>
    <published>2018-07-25T03:36:06.050Z</published>
    <updated>2018-07-25T06:59:41.421Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一-Spark-的闭包处理"><a href="#一-Spark-的闭包处理" class="headerlink" title="一. Spark 的闭包处理"></a>一. Spark 的闭包处理</h1><p><strong>RDD</strong>, <code>resilient distributed dataset</code>,弹性(容错)分布式数据集。</p><p>分区列表, function,dep Option(分区类, Pair[Key,Value]),首选位置。</p><p><strong>运行job时，spark将rdd打碎变换成task</strong>,<strong>每个task由一个executor执行</strong>。</p><p>执行之前，<strong>spark会进行task的闭包(closure)计算。</strong></p><p>闭包是指针对executor可见的变量和方法,以备在rdd的foreach中进行计算。</p><p><strong>闭包就是串行化后并发送给每个executor.</strong></p><p><strong>local模式下</strong>，所有spark程序运行在同一JVM中，共享对象，<strong>counter是可以累加的</strong>。</p><p>原因是所有executor指向的是同一个引用。</p><p><strong>cluster模式下，不可以，counter是闭包处理的。</strong></p><p>每个节点对driver上的counter是不可见的。</p><p>只能看到自己内部串行化的counter副本。</p><h1 id="二-Spark的应用的-部署模式"><a href="#二-Spark的应用的-部署模式" class="headerlink" title="二. Spark的应用的 部署模式"></a>二. Spark的应用的 部署模式</h1><h2 id="1-部署模式概述"><a href="#1-部署模式概述" class="headerlink" title="1. 部署模式概述"></a>1. 部署模式概述</h2><p><code>spark-submit --class xxx xx.jar --deploy-mode (client | cluster)</code></p><p>  <strong><code>--deploy-mode</code>指定部署driver程序在client主机上还是在worker节点上。</strong> </p><p><strong>[client]</strong></p><ul><li>driver运行在client主机上。</li><li>client可以不在cluster(集群)中。 </li></ul><p><strong>[cluster]</strong></p><ul><li><code>driver</code>程序提交给<code>spark cluster</code>的某个<code>worker</code>节点来执行。</li><li>worker是cluster中的一员。</li><li>导出的jar需要放置到所有worker节点都可见的位置(如<code>hdfs</code>)才可以。</li></ul><p><strong>不论哪种方式，rdd的运算都在worker执行</strong></p><h2 id="2-deploy-mode部署模式验证"><a href="#2-deploy-mode部署模式验证" class="headerlink" title="2. deploy mode部署模式验证"></a>2. deploy mode部署模式验证</h2><h4 id="部署模式划分"><a href="#部署模式划分" class="headerlink" title="部署模式划分"></a>部署模式划分</h4><ul><li><strong>Client(客户端模式)</strong><ul><li><code>spark-submit --class com.rox.spark.scala.DeployModeTest --master [spark://cs1:7077](spark://s201:7077) --deploy-mode client SparkDemo-1-deploymode.jar</code></li><li>driver就是自己, 如果在cs1上, driver 就是cs1</li></ul></li><li><strong>Cluster(集群模式)</strong><ul><li><code>spark-submit --class com.it18zhang.spark.scala.DeployModeTest --master [spark://cs1:7077](spark://s201:7077) --deploy-mode cluster [hdfs://cs1:8020/user/centos/SparkDemo-1-deploymode.jar](hdfs://s201:8020/user/centos/SparkDemo1-1.0-SNAPSHOT.jar)</code></li><li>driver是由 spark 挑选的,如果是 cs1上提交的,因为 cs1也不是 worker(slave), 所以 driver和 worker 都跟cs1无关, 注意, cluster 模式,只要一提交完就结束了, 因为接下来跟他自己没关系了 , 所以此时在 cs1上执行的非常快</li><li>必须要将jar放置到所有worker都能够看到的地方才可以，例如hdfs。</li></ul></li></ul><h4 id="代码解释"><a href="#代码解释" class="headerlink" title="代码解释"></a><strong>代码解释</strong></h4><ul><li>通过Socket把打印的数据, 发送到指定机器<code>cs5</code>上</li><li>在<code>cs5</code>上, 开启 <code>nc</code>端口, 监听消息</li><li><code>standalone</code> 和 <code>yarn</code>模式分别打成 <code>jar包</code>,  在spark 环境中执行</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.spark.scala</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.net.&#123;<span class="type">InetAddress</span>, <span class="type">Socket</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DeployModeTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * write写出去</span></span><br><span class="line"><span class="comment">    * 从运行 此程序的节点, 写到 cs5 上</span></span><br><span class="line"><span class="comment">    * @param str</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">printInfo</span></span>(str:<span class="type">String</span>):<span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ip = <span class="type">InetAddress</span>.getLocalHost.getHostAddress</span><br><span class="line">    <span class="keyword">val</span> sock = <span class="keyword">new</span> <span class="type">Socket</span>(<span class="string">"cs5"</span>,<span class="number">8888</span>)</span><br><span class="line">    <span class="keyword">val</span> out = sock.getOutputStream</span><br><span class="line">    out.write((ip + <span class="string">""</span> + str + <span class="string">"\r\n"</span>).getBytes())</span><br><span class="line">    out.flush()</span><br><span class="line">    sock.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">"DeployModeTest"</span>)</span><br><span class="line">    <span class="comment">// standalone 模式</span></span><br><span class="line">    <span class="comment">//conf.setMaster("spark://cs1:7077")</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Yarn模式</span></span><br><span class="line">    conf.setMaster(<span class="string">"yarn"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// local 模式</span></span><br><span class="line">    <span class="comment">// conf.setMaster("local[4]")</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    printInfo(<span class="string">"hello guys---我就是Driver"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * Distribute a local Scala collection to form an RDD.</span></span><br><span class="line"><span class="comment">      * numSlices: the partition number of the new RDD.</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>,  <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印第一次map</span></span><br><span class="line">    <span class="keyword">val</span> rdd2 = rdd1.map(e =&gt; &#123;</span><br><span class="line">      printInfo(<span class="string">"直接定义3个分区, map1: "</span>+e)</span><br><span class="line">      e * <span class="number">2</span></span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 重新分区为 2</span></span><br><span class="line">    <span class="keyword">val</span> rdd3 = rdd2.repartition(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="keyword">val</span> rdd4 = rdd3.map(e =&gt; &#123;</span><br><span class="line">      printInfo(<span class="string">"重分区为2后, map2: "</span> + e)</span><br><span class="line">      e</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> res = rdd4.reduce((a,b)=&gt;&#123;</span><br><span class="line">      printInfo(<span class="string">"求和, reduce: "</span> + a + <span class="string">","</span> + b)</span><br><span class="line">      a + b</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    println(res)</span><br><span class="line">    printInfo(<span class="string">"最后发给driver: "</span> + res)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="三-Spark-集群的运行方式"><a href="#三-Spark-集群的运行方式" class="headerlink" title="三. Spark 集群的运行方式"></a>三. Spark 集群的运行方式</h1><p><strong>主要是<code>cluster manager</code>的区别。</strong> </p><p><strong>[local]</strong> </p><ul><li>…</li></ul><p><strong>[standalone]</strong> </p><ul><li>使用SparkMaster进程作为管理节点, 需要开启Spark 集群</li></ul><p><strong>[mesos]</strong> </p><ul><li>使用mesos的master作为管理节点。 </li></ul><p><strong>[yarn]</strong> </p><ul><li><p>使用hadoop的ResourceManager作为master节点。</p></li><li><p>不用开启 spark 集群。因为是依托<code>yarn 集群</code>来执行<code>spark</code> application,  高可用也是依托于 yarn 的高可用, 程序在执行时, 会拷贝 一些spark 依赖环境到 hdfs 上, 事后会删除</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一-Spark-的闭包处理&quot;&gt;&lt;a href=&quot;#一-Spark-的闭包处理&quot; class=&quot;headerlink&quot; title=&quot;一. Spark 的闭包处理&quot;&gt;&lt;/a&gt;一. Spark 的闭包处理&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;RDD&lt;/strong&gt;, &lt;c
      
    
    </summary>
    
      <category term="Spark" scheme="https://airpoet.github.io/categories/Spark/"/>
    
      <category term="部署模式" scheme="https://airpoet.github.io/categories/Spark/%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Spark" scheme="https://airpoet.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Python &amp; Hadoop | Spark 生态</title>
    <link href="https://airpoet.github.io/2018/07/23/Spark/Python%20&amp;%20Hadoop%7CSpark%E7%94%9F%E6%80%81/"/>
    <id>https://airpoet.github.io/2018/07/23/Spark/Python &amp; Hadoop|Spark生态/</id>
    <published>2018-07-23T06:27:10.923Z</published>
    <updated>2018-07-23T08:06:55.514Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一-Python-访问-MySQL"><a href="#一-Python-访问-MySQL" class="headerlink" title="一. Python 访问 MySQL"></a>一. Python 访问 MySQL</h1><h2 id="1-安装-pymysql-模块"><a href="#1-安装-pymysql-模块" class="headerlink" title="1.安装 pymysql 模块"></a>1.安装 pymysql 模块</h2><p>1) <code>idea</code>中,  <code>import pymysql</code>, 没有安装的话, <code>option + return</code> 安装</p><h2 id="2-访问-mysql-测试"><a href="#2-访问-mysql-测试" class="headerlink" title="2. 访问 mysql 测试"></a>2. 访问 mysql 测试</h2><p>看看能否打印 <code>mysql</code> 的版本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python3</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8-*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="comment"># 开启链接</span></span><br><span class="line">    conn = pymysql.connect(host=<span class="string">'localhost'</span>,user=<span class="string">'root'</span>,passwd=<span class="string">'123'</span>,</span><br><span class="line">                           db=<span class="string">'python'</span>,port=<span class="number">3306</span>,charset=<span class="string">'utf8'</span>)</span><br><span class="line">    <span class="comment"># 打开游标</span></span><br><span class="line">    cur = conn.cursor()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 执行 sql</span></span><br><span class="line">    cur.execute(<span class="string">'select version()'</span>)</span><br><span class="line"></span><br><span class="line">    version = cur.fetchone()</span><br><span class="line"></span><br><span class="line">    print(version)</span><br><span class="line">    cur.close()</span><br><span class="line">    conn.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">except</span> Exception:</span><br><span class="line">    print(<span class="string">"发生异常"</span>)</span><br></pre></td></tr></table></figure><h2 id="3-查询-mysql"><a href="#3-查询-mysql" class="headerlink" title="3. 查询 mysql"></a>3. 查询 mysql</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python3</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8-*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="comment"># 开启链接</span></span><br><span class="line">    conn = pymysql.connect(host=<span class="string">'localhost'</span>,user=<span class="string">'root'</span>,passwd=<span class="string">'123'</span>,</span><br><span class="line">                           db=<span class="string">'python'</span>,port=<span class="number">3306</span>,charset=<span class="string">'utf8'</span>)</span><br><span class="line">    <span class="comment"># 打开游标</span></span><br><span class="line">    cur = conn.cursor()</span><br><span class="line"></span><br><span class="line">    sql = <span class="string">"select id, name,age from t1 where name like 'tom8%'"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 执行 sql</span></span><br><span class="line">    cur.execute(sql)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 取结果</span></span><br><span class="line">    all = cur.fetchall()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> rec <span class="keyword">in</span> all:</span><br><span class="line">        print(rec)</span><br><span class="line">        <span class="comment"># print(str(rec[0]))</span></span><br><span class="line"></span><br><span class="line">    conn.commit()</span><br><span class="line">    cur.close()</span><br><span class="line">    conn.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">except</span> Exception:</span><br><span class="line">    print(<span class="string">"发生异常"</span>)</span><br></pre></td></tr></table></figure><h2 id="4-大批量插入-mysql"><a href="#4-大批量插入-mysql" class="headerlink" title="4. 大批量插入 mysql"></a>4. 大批量插入 mysql</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python3</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8-*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="comment"># 开启链接</span></span><br><span class="line">    conn = pymysql.connect(host=<span class="string">'localhost'</span>,user=<span class="string">'root'</span>,passwd=<span class="string">'123'</span>,</span><br><span class="line">                           db=<span class="string">'python'</span>,port=<span class="number">3306</span>,charset=<span class="string">'utf8'</span>)</span><br><span class="line">    <span class="comment"># 打开游标</span></span><br><span class="line">    cur = conn.cursor()</span><br><span class="line"></span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; <span class="number">10000</span>:</span><br><span class="line">        sql = <span class="string">"insert into t1(name,age) VALUES ('%s','%d')"</span> % (<span class="string">'tom'</span> + str(i), i % <span class="number">100</span>)</span><br><span class="line">        print(sql)</span><br><span class="line">        cur.execute(sql)</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    conn.commit()</span><br><span class="line">    cur.close()</span><br><span class="line">    conn.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">except</span> Exception:</span><br><span class="line">    print(<span class="string">"发生异常"</span>)</span><br></pre></td></tr></table></figure><h2 id="5-执行事务"><a href="#5-执行事务" class="headerlink" title="5. 执行事务"></a>5. 执行事务</h2><p>关闭 autocommit</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python3</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8-*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="comment"># 开启连接</span></span><br><span class="line">    conn = pymysql.connect(host=<span class="string">'localhost'</span>,user=<span class="string">'root'</span>,passwd=<span class="string">'123'</span>,db=<span class="string">'python'</span>,port=<span class="number">3306</span>,charset=<span class="string">'utf8'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 关闭自动提交</span></span><br><span class="line">    conn.autocommit(<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#开启事务</span></span><br><span class="line">    conn.begin()</span><br><span class="line">    <span class="comment"># 打开游标</span></span><br><span class="line">    cur = conn.cursor()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 删除</span></span><br><span class="line">    sql = <span class="string">"delete from t1 WHERE id &gt; 20000"</span></span><br><span class="line">    <span class="comment"># 改</span></span><br><span class="line">    sql = <span class="string">"update t1 set age = age -1 where age &gt;=50 "</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 聚合</span></span><br><span class="line">    sql = <span class="string">"select count(*) from t1 where age &lt; 20"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 执行 sql</span></span><br><span class="line">    cur.execute(sql)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 有结果的, 执行完后需要fetch结果</span></span><br><span class="line">    res = cur.fetchone()</span><br><span class="line">    print(res[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提交连接</span></span><br><span class="line">    conn.commit()</span><br><span class="line">    <span class="comment"># 关闭游标</span></span><br><span class="line">    cur.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">except</span> Exception:</span><br><span class="line">    print(<span class="string">"发生异常"</span>)</span><br><span class="line">    conn.rollback()</span><br><span class="line"></span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    conn.close()</span><br></pre></td></tr></table></figure><h1 id="二-Spark-环境使用-Python-操作-HBase"><a href="#二-Spark-环境使用-Python-操作-HBase" class="headerlink" title="二. Spark 环境使用 Python 操作 HBase"></a>二. Spark 环境使用 Python 操作 HBase</h1><h2 id="1-环境准备"><a href="#1-环境准备" class="headerlink" title="1. 环境准备"></a>1. 环境准备</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">0.启动hbase集群</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line">如果时钟不同步。</span><br><span class="line">$&gt;su root</span><br><span class="line">$&gt;xcall.sh <span class="string">"ntpdate asia.pool.ntp.org"</span></span><br><span class="line">当然, 也可以设置脚本自动同步, 详细见我的 hadoopHA 环境搭建</span><br><span class="line"></span><br><span class="line">1.在 HBase 目录下,启动hbase的thriftserver，满足和第三方应用通信。</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line">$&gt;hbase-daemon.sh start thrift2</span><br><span class="line"></span><br><span class="line">2.查看WebUI</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line">/<span class="comment"># webui端口 , 9090 rpc端口</span></span><br><span class="line">http://cs1:9095/        </span><br><span class="line"></span><br><span class="line">3.安装mac下thrift的编译器</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line">brew install thrift</span><br><span class="line"></span><br><span class="line">4.下载并安装thrift的python模块.</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line">import thrift  ==&gt;没有的话, option + <span class="built_in">return</span> 安装</span><br><span class="line"></span><br><span class="line">5.找到hbase.thrift文件进行编译，产生python文件。</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line">使用以下命令进行编译</span><br><span class="line">cmd&gt; thrift  -o ./out -gen py hbase.thrift</span><br><span class="line">生成后的路径在这里: /Users/shixuanji/Documents/资源/Jar包/HBase/out/gen-py</span><br><span class="line"></span><br><span class="line">6.创建idea的下的新模块</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">7.创建python文件Demo1.py</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">8.复制生成python文件到idea下。</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line">mythrift/hbase/..</span><br><span class="line"></span><br><span class="line">9.控制台环境测试</span><br><span class="line">---------------------------------------------------------------------</span><br><span class="line">    移除spark/conf/core-site.xml | hdfs-site.xml | hive-site.xml文件</span><br><span class="line">    [scala] ~/apps/spark/bin</span><br><span class="line">    &lt;spark-shell&gt;</span><br><span class="line">    val rdd = sc.makeRDD(1 to 10)</span><br><span class="line">    rdd.map(e=&gt;(e,1))</span><br><span class="line"></span><br><span class="line">    [python] ~/apps/spark/bin</span><br><span class="line">    &lt;pyspark&gt;</span><br><span class="line">    arr = [1,2,3,4]</span><br><span class="line">    rdd = sc.parallelize(arr);</span><br><span class="line">    rdd.map(lambda e : (e,1))</span><br></pre></td></tr></table></figure><h2 id="2-具体代码"><a href="#2-具体代码" class="headerlink" title="2.具体代码"></a>2.具体代码</h2><h4 id="2-1-对-hbase-的增删改查"><a href="#2-1-对-hbase-的增删改查" class="headerlink" title="2.1 对 hbase 的增删改查"></a>2.1 对 hbase 的增删改查</h4><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/HBasePythonDemo/BasicPyHbase.py" target="_blank" rel="noopener">https://github.com/airpoet/bigdata/blob/master/Spark_Project/HBasePythonDemo/BasicPyHbase.py</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- encoding=utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment">#导入thrift的python模块</span></span><br><span class="line"><span class="keyword">from</span> thrift <span class="keyword">import</span> Thrift</span><br><span class="line"><span class="keyword">from</span> thrift.transport <span class="keyword">import</span> TSocket</span><br><span class="line"><span class="keyword">from</span> thrift.transport <span class="keyword">import</span> TTransport</span><br><span class="line"><span class="keyword">from</span> thrift.protocol <span class="keyword">import</span> TBinaryProtocol</span><br><span class="line"></span><br><span class="line"><span class="comment">#导入自已编译生成的hbase python模块</span></span><br><span class="line"><span class="keyword">from</span> mythrift.hbase <span class="keyword">import</span> THBaseService</span><br><span class="line"><span class="keyword">from</span> mythrift.hbase.ttypes <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> mythrift.hbase.ttypes <span class="keyword">import</span> TResult</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#创建Socket连接，到s201:9090</span></span><br><span class="line">transport = TSocket.TSocket(<span class="string">'cs1'</span>, <span class="number">9090</span>)</span><br><span class="line">transport = TTransport.TBufferedTransport(transport)</span><br><span class="line">protocol = TBinaryProtocol.TBinaryProtocol(transport)</span><br><span class="line">client = THBaseService.Client(protocol)</span><br><span class="line"></span><br><span class="line"><span class="comment">#打开传输端口!!!</span></span><br><span class="line">transport.open()</span><br><span class="line"></span><br><span class="line"><span class="comment"># put操作</span></span><br><span class="line">table = <span class="string">b'ns1:t1'</span></span><br><span class="line">row = <span class="string">b'row1'</span></span><br><span class="line">v1 = TColumnValue(<span class="string">b'f1'</span>, <span class="string">b'id'</span>, <span class="string">b'101'</span>)</span><br><span class="line">v2 = TColumnValue(<span class="string">b'f1'</span>, <span class="string">b'name'</span>, <span class="string">b'tomas'</span>)</span><br><span class="line">v3 = TColumnValue(<span class="string">b'f1'</span>, <span class="string">b'age'</span>, <span class="string">b'12'</span>)</span><br><span class="line">vals = [v1, v2, v3]</span><br><span class="line">put = TPut(row, vals)</span><br><span class="line">client.put(table, put)</span><br><span class="line">print(<span class="string">"okkkk!!"</span>)</span><br><span class="line">transport.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#get</span></span><br><span class="line">table = <span class="string">b'ns1:t1'</span></span><br><span class="line">rowkey=<span class="string">b"row1"</span></span><br><span class="line">col_id = TColumn(<span class="string">b"f1"</span>,<span class="string">b"id"</span>)</span><br><span class="line">col_name = TColumn(<span class="string">b"f1"</span>,<span class="string">b"name"</span>)</span><br><span class="line">col_age = TColumn(<span class="string">b"f1"</span>,<span class="string">b"age"</span>)</span><br><span class="line"></span><br><span class="line">cols = [col_id,col_name,col_age]</span><br><span class="line">get = TGet(rowkey,cols)</span><br><span class="line">res = client.get(table,get)</span><br><span class="line">print(bytes.decode(res.columnValues[<span class="number">0</span>].qualifier))</span><br><span class="line">print(bytes.decode(res.columnValues[<span class="number">0</span>].family))</span><br><span class="line">print(res.columnValues[<span class="number">0</span>].timestamp)</span><br><span class="line">print(bytes.decode(res.columnValues[<span class="number">0</span>].value))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#delete</span></span><br><span class="line">table = <span class="string">b'ns1:t1'</span></span><br><span class="line">rowkey = <span class="string">b"row1"</span></span><br><span class="line">col_id = TColumn(<span class="string">b"f1"</span>, <span class="string">b"id"</span>)</span><br><span class="line">col_name = TColumn(<span class="string">b"f1"</span>, <span class="string">b"name"</span>)</span><br><span class="line">col_age = TColumn(<span class="string">b"f1"</span>, <span class="string">b"age"</span>)</span><br><span class="line">cols = [col_id, col_name]</span><br><span class="line"></span><br><span class="line"><span class="comment">#构造删除对象</span></span><br><span class="line">delete = TDelete(rowkey,cols)</span><br><span class="line">res = client.deleteSingle(table, delete)</span><br><span class="line">transport.close()</span><br><span class="line">print(<span class="string">"ok"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Scan</span></span><br><span class="line">table = <span class="string">b'ns1:t12'</span></span><br><span class="line">startRow = <span class="string">b'1530357094900-43dwMLjxI5-0'</span></span><br><span class="line">stopRow = <span class="string">b'1530357183537-43dwMLjxI5-6'</span></span><br><span class="line">payload = TColumn(<span class="string">b"f1"</span>, <span class="string">b"payload"</span>)</span><br><span class="line"></span><br><span class="line">cols = [payload]</span><br><span class="line"></span><br><span class="line">scan = TScan(startRow=startRow,stopRow=stopRow,columns=cols)</span><br><span class="line"><span class="comment"># 这里如果不传 stopRow 就是扫描到结尾</span></span><br><span class="line">scan = TScan(startRow=startRow, columns=cols)</span><br><span class="line">r = client.getScannerResults(table,scan,<span class="number">100</span>);</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> r:</span><br><span class="line">    print(<span class="string">"============"</span>)</span><br><span class="line">    print(bytes.decode(x.columnValues[<span class="number">0</span>].qualifier))</span><br><span class="line">    print(bytes.decode(x.columnValues[<span class="number">0</span>].family))</span><br><span class="line">    print(x.columnValues[<span class="number">0</span>].timestamp)</span><br><span class="line">    print(bytes.decode(x.columnValues[<span class="number">0</span>].value))</span><br></pre></td></tr></table></figure><h4 id="2-2-将爬虫爬取的网页存入-hbase"><a href="#2-2-将爬虫爬取的网页存入-hbase" class="headerlink" title="2.2 将爬虫爬取的网页存入 hbase"></a>2.2 将爬虫爬取的网页存入 hbase</h4><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/HBasePythonDemo/Crawler2HBase.py" target="_blank" rel="noopener">https://github.com/airpoet/bigdata/blob/master/Spark_Project/HBasePythonDemo/Crawler2HBase.py</a></p><blockquote><p>   CrawerPageDao.py  </p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python3</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8-*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入thrift的python模块</span></span><br><span class="line"><span class="keyword">from</span> thrift <span class="keyword">import</span> Thrift</span><br><span class="line"><span class="keyword">from</span> thrift.transport <span class="keyword">import</span> TSocket</span><br><span class="line"><span class="keyword">from</span> thrift.transport <span class="keyword">import</span> TTransport</span><br><span class="line"><span class="keyword">from</span> thrift.protocol <span class="keyword">import</span> TBinaryProtocol</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入自已编译生成的hbase python模块</span></span><br><span class="line"><span class="keyword">from</span> mythrift.hbase <span class="keyword">import</span> THBaseService</span><br><span class="line"><span class="keyword">from</span> mythrift.hbase.ttypes <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> mythrift.hbase.ttypes <span class="keyword">import</span> TResult</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> base64</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">创建Socket连接，到s201:9090</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">transport = TSocket.TSocket(<span class="string">'cs1'</span>, <span class="number">9090</span>)</span><br><span class="line">transport = TTransport.TBufferedTransport(transport)</span><br><span class="line">protocol = TBinaryProtocol.TBinaryProtocol(transport)</span><br><span class="line">client = THBaseService.Client(protocol)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">定义函数，保存网页</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">savePage</span><span class="params">(url,page)</span>:</span></span><br><span class="line">    <span class="comment">#开启连接</span></span><br><span class="line">    transport.open()</span><br><span class="line">    <span class="comment">#对url进行base64编码，形成bytes,作为rowkey</span></span><br><span class="line">    urlBase64Bytes = base64.encodebytes(url.encode(<span class="string">"utf-8"</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># put操作</span></span><br><span class="line">    table = <span class="string">b'ns1:pages'</span></span><br><span class="line">    rowkey = urlBase64Bytes</span><br><span class="line">    v1 = TColumnValue(<span class="string">b'f1'</span>, <span class="string">b'page'</span>, page)</span><br><span class="line">    vals = [v1]</span><br><span class="line">    put = TPut(rowkey, vals)</span><br><span class="line">    client.put(table, put)</span><br><span class="line">    transport.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">判断网页是否存在</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exists</span><span class="params">(url)</span>:</span></span><br><span class="line">    transport.open()</span><br><span class="line">    <span class="comment"># 对url进行base64编码，形成bytes,作为rowkey</span></span><br><span class="line">    urlBase64Bytes = base64.encodebytes(url.encode(<span class="string">"utf-8"</span>))</span><br><span class="line">    print(urlBase64Bytes)</span><br><span class="line"></span><br><span class="line">    table = <span class="string">b'ns1:pages'</span></span><br><span class="line">    rowkey = urlBase64Bytes</span><br><span class="line">    col_page = TColumn(<span class="string">b"f1"</span>,<span class="string">b"page"</span>)</span><br><span class="line"></span><br><span class="line">    cols = [col_page]</span><br><span class="line">    get = TGet(rowkey,cols)</span><br><span class="line">    res = client.get(table, get)</span><br><span class="line">    transport.close()</span><br><span class="line">    <span class="keyword">return</span> res.row <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span></span><br></pre></td></tr></table></figure><blockquote><p>  Crawler2HBase.py  </p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python3</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8-*-</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 首先创建 hbase 表: pages</span></span><br><span class="line"><span class="comment"># $hbase&gt; create 'ns1:pages','f1'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> CrawerPageDao</span><br><span class="line"></span><br><span class="line"><span class="comment">#下载网页方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="comment">#判断当前的网页是否已经下载</span></span><br><span class="line">    resp = urllib.request.urlopen(url)</span><br><span class="line">    pageBytes = resp.read()</span><br><span class="line">    resp.close</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> CrawerPageDao.exists(url):</span><br><span class="line">        CrawerPageDao.savePage(url, pageBytes);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment">#解析网页的内容</span></span><br><span class="line">        pageStr = pageBytes.decode(<span class="string">"utf-8"</span>);</span><br><span class="line">        <span class="comment">#解析href地址</span></span><br><span class="line">        pattern = <span class="string">u'&lt;a[\u0000-\uffff&amp;&amp;&lt;sup&gt;[href]]*href="([\u0000-\uffff&amp;&amp;&lt;/sup&gt;"]*?)"'</span></span><br><span class="line">        res = re.finditer(pattern, pageStr)</span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> res:</span><br><span class="line">            addr = r.group(<span class="number">1</span>);</span><br><span class="line">            print(addr)</span><br><span class="line">            <span class="keyword">if</span> addr.startswith(<span class="string">"//"</span>):</span><br><span class="line">                addr = addr.replace(<span class="string">"//"</span>,<span class="string">"http://"</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">#判断网页中是否包含自己的地址</span></span><br><span class="line">            <span class="keyword">if</span> addr.startswith(<span class="string">"http://"</span>) <span class="keyword">and</span> url != addr <span class="keyword">and</span> (<span class="keyword">not</span> CrawerPageDao.exists(addr)):</span><br><span class="line">                download(addr) ;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        print(e)</span><br><span class="line">        print(pageBytes.decode(<span class="string">"gbk"</span>, errors=<span class="string">'ignore'</span>));</span><br><span class="line">        <span class="keyword">return</span> ;</span><br><span class="line"></span><br><span class="line">download(<span class="string">"http://jd.com"</span>);</span><br></pre></td></tr></table></figure><hr><h1 id="三-使用python实现spark的数据分析"><a href="#三-使用python实现spark的数据分析" class="headerlink" title="三. 使用python实现spark的数据分析"></a>三. 使用python实现spark的数据分析</h1><p><strong>参考这本书, data 等都有下载地址</strong> </p><blockquote><p>Apache Spark 2 for Beginners</p></blockquote><h2 id="1-环境准备-1"><a href="#1-环境准备-1" class="headerlink" title="1.环境准备"></a>1.环境准备</h2><p><strong>首先当前python环境必须安装了这些组件, 由于我的mac上已经装了, 这里就不再装了</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1.numpy</span><br><span class="line">    cmd&gt;pip install -i https://pypi.tuna.tsinghua.edu.cn/simple numpy</span><br><span class="line"></span><br><span class="line">2.scipy</span><br><span class="line">    pip install -i https://pypi.tuna.tsinghua.edu.cn/simple scipy</span><br><span class="line"></span><br><span class="line">3.matplotpy</span><br><span class="line">    pip install -i https://pypi.tuna.tsinghua.edu.cn/simple matplotlib</span><br><span class="line">    python -m pip install -U pip setuptools</span><br><span class="line">    python -m pip install matplotlib</span><br></pre></td></tr></table></figure><h2 id="2-在-mac-环境的-Spark-下"><a href="#2-在-mac-环境的-Spark-下" class="headerlink" title="2.在 mac 环境的 Spark 下"></a>2.在 mac 环境的 Spark 下</h2><p>也可以在 Linux 下的图形界面中通过 terminal 操作.</p><p><strong>目录建议不要有中文,  否则会有一些警告甚至错误</strong></p><p><strong>我的目录在这里:</strong> <code>/Users/shixuanji/Documents/资源/Jar包/Spark/spark-2.1.3-bin-hadoop2.7/bin/pyspark</code></p><p>进入我的 <code>iTerm2</code>, 进入<code>pyspark</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">#导入sql</span></span><br><span class="line">    <span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line">    <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">    <span class="keyword">import</span> pylab <span class="keyword">as</span> P</span><br><span class="line">    plt.rcdefaults()        <span class="comment"># Restore the rc params from Matplotlib's internal defaults</span></span><br><span class="line">    dataDir =<span class="string">"file:///home/centos/ml-data/ml-1m/users.dat"</span></span><br><span class="line">    lines = sc.textFile(dataDir)</span><br><span class="line">    splitLines = lines.map(<span class="keyword">lambda</span> l: l.split(<span class="string">"::"</span>))</span><br><span class="line">    usersRDD = splitLines.map(<span class="keyword">lambda</span> p: Row(id=p[<span class="number">0</span>],gender=p[<span class="number">1</span>],age=int(p[<span class="number">2</span>]), occupation=p[<span class="number">3</span>], zipcode=p[<span class="number">4</span>]))</span><br><span class="line">    usersDF = spark.createDataFrame(usersRDD)</span><br><span class="line">    usersDF.createOrReplaceTempView(<span class="string">"users"</span>)</span><br><span class="line">    usersDF.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#生成直方图</span></span><br><span class="line">    ageDF = spark.sql(<span class="string">"SELECT age FROM users"</span>)</span><br><span class="line">    ageList = ageDF.rdd.map(<span class="keyword">lambda</span> p: p.age).collect()</span><br><span class="line">    ageDF.describe().show()</span><br><span class="line"></span><br><span class="line">    plt.hist(ageList)</span><br><span class="line">    plt.title(<span class="string">"Age distribution of the users\n"</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"Age"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"Number of users"</span>)</span><br><span class="line">    plt.show(block=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#密度图</span></span><br><span class="line">    <span class="keyword">from</span> scipy.stats <span class="keyword">import</span> gaussian_kde</span><br><span class="line">    density = gaussian_kde(ageList)</span><br><span class="line">    xAxisValues = np.linspace(<span class="number">0</span>,<span class="number">100</span>,<span class="number">1000</span>)</span><br><span class="line">    density.covariance_factor = <span class="keyword">lambda</span> : <span class="number">.5</span></span><br><span class="line">    density._compute_covariance()</span><br><span class="line">    plt.title(<span class="string">"Age density plot of the users\n"</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"Age"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"Density"</span>)</span><br><span class="line">    plt.plot(xAxisValues, density(xAxisValues))</span><br><span class="line">    plt.show(block=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成嵌套子图</span></span><br><span class="line">    plt.subplot(<span class="number">121</span>)</span><br><span class="line">    plt.hist(ageList)</span><br><span class="line">    plt.title(<span class="string">"Age distribution of the users\n"</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"Age"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"Number of users"</span>)</span><br><span class="line">    plt.subplot(<span class="number">122</span>)</span><br><span class="line">    plt.title(<span class="string">"Summary of distribution\n"</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"Age"</span>)</span><br><span class="line">    plt.boxplot(ageList, vert=<span class="keyword">False</span>)</span><br><span class="line">    plt.show(block=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#柱状图</span></span><br><span class="line">    occ10 = spark.sql(<span class="string">"SELECT occupation, count(occupation) as usercount FROM users GROUP BY occupation ORDER BY usercount DESC LIMIT 10"</span>)</span><br><span class="line">    occ10.show()</span><br><span class="line"></span><br><span class="line">    occTuple = occ10.rdd.map(<span class="keyword">lambda</span> p:(p.occupation,p.usercount)).collect()</span><br><span class="line">    occList, countList = zip(*occTuple)</span><br><span class="line">    occList</span><br><span class="line"></span><br><span class="line">    y_pos = np.arange(len(occList))</span><br><span class="line">    plt.barh(y_pos, countList, align=<span class="string">'center'</span>, alpha=<span class="number">0.4</span>)</span><br><span class="line">    plt.yticks(y_pos, occList)</span><br><span class="line">    plt.xlabel(<span class="string">'Number of users'</span>)</span><br><span class="line">    plt.title(<span class="string">'Top 10 user types\n'</span>)</span><br><span class="line">    plt.gcf().subplots_adjust(left=<span class="number">0.15</span>)</span><br><span class="line">    plt.show(block=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#堆栈条形图</span></span><br><span class="line">    occGender = spark.sql(<span class="string">"SELECT occupation, gender FROM users"</span>)</span><br><span class="line">    occGender.show()</span><br><span class="line"></span><br><span class="line">    occCrossTab = occGender.stat.crosstab(<span class="string">"occupation"</span>,<span class="string">"gender"</span>)</span><br><span class="line">    occupationsCrossTuple = occCrossTab.rdd.map(<span class="keyword">lambda</span> p:(p.occupation_gender,p.M, p.F)).collect()</span><br><span class="line">    occList, mList, fList = zip(*occupationsCrossTuple)</span><br><span class="line">    N = len(occList)</span><br><span class="line">    ind = np.arange(N)</span><br><span class="line">    width = <span class="number">0.75</span></span><br><span class="line">    p1 = plt.bar(ind, mList, width, color=<span class="string">'r'</span>)</span><br><span class="line">    p2 = plt.bar(ind, fList, width, color=<span class="string">'y'</span>, bottom=mList)</span><br><span class="line">    plt.ylabel(<span class="string">'Count'</span>)</span><br><span class="line">    plt.title(<span class="string">'Gender distribution by occupation\n'</span>)</span><br><span class="line">    plt.xticks(ind + width/<span class="number">2.</span>, occList, rotation=<span class="number">90</span>)</span><br><span class="line">    plt.legend((p1[<span class="number">0</span>], p2[<span class="number">0</span>]), (<span class="string">'Male'</span>, <span class="string">'Female'</span>))</span><br><span class="line">    plt.gcf().subplots_adjust(bottom=<span class="number">0.25</span>)</span><br><span class="line">    plt.show(block=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#饼图</span></span><br><span class="line">    occupationsBottom10 = spark.sql(<span class="string">"SELECT occupation,count(occupation) as usercount FROM users GROUP BY occupation ORDER BY usercount LIMIT 10"</span>)</span><br><span class="line">    occupationsBottom10Tuple = occupationsBottom10.rdd.map(<span class="keyword">lambda</span> p:(p.occupation,p.usercount)).collect()</span><br><span class="line">    occupationsBottom10List, countBottom10List =zip(*occupationsBottom10Tuple)</span><br><span class="line">    explode = (<span class="number">0</span>, <span class="number">0.3</span>, <span class="number">0.2</span>, <span class="number">0.15</span>,<span class="number">0.1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0.1</span>)</span><br><span class="line">    plt.pie(countBottom10List, explode=explode,labels=occupationsBottom10List, autopct=<span class="string">'%1.1f%%'</span>, shadow=<span class="keyword">True</span>,startangle=<span class="number">90</span>)</span><br><span class="line">    plt.title(<span class="string">'Bottom 10 user types\n'</span>)</span><br><span class="line">    plt.show(block=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一-Python-访问-MySQL&quot;&gt;&lt;a href=&quot;#一-Python-访问-MySQL&quot; class=&quot;headerlink&quot; title=&quot;一. Python 访问 MySQL&quot;&gt;&lt;/a&gt;一. Python 访问 MySQL&lt;/h1&gt;&lt;h2 id=&quot;1-安
      
    
    </summary>
    
      <category term="Spark" scheme="https://airpoet.github.io/categories/Spark/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Spark" scheme="https://airpoet.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>i-Spark-4</title>
    <link href="https://airpoet.github.io/2018/07/20/Spark/i-Spark-4/"/>
    <id>https://airpoet.github.io/2018/07/20/Spark/i-Spark-4/</id>
    <published>2018-07-20T14:11:10.340Z</published>
    <updated>2018-07-23T06:20:44.679Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一-机器学习分类"><a href="#一-机器学习分类" class="headerlink" title="一. 机器学习分类"></a>一. 机器学习分类</h1><h2 id="1-监督学习"><a href="#1-监督学习" class="headerlink" title="1.监督学习"></a>1.监督学习</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-20-141755.png" alt="image-20180720221754476"></p><ul><li>有训练数据集。规范数据。合规数据。产生推断函数.然后对新数据应用函数。 </li><li>director actor edit         Label </li></ul><h2 id="2-非监督学习"><a href="#2-非监督学习" class="headerlink" title="2.非监督学习"></a>2.非监督学习</h2><ul><li>没有训练数据。 </li><li>分组。 </li></ul><h2 id="3-推荐"><a href="#3-推荐" class="headerlink" title="3.推荐"></a>3.推荐</h2><ul><li>协同过滤. </li><li>猜测你喜欢. </li><li>电商。 </li></ul><hr><h1 id="二-Spark机器学习库"><a href="#二-Spark机器学习库" class="headerlink" title="二. Spark机器学习库"></a>二. Spark机器学习库</h1><p><strong>[Estimator]</strong></p><ul><li><p>运行在包含了feature和label(结果)的dataFrame之上，对数据进行训练创建model。</p></li><li><p>该模型用于以后的预测。</p></li></ul><p><strong>[Transformer]</strong></p><ul><li>将包含feature的Dataframe变换成了包含了预测的dataframe.</li><li>由Estimator创建的model就是Transformer。</li></ul><p><strong>[Parameter]</strong></p><ul><li>Estimator和Transformer使用的数据，通常和机器学习的算法相关。</li><li>Spark API给出了一致性API针对算法。</li></ul><p><strong>[Pipeline]</strong></p><ul><li>将Estimators和Transformers组合在一起，形成机器学习工作流.</li></ul><h4 id="机器学习应用步骤"><a href="#机器学习应用步骤" class="headerlink" title="机器学习应用步骤"></a>机器学习应用步骤</h4><ol><li>读取数据文件形成训练数据框 </li><li>创建LinearRegression并设置参数 </li><li>对训练数据进行模型拟合，完成评估管线. </li><li>创建包含测试数据的DataFrame，典型包含feature和label，可以通过比较预测标签和测试标签确认model是ok， </li><li>使用模型，对测试数据进行变换(应用模型),抽取feature ，label，predication. </li></ol><h1 id="三-代码实例"><a href="#三-代码实例" class="headerlink" title="三. 代码实例"></a>三. 代码实例</h1><h2 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1. 线性回归"></a>1. 线性回归</h2><blockquote><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/scala/SparkMLDemo1.scala" target="_blank" rel="noopener">测试案例</a></p></blockquote><h2 id="2-逻辑回归"><a href="#2-逻辑回归" class="headerlink" title="2. 逻辑回归"></a>2. 逻辑回归</h2><blockquote><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/scala/LogicRegressWineClassifyDemo.scala" target="_blank" rel="noopener">酒质量预测</a></p></blockquote><blockquote><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/scala/SpamFilterDemo1.scala" target="_blank" rel="noopener">垃圾邮件过滤</a></p></blockquote><h2 id="3-ALS-最小二乘法"><a href="#3-ALS-最小二乘法" class="headerlink" title="3. ALS (最小二乘法)"></a>3. ALS (最小二乘法)</h2><blockquote><p>商品推荐: <a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/scala/RecommDemo.scala" target="_blank" rel="noopener">添加向指定用户推荐n款商品; 将指定的商品推荐给n个用户; 向所有用户推荐n种商品</a></p></blockquote><blockquote><p>电影推荐: <a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/scala/MovieRecommDemo.scala" target="_blank" rel="noopener">ALS算法电影推荐</a> </p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一-机器学习分类&quot;&gt;&lt;a href=&quot;#一-机器学习分类&quot; class=&quot;headerlink&quot; title=&quot;一. 机器学习分类&quot;&gt;&lt;/a&gt;一. 机器学习分类&lt;/h1&gt;&lt;h2 id=&quot;1-监督学习&quot;&gt;&lt;a href=&quot;#1-监督学习&quot; class=&quot;header
      
    
    </summary>
    
      <category term="Spark" scheme="https://airpoet.github.io/categories/Spark/"/>
    
      <category term="MLLib" scheme="https://airpoet.github.io/categories/Spark/MLLib/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="MLLib" scheme="https://airpoet.github.io/tags/MLLib/"/>
    
      <category term="机器学习" scheme="https://airpoet.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>i-Spark-3</title>
    <link href="https://airpoet.github.io/2018/07/20/Spark/i-Spark-3/"/>
    <id>https://airpoet.github.io/2018/07/20/Spark/i-Spark-3/</id>
    <published>2018-07-20T12:06:12.974Z</published>
    <updated>2018-07-27T14:09:20.029Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一-Spark-Streaming-简介"><a href="#一-Spark-Streaming-简介" class="headerlink" title="一. Spark Streaming 简介"></a>一. Spark Streaming 简介</h1><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h2><ul><li>是 <code>spark core</code> 的扩展，针对实时数据流处理,具有可扩展、高吞吐量、容错.</li><li>数据可以是来自于<code>kafka, flume, tcpsocket</code>,使用高级函数(map reduce filter ,join , window)</li><li>处理的数据可以推送到 <code>database</code>, <code>hdfs</code>, 针对数据流处理可以应用到机器学习和图计算中。</li></ul><h2 id="2-DStream-Discretized-Stream-离散流"><a href="#2-DStream-Discretized-Stream-离散流" class="headerlink" title="2. DStream (Discretized Stream ) 离散流"></a>2. DStream (Discretized Stream ) 离散流</h2><h4 id="概念"><a href="#概念" class="headerlink" title="概念:"></a><strong>概念:</strong></h4><ul><li>离散流,表示的是连续的数据流。连续的RDD序列。准实时计算。</li><li>通过kafka、flume等输入数据流产生，也可以通过对其他DStream进行高阶变换产生。</li><li>在内部，DStream表现为RDD序列。</li></ul><h4 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项:"></a><strong>注意事项:</strong></h4><ul><li>启动上下文之后，不能启动新的离散流</li><li>上下文停止后不能restart</li><li>同一 JVM只有一个 active 的 StreamingContext</li><li>停止StreamingContext会一同stop掉SparkContext，如若只停止StreamingContext. <code>ssc.stop(false|true);</code> :TODO</li><li>SparkContext可以创建多个StreamingContext, 创建新的之前停掉旧的。</li></ul><h4 id="DStream和Receiver"><a href="#DStream和Receiver" class="headerlink" title="DStream和Receiver:"></a>DStream和Receiver:</h4><ul><li><strong>介绍</strong><ul><li>Receiver是接受者，从source接受数据，存储在内存中供spark处理。</li></ul></li><li><strong>源</strong><ul><li>基本源:fileSystem | socket,内置API支持。</li><li>高级源:kafka | flume | …，需要引入pom.xml依赖.</li></ul></li><li><strong>注意</strong><ul><li>使用local模式时，不能使用一个线程.使用的local[n]，n需要大于receiver的个数。 </li></ul></li></ul><hr><h1 id="二-体验Spark-Streaming"><a href="#二-体验Spark-Streaming" class="headerlink" title="二. 体验Spark Streaming"></a>二. 体验Spark Streaming</h1><h2 id="1-引入-pom-依赖"><a href="#1-引入-pom-依赖" class="headerlink" title="1.引入 pom 依赖"></a>1.引入 pom 依赖</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="2-Scala-操作代码-in-IDEA"><a href="#2-Scala-操作代码-in-IDEA" class="headerlink" title="2. Scala 操作代码 in IDEA"></a>2. Scala 操作代码 in IDEA</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//local[n] n &gt; 1</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建Spark流上下文,批次时长是1s</span></span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建socket文本流</span></span><br><span class="line"><span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//压扁</span></span><br><span class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">//变换成对偶</span></span><br><span class="line"><span class="keyword">val</span> pairs = words.map((_,<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 化简</span></span><br><span class="line"><span class="keyword">val</span> count = pairs.reduceByKey(_+_) ;</span><br><span class="line">count.print()</span><br><span class="line"></span><br><span class="line"><span class="comment">//启动</span></span><br><span class="line">ssc.start()</span><br><span class="line"></span><br><span class="line"><span class="comment">//等待结束</span></span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure><h2 id="3-通过-nc-与-Spark-交互"><a href="#3-通过-nc-与-Spark-交互" class="headerlink" title="3. 通过 nc 与 Spark 交互"></a>3. 通过 nc 与 Spark 交互</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1&gt; 启动 nc</span><br><span class="line">nc -lk 9999</span><br><span class="line"></span><br><span class="line">2&gt; 运行 Spark Streaming 代码, 开启监听 localhost:9999 </span><br><span class="line"></span><br><span class="line">3&gt; 在 nc 命令行输入单词 </span><br><span class="line">$&gt; hello world </span><br><span class="line">$&gt; ....</span><br><span class="line"></span><br><span class="line">$&gt; 观察 spark 控制台打印</span><br><span class="line"></span><br><span class="line">PS: 把 log4j 文件放到项目的 resources 下, 设置log4j 的打印级别为 WARN, 否则很难观察清楚</span><br></pre></td></tr></table></figure><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/resources/log4j.properties" target="_blank" rel="noopener">log4j 文件请参考</a></p><h2 id="4-导出-Spark-Streaming-为-jar-包-放到-Linux-下运行"><a href="#4-导出-Spark-Streaming-为-jar-包-放到-Linux-下运行" class="headerlink" title="4.导出 Spark Streaming 为 jar 包, 放到 Linux 下运行"></a>4.导出 Spark Streaming 为 jar 包, 放到 Linux 下运行</h2><p><strong>注意: 直接spark-submit   或者 spark-submit –help , 会弹出帮助</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$&gt; spark-submit --name wcstreaming </span><br><span class="line">                --<span class="class"><span class="keyword">class</span> <span class="title">com</span>.<span class="title">rox</span>.<span class="title">spark</span>.<span class="title">scala</span>.<span class="title">SparkStreamingDemo</span></span></span><br><span class="line"><span class="class">                <span class="title">--master</span> <span class="title">spark</span></span>:<span class="comment">//cs1:7077</span></span><br><span class="line">                <span class="type">SparkDemo1</span><span class="number">-1.0</span>-<span class="type">SNAPSHOT</span>.jar</span><br></pre></td></tr></table></figure><hr><h1 id="三-Kafka-与-Spark-Streaming-整合"><a href="#三-Kafka-与-Spark-Streaming-整合" class="headerlink" title="三. Kafka 与 Spark Streaming 整合"></a>三. Kafka 与 Spark Streaming 整合</h1><h2 id="1-回忆Kafka"><a href="#1-回忆Kafka" class="headerlink" title="1. 回忆Kafka"></a>1. <strong>回忆Kafka</strong></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1&gt; 启动 kafka,cs2~cs4  前提先启动 zk</span><br><span class="line">$&gt; kafka-server-start.sh /home/ap/apps/kafka/config/server.properties</span><br><span class="line"></span><br><span class="line">2&gt; 查看主题: </span><br><span class="line">$&gt; kafka-topics.sh --zookeeper cs1:2181 --list</span><br><span class="line"></span><br><span class="line">3&gt; 开启消费者</span><br><span class="line">[ap@cs4]~%&gt;  kafka-console-consumer.sh --zookeeper cs3:2181 --topic kafka-test</span><br><span class="line"></span><br><span class="line">4&gt; 开启生产者</span><br><span class="line">[ap@cs2]~%&gt;  kafka-console-producer.sh --broker-list cs4:9092 --topic kafka-test </span><br><span class="line"></span><br><span class="line">&gt;&gt; 在生产者发送消息, 查看消费者</span><br></pre></td></tr></table></figure><h2 id="2-使用-Java编写-Kafka-SparkStreaming-代码"><a href="#2-使用-Java编写-Kafka-SparkStreaming-代码" class="headerlink" title="2. 使用 Java编写 Kafka-SparkStreaming 代码"></a>2. 使用 Java编写 Kafka-SparkStreaming 代码</h2><p><a href="https://github.com/airpoet/bigdata/blob/068fbf3e23f6514fadcef10cef5b6ce8d87318d8/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/java/KafkaSparkstreamingDemo.java" target="_blank" rel="noopener">具体见 github</a></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.spark.java;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.TaskContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Seconds;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.TopicPartition;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaSparkstreamingDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">        conf.setAppName(<span class="string">"KafkaSparkstreamingDemo"</span>);</span><br><span class="line">        conf.setMaster(<span class="string">"local[4]"</span>);</span><br><span class="line">        <span class="comment">//创建Spark流应用上下文</span></span><br><span class="line">        JavaStreamingContext streamingContext = <span class="keyword">new</span> JavaStreamingContext(conf, Seconds.apply(<span class="number">2</span>));</span><br><span class="line"></span><br><span class="line">        Map&lt;String, Object&gt; kafkaParams = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        kafkaParams.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"cs2:9092,cs3:9092"</span>);</span><br><span class="line">        kafkaParams.put(<span class="string">"key.deserializer"</span>, StringDeserializer.class);</span><br><span class="line">        kafkaParams.put(<span class="string">"value.deserializer"</span>, StringDeserializer.class);</span><br><span class="line">        kafkaParams.put(<span class="string">"group.id"</span>, <span class="string">"g6"</span>);</span><br><span class="line">        kafkaParams.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>);</span><br><span class="line">        kafkaParams.put(<span class="string">"enable.auto.commit"</span>, <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">        Collection&lt;String&gt; topics = Arrays.asList(<span class="string">"kafka-test"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 取出 kafka stream</span></span><br><span class="line">        <span class="keyword">final</span> JavaInputDStream&lt;ConsumerRecord&lt;String, String&gt;&gt; stream =</span><br><span class="line">                KafkaUtils.createDirectStream(</span><br><span class="line">                        streamingContext,</span><br><span class="line">                        LocationStrategies.PreferConsistent(),</span><br><span class="line">                        ConsumerStrategies.&lt;String, String&gt;Subscribe(topics, kafkaParams)</span><br><span class="line">                );</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 压扁</span></span><br><span class="line">        JavaDStream&lt;String&gt; wordDS = stream.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;ConsumerRecord&lt;String, String&gt;, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Iterator&lt;String&gt; <span class="title">call</span><span class="params">(ConsumerRecord&lt;String, String&gt; r)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String value = r.value();</span><br><span class="line">                List&lt;String&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">                String[] arr = value.split(<span class="string">" "</span>);</span><br><span class="line">                <span class="keyword">for</span> (String s : arr) &#123;</span><br><span class="line">                    list.add(s);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> list.iterator();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 映射成元祖 (拼1)</span></span><br><span class="line">        JavaPairDStream&lt;String,Integer&gt; pairDS = wordDS.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(s,<span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">              </span><br><span class="line">        <span class="comment">// 聚合</span></span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; countDS = pairDS.reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer v1, Integer v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 打印计算结果</span></span><br><span class="line">        countDS.print();</span><br><span class="line"></span><br><span class="line">        streamingContext.start();</span><br><span class="line"></span><br><span class="line">        streamingContext.awaitTermination();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="四-跨单位时间-单位距离-Window-跨批次-updateStateByKey"><a href="#四-跨单位时间-单位距离-Window-跨批次-updateStateByKey" class="headerlink" title="四.  跨单位时间,单位距离 (Window) ,  跨批次(updateStateByKey)"></a>四.  跨单位时间,单位距离 (Window) ,  跨批次(updateStateByKey)</h1><h2 id="1-Window"><a href="#1-Window" class="headerlink" title="1. Window"></a>1. Window</h2><h4 id="关键参数"><a href="#关键参数" class="headerlink" title="关键参数:"></a><strong>关键参数</strong>:</h4><p><strong>batch interval</strong> </p><ul><li>批次的间隔. </li></ul><p><strong>windows length</strong></p><ul><li>窗口长度,跨批次。是批次的整数倍。 </li></ul><p><strong>slide interval</strong> </p><ul><li>滑动间隔,窗口计算的间隔时间，有时批次interval的整倍数。 </li></ul><p><br></p><h4 id="关键代码示例"><a href="#关键代码示例" class="headerlink" title="关键代码示例:"></a>关键代码示例:</h4><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/java/WCSparkStreamWindowApp.java" target="_blank" rel="noopener">具体查看 github</a></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//聚合</span></span><br><span class="line"><span class="comment">/** 统计同一 window 下的 key 的聚合 (用的比较多...)</span></span><br><span class="line"><span class="comment">     def reduceByKeyAndWindow(reduceFunc: Function2[V, V, V],</span></span><br><span class="line"><span class="comment">             windowDuration: Duration,</span></span><br><span class="line"><span class="comment">             slideDuration: Duration): JavaPairDStream[K, V]</span></span><br><span class="line"><span class="comment">    &gt;&gt;&gt; Return a new DStream by applying `reduceByKey` over a sliding window</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="type">JavaPairDStream</span>&lt;<span class="type">String</span>,<span class="type">Integer</span>&gt; countDS = pairDS.reduceByKeyAndWindow(<span class="keyword">new</span> <span class="type">Function2</span>&lt;<span class="type">Integer</span>, <span class="type">Integer</span>, <span class="type">Integer</span>&gt;() &#123;</span><br><span class="line">    public <span class="type">Integer</span> call(<span class="type">Integer</span> v1, <span class="type">Integer</span> v2) &#123;</span><br><span class="line">        <span class="keyword">return</span> v1 + v2;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;,<span class="type">Seconds</span>.apply(<span class="number">6</span>),<span class="type">Seconds</span>.apply(<span class="number">4</span>));</span><br></pre></td></tr></table></figure><h2 id="2-updateStateByKey"><a href="#2-updateStateByKey" class="headerlink" title="2. updateStateByKey"></a>2. updateStateByKey</h2><p><strong><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/java/WordCountSparkStreamingJava.java" target="_blank" rel="noopener">跨批次统计, 会一直累加, 具体查看 github</a></strong></p><h4 id="关键代码示例-1"><a href="#关键代码示例-1" class="headerlink" title="关键代码示例"></a>关键代码示例</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 可用于跨批次统计 updateStateByKeya</span></span><br><span class="line">JavaPairDStream&lt;String,Integer&gt; jps = pairDS.updateStateByKey(<span class="keyword">new</span> Function2&lt;List&lt;Integer&gt;, Optional&lt;Integer&gt;, Optional&lt;Integer&gt;&gt;() &#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Optional&lt;Integer&gt; <span class="title">call</span><span class="params">(List&lt;Integer&gt; v1, Optional&lt;Integer&gt; v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Integer newCount = v2.isPresent() ? v2.get() : <span class="number">0</span>  ;</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"old value : "</span> + newCount);</span><br><span class="line">        <span class="keyword">for</span>(Integer i : v1)&#123;</span><br><span class="line">            System.out.println(<span class="string">"new value : "</span> + i);</span><br><span class="line">            newCount = newCount +  i;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> Optional.of(newCount);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><hr><h1 id="五-spark-streaming中的容错实现-※"><a href="#五-spark-streaming中的容错实现-※" class="headerlink" title="五. spark streaming中的容错实现 ※"></a>五. spark streaming中的容错实现 ※</h1><h2 id="1-生产环境中spark-streaming的job的注意事项"><a href="#1-生产环境中spark-streaming的job的注意事项" class="headerlink" title="1.生产环境中spark streaming的job的注意事项"></a>1.生产环境中spark streaming的job的注意事项</h2><p><strong>避免单点故障。</strong></p><p><strong>Driver</strong>               </p><ul><li>驱动,运行用户编写的程序代码的主机。 </li></ul><p><strong>Executors</strong>        </p><ul><li>执行的spark driver提交的job,内部含有附加组件比如receiver</li><li>receiver接受数据并以block方式保存在memory中，同时，将数据块复制到其他executor中，以备容错。</li><li>每个批次末端会形成新的DStream，交给 下游处理。</li><li>如果receiver故障，其他执行器中的receiver会启动进行数据的接收。 </li></ul><p><strong>checkpoint</strong></p><ul><li>检查点</li><li>用于容错处理, 设置后, 会把数据存储到检查点目录,   当出问题后,  从检查点恢复.</li></ul><h2 id="2-通过checkpoint-实现-Spark-Streaming-的容错"><a href="#2-通过checkpoint-实现-Spark-Streaming-的容错" class="headerlink" title="2. 通过checkpoint 实现 Spark Streaming 的容错"></a>2. 通过<strong>checkpoint</strong> 实现 Spark Streaming 的容错</h2><p>如果<strong>executor故障</strong>，所有未被处理的数据都会丢失，解决办法可以通过wal (hbase,hdfs/WALs)方式将数据预先写入到hdfs或者s3.</p><p>如果<strong>Driver故障</strong>，driver程序就会停止，所有executor都是丢失连接，停止计算过程。</p><p><strong>解决办法需要配置和编程。</strong></p><ul><li>配置Driver程序自动重启，使用特定的clustermanager实现。 :TODO   how?</li><li><strong>重启时，从宕机的地方进行重启，通过检查点机制可以实现该功能。</strong><ul><li>检查点目录可以是本地，可以是hdfs, <strong>生产中一般都是 hdfs</strong></li><li><strong>不再使用new方式</strong>创建<code>SparkStreamContext</code>对象，而是通过工厂方式<code>JavaStreamingContext.getOrCreate()</code>方法创建</li><li>上下文对象,首先会检查检查点目录，看是否有job运行，没有就new新的。</li></ul></li><li>编写容错测试代码,计算过程编写到 <code>Function0</code> 的<code>call</code>方法中。 </li></ul><p><strong>知识点</strong>: Function0, 1, 2, 3, 4</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">为何要使用 Function0?</span><br><span class="line">    因为 JavaStreamingContext.getOrCreate(path,function0) 的第二个参数就是 Function0</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A zero-argument function that returns an R.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Function0</span>&lt;<span class="title">R</span>&gt; <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="function">R <span class="title">call</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/java/SparkStreamingCheckpointDemo.java" target="_blank" rel="noopener">Java 代码, 具体见 github</a></p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">package</span> com.rox.spark.java;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function0;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Duration;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkStreamingCheckpointDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Create a factory object that can create and setup a new JavaStreamingContext</span></span><br><span class="line"><span class="comment">         * 可以使用 Function0</span></span><br><span class="line"><span class="comment">         * A zero-argument function that returns an R.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        Function0&lt;JavaStreamingContext&gt; contextFactory = <span class="keyword">new</span> Function0&lt;JavaStreamingContext&gt;()&#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="comment">// 首次创建 context 时, 调用此方法</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> JavaStreamingContext <span class="title">call</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">                conf.setMaster(<span class="string">"local[4]"</span>);</span><br><span class="line">                conf.setAppName(<span class="string">"SparkStreamingCheckpointDemo"</span>);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 创建流上下文对象</span></span><br><span class="line">                JavaStreamingContext jsc = <span class="keyword">new</span> JavaStreamingContext(conf, <span class="keyword">new</span> Duration(<span class="number">2</span> * <span class="number">1000</span>));</span><br><span class="line"></span><br><span class="line">                <span class="comment">// Create an input stream from network source hostname:port.</span></span><br><span class="line">                JavaDStream&lt;String&gt; lines = jsc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">10086</span>);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// =============== 变换代码 ===============</span></span><br><span class="line">                <span class="comment">// 设置一个窗口时长为1天, 滚动间隔为 2s</span></span><br><span class="line">                JavaDStream&lt;Long&gt; longJavaDStream = lines.countByWindow(<span class="keyword">new</span> Duration(<span class="number">24</span> * <span class="number">60</span> * <span class="number">60</span> * <span class="number">1000</span>), <span class="keyword">new</span> Duration(<span class="number">2</span> * <span class="number">1000</span>));</span><br><span class="line"></span><br><span class="line">                longJavaDStream.print();</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 设置检查点 目录</span></span><br><span class="line">                jsc.checkpoint(<span class="string">"file:///Users/shixuanji/Documents/Code/temp/check"</span>);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 返回流上下文对象</span></span><br><span class="line">                <span class="keyword">return</span> jsc;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         *   def getOrCreate(</span></span><br><span class="line"><span class="comment">         checkpointPath: String,</span></span><br><span class="line"><span class="comment">         creatingFunc: JFunction0[JavaStreamingContext]</span></span><br><span class="line"><span class="comment">         ): JavaStreamingContext</span></span><br><span class="line"><span class="comment">         注意: 第2个参数是一个 Function0对象</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="comment">// 失败后, 重新创建时, 会经过检查点</span></span><br><span class="line">        JavaStreamingContext context = JavaStreamingContext.getOrCreate(<span class="string">"file:///Users/shixuanji/Documents/Code/temp/check"</span>, contextFactory);</span><br><span class="line"></span><br><span class="line">        context.start();</span><br><span class="line">        context.awaitTermination();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一-Spark-Streaming-简介&quot;&gt;&lt;a href=&quot;#一-Spark-Streaming-简介&quot; class=&quot;headerlink&quot; title=&quot;一. Spark Streaming 简介&quot;&gt;&lt;/a&gt;一. Spark Streaming 简介&lt;/h1
      
    
    </summary>
    
      <category term="Spark" scheme="https://airpoet.github.io/categories/Spark/"/>
    
      <category term="SparkStreaming" scheme="https://airpoet.github.io/categories/Spark/SparkStreaming/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Spark" scheme="https://airpoet.github.io/tags/Spark/"/>
    
      <category term="SparkStreaming" scheme="https://airpoet.github.io/tags/SparkStreaming/"/>
    
  </entry>
  
  <entry>
    <title>i-Spark-2</title>
    <link href="https://airpoet.github.io/2018/07/20/Spark/i-Spark-2/"/>
    <id>https://airpoet.github.io/2018/07/20/Spark/i-Spark-2/</id>
    <published>2018-07-20T09:03:11.064Z</published>
    <updated>2018-07-30T17:37:10.515Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一-Spark-核心-API"><a href="#一-Spark-核心-API" class="headerlink" title="一. Spark 核心 API"></a>一. Spark 核心 API</h1><p><strong>[SparkContext]</strong></p><ul><li>连接到spark集群,入口点.</li></ul><p><strong>[HadoopRDD]</strong></p><ul><li>读取hadoop上的数据，</li></ul><p><strong>[MapPartitionsRDD]</strong></p><ul><li>针对父RDD的每个分区提供了函数构成的新类型RDD.</li></ul><p><strong>[PairRDDFunctions]</strong></p><ul><li>对偶RDD函数类。</li><li>可用于KV类型RDD的附加函数。可以通过隐式转化得到.</li></ul><p><strong>[ShuffleRDD]</strong></p><ul><li>从Shuffle中计算结果的RDD.</li></ul><p><strong>[RDD]</strong></p><ul><li>是分区的集合, 弹性分布式数据集, 不可变的数据分区集合.</li><li>基本操作(map filter , persist)</li><li><strong>特点:</strong> <ul><li>分区列表                    //数据</li><li>应用给每个切片的计算函数    //行为</li><li>到其他RDD的依赖列表            //依赖关系</li><li>(可选)针对kv类型RDD的分区类</li><li>(可选)首选位置列表</li></ul></li></ul><p><strong>[DAGScheduler]</strong></p><ul><li>高级调度器层面，实现按照阶段(stage) 进行 shuffle</li><li>对每个JOB的各阶段(stage)计算有向无环图(DAG)，并且跟踪RDD和每个阶段的输出。</li><li>找出最小调度运行作业, 将Stage对象以TaskSet方式提交给底层的调度器。</li><li>底层调度器实现TaskScheduler, 进而在cluster上运行job.</li><li>TaskSet已经包含了全部的单独的task，这些Task都能够基于cluster的数据进行正确运行。</li><li>Stage通过在需要shuffle的边界处将RDD打碎来创建Stage对象。</li><li>具有<code>窄依赖</code>的RDD操作(比如map /filter)被管道化至一个taskset中.</li><li>而具有shuffle依赖的操作则包含多个Stage(一个进行输出，另一个进行输入)</li><li>最后，每个stage都有一个针对其他stage的shuffle依赖，可以计算多个操作。</li><li>DAG调度器检测首选位置来运行task，通过基于当前的缓存状态，并传递给底层的task调度器来实现。根据shuffle的输出是否丢失处理故障问题。</li><li>不是因为随机文件丢失造成的故障会由任务调度程序处理，它在取消整个stage前花一小段时间重试每个任务</li><li>为了容错，同一stage可能会运行多次，称之为”attempts”,如果task调度器报告了一个故障(该故障是由于上一个stage丢失输出文件而导致的)DAG调度就会重新提交丢失的stage。这个通过具有 FetchFailed的CompletionEvent对象或者ExecutorLost进行检测的。</li><li>DAG调度器会等待一段时间看其他节点或task是否失败，然后对丢失的stage重新提交taskset，计算丢失的task。</li></ul><p><br></p><h4 id="术语介绍"><a href="#术语介绍" class="headerlink" title="术语介绍"></a><strong>术语介绍</strong></h4><p><strong>[ job ]</strong></p><ul><li>提交给调度的顶层的工作项目，由 <strong>ActiveJob</strong> 表示。</li><li>是Stage集合。</li></ul><p><u><strong>[Stage]</strong></u></p><ul><li>是task的集合，计算job中的中间结果。同一RDD的每个分区都会应用相同的计算函数。</li><li>在shuffle的边界处进行隔离(因此引入了隔断，需要上一个stage完成后，才能得到output结果)</li><li><strong>Stage有两个子类:</strong><ul><li>1) ResultStage，用于执行action动作的最终stage。</li><li>2) ShuffleMapStage,  对shuffle进行输出文件的写操作的。如果job重用了同一个rdd的话，stage通常可以跨越多个job实现共享。</li></ul></li><li>并行任务的集合，都会计算同一函数。所有task有着同样的shuffle依赖，调度器运行的task DAG 在shuffle边界处划分成不同阶段。调度器以拓扑顺序执行.</li><li>每个stage可以shuffleMapStage,该阶段下输出是下一个stage的输入，也可以是resultStage,该阶段 task直接执行spark action。对于shuffleMapStage，需要跟踪每个输出分区所在的节点。</li><li>每个stage都有FirstJobId,区分于首次提交的id</li></ul><p><strong>[ShuffleMapStage]</strong></p><ul><li>产生输出数据，在每次shuffle之前发生。内部含有shuffleDep字段,有相关字段记录产生多少输出以及多少输出可用</li><li>DAGScheduler.submitMapStage()方法可以单独提交submitMapStage().</li></ul><p><strong>[ResultStage]</strong></p><ul><li>该阶段在RDD的一些分区中应用函数来计算Action的结果。有些stage并不会在所有分区上执行。例如first(),lookup();</li></ul><p><u><strong>[Task]</strong></u></p><ul><li>单独的工作单元，每个发送给一台主机。</li></ul><p><strong>[Cache tracking]</strong></p><ul><li>Dag调度器找出哪些RDD被缓存，避免不必要的重复计算，同时，也会记住哪些shuffleMap已经输出了结果，避免map端shuffle的重复处理。</li></ul><p><strong>[Preferred locations]</strong></p><ul><li>dag调度器根据rdd的中首选位置属性计算task在哪里运行。</li></ul><p><strong>[Cleanup]</strong></p><ul><li>运行的job如果完成就会清楚数据结构避免内存泄漏，主要是针对耗时应用。</li></ul><p><strong>[ActiveJob]</strong></p><ul><li>在Dag调度器中运行job。作业分为两种类型，<ul><li>1) result job，计算ResultStage来执行action.</li><li>2 )map-state job,为shuffleMapState结算计算输出结果以供下游stage使用。主要使用finalStage字段进行类型划分。</li></ul></li><li>job只跟踪客户端提交的”leaf” stage，通过调用Dag调度器的submitjob或者- submitMapStage()方法实现.</li><li>job类型引发之前stage的执行，而且多个job可以共享之前的stage。这些依赖关系由DAG调度器内部管理。</li></ul><p><strong>[LiveListenerBus]</strong></p><ul><li>异步传输spark监听事件到监听器事件集合中。</li></ul><p><strong>[EventLoop]</strong></p><ul><li>从caller接受事件，在单独的事件线程中处理所有事件，该类的唯一子类是DAGSchedulerEventProcessLoop。</li></ul><p><strong>[LiveListenerBus]</strong></p><ul><li>监听器总线，存放Spark监听器事件的队列。用于监控。</li></ul><p><strong>[OutputCommitCoordinator]</strong></p><ul><li>输出提交协调器.决定提交的输出是否进入hdfs。</li></ul><p><strong>[TaskScheduler]</strong></p><ul><li>底层的调度器，唯一实现TaskSchedulerImpl。可插拔，同Dag调度器接受task，发送给cluster，运行任务，失败重试，返回事件给DAG调度器。</li></ul><p><strong>[TaskSchedulerImpl]</strong></p><ul><li>TaskScheduler调度器的唯一实现，通过BackendScheduler(后台调度器)实现各种类型集群的任务调度。</li></ul><p><strong>[SchedulerBackend]</strong></p><ul><li><p>可插拔的后台调度系统，本地调度，mesos调度，。。。</p></li><li><p>在SchedulerBackend下方，实现有三种</p><ul><li>LocalSchedulerBackend  本地后台调度器,启动task.</li><li><p>StandaloneSchedulerBackend  独立后台调度器</p></li><li><p>CoarseGrainedSchedulerBackend    粗粒度后台调度器</p></li></ul></li></ul><p><strong>[Executor]</strong></p><ul><li>spark程序执行者，通过线程池执行任务。</li></ul><p><strong>[Dependency]:依赖</strong></p><ul><li><p><strong>NarrowDependency</strong>:    子RDD的每个分区依赖于父RDD的少量分区, 也叫完全依赖</p><p>​         | </p><p>​        / \ </p><p>​        — </p><p>​         |—-    OneToOneDependency        //父子RDD之间的分区存在一对一关系。 </p><p>​         |—-    RangeDependency            //父RDD的一个分区范围和子RDD存在一对一关系。 </p><p>​         |—-    PruneDependency            // 在PartitionPruningRDD和其父RDD之间的依赖,   子RDD包含了父RDD的分区子集。 </p></li><li><p><strong>ShuffleDependency</strong>:    在shuffle阶段输出时的一种依赖, 属于一种宽依赖, 也叫部分依赖</p></li></ul><h1 id="二-Spark-其它概念"><a href="#二-Spark-其它概念" class="headerlink" title="二. Spark 其它概念"></a>二. Spark 其它概念</h1><h2 id="1-创建-Spark-上下文-Context"><a href="#1-创建-Spark-上下文-Context" class="headerlink" title="1. 创建 Spark 上下文(Context)"></a>1. 创建 Spark 上下文(Context)</h2><p><strong>[本地模式,通过线程模拟]</strong></p><blockquote><p>​    本地后台调度器 </p><p>​    spark local </p><p>​    spark local[3]                            //3线程,模拟cluster集群 </p><p>​    spark local[*]                            //匹配cpu个数， </p><p>​    spark local[3,2]                        //3:3个线程，2为maxFailures。 </p><ul><li>// local[*, M] means the number of cores on the computer with M failures</li><li>// local[N, M] means exactly N threads with M failures </li><li>0和1等价，只执行一次, 失败后不会重试</li></ul></blockquote><p><strong>[相当于伪分布式]</strong></p><blockquote><p>​    StandaloneSchedulerBackend </p><p>​    spark local-cluster[N, cores, memory]    //模拟spark集群。  </p></blockquote><p><strong>[完全分布式]</strong></p><blockquote><p>​    StandaloneSchedulerBackend </p><p>​    spark <a href="spark://cs1:7077" target="_blank" rel="noopener">spark://cs1:7077</a>                    //连接到spark集群上. </p></blockquote><h2 id="2-RDD-持久化"><a href="#2-RDD-持久化" class="headerlink" title="2. RDD 持久化"></a>2. RDD 持久化</h2><ul><li><p>跨操作进行RDD的内存式存储。 </p></li><li><p>持久化RDD时，节点上的每个分区都会保存操内存中,以备在其他操作中进行重用。 </p></li><li><p>缓存技术是迭代式计算和交互式查询的重要工具。 </p></li><li><p>使用persist()和cache()进行rdd的持久化。 </p></li><li><p>cache()是persist()一种. </p></li><li><p>action第一次计算时会发生persist(). </p></li><li><p>spark的cache是容错的，如果rdd的任何一个分区丢失了，都可以通过最初创建rdd的进行重新计算。 </p></li><li><p>persist可以使用不同的存储级别进行持久化。 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">MEMORY_ONLY            //只在内存</span><br><span class="line">MEMORY_AND_DISK</span><br><span class="line">MEMORY_ONLY_SER        //内存存储(串行化)</span><br><span class="line">MEMORY_AND_DISK_SER </span><br><span class="line">DISK_ONLY            //硬盘                        // 默认的路径似乎在程序执行完后自己会删掉??</span><br><span class="line">MEMORY_ONLY_2        //带有副本 </span><br><span class="line">MEMORY_AND_DISK_2    //快速容错。</span><br><span class="line">OFF_HEAP             // 离堆内存</span><br></pre></td></tr></table></figure></li><li><p>删除持久化数据 <code>rdd.unpersist();</code></p></li></ul><h2 id="3-数据传递"><a href="#3-数据传递" class="headerlink" title="3.数据传递"></a>3.数据传递</h2><ul><li>map(),filter()高级函数中访问的对象被串行化到各个节点。每个节点都有一份拷贝。 </li><li>变量值并不会回传到driver程序。 </li></ul><h2 id="4-共享变量"><a href="#4-共享变量" class="headerlink" title="4.共享变量"></a>4.共享变量</h2><p> <strong>spark通过广播变量和累加器实现共享变量。</strong></p><p><br></p><p><strong>[广播变量]</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建广播变量</span></span><br><span class="line"><span class="keyword">val</span> bc1 = sc.broadcast(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">bc1.value</span><br></pre></td></tr></table></figure><p><strong>[累加器]</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建累加器</span></span><br><span class="line"><span class="keyword">val</span> ac1 = sc.longaccumulator(<span class="string">"ac1"</span>)</span><br><span class="line">ac1.value</span><br><span class="line">sc.parallelize(<span class="number">1</span> to <span class="number">10</span>).map(_ * <span class="number">2</span>).map(e=&gt;&#123;ac1.add(<span class="number">1</span>) ; e&#125;).reduce(_+_)</span><br><span class="line">ac1.value            <span class="comment">//10</span></span><br></pre></td></tr></table></figure><h2 id="5-通过-Spark-实现-PI-的分布式计算"><a href="#5-通过-Spark-实现-PI-的分布式计算" class="headerlink" title="5.通过 Spark 实现 PI 的分布式计算"></a>5.通过 Spark 实现 PI 的分布式计算</h2><p>:TODO 计算出来的结果不精准 ??</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize(<span class="number">1</span> to <span class="number">100000000</span>).map(e=&gt;&#123;</span><br><span class="line">    <span class="keyword">val</span> a = <span class="number">1</span>f / (<span class="number">2</span> * e - <span class="number">1</span>) ;</span><br><span class="line">    <span class="keyword">val</span> b = <span class="keyword">if</span> (e % <span class="number">2</span> == <span class="number">0</span>) <span class="number">-1</span> <span class="keyword">else</span> <span class="number">1</span> ;</span><br><span class="line">    a * b * <span class="number">4</span></span><br><span class="line">&#125;).reduce(_+_)</span><br><span class="line"></span><br><span class="line">res25: <span class="type">Float</span> = <span class="number">3.1415968</span>  </span><br><span class="line">==&gt; 用了一个亿算出来这个...</span><br></pre></td></tr></table></figure><h1 id="三-SparkSQL"><a href="#三-SparkSQL" class="headerlink" title="三. SparkSQL"></a>三. SparkSQL</h1><blockquote><p>Hive                  //hadoop mr sql<br>pheonix            //hbase之上构建sql交互过程<br>DataFrame        //收据框.表.该模块能在spark运行sql语句。<br>SparkSQL        //SQL | DataFrame API.</p></blockquote><h2 id="1-Spark-SQL-Shell-操作"><a href="#1-Spark-SQL-Shell-操作" class="headerlink" title="1.Spark SQL  Shell 操作"></a>1.<strong>Spark SQL  Shell 操作</strong></h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建样例类</span></span><br><span class="line">$scala&gt;<span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Customer</span>(<span class="params">id:<span class="type">Int</span>,name:<span class="type">String</span>,age:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">//构造数据</span></span></span><br><span class="line"><span class="class"><span class="title">$scala&gt;val</span> <span class="title">arr</span> </span>= <span class="type">Array</span>(<span class="string">"1,tom,12"</span>,<span class="string">"2,tomas,13"</span>,<span class="string">"3,tomasLee,14"</span>)</span><br><span class="line">$scala&gt;<span class="keyword">val</span> rdd1 = parallelize(arr)</span><br><span class="line"><span class="comment">//创建对象rdd</span></span><br><span class="line">$scala&gt;<span class="keyword">val</span> rdd2 = rdd1.map(e=&gt;&#123;e.split(<span class="string">","</span>) ; <span class="type">Customer</span>(arr(<span class="number">0</span>).toInt,arr(<span class="number">1</span>),arr(<span class="number">2</span>).toInt)&#125;)</span><br><span class="line"><span class="comment">//通过rdd创建数据框</span></span><br><span class="line">$scala&gt;<span class="keyword">val</span> df = spark.createDataFrame(rdd2);</span><br><span class="line"><span class="comment">//打印表结构</span></span><br><span class="line">$scala&gt;df.printSchema</span><br><span class="line">$scala&gt;df.show            <span class="comment">//插叙数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//创建临时视图</span></span><br><span class="line">$scala&gt;df.createTempView(<span class="string">"customers"</span>)</span><br><span class="line">$scala&gt;<span class="keyword">val</span> df2 = spark.sql(<span class="string">"select * from customers"</span>)</span><br><span class="line">$scala&gt;spark.sql(<span class="string">"select * from customers"</span>).show        <span class="comment">//使用sql语句</span></span><br><span class="line">$scala&gt;<span class="keyword">val</span> df1 = spark.sql(<span class="string">"select * from cusotmers where id &lt; 2"</span>);</span><br><span class="line">$scala&gt;<span class="keyword">val</span> df2 = spark.sql(<span class="string">"select * from cusotmers where id &gt; 2"</span>);</span><br><span class="line">$scala&gt;df1.createTempView(<span class="string">"c1"</span>)</span><br><span class="line">$scala&gt;df2.createTempView(<span class="string">"c2"</span>)</span><br><span class="line">$scala&gt;spark.sql(<span class="string">"select * from c1 union select * from c2"</span>).show</span><br><span class="line">$scala&gt;df1.union(df2);</span><br><span class="line">$scala&gt;spark.sql(<span class="string">"select id,name from customers"</span>).show</span><br><span class="line">$scala&gt;df.selectExpr(<span class="string">"id"</span>,<span class="string">"name"</span>)</span><br><span class="line">$scala&gt; spark.sql(<span class="string">"select * from cus where name like 't%' order by name desc"</span>).show</span><br><span class="line">$scala&gt;df.where(<span class="string">"name like 't%'"</span>).show</span><br><span class="line"></span><br><span class="line"><span class="comment">//映射</span></span><br><span class="line">$scala&gt;df.map(_.getAs[<span class="type">Int</span>](<span class="string">"age"</span>)).reduce(_+_)            <span class="comment">//聚合操作DataSet[Int]</span></span><br><span class="line">$scala&gt;df.agg(sum(<span class="string">"age"</span>),max(<span class="string">"age"</span>),min(<span class="string">"age"</span>))            <span class="comment">//聚合函数</span></span><br></pre></td></tr></table></figure><h2 id="2-IDEA-操作-SQL"><a href="#2-IDEA-操作-SQL" class="headerlink" title="2. IDEA 操作 SQL"></a>2. IDEA 操作 SQL</h2><p><a href="https://github.com/airpoet/bigdata/tree/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/java" target="_blank" rel="noopener">具体查看我的 Github</a></p><h4 id="1-gt-通过读写文件完成基本-SQL-操作"><a href="#1-gt-通过读写文件完成基本-SQL-操作" class="headerlink" title="1&gt; 通过读写文件完成基本 SQL 操作"></a>1&gt; 通过读写文件完成基本 SQL 操作</h4><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/java/SQLJava.java" target="_blank" rel="noopener">见我的 Github 代码</a></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 源码 ==&gt; //DataFrame 类似于table操作。</span></span><br><span class="line">type DataFrame = Dataset[Row]</span><br><span class="line"></span><br><span class="line"><span class="number">1</span>&gt; 通过RDD创建 DataFrame</span><br><span class="line">    df = sc.createDataFrame(rdd);</span><br><span class="line"></span><br><span class="line"><span class="number">2</span>&gt; DataFrame 转回为 RDD</span><br><span class="line">JavaRDD&lt;Row&gt; rdd = df1.toJavaRDD();</span><br><span class="line"></span><br><span class="line"><span class="number">3</span>&gt; SparkSession 对象,通过读取 json 文件, 创建 DataSet</span><br><span class="line">Dataset&lt;Row&gt; df = session.read().json(<span class="string">"file:///tmp/test.json"</span>);</span><br><span class="line"></span><br><span class="line"><span class="number">4</span>&gt; 保存spark的sql计算结果为 json 文件</span><br><span class="line"><span class="comment">//保存成json文件。 模式为添加模式</span></span><br><span class="line">    df.write().mode(SaveMode.Append).json(<span class="string">"file://..."</span>)</span><br></pre></td></tr></table></figure><h4 id="2-gt-通过-JDBC-操作-MySQL"><a href="#2-gt-通过-JDBC-操作-MySQL" class="headerlink" title="2&gt; 通过 JDBC 操作 MySQL"></a>2&gt; 通过 JDBC 操作 MySQL</h4><ol><li><p>添加 pom 依赖 <code>mysql-connector-java</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.17<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/java/SQLJDBCJava.java" target="_blank" rel="noopener">编写代码, 见我的 Github</a></p></li></ol><h2 id="3-整合-Hive"><a href="#3-整合-Hive" class="headerlink" title="3.整合 Hive"></a>3.整合 Hive</h2><h4 id="1-配置-amp-Spark-shell-操作"><a href="#1-配置-amp-Spark-shell-操作" class="headerlink" title="1.配置 &amp; Spark-shell 操作"></a>1.配置 &amp; Spark-shell 操作</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">1&gt; .hive的类库需要在spark worker节点。</span><br><span class="line">----------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">2&gt; .复制hive-site.xml(hive) 到 spark/conf下。</span><br><span class="line">----------------------------------------------------------------------------------</span><br><span class="line">(core-site.xml(hdfs) + hdfs-site.xml(hdfs) 就不用复制了, 在 conf/spark-env.sh 中, 配置了 HADOOP_CONF_DIR 就可以了)</span><br><span class="line"></span><br><span class="line">3&gt; .复制mysql驱动程序到/soft/spark/jars下 (Hive 的 metadata 存在 mysql)</span><br><span class="line">----------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">4&gt; .启动spark-shell,指定启动模式</span><br><span class="line">----------------------------------------------------------------------------------</span><br><span class="line">spark-shell --master <span class="built_in">local</span>[4]</span><br><span class="line"><span class="comment"># 创建 Hive 表</span></span><br><span class="line"><span class="variable">$scala</span>&gt;create table tt(id int,name string , age int) </span><br><span class="line">row format delimited fields terminated by <span class="string">','</span> </span><br><span class="line">lines terminated by <span class="string">'\n'</span> </span><br><span class="line">stored as textfile</span><br><span class="line"></span><br><span class="line">//加载数据到hive表</span><br><span class="line"><span class="variable">$scala</span>&gt;spark.sql(<span class="string">"load data local inpath 'file:///home/centos/data.txt' into table mydb.tt"</span>);</span><br></pre></td></tr></table></figure><h4 id="2-Java版SparkSQL-操作-Hive-表"><a href="#2-Java版SparkSQL-操作-Hive-表" class="headerlink" title="2. Java版SparkSQL 操作 Hive 表"></a>2. Java版SparkSQL 操作 Hive 表</h4><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/pom.xml" target="_blank" rel="noopener">所有的 pom 文件配置</a></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><blockquote><p>Java 代码</p><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/java/SQLHiveJava.java" target="_blank" rel="noopener">详细代码见github</a></p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">conf.setMaster(<span class="string">"local"</span>) ;</span><br><span class="line">conf.setAppName(<span class="string">"SQLJava"</span>);</span><br><span class="line">SparkSession sess = SparkSession.builder()</span><br><span class="line">    .appName(<span class="string">"HiveSQLJava"</span>)</span><br><span class="line">    .config(<span class="string">"spark.master"</span>,<span class="string">"local"</span>)</span><br><span class="line">    .getOrCreate();</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; df = sess.sql(<span class="string">"create table mytt(id int)"</span>);</span><br><span class="line">df.show();</span><br></pre></td></tr></table></figure><h4 id="3-Spark下-分布式访问-Hive-Spark-Shell-—–-开启thriftserver-通过-beeline-访问-hive"><a href="#3-Spark下-分布式访问-Hive-Spark-Shell-—–-开启thriftserver-通过-beeline-访问-hive" class="headerlink" title="3. Spark下 分布式访问 Hive (Spark-Shell) —– 开启thriftserver, 通过 beeline 访问 hive"></a>3. Spark下 分布式访问 Hive (Spark-Shell) —– 开启thriftserver, 通过 beeline 访问 hive</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>&gt; 在默认库下创建hive的数据表。</span><br><span class="line">    $&gt;hive -e <span class="string">"create table tt(id int,name string , age int) row format delimited fields terminated by ',' lines terminated by '\n' stored as textfile"</span></span><br><span class="line"></span><br><span class="line"><span class="number">2</span>&gt; 加载数据到hive表中.</span><br><span class="line">    $&gt;hive -e <span class="string">"load data local inpath 'file:///home/ap/stu' into table tt"</span></span><br><span class="line">    $&gt;hive -e <span class="string">"select * from tt"</span></span><br><span class="line"></span><br><span class="line"><span class="number">3</span>&gt; 启动 thriftserver 服务器</span><br><span class="line"># 启动 thriftServer</span><br><span class="line">    [ap<span class="meta">@cs</span>1]~/apps/spark/sbin%&gt; start-thriftserver.sh --master spark:<span class="comment">//cs1:7077</span></span><br><span class="line"></span><br><span class="line">#  查看</span><br><span class="line">    netstat -anop | grep <span class="number">10000</span> </span><br><span class="line"></span><br><span class="line"><span class="number">4</span>&gt; 启动 beeline</span><br><span class="line"># 连接beeline, 记得一定要加上用户名!!!!  -n ap</span><br><span class="line"># 下面的情况是, cs1是装了 hive 的</span><br><span class="line">[ap<span class="meta">@cs</span>1]~/apps/spark%&gt; bin/beeline -u jdbc:hive2:<span class="comment">//localhost:10000 -n ap -d org.apache.hive.jdbc.HiveDriver</span></span><br><span class="line"></span><br><span class="line"><span class="number">5</span>&gt; 接下来, 跟直接在hive 中, 通过 beeline 访问 hive, 是一样的操作, 就是直接操作 hive</span><br></pre></td></tr></table></figure><h4 id="4-使用-Java-通过-ThreadServer-使用-JDBC-访问-Hive"><a href="#4-使用-Java-通过-ThreadServer-使用-JDBC-访问-Hive" class="headerlink" title="4. 使用 Java 通过 ThreadServer, 使用 JDBC 访问 Hive"></a>4. 使用 Java 通过 ThreadServer, 使用 JDBC 访问 Hive</h4><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/java/ThriftServerClientJava.java" target="_blank" rel="noopener">详情看代码</a></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Class.forName(<span class="string">"org.apache.hive.jdbc.HiveDriver"</span>);</span><br><span class="line"></span><br><span class="line">Connection conn = DriverManager.getConnection(<span class="string">"jdbc:hive2://cs1:10000"</span>,<span class="string">"ap"</span>,<span class="string">"123"</span>);</span><br><span class="line"></span><br><span class="line">Statement st = conn.createStatement();</span><br><span class="line"></span><br><span class="line">ResultSet rs = st.executeQuery(<span class="string">"select * from tt where age &gt; 12 ORDER BY age DESC "</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (rs.next()) &#123;</span><br><span class="line">    <span class="keyword">int</span> id = rs.getInt(<span class="number">1</span>);</span><br><span class="line">    String name = rs.getString(<span class="number">2</span>);</span><br><span class="line">    <span class="keyword">int</span> age = rs.getInt(<span class="number">3</span>);</span><br><span class="line">    System.out.println(id + <span class="string">","</span> + name + <span class="string">","</span> + age);</span><br><span class="line">&#125;</span><br><span class="line">rs.close();</span><br></pre></td></tr></table></figure><h4 id="5-直接使用-SPARK-HOME-bin-spark-sql-脚本访问-hive"><a href="#5-直接使用-SPARK-HOME-bin-spark-sql-脚本访问-hive" class="headerlink" title="5. 直接使用  $SPARK_HOME/bin/spark-sql 脚本访问 hive"></a>5. 直接使用  <code>$SPARK_HOME/bin/spark-sql</code> 脚本访问 hive</h4><p><strong>1.配置 <code>hive-site.xml</code>, 分发到各个节点</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--加上一下配置, 指明访问 hive 的 metadata 服务--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://cs2:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>2.在装有 hive &amp; mysql 的节点上启动 hive 的 metastore 服务</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup hive --service metastore 1&gt;/home/ap/logs/hive_thriftserver.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p><strong>3.在 cs1节点启动 <code>spark-sql</code>服务, 即可进入 <code>spark-sql</code> 命令行模式</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$cs1</span>&gt; spark-sql</span><br></pre></td></tr></table></figure><p><strong>4.在 idea 上直接操作 hive 有问题</strong></p><p>spark-hive_2.1.1 pom 导入失败</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一-Spark-核心-API&quot;&gt;&lt;a href=&quot;#一-Spark-核心-API&quot; class=&quot;headerlink&quot; title=&quot;一. Spark 核心 API&quot;&gt;&lt;/a&gt;一. Spark 核心 API&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;[SparkContex
      
    
    </summary>
    
      <category term="Spark" scheme="https://airpoet.github.io/categories/Spark/"/>
    
      <category term="SparkSQL" scheme="https://airpoet.github.io/categories/Spark/SparkSQL/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Spark" scheme="https://airpoet.github.io/tags/Spark/"/>
    
      <category term="SparkSQL" scheme="https://airpoet.github.io/tags/SparkSQL/"/>
    
      <category term="Spark源码" scheme="https://airpoet.github.io/tags/Spark%E6%BA%90%E7%A0%81/"/>
    
  </entry>
  
  <entry>
    <title>i-Spark-1</title>
    <link href="https://airpoet.github.io/2018/07/20/Spark/i-Spark-1/"/>
    <id>https://airpoet.github.io/2018/07/20/Spark/i-Spark-1/</id>
    <published>2018-07-20T02:05:35.625Z</published>
    <updated>2018-07-25T14:34:33.915Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Tips:</p><blockquote><p> <strong>并行</strong></p></blockquote><p>集群计算。 </p><p>并行计算。 </p><p>硬件方面的概念 </p><blockquote><p><strong>并发</strong></p></blockquote><p>并发执行。  </p><p>线程方面的概念 </p></blockquote><h1 id="一-Spark-简介"><a href="#一-Spark-简介" class="headerlink" title="一. Spark 简介"></a>一. Spark 简介</h1><h2 id="1-概念"><a href="#1-概念" class="headerlink" title="1.概念"></a>1.概念</h2><p>Lightning-fast cluster computing。 </p><p><strong>快如闪电的集群计算。</strong> </p><p><strong>大规模快速通用的计算引擎。</strong> </p><p><strong>速度:</strong>    比hadoop 100x,磁盘计算快10x </p><p><strong>使用:</strong>    java / Scala /R /python    </p><p>​                提供80+算子(操作符)，容易构建并行应用。 </p><p><strong>通用:</strong>    组合SQL ，流计算 + 复杂分析。 </p><p><strong>DAG:</strong>     direct acycle graph,有向无环图。</p><p><strong>Spark 与 MR 关系</strong>:     基于hadoop的mr，扩展MR模型高效使用MR模型，内存型集群计算，提高app处理速度。 </p><h2 id="2-Spark-模块"><a href="#2-Spark-模块" class="headerlink" title="2.Spark 模块"></a>2.Spark 模块</h2><blockquote><p>Spark core                //核心模块 ,通用执行引擎，提供内存计算和对外部数据集的引用。RDD</p><p>Spark SQL               //SQL ,构建在core之上，引入新的抽象SchemaRDD，提供了结构化和半结构化支持。</p><p>Spark Streaming       //流计算  DStream</p><p>Spark MLlib                //机器学习 </p><p>Spark graph             //图计算 </p></blockquote><h2 id="3-RDD"><a href="#3-RDD" class="headerlink" title="3.RDD"></a>3.RDD</h2><p>是spark的基本数据结构，是不可变数据集。</p><p>RDD中的数据集进行逻辑分区，每个分区可以单独在集群节点进行计算。</p><p>可以包含任何java,scala，python和自定义类型。 </p><p>RDD是只读的记录分区集合。RDD具有容错机制。 </p><p><strong>创建RDD方式:</strong>  1)并行化一个现有集合.  2) 由另一个 RDD 转换</p><p>hadoop 花费90%时间 read, write。 </p><p>内存处理计算。在job间进行数据共享。内存的IO速率高于网络和disk的10 ~ 100之间。 </p><h2 id="4-Spark-RDD-内部包含5个主要属性"><a href="#4-Spark-RDD-内部包含5个主要属性" class="headerlink" title="4. Spark RDD 内部包含5个主要属性"></a>4. Spark RDD 内部包含5个主要属性</h2><ol><li>分区列表</li><li>针对每个split的计算函数。 </li><li>对其他rdd的依赖列表 </li><li>可选，如果是KeyValueRDD的话，可以带分区类。 </li><li>可选，首选块位置列表(hdfs block location); </li></ol><hr><h1 id="二-Spark-安装"><a href="#二-Spark-安装" class="headerlink" title="二. Spark 安装"></a>二. Spark 安装</h1><h2 id="1-local模式"><a href="#1-local模式" class="headerlink" title="1. local模式"></a>1. local模式</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">1).下载spark-2.1.3-bin-hadoop2.7.tgz</span><br><span class="line"></span><br><span class="line">2).解压到/home/ap/apps</span><br><span class="line">  创建符号连接 :  ln -s spark-2.1.3-bin-hadoop2.7 spark</span><br><span class="line"></span><br><span class="line">3).环境变量</span><br><span class="line">    [.zshrc]</span><br><span class="line">    SPARK_HOME=/home/ap/apps/spark</span><br><span class="line">    PATH=<span class="variable">$PATH</span>:<span class="variable">$SPARK_HOME</span>/bin:<span class="variable">$SPARK_HOME</span>/sbin</span><br><span class="line"></span><br><span class="line">[<span class="built_in">source</span>]</span><br><span class="line">$&gt;<span class="built_in">source</span> ~/.zshrc</span><br><span class="line"></span><br><span class="line">4).验证spark</span><br><span class="line">    $&gt;<span class="built_in">cd</span> apps/spark/sbin</span><br><span class="line">    $&gt;./spark-shell</span><br><span class="line"></span><br><span class="line">5).webui</span><br><span class="line">http://cs1:4040/</span><br><span class="line"></span><br><span class="line"><span class="comment"># PS : 启动本地模式,登录4040 webUI端口, 只需要启动spark-shell. 要把master 或者 worker 关掉</span></span><br></pre></td></tr></table></figure><h2 id="2-Stand-alone-分布式"><a href="#2-Stand-alone-分布式" class="headerlink" title="2. Stand alone 分布式"></a>2. Stand alone 分布式</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">1). 配置其他主机的所有环境变量, 发送cs1上的.zshrc 到其它机器</span><br><span class="line">--------------------</span><br><span class="line">    <span class="comment"># 发送</span></span><br><span class="line">    &lt;sub&gt;&gt; xsync.sh &lt;/sub&gt;/.zshrc</span><br><span class="line">    <span class="comment"># source</span></span><br><span class="line">    &lt;sub&gt;&gt; xcall.sh <span class="string">"source &lt;/sub&gt;/.zshrc"</span></span><br><span class="line"></span><br><span class="line">2). 配置master节点的slaves</span><br><span class="line">--------------------</span><br><span class="line">    [/soft/spark/conf/slaves]</span><br><span class="line">    cs2</span><br><span class="line">    cs3</span><br><span class="line">    cs4</span><br><span class="line">    cs5</span><br><span class="line"></span><br><span class="line">3). 发送文件到其它节点</span><br><span class="line">--------------------</span><br><span class="line">    ~&gt; xsync.sh apps/spark</span><br><span class="line"></span><br><span class="line">4). 启动spark集群</span><br><span class="line">--------------------</span><br><span class="line">    ~&gt; apps/spark/sbin/start-all.sh</span><br><span class="line"></span><br><span class="line">5). 查看进程</span><br><span class="line">--------------------</span><br><span class="line">    $&gt;xcall.jps jps</span><br><span class="line">    master        //cs1</span><br><span class="line">    worker        //cs2</span><br><span class="line">    worker        //cs3</span><br><span class="line">    worker        //cs4</span><br><span class="line"></span><br><span class="line">6). 查看 webUI</span><br><span class="line">--------------------</span><br><span class="line">    http://cs1:8080</span><br></pre></td></tr></table></figure><h2 id="3-Stand-alone-HA-模式"><a href="#3-Stand-alone-HA-模式" class="headerlink" title="3. Stand alone HA 模式"></a>3. Stand alone HA 模式</h2><blockquote><p><strong>规划:</strong> </p><p>cs1, cs6 为 master</p><p>cs2~cs5 为 worker</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">1). 配置 HADOOP_CONF_DIR, 目的是找到 hdfs-site.xml 和 core-site.xml</span><br><span class="line">--------------<span class="comment"># conf/spark-env.sh 中----------------</span></span><br><span class="line">    <span class="comment"># 配 HADOOP_CONF_DIR, 这样就不用拷贝hdfs-site.xml 和 core-site.xml 到 conf 目录了</span></span><br><span class="line">    <span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$HADOOP_HOME</span>/etc/hadoop</span><br><span class="line"></span><br><span class="line">    ps: 也可以直接复制 hdfs-site.xml 和 core-site.xml 到 spark/conf 下, 可能还得在 conf/defaults.conf中指定 </span><br><span class="line">    spark.files  file:///home/ap/apps/spark/conf/hdfs-site.xml,file:///home/ap/apps/spark/conf/core-site.xml ,但是不推荐这样</span><br><span class="line"></span><br><span class="line">2). 配置 JAVA_HOME &amp; zookeeper</span><br><span class="line">--------------<span class="comment"># conf/spark-env.sh 中----------------</span></span><br><span class="line">    <span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/jdk1.8.0_73</span><br><span class="line">    <span class="built_in">export</span> SPARK_DAEMON_JAVA_OPTS=<span class="string">"-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=cs1,cs2,cs3  -Dspark.deploy.zookeeper.dir=/spark"</span></span><br><span class="line"></span><br><span class="line">3) 分发整个 conf 目录文件到集群</span><br><span class="line">------------------------------------------------</span><br><span class="line"><span class="variable">$cs1</span> &lt;sub&gt;&gt; xsync.sh &lt;/sub&gt;/apps/spark/conf</span><br><span class="line"></span><br><span class="line">4) 启动集群</span><br><span class="line">------------------------------------------------</span><br><span class="line">cs1&gt; ~/apps/spark/sbin/start-all.sh</span><br><span class="line">cs6&gt; ~/apps/spark/sbin/start-master.sh</span><br><span class="line">查看&gt; </span><br><span class="line">http://cs1:8080</span><br><span class="line">http://cs6:8080</span><br><span class="line"></span><br><span class="line">5) 测试: 启动spark-shell,连接spark集群上</span><br><span class="line">------------------------------------------------ </span><br><span class="line">$&gt; spark-shell --master spark://cs1:7077</span><br><span class="line">$&gt; sc.textFile(<span class="string">"hdfs://mycluster/user/ap/a.txt"</span>).collect();</span><br></pre></td></tr></table></figure><h2 id="4-Spark-历史服务器-可选"><a href="#4-Spark-历史服务器-可选" class="headerlink" title="4. Spark 历史服务器(可选)"></a>4. Spark 历史服务器(可选)</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">第一步：</span><br><span class="line">------------------------------------------------ </span><br><span class="line"><span class="built_in">cd</span> /home/hadoop/apps/spark-2.3.0-bin-hadoop2.7/conf</span><br><span class="line">cp spark-defaults.conf.template    spark-defaults.conf</span><br><span class="line"></span><br><span class="line">在文件里面添加如下内容：</span><br><span class="line">spark.eventLog.enabled        <span class="literal">true</span></span><br><span class="line">spark.eventLog.dir            hdfs://mycluster/sparklog</span><br><span class="line"></span><br><span class="line">第二步：</span><br><span class="line">------------------------------------------------ </span><br><span class="line">在 spark-env.sh 的文件里面添加如下内容：</span><br><span class="line"><span class="built_in">export</span> SPARK_HISTORY_OPTS=<span class="string">"-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=30 -Dspark.history.fs.logDirectory=hdfs://mycluster/sparklog"</span></span><br><span class="line"></span><br><span class="line">第三步：</span><br><span class="line">------------------------------------------------ </span><br><span class="line">在启动 HistorServer 服务之前 hdfs://mycluster/sparklog 目录要提前创建</span><br><span class="line">hadoop fs -mkdir -p hdfs://mycluster/sparklog</span><br><span class="line"></span><br><span class="line">第四步：启动 Spark HistoryServer</span><br><span class="line">------------------------------------------------ </span><br><span class="line">$&gt; SPARK_HOME/sbin/start-history-server.sh</span><br><span class="line"></span><br><span class="line">第五步：访问 Spark History WebUI</span><br><span class="line">------------------------------------------------ </span><br><span class="line">http://cs1:18080/</span><br></pre></td></tr></table></figure><h2 id="5-yarn-模式"><a href="#5-yarn-模式" class="headerlink" title="5. yarn 模式"></a>5. yarn 模式</h2><p>…</p><hr><h1 id="三-Spark-简单体验"><a href="#三-Spark-简单体验" class="headerlink" title="三. Spark 简单体验"></a>三. Spark 简单体验</h1><h2 id="1-API"><a href="#1-API" class="headerlink" title="1. API"></a>1. API</h2><p><strong>[SparkContext]</strong></p><ul><li>Spark功能的主要入口点。代表到Spark集群的连接，可以创建RDD、累加器和广播变量. </li><li>每个JVM只能激活一个SparkContext对象，在创建sc之前需要stop掉active的sc。 </li></ul><p><strong>[RDD]</strong></p><ul><li>resilient distributed dataset,弹性分布式数据集。等价于集合。 </li></ul><p><strong>[SparkConf]</strong></p><ul><li>spark配置对象，设置Spark应用各种参数，kv形式。 </li></ul><h2 id="2-Spark-实现-WordCount"><a href="#2-Spark-实现-WordCount" class="headerlink" title="2. Spark 实现 WordCount"></a>2. Spark 实现 WordCount</h2><h4 id="2-1-spark-shell"><a href="#2-1-spark-shell" class="headerlink" title="2.1 spark-shell"></a>2.1 spark-shell</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 登录 shell </span></span><br><span class="line">spark-shell --master spark:<span class="comment">//cs1:7077</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//加载文本文件,以换行符方式切割文本.Array(hello  world2,hello world2 ,...)</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"/home/ap/a.txt"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//单词统计1 =&gt; 分步骤</span></span><br><span class="line">    $scala&gt;<span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"/home/centos/test.txt"</span>)</span><br><span class="line">    $scala&gt;<span class="keyword">val</span> rdd2 = rdd1.flatMap(line=&gt;line.split(<span class="string">" "</span>))</span><br><span class="line">    $scala&gt;<span class="keyword">val</span> rdd3 = rdd2.map(word = &gt; (word,<span class="number">1</span>))</span><br><span class="line">    $scala&gt;<span class="keyword">val</span> rdd4 = rdd3.reduceByKey(_ + _)</span><br><span class="line">    $scala&gt;rdd4.collect</span><br><span class="line"></span><br><span class="line"><span class="comment">// 一句话</span></span><br><span class="line">    scala&gt; sc.textFile(<span class="string">"/home/ap/a.txt"</span>).flatMap(_.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>)).reduceByKey(_+_).collect</span><br><span class="line">    res6: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((world2,<span class="number">1</span>), (world1,<span class="number">1</span>), (world4,<span class="number">1</span>), (<span class="string">""</span>,<span class="number">1</span>), (hello,<span class="number">4</span>), (world3,<span class="number">1</span>))</span><br></pre></td></tr></table></figure><h4 id="2-2-idea-编程"><a href="#2-2-idea-编程" class="headerlink" title="2.2 idea 编程"></a>2.2 idea 编程</h4><blockquote><p><strong>[pom依赖文件]</strong></p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>Scala 版</strong></p></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCountDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//创建Spark配置对象</span></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>();</span><br><span class="line">        conf.setAppName(<span class="string">"WordCountSpark"</span>)</span><br><span class="line">        <span class="comment">//设置master属性</span></span><br><span class="line">        conf.setMaster(<span class="string">"local"</span>) ;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//通过conf创建sc</span></span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//加载文本文件</span></span><br><span class="line">        <span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"d:/scala/test.txt"</span>);</span><br><span class="line">        <span class="comment">//压扁</span></span><br><span class="line">        <span class="keyword">val</span> rdd2 = rdd1.flatMap(line =&gt; line.split(<span class="string">" "</span>)) ;</span><br><span class="line">        <span class="comment">//映射w =&gt; (w,1)</span></span><br><span class="line">        <span class="keyword">val</span> rdd3 = rdd2.map((_,<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> rdd4 = rdd3.reduceByKey(_ + _)</span><br><span class="line">        <span class="keyword">val</span> r = rdd4.collect()</span><br><span class="line">        r.foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p><strong>Java 版</strong></p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">     * java版</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountJava2</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//创建SparkConf对象</span></span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">        conf.setAppName(<span class="string">"WordCountJava2"</span>);</span><br><span class="line">        conf.setMaster(<span class="string">"local"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建java sc</span></span><br><span class="line">        JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">        <span class="comment">//加载文本文件</span></span><br><span class="line">        JavaRDD&lt;String&gt; rdd1 = sc.textFile(<span class="string">"d:/scala//test.txt"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//压扁</span></span><br><span class="line">        JavaRDD&lt;String&gt; rdd2 = rdd1.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Iterator&lt;String&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                List&lt;String&gt; list = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">                String[] arr = s.split(<span class="string">" "</span>);</span><br><span class="line">                <span class="keyword">for</span>(String ss :arr)&#123;</span><br><span class="line">                    list.add(ss);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> list.iterator();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//映射,word -&gt; (word,1)</span></span><br><span class="line">        JavaPairRDD&lt;String,Integer&gt; rdd3 = rdd2.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(s,<span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//reduce化简</span></span><br><span class="line">        JavaPairRDD&lt;String,Integer&gt; rdd4 = rdd3.reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer v1, Integer v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//</span></span><br><span class="line">        List&lt;Tuple2&lt;String,Integer&gt;&gt; list = rdd4.collect();</span><br><span class="line">        <span class="keyword">for</span>(Tuple2&lt;String, Integer&gt; t : list)&#123;</span><br><span class="line">            System.out.println(t._1() + <span class="string">" : "</span> + t._2());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p><strong>提交作业到 Spark 集群运行</strong></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">======================================提交到本机</span><br><span class="line">1) 导出jar包</span><br><span class="line">-------------------------------</span><br><span class="line">2) spark-submit提交命令运行job</span><br><span class="line">-------------------------------</span><br><span class="line">    <span class="comment"># scala 版 wordcount</span></span><br><span class="line">    spark-submit --master <span class="built_in">local</span> --name MyWordCount --class MyWordCount_Scala SparkDemo-1-1.0-SNAPSHOT.jar /home/ap/a.txt</span><br><span class="line"></span><br><span class="line">    <span class="comment"># java 版 wordcount</span></span><br><span class="line">    spark-submit --master <span class="built_in">local</span> --name MyWordCountJava --class  com.rox.spark.WordCountJava SparkDemo-1-1.0-SNAPSHOT.jar /home/ap/a.txt</span><br><span class="line"></span><br><span class="line">======================================提交到Spark集群</span><br><span class="line">1) 代码中把textFile改为 args[0]</span><br><span class="line"></span><br><span class="line">2) 将源文件传到 hdfs 上</span><br><span class="line"></span><br><span class="line">3) 运行spark-submit</span><br><span class="line">    $&gt; spark-submit  --master  spark://cs1:7077 --name MyWordCount2 --class MyWordCount_Scala  SparkDemo-1-1.0-SNAPSHOT.jar hdfs://cs1:8020/user/ap/a.txt</span><br></pre></td></tr></table></figure><h2 id="3-spark-sbin-下脚本分析"><a href="#3-spark-sbin-下脚本分析" class="headerlink" title="3. spark/sbin 下脚本分析"></a>3. spark/sbin 下脚本分析</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">[start-all.sh]</span><br><span class="line">-------------------------------</span><br><span class="line">    sbin/spark-config.sh</span><br><span class="line">    sbin/spark-master.sh        //启动master进程</span><br><span class="line">    sbin/spark-slaves.sh        //启动worker进程</span><br><span class="line"></span><br><span class="line">[start-master.sh]</span><br><span class="line">-------------------------------</span><br><span class="line">    sbin/spark-config.sh</span><br><span class="line">    org.apache.spark.deploy.master.Master</span><br><span class="line">    spark-daemon.sh start org.apache.spark.deploy.master.Master --host --port --webui-port ...</span><br><span class="line"></span><br><span class="line">[spark-slaves.sh]</span><br><span class="line">-------------------------------</span><br><span class="line">    sbin/spark-config.sh</span><br><span class="line">    slaves.sh                //conf/slaves</span><br><span class="line"></span><br><span class="line">[slaves.sh]</span><br><span class="line">-------------------------------</span><br><span class="line">    <span class="keyword">for</span> conf/slaves &#123;</span><br><span class="line">     ssh host start-slave.sh ...</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">[start-slave.sh]</span><br><span class="line">-------------------------------</span><br><span class="line">    CLASS=<span class="string">"org.apache.spark.deploy.worker.Worker"</span></span><br><span class="line">    sbin/spark-config.sh</span><br><span class="line">    <span class="keyword">for</span> ((  .. )) ; <span class="keyword">do</span></span><br><span class="line">    start_instance $(( 1 + <span class="variable">$i</span> )) <span class="string">"<span class="variable">$@</span>"</span></span><br><span class="line">    <span class="keyword">done</span> </span><br><span class="line"></span><br><span class="line">    $&gt;<span class="built_in">cd</span> /soft/spark/sbin</span><br><span class="line">    $&gt;./stop-all.sh                //停掉整个spark集群.</span><br><span class="line">    $&gt;./start-master.sh            //停掉整个spark集群.</span><br><span class="line">    $&gt;./start-master.sh            //启动master节点</span><br><span class="line">    $&gt;./start-slaves.sh            //启动所有worker节点</span><br><span class="line"></span><br><span class="line">    $&gt;./stop-slave.sh --<span class="built_in">help</span>  // 查看帮助, 其它的查看帮助也是这样, 命令后面加上 --<span class="built_in">help</span></span><br></pre></td></tr></table></figure><h2 id="4-解决数据倾斜问题-多次-map-reduce"><a href="#4-解决数据倾斜问题-多次-map-reduce" class="headerlink" title="4. 解决数据倾斜问题 (多次 map-reduce)"></a>4. 解决数据倾斜问题 (多次 map-reduce)</h2><blockquote><p> 解决数据倾斜:</p><p>1&gt; 先对每个 word 添加 个数 (word,1)</p><p>2&gt; 取出 word, 在 word后随机拼接 _num, 然后再与 v 组成 (newK, v)元祖</p><p>3&gt; 对新的 (k,v)进行 reduceByKey, 统计出每个 k 的个数 (newK, count)</p><p>4&gt; 对结果继续进行 map, 去掉 _num, 把原本的 k 取出来, 再跟 count 组成新的元祖 (k,count)</p><p>5&gt; 继续进行 reduceByKey, 此时并发量最多也只是随机数的个数了, 不会产生严重的数据倾斜, 计算出最终的结果</p><p>6&gt; 存到文件中 saveAsTextFile</p></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 执行防止数据倾斜的 MR, 最后存到 hdfs 上</span></span><br><span class="line">sc.textFile(<span class="string">"hdfs://mycluster/user/ap/a.txt"</span>,<span class="number">4</span>).flatMap(_.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>)).map(t =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> w1 = t._1; </span><br><span class="line">  <span class="keyword">import</span> scala.util.<span class="type">Random</span>;</span><br><span class="line">  <span class="keyword">val</span> n = <span class="type">Random</span>.nextInt(<span class="number">100</span>); </span><br><span class="line">  (w1 + <span class="string">"_"</span> + n, t._2);</span><br><span class="line">&#125;).reduceByKey(_ + _,<span class="number">4</span>).map(t =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> w2 = t._1; </span><br><span class="line">  <span class="keyword">val</span> count = t._2; </span><br><span class="line">  <span class="keyword">val</span> w3 = w2.split(<span class="string">"_"</span>)(<span class="number">0</span>); </span><br><span class="line">  (w3, count);</span><br><span class="line">&#125;).reduceByKey(_ + _,<span class="number">4</span>).saveAsTextFile(<span class="string">"/user/ap/scala/DataSkew"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 查看 hdfs 上的数据</span></span><br><span class="line">[ap<span class="meta">@cs</span>2]~/apps/spark/conf% hdfs dfs -text /user/ap/scala/<span class="type">DataSkew</span>/part<span class="number">-00000</span></span><br><span class="line">(world2,<span class="number">1</span>)</span><br><span class="line">(,<span class="number">1</span>)</span><br></pre></td></tr></table></figure><blockquote><p>执行的 DAG(有向无环图)</p></blockquote><p><img src="" alt=""> </p><hr><h1 id="四-RDD-的-transform-amp-action"><a href="#四-RDD-的-transform-amp-action" class="headerlink" title="四. RDD  的 transform &amp; action"></a>四. RDD  的 transform &amp; action</h1><p><a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations" target="_blank" rel="noopener">详细的见官方文档</a> </p><p><a href="https://github.com/airpoet/bigdata/tree/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/scala" target="_blank" rel="noopener">我的github代码</a></p><h2 id="1-变换-transform"><a href="#1-变换-transform" class="headerlink" title="1. 变换 (transform )"></a>1. 变换 (transform )</h2><p><strong>注意:  直到遇到一个不是返回 RDD 对象的方法/函数,  才会开始计算</strong></p><p>Spark is lazy, so nothing will be executed unless you call some transformation or action that will trigger job creation and execution. Look at the following snippet of the word-count example. </p><p><strong>变换: 返回指向新rdd的指针，在rdd之间创建依赖关系。每个rdd都有计算函数和指向父RDD的指针。</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">map()                                    <span class="comment">//对每个元素进行变换，应用变换函数</span></span><br><span class="line">                                         <span class="comment">//(T)=&gt;V</span></span><br><span class="line">--------------------------------------------------------------------------------------------------------------</span><br><span class="line">filter()                                 <span class="comment">//过滤器,(T)=&gt;Boolean</span></span><br><span class="line">--------------------------------------------------------------------------------------------------------------</span><br><span class="line">flatMap()                                <span class="comment">//压扁,T =&gt; TraversableOnce[U]</span></span><br><span class="line">--------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">mapPartitions()                          <span class="comment">//对每个分区进行应用变换，输入的Iterator,返回新的迭代器，可以对分区进行函数处理。</span></span><br><span class="line">                                         <span class="comment">//Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt;</span></span><br><span class="line">--------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">mapPartitionsWithIndex(func)             <span class="comment">//同上，(Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt;</span></span><br><span class="line">--------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">sample(withReplacement, fraction, seed)    <span class="comment">//采样返回采样的RDD子集。</span></span><br><span class="line">                                        <span class="comment">//withReplacement 元素是否可以多次采样.</span></span><br><span class="line">                                        <span class="comment">//fraction : 期望采样数量.[0,1]</span></span><br><span class="line">--------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">union()                                    <span class="comment">//类似于mysql union操作。</span></span><br><span class="line">                                        <span class="comment">//select * from persons where id &lt; 10 </span></span><br><span class="line">                                        <span class="comment">//union select * from id persons where id &gt; 29 ;</span></span><br><span class="line">--------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">intersection                            <span class="comment">//交集,提取两个rdd中都含有的元素。</span></span><br><span class="line">--------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">distinct([numTasks]))                    <span class="comment">//去重,去除重复的元素。</span></span><br><span class="line">--------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">groupByKey()                            <span class="comment">//(K,V) =&gt; (K,Iterable&lt;V&gt;)</span></span><br><span class="line">--------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">reduceByKey(*)                            <span class="comment">//按key聚合。 </span></span><br><span class="line">--------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])      <span class="comment">//按照key进行聚合  :TODO 没完全清楚</span></span><br><span class="line">key:<span class="type">String</span> <span class="type">U</span>:<span class="type">Int</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">其实reduceBykey就是aggregateByKey的简化版。 </span><br><span class="line">就是aggregateByKey多提供了一个函数 seqOp</span><br><span class="line">类似于<span class="type">Mapreduce</span>的combine操作（就在map端执行reduce的操作）</span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">sortByKey                                <span class="comment">//排序</span></span><br><span class="line">--------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">join(otherDataset, [numTasks])            <span class="comment">//连接,(K,V).join(K,W) =&gt;(K,(V,W)) </span></span><br><span class="line">--------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">cogroup                                    <span class="comment">//协分组</span></span><br><span class="line">                                        <span class="comment">//(K,V).cogroup(K,W) =&gt;(K,(Iterable&lt;V&gt;,Iterable&lt;!-- &lt;W&gt; --&gt;)) </span></span><br><span class="line">--------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">cartesian(otherDataset)                    <span class="comment">//笛卡尔积,RR[T] RDD[U] =&gt; RDD[(T,U)]</span></span><br><span class="line">--------------------------------------------------------------------------------------------------------------</span><br><span class="line">pipe                                    <span class="comment">//将rdd的元素传递给脚本或者命令，执行结果返回形成新的RDD</span></span><br><span class="line"><span class="comment">//例子:  在 spark-shell 下</span></span><br><span class="line">scala&gt; sc.parallelize(<span class="type">Array</span>(<span class="string">"/home/ap"</span>)).pipe(<span class="string">"ls"</span>).collect</span><br><span class="line">res0: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(apps, a.txt, calllog, dump.rdb, flumedata, hadoopdata, ihivedata, jars, kafka, logs, python, softs, <span class="type">SparkDemo</span><span class="number">-1</span><span class="number">-1.0</span>-<span class="type">SNAPSHOT</span>.jar, spool, zookeeper, zookeeper.out)</span><br><span class="line">--------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">coalesce(numPartitions)                    <span class="comment">//减少分区</span></span><br><span class="line">--------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">repartition                                <span class="comment">//可增可减</span></span><br><span class="line">--------------------------------------------------------------------------------------------------------------</span><br><span class="line">repartitionAndSortWithinPartitions(partitioner)              <span class="comment">//再分区并在分区内进行排序</span></span><br></pre></td></tr></table></figure><h2 id="2-产生作业-Actions"><a href="#2-产生作业-Actions" class="headerlink" title="2. 产生作业 Actions"></a>2. 产生作业 Actions</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">collect()                                <span class="comment">//收集rdd元素形成数组.</span></span><br><span class="line"></span><br><span class="line">count()                                    <span class="comment">//统计rdd元素的个数</span></span><br><span class="line"></span><br><span class="line">reduce()                                 <span class="comment">//聚合,返回一个值。</span></span><br><span class="line"></span><br><span class="line">first                                    <span class="comment">//取出第一个元素take(1)</span></span><br><span class="line"></span><br><span class="line">take                                     <span class="comment">// take(n) , 是一个集合, 需要迭代打印</span></span><br><span class="line"></span><br><span class="line">takeSample (withReplacement,num, [seed])</span><br><span class="line"></span><br><span class="line">takeOrdered(n, [ordering])               <span class="comment">// 返回前 n 个排序的元素(可默认, 可自定义排序)</span></span><br><span class="line"></span><br><span class="line">saveAsTextFile(path)                    <span class="comment">//保存到文件                    生产中使用</span></span><br><span class="line"></span><br><span class="line">saveAsSequenceFile(path)                <span class="comment">//保存成序列文件</span></span><br><span class="line"></span><br><span class="line">saveAsObjectFile(path) (<span class="type">Java</span> and <span class="type">Scala</span>)</span><br><span class="line"></span><br><span class="line">countByKey()                            <span class="comment">//按照key,统计每个key下value的个数</span></span><br><span class="line"></span><br><span class="line">reduceByKey()                           <span class="comment">//按照 key, 计算 key 后的 value </span></span><br><span class="line"></span><br><span class="line">注意 : rdd2.map((_, <span class="number">1</span>)) 和 rdd2.map((_, <span class="number">2</span>)) 针对于 前两者的计算结果是不一样的</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;Tips:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt; &lt;strong&gt;并行&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;集群计算。 &lt;/p&gt;
&lt;p&gt;并行计算。 &lt;/p&gt;
&lt;p&gt;硬件方面的概念 &lt;/p&gt;
&lt;blockquote&gt;
&lt;p
      
    
    </summary>
    
      <category term="Spark" scheme="https://airpoet.github.io/categories/Spark/"/>
    
      <category term="SparkCore" scheme="https://airpoet.github.io/categories/Spark/SparkCore/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Spark" scheme="https://airpoet.github.io/tags/Spark/"/>
    
      <category term="SparkCore" scheme="https://airpoet.github.io/tags/SparkCore/"/>
    
  </entry>
  
  <entry>
    <title>Spark中Bug集锦</title>
    <link href="https://airpoet.github.io/2018/07/13/Spark/Spark%E4%B8%ADBug%E9%9B%86%E9%94%A6/"/>
    <id>https://airpoet.github.io/2018/07/13/Spark/Spark中Bug集锦/</id>
    <published>2018-07-13T06:22:08.652Z</published>
    <updated>2018-07-25T09:30:34.264Z</updated>
    
    <content type="html"><![CDATA[<h4 id="1-创建-SparkContext-时-遇到的问题"><a href="#1-创建-SparkContext-时-遇到的问题" class="headerlink" title="1.创建 SparkContext 时, 遇到的问题"></a>1.创建 SparkContext 时, 遇到的问题</h4><p><strong>错误描述:</strong> <code>A master URL must be set in your configuration</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建 spark 配置对象</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">conf.setAppName(<span class="string">"MyWordCount_Scala"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//设置 master 属性</span></span><br><span class="line">conf.setMaster(<span class="string">"local"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 通过 conf 创建 sc (注意, 这里一定要传入 conf, 否则就会报下面这个错误)</span></span><br><span class="line"><span class="comment">// A master URL must be set in your configuration</span></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br></pre></td></tr></table></figure><h4 id="2-配置-Spark-独立模式时-找不到-JAVA-HOME"><a href="#2-配置-Spark-独立模式时-找不到-JAVA-HOME" class="headerlink" title="2. 配置 Spark 独立模式时, 找不到 JAVA_HOME"></a>2. 配置 Spark 独立模式时, 找不到 JAVA_HOME</h4><p><strong>错误描述: <code>JAVA_HOME is not set</code></strong></p><p><em>解决方法</em>: </p><ul><li><strong>在 <code>sbin</code>目录中,  在<code>spark-config.sh</code>中加上</strong> <strong><code>export JAVA_HOME=...</code></strong></li><li>分发到其它主机</li></ul><h4 id="3-一个-JVM-中如果存在多个-SparkContext"><a href="#3-一个-JVM-中如果存在多个-SparkContext" class="headerlink" title="3.一个 JVM 中如果存在多个 SparkContext"></a>3.一个 JVM 中如果存在多个 SparkContext</h4><p>Exception in thread “main” org.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:</p><p>原因是因为, 每个 static 方法中, 都创建了 sc, 当然, 这里只是测试用, 可忽略, 见这里: </p><p><a href="https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/java/TransfromationOperation.java" target="_blank" rel="noopener">https://github.com/airpoet/bigdata/blob/master/Spark_Project/SparkDemo-1/src/main/java/com/rox/spark/java/TransfromationOperation.java</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;1-创建-SparkContext-时-遇到的问题&quot;&gt;&lt;a href=&quot;#1-创建-SparkContext-时-遇到的问题&quot; class=&quot;headerlink&quot; title=&quot;1.创建 SparkContext 时, 遇到的问题&quot;&gt;&lt;/a&gt;1.创建 Spark
      
    
    </summary>
    
      <category term="Spark" scheme="https://airpoet.github.io/categories/Spark/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Spark" scheme="https://airpoet.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>i-Scala</title>
    <link href="https://airpoet.github.io/2018/07/12/Hadoop/12_Scala/i-Scala/"/>
    <id>https://airpoet.github.io/2018/07/12/Hadoop/12_Scala/i-Scala/</id>
    <published>2018-07-12T02:05:03.898Z</published>
    <updated>2018-07-17T01:45:44.012Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一-概览"><a href="#一-概览" class="headerlink" title="一. 概览"></a>一. <strong>概览</strong></h1><p><strong>scala</strong> : java语言的脚本化。          </p><p><strong>Scala 的类关系图</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-12-061916.png" alt="image-20180712141916222"></p><p><strong>数据类型注意点</strong></p><p>1、 Any 是所有类的父类，包括值类型 AnyVal，和引用类型 AnyRef<br>2、 AnyVal 是所有值类型的父类，包括 Int，Double，Boolean，Unit 等等<br>3、 AnyRef 是所有引用类型的父类，包括 Null<br>4、 Null 是所有引用类型的子类<br>5、 Nothing 是所有类的子类<br>6、 Unit 类型只有一个实例，是()，相当于 java 中的 void，没有任何的实质意义<br>7、 Null 也只有一个实例，是 null，相当于 java 中的 null，能赋值给任何引用类型变量，不<br>能赋值给值类型变量</p><p><strong>方法的返回值:</strong> </p><ul><li>如果没有显示定义返回值, 会返回 <strong>有可能返回 的 值</strong> <strong>的</strong>共同类型(<strong>父类</strong>)</li></ul><p><strong>定义方法</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-12-081734.png" alt="image-20180712161733886"></p><p><strong>定义函数</strong></p><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-12-152103.png" alt="image-20180712232102657"></p><h4 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h4> <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//变量</span></span><br><span class="line">scala&gt;<span class="keyword">var</span> a = <span class="number">100</span>            <span class="comment">//变量</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//常量</span></span><br><span class="line">scala&gt;<span class="keyword">val</span> a = <span class="number">100</span>            <span class="comment">//常量，不能重新赋值。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//定义类型  //val 常量, 不能再重新赋值</span></span><br><span class="line">scala&gt;<span class="keyword">val</span> a:<span class="type">String</span> = <span class="string">"hello"</span> ;</span><br><span class="line"></span><br><span class="line"><span class="comment">//操作符重载 </span></span><br><span class="line">    scala&gt;<span class="number">1</span> + <span class="number">2</span></span><br><span class="line">    scala&gt;<span class="number">1.</span>+(<span class="number">2</span>)          </span><br><span class="line"></span><br><span class="line"><span class="comment">//scala方法，可以直接调用</span></span><br><span class="line">    scala&gt;<span class="keyword">import</span> scala.math._        <span class="comment">//_ ===&gt; * 下划线是通配的意思</span></span><br><span class="line">    scala&gt;min(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//apply</span></span><br><span class="line">    scala&gt;<span class="string">"hello"</span>.apply(<span class="number">1</span>)            <span class="comment">//等价于xxx.apply()</span></span><br><span class="line">    scala&gt;<span class="string">"hello"</span>(<span class="number">1</span>)                 </span><br><span class="line"></span><br><span class="line"><span class="comment">//条件表达式,scala的表达式有值,是最后一条语句的值。</span></span><br><span class="line">    scala&gt;<span class="keyword">val</span> x = <span class="number">1</span> ;</span><br><span class="line">scala&gt;<span class="keyword">val</span> b = <span class="keyword">if</span> x &gt; <span class="number">0</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">-1</span> ;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Any 是Int和String的超类。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//类型转换</span></span><br><span class="line">    scala&gt;<span class="number">1.</span>toString()</span><br><span class="line">    scala&gt;<span class="string">"100"</span>.toInt()                <span class="comment">//</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//空值   </span></span><br><span class="line">   scala&gt; <span class="keyword">val</span> y = ()  <span class="comment">//y:Unit= ()类似于java void.</span></span><br><span class="line">   y: <span class="type">Unit</span> = ()</span><br><span class="line"></span><br><span class="line"><span class="comment">//粘贴复制</span></span><br><span class="line">    <span class="comment">// 有左大括号的话, 回车会直接换行</span></span><br><span class="line"><span class="comment">// 进入 paste 模式</span></span><br><span class="line">    scala&gt;:paste</span><br><span class="line">            ....</span><br><span class="line">    ctrl + d                    <span class="comment">//结束粘贴模式</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// scala 的编译</span></span><br><span class="line">    javac               java</span><br><span class="line">    *.java --------&gt; *.<span class="keyword">class</span>  --------&gt;程序</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出</span></span><br><span class="line">    scala&gt;print(<span class="string">"hello"</span>)</span><br><span class="line">    scala&gt;println(<span class="string">"hello"</span>)</span><br><span class="line">    scala&gt;printf(<span class="string">"name is %s , age is %d"</span>, <span class="string">"tom"</span>,<span class="number">12</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//读行</span></span><br><span class="line">    scala&gt;<span class="keyword">val</span> password = readLine(<span class="string">"请输入密码 : "</span>) ;</span><br><span class="line"></span><br><span class="line"><span class="comment">//查看帮助</span></span><br><span class="line">    scala&gt;:help</span><br></pre></td></tr></table></figure><hr><h1 id="二-循环"><a href="#二-循环" class="headerlink" title="二. 循环"></a>二. 循环</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 普通循环</span></span><br><span class="line">    <span class="keyword">var</span> i = <span class="number">0</span> ;</span><br><span class="line">    <span class="keyword">while</span>(i &lt; <span class="number">10</span> )&#123;</span><br><span class="line">        println(i) ;</span><br><span class="line">        i += <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">===================================================================</span><br><span class="line"><span class="comment">// 99表格</span></span><br><span class="line">    <span class="keyword">var</span> row = <span class="number">1</span> ; </span><br><span class="line">    <span class="keyword">while</span>(row &lt;= <span class="number">9</span> )&#123;</span><br><span class="line">        <span class="keyword">var</span> col = <span class="number">1</span> ; </span><br><span class="line">        <span class="keyword">while</span>(col &lt;= row)&#123;</span><br><span class="line">            printf(<span class="string">"%d x %d = %d\t"</span>,col,row,(row * col)) ;</span><br><span class="line">            col += <span class="number">1</span> ;</span><br><span class="line">        &#125;</span><br><span class="line">        println();</span><br><span class="line">        row += <span class="number">1</span> ;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">===================================================================</span><br><span class="line"><span class="comment">// 百钱买百鸡问题</span></span><br><span class="line">    <span class="comment">// 100块钱能各买3种🐓, 各买多少种</span></span><br><span class="line">    <span class="comment">// 公鸡:5块/只</span></span><br><span class="line">    <span class="comment">// 母鸡:3块/只</span></span><br><span class="line">    <span class="comment">// 小鸡:1块/3只</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//公鸡</span></span><br><span class="line"><span class="keyword">var</span> cock = <span class="number">0</span> ;</span><br><span class="line"><span class="keyword">while</span>(cock &lt;= <span class="number">20</span>)&#123;</span><br><span class="line">    <span class="comment">//母鸡</span></span><br><span class="line">    <span class="keyword">var</span> hen = <span class="number">0</span> ;</span><br><span class="line">    <span class="keyword">while</span>(hen &lt;= <span class="number">100</span>/<span class="number">3</span>)&#123;</span><br><span class="line">        <span class="comment">// 小鸡🐥</span></span><br><span class="line">        <span class="keyword">var</span> chicken = <span class="number">0</span> ;</span><br><span class="line">        <span class="keyword">while</span>(chicken &lt;= <span class="number">100</span>)&#123;</span><br><span class="line">            <span class="comment">// 钱数</span></span><br><span class="line">            <span class="keyword">var</span> money = cock * <span class="number">5</span> + hen * <span class="number">3</span> + chicken / <span class="number">3</span> ;</span><br><span class="line">            <span class="comment">// 个数</span></span><br><span class="line">            <span class="keyword">var</span> mount = cock + hen + chicken ;</span><br><span class="line">            <span class="keyword">if</span>(money == <span class="number">100</span> &amp;&amp; mount == <span class="number">100</span>)&#123;</span><br><span class="line">                printf(<span class="string">"cock : %d , hen : %d , chicken : %d\n"</span>,cock,hen,chicken) ;</span><br><span class="line">            &#125;</span><br><span class="line">            chicken += <span class="number">3</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        hen += <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    cock += <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">===================================================================</span><br><span class="line"><span class="comment">//for循环</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// to </span></span><br><span class="line"><span class="comment">// 注意:  &lt;- 是写在一起的 </span></span><br><span class="line">scala&gt; <span class="keyword">for</span> (x  &lt;-  <span class="number">1</span> to <span class="number">10</span>)&#123;</span><br><span class="line">    println(x) ;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">-------------------------------------</span><br><span class="line"><span class="comment">// until   [1,...10)</span></span><br><span class="line"><span class="comment">// 左闭右开 </span></span><br><span class="line"><span class="keyword">for</span> (x &lt;- <span class="number">1</span> until <span class="number">10</span>)&#123;</span><br><span class="line">    println(x) ;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// step 为2 </span></span><br><span class="line"><span class="keyword">for</span> (x &lt;- <span class="number">1</span> until (<span class="number">10</span>,<span class="number">2</span>))&#123;</span><br><span class="line">    println(x) ;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 倒序打印</span></span><br><span class="line"><span class="keyword">for</span> (str &lt;- (<span class="number">1</span> to <span class="number">10</span>).reverse)&#123;</span><br><span class="line">println(str)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用数组下标的方式进行打印</span></span><br><span class="line"><span class="keyword">for</span> (i &lt;- <span class="number">0</span> to arr.length - <span class="number">1</span>)&#123;</span><br><span class="line">println(arr(i))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">-------------------------------------</span><br><span class="line"><span class="comment">//scala没有break continue语句。可以使用Breaks对象的break()方法。</span></span><br><span class="line">scala&gt; <span class="keyword">import</span> scala.util.control.<span class="type">Breaks</span>._</span><br><span class="line">scala&gt; <span class="keyword">for</span>(x &lt;- <span class="number">1</span> to <span class="number">10</span>) &#123;<span class="keyword">if</span> (x&gt;<span class="number">8</span>) <span class="keyword">break</span>() ; print(x)&#125; ;</span><br><span class="line">eg: scala&gt; <span class="keyword">for</span>(x &lt;- <span class="number">1</span> to <span class="number">10</span>) &#123; println(x); <span class="keyword">if</span>(x == <span class="number">5</span>)<span class="keyword">break</span>;&#125;  </span><br><span class="line"><span class="number">12345</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">===================================================================</span><br><span class="line"><span class="comment">//for循环高级</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//双循环,守卫条件</span></span><br><span class="line">scala&gt; <span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">3</span> ; j &lt;- <span class="number">1</span> to <span class="number">4</span> <span class="keyword">if</span> i != j ) &#123;printf(<span class="string">"i = %d, j = %d , res = %d \n"</span>,i,j,i*j);&#125; ;    </span><br><span class="line"></span><br><span class="line">-------------------------------------</span><br><span class="line"><span class="comment">//yield，是循环中处理每个元素，产生新集合</span></span><br><span class="line">scala&gt;<span class="keyword">for</span> (x &lt;- <span class="number">1</span> to <span class="number">10</span> ) <span class="keyword">yield</span> x % <span class="number">2</span> ;</span><br></pre></td></tr></table></figure><h1 id="三-定义函数"><a href="#三-定义函数" class="headerlink" title="三. 定义函数"></a>三. 定义函数</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 方式1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(a:<span class="type">Int</span>,b:<span class="type">Int</span>):<span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> c = a + b  ;</span><br><span class="line">    <span class="keyword">return</span> c  ;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 方式2</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(a:<span class="type">Int</span>,b:<span class="type">Int</span>):<span class="type">Int</span> =  a + b</span><br><span class="line"></span><br><span class="line">===================================================================</span><br><span class="line"></span><br><span class="line"><span class="comment">//scala实现递归</span></span><br><span class="line"></span><br><span class="line">核心:  n! = n * (n - <span class="number">1</span>)!  <span class="comment">// n 的阶乘 = n-1的阶乘, 并且 出口是 n=1</span></span><br><span class="line"><span class="number">4</span>!  = <span class="number">4</span> x <span class="number">3</span>!</span><br><span class="line"><span class="number">4</span>!  = <span class="number">4</span> x <span class="number">3</span> x <span class="number">2</span>!</span><br><span class="line"><span class="number">4</span>!  = <span class="number">4</span> x <span class="number">3</span> x <span class="number">2</span> x <span class="number">1</span>!</span><br><span class="line"></span><br><span class="line"><span class="comment">//递归函数必须显式定义返回类型</span></span><br><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">fac</span></span>(n:<span class="type">Int</span>):<span class="type">Int</span> = <span class="keyword">if</span>(n == <span class="number">1</span>) <span class="number">1</span> <span class="keyword">else</span> n * fac(n<span class="number">-1</span>) ;</span><br><span class="line"></span><br><span class="line">===================================================================</span><br><span class="line"><span class="comment">//函数的默认值和命名参数</span></span><br><span class="line">scala&gt;<span class="function"><span class="keyword">def</span> <span class="title">decorate</span></span>(prefix:<span class="type">String</span> = <span class="string">"[["</span>,str:<span class="type">String</span>,suffix:<span class="type">String</span> = <span class="string">"]]"</span>) = &#123;</span><br><span class="line">        prefix + str + suffix </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果都指定了默认值, 调用的时候 可以不传参数</span></span><br><span class="line"><span class="comment">// 也可以不指定参数名 传任意个数 的参数, 会从第一个开始匹配</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decorate</span></span>(prefix:<span class="type">String</span>=<span class="string">"[["</span>, str:<span class="type">String</span>=<span class="string">"👌"</span>, suffix:<span class="type">String</span>=<span class="string">"]]"</span>)=&#123;</span><br><span class="line">    prefix + str + suffix</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">----调用</span><br><span class="line">scala&gt;decorate(str=<span class="string">"hello"</span>)</span><br><span class="line">scala&gt;decorate(str=<span class="string">"hello"</span>,prefix=<span class="string">"&lt;&lt;"</span>)</span><br><span class="line"></span><br><span class="line">===================================================================</span><br><span class="line"></span><br><span class="line"><span class="comment">//变长参数 (就相当于可以传多个值)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(a:<span class="type">Int</span>*) = &#123;</span><br><span class="line">    <span class="keyword">var</span> s = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(x &lt;- a) s += x;</span><br><span class="line">    s</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">sum(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">sum (<span class="number">1</span> to <span class="number">4</span>:_*)  <span class="comment">// 将1 to 4当做序列处理</span></span><br><span class="line"><span class="comment">// 两者是一样的效果</span></span><br><span class="line"></span><br><span class="line">sum(<span class="number">1</span> to <span class="number">4</span>) <span class="comment">// wrong, 这样是错误的</span></span><br><span class="line"></span><br><span class="line">------------------------------------------------</span><br><span class="line"><span class="comment">// 递归相加(跟上面变长参数循环遍历的效果一样)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(args:<span class="type">Int</span>*):<span class="type">Int</span> = &#123;<span class="keyword">if</span> (args.length == <span class="number">0</span>) <span class="number">0</span> <span class="keyword">else</span> args.head + sum(args.tail:_*)&#125;</span><br><span class="line">----调用</span><br><span class="line">sum (<span class="number">1</span> to <span class="number">4</span>:_*) </span><br><span class="line">===================================================================</span><br><span class="line"></span><br><span class="line"><span class="comment">// 过程 (没有返回值，没有=号)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">out</span></span>(a:<span class="type">Int</span>)&#123;println(a)&#125;</span><br><span class="line">   </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">out</span></span>(a:<span class="type">Int</span>)=&#123;println(a)&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">out</span></span>(a:<span class="type">Int</span>):<span class="type">Unit</span> = &#123;println(a)&#125;</span><br><span class="line"></span><br><span class="line">&gt;&gt; 以上<span class="number">3</span>种方式等价</span><br><span class="line"></span><br><span class="line">===================================================================</span><br><span class="line"><span class="comment">//lazy延迟计算</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">lazy</span> <span class="keyword">val</span> x = scala.io.<span class="type">Source</span>.fromFile(<span class="string">"/Users/shixuanji/Documents/IDEs/iTerm2/scala/buyChicked.scala"</span>).mkString</span><br><span class="line">x: <span class="type">String</span> = &lt;<span class="keyword">lazy</span>&gt;</span><br><span class="line">scala&gt; x  </span><br><span class="line">res63: <span class="type">String</span> = <span class="string">"...这里就是加载出来的的文件内容"</span></span><br><span class="line"></span><br><span class="line">===================================================================</span><br><span class="line"><span class="comment">//异常</span></span><br><span class="line">scala&gt;</span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">          <span class="string">"hello"</span>.toInt;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">catch</span>&#123;                    <span class="comment">//交给</span></span><br><span class="line">            <span class="keyword">case</span> _:<span class="type">Exception</span>    =&gt; print(<span class="string">"xxxx"</span>) ;</span><br><span class="line">            <span class="keyword">case</span> ex:java.io.<span class="type">IOException</span> =&gt; print(ex)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">-----&gt; 最简单的异常</span><br><span class="line">scala&gt; <span class="keyword">try</span> &#123; <span class="number">1</span>/<span class="number">0</span> &#125; <span class="keyword">catch</span> &#123; <span class="keyword">case</span> _: <span class="type">Exception</span> =&gt; println(<span class="string">"错啦....."</span>) &#125;</span><br><span class="line">错啦.....</span><br><span class="line">res76: <span class="type">AnyVal</span> = ()</span><br><span class="line"></span><br><span class="line"><span class="comment">// _ 的意义</span></span><br><span class="line"><span class="number">1</span>&gt; 通配相当于*</span><br><span class="line"><span class="number">2</span>&gt; <span class="number">1</span> to <span class="number">10</span> :_*    ,转成序列</span><br><span class="line"><span class="number">3</span>&gt; <span class="keyword">case</span> _:<span class="type">Exception</span>    =&gt; print(<span class="string">"xxxx"</span>) ;</span><br></pre></td></tr></table></figure><hr><h1 id="四-数组"><a href="#四-数组" class="headerlink" title="四. 数组"></a>四. 数组</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定长</span></span><br><span class="line">---------------</span><br><span class="line">    java&gt; int[] arr = int int[<span class="number">4</span>] ;</span><br><span class="line">    scala&gt;<span class="keyword">var</span> arr = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](<span class="number">10</span>);            <span class="comment">//apply(10)</span></span><br><span class="line">    scala&gt;<span class="keyword">var</span> arr = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>);                <span class="comment">//推断</span></span><br><span class="line">    scala&gt;arr(<span class="number">0</span>)                                   <span class="comment">//按照下标访问元素</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用 +: 添加元素, 使用 ++: 添加数组, 结果是一个新数组</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> arr = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">arr: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">var</span> arr2 = arr :+ <span class="number">2</span></span><br><span class="line">arr2: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; arr2</span><br><span class="line">res2: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    scala&gt; arr</span><br><span class="line">    res3: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//变长数组</span></span><br><span class="line">-------------------</span><br><span class="line">    scala&gt;<span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line">    scala&gt;<span class="keyword">val</span> buf = <span class="type">ArrayBuffer</span>[<span class="type">Int</span>]();            <span class="comment">//创建数组缓冲区对象</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//+=在末尾追加</span></span><br><span class="line">-------------------</span><br><span class="line">    scala&gt; buf += <span class="number">3</span></span><br><span class="line">    res156: buf.<span class="keyword">type</span> = <span class="type">ArrayBuffer</span>(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    scala&gt; buf .+= (<span class="number">23</span>)</span><br><span class="line">    res157: buf.<span class="keyword">type</span> = <span class="type">ArrayBuffer</span>(<span class="number">3</span>, <span class="number">23</span>)</span><br><span class="line"></span><br><span class="line">    scala&gt; buf ++= <span class="type">Array</span>(<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>)</span><br><span class="line">    res158: buf.<span class="keyword">type</span> = <span class="type">ArrayBuffer</span>(<span class="number">3</span>, <span class="number">23</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//trimEnd,从末尾移除元素</span></span><br><span class="line">-------------------</span><br><span class="line">    scala&gt;buf.trimStart(<span class="number">2</span>)</span><br><span class="line">    scala&gt;buf.trimEnd(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//remove按照索引移除</span></span><br><span class="line">-------------------</span><br><span class="line">    scala&gt;buf.remove(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//insert,在0元素位置插入后续数据</span></span><br><span class="line">-------------------</span><br><span class="line">    scala&gt;buf.insert(<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//toArray</span></span><br><span class="line">-------------------</span><br><span class="line">    scala&gt;buf.toArray</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">=================================数组操作==================================</span><br><span class="line"><span class="comment">//数组操作</span></span><br><span class="line">  <span class="comment">// 需求: 构造出20以内, 4的倍数的数组</span></span><br><span class="line"><span class="comment">// 方式1</span></span><br><span class="line">    scala&gt; (<span class="keyword">for</span>(x &lt;- <span class="number">1</span> to <span class="number">10</span> <span class="keyword">if</span> x % <span class="number">2</span> == <span class="number">0</span>) <span class="keyword">yield</span> x * <span class="number">2</span>).toArray</span><br><span class="line">    res107: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">4</span>, <span class="number">8</span>, <span class="number">12</span>, <span class="number">16</span>, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 方式2</span></span><br><span class="line">    scala&gt; <span class="type">Array</span>(<span class="number">1</span> to <span class="number">10</span>:_*).filter(_ % <span class="number">2</span> == <span class="number">0</span>).map(_ * <span class="number">2</span>)</span><br><span class="line">    res109: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">4</span>, <span class="number">8</span>, <span class="number">12</span>, <span class="number">16</span>, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">==========================================</span><br><span class="line"><span class="comment">// map函数, 参数是一个函数, 函数的参数必须与调用者的参数构成一致</span></span><br><span class="line"><span class="comment">// 每次拿出一个参数出来算</span></span><br><span class="line"><span class="symbol">'ma</span>p':    把一批元素经过操作以后映射出另一批元素, kv 进, kv 出 </span><br><span class="line"><span class="comment">// filter 函数, 同上</span></span><br><span class="line"><span class="symbol">'filte</span>r': 过滤, 把不符合条件的元素过滤掉</span><br><span class="line">==========================================</span><br><span class="line"></span><br><span class="line">---------------------------------------------------------</span><br><span class="line"><span class="comment">//数组常用方法</span></span><br><span class="line">--------------------</span><br><span class="line">    scala&gt;arr.sum</span><br><span class="line">    scala&gt;arr.min</span><br><span class="line">    scala&gt;arr.max</span><br><span class="line"></span><br><span class="line"><span class="comment">//排序</span></span><br><span class="line">--------------------</span><br><span class="line">    scala&gt;<span class="keyword">import</span> scala.util.<span class="type">Sorting</span>._</span><br><span class="line">    scala&gt;<span class="keyword">val</span> arr = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">    scala&gt;quickSort(arr)                <span class="comment">//arr有序</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//Array.mkString =&gt; array 转为 string</span></span><br><span class="line">--------------------</span><br><span class="line">    scala&gt; <span class="keyword">val</span> arr = <span class="type">Array</span>(<span class="number">3</span>, <span class="number">4</span>, <span class="number">14</span>, <span class="number">21</span>)</span><br><span class="line">    arr: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">3</span>, <span class="number">4</span>, <span class="number">14</span>, <span class="number">21</span>)</span><br><span class="line"></span><br><span class="line">    scala&gt; arr</span><br><span class="line">    res164: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">3</span>, <span class="number">4</span>, <span class="number">14</span>, <span class="number">21</span>)</span><br><span class="line"></span><br><span class="line">    scala&gt; arr.mkString(<span class="string">"&lt;&lt;"</span>,<span class="string">","</span>,<span class="string">"&gt;&gt;"</span>)</span><br><span class="line">    res165: <span class="type">String</span> = &lt;&lt;<span class="number">3</span>,<span class="number">4</span>,<span class="number">14</span>,<span class="number">21</span>&gt;&gt;</span><br><span class="line">===================================================================</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//多维数组</span></span><br><span class="line">--------------------</span><br><span class="line"><span class="comment">// 定义方式1</span></span><br><span class="line">    scala&gt; <span class="keyword">var</span> arr = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">Int</span>]](<span class="number">4</span>)</span><br><span class="line">    arr: <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">Int</span>]] = <span class="type">Array</span>(<span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>)</span><br><span class="line">    scala&gt; arr(<span class="number">0</span>) = <span class="type">Array</span>(<span class="number">1</span>)</span><br><span class="line">    scala&gt; arr(<span class="number">1</span>) = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    scala&gt; arr(<span class="number">2</span>) = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">    scala&gt; arr(<span class="number">3</span>) = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    scala&gt; arr</span><br><span class="line">    res128: <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">Int</span>]] = <span class="type">Array</span>(<span class="type">Array</span>(<span class="number">1</span>), <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>), <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>), <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义方式2</span></span><br><span class="line">    <span class="comment">//二维数组,3行4列</span></span><br><span class="line">    scala&gt;<span class="keyword">val</span> arr = <span class="type">Array</span>.ofDim[<span class="type">Int</span>](<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">    <span class="comment">//下标访问数组元素</span></span><br><span class="line">    scala&gt;arr(<span class="number">0</span>)(<span class="number">1</span>)</span><br><span class="line">    scala&gt;arr.length</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//数组的遍历</span></span><br><span class="line">--------------------</span><br><span class="line">    scala&gt; a</span><br><span class="line">    res178: <span class="type">String</span> = hello</span><br><span class="line"></span><br><span class="line">    scala&gt; <span class="keyword">for</span>(i &lt;- <span class="number">0</span> until a.length) println(i+<span class="string">":"</span>+a(i))</span><br><span class="line">    <span class="number">0</span>:h</span><br><span class="line">    <span class="number">1</span>:e</span><br><span class="line">    <span class="number">2</span>:l</span><br><span class="line">    <span class="number">3</span>:l</span><br><span class="line">    <span class="number">4</span>:o</span><br><span class="line"></span><br><span class="line"><span class="comment">// 拿到每个数组中的元素(跟上面的遍历结果一样)</span></span><br><span class="line">--------------------</span><br><span class="line">    scala&gt; arr</span><br><span class="line">    res179: <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">Int</span>]] = <span class="type">Array</span>(<span class="type">Array</span>(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>), <span class="type">Array</span>(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>), <span class="type">Array</span>(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    scala&gt; <span class="keyword">for</span>(i &lt;- <span class="number">0</span> until arr.length) println(i+<span class="string">":"</span>+arr(i))</span><br><span class="line">    <span class="number">0</span>:[<span class="type">I</span>@<span class="number">36081062</span></span><br><span class="line">    <span class="number">1</span>:[<span class="type">I</span>@<span class="number">70</span>c6f11a</span><br><span class="line">    <span class="number">2</span>:[<span class="type">I</span>@<span class="number">378953</span>bd</span><br><span class="line">     </span><br><span class="line"><span class="comment">//和java对象交互，导入转换类型,使用的隐式转换</span></span><br><span class="line">--------------------</span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>.bufferAsJavaList</span><br><span class="line"><span class="keyword">val</span> buf = <span class="type">ArrayBuffer</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>);</span><br><span class="line"><span class="keyword">val</span> list:java.util.<span class="type">List</span>[<span class="type">Int</span>] = buf ; <span class="comment">// scala 语法中, 泛型时在 中括号中</span></span><br><span class="line">       </span><br><span class="line">       </span><br><span class="line"><span class="comment">// 打印引用类型</span></span><br><span class="line">println(result.toBuffer)<span class="comment">// 打印引用类型的值</span></span><br><span class="line">println(result.mkString(<span class="string">","</span>))<span class="comment">// 以',' 分隔符打印</span></span><br><span class="line">println(arr2.mkString(<span class="string">"&lt;"</span>,<span class="string">":"</span>,<span class="string">"&gt;"</span>))      <span class="comment">// 前后缀 和 分隔符 打印</span></span><br><span class="line">       </span><br><span class="line">println(foreach(println))<span class="comment">// 循环打印</span></span><br></pre></td></tr></table></figure><hr><h1 id="五-Map-映射"><a href="#五-Map-映射" class="headerlink" title="五. Map 映射"></a>五. Map 映射</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">// 不可变 immutable</span></span><br><span class="line">=============================================================================</span><br><span class="line">    <span class="comment">//scala.collection.immutable.Map[Int,String] =&gt;不可变集合</span></span><br><span class="line">scala&gt;<span class="keyword">val</span> map = <span class="type">Map</span>(<span class="number">100</span>-&gt;<span class="string">"tom"</span>,<span class="number">200</span>-&gt;<span class="string">"tomas"</span>,<span class="number">300</span>-&gt;<span class="string">"tomasLee"</span>)</span><br><span class="line">map: scala.collection.immutable.<span class="type">Map</span>[<span class="type">Int</span>,<span class="type">String</span>] = <span class="type">Map</span>(<span class="number">100</span> -&gt; tom, <span class="number">200</span> -&gt; tomas, <span class="number">300</span> -&gt; tomasLee)</span><br><span class="line">    <span class="comment">//通过key访问value</span></span><br><span class="line">    scala&gt;map(<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">注意:</span><br><span class="line">    <span class="number">1</span>) 如果是 immutable 的 map 的 <span class="keyword">val</span> 要改变, 只能赋值给新的 <span class="keyword">val</span> , 添加只能用 + 号</span><br><span class="line">    <span class="number">2</span>) 如果是 immutable 的 map 的 <span class="keyword">var</span> 可以直接使用 +=, -= </span><br><span class="line">      <span class="keyword">val</span> newmap = map + (<span class="number">4</span>-&gt;<span class="string">"ttt"</span>)</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 可变 mutable</span></span><br><span class="line">=============================================================================</span><br><span class="line"><span class="comment">// 直接定义 hashMap 并赋值</span></span><br><span class="line">---------------------------------------------</span><br><span class="line">    <span class="keyword">import</span> scala.collection.mutable.<span class="type">HashMap</span></span><br><span class="line">    <span class="keyword">var</span> map1 = <span class="type">HashMap</span>(<span class="number">1</span>-&gt;<span class="number">22</span>, <span class="string">"dd"</span>-&gt;<span class="number">33</span>)<span class="comment">//注意, 这里字符串只能使用 双引号 </span></span><br><span class="line">    $scala&gt; map1</span><br><span class="line">    res145: scala.collection.mutable.<span class="type">HashMap</span>[<span class="type">Int</span>,<span class="type">Int</span>] = <span class="type">Map</span>(<span class="number">2</span> -&gt; <span class="number">33</span>, <span class="number">1</span> -&gt; <span class="number">22</span>)</span><br><span class="line">    scala&gt; map1(<span class="string">"dd"</span>)</span><br><span class="line">    res197: <span class="type">Int</span> = <span class="number">33</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 先定义空hashmap</span></span><br><span class="line">---------------------------------------------</span><br><span class="line">    scala&gt; <span class="keyword">var</span> map2 = <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">Int</span>,<span class="type">Int</span>]</span><br><span class="line">    map2: scala.collection.mutable.<span class="type">HashMap</span>[<span class="type">Int</span>,<span class="type">Int</span>] = <span class="type">Map</span>()</span><br><span class="line"></span><br><span class="line">    scala&gt; <span class="keyword">var</span> map3 = <span class="type">HashMap</span>[<span class="type">Int</span>,<span class="type">Int</span>]()  <span class="comment">// 这里其实就是调用了 apply()</span></span><br><span class="line">    map3: scala.collection.mutable.<span class="type">HashMap</span>[<span class="type">Int</span>,<span class="type">Int</span>] = <span class="type">Map</span>()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// hashMap 赋值</span></span><br><span class="line">---------------------------------------------</span><br><span class="line">注意: += / -= 会改变原值,  + / - 不会改变原值,只会改变当次返回值</span><br><span class="line">---------------------------------------------</span><br><span class="line"><span class="comment">///添加元素: += (k-&gt;v) 注意: 要与定义时类型一致</span></span><br><span class="line">    scala&gt; map2 += (<span class="number">1</span>-&gt;<span class="number">100</span>,<span class="number">2</span>-&gt;<span class="number">200</span>)</span><br><span class="line">    res150: scala.collection.mutable.<span class="type">HashMap</span>[<span class="type">Int</span>,<span class="type">Int</span>] = <span class="type">Map</span>(<span class="number">2</span> -&gt; <span class="number">200</span>, <span class="number">1</span> -&gt; <span class="number">100</span>)</span><br><span class="line">    ====&gt; 注意: 此时 mutable de  <span class="keyword">var</span> 的  '+=' 和 '+', 效果是一样的</span><br><span class="line">    =====&gt; map2 = map2 + (<span class="number">1</span>-&gt;<span class="number">100</span>,<span class="number">2</span>-&gt;<span class="number">200</span>)  <span class="comment">// 这个写法是错误的, 要么用+, 要么用+=</span></span><br><span class="line"></span><br><span class="line"><span class="comment">///删除元素: -= k</span></span><br><span class="line">    scala&gt; map2 -= <span class="number">1</span></span><br><span class="line">    res151: scala.collection.mutable.<span class="type">HashMap</span>[<span class="type">Int</span>,<span class="type">Int</span>] = <span class="type">Map</span>(<span class="number">2</span> -&gt; <span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">==&gt;  注意: 如果key 一样, 会覆盖掉前面的赋值, 相当于修改</span><br><span class="line">==&gt;  当然, 也可以这样修改: &gt;&gt;&gt;  map2(<span class="number">1</span>) = <span class="number">500</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//迭代map</span></span><br><span class="line">---------------------------------------------</span><br><span class="line">scala&gt;<span class="keyword">for</span> ((k,v) &lt;- map) println(k + <span class="string">":::"</span> + v);</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">for</span> ((k,v) &lt;- map) println(k +<span class="string">":::::"</span> + v)</span><br><span class="line"><span class="number">1</span>:::::<span class="number">33</span></span><br><span class="line"><span class="number">2</span>:::::<span class="number">44</span></span><br><span class="line"><span class="number">3</span>:::::<span class="number">55</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//使用yield操作进行倒排序(kv对调)</span></span><br><span class="line">---------------------------------------------</span><br><span class="line">scala&gt; <span class="keyword">for</span> ((k,v) &lt;- map) <span class="keyword">yield</span> (v,k)</span><br><span class="line">res210: scala.collection.immutable.<span class="type">Map</span>[<span class="type">Int</span>,<span class="type">Int</span>] = <span class="type">Map</span>(<span class="number">33</span> -&gt; <span class="number">1</span>, <span class="number">44</span> -&gt; <span class="number">2</span>, <span class="number">55</span> -&gt; <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// map方法:</span></span><br><span class="line">此方法只能接受一个参数, 这个参数是函数</span><br><span class="line">此参数函数, 也只能接受一个参数, 此参数的类型, 和数组中的元素类型一致</span><br><span class="line">此参数函数最终会返回一个值, 值的类型可以自定义</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> add1 = (x : <span class="type">Int</span>) =&gt; x+<span class="number">1</span> </span><br><span class="line">arr.map(add1)</span><br><span class="line"></span><br><span class="line">arr.map( (x : <span class="type">Int</span>) =&gt; x+<span class="number">1</span> )</span><br><span class="line">arr. map(x =&gt; x +<span class="number">1</span>)</span><br><span class="line">arr.map(_ + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 取值</span></span><br><span class="line">map.get(<span class="string">"xx"</span>)      <span class="comment">//没有的话就返回空</span></span><br><span class="line">scala&gt; m.getOrElse(<span class="number">0</span>,<span class="string">"no"</span>)   <span class="comment">// 没有的话就返回后面指定的元素</span></span><br></pre></td></tr></table></figure><hr><h1 id="六-元组tuple-最大Tuple22"><a href="#六-元组tuple-最大Tuple22" class="headerlink" title="六. 元组tuple, 最大Tuple22"></a>六. 元组tuple, 最大Tuple22</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 二元元祖 叫 对偶</span></span><br><span class="line">---------------------------------------------</span><br><span class="line">元组的定义: 用小括号包起来</span><br><span class="line">---------------------------------------------</span><br><span class="line">    scala&gt; <span class="keyword">val</span> t = (<span class="number">1</span>,<span class="string">"tom"</span>,<span class="number">12</span>) ;</span><br><span class="line">    scala&gt; t</span><br><span class="line">    res216: (<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Int</span>) = (<span class="number">1</span>,tom,<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">    scala&gt; <span class="keyword">val</span> t = (<span class="number">1</span>,<span class="string">"tom"</span>,<span class="number">12</span>,<span class="number">1</span>,<span class="string">"tom"</span>,<span class="number">12</span>,<span class="number">1</span>,<span class="string">"tom"</span>,<span class="number">12</span>,<span class="number">1</span>,<span class="string">"tom"</span>,<span class="number">12</span>,<span class="number">1</span>,<span class="string">"tom"</span>,<span class="number">12</span>,<span class="number">1</span>,<span class="string">"tom"</span>,<span class="number">12</span>,<span class="number">1</span>,<span class="string">"tom"</span>,<span class="number">12</span>,<span class="number">1</span>)</span><br><span class="line">    t: (<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">String</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">String</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">String</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">String</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">String</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">String</span>, <span class="type">Int</span>, <span class="type">Int</span>) = (<span class="number">1</span>,tom,<span class="number">12</span>,<span class="number">1</span>,tom,<span class="number">12</span>,<span class="number">1</span>,tom,<span class="number">12</span>,<span class="number">1</span>,tom,<span class="number">12</span>,<span class="number">1</span>,tom,<span class="number">12</span>,<span class="number">1</span>,tom,<span class="number">12</span>,<span class="number">1</span>,tom,<span class="number">12</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    scala&gt; <span class="keyword">val</span> t = (<span class="number">1</span>,<span class="string">"tom"</span>,<span class="number">12</span>,<span class="number">1</span>,<span class="string">"tom"</span>,<span class="number">12</span>,<span class="number">1</span>,<span class="string">"tom"</span>,<span class="number">12</span>,<span class="number">1</span>,<span class="string">"tom"</span>,<span class="number">12</span>,<span class="number">1</span>,<span class="string">"tom"</span>,<span class="number">12</span>,<span class="number">1</span>,<span class="string">"tom"</span>,<span class="number">12</span>,<span class="number">1</span>,<span class="string">"tom"</span>,<span class="number">12</span>,<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">    &lt;console&gt;:<span class="number">1</span>: error: too many elements <span class="keyword">for</span> tuple: <span class="number">23</span>, allowed: <span class="number">22</span></span><br><span class="line">           <span class="keyword">val</span> t = (<span class="number">1</span>,<span class="string">"tom"</span>,<span class="number">12</span>,<span class="number">1</span>,<span class="string">"tom"</span>,<span class="number">12</span>,<span class="number">1</span>,<span class="string">"tom"</span>,<span class="number">12</span>,<span class="number">1</span>,<span class="string">"tom"</span>,<span class="number">12</span>,<span class="number">1</span>,<span class="string">"tom"</span>,<span class="number">12</span>,<span class="number">1</span>,<span class="string">"tom"</span>,<span class="number">12</span>,<span class="number">1</span>,<span class="string">"tom"</span>,<span class="number">12</span>,<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">注意: error: too many elements <span class="keyword">for</span> tuple: <span class="number">23</span>, allowed: <span class="number">22</span>  元祖最多<span class="number">22</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 访问元组指定元</span></span><br><span class="line">---------------------------------------------</span><br><span class="line">==&gt; 注意: 元祖序列从 <span class="number">1</span> 开始</span><br><span class="line">scala&gt; <span class="keyword">val</span> t = (<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>) ;        <span class="comment">//元组</span></span><br><span class="line">scala&gt;t._2</span><br><span class="line">scala&gt;t _2 (不推荐)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> (a,b,c) = t</span><br><span class="line">==&gt; 相当于一下声明了<span class="number">3</span>个变量, a,b,c</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//直接取出元组中的各分量</span></span><br><span class="line">---------------------------------------------</span><br><span class="line">scala&gt; <span class="keyword">val</span> t = (<span class="number">132</span>,<span class="string">"d4"</span>,<span class="number">3</span>)</span><br><span class="line">t: (<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Int</span>) = (<span class="number">132</span>,d4,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">注意: 前后的元数必须相等</span><br><span class="line">scala&gt; <span class="keyword">val</span> (a,b,c) = t</span><br><span class="line">a: <span class="type">Int</span> = <span class="number">132</span></span><br><span class="line">b: <span class="type">String</span> = d4</span><br><span class="line">c: <span class="type">Int</span> = <span class="number">3</span></span><br><span class="line">==&gt; 这里就相当于直接定义了 a,b,c 三个变量</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//数组的zip</span></span><br><span class="line">---------------------------------------------</span><br><span class="line"><span class="comment">//西门庆 -&gt; 潘金莲  牛郎 -&gt; 侄女  ,</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> hus = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>);</span><br><span class="line">hus: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> wife = <span class="type">Array</span>(<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>)</span><br><span class="line">wife: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; hus.zip(wife)</span><br><span class="line">res226: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="number">1</span>,<span class="number">4</span>), (<span class="number">2</span>,<span class="number">5</span>), (<span class="number">3</span>,<span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">注意: 个数对不上的就砍掉了</span><br></pre></td></tr></table></figure><hr><h1 id="七-OOP"><a href="#七-OOP" class="headerlink" title="七. OOP"></a>七. OOP</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义类 &amp; 基本操作</span></span><br><span class="line">===============================================================</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定义变量, 私有类型, 必须初始化</span></span><br><span class="line">    <span class="comment">// .class 中私有方法查看: javap -private xxx.class</span></span><br><span class="line">    <span class="comment">// getter/setter 也私有</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> id = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// p.name  =&gt; getter</span></span><br><span class="line">    <span class="comment">// p.name_=(xx) =&gt; setter </span></span><br><span class="line">    <span class="comment">// p.name = xx  =&gt; setter</span></span><br><span class="line">    <span class="comment">// 生成私有属性, 和共有的 getter/setter 方法</span></span><br><span class="line">    <span class="keyword">var</span> name = <span class="string">"tom"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// val 只有 getter, 没有setter, 定义的是 val 常量</span></span><br><span class="line">    <span class="keyword">val</span> age = <span class="number">100</span>; </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">incre</span></span>(a:<span class="type">Int</span>) = &#123;id += a&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//如果定义时，没有(),调用就不能加()</span></span><br><span class="line">    <span class="comment">// 有的话, 调用的时候就可加可不加</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">current</span></span>() = id</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">------------------------</span><br><span class="line">-&gt;调用</span><br><span class="line"></span><br><span class="line">    scala&gt;<span class="keyword">var</span> p = <span class="keyword">new</span> <span class="type">Person</span>();</span><br><span class="line">    scala&gt;p.current()</span><br><span class="line">    scala&gt;p.current</span><br><span class="line">    scala&gt;p.incr(<span class="number">100</span>)</span><br><span class="line">    scala&gt;p.name</span><br><span class="line">    scala&gt;p.name_=(<span class="string">"kkkk"</span>)</span><br><span class="line">    scala&gt;p.name = <span class="string">"kkkk"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// private[this]作用,控制成员只能在自己的对象中访问。 (对象私有)</span></span><br><span class="line">===============================================================</span><br><span class="line"><span class="number">1</span>) <span class="type">Scala</span>中(<span class="type">Java</span>和 <span class="type">C</span>++ 也一样), 方法可以访问该类的所有对象的私有字段</span><br><span class="line"><span class="number">2</span>) <span class="keyword">private</span>[<span class="keyword">this</span>], 是 <span class="type">Counter</span> 类的方法, 只能访问到当前对象的 value 方法, 而不能访问同样是 <span class="type">Counter</span> 类型的其他对象的该字段.</span><br><span class="line"><span class="number">3</span>) 这样的访问被称为 对象私有的</span><br><span class="line"><span class="number">4</span>) 对于类私有的字段, <span class="type">Scala</span> 生成私有的 getter 和 setter 方法;</span><br><span class="line">    但对于对象私有的字段, <span class="type">Scala</span> 根本不会生成 getter 和 setter 方法</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Counter</span></span>&#123;</span><br><span class="line">        <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">var</span> value =  <span class="number">0</span> ;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">incre</span></span>(n:<span class="type">Int</span>)&#123;value += n&#125;</span><br><span class="line">        <span class="comment">// 类会定义失败, 下面这句不能访问别类对象的 value</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">isLess</span></span>(other:<span class="type">Counter</span>) = value &lt; other.value ;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义BeanProperty注解 :TODO</span></span><br><span class="line">------------------------------------------------------------------</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 构造函数</span></span><br><span class="line">===============================================================</span><br><span class="line">&gt; 辅助构造</span><br><span class="line">----------------------</span><br><span class="line">&gt;&gt; 定义</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Person</span></span>&#123;</span><br><span class="line">        <span class="keyword">var</span> id = <span class="number">1</span> ;</span><br><span class="line">        <span class="keyword">var</span> name = <span class="string">"tom"</span> ;</span><br><span class="line">        <span class="keyword">var</span> age = <span class="number">12</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//一个辅助构造器</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(name:<span class="type">String</span>)&#123;</span><br><span class="line">            <span class="keyword">this</span>();  <span class="comment">// 调用主构造器</span></span><br><span class="line">            <span class="keyword">this</span>.name = name ;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//另一个辅助构造</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(name:<span class="type">String</span>,age:<span class="type">Int</span>)&#123;</span><br><span class="line">            <span class="comment">//调用前一个辅助构造器</span></span><br><span class="line">            <span class="keyword">this</span>(name) ;</span><br><span class="line">            <span class="keyword">this</span>.age = age ;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&gt;&gt; 使用</span><br><span class="line">scala&gt; <span class="keyword">var</span> p = <span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"tm"</span>)</span><br><span class="line">p: <span class="type">Person</span> = <span class="type">Person</span>@<span class="number">2492</span>f6fb</span><br><span class="line"></span><br><span class="line">scala&gt; p.name</span><br><span class="line">res9: <span class="type">String</span> = tm</span><br><span class="line"></span><br><span class="line">scala&gt; p = <span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"jr"</span>,<span class="number">23</span>)   <span class="comment">// 注意: 此 p 非上面的 p 了</span></span><br><span class="line">p: <span class="type">Person</span> = <span class="type">Person</span>@<span class="number">2536</span>edc3</span><br><span class="line"></span><br><span class="line">scala&gt; p.name</span><br><span class="line">res11: <span class="type">String</span> = jr</span><br><span class="line"></span><br><span class="line">scala&gt; p.age</span><br><span class="line">res12: <span class="type">Int</span> = <span class="number">23</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt; 主构造函数</span><br><span class="line">---------------------------------------------</span><br><span class="line">    <span class="comment">//val ===&gt; 只读</span></span><br><span class="line">    <span class="comment">//var ==&gt; get/set</span></span><br><span class="line">    <span class="comment">//none ==&gt; none</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 注意:  id 前面没有修饰的话, 只有在调用后才升级为成员变量, 否则在 .class 文件中不会出现</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">val name:<span class="type">String</span>, var age:<span class="type">Int</span>, id :<span class="type">Int</span></span>)</span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hello</span></span>() = println(id)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&gt;&gt;编译后生成的文件</span><br><span class="line">javap -<span class="keyword">private</span> <span class="type">Person</span>.<span class="keyword">class</span> -&gt; 显示编译后的 <span class="keyword">private</span> 成员</span><br><span class="line"></span><br><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;                <span class="comment">// id 调用后才升级为成员变量</span></span><br><span class="line">  <span class="keyword">private</span> int age;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> int id;              </span><br><span class="line">  public java.lang.<span class="type">String</span> name();    <span class="comment">// getter</span></span><br><span class="line">  public int age();                  <span class="comment">// getter</span></span><br><span class="line">  public void age_$eq(int);          <span class="comment">// setter</span></span><br><span class="line">  public void hello();               </span><br><span class="line">  public <span class="type">Person</span>(java.lang.<span class="type">String</span>, int, int);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// object</span></span><br><span class="line">===============================================================</span><br><span class="line">说明：scala没有静态的概念，'如果需要定义静态成员，可以通过<span class="class"><span class="keyword">object</span><span class="title">实现。</span>'</span></span><br><span class="line"><span class="class">   <span class="title">编译完成后，会生成对应的类，方法都是静态方法，非静态成员对应到单例类中</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">单例类以Util$作为类名称。</span>   </span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">静态方法类</span> <span class="title">以</span> <span class="title">Util</span> <span class="title">为类名称</span>.</span></span><br><span class="line"><span class="class">    <span class="title">scala&gt;object</span> <span class="title">Util</span></span>&#123;</span><br><span class="line">        <span class="comment">//被编译为单例类中.(Util$)</span></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">var</span> brand = <span class="string">"benz"</span> ;</span><br><span class="line">        <span class="comment">//被编译为 单独Util 类中的 静态方法 </span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">hello</span></span>() = println(<span class="string">"hello world"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">➜  scala javap -<span class="keyword">private</span> <span class="type">Util</span></span><br><span class="line"><span class="type">Compiled</span> from <span class="string">"util.scala"</span></span><br><span class="line">public <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Util</span> </span>&#123;</span><br><span class="line">  public static void hello();</span><br><span class="line">&#125;</span><br><span class="line">➜  scala javap -<span class="keyword">private</span> <span class="type">Util</span>$</span><br><span class="line"><span class="type">Compiled</span> from <span class="string">"util.scala"</span></span><br><span class="line">public <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Util$</span> </span>&#123;</span><br><span class="line">  public static <span class="type">Util</span>$ <span class="type">MODULE</span>$;</span><br><span class="line">  <span class="keyword">private</span> java.lang.<span class="type">String</span> brand;</span><br><span class="line">  public static &#123;&#125;;</span><br><span class="line">  <span class="keyword">private</span> java.lang.<span class="type">String</span> brand();</span><br><span class="line">  <span class="keyword">private</span> void brand_$eq(java.lang.<span class="type">String</span>);</span><br><span class="line">  public void hello();</span><br><span class="line">  <span class="keyword">private</span> <span class="type">Util</span>$();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 伴生对象(companions object)</span></span><br><span class="line">------------------------------------------------------------------</span><br><span class="line"><span class="number">1.</span> 类名和<span class="class"><span class="keyword">object</span><span class="title">名称相同，而且必须在一个scala文件中定义。</span></span></span><br><span class="line"><span class="class">2. <span class="title">编译后</span>, <span class="title">会产生2个文件</span>, <span class="title">Car</span>.<span class="title">class</span> <span class="title">和</span> <span class="title">Car$</span>.<span class="title">class</span></span></span><br><span class="line"><span class="class">3. <span class="title">静态方法会被编译到</span> <span class="title">Car</span>.<span class="title">class</span> <span class="title">中去</span>, <span class="title">静态方式可直接用类调用</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">Car</span></span>&#123;</span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">stop</span></span>() = println(<span class="string">"stop...."</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Car</span></span>&#123;   <span class="comment">// 伴生对象, 目的就是为了产生静态方法</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() = println(<span class="string">"run..."</span>)</span><br><span class="line">&#125;</span><br><span class="line">------------</span><br><span class="line">&gt; 编译后</span><br><span class="line">➜  scala javap -<span class="keyword">private</span> <span class="type">Car</span></span><br><span class="line"><span class="type">Compiled</span> from <span class="string">"companionsObject.scala"</span></span><br><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">Car</span> </span>&#123;</span><br><span class="line">  public static void run();</span><br><span class="line">  public void stop();</span><br><span class="line">  public <span class="type">Car</span>();</span><br><span class="line">&#125;</span><br><span class="line">➜  scala javap -<span class="keyword">private</span> <span class="type">Car</span>$</span><br><span class="line"><span class="type">Compiled</span> from <span class="string">"companionsObject.scala"</span></span><br><span class="line">public <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Car$</span> </span>&#123;</span><br><span class="line">  public static <span class="type">Car</span>$ <span class="type">MODULE</span>$;</span><br><span class="line">  public static &#123;&#125;;</span><br><span class="line">  public void run();</span><br><span class="line">  <span class="keyword">private</span> <span class="type">Car</span>$();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="number">4.</span> 因为 main 函数是静态方法, 所以写在伴生对象中</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">val name:<span class="type">String</span>, var age:<span class="type">Int</span>, id : <span class="type">Int</span></span>) </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hello</span></span>() = println(id);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args:<span class="type">Array</span>[<span class="type">String</span>]) = println(<span class="string">"hello world"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">➜  scala scala main.scala  <span class="comment">// 直接 scala xx.scala 会先编译再运行</span></span><br><span class="line">hello world</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 抽象 &amp; 静态 </span></span><br><span class="line">===============================================================</span><br><span class="line">➜ 抽象 &amp; 静态 test</span><br><span class="line"></span><br><span class="line">➜定义类</span><br><span class="line"></span><br><span class="line"><span class="comment">// 抽象类</span></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Dog</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">a</span></span>():<span class="type">Unit</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// object等价于java中的静态。</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Jing8</span> <span class="keyword">extends</span> <span class="title">Dog</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 重写方法</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">a</span></span>():<span class="type">Unit</span>= println(<span class="string">"hello"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Zangao</span> <span class="keyword">extends</span> <span class="title">Dog</span> </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">a</span></span>():<span class="type">Unit</span> = println(<span class="string">"老子是个🐶??"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">➜ 调用</span><br><span class="line">scala&gt; <span class="type">Jing8</span>.a            <span class="comment">// 静态方法调用</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">Jing8</span>.a</span><br><span class="line">hello</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">var</span> za = <span class="keyword">new</span> <span class="type">Zangao</span>()  <span class="comment">// 类创建对象调用</span></span><br><span class="line">za: <span class="type">Zangao</span> = <span class="type">Zangao</span>@<span class="number">77505</span>c0e</span><br><span class="line"></span><br><span class="line">scala&gt; za.a</span><br><span class="line">老子是个🐶??</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// apply</span></span><br><span class="line">===============================================================</span><br><span class="line"><span class="number">1</span>) 使用</span><br><span class="line"><span class="comment">// 定义: </span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Util</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(s:<span class="type">String</span>) = println(s) ;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 调用: 下面2者是一个意思</span></span><br><span class="line"><span class="type">Util</span>(<span class="string">"hello world"</span>);</span><br><span class="line"><span class="type">Util</span>.apply(<span class="string">"hello world"</span>);</span><br><span class="line">&gt; 运行结果: hello world</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="number">2</span>) 注意点</span><br><span class="line"><span class="comment">// 注意一下2者的区别</span></span><br><span class="line">scala&gt; <span class="keyword">var</span> arr = <span class="type">Array</span>(<span class="number">10</span>)</span><br><span class="line">arr: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; arr</span><br><span class="line">res18: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">var</span> arr = <span class="keyword">new</span> <span class="type">Array</span>(<span class="number">10</span>)</span><br><span class="line">arr: <span class="type">Array</span>[<span class="type">Nothing</span>] = <span class="type">Array</span>(<span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 终端编译运行 scala文件</span></span><br><span class="line">===============================================================</span><br><span class="line">scalac编译scala文件，产生<span class="class"><span class="keyword">class</span><span class="title">文件。</span></span></span><br><span class="line"><span class="class"><span class="title">------------------------------------</span></span></span><br><span class="line"><span class="class">    <span class="title">cmd&gt;scalac</span> <span class="title">xxxx</span>.<span class="title">scala</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">运行class程序</span></span></span><br><span class="line"><span class="class"><span class="title">--------------------</span></span></span><br><span class="line"><span class="class">    <span class="title">cmd&gt;scala</span> <span class="title">Person</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">一步到位</span></span></span><br><span class="line"><span class="class"><span class="title">-------------------</span></span></span><br><span class="line"><span class="class">    <span class="title">cmd&gt;scala</span> <span class="title">Person</span>.<span class="title">scala</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">trait</span>  (<span class="params">类似于 <span class="type">Java</span> 中的 interface </span>)</span></span><br><span class="line"><span class="class"><span class="title">===============================================================</span></span></span><br><span class="line"><span class="class">    <span class="title">traint</span> <span class="title">HelloService</span></span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//包对象，编译完之后生成以 /a/a1/aa1/people/xxx.class..., 一层一层文件夹</span></span><br><span class="line">===============================================================</span><br><span class="line"><span class="keyword">package</span> a.a1.aa1;</span><br><span class="line"><span class="keyword">package</span> <span class="class"><span class="keyword">object</span> <span class="title">people</span> </span>&#123;</span><br><span class="line">    <span class="keyword">val</span> name = <span class="string">"hello world"</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//约束可见性。</span></span><br><span class="line">-----------------------</span><br><span class="line"><span class="keyword">private</span>[<span class="keyword">package</span>|<span class="keyword">this</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 包的导入</span></span><br><span class="line">--------------</span><br><span class="line"><span class="keyword">import</span> java.io.<span class="type">Exception</span></span><br><span class="line"><span class="keyword">import</span> java.io.&#123;<span class="type">A</span>,<span class="type">B</span>,<span class="type">C</span>&#125;            <span class="comment">//同一包下的不同类, 可以用大括号包起来</span></span><br><span class="line"><span class="keyword">import</span> java.io.&#123;<span class="type">A</span> =&gt; <span class="type">A0</span>&#125;        <span class="comment">//别名</span></span><br></pre></td></tr></table></figure><hr><h1 id="八-继承-inheritance"><a href="#八-继承-inheritance" class="headerlink" title="八. 继承 inheritance"></a>八. 继承 inheritance</h1><p><strong>extends</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">继承/扩展</span><br><span class="line">-------------------------------</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Dog</span> <span class="keyword">extends</span> <span class="title">Animal</span></span>&#123;</span><br><span class="line">        <span class="comment">//重写,覆盖</span></span><br><span class="line">        <span class="comment">//overload</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>()=&#123;...&#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">继承实例</span><br><span class="line">------------------------------</span><br><span class="line">$scala&gt;<span class="class"><span class="keyword">class</span> <span class="title">Animal</span>(<span class="params">val name:<span class="type">String</span></span>)</span>&#123;&#125;</span><br><span class="line">$scala&gt;<span class="class"><span class="keyword">class</span> <span class="title">Dog</span>(<span class="params">name:<span class="type">String</span>,val age:<span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Animal</span>(<span class="params">name</span>)</span>&#123;&#125;</span><br><span class="line">$scala&gt;<span class="class"><span class="keyword">class</span> <span class="title">Duck</span>(<span class="params">override val name:<span class="type">String</span>, val age:<span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Animal</span>(<span class="params">name:<span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">注意</span></span>: 如果在构造里为继承的父类传参, 创建的子类对象直接会有值</span><br><span class="line">scala&gt; <span class="class"><span class="keyword">class</span> <span class="title">Duck</span>(<span class="params">name:<span class="type">String</span>, val age:<span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Animal</span>(<span class="params">"aa"</span>)</span></span><br><span class="line"><span class="class"><span class="title">defined</span> <span class="title">class</span> <span class="title">Duck</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">var</span> <span class="title">dd</span> </span>= <span class="keyword">new</span> <span class="type">Duck</span>(<span class="string">"cc"</span>,<span class="number">32</span>)</span><br><span class="line">dd: <span class="type">Duck</span> = <span class="type">Duck</span>@<span class="number">12</span>a63c27</span><br><span class="line"></span><br><span class="line">scala&gt; dd.name</span><br><span class="line">res33: <span class="type">String</span> = aa</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">类型检查和转换</span><br><span class="line">------------------------------</span><br><span class="line">    $scala&gt;<span class="class"><span class="keyword">class</span> <span class="title">Animal</span></span>&#123;&#125;</span><br><span class="line">    $scala&gt;<span class="class"><span class="keyword">class</span> <span class="title">Dog</span> <span class="keyword">extends</span> <span class="title">Animal</span></span>&#123;&#125;</span><br><span class="line">    $scala&gt;<span class="keyword">val</span> d = <span class="keyword">new</span> <span class="type">Dog</span>();</span><br><span class="line">    $scala&gt;d.isInstanceOf[<span class="type">Animal</span>]            <span class="comment">//true,===&gt; instanceOf</span></span><br><span class="line">    $scala&gt;<span class="keyword">val</span> a = d.asInstanceOf[<span class="type">Animal</span>]    <span class="comment">//强转,===&gt; (Animal)d</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//得到对象的类 对象类型 得到类型</span></span><br><span class="line">    $scala&gt;d.getClass                        <span class="comment">//d.getClass(); 获取对象类型</span></span><br><span class="line">    $scala&gt;d.getClass == classOf[<span class="type">Dog</span>]        <span class="comment">//精确匹配</span></span><br></pre></td></tr></table></figure><hr><h1 id="九-abstract-trait-file-apply-操作符"><a href="#九-abstract-trait-file-apply-操作符" class="headerlink" title="九. abstract , trait, file, apply, 操作符"></a>九. abstract , trait, file, apply, 操作符</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">//抽象类 abstract </span></span><br><span class="line">================================================</span><br><span class="line">$scala&gt;<span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Animal</span>(<span class="params">val name:<span class="type">String</span></span>)</span>&#123;</span><br><span class="line">    <span class="comment">//抽象字段，没有初始化。</span></span><br><span class="line">    <span class="keyword">val</span> id:<span class="type">Int</span>  ;</span><br><span class="line">    <span class="comment">//抽象方法，没有方法体，不需要抽象关键字修饰。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() ;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// scala类型树  :TODO</span></span><br><span class="line">================================================</span><br><span class="line">    <span class="type">Any</span></span><br><span class="line">     |</span><br><span class="line">    /|\</span><br><span class="line">     |------------<span class="type">AnyVal</span> &lt;|------<span class="type">Int</span>|<span class="type">Boolean</span>|...|<span class="type">Unit</span></span><br><span class="line">     |------------<span class="type">AnyRef</span> &lt;|------<span class="class"><span class="keyword">class</span> ...</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">读取文件</span> <span class="title">file</span></span></span><br><span class="line"><span class="class"><span class="title">================================================</span></span></span><br><span class="line"><span class="class"><span class="title">-----------in</span> <span class="title">idea</span></span></span><br><span class="line"><span class="class">    <span class="title">import</span> <span class="title">scala</span>.<span class="title">io</span>.<span class="title">Source</span> </span>;</span><br><span class="line">    <span class="class"><span class="keyword">object</span> <span class="title">FileDemo</span> </span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">            <span class="keyword">val</span> s = <span class="type">Source</span>.fromFile(<span class="string">"d:/hello.txt"</span>) ;</span><br><span class="line">            <span class="keyword">val</span> lines = s.getLines();</span><br><span class="line">            <span class="keyword">for</span>(line &lt;- lines)&#123;</span><br><span class="line">                println(line)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">---------in terminal</span><br><span class="line">    scala&gt; scala.io.<span class="type">Source</span>.fromFile(<span class="string">"/Users/shixuanji/Documents/IDEs/iTerm2/scala/util.scala"</span>).mkString</span><br><span class="line"><span class="comment">// 下面是上面执行的结果</span></span><br><span class="line">    res34: <span class="type">String</span> =</span><br><span class="line">    <span class="class"><span class="keyword">object</span> <span class="title">Util</span> </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">var</span> brand = <span class="string">"benz"</span>;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">hello</span></span>() = println(<span class="string">"hello world"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//通过正则 (具体的查看 dash 的 jdk)</span></span><br><span class="line">\s : 空白字符：[ \t\n\x0B\f\r]</span><br><span class="line">\<span class="type">S</span> : 非空白字符：[^\s]</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> str = <span class="type">Source</span>.fromFile(<span class="string">"/hello.txt"</span>).mkString</span><br><span class="line"><span class="keyword">val</span> it = str.split(<span class="string">"\\s+"</span>)     <span class="comment">// 过滤掉所有不可见字符分割</span></span><br><span class="line"><span class="keyword">for</span>(i &lt;- it)&#123;</span><br><span class="line">    println(i)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// trait 接口</span></span><br><span class="line">================================================</span><br><span class="line">&gt;  java中, 实现接口, 用implement ,  scala 中用 <span class="keyword">extends</span></span><br><span class="line">&gt;  如果只有一个<span class="class"><span class="keyword">trait</span><span class="title">使用extends进行扩展，如果多个，使用with对剩余的trait进行扩展。</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    <span class="title">trait</span> <span class="title">logger1</span></span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">log1</span></span>() = println(<span class="string">"hello log1"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">trait</span> <span class="title">logger2</span></span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">log2</span></span>() = println(<span class="string">"hello log2"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&gt;  <span class="class"><span class="keyword">trait</span>(<span class="params">接口</span>)<span class="title">之间也可以存在扩展。</span>     </span></span><br><span class="line"><span class="class">    <span class="title">class</span> <span class="title">Dog</span> <span class="keyword">extends</span> <span class="title">logger1</span> <span class="keyword">with</span> <span class="title">logger2</span></span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//自身类型 :TODO  还不明白是啥意思, 有空看看  书 : 10.13 Self Types &amp; 18.10</span></span><br><span class="line">================================================</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">trait</span> <span class="title">logger</span></span>&#123;</span><br><span class="line">        <span class="keyword">this</span>:<span class="type">Dog</span> =&gt; </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() = println(<span class="string">"run...."</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">trait</span> <span class="title">Dog</span> </span>&#123;&#125;</span><br><span class="line">    <span class="class"><span class="keyword">trait</span> <span class="title">Jing8</span> <span class="keyword">extends</span> <span class="title">Dog</span> <span class="keyword">with</span> <span class="title">logger</span></span>&#123;&#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Cat</span> <span class="keyword">extends</span> <span class="title">logger</span></span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 操作符</span></span><br><span class="line">================================================</span><br><span class="line">    <span class="comment">//中置操作符</span></span><br><span class="line">    scala&gt; <span class="number">1</span> + <span class="number">2</span>            <span class="comment">//</span></span><br><span class="line">    scala&gt; <span class="number">1.</span>+(<span class="number">2</span>)            <span class="comment">//</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//单元操作符</span></span><br><span class="line">    scala&gt; <span class="number">1</span> toString        <span class="comment">//+: -:取反 !:boolean取反 ~:按位取反</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//赋值操作符 </span></span><br><span class="line">    $scala&gt;+= , -= , *=,  /=</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//:表示右结合,只有:结尾的操作符是右结合,优先级从右侧开始</span></span><br><span class="line">----&gt; 用于构造列表的操作符 :: 是右结合的</span><br><span class="line">    scala&gt;<span class="keyword">val</span> l = <span class="type">Nil</span>        <span class="comment">//构造空集合.</span></span><br><span class="line">    scala&gt;<span class="number">1</span>::<span class="number">2</span>::<span class="type">Nil</span>            <span class="comment">//1::(2::Nil), 先创建包含2的列表, 这个列表又被作为为嘛频道以1为头部的列表中</span></span><br><span class="line">    scala&gt;<span class="type">Nil</span>.::(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// apply()/update()   --&gt; 其实就是在等号两侧的问题</span></span><br><span class="line">================================================</span><br><span class="line">    <span class="keyword">var</span> arr = <span class="type">Array</span>(<span class="number">100</span>)            <span class="comment">//Array.apply(100);</span></span><br><span class="line">    arr(<span class="number">0</span>) = <span class="number">200</span>            <span class="comment">//arr.update(0,1234)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// apply(), 接受的是属性参数, 返回的是对象, 内部就是 new 了一个对象</span></span><br><span class="line"><span class="comment">//unapply(),是apply的逆向过程,接受的是对象, 返回的是参数(对象中的属性值), 如果用一个元祖中, 传入相应的变量名, 就相当于给这些变量赋值了</span></span><br><span class="line">    <span class="comment">//定义类</span></span><br><span class="line">------------</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Fraction</span>(<span class="params">val n:<span class="type">Int</span>,val d:<span class="type">Int</span></span>)</span>&#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Fraction</span></span>&#123;</span><br><span class="line">    <span class="comment">//通过</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(n : <span class="type">Int</span>,d:<span class="type">Int</span>)= <span class="keyword">new</span> <span class="type">Fraction</span>(n,d) </span><br><span class="line">    <span class="comment">//逆向过程</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">unapply</span></span>(f:<span class="type">Fraction</span>) = <span class="type">Some</span>(f.n,f.d)</span><br><span class="line">&#125;</span><br><span class="line">-------------</span><br><span class="line"><span class="comment">// 因为定义了 apply, 所以直接调用  Fraction(1,2) 就会创建一个对象</span></span><br><span class="line">    scala&gt; <span class="keyword">val</span> f = <span class="type">Fraction</span>(<span class="number">1</span>,<span class="number">2</span>)            <span class="comment">//apply(...)</span></span><br><span class="line">&gt; 执行结果</span><br><span class="line">    scala&gt; f</span><br><span class="line">    res254: <span class="type">Fraction</span> = <span class="type">Fraction</span>@<span class="number">30</span>d2895e</span><br><span class="line"></span><br><span class="line"><span class="comment">// 把定义的函数 还原成 Fraction, 此时, 就相当于定义了 a,b</span></span><br><span class="line">    scala&gt; <span class="keyword">val</span> <span class="type">Fraction</span>(a,b) = f            <span class="comment">//unapply(...)</span></span><br><span class="line">&gt; 执行结果</span><br><span class="line">    a: <span class="type">Int</span> = <span class="number">1</span></span><br><span class="line">    b: <span class="type">Int</span> = <span class="number">2</span></span><br></pre></td></tr></table></figure><hr><h1 id="十-高阶函数"><a href="#十-高阶函数" class="headerlink" title="十. 高阶函数"></a>十. 高阶函数</h1><p><a href="https://www.jianshu.com/p/d5ce4c683703" target="_blank" rel="noopener">—&gt; 函数 Function 和  方法 Method 的区别, 见这里</a></p><p><strong>查看函数</strong>: 直接输入函数名</p><p><strong>查看方法</strong>: 输入方法名之后, 按 tab 键提示, 也会展示方法</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br></pre></td><td class="code"><pre><span class="line">基本函数定义</span><br><span class="line">           参数&amp;类型       =&gt; 实现</span><br><span class="line"><span class="keyword">val</span> f1 = (x: <span class="type">Int</span>, y: <span class="type">Int</span>) =&gt; x + y</span><br><span class="line"><span class="keyword">val</span> f2 = (a:<span class="type">Double</span>)=&gt;a*a</span><br><span class="line"></span><br><span class="line"><span class="comment">//定义高阶函数 例子1</span></span><br><span class="line">================================================================</span><br><span class="line">    <span class="comment">// f 指参数, f的类型是 (Double)=&gt;Double 函数</span></span><br><span class="line">   <span class="comment">//  这里的参数 f 可以是任何接受 Double 并返回 Double 的函数, valueAtOneQuarter 函数, 将计算那个函数在 0.25位置的值</span></span><br><span class="line"><span class="comment">// 注意: 这里只接受一个 Double 参数</span></span><br><span class="line"><span class="keyword">import</span> scala.math._  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">valueAtOneQuarter</span></span>(f:<span class="type">Double</span>=&gt;<span class="type">Double</span>) = f(<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 调用</span></span><br><span class="line">    <span class="comment">// 往单函数参数的函数 的 参数中, 传一个函数, 此函数符合 (Double)=&gt;Double) 的特征, 然后往此函数中, 传值 0.25</span></span><br><span class="line"><span class="keyword">val</span> f2 = (a:<span class="type">Double</span>)=&gt;a*</span><br><span class="line">valueAtOneQuarter(f2)   <span class="comment">// 执行时, 会把后面的 0.25传入到</span></span><br><span class="line">valueAtOneQuarter(cail _)<span class="comment">// cail 函数: 大于传入值的最小整数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 高阶函数例子2</span></span><br><span class="line">================================================================</span><br><span class="line">    <span class="comment">//              参数       =  函数体(函数) </span></span><br><span class="line"><span class="comment">//    参数      =&gt;  返回值  </span></span><br><span class="line"><span class="comment">// 方法的实现是一个函数, 此函数有自己定义的参数, 有从方法体中传来的参数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mulby</span></span>(factor : <span class="type">Double</span>) = (x:<span class="type">Double</span>) =&gt; x * factor</span><br><span class="line">    mulby(<span class="number">2</span>)</span><br><span class="line">    scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">mulby</span></span>(factor : <span class="type">Double</span>) = (x:<span class="type">Double</span>) =&gt; x * factor</span><br><span class="line">    mulby: (factor: <span class="type">Double</span>)<span class="type">Double</span> =&gt; <span class="type">Double</span></span><br><span class="line"></span><br><span class="line">    scala&gt; mulby(<span class="number">2</span>)</span><br><span class="line">    res9: <span class="type">Double</span> =&gt; <span class="type">Double</span> = $$<span class="type">Lambda</span>$<span class="number">1165</span>/<span class="number">745462106</span>@<span class="number">3</span>d3e9163</span><br><span class="line"></span><br><span class="line">    scala&gt; mulby(<span class="number">2</span>)(<span class="number">2</span>)</span><br><span class="line">    res10: <span class="type">Double</span> = <span class="number">4.0</span></span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; 接下来把次mulby 方法, 转成函数试试</span><br><span class="line">    scala&gt; <span class="keyword">val</span> f_mulby = mulby _</span><br><span class="line">    f_mulby: <span class="type">Double</span> =&gt; (<span class="type">Double</span> =&gt; <span class="type">Double</span>) = $$<span class="type">Lambda</span>$<span class="number">1364</span>/<span class="number">1154393787</span>@<span class="number">6307382</span></span><br><span class="line">&gt;&gt;&gt; 调用方式还是一样的</span><br><span class="line">f_mulby(<span class="number">2</span>)(<span class="number">2</span>)</span><br><span class="line">-------------------------</span><br><span class="line">    scala&gt;<span class="function"><span class="keyword">def</span> <span class="title">multi</span></span>(n:<span class="type">Int</span>) = n * <span class="number">2</span></span><br><span class="line">    scala&gt;<span class="function"><span class="keyword">def</span> <span class="title">f</span> </span>= multi _                <span class="comment">// _ 表示:把multi 转为函数</span></span><br><span class="line">    scala&gt;<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>).map(f)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜</span><br><span class="line"></span><br><span class="line"><span class="comment">//匿名函数</span></span><br><span class="line">    scala&gt;(n:<span class="type">Double</span>)=&gt;<span class="number">3</span> * n                <span class="comment">// 匿名函数</span></span><br><span class="line">    scala&gt;<span class="keyword">val</span> f = (n:<span class="type">Double</span>)=&gt;<span class="number">3</span> * n     <span class="comment">// 变量引用匿名函数</span></span><br><span class="line">    scala&gt;<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>).map( (x) =&gt; x * <span class="number">3</span> );    <span class="comment">//把匿名函数传递给 map 方法</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜ </span><br><span class="line"></span><br><span class="line"><span class="comment">//定义高阶函数 例子3</span></span><br><span class="line">==============================================================================</span><br><span class="line"><span class="comment">// 定义 call 方法, 其中2个参数也为函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call</span></span>(a:<span class="type">Int</span>, b:<span class="type">Int</span>, f1:(<span class="type">Int</span>,<span class="type">Int</span>)=&gt; <span class="type">Int</span>, f2:(<span class="type">Int</span>,<span class="type">Int</span>)=&gt; <span class="type">Int</span>) = &#123;</span><br><span class="line">    <span class="keyword">if</span>(a &gt; <span class="number">0</span>)  f1(a,b);</span><br><span class="line">    <span class="keyword">else</span>       f2(a,b);</span><br><span class="line">&#125;</span><br><span class="line">&gt;&gt;&gt;&gt; call: (a: <span class="type">Int</span>, b: <span class="type">Int</span>, f1: (<span class="type">Int</span>, <span class="type">Int</span>) =&gt; <span class="type">Int</span>, f2: (<span class="type">Int</span>, <span class="type">Int</span>) =&gt; <span class="type">Int</span>)<span class="type">Int</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// -------&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  定义方法, 再把方法转为函数</span></span><br><span class="line"><span class="comment">// 定义2个参数的方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(a:<span class="type">Int</span>,b:<span class="type">Int</span>) = a+b</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sub</span></span>(a:<span class="type">Int</span>,b:<span class="type">Int</span>) = a-b</span><br><span class="line"></span><br><span class="line"><span class="comment">// 方法转为 f1, f2  (当然, 也可以直接定义函数)</span></span><br><span class="line"><span class="keyword">val</span> f1 = add _</span><br><span class="line"><span class="keyword">val</span> f2 = sub _</span><br><span class="line"></span><br><span class="line"><span class="comment">// -------&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  直接定义函数</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> f1 = (a : <span class="type">Int</span>, b : <span class="type">Int</span>) =&gt; &#123; a + b &#125;</span><br><span class="line">f1: (<span class="type">Int</span>, <span class="type">Int</span>) =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1388</span>/<span class="number">1164150759</span>@<span class="number">4</span>d41b630</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> f2 = (a : <span class="type">Int</span>, b : <span class="type">Int</span>) =&gt; &#123; a - b &#125;</span><br><span class="line">f2: (<span class="type">Int</span>, <span class="type">Int</span>) =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1389</span>/<span class="number">1260221930</span>@<span class="number">37</span>dc38cc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 调用, 可以用f1,f2,  也可以直接使用方法名调用, 会默认转为函数 :TODO 了解原理</span></span><br><span class="line">call(<span class="number">-1</span>,<span class="number">2</span>,f1,f2)</span><br><span class="line">call(<span class="number">1</span>,<span class="number">2</span>,add _, sub _)</span><br><span class="line">call(<span class="number">1</span>,<span class="number">2</span>,add,sub)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// call 函数提升</span></span><br><span class="line">============================================================</span><br><span class="line">需求: 以 call 函数的返回值<span class="number">3</span>为系数, 然后传另一个数 x 到  函数y = <span class="number">3</span> * x 中, 得到 y</span><br><span class="line">------------------------------------------------------------</span><br><span class="line">    <span class="keyword">val</span> f = call(<span class="number">1</span>,<span class="number">2</span>,f1,f2)</span><br><span class="line">    f(<span class="number">100</span>) = <span class="number">300</span> ;</span><br><span class="line">    call(<span class="number">1</span>,<span class="number">2</span>,add _,sub _)(<span class="number">100</span>) = <span class="number">300</span></span><br><span class="line"></span><br><span class="line">--&gt;&gt; 定义</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span></span>(a:<span class="type">Int</span>,b:<span class="type">Int</span>,f1:(<span class="type">Int</span>,<span class="type">Int</span>)=&gt;<span class="type">Int</span>,f2:(<span class="type">Int</span>,<span class="type">Int</span>)=&gt;<span class="type">Int</span>)= &#123;</span><br><span class="line">        <span class="keyword">var</span> n = <span class="number">0</span> ;</span><br><span class="line">        <span class="keyword">if</span>(a &gt; <span class="number">0</span>) n = f1(a,b) ;</span><br><span class="line">        <span class="keyword">else</span>      n = f2(a,b) ;</span><br><span class="line">        <span class="comment">// 定义方法</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">multi</span></span>(x:<span class="type">Int</span>) = x * n ;</span><br><span class="line">        <span class="comment">// 转为函数</span></span><br><span class="line">        multi _</span><br><span class="line">    &#125;</span><br><span class="line">➜ </span><br><span class="line">call: (a: <span class="type">Int</span>, b: <span class="type">Int</span>, f1: (<span class="type">Int</span>, <span class="type">Int</span>) =&gt; <span class="type">Int</span>, f2: (<span class="type">Int</span>, <span class="type">Int</span>) =&gt; <span class="type">Int</span>)<span class="type">Int</span> =&gt; <span class="type">Int</span></span><br><span class="line"></span><br><span class="line">➜ 调用</span><br><span class="line">    scala&gt; call(<span class="number">1</span>,<span class="number">2</span>,add _, sub _)(<span class="number">100</span>)</span><br><span class="line">    res4: <span class="type">Int</span> = <span class="number">300</span></span><br><span class="line"></span><br><span class="line">➜ 也可以在这里直接调用函数, 传的是函数的实参, 函数的实现:</span><br><span class="line"><span class="comment">// f1 的实现是 a+b, f2 的实现是 a-b, 而调用f1或f2 在 call 方法定义的时候</span></span><br><span class="line"><span class="comment">// 就已经指定了, 而参数也由前面的 a, b 传进来, 所以这里就可以直接调用了.</span></span><br><span class="line"><span class="comment">// 此call方法的返回值是一个函数, 此函数需要2个参数, 一个是前面的执行结果</span></span><br><span class="line"><span class="comment">// 一个是需要调用者传进来</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 这里也可以把 call方法 的返回值赋值给一个 函数变量, 再给这个函数变量传值</span></span><br><span class="line"><span class="keyword">val</span> f = call(<span class="number">1</span>,<span class="number">2</span>,(a:<span class="type">Int</span>,b:<span class="type">Int</span>) =&gt; a+b,(a:<span class="type">Int</span>,b:<span class="type">Int</span>) =&gt; a-b)</span><br><span class="line">scala&gt; f(<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">也可以直接在后面接上 <span class="number">100</span>, 直接传参</span><br><span class="line">call(<span class="number">1</span>,<span class="number">2</span>,(a:<span class="type">Int</span>,b:<span class="type">Int</span>) =&gt; a+b,(a:<span class="type">Int</span>,b:<span class="type">Int</span>) =&gt; a-b)(<span class="number">100</span>)</span><br><span class="line">res7: <span class="type">Int</span> = <span class="number">300</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 类型参数可以去掉, 因为使用的参数可以自动推导出参数的类型</span></span><br><span class="line">    call(<span class="number">1</span>,<span class="number">2</span>,(a:<span class="type">Int</span>,b:<span class="type">Int</span>)=&gt;a + b , (a:<span class="type">Int</span>,b:<span class="type">Int</span>)=&gt; a- b)(<span class="number">100</span>)</span><br><span class="line"><span class="comment">// 精简过后</span></span><br><span class="line">    call(<span class="number">1</span>,<span class="number">2</span>,(a,b)=&gt;a + b , (a,b)=&gt; a- b)(<span class="number">100</span>)            </span><br><span class="line"></span><br><span class="line">➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜ ➜➜➜➜➜➜➜➜➜➜➜➜➜➜ </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//函数推断</span></span><br><span class="line"><span class="comment">// 很显然, 此方法参数是一个函数, 函数只有一个参数, 参数\返回值都是 Double 类型</span></span><br><span class="line"><span class="comment">// 注意: 里面的函数是 形参, 也就是说 只是顶一个一个 f 函数, 参数&amp;返回值类型为 Double, 并没有定义实现, 在传函数的时候, 要给出实现</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">valueAt</span></span>(f:(<span class="type">Double</span>)=&gt;<span class="type">Double</span>) = f(<span class="number">0.25</span>)</span><br><span class="line">➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜</span><br><span class="line">    valueAt((x:<span class="type">Double</span>)=&gt;x * <span class="number">3</span>)                <span class="comment">//定义类型</span></span><br><span class="line">    valueAt((x)=&gt;x * <span class="number">3</span>)                       <span class="comment">//推断类型</span></span><br><span class="line">    valueAt(x=&gt;x * <span class="number">3</span>)                         <span class="comment">//省略()</span></span><br><span class="line">    valueAt(<span class="number">3</span> * _)                            <span class="comment">//参数在右侧出现1次，就可以使用_代替。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//高级函数</span></span><br><span class="line">➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜</span><br><span class="line">    scala&gt; <span class="keyword">val</span> arr = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">    scala&gt; arr.map(<span class="number">2</span> * _);                    <span class="comment">//每个元素x2</span></span><br><span class="line">    scala&gt; arr.map((e:<span class="type">Int</span>)=&gt; e * <span class="number">2</span>);           <span class="comment">//每个元素x2</span></span><br><span class="line">    scala&gt; arr.map(_ * <span class="number">2</span>);                    <span class="comment">//每个元素x2</span></span><br><span class="line">scala&gt; arr.filter(_ % <span class="number">2</span> == <span class="number">0</span>)<span class="comment">// 过滤出集合中的偶数</span></span><br><span class="line">res55: scala.collection.immutable.<span class="type">IndexedSeq</span>[<span class="type">Int</span>] = <span class="type">Vector</span>(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 其它的简单应用</span></span><br><span class="line">➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜</span><br><span class="line"><span class="comment">// 在元素 * 3前先打印</span></span><br><span class="line">    scala&gt; <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>).map(e =&gt; &#123;println(e); e * <span class="number">3</span>&#125;)</span><br><span class="line">    <span class="number">1</span></span><br><span class="line">    <span class="number">2</span></span><br><span class="line">    <span class="number">3</span></span><br><span class="line">res56: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">3</span>, <span class="number">6</span>, <span class="number">9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// filter 过滤</span></span><br><span class="line">    scala&gt; <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>).filter(e =&gt; e%<span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">    res14: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1-10 先扩大3倍, 再过滤出偶数</span></span><br><span class="line">    scala&gt; <span class="type">Array</span>(<span class="number">1</span> to <span class="number">10</span>:_*).map(<span class="number">3</span> * _).filter(_ % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">    res16: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">6</span>, <span class="number">12</span>, <span class="number">18</span>, <span class="number">24</span>, <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//输出三角形 foreach 函数</span></span><br><span class="line">➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜</span><br><span class="line">    scala&gt;(<span class="number">1</span> to <span class="number">9</span>).map(<span class="string">"*"</span> * _).foreach(println)</span><br><span class="line">    scala&gt;(<span class="number">1</span> to <span class="number">9</span>).map(<span class="string">"*"</span> * _).foreach(println _)</span><br><span class="line">    scala&gt;(<span class="number">1</span> to <span class="number">9</span>).map(<span class="string">"*"</span> * _).foreach(println (_))</span><br><span class="line">    scala&gt; <span class="type">Array</span>(<span class="number">1</span> to <span class="number">9</span>:_*).map(<span class="string">"*"</span> * _).foreach(println _)</span><br><span class="line"></span><br><span class="line"><span class="comment">// reduceLeft / reduceRight 函数</span></span><br><span class="line">➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜</span><br><span class="line"><span class="comment">//reduceLeft,由左至右, 每次进2个参数</span></span><br><span class="line">    <span class="keyword">val</span> r = (<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">    scala&gt; r.reduceLeft(_ + _)</span><br><span class="line">    scala&gt; r.reduceLeft(_ - _)</span><br><span class="line">...</span><br><span class="line">→ 执行流程</span><br><span class="line"><span class="comment">//1,2,3 ==&gt; (1 - 2) -3)    = -4</span></span><br><span class="line"></span><br><span class="line"><span class="type">MR</span>:<span class="type">MapTask</span> + reduceTask,映射化简.</span><br><span class="line"></span><br><span class="line"><span class="comment">//reduceRight,由右至左</span></span><br><span class="line">scala&gt; r.reduceRight(_-_)</span><br><span class="line">...</span><br><span class="line">→ 执行流程</span><br><span class="line">    <span class="comment">//1,2,3 ==&gt;1 - (2 - 3)= 2</span></span><br><span class="line">    <span class="comment">//1,2,3,4 ==&gt; 1 - (2 - (3 - 4)) = -2</span></span><br><span class="line">    <span class="comment">//1,2,3,4 ==&gt; 1 - (2 - (3 - 4)) = -2</span></span><br></pre></td></tr></table></figure><hr><h1 id="十一-柯里化"><a href="#十一-柯里化" class="headerlink" title="十一. 柯里化"></a>十一. 柯里化</h1><p><strong>概念:</strong>  以逻辑学家 Haskell Brooks Curry 的名字命名 </p><p><strong>将原来接受2个参数的函数, 变成新的接受一个参数的函数的过程.</strong> </p><p><strong>新的函数返回一个以原有第二个参数作为参数的函数</strong> </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">→ 原函数</span><br><span class="line">scala&gt;<span class="function"><span class="keyword">def</span> <span class="title">mul</span></span>(a:<span class="type">Int</span>,b:<span class="type">Int</span>) = a  * b;</span><br><span class="line">scala&gt;mul(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">→ 柯里化</span><br><span class="line">scala&gt;<span class="function"><span class="keyword">def</span> <span class="title">mulone</span></span>(a:<span class="type">Int</span>) = &#123;(x:<span class="type">Int</span>) =&gt; a * x ;&#125;</span><br><span class="line">scala&gt;mulone(<span class="number">1</span>)(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">→→→→→→→ 柯里化测试</span><br><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">mulone</span></span>(a:<span class="type">Int</span>) = &#123;(x:<span class="type">Int</span>) =&gt; a * x&#125;</span><br><span class="line">mulone: (a: <span class="type">Int</span>)<span class="type">Int</span> =&gt; <span class="type">Int</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 首先传入一个参数, 得到的是另一函数 x:Int =&gt; x * 2 (也就是原方法的方法体)</span></span><br><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">f</span> </span>= mulone(<span class="number">2</span>)</span><br><span class="line">f: <span class="type">Int</span> =&gt; <span class="type">Int</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 此时接着再传入参数 4, 得到 4*2 =&gt; 8 </span></span><br><span class="line">scala&gt; f(<span class="number">4</span>)</span><br><span class="line">res33: <span class="type">Int</span> = <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 直接传入2个参数可直接调用此方法, 返回结果</span></span><br><span class="line">scala&gt; mulone(<span class="number">2</span>)(<span class="number">4</span>)</span><br><span class="line">res34: <span class="type">Int</span> = <span class="number">8</span></span><br></pre></td></tr></table></figure><p><strong>与柯里化相关的知识点(方法向函数的转换)</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>) 多个参数的方法可以转换为多元函数</span><br><span class="line">-------------------</span><br><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">plus</span></span>(x: <span class="type">Int</span>, y: <span class="type">Int</span>): <span class="type">Int</span> = x + y</span><br><span class="line">plus: (x: <span class="type">Int</span>, y: <span class="type">Int</span>)<span class="type">Int</span></span><br><span class="line"></span><br><span class="line">scala&gt; plus _</span><br><span class="line">res63: (<span class="type">Int</span>, <span class="type">Int</span>) =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1495</span>/<span class="number">772136419</span>@<span class="number">16</span>b54416</span><br><span class="line"></span><br><span class="line"><span class="number">2</span>) 多个参数的方法变成柯里函数 ---&gt;  方法定义的时候, 不同的参数用 '()' 分开</span><br><span class="line">---------------------</span><br><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">plus</span></span>(x: <span class="type">Int</span>)(y: <span class="type">Int</span>): <span class="type">Int</span> = x + y</span><br><span class="line">plus: (x: <span class="type">Int</span>)(y: <span class="type">Int</span>)<span class="type">Int</span></span><br><span class="line"></span><br><span class="line">scala&gt; plus _</span><br><span class="line">res64: <span class="type">Int</span> =&gt; (<span class="type">Int</span> =&gt; <span class="type">Int</span>) = $$<span class="type">Lambda</span>$<span class="number">1496</span>/<span class="number">662093445</span>@<span class="number">51927988</span></span><br></pre></td></tr></table></figure><hr><h1 id="十二-控制抽象"><a href="#十二-控制抽象" class="headerlink" title="十二. 控制抽象"></a>十二. 控制抽象</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 类似于 OC 中的 block, 此 block 无参数无返回值 :TODO 是控制</span></span><br><span class="line">➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜</span><br><span class="line"><span class="number">1</span>) 定义过程，启动分线程执行block代码. 方法的参数是一个 block 无参无返函数</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">newThread</span></span>(block: () =&gt; <span class="type">Unit</span>) &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">Thread</span>() &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</span><br><span class="line">          block();</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;.start();</span><br><span class="line">    &#125;</span><br><span class="line">-----------------</span><br><span class="line"><span class="number">2</span>) 调用 newThread方法, 并给 block 传值 <span class="symbol">'newThread</span>(()=&gt;&#123;...&#125;)'</span><br><span class="line">    newThread(() =&gt; &#123;</span><br><span class="line">      (<span class="number">1</span> to <span class="number">10</span>).foreach(e =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> tname = <span class="type">Thread</span>.currentThread.getName();</span><br><span class="line">        println(tname + <span class="string">" : "</span> + e);</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">    );</span><br><span class="line">-------打印结果----------</span><br><span class="line">scala&gt; <span class="type">Thread</span><span class="number">-3</span> : <span class="number">1</span></span><br><span class="line"><span class="type">Thread</span><span class="number">-3</span> : <span class="number">2</span></span><br><span class="line"><span class="type">Thread</span><span class="number">-3</span> : <span class="number">3</span></span><br><span class="line"><span class="type">Thread</span><span class="number">-3</span> : <span class="number">4</span></span><br><span class="line"><span class="type">Thread</span><span class="number">-3</span> : <span class="number">5</span></span><br><span class="line"><span class="type">Thread</span><span class="number">-3</span> : <span class="number">6</span></span><br><span class="line"><span class="type">Thread</span><span class="number">-3</span> : <span class="number">7</span></span><br><span class="line"><span class="type">Thread</span><span class="number">-3</span> : <span class="number">8</span></span><br><span class="line"><span class="type">Thread</span><span class="number">-3</span> : <span class="number">9</span></span><br><span class="line"><span class="type">Thread</span><span class="number">-3</span> : <span class="number">10</span></span><br><span class="line"></span><br><span class="line">➜➜➜➜➜➜➜➜➜➜➜➜➜ </span><br><span class="line"><span class="type">PS</span>: 在 定义方法的时候, 可省略<span class="symbol">'newThread</span>(()=&gt;&#123;...&#125;)'前面的括号, 因为没有参数</span><br><span class="line">   在调用方法, 写 block 中的内容时 <span class="symbol">'newThread</span>(() =&gt; &#123;&#125;)', 可以直接写大括号&#123;&#125;中的代码逻辑, 其实这玩意儿就是 语法糖</span><br></pre></td></tr></table></figure><hr><h1 id="十三-集合"><a href="#十三-集合" class="headerlink" title="十三. 集合"></a>十三. 集合</h1><h4 id="List-集合"><a href="#List-集合" class="headerlink" title="List 集合"></a>List 集合</h4><p><strong>List 在添加元素时注意点,详细代码见下面:</strong> </p><ul><li>使用 <code>+:</code>  或<code>:=</code>添加单个元素时,  <code>:</code>必须靠着 list 一侧, <code>+</code> 必须靠着 单个元素 </li><li>如果2边都是 list 集合, 必须用  <code>++:</code>或者 <code>:::</code>  或者 <code>++</code></li><li>不管是 <code>+: , :+ ,  ++:</code>,  都不会改变原 list 的值 </li><li>要添加的 <code>元素 / list</code> 在原 list 的哪边, 就添加到 list 的 那边</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Nil (List空集合)</span></span><br><span class="line">➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜ </span><br><span class="line">scala&gt; <span class="number">1</span>::<span class="number">2</span>::<span class="type">Nil</span></span><br><span class="line">res71: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">var</span> l1 = <span class="number">1</span>::<span class="number">2</span>::<span class="type">Nil</span></span><br><span class="line">l1: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; l1</span><br><span class="line">res72: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="number">9</span>::l1</span><br><span class="line">res73: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">9</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// List 求和</span></span><br><span class="line">➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜ </span><br><span class="line">方式<span class="number">1</span>-递归</span><br><span class="line">-----------</span><br><span class="line">定义&gt;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(list:<span class="type">List</span>[<span class="type">Int</span>]) : <span class="type">Int</span> = &#123;</span><br><span class="line">        <span class="keyword">if</span>(list == <span class="type">Nil</span>) <span class="number">0</span> <span class="keyword">else</span> list.head + sum(list.tail)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">sum: (list: <span class="type">List</span>[<span class="type">Int</span>])<span class="type">Int</span></span><br><span class="line"></span><br><span class="line">调用&gt;</span><br><span class="line">    scala&gt; <span class="keyword">val</span> list = <span class="number">1</span>::<span class="number">2</span>::<span class="number">3</span>::<span class="number">4</span>::<span class="type">Nil</span></span><br><span class="line">    list: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; sum(list)</span><br><span class="line">res41: <span class="type">Int</span> = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">方式<span class="number">2</span>-模式匹配</span><br><span class="line">-----------</span><br><span class="line">其实本质上也是递归</span><br><span class="line">定义&gt;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(list:<span class="type">List</span>[<span class="type">Int</span>]) : <span class="type">Int</span> = list <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Nil</span> =&gt; <span class="number">0</span></span><br><span class="line">    <span class="keyword">case</span> a::b =&gt; a + sum(b)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">注意: ::将列表 析构 成头部和尾部</span><br><span class="line">递归之所以那么自然, 因为列表的尾部正好又是一个列表</span><br><span class="line">-------------------------------------------------------</span><br><span class="line">scala 类库的 sum</span><br><span class="line"><span class="type">List</span>(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>).sum</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// List 添加元素</span></span><br><span class="line">➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜ </span><br><span class="line">注意点: <span class="number">1</span>) 使用 +:  或 := 添加单个元素时,  : 必须靠着 list 一侧, + 必须靠着 单个元素</span><br><span class="line">        <span class="number">2</span>) 如果<span class="number">2</span>边都是 list, 必须用  ++: </span><br><span class="line">        <span class="number">3</span>) 不管是 +:  :+   ++:,  都不会改变原 list 的值</span><br><span class="line">    <span class="number">4</span>) 要添加的 元素 / list 在哪边, 就添加到数组的哪边</span><br><span class="line">    scala&gt; <span class="keyword">var</span> list2 = <span class="number">1</span>::<span class="number">2</span>::<span class="number">3</span>::<span class="number">5</span>::<span class="number">4</span>::<span class="type">Nil</span></span><br><span class="line">    list2: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    scala&gt; list2</span><br><span class="line">    res46: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    scala&gt; list2 :+ <span class="number">3</span></span><br><span class="line">    res81: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    scala&gt; <span class="number">3</span> +: list2</span><br><span class="line">    res82: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    scala&gt; list2</span><br><span class="line">    res88: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    scala&gt; list2 ++: <span class="type">List</span>(<span class="number">99</span>)</span><br><span class="line">    res86: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">99</span>)</span><br><span class="line"></span><br><span class="line">    scala&gt; <span class="type">List</span>(<span class="number">78</span>) ++: list2</span><br><span class="line">    res87: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">78</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// ++ 添加 实现2集合合并</span></span><br><span class="line">    scala&gt;<span class="keyword">val</span> l1 = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    scala&gt;<span class="keyword">val</span> l2 = <span class="type">List</span>(<span class="number">3</span>,<span class="number">4</span>)  </span><br><span class="line">    scala&gt;l1 ++ l2               ++  与          :: 效果相同     <span class="comment">// 是在l1的后面加上l2</span></span><br><span class="line">    scala&gt;l1 ++: l2              ++: 与 :::  效果相同          <span class="comment">// 是在 l2的前面加上l1</span></span><br></pre></td></tr></table></figure><h4 id="Set-集合"><a href="#Set-集合" class="headerlink" title="Set 集合"></a>Set 集合</h4><p><strong>注意:</strong> </p><ul><li><strong>不可变 Set</strong> 添加的是 <strong>元祖</strong>, 使用<code>+=</code>  产生新集合, <strong>自身无法改变</strong>, 可以赋值给新的常量 val</li><li><strong>可变 Set 也可以使用 <code>+=</code> 添加 元祖</strong>, <strong>改变自身</strong>,  还可以<strong>使用 <code>++=</code> 添加其他集合, 比如 List, Array, Range(1 to 10) 等…</strong></li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Set 可变集合的操作(默认)</span></span><br><span class="line">------------------------------------------------------------</span><br><span class="line"><span class="comment">// Set 添加成员(元祖, 会自动去重, 无序)</span></span><br><span class="line">    scala&gt;<span class="keyword">val</span> set = <span class="type">Set</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">    scala&gt;set + (<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>)</span><br><span class="line">    scala&gt;set - (<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    scala&gt;<span class="keyword">val</span> s1 = <span class="type">Set</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">    scala&gt;<span class="keyword">val</span> s2 = <span class="type">Set</span>(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">    scala&gt;s1 | s2                   <span class="comment">//并集</span></span><br><span class="line">    scala&gt;s1 &amp; s2                  <span class="comment">//交集</span></span><br><span class="line">    scala&gt;s1 &amp;~ s2                <span class="comment">//差集(1,2,3) - (2,3,4) = (1)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Set 不可变集合的操作</span></span><br><span class="line">------------------------------------------------------------</span><br><span class="line"># 导包</span><br><span class="line">scala&gt;<span class="keyword">import</span> scala.collection.mutable.&#123;<span class="type">Set</span> =&gt; <span class="type">MSet</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用 += 添加元祖</span></span><br><span class="line">scala&gt; mset += <span class="number">3</span></span><br><span class="line">res129: mset.<span class="keyword">type</span> = <span class="type">Set</span>(<span class="number">0</span>, <span class="number">9</span>, <span class="number">45</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">4</span>, <span class="number">98</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">43</span>, 😎</span><br><span class="line"></span><br><span class="line">scala&gt; mset += (<span class="number">24</span>,<span class="number">25</span>,<span class="number">26</span>)</span><br><span class="line">res132: mset.<span class="keyword">type</span> = <span class="type">Set</span>(<span class="number">0</span>, <span class="number">9</span>, <span class="number">45</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">24</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">25</span>, <span class="number">4</span>, <span class="number">98</span>, <span class="number">26</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">21</span>, <span class="number">7</span>, <span class="number">43</span>, 😎</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用 ++= 添加 Array, List, Range...</span></span><br><span class="line">scala&gt; mset ++= <span class="type">Array</span>(<span class="number">3</span>,<span class="number">45</span>)</span><br><span class="line">res125: mset.<span class="keyword">type</span> = <span class="type">Set</span>(<span class="number">0</span>, <span class="number">9</span>, <span class="number">45</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">7</span>, <span class="number">4</span>, 😎</span><br><span class="line"></span><br><span class="line">scala&gt; mset ++= <span class="type">List</span>(<span class="number">43</span>,<span class="number">98</span>)</span><br><span class="line">res128: mset.<span class="keyword">type</span> = <span class="type">Set</span>(<span class="number">0</span>, <span class="number">9</span>, <span class="number">45</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">4</span>, <span class="number">98</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">43</span>, 😎</span><br><span class="line"></span><br><span class="line">scala&gt; mset ++= (<span class="number">0</span> to <span class="number">10</span>)</span><br><span class="line">res123: mset.<span class="keyword">type</span> = <span class="type">Set</span>(<span class="number">0</span>, <span class="number">9</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">7</span>, <span class="number">4</span>, 😎</span><br></pre></td></tr></table></figure><h4 id="其他的一些操作"><a href="#其他的一些操作" class="headerlink" title="其他的一些操作"></a>其他的一些操作</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 主要是 ArrayBuffer的</span></span><br><span class="line">------------------------------------------------------------</span><br><span class="line">scala&gt; <span class="keyword">import</span> scala.collection.mutable._</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable._</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> buf = <span class="type">ArrayBuffer</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">buf: scala.collection.mutable.<span class="type">ArrayBuffer</span>[<span class="type">Int</span>] = <span class="type">ArrayBuffer</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="number">2</span> +=: buf</span><br><span class="line">res138: buf.<span class="keyword">type</span> = <span class="type">ArrayBuffer</span>(<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 提取前2个元素, buf 本身不变</span></span><br><span class="line">scala&gt; buf.take(<span class="number">2</span>)</span><br><span class="line">res139: scala.collection.mutable.<span class="type">ArrayBuffer</span>[<span class="type">Int</span>] = <span class="type">ArrayBuffer</span>(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 删除前2个元素, buf 本身不变</span></span><br><span class="line">scala&gt; buf.drop(<span class="number">2</span>)</span><br><span class="line">res141: scala.collection.mutable.<span class="type">ArrayBuffer</span>[<span class="type">Int</span>] = <span class="type">ArrayBuffer</span>(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; buf</span><br><span class="line">res142: scala.collection.mutable.<span class="type">ArrayBuffer</span>[<span class="type">Int</span>] = <span class="type">ArrayBuffer</span>(<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在指定位置进行切割，形成两个新集合。</span></span><br><span class="line">scala&gt; buf.splitAt(<span class="number">2</span>)</span><br><span class="line">res143: (scala.collection.mutable.<span class="type">ArrayBuffer</span>[<span class="type">Int</span>], scala.collection.mutable.<span class="type">ArrayBuffer</span>[<span class="type">Int</span>]) = (<span class="type">ArrayBuffer</span>(<span class="number">2</span>, <span class="number">1</span>),<span class="type">ArrayBuffer</span>(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> b1 = <span class="type">ArrayBuffer</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">scala&gt; <span class="keyword">val</span> b2 = <span class="type">ArrayBuffer</span>(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 舍弃掉少的</span></span><br><span class="line">scala&gt; b1.zip(b2)</span><br><span class="line">res145: scala.collection.mutable.<span class="type">ArrayBuffer</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ArrayBuffer</span>((<span class="number">1</span>,<span class="number">3</span>), (<span class="number">2</span>,<span class="number">4</span>), (<span class="number">3</span>,<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">//  b1不够, 用-1补, b2不够, 用-2补</span></span><br><span class="line">scala&gt; b1.zipAll(b2,<span class="number">-1</span>,<span class="number">-2</span>)</span><br><span class="line">res146: scala.collection.mutable.<span class="type">ArrayBuffer</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ArrayBuffer</span>((<span class="number">1</span>,<span class="number">3</span>), (<span class="number">2</span>,<span class="number">4</span>), (<span class="number">3</span>,<span class="number">5</span>), (<span class="number">-1</span>,<span class="number">6</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">//元素和自己的索引形成tuple.,元素在前, 索引在后</span></span><br><span class="line">scala&gt; b1.zipWithIndex</span><br><span class="line">res147: scala.collection.mutable.<span class="type">ArrayBuffer</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ArrayBuffer</span>((<span class="number">1</span>,<span class="number">0</span>), (<span class="number">2</span>,<span class="number">1</span>), (<span class="number">3</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; b1.zip(b2).zipWithIndex</span><br><span class="line">res148: scala.collection.mutable.<span class="type">ArrayBuffer</span>[((<span class="type">Int</span>, <span class="type">Int</span>), <span class="type">Int</span>)] = <span class="type">ArrayBuffer</span>(((<span class="number">1</span>,<span class="number">3</span>),<span class="number">0</span>), ((<span class="number">2</span>,<span class="number">4</span>),<span class="number">1</span>), ((<span class="number">3</span>,<span class="number">5</span>),<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 左右折叠</span></span><br><span class="line">------------------------------------------------------------</span><br><span class="line"><span class="comment">//左折叠</span></span><br><span class="line">scala&gt; <span class="type">List</span>(<span class="number">1</span>,<span class="number">7</span>,<span class="number">2</span>,<span class="number">9</span>).foldLeft(<span class="number">0</span>)(_-_)</span><br><span class="line">res87: <span class="type">Int</span> = <span class="number">-19</span></span><br><span class="line">    (<span class="number">0</span> - <span class="number">1</span>) - <span class="number">7</span> - <span class="number">2</span> - <span class="number">9</span>            <span class="comment">//-19</span></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment">//右折叠</span></span><br><span class="line">scala&gt; <span class="type">List</span>(<span class="number">1</span>,<span class="number">7</span>,<span class="number">2</span>,<span class="number">9</span>).foldRight(<span class="number">0</span>)(_-_)</span><br><span class="line">res88: <span class="type">Int</span> = <span class="number">-13</span></span><br><span class="line">    <span class="number">1</span>  - (<span class="number">7</span> - (<span class="number">2</span> - (<span class="number">9</span> - <span class="number">0</span>))_    <span class="comment">//-13</span></span><br></pre></td></tr></table></figure><hr><h1 id="十四-模式匹配-amp-偏函数"><a href="#十四-模式匹配-amp-偏函数" class="headerlink" title="十四. 模式匹配 &amp; 偏函数"></a>十四. 模式匹配 &amp; 偏函数</h1><h4 id="1-字符串匹配"><a href="#1-字符串匹配" class="headerlink" title="1. 字符串匹配"></a>1. 字符串匹配</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> x = '<span class="number">9</span>'</span><br><span class="line">x: <span class="type">Char</span> = <span class="number">9</span></span><br><span class="line"></span><br><span class="line">scala&gt; x <span class="keyword">match</span>&#123;</span><br><span class="line">     |   <span class="keyword">case</span> '+' =&gt; print(<span class="string">"++++"</span>)</span><br><span class="line">     |   <span class="keyword">case</span> '_' =&gt; print(<span class="string">"-----"</span>)</span><br><span class="line">     |   <span class="keyword">case</span> _ <span class="keyword">if</span> <span class="type">Character</span>.isDigit(x) =&gt; print(<span class="string">"is number!"</span>)</span><br><span class="line">     |   <span class="keyword">case</span> _ =&gt; print(<span class="string">"..."</span>)</span><br><span class="line">     | &#125;</span><br><span class="line">&gt; is number!</span><br></pre></td></tr></table></figure><h4 id="2-匹配类型-x类型定义成判断类型的共同超类。"><a href="#2-匹配类型-x类型定义成判断类型的共同超类。" class="headerlink" title="2. 匹配类型,x类型定义成判断类型的共同超类。"></a>2. 匹配类型,x类型定义成判断类型的共同超类。</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;    <span class="keyword">val</span> x:<span class="type">Any</span> = <span class="string">"123"</span></span><br><span class="line">x: <span class="type">Any</span> = <span class="number">123</span></span><br><span class="line"></span><br><span class="line">scala&gt; x <span class="keyword">match</span>&#123;</span><br><span class="line">     |   <span class="keyword">case</span> b:<span class="type">Int</span> =&gt; print(<span class="string">"is Int"</span>) ;</span><br><span class="line">     |   <span class="keyword">case</span> a:<span class="type">String</span> =&gt; print(<span class="string">"is String"</span>) ;</span><br><span class="line">     |   <span class="keyword">case</span> _ =&gt; print(<span class="string">"is Int"</span>) ;</span><br><span class="line">     | &#125;</span><br><span class="line">is <span class="type">String</span></span><br></pre></td></tr></table></figure><h4 id="3-匹配数组数据"><a href="#3-匹配数组数据" class="headerlink" title="3. 匹配数组数据"></a>3. 匹配数组数据</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> arr = <span class="type">Array</span>(<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">arr: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; arr <span class="keyword">match</span>&#123;</span><br><span class="line">     |     <span class="comment">//匹配arr中只有 0 </span></span><br><span class="line">     |     <span class="keyword">case</span> <span class="type">Array</span>(<span class="number">0</span>) =&gt; println(<span class="string">"arr 中只含有0"</span>)</span><br><span class="line">     |     <span class="comment">//匹配是否两个元素</span></span><br><span class="line">     |     <span class="keyword">case</span> <span class="type">Array</span>(x,y) =&gt; println(<span class="string">"有两个元素"</span>)</span><br><span class="line">     |     <span class="comment">//是否从0开始</span></span><br><span class="line">     |     <span class="keyword">case</span> <span class="type">Array</span>(<span class="number">0</span>,_*) =&gt; println(<span class="string">"从0开始"</span>)</span><br><span class="line">     |     <span class="keyword">case</span> _ =&gt; println(<span class="string">"有0"</span>) &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 还可以匹配其它复杂数据类型中某个位置上的值.</span></span><br></pre></td></tr></table></figure><h4 id="4-偏函数"><a href="#4-偏函数" class="headerlink" title="4. 偏函数"></a>4. 偏函数</h4><p><strong>把模式匹配抽出来变成函数了</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"> ---定义</span><br><span class="line"><span class="keyword">val</span> f : <span class="type">PartialFunction</span>[<span class="type">Char</span>, <span class="type">Int</span>] = &#123;</span><br><span class="line">        <span class="keyword">case</span> '+' =&gt; <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">case</span> '-' =&gt; <span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">case</span> _ =&gt; <span class="number">0</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">---测试</span><br><span class="line">    scala&gt; f('x')</span><br><span class="line">    res106: <span class="type">Int</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    scala&gt; f('+')</span><br><span class="line">    res107: <span class="type">Int</span> = <span class="number">1</span></span><br></pre></td></tr></table></figure><hr><h1 id="十五-样例类"><a href="#十五-样例类" class="headerlink" title="十五. 样例类"></a>十五. 样例类</h1><p><strong>关键字 : case</strong></p><p>主要用于模式匹配.  </p><p><strong>内置了apply和unapply方法，还有串行化等接口。</strong> </p><p>创建对象时不需要使用new. </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">// 创建样例类</span></span><br><span class="line">➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜</span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Dog</span></span>&#123;&#125;</span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Jing8</span>(<span class="params">name:<span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Dog</span></span>&#123;&#125;</span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Shapi</span>(<span class="params">age:<span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Dog</span></span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    scala&gt; <span class="keyword">val</span> d = <span class="type">Jing8</span>(<span class="string">"tom"</span>)<span class="comment">// 可以直接创建, 因为内置了 apply</span></span><br><span class="line"></span><br><span class="line">    d <span class="keyword">match</span>&#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Jing8</span>(name) =&gt; print(<span class="string">"是Jing8 : "</span> + name);</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Shapi</span>(age) =&gt; print(<span class="string">"是Shapi : "</span> + age);</span><br><span class="line">        <span class="keyword">case</span> _ =&gt; print(<span class="string">"aishiuihsui"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">--&gt; 运行结果:</span><br><span class="line">是 <span class="type">Jing8</span>: tom</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 查看 .class 文件</span></span><br><span class="line"><span class="comment">// 发现有 apply, unapply, implements scala.Serializable 等</span></span><br><span class="line">--------------------------------------------------</span><br><span class="line">➜  scala javap -<span class="keyword">private</span> <span class="type">Jing8</span>$.<span class="keyword">class</span></span><br><span class="line"><span class="type">Compiled</span> from <span class="string">"dog.scala"</span></span><br><span class="line">public <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Jing8$</span> <span class="keyword">extends</span> <span class="title">scala</span>.<span class="title">runtime</span>.<span class="title">AbstractFunction1&lt;java</span>.<span class="title">lang</span>.<span class="title">String</span>, <span class="title">Jing8&gt;</span> <span class="title">implements</span> <span class="title">scala</span>.<span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  public static <span class="type">Jing8</span>$ <span class="type">MODULE</span>$;</span><br><span class="line">  public static &#123;&#125;;</span><br><span class="line">  public <span class="keyword">final</span> java.lang.<span class="type">String</span> toString();</span><br><span class="line">  public <span class="type">Jing8</span> apply(java.lang.<span class="type">String</span>);</span><br><span class="line">  public scala.<span class="type">Option</span>&lt;java.lang.<span class="type">String</span>&gt; unapply(<span class="type">Jing8</span>);</span><br><span class="line">  <span class="keyword">private</span> java.lang.<span class="type">Object</span> readResolve();</span><br><span class="line">  public java.lang.<span class="type">Object</span> apply(java.lang.<span class="type">Object</span>);</span><br><span class="line">  <span class="keyword">private</span> <span class="type">Jing8</span>$();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 密封样例类</span></span><br><span class="line">➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜</span><br><span class="line"> 子类和父类必须定义在同一文件中。</span><br><span class="line">    <span class="keyword">sealed</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Dog</span></span>&#123;&#125;</span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Jing8</span>(<span class="params">name:<span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Dog</span></span>&#123;&#125;</span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Shapi</span>(<span class="params">age:<span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Dog</span></span>&#123;&#125;</span><br></pre></td></tr></table></figure><h1 id="十六-泛型"><a href="#十六-泛型" class="headerlink" title="十六. 泛型"></a>十六. 泛型</h1><h4 id="1-泛型的定义-amp-上下界"><a href="#1-泛型的定义-amp-上下界" class="headerlink" title="1. 泛型的定义 &amp; 上下界"></a>1. 泛型的定义 &amp; 上下界</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Java 中常用的泛型</span></span><br><span class="line">    <span class="type">List</span>&lt;<span class="type">String</span>&gt;            </span><br><span class="line">    <span class="type">Map</span>&lt;<span class="type">String</span>,<span class="type">String</span>&gt;      </span><br><span class="line"></span><br><span class="line"><span class="comment">// Scala 中的泛型</span></span><br><span class="line">➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜</span><br><span class="line"></span><br><span class="line"><span class="comment">//类的泛型,定义泛型类</span></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Pair</span>[<span class="type">T</span>,<span class="type">S</span>](<span class="params">one:<span class="type">T</span>,second:<span class="type">S</span></span>)</span>;        <span class="comment">//定义泛型类</span></span><br><span class="line">    <span class="keyword">val</span> p = <span class="keyword">new</span> <span class="type">Pair</span>[<span class="type">String</span>,<span class="type">Int</span>](<span class="string">"tom"</span>,<span class="number">12</span>);    <span class="comment">// 创建泛型对象</span></span><br><span class="line">    <span class="keyword">val</span> p = <span class="keyword">new</span> <span class="type">Pair</span>(<span class="string">"tom"</span>,<span class="number">12</span>);                <span class="comment">//类型推断</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//方法泛型</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getMiddle</span></span>[<span class="type">T</span>](arr:<span class="type">Array</span>[<span class="type">T</span>]) = arr(arr.length / <span class="number">2</span>);</span><br><span class="line">-&gt; 可以传入任意类型的 <span class="type">Array</span></span><br><span class="line"></span><br><span class="line">➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜</span><br><span class="line"><span class="comment">//泛型的上界,T必须是Dog的子类。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>[<span class="type">T</span> &lt;: <span class="type">Dog</span>](d:<span class="type">T</span>) = println(<span class="string">"hello"</span>)</span><br><span class="line"><span class="comment">// 定义下界,T必须是shapi的父类。有问题: 测试是通配的, 传啥都可以 :TODO</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run2</span></span>[<span class="type">T</span> &gt;: <span class="type">Shapi</span>](d:<span class="type">T</span>) = println(<span class="string">"hello"</span>)  </span><br><span class="line"></span><br><span class="line">    &lt;:            <span class="comment">//上界，子类</span></span><br><span class="line">    &gt;:            <span class="comment">//下界，父类 ???</span></span><br><span class="line">    &lt;%            <span class="comment">// A &lt;% B,A能够隐式转换成B</span></span><br><span class="line"><span class="type">T</span> &lt;:<span class="type">Dog</span> &gt;:<span class="type">Cat</span>        <span class="comment">//约束多个条件。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 视图界定</span></span><br><span class="line">[<span class="type">T</span> &lt;: <span class="type">Comparable</span>]</span><br></pre></td></tr></table></figure><h4 id="2-泛型的型变-TODO-不懂"><a href="#2-泛型的型变-TODO-不懂" class="headerlink" title="2. 泛型的型变 :TODO 不懂"></a>2. 泛型的型变 :TODO 不懂</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Friend</span>[+<span class="type">Dog</span>]            <span class="comment">//协变, 如果 A是 B的子类, 那么ListA 也是  ListB 的子类</span></span><br><span class="line"><span class="type">Friend</span>[-<span class="type">Dog</span>]            <span class="comment">//逆变</span></span><br><span class="line"></span><br><span class="line"><span class="type">Friend</span>[-<span class="type">Dog</span>]    <span class="comment">// 当采用 -号 约束时, 原本 Friend[NafangShapi] 为   Friend[Shapi] 的子类的, 现在变成父类</span></span><br><span class="line"></span><br><span class="line"><span class="type">Friend</span>[<span class="type">Shapi</span>]</span><br><span class="line"><span class="type">Friend</span>[<span class="type">NafangShapi</span>]</span><br></pre></td></tr></table></figure><hr><h1 id="十七-隐式转换-implicit"><a href="#十七-隐式转换-implicit" class="headerlink" title="十七. 隐式转换 implicit"></a>十七. 隐式转换 <strong>implicit</strong></h1><pre><code>隐式转换函数:使用 **implicit** 修饰的具有一个参数的函数。 </code></pre><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义隐式转换函数 </span></span><br><span class="line">➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜</span><br><span class="line">   <span class="comment">// 定义了之后, 调用 run(100) , 此100会自动隐式的转为 dog 对象</span></span><br><span class="line">   <span class="comment">// 当调用一个对象不存在方法时, 会寻找是否能隐式转换为其它对象</span></span><br><span class="line">   <span class="comment">// 需要当前代码上下文可见</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">int2Dog</span></span>(n:<span class="type">Int</span>) = <span class="type">Shapi</span>(n);   <span class="comment">//定义</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(d:<span class="type">Dog</span>) = print(<span class="string">"hello world"</span>);</span><br><span class="line"> </span><br><span class="line">    run(<span class="number">100</span>) ;      <span class="comment">//调用run 方法, 会直接调用隐式转换函数。 把100作为参数, 创建 Shapi 对象</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义隐式单例对象 </span></span><br><span class="line">➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜</span><br><span class="line">    <span class="class"><span class="keyword">object</span> <span class="title">DogUtil</span></span>&#123;</span><br><span class="line">        <span class="comment">//定义隐式转换函数</span></span><br><span class="line">        <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">str2Dog</span></span>(s:<span class="type">String</span>) = <span class="type">Jing8</span>(s) ;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">--- 使用</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run3</span></span>(d:<span class="type">Dog</span>) = println(<span class="string">"hello world"</span>);</span><br><span class="line">    <span class="keyword">import</span> <span class="type">DogUtil</span>._<span class="comment">// </span></span><br><span class="line">    run3(<span class="string">"tom"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//参数默认值</span></span><br><span class="line">---------------------------------------------</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decorate</span></span>(prefix:<span class="type">String</span> = <span class="string">"[[["</span>,c:<span class="type">String</span>,suffix:<span class="type">String</span>=<span class="string">"]]]"</span>) = ...</span><br><span class="line">    decorate(c= <span class="string">"hello world"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//  隐式参数(默认参数) </span></span><br><span class="line"><span class="comment">// 参数为隐式的参数, 回去找此参数, 如果参数有默认值, 就不需要对此参数赋值</span></span><br><span class="line">➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜➜</span><br><span class="line">    <span class="class"><span class="keyword">object</span> <span class="title">DogUtil2</span></span>&#123;</span><br><span class="line">        <span class="keyword">implicit</span> <span class="keyword">val</span> dog = <span class="type">Jing8</span>(<span class="string">"tomas"</span>) ;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">import</span> <span class="type">DogUtil2</span>._</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run4</span></span>(<span class="keyword">implicit</span> dog:<span class="type">Jing8</span>) = println(<span class="string">"hello : "</span>) ;</span><br><span class="line"></span><br><span class="line">    run4;    <span class="comment">// 注意, 如果不想传参数, 想要调用隐式的默认值, 后面不需要加 (). 或者里面传 null, run4(null)</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一-概览&quot;&gt;&lt;a href=&quot;#一-概览&quot; class=&quot;headerlink&quot; title=&quot;一. 概览&quot;&gt;&lt;/a&gt;一. &lt;strong&gt;概览&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;scala&lt;/strong&gt; : java语言的脚本化。       
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="Scala" scheme="https://airpoet.github.io/categories/Hadoop/Scala/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="Scala" scheme="https://airpoet.github.io/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>数据结构 &amp; 算法</title>
    <link href="https://airpoet.github.io/2018/07/11/Hadoop/10_%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84&amp;%E7%AE%97%E6%B3%95/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-&amp;-%E7%AE%97%E6%B3%95/"/>
    <id>https://airpoet.github.io/2018/07/11/Hadoop/10_数据结构&amp;算法/数据结构-&amp;-算法/</id>
    <published>2018-07-11T15:21:45.174Z</published>
    <updated>2018-07-11T16:20:12.824Z</updated>
    
    <content type="html"><![CDATA[<p><br></p><h2 id="参考-PDF-文档"><a href="#参考-PDF-文档" class="headerlink" title="参考 PDF 文档"></a>参考 PDF 文档</h2><p><a href="https://app.yinxiang.com/shard/s37/nl/7399077/02c256fa-61c7-4ffb-b956-5159389373a0/" target="_blank" rel="noopener">数据结构参考PDF</a></p><p><a href="https://app.yinxiang.com/shard/s37/nl/7399077/2a72211a-8ead-4e30-989f-256b7f15bc83/" target="_blank" rel="noopener">算法基础参考PDF</a></p><p><br></p><h2 id="参考大牛笔记"><a href="#参考大牛笔记" class="headerlink" title="参考大牛笔记"></a>参考大牛笔记</h2><p><a href="https://airpoet.github.io/2018/07/04/Java/Interview/Interview-Notebook/%E7%AE%97%E6%B3%95/">算法综述及代码实现</a></p><p><a href="https://airpoet.github.io/2018/07/04/Java/Interview/Interview-Notebook/Leetcode%20%E9%A2%98%E8%A7%A3/">Leetcode解题</a></p><p><a href="https://airpoet.github.io/2018/07/04/Java/Interview/Interview-Notebook/%E5%89%91%E6%8C%87%20offer%20%E9%A2%98%E8%A7%A3/">剑指 Offer 解题</a></p><p>其它的自行查看</p><p><br></p><h2 id="参考自己代码"><a href="#参考自己代码" class="headerlink" title="参考自己代码"></a>参考自己代码</h2><p><a href="https://github.com/airpoet/bigdata/tree/master/Arithmetic_Project" target="_blank" rel="noopener">mygithub</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&quot;参考-PDF-文档&quot;&gt;&lt;a href=&quot;#参考-PDF-文档&quot; class=&quot;headerlink&quot; title=&quot;参考 PDF 文档&quot;&gt;&lt;/a&gt;参考 PDF 文档&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://app.yinxiang.
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="数据结构&amp;算法" scheme="https://airpoet.github.io/categories/Hadoop/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="数据结构" scheme="https://airpoet.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
      <category term="算法" scheme="https://airpoet.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>i-NIO</title>
    <link href="https://airpoet.github.io/2018/07/11/Hadoop/11_NIO/NIO/"/>
    <id>https://airpoet.github.io/2018/07/11/Hadoop/11_NIO/NIO/</id>
    <published>2018-07-11T03:38:15.042Z</published>
    <updated>2018-07-12T01:03:01.212Z</updated>
    
    <content type="html"><![CDATA[<!-- GFM-TOC --><h1 id="一-同步-异步-amp-阻塞-非阻塞"><a href="#一-同步-异步-amp-阻塞-非阻塞" class="headerlink" title="一.  同步, 异步 &amp; 阻塞, 非阻塞"></a>一.  同步, 异步 &amp; 阻塞, 非阻塞</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p><strong>同步和异步其实指的是，请求发起方对消息结果的获取是主动发起的，还是等被动通知的</strong>。</p><ul><li>如果是请求方主动发起的，<strong>一直在等待应答结果（同步阻塞）</strong></li><li>如果是<strong>结果由服务方通知的，也就是请求方发出请求后，要么在一直等待通知（异步阻塞）, 要么就先去干自己的事了（异步非阻塞)</strong></li></ul><p><strong>调用了一个函数之后，在等待这个函数返回结果之前，当前的线程是处于挂起状态，还是运行状态：</strong></p><ul><li>如果是挂起状态，就意味着<strong>当前线程什么都不能干</strong>，就等着获取结果，这就叫<strong>同步阻塞</strong>，</li><li>如果仍然是运行状态，就意味<strong>当前线程是可以的继续处理其他任务，但要时不时的去看下是否有结果了，这就是同步非阻塞</strong>。</li></ul><h2 id="形象的例子"><a href="#形象的例子" class="headerlink" title="形象的例子"></a>形象的例子</h2><p>故事：老王烧开水。</p><p>出场人物：老王，水壶两把（普通水壶，简称水壶；会响的水壶，简称响水壶）。</p><p>老王想了想，有好几种等待方式<br>1、老王用水壶煮水，并且站在那里，不管水开没开，每隔一定时间看看水开了没    ———-同步阻塞<br>老王想了想，这种方法不够聪明。</p><p>2、老王还是用水壶煮水，不再傻傻的站在那里看水开，跑去寝室上网，但是还是会每隔一段时间过来看看水开了没有，水没有开就走人。    ———-同步非阻塞<br>老王想了想，现在的方法聪明了些，但是还是不够好。</p><p>3、老王这次使用高大上的响水壶来煮水，站在那里，但是不会再每隔一段时间去看水开，而是等水开了，水壶会自动的通知他。    ———-异步阻塞<br>老王想了想，不会呀，既然水壶可以通知我，那我为什么还要傻傻的站在那里等呢，嗯，得换个方法。</p><p>4、老王还是使用响水壶煮水，跑到客厅上网去，等着响水壶自己把水煮熟了以后通知他。    ———-异步非阻塞</p><p>老王豁然，这下感觉轻松了很多。</p><hr><h1 id="二-NIO-中一些重要概念"><a href="#二-NIO-中一些重要概念" class="headerlink" title="二. NIO 中一些重要概念"></a>二. NIO 中一些重要概念</h1><h2 id="Java-IO-模型"><a href="#Java-IO-模型" class="headerlink" title="Java IO 模型"></a>Java IO 模型</h2><p><strong>BIO–同步阻塞：</strong></p><p>JDK1.4 以前我们使用的都是 BIO</p><p>阻塞到我们的读写方法，阻塞到线程来提高并发性能，但是效果不是很好</p><p><strong>NIO–同步非阻塞：</strong>(New IO)</p><p>JDK1.4 Linux 多路复用技术（select 模式）实现 IO 事件的轮询方式：同步 非阻塞的模式，这种方式目前是主流的网络通信模式</p><p>Mina 和 Netty</p><p>– 网络通信框架，比自己写 NIO 要容易些，并且代码可读性更好</p><p><strong>AIO–异步非阻塞 IO：</strong></p><p>JDK1.7（NIO2）真正的异步非阻塞 IO(基于 linux 的 epoll 模式）AIO 目 前使用的还比较少</p><h2 id="通道-channel-amp-缓冲区-buffer"><a href="#通道-channel-amp-缓冲区-buffer" class="headerlink" title="通道 channel &amp; 缓冲区 buffer"></a>通道 channel &amp; 缓冲区 buffer</h2><h4 id="1-通道-channel"><a href="#1-通道-channel" class="headerlink" title="1. 通道 channel"></a>1. 通道 channel</h4><p><strong>关键词:</strong> 模拟流,  打开的连接, 双向可读写, 线程安全,  只能通过缓冲区操作</p><p><strong>类型</strong></p><ul><li>FileChannel：从文件中读写数据；</li><li>DatagramChannel：通过 UDP 读写网络中数据；</li><li><strong>SocketChannel</strong>：通过 TCP 读写网络中数据；</li><li><strong>ServerSocketChannel</strong>：可以监听新进来的 TCP 连接，对每一个新进来的连接都会创建一个 SocketChannel。</li></ul><h4 id="2-缓冲区-buffer"><a href="#2-缓冲区-buffer" class="headerlink" title="2. 缓冲区 buffer"></a>2. 缓冲区 buffer</h4><p>不能直接对通道进行读写数据，而是要先经过缓冲区。</p><p>缓冲区实质上是一个数组, 提供了对数据的结构化访问。</p><p><strong>类型:</strong> </p><ul><li><strong>ByteBuffer</strong></li><li>CharBuffer</li><li>ShortBuffer</li><li>IntBuffer</li><li>LongBuffer</li><li>FloatBuffer</li><li>DoubleBuffer</li></ul><p><strong>关键词:</strong> </p><ul><li>capacity：最大容量；</li><li>position：当前已经读写的字节数；</li><li>limit：还可以读写的字节数。</li></ul><p><a href="https://airpoet.github.io/2018/07/04/Java/Interview/Interview-Notebook/Java%20IO/#%E4%B8%83nio">其它详情可见 Java IO</a> </p><hr><h1 id="三-代码实例"><a href="#三-代码实例" class="headerlink" title="三. 代码实例"></a>三. 代码实例</h1><h2 id="Server"><a href="#Server" class="headerlink" title="Server"></a>Server</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.nio;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.InetSocketAddress;</span><br><span class="line"><span class="keyword">import</span> java.net.Socket;</span><br><span class="line"><span class="keyword">import</span> java.nio.ByteBuffer;</span><br><span class="line"><span class="keyword">import</span> java.nio.channels.*;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 注意: selector挑选器, 维护若干集合</span></span><br><span class="line"><span class="comment"> * selectionKeys : 注册的key</span></span><br><span class="line"><span class="comment"> * selectedKeys  : 挑选出来的key -- 有事件的 key</span></span><br><span class="line"><span class="comment"> * 客户端 &amp; 服务端对应的客户端 SocketChannel(套接字通道), 一般来讲, 只需要对 read 感兴趣</span></span><br><span class="line"><span class="comment"> * 也就是拦截 read 消息, write 的话随时都可以 write</span></span><br><span class="line"><span class="comment"> * sc 是可读可写的</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyServer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 分配缓冲区buf内存</span></span><br><span class="line">        ByteBuffer buf = ByteBuffer.allocate(<span class="number">1024</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 开启挑选器</span></span><br><span class="line">        Selector sel = Selector.open();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 开启 ServerSocket 通道</span></span><br><span class="line">        ServerSocketChannel ssc = ServerSocketChannel.open();</span><br><span class="line">        <span class="comment">// 绑定端口</span></span><br><span class="line">        ssc.socket().bind(<span class="keyword">new</span> InetSocketAddress(<span class="string">"localhost"</span>, <span class="number">8888</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置非阻塞</span></span><br><span class="line">        ssc.configureBlocking(<span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">           <span class="comment">// 在挑选器中注册通道(服务器通道, 和感兴趣的事件 - OP_ACCEPT)</span></span><br><span class="line">        ssc.register(sel, SelectionKey.OP_ACCEPT);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 初始化可选择通道对象(为 ServerSocketChannel &amp; SocketChannel 的共同父类)</span></span><br><span class="line">        SelectableChannel sc = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 挑选器开始选择(阻塞的)</span></span><br><span class="line">            <span class="comment">// 如果么有接收到 感兴趣事件, 就塞在这里</span></span><br><span class="line">            sel.select();</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 能走到这一步, 就是已经接收到了 accept 事件</span></span><br><span class="line">            <span class="comment">// 迭代挑选出来的 key 的集合</span></span><br><span class="line">            Iterator&lt;SelectionKey&gt; it = sel.selectedKeys().iterator();</span><br><span class="line">            <span class="keyword">while</span> (it.hasNext()) &#123;</span><br><span class="line">                SelectionKey key = it.next();</span><br><span class="line"></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    <span class="comment">// 如果注册的 key是可接受的, 就一定是服务器通道</span></span><br><span class="line">                    <span class="keyword">if</span> (key.isAcceptable()) &#123;</span><br><span class="line"></span><br><span class="line">                        <span class="comment">// 取出该通道, 返回一个 SelectableChannel 的超类对象</span></span><br><span class="line">                        sc = key.channel();</span><br><span class="line"></span><br><span class="line">                        <span class="comment">// 因为拿到的 key 是 isAcceptable, 所以可以判断是 ssc 对象, 强转</span></span><br><span class="line">                        <span class="comment">// 并通过 accept()方法, 返回一个 sc 对象(类似于套接字, 与客户端的 sc 对应, 负责跟客户端的 sc 通信)</span></span><br><span class="line">                        SocketChannel sc0 = ((ServerSocketChannel) sc).accept();</span><br><span class="line"></span><br><span class="line">                        <span class="comment">// 设置 sc0 为非阻塞</span></span><br><span class="line">                        sc0.configureBlocking(<span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">                        <span class="comment">// 为接收到的这个 sc 在选择器中, 注册读事件</span></span><br><span class="line">                        sc0.register(sel, SelectionKey.OP_READ);</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 下一次轮询, 发现是来自 读事件 的key</span></span><br><span class="line">                    <span class="keyword">if</span> (key.isReadable()) &#123;</span><br><span class="line">                        <span class="comment">// 取出 channel, 直接强转为 SocketChannel</span></span><br><span class="line">                        SocketChannel sc1 = (SocketChannel) key.channel();</span><br><span class="line"></span><br><span class="line">                        <span class="comment">/// 回复客户端消息, 前面加个头'hello', 然后再写回去</span></span><br><span class="line">                        <span class="comment">// 创建消息字节数组</span></span><br><span class="line">                        <span class="keyword">byte</span>[] helloBytes = <span class="string">"hello: "</span>.getBytes();</span><br><span class="line">                        <span class="comment">// 把字节数组放入 buf</span></span><br><span class="line">                        buf.put(helloBytes, <span class="number">0</span>, helloBytes.length);</span><br><span class="line"></span><br><span class="line">                        <span class="comment">// 读取消息</span></span><br><span class="line">                        <span class="comment">// 不存在读完, 只要不为0, 就一直轮询读</span></span><br><span class="line">                        <span class="comment">// 从通道里读出来,放到缓冲区里</span></span><br><span class="line">                        <span class="keyword">while</span> (sc1.read(buf) &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                            <span class="comment">// 拍板, 定稿, &gt; 0说明有数据</span></span><br><span class="line">                            <span class="comment">// position 归0, limit 置在 已写元素 的后面一格, 此时不接受其它写入了</span></span><br><span class="line">                            buf.flip();</span><br><span class="line">                            <span class="comment">// 持有的 sc 对象写入到channel</span></span><br><span class="line">                            sc1.write(buf);</span><br><span class="line">                            <span class="comment">// 写完后 清空, position 归0, limit 最大</span></span><br><span class="line">                            buf.clear();</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                    <span class="comment">// 如果失败, 移除本次接收到的 keys</span></span><br><span class="line">                    sel.keys().remove(key);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 本次选择器事件处理完了之后</span></span><br><span class="line">            <span class="comment">// 移除所有选择出来的 key</span></span><br><span class="line">            sel.selectedKeys().clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rox.nio;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.ByteArrayOutputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.InetSocketAddress;</span><br><span class="line"><span class="keyword">import</span> java.net.Socket;</span><br><span class="line"><span class="keyword">import</span> java.net.SocketAddress;</span><br><span class="line"><span class="keyword">import</span> java.nio.ByteBuffer;</span><br><span class="line"><span class="keyword">import</span> java.nio.channels.SelectionKey;</span><br><span class="line"><span class="keyword">import</span> java.nio.channels.Selector;</span><br><span class="line"><span class="keyword">import</span> java.nio.channels.SocketChannel;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyClient</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// buf</span></span><br><span class="line">        ByteBuffer buf = ByteBuffer.allocate(<span class="number">1024</span> * <span class="number">8</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 开一个 selector</span></span><br><span class="line">        Selector sel = Selector.open();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 开一个套接字通道</span></span><br><span class="line">        SocketChannel sc = SocketChannel.open();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 下面2种连接没区别</span></span><br><span class="line">        sc.socket().connect(<span class="keyword">new</span> InetSocketAddress(<span class="string">"localhost"</span>, <span class="number">8888</span>));</span><br><span class="line"><span class="comment">//        sc.connect(new InetSocketAddress("localhost", 8888));</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置是否阻塞</span></span><br><span class="line">        sc.configureBlocking(<span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 注册 sel 对象及 关注的 key</span></span><br><span class="line">        sc.register(sel, SelectionKey.OP_READ);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 往 buf 中 put 进数据(一次)</span></span><br><span class="line"><span class="comment">//        buf.put("tom".getBytes());</span></span><br><span class="line"><span class="comment">//        buf.flip();</span></span><br><span class="line"><span class="comment">//        sc.write(buf);</span></span><br><span class="line"><span class="comment">//        buf.clear();</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果想要持续不断的通信, 可以开个分线程 (持续收发消息)</span></span><br><span class="line">        <span class="comment">// 程序是从上往下运行的, 运行到下面没有接受到消息的话会塞住</span></span><br><span class="line">        <span class="comment">// 一旦塞住了, 也就无法继续同服务端通信了</span></span><br><span class="line">        <span class="keyword">new</span> Thread()&#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">int</span> i = <span class="number">0</span> ;</span><br><span class="line">                <span class="comment">// 这里创建一个新的缓冲区专用</span></span><br><span class="line">                ByteBuffer buf2 = ByteBuffer.allocate(<span class="number">1024</span> * <span class="number">8</span>);</span><br><span class="line">                <span class="keyword">while</span>(<span class="keyword">true</span>)&#123;</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        buf2.put((<span class="string">"Tom"</span> +  (i ++)).getBytes());</span><br><span class="line">                        buf2.flip();   <span class="comment">//切换读写</span></span><br><span class="line">                        sc.write(buf2);</span><br><span class="line">                        buf2.clear();</span><br><span class="line">                        Thread.sleep(<span class="number">500</span>);</span><br><span class="line">                    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;.start();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 选择器选择, 接受 服务端返回的 key</span></span><br><span class="line">            sel.select();</span><br><span class="line"></span><br><span class="line">            ByteArrayOutputStream baos = <span class="keyword">new</span> ByteArrayOutputStream();</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 从通道读取到缓冲区</span></span><br><span class="line">            <span class="keyword">while</span> (sc.read(buf) &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="comment">// 读到了数据, 就拍一下板</span></span><br><span class="line">                buf.flip();</span><br><span class="line">                <span class="comment">// 从缓冲区 buf 写入到 ByteArrayOutputStream 流</span></span><br><span class="line">                baos.write(buf.array(), <span class="number">0</span>, buf.limit());</span><br><span class="line">                <span class="comment">// 写完了就清空 缓冲区buf</span></span><br><span class="line">                buf.clear();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 打印接受到的数据</span></span><br><span class="line">            System.out.println(<span class="keyword">new</span> String(baos.toByteArray()));</span><br><span class="line">            baos.close();</span><br><span class="line">            <span class="comment">// 清空 selectedKeys的 set</span></span><br><span class="line">            sel.selectedKeys().clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="其它的代码参考我的-Github"><a href="#其它的代码参考我的-Github" class="headerlink" title="其它的代码参考我的 Github"></a>其它的代码参考我的 Github</h2><p><a href="https://github.com/airpoet/bigdata/tree/master/Java_Project/NIO" target="_blank" rel="noopener">mygithub</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- GFM-TOC --&gt;
&lt;h1 id=&quot;一-同步-异步-amp-阻塞-非阻塞&quot;&gt;&lt;a href=&quot;#一-同步-异步-amp-阻塞-非阻塞&quot; class=&quot;headerlink&quot; title=&quot;一.  同步, 异步 &amp;amp; 阻塞, 非阻塞&quot;&gt;&lt;/a&gt;一.  同步, 异
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="NIO" scheme="https://airpoet.github.io/categories/Hadoop/NIO/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="NIO" scheme="https://airpoet.github.io/tags/NIO/"/>
    
  </entry>
  
  <entry>
    <title>newObject</title>
    <link href="https://airpoet.github.io/2018/07/09/Java/Interview/Java-Interview/newObject/"/>
    <id>https://airpoet.github.io/2018/07/09/Java/Interview/Java-Interview/newObject/</id>
    <published>2018-07-09T03:26:48.528Z</published>
    <updated>2018-07-09T07:22:23.926Z</updated>
    
    <content type="html"><![CDATA[<h1 id="对象的创建与内存分配"><a href="#对象的创建与内存分配" class="headerlink" title="对象的创建与内存分配"></a>对象的创建与内存分配</h1><h2 id="创建对象"><a href="#创建对象" class="headerlink" title="创建对象"></a>创建对象</h2><p>当 <code>JVM</code> 收到一个 <code>new</code> 指令时，会检查指令中的参数在常量池是否有这个符号的引用，还会检查该类是否已经被<a href="https://github.com/crossoverJie/Java-Interview/blob/master/MD/ClassLoad.md" target="_blank" rel="noopener">加载</a>过了，如果没有的话则要进行一次类加载。</p><p>接着就是分配内存了，通常有两种方式：</p><ul><li>指针碰撞</li><li>空闲列表</li></ul><p>使用指针碰撞的前提是堆内存是<strong>完全工整</strong>的，用过的内存和没用的内存各在一边每次分配的时候只需要将指针向空闲内存一方移动一段和内存大小相等区域即可。</p><p>当堆中已经使用的内存和未使用的内存<strong>互相交错</strong>时，指针碰撞的方式就行不通了，这时就需要采用空闲列表的方式。虚拟机会维护一个空闲的列表，用于记录哪些内存是可以进行分配的，分配时直接从可用内存中直接分配即可。</p><p>堆中的内存是否工整是有<strong>垃圾收集器</strong>来决定的，如果带有压缩功能的垃圾收集器就是采用指针碰撞的方式来进行内存分配的。</p><p>分配内存时也会出现并发问题:</p><p>这样可以在创建对象的时候使用 <code>CAS</code> 这样的乐观锁来保证。</p><p>也可以将内存分配安排在每个线程独有的空间进行，每个线程首先在堆内存中分配一小块内存，称为本地分配缓存(<code>TLAB : Thread Local Allocation Buffer</code>)。</p><p>分配内存时，只需要在自己的分配缓存中分配即可，由于这个内存区域是线程私有的，所以不会出现并发问题。</p><p>可以使用 <code>-XX:+/-UseTLAB</code> 参数来设定 <code>JVM</code> 是否开启 <code>TLAB</code> 。</p><p>内存分配之后需要对该对象进行设置，如对象头。对象头的一些应用可以查看 <a href="https://github.com/crossoverJie/Java-Interview/blob/master/MD/Synchronize.md" target="_blank" rel="noopener">Synchronize 关键字原理</a>。</p><h3 id="对象访问"><a href="#对象访问" class="headerlink" title="对象访问"></a>对象访问</h3><p>一个对象被创建之后自然是为了使用，在 <code>Java</code> 中是通过栈来引用堆内存中的对象来进行操作的。</p><p>对于我们常用的 <code>HotSpot</code> 虚拟机来说，这样引用关系是通过直接指针来关联的。</p><p>如图:</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fnkmy0bvu3j30o60heaaq.jpg" alt=""></p><p>这样的好处就是：在 Java 里进行频繁的对象访问可以提升访问速度(相对于使用句柄池来说)。</p><h2 id="内存分配"><a href="#内存分配" class="headerlink" title="内存分配"></a>内存分配</h2><h3 id="Eden-区分配"><a href="#Eden-区分配" class="headerlink" title="Eden 区分配"></a>Eden 区分配</h3><p>简单的来说对象都是在堆内存中分配的，往细一点看则是优先在 <code>Eden</code> 区分配。</p><p>这里就涉及到堆内存的划分了，为了方便垃圾回收，JVM 将堆内存分为新生代和老年代。</p><p>而新生代中又会划分为 <code>Eden</code> 区，<code>from Survivor、to Survivor</code> 区。</p><p>其中 <code>Eden</code> 和 <code>Survivor</code> 区的比例默认是 <code>8:1:1</code>，当然也支持参数调整 <code>-XX:SurvivorRatio=8</code>。</p><p>当在 <code>Eden</code> 区分配内存不足时，则会发生 <code>minorGC</code> ，由于 <code>Java</code> 对象多数是<strong>朝生夕灭</strong>的特性，所以 <code>minorGC</code> 通常会比较频繁，效率也比较高。</p><p>当发生 <code>minorGC</code> 时，JVM 会根据<a href="https://github.com/crossoverJie/Java-Interview/blob/master/MD/GarbageCollection.md#%E5%A4%8D%E5%88%B6%E7%AE%97%E6%B3%95" target="_blank" rel="noopener">复制算法</a>将存活的对象拷贝到另一个未使用的 <code>Survivor</code> 区，如果 <code>Survivor</code> 区内存不足时，则会使用分配担保策略将对象移动到老年代中。</p><p>谈到 <code>minorGC</code> 时，就不得不提到 <code>fullGC(majorGC)</code> ，这是指发生在老年代的 <code>GC</code> ，不论是效率还是速度都比 <code>minorGC</code> 慢的多，回收时还会发生 <code>stop the world</code> 使程序发生停顿，所以应当尽量避免发生 <code>fullGC</code> 。</p><h3 id="老年代分配"><a href="#老年代分配" class="headerlink" title="老年代分配"></a>老年代分配</h3><p>也有一些情况会导致对象直接在老年代分配，比如当分配一个大对象时(大的数组，很长的字符串)，由于 <code>Eden</code> 区没有足够大的连续空间来分配时，会导致提前触发一次 <code>GC</code>，所以尽量别频繁的创建大对象。</p><p>因此 <code>JVM</code> 会根据一个阈值来判断大于该阈值对象直接分配到老年代，这样可以避免在新生代频繁的发生 <code>GC</code>。</p><p>对于一些在新生代的老对象 <code>JVM</code> 也会根据某种机制移动到老年代中。</p><p>JVM 是根据记录对象年龄的方式来判断该对象是否应该移动到老年代，根据新生代的复制算法，当一个对象被移动到 <code>Survivor</code> 区之后 JVM 就给该对象的年龄记为1，每当熬过一次 <code>minorGC</code> 后对象的年龄就 +1 ，直到达到阈值(默认为15)就移动到老年代中。</p><blockquote><p>可以使用 <code>-XX:MaxTenuringThreshold=15</code> 来配置这个阈值。</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>虽说这些内容略显枯燥，但当应用发生不正常的 <code>GC</code> 时，可以方便更快的定位问题。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;对象的创建与内存分配&quot;&gt;&lt;a href=&quot;#对象的创建与内存分配&quot; class=&quot;headerlink&quot; title=&quot;对象的创建与内存分配&quot;&gt;&lt;/a&gt;对象的创建与内存分配&lt;/h1&gt;&lt;h2 id=&quot;创建对象&quot;&gt;&lt;a href=&quot;#创建对象&quot; class=&quot;head
      
    
    </summary>
    
      <category term="Github" scheme="https://airpoet.github.io/categories/Github/"/>
    
      <category term="Java-Interview" scheme="https://airpoet.github.io/categories/Github/Java-Interview/"/>
    
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Java" scheme="https://airpoet.github.io/tags/Java/"/>
    
      <category term="转载" scheme="https://airpoet.github.io/tags/%E8%BD%AC%E8%BD%BD/"/>
    
  </entry>
  
  <entry>
    <title>LinkedHashMap</title>
    <link href="https://airpoet.github.io/2018/07/09/Java/Interview/Java-Interview/collection/LinkedHashMap/"/>
    <id>https://airpoet.github.io/2018/07/09/Java/Interview/Java-Interview/collection/LinkedHashMap/</id>
    <published>2018-07-09T03:26:48.526Z</published>
    <updated>2018-07-09T07:16:53.075Z</updated>
    
    <content type="html"><![CDATA[<h1 id="LinkedHashMap-底层分析"><a href="#LinkedHashMap-底层分析" class="headerlink" title="LinkedHashMap 底层分析"></a>LinkedHashMap 底层分析</h1><p>众所周知 <a href="https://github.com/crossoverJie/Java-Interview/blob/master/MD/HashMap.md" target="_blank" rel="noopener">HashMap</a> 是一个无序的 <code>Map</code>，因为每次根据 <code>key</code> 的 <code>hashcode</code> 映射到 <code>Entry</code> 数组上，所以遍历出来的顺序并不是写入的顺序。</p><p>因此 JDK 推出一个基于 <code>HashMap</code> 但具有顺序的 <code>LinkedHashMap</code> 来解决有排序需求的场景。</p><p>它的底层是继承于 <code>HashMap</code> 实现的，由一个双向链表所构成。</p><p><code>LinkedHashMap</code> 的排序方式有两种：</p><ul><li>根据写入顺序排序。</li><li>根据访问顺序排序。</li></ul><p>其中根据访问顺序排序时，每次 <code>get</code> 都会将访问的值移动到链表末尾，这样重复操作就能得到一个按照访问顺序排序的链表。</p><h2 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test</span><span class="params">()</span></span>&#123;</span><br><span class="line">Map&lt;String, Integer&gt; map = <span class="keyword">new</span> LinkedHashMap&lt;String, Integer&gt;();</span><br><span class="line">map.put(<span class="string">"1"</span>,<span class="number">1</span>) ;</span><br><span class="line">map.put(<span class="string">"2"</span>,<span class="number">2</span>) ;</span><br><span class="line">map.put(<span class="string">"3"</span>,<span class="number">3</span>) ;</span><br><span class="line">map.put(<span class="string">"4"</span>,<span class="number">4</span>) ;</span><br><span class="line">map.put(<span class="string">"5"</span>,<span class="number">5</span>) ;</span><br><span class="line">System.out.println(map.toString());</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>调试可以看到 <code>map</code> 的组成：</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fo6l9xp91lj319m0s4tgi.jpg" alt=""></p><p>打开源码可以看到：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The head of the doubly linked list.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">transient</span> Entry&lt;K,V&gt; header;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The iteration ordering method for this linked hash map: &lt;tt&gt;true&lt;/tt&gt;</span></span><br><span class="line"><span class="comment"> * for access-order, &lt;tt&gt;false&lt;/tt&gt; for insertion-order.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@serial</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">boolean</span> accessOrder;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Entry</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">HashMap</span>.<span class="title">Entry</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">// These fields comprise the doubly linked list used for iteration.</span></span><br><span class="line">    Entry&lt;K,V&gt; before, after;</span><br><span class="line"></span><br><span class="line">    Entry(<span class="keyword">int</span> hash, K key, V value, HashMap.Entry&lt;K,V&gt; next) &#123;</span><br><span class="line">        <span class="keyword">super</span>(hash, key, value, next);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中 <code>Entry</code> 继承于 <code>HashMap</code> 的 <code>Entry</code>，并新增了上下节点的指针，也就形成了双向链表。</p><p>还有一个 <code>header</code> 的成员变量，是这个双向链表的头结点。 </p><p>上边的 demo 总结成一张图如下：</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fodggwc523j30za0n4wgj.jpg" alt=""></p><p>第一个类似于 <code>HashMap</code> 的结构，利用 <code>Entry</code> 中的 <code>next</code> 指针进行关联。</p><p>下边则是 <code>LinkedHashMap</code> 如何达到有序的关键。</p><p>就是利用了头节点和其余的各个节点之间通过 <code>Entry</code> 中的 <code>after</code> 和 <code>before</code> 指针进行关联。</p><p>其中还有一个 <code>accessOrder</code> 成员变量，默认是 <code>false</code>，默认按照插入顺序排序，为 <code>true</code> 时按照访问顺序排序，也可以调用:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">public LinkedHashMap(int initialCapacity,</span><br><span class="line">                     float loadFactor,</span><br><span class="line">                     boolean accessOrder) &#123;</span><br><span class="line">    super(initialCapacity, loadFactor);</span><br><span class="line">    this.accessOrder = accessOrder;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个构造方法可以显示的传入 <code>accessOrder</code>。</p><h2 id="构造方法"><a href="#构造方法" class="headerlink" title="构造方法"></a>构造方法</h2><p><code>LinkedHashMap</code> 的构造方法:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">LinkedHashMap</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>();</span><br><span class="line">    accessOrder = <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其实就是调用的 <code>HashMap</code> 的构造方法:</p><p><code>HashMap</code> 实现：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">HashMap</span><span class="params">(<span class="keyword">int</span> initialCapacity, <span class="keyword">float</span> loadFactor)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (initialCapacity &lt; <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Illegal initial capacity: "</span> +</span><br><span class="line">                                           initialCapacity);</span><br><span class="line">    <span class="keyword">if</span> (initialCapacity &gt; MAXIMUM_CAPACITY)</span><br><span class="line">        initialCapacity = MAXIMUM_CAPACITY;</span><br><span class="line">    <span class="keyword">if</span> (loadFactor &lt;= <span class="number">0</span> || Float.isNaN(loadFactor))</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Illegal load factor: "</span> +</span><br><span class="line">                                           loadFactor);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.loadFactor = loadFactor;</span><br><span class="line">    threshold = initialCapacity;</span><br><span class="line">    <span class="comment">//HashMap 只是定义了改方法，具体实现交给了 LinkedHashMap</span></span><br><span class="line">    init();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到里面有一个空的 <code>init()</code>，具体是由 <code>LinkedHashMap</code> 来实现的：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    header = <span class="keyword">new</span> Entry&lt;&gt;(-<span class="number">1</span>, <span class="keyword">null</span>, <span class="keyword">null</span>, <span class="keyword">null</span>);</span><br><span class="line">    header.before = header.after = header;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其实也就是对 <code>header</code> 进行了初始化。</p><h2 id="put-方法"><a href="#put-方法" class="headerlink" title="put() 方法"></a>put() 方法</h2><p>看 <code>LinkedHashMap</code> 的 <code>put()</code> 方法之前先看看 <code>HashMap</code> 的 <code>put</code> 方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">public V put(K key, V value) &#123;</span><br><span class="line">    if (table == EMPTY_TABLE) &#123;</span><br><span class="line">        inflateTable(threshold);</span><br><span class="line">    &#125;</span><br><span class="line">    if (key == null)</span><br><span class="line">        return putForNullKey(value);</span><br><span class="line">    int hash = hash(key);</span><br><span class="line">    int i = indexFor(hash, table.length);</span><br><span class="line">    for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123;</span><br><span class="line">        Object k;</span><br><span class="line">        if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123;</span><br><span class="line">            V oldValue = e.value;</span><br><span class="line">            e.value = value;</span><br><span class="line">            //空实现，交给 LinkedHashMap 自己实现</span><br><span class="line">            e.recordAccess(this);</span><br><span class="line">            return oldValue;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    modCount++;</span><br><span class="line">    // LinkedHashMap 对其重写</span><br><span class="line">    addEntry(hash, key, value, i);</span><br><span class="line">    return null;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// LinkedHashMap 对其重写</span><br><span class="line">void addEntry(int hash, K key, V value, int bucketIndex) &#123;</span><br><span class="line">    if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) &#123;</span><br><span class="line">        resize(2 * table.length);</span><br><span class="line">        hash = (null != key) ? hash(key) : 0;</span><br><span class="line">        bucketIndex = indexFor(hash, table.length);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    createEntry(hash, key, value, bucketIndex);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// LinkedHashMap 对其重写</span><br><span class="line">void createEntry(int hash, K key, V value, int bucketIndex) &#123;</span><br><span class="line">    Entry&lt;K,V&gt; e = table[bucketIndex];</span><br><span class="line">    table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e);</span><br><span class="line">    size++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>主体的实现都是借助于 <code>HashMap</code> 来完成的，只是对其中的 <code>recordAccess(), addEntry(), createEntry()</code> 进行了重写。</p><p><code>LinkedHashMap</code> 的实现：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment">//就是判断是否是根据访问顺序排序，如果是则需要将当前这个 Entry 移动到链表的末尾</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">recordAccess</span><span class="params">(HashMap&lt;K,V&gt; m)</span> </span>&#123;</span><br><span class="line">        LinkedHashMap&lt;K,V&gt; lm = (LinkedHashMap&lt;K,V&gt;)m;</span><br><span class="line">        <span class="keyword">if</span> (lm.accessOrder) &#123;</span><br><span class="line">            lm.modCount++;</span><br><span class="line">            remove();</span><br><span class="line">            addBefore(lm.header);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment">//调用了 HashMap 的实现，并判断是否需要删除最少使用的 Entry(默认不删除)    </span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">addEntry</span><span class="params">(<span class="keyword">int</span> hash, K key, V value, <span class="keyword">int</span> bucketIndex)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>.addEntry(hash, key, value, bucketIndex);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Remove eldest entry if instructed</span></span><br><span class="line">    Entry&lt;K,V&gt; eldest = header.after;</span><br><span class="line">    <span class="keyword">if</span> (removeEldestEntry(eldest)) &#123;</span><br><span class="line">        removeEntryForKey(eldest.key);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">createEntry</span><span class="params">(<span class="keyword">int</span> hash, K key, V value, <span class="keyword">int</span> bucketIndex)</span> </span>&#123;</span><br><span class="line">    HashMap.Entry&lt;K,V&gt; old = table[bucketIndex];</span><br><span class="line">    Entry&lt;K,V&gt; e = <span class="keyword">new</span> Entry&lt;&gt;(hash, key, value, old);</span><br><span class="line">    <span class="comment">//就多了这一步，将新增的 Entry 加入到 header 双向链表中</span></span><br><span class="line">    table[bucketIndex] = e;</span><br><span class="line">    e.addBefore(header);</span><br><span class="line">    size++;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//写入到双向链表中</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">addBefore</span><span class="params">(Entry&lt;K,V&gt; existingEntry)</span> </span>&#123;</span><br><span class="line">        after  = existingEntry;</span><br><span class="line">        before = existingEntry.before;</span><br><span class="line">        before.after = <span class="keyword">this</span>;</span><br><span class="line">        after.before = <span class="keyword">this</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="get-方法"><a href="#get-方法" class="headerlink" title="get 方法"></a>get 方法</h2><p>LinkedHashMap 的 <code>get()</code> 方法也重写了：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">get</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">    Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;)getEntry(key);</span><br><span class="line">    <span class="keyword">if</span> (e == <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        </span><br><span class="line">    <span class="comment">//多了一个判断是否是按照访问顺序排序，是则将当前的 Entry 移动到链表头部。   </span></span><br><span class="line">    e.recordAccess(<span class="keyword">this</span>);</span><br><span class="line">    <span class="keyword">return</span> e.value;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">recordAccess</span><span class="params">(HashMap&lt;K,V&gt; m)</span> </span>&#123;</span><br><span class="line">    LinkedHashMap&lt;K,V&gt; lm = (LinkedHashMap&lt;K,V&gt;)m;</span><br><span class="line">    <span class="keyword">if</span> (lm.accessOrder) &#123;</span><br><span class="line">        lm.modCount++;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//删除</span></span><br><span class="line">        remove();</span><br><span class="line">        <span class="comment">//添加到头部</span></span><br><span class="line">        addBefore(lm.header);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>clear()</code> 清空就要比较简单了：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//只需要把指针都指向自己即可，原本那些 Entry 没有引用之后就会被 JVM 自动回收。</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">clear</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>.clear();</span><br><span class="line">    header.before = header.after = header;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总的来说 <code>LinkedHashMap</code> 其实就是对 <code>HashMap</code> 进行了拓展，使用了双向链表来保证了顺序性。</p><p>因为是继承与 <code>HashMap</code> 的，所以一些 <code>HashMap</code> 存在的问题 <code>LinkedHashMap</code> 也会存在，比如不支持并发等。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;LinkedHashMap-底层分析&quot;&gt;&lt;a href=&quot;#LinkedHashMap-底层分析&quot; class=&quot;headerlink&quot; title=&quot;LinkedHashMap 底层分析&quot;&gt;&lt;/a&gt;LinkedHashMap 底层分析&lt;/h1&gt;&lt;p&gt;众所周知 &lt;
      
    
    </summary>
    
      <category term="Github" scheme="https://airpoet.github.io/categories/Github/"/>
    
      <category term="Java-Interview" scheme="https://airpoet.github.io/categories/Github/Java-Interview/"/>
    
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Java" scheme="https://airpoet.github.io/tags/Java/"/>
    
      <category term="转载" scheme="https://airpoet.github.io/tags/%E8%BD%AC%E8%BD%BD/"/>
    
  </entry>
  
  <entry>
    <title>Thread-common-problem</title>
    <link href="https://airpoet.github.io/2018/07/09/Java/Interview/Java-Interview/Thread-common-problem/"/>
    <id>https://airpoet.github.io/2018/07/09/Java/Interview/Java-Interview/Thread-common-problem/</id>
    <published>2018-07-09T03:26:48.524Z</published>
    <updated>2018-07-09T07:24:19.160Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Java-多线程常见问题"><a href="#Java-多线程常见问题" class="headerlink" title="Java 多线程常见问题"></a>Java 多线程常见问题</h1><h2 id="上下文切换"><a href="#上下文切换" class="headerlink" title="上下文切换"></a>上下文切换</h2><p>多线程并不一定是要在多核处理器才支持的，就算是单核也是可以支持多线程的。<br>CPU 通过给每个线程分配一定的时间片，由于时间非常短通常是几十毫秒，所以 CPU 可以不停的切换线程执行任务从而达到了多线程的效果。</p><p>但是由于在线程切换的时候需要保存本次执行的信息(<a href="https://github.com/crossoverJie/Java-Interview/blob/master/MD/MemoryAllocation.md#%E7%A8%8B%E5%BA%8F%E8%AE%A1%E6%95%B0%E5%99%A8" target="_blank" rel="noopener">详见</a>)，在该线程被 CPU 剥夺时间片后又再次运行恢复上次所保存的信息的过程就成为上下文切换。</p><blockquote><p>上下文切换是非常耗效率的。</p></blockquote><p>通常有以下解决方案:</p><ul><li>采用无锁编程，比如将数据按照 <code>Hash(id)</code> 进行取模分段，每个线程处理各自分段的数据，从而避免使用锁。</li><li>采用 CAS(compare and swap) 算法，如 <code>Atomic</code> 包就是采用 CAS 算法(<a href="https://github.com/crossoverJie/Java-Interview/blob/master/Threadcore.md#%E5%8E%9F%E5%AD%90%E6%80%A7" target="_blank" rel="noopener">详见</a>)。</li><li>合理的创建线程，避免创建了一些线程但其中大部分都是出于 <code>waiting</code> 状态，因为每当从 <code>waiting</code> 状态切换到 <code>running</code> 状态都是一次上下文切换。</li></ul><h2 id="死锁"><a href="#死锁" class="headerlink" title="死锁"></a>死锁</h2><p>死锁的场景一般是：线程 A 和线程 B 都在互相等待对方释放锁，或者是其中某个线程在释放锁的时候出现异常如死循环之类的。这时就会导致系统不可用。</p><p>常用的解决方案如下：</p><ul><li>尽量一个线程只获取一个锁。</li><li>一个线程只占用一个资源。</li><li>尝试使用定时锁，至少能保证锁最终会被释放。</li></ul><h2 id="资源限制"><a href="#资源限制" class="headerlink" title="资源限制"></a>资源限制</h2><p>当在带宽有限的情况下一个线程下载某个资源需要 <code>1M/S</code>,当开 10 个线程时速度并不会乘 10 倍，反而还会增加时间，毕竟上下文切换比较耗时。</p><p>如果是受限于资源的话可以采用集群来处理任务，不同的机器来处理不同的数据，就类似于开始提到的无锁编程。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Java-多线程常见问题&quot;&gt;&lt;a href=&quot;#Java-多线程常见问题&quot; class=&quot;headerlink&quot; title=&quot;Java 多线程常见问题&quot;&gt;&lt;/a&gt;Java 多线程常见问题&lt;/h1&gt;&lt;h2 id=&quot;上下文切换&quot;&gt;&lt;a href=&quot;#上下文切换&quot; c
      
    
    </summary>
    
      <category term="Github" scheme="https://airpoet.github.io/categories/Github/"/>
    
      <category term="Java-Interview" scheme="https://airpoet.github.io/categories/Github/Java-Interview/"/>
    
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Java" scheme="https://airpoet.github.io/tags/Java/"/>
    
      <category term="转载" scheme="https://airpoet.github.io/tags/%E8%BD%AC%E8%BD%BD/"/>
    
  </entry>
  
  <entry>
    <title>HashMap</title>
    <link href="https://airpoet.github.io/2018/07/09/Java/Interview/Java-Interview/HashMap/"/>
    <id>https://airpoet.github.io/2018/07/09/Java/Interview/Java-Interview/HashMap/</id>
    <published>2018-07-09T03:26:48.519Z</published>
    <updated>2018-07-09T07:20:35.760Z</updated>
    
    <content type="html"><![CDATA[<h1 id="HashMap-底层分析"><a href="#HashMap-底层分析" class="headerlink" title="HashMap 底层分析"></a>HashMap 底层分析</h1><blockquote><p>以下基于 JDK1.7 分析。</p></blockquote><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fn84b0ftj4j30eb0560sv.jpg" alt=""></p><p>如图所示，HashMap 底层是基于数组和链表实现的。其中有两个重要的参数：</p><ul><li>容量</li><li>负载因子</li></ul><p>容量的默认大小是 16，负载因子是 0.75，当 <code>HashMap</code> 的 <code>size &gt; 16*0.75</code> 时就会发生扩容(容量和负载因子都可以自由调整)。</p><h2 id="put-方法"><a href="#put-方法" class="headerlink" title="put 方法"></a>put 方法</h2><p>首先会将传入的 Key 做 <code>hash</code> 运算计算出 hashcode,然后根据数组长度取模计算出在数组中的 index 下标。</p><p>由于在计算中位运算比取模运算效率高的多，所以 HashMap 规定数组的长度为 <code>2&lt;sup&gt;n</code> 。这样用 <code>2&lt;/sup&gt;n - 1</code> 做位运算与取模效果一致，并且效率还要高出许多。</p><p>由于数组的长度有限，所以难免会出现不同的 Key 通过运算得到的 index 相同，这种情况可以利用链表来解决，HashMap 会在 <code>table[index]</code>处形成链表，采用头插法将数据插入到链表中。</p><h2 id="get-方法"><a href="#get-方法" class="headerlink" title="get 方法"></a>get 方法</h2><p>get 和 put 类似，也是将传入的 Key 计算出 index ，如果该位置上是一个链表就需要遍历整个链表，通过 <code>key.equals(k)</code> 来找到对应的元素。</p><h2 id="遍历方式"><a href="#遍历方式" class="headerlink" title="遍历方式"></a>遍历方式</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Iterator&lt;Map.Entry&lt;String, Integer&gt;&gt; entryIterator = map.entrySet().iterator();</span><br><span class="line">       <span class="keyword">while</span> (entryIterator.hasNext()) &#123;</span><br><span class="line">           Map.Entry&lt;String, Integer&gt; next = entryIterator.next();</span><br><span class="line">           System.out.println(<span class="string">"key="</span> + next.getKey() + <span class="string">" value="</span> + next.getValue());</span><br><span class="line">       &#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Iterator&lt;String&gt; iterator = map.keySet().iterator();</span><br><span class="line">        <span class="keyword">while</span> (iterator.hasNext())&#123;</span><br><span class="line">            String key = iterator.next();</span><br><span class="line">            System.out.println(<span class="string">"key="</span> + key + <span class="string">" value="</span> + map.get(key));</span><br><span class="line"></span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">map.forEach((key,value)-&gt;&#123;</span><br><span class="line">    System.out.println(<span class="string">"key="</span> + key + <span class="string">" value="</span> + value);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p><strong>强烈建议</strong>使用第一种 EntrySet 进行遍历。</p><p>第一种可以把 key value 同时取出，第二种还得需要通过 key 取一次 value，效率较低, 第三种需要 <code>JDK1.8</code> 以上，通过外层遍历 table，内层遍历链表或红黑树。 </p><h2 id="notice"><a href="#notice" class="headerlink" title="notice"></a>notice</h2><p>在并发环境下使用 <code>HashMap</code> 容易出现死循环。</p><p>并发场景发生扩容，调用 <code>resize()</code> 方法里的 <code>rehash()</code> 时，容易出现环形链表。这样当获取一个不存在的 <code>key</code> 时，计算出的 <code>index</code> 正好是环形链表的下标时就会出现死循环。</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fn85u0a0d9j30n20ii0tp.jpg" alt=""></p><blockquote><p>所以 HashMap 只能在单线程中使用，并且尽量的预设容量，尽可能的减少扩容。</p></blockquote><p>在 <code>JDK1.8</code> 中对 <code>HashMap</code> 进行了优化：<br>当 <code>hash</code> 碰撞之后写入链表的长度超过了阈值(默认为8)，链表将会转换为<strong>红黑树</strong>。</p><p>假设 <code>hash</code> 冲突非常严重，一个数组后面接了很长的链表，此时重新的时间复杂度就是 <code>O(n)</code> 。</p><p>如果是红黑树，时间复杂度就是 <code>O(logn)</code> 。</p><p>大大提高了查询效率。</p><p>多线程场景下推荐使用 <a href="https://github.com/crossoverJie/Java-Interview/blob/master/MD/ConcurrentHashMap.md" target="_blank" rel="noopener">ConcurrentHashMap</a>。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;HashMap-底层分析&quot;&gt;&lt;a href=&quot;#HashMap-底层分析&quot; class=&quot;headerlink&quot; title=&quot;HashMap 底层分析&quot;&gt;&lt;/a&gt;HashMap 底层分析&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;以下基于 JDK1.7 分析。&lt;/
      
    
    </summary>
    
      <category term="Github" scheme="https://airpoet.github.io/categories/Github/"/>
    
      <category term="Java-Interview" scheme="https://airpoet.github.io/categories/Github/Java-Interview/"/>
    
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Java" scheme="https://airpoet.github.io/tags/Java/"/>
    
      <category term="转载" scheme="https://airpoet.github.io/tags/%E8%BD%AC%E8%BD%BD/"/>
    
  </entry>
  
  <entry>
    <title>ConcurrentHashMap</title>
    <link href="https://airpoet.github.io/2018/07/09/Java/Interview/Java-Interview/ConcurrentHashMap/"/>
    <id>https://airpoet.github.io/2018/07/09/Java/Interview/Java-Interview/ConcurrentHashMap/</id>
    <published>2018-07-09T03:26:48.516Z</published>
    <updated>2018-07-09T07:19:20.903Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ConcurrentHashMap-实现原理"><a href="#ConcurrentHashMap-实现原理" class="headerlink" title="ConcurrentHashMap 实现原理"></a>ConcurrentHashMap 实现原理</h1><p>由于 <code>HashMap</code> 是一个线程不安全的容器，主要体现在容量大于<code>总量*负载因子</code>发生扩容时会出现环形链表从而导致死循环。</p><p>因此需要支持线程安全的并发容器 <code>ConcurrentHashMap</code> 。</p><h2 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h2><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fn2f5pgxinj30dw0730t7.jpg" alt=""></p><p>如图所示，是由 <code>Segment</code> 数组、<code>HashEntry</code> 数组组成，和 <code>HashMap</code> 一样，仍然是数组加链表组成。</p><p><code>ConcurrentHashMap</code> 采用了分段锁技术，其中 <code>Segment</code> 继承于 <code>ReentrantLock</code>。不会像 <code>HashTable</code> 那样不管是 <code>put</code> 还是 <code>get</code> 操作都需要做同步处理，理论上 ConcurrentHashMap 支持 <code>CurrencyLevel</code> (Segment 数组数量)的线程并发。每当一个线程占用锁访问一个 <code>Segment</code> 时，不会影响到其他的 <code>Segment</code>。</p><h2 id="get-方法"><a href="#get-方法" class="headerlink" title="get 方法"></a>get 方法</h2><p><code>ConcurrentHashMap</code> 的 <code>get</code> 方法是非常高效的，因为整个过程都不需要加锁。</p><p>只需要将 <code>Key</code> 通过 <code>Hash</code> 之后定位到具体的 <code>Segment</code> ，再通过一次 <code>Hash</code> 定位到具体的元素上。由于 <code>HashEntry</code> 中的 <code>value</code> 属性是用 <code>volatile</code> 关键词修饰的，保证了内存可见性，所以每次获取时都是最新值(<a href="https://github.com/crossoverJie/Java-Interview/blob/master/MD/Threadcore.md#%E5%8F%AF%E8%A7%81%E6%80%A7" target="_blank" rel="noopener">volatile 相关知识点</a>)。</p><h2 id="put-方法"><a href="#put-方法" class="headerlink" title="put 方法"></a>put 方法</h2><p>内部 <code>HashEntry</code> 类 ：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">HashEntry</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> hash;</span><br><span class="line">    <span class="keyword">final</span> K key;</span><br><span class="line">    <span class="keyword">volatile</span> V value;</span><br><span class="line">    <span class="keyword">volatile</span> HashEntry&lt;K,V&gt; next;</span><br><span class="line"></span><br><span class="line">    HashEntry(<span class="keyword">int</span> hash, K key, V value, HashEntry&lt;K,V&gt; next) &#123;</span><br><span class="line">        <span class="keyword">this</span>.hash = hash;</span><br><span class="line">        <span class="keyword">this</span>.key = key;</span><br><span class="line">        <span class="keyword">this</span>.value = value;</span><br><span class="line">        <span class="keyword">this</span>.next = next;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>虽然 HashEntry 中的 value 是用 <code>volatile</code> 关键词修饰的，但是并不能保证并发的原子性，所以 put 操作时仍然需要加锁处理。</p><p>首先也是通过 Key 的 Hash 定位到具体的 Segment，在 put 之前会进行一次扩容校验。这里比 HashMap 要好的一点是：HashMap 是插入元素之后再看是否需要扩容，有可能扩容之后后续就没有插入就浪费了本次扩容(扩容非常消耗性能)。</p><p>而 ConcurrentHashMap 不一样，它是先将数据插入之后再检查是否需要扩容，之后再做插入。</p><h2 id="size-方法"><a href="#size-方法" class="headerlink" title="size 方法"></a>size 方法</h2><p>每个 <code>Segment</code> 都有一个 <code>volatile</code> 修饰的全局变量 <code>count</code> ,求整个 <code>ConcurrentHashMap</code> 的 <code>size</code> 时很明显就是将所有的 <code>count</code> 累加即可。但是 <code>volatile</code> 修饰的变量却不能保证多线程的原子性，所有直接累加很容易出现并发问题。</p><p>但如果每次调用 <code>size</code> 方法将其余的修改操作加锁效率也很低。所以做法是先尝试两次将 <code>count</code> 累加，如果容器的 <code>count</code> 发生了变化再加锁来统计 <code>size</code>。</p><p>至于 <code>ConcurrentHashMap</code> 是如何知道在统计时大小发生了变化呢，每个 <code>Segment</code> 都有一个 <code>modCount</code> 变量，每当进行一次 <code>put remove</code> 等操作，<code>modCount</code> 将会 +1。只要 <code>modCount</code> 发生了变化就认为容器的大小也在发生变化。</p><blockquote><p>以上内容 base JDK1.7，1.8 的实现更加复杂但是原理类似，建议在 1.7 的基础上查看源码。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;ConcurrentHashMap-实现原理&quot;&gt;&lt;a href=&quot;#ConcurrentHashMap-实现原理&quot; class=&quot;headerlink&quot; title=&quot;ConcurrentHashMap 实现原理&quot;&gt;&lt;/a&gt;ConcurrentHashMap 实现
      
    
    </summary>
    
      <category term="Github" scheme="https://airpoet.github.io/categories/Github/"/>
    
      <category term="Java-Interview" scheme="https://airpoet.github.io/categories/Github/Java-Interview/"/>
    
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Java" scheme="https://airpoet.github.io/tags/Java/"/>
    
      <category term="转载" scheme="https://airpoet.github.io/tags/%E8%BD%AC%E8%BD%BD/"/>
    
  </entry>
  
  <entry>
    <title>爬虫的简单入门</title>
    <link href="https://airpoet.github.io/2018/07/06/Python/Python%E7%AE%80%E5%8D%95%E5%85%A5%E9%97%A8/"/>
    <id>https://airpoet.github.io/2018/07/06/Python/Python简单入门/</id>
    <published>2018-07-06T01:29:54.058Z</published>
    <updated>2018-07-23T07:47:02.388Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-数据来源"><a href="#1-数据来源" class="headerlink" title="1.数据来源"></a>1.数据来源</h2><ul><li>业务库</li><li>日志数据</li><li>公共数据</li><li>购买 – 倒卖 – 有法律风险</li></ul><h2 id="2-爬虫工程师技能清单"><a href="#2-爬虫工程师技能清单" class="headerlink" title="2.爬虫工程师技能清单"></a>2.爬虫工程师技能清单</h2><ol><li>python编程语言基础</li><li>HTTP协议</li><li>html,css,javascript基本web技能</li><li>mysql/mongodb/redis等存储系统</li><li>scrapy/pyspider/django</li><li>抓包工具和网页分析工具(正则，bs4，xpath，selenuim)</li><li>json/csv/db</li></ol><h2 id="3-Python3基础内容"><a href="#3-Python3基础内容" class="headerlink" title="3.Python3基础内容"></a>3.Python3基础内容</h2><p><a href="http://www.liaoxuefeng.com/" target="_blank" rel="noopener">廖雪峰Python3教程(文档)</a><br><a href="http://www.runoob.com/python3/python3-tutorial.html" target="_blank" rel="noopener">菜鸟教程Python3教程(文档)</a></p><p>python编程语言简单介绍（产生背景，优缺点，流行度）<br>python的开发环境搭建（linux，windows，python，pycharm）<br>python的hello world<br>python关键字查看<br>python的变量定义<br>python的数据类型（Number String List Tuple Set Dict）<br>python的注释（单行和段落）<br>python的输入输出（print 和 input）<br>python数据类型转换/常用数值运算/类型判断<br>python的集合，列表，元组，字典<br>python的流程控制for和while和if（break， continue， pass）<br>python的切片<br>python的代码缩进（换行，段落）<br>python函数（自定义函数，常用内置模块，常用函数，函数调用）<br>python异常<br>python模块（内置模块，导入模块，自定义模块）<br>python迭代器和生成器<br>python面向对象<br>python读写文件IO<br>python数据库和JSON和CSV</p><h4 id="3-1-基本语法记录"><a href="#3-1-基本语法记录" class="headerlink" title="3.1 基本语法记录"></a>3.1 基本语法记录</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 同时遍历2个长度相同的 list</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(companys)):</span><br><span class="line">    print(companys[i] + <span class="string">","</span> + fincs[i])</span><br><span class="line">    </span><br><span class="line">jobs <span class="comment"># 工作岗位</span></span><br><span class="line">companys <span class="comment">#公司名</span></span><br><span class="line">m</span><br><span class="line">moneys <span class="comment"># 薪资</span></span><br><span class="line">edus <span class="comment"># 学历</span></span><br><span class="line">exps <span class="comment"># 经验</span></span><br><span class="line">cmptypes <span class="comment"># 公司类型</span></span><br><span class="line">fincs  <span class="comment"># 融资状态</span></span><br><span class="line">true_tags <span class="comment">#job 标签</span></span><br><span class="line">c_b_s  <span class="comment"># 公司优势</span></span><br></pre></td></tr></table></figure><h2 id="4-搜索引擎基本工作原理"><a href="#4-搜索引擎基本工作原理" class="headerlink" title="4.搜索引擎基本工作原理"></a>4.搜索引擎基本工作原理</h2><p><img src="http://p6i5vzkfk.bkt.clouddn.com/study/2018-07-06-023330.png" alt="image-20180706103329915"></p><h2 id="5-python-基本操作-amp-爬虫代码"><a href="#5-python-基本操作-amp-爬虫代码" class="headerlink" title="5.python 基本操作 &amp; 爬虫代码"></a>5.python 基本操作 &amp; 爬虫代码</h2><h4 id="5-1基本语法"><a href="#5-1基本语法" class="headerlink" title="5.1基本语法"></a>5.1基本语法</h4><p><a href="https://github.com/airpoet/bigdata/tree/master/Python_Project/python_basic" target="_blank" rel="noopener">https://github.com/airpoet/bigdata/tree/master/Python_Project/python_basic</a></p><h4 id="5-2-高级语法-tcp-udp-多线程-面向对象OOP-py-操作-mysql"><a href="#5-2-高级语法-tcp-udp-多线程-面向对象OOP-py-操作-mysql" class="headerlink" title="5.2 高级语法(tcp,udp / 多线程 /  面向对象OOP /  py 操作 mysql)"></a>5.2 高级语法(tcp,udp / 多线程 /  面向对象OOP /  py 操作 mysql)</h4><p><a href="https://github.com/airpoet/bigdata/tree/master/Python_Project/mypython-1" target="_blank" rel="noopener">https://github.com/airpoet/bigdata/tree/master/Python_Project/mypython-1</a></p><h4 id="5-3-爬虫"><a href="#5-3-爬虫" class="headerlink" title="5.3 爬虫"></a>5.3 爬虫</h4><p><a href="https://github.com/airpoet/bigdata/tree/master/Python_Project/spiderDemo" target="_blank" rel="noopener">https://github.com/airpoet/bigdata/tree/master/Python_Project/spiderDemo</a></p><h4 id="5-5-python-与-Hadoop-Spark-生态的交互"><a href="#5-5-python-与-Hadoop-Spark-生态的交互" class="headerlink" title="5.5 python 与 Hadoop / Spark 生态的交互"></a>5.5 python 与 Hadoop / Spark 生态的交互</h4><p><a href="https://github.com/airpoet/bigdata/tree/master/Spark_Project/HBasePythonDemo" target="_blank" rel="noopener">https://github.com/airpoet/bigdata/tree/master/Spark_Project/HBasePythonDemo</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-数据来源&quot;&gt;&lt;a href=&quot;#1-数据来源&quot; class=&quot;headerlink&quot; title=&quot;1.数据来源&quot;&gt;&lt;/a&gt;1.数据来源&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;业务库&lt;/li&gt;
&lt;li&gt;日志数据&lt;/li&gt;
&lt;li&gt;公共数据&lt;/li&gt;
&lt;li&gt;购买 – 倒卖
      
    
    </summary>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/categories/Hadoop/"/>
    
      <category term="爬虫" scheme="https://airpoet.github.io/categories/Hadoop/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Hadoop" scheme="https://airpoet.github.io/tags/Hadoop/"/>
    
      <category term="爬虫" scheme="https://airpoet.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>CentOS6.x下Python3的安装</title>
    <link href="https://airpoet.github.io/2018/07/05/Python/CentOS6.x%E4%B8%8BPython3%E7%9A%84%E5%AE%89%E8%A3%85/"/>
    <id>https://airpoet.github.io/2018/07/05/Python/CentOS6.x下Python3的安装/</id>
    <published>2018-07-05T06:03:43.250Z</published>
    <updated>2018-07-05T06:06:10.115Z</updated>
    
    <content type="html"><![CDATA[<h3 id="python-3-6-4在centos-6-7安装："><a href="#python-3-6-4在centos-6-7安装：" class="headerlink" title="python-3.6.4在centos-6.7安装："></a><strong>python-3.6.4在centos-6.7安装：</strong></h3><blockquote><p>详细步骤：</p></blockquote><p><strong>1、安装一些依赖的软件包</strong><br>yum -y groupinstall “Development tools”<br>yum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel<br>yum -y install yum-plugin-remove-with-leaves</p><p><strong>2、下载Python3.6的源码包并编译（在/usr/local目录下）</strong><br>wget <a href="https://www.python.org/ftp/python/3.6.4/Python-3.6.4.tgz" target="_blank" rel="noopener">https://www.python.org/ftp/python/3.6.4/Python-3.6.4.tgz</a><br>tar -vxf Python-3.6.4.tgz -C ~/apps<br>cd Python-3.6.4<br>mkdir /usr/local/python3<br>./configure –prefix=/usr/local/python3 –enable-shared –enable-optimizations<br>make<br>make install</p><p><strong>3、把新安装的python3.6拷贝到/usr/bin/目录下</strong><br>cp /usr/local/python3/bin/python3.6 /usr/bin/python3.6<br>然后新建快捷方式：<br>ln -s /usr/bin/python3.6 /usr/bin/python3</p><p><strong>4、一步操作：</strong><br>echo /usr/local/python3/lib/ &gt;&gt; /etc/ld.so.conf.d/local.conf<br>ldconfig</p><p><strong>5、验证安装是否成功</strong><br>/usr/bin/python3 –version</p><p><strong>6、配置环境变量</strong><br>vim /etc/profile<br>export PYTHON_HOME=/usr/local/python3<br>export PATH=$PATH:$PYTHON_HOME/bin<br>source /etc/profile</p><p><strong>7、尝试安装一个模块</strong><br>pip3 install beautifulsoup4</p><p><strong>8、修改pip源：</strong><br>mkdir ~/.pip<br>cd ~/.pip<br>vi pip.conf<br>[global]<br>trusted-host =  pypi.douban.com<br>index-url = <a href="http://pypi.douban.com/simple" target="_blank" rel="noopener">http://pypi.douban.com/simple</a></p><p><strong>9、安装numpy</strong><br>pip3 install numpy</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;python-3-6-4在centos-6-7安装：&quot;&gt;&lt;a href=&quot;#python-3-6-4在centos-6-7安装：&quot; class=&quot;headerlink&quot; title=&quot;python-3.6.4在centos-6.7安装：&quot;&gt;&lt;/a&gt;&lt;strong&gt;
      
    
    </summary>
    
      <category term="Python" scheme="https://airpoet.github.io/categories/Python/"/>
    
    
      <category term="原创" scheme="https://airpoet.github.io/tags/%E5%8E%9F%E5%88%9B/"/>
    
      <category term="技术" scheme="https://airpoet.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Python" scheme="https://airpoet.github.io/tags/Python/"/>
    
  </entry>
  
</feed>
